Thank you very much, Sergei, for this invitation. And so I emphasize electron density right away here. I'll spend some slides on that as well, because it's actually important for the strategy behind this force field, which will bring in machine learning. So it's, you could also call it a machine learning learning learning learning. You could also call it a machine learning potential. And yeah, we start with a big carrot that may surprise you, namely that we try to make reliable predictions of the structure and dynamics of peptides and proteins in aqueous solution. And if you turn around and say to me, well, really, we have been using amber and charm. Amber and charm and other false fields for decades now. Is this really a challenge? And then, unfortunately, the sad answer is yes. There is a literature that I will not review here, but certainly since 2000, I think of a paper 2003 showing that there are already discrepancies, serious discrepancies between Serious discrepancies between computational, say, confirmational predictions and experiment for very small peptides. And here I just take an example. It's part of a story that goes on, but this is very dramatic. This comes from the abstract saying that changing the force field between various versions or between force fields. Or between phosphorus has a stronger effect on secondary structure than changing the entire peptide sequence. I think that's a very serious problem. And then the winner, if you like, is this version. But then later on, that has been taken on by DE Shaw, and then they find that for their systems, that's not the right. For their systems, that's not the right, the correct predictor either, or there's another winner there. So against this background, I decided, rightly or wrongly, to start all over again, admittedly many years ago, and to actually overhaul the current Foss field architecture. So not to think of So not to think of bonded potentials, harmonic-anharmonic corrections, valence, angle potentials, and torsional potentials, etc. But to go back to the atom itself and look at it the way it sits inside a system and then ask how is an atom preserved, if you like. I have a metaphor here. I have a metaphor here. The sun is an enormous object that has been constructed a very long time ago from gas. And there it sits with an enormous energy. That is the atom itself, if you like, constructed from the nucleus and electrons. It's an enormous amount of energy. But we're not interested in constructing or deconstructing that. Constructing or deconstructing that, we want to largely preserve it and put it inside a molecule or a cluster, van der Waals cluster, anything. So going back to this metaphor, we're interested in the solar wind, the Pro 2 branzers, I think it's called, that come from the sun. And that's really chemistry. Chemistry is about small effects. Effects tens of kilojoules per mole up to a kilojoule per mole for covalent bonds, hundreds of kilojoules per mole. So, you know, there is no unique answer to this, but we are proceeding with Q-Tame, which has been reviewed by Sheriff Matter earlier today. I will still insert a slide to further elaborate on that. On that. But there are ways of defining atoms' atomic properties in Hilbert space, and that's not what we're going to do. We're going to start from real space, 3D space, the electronicity itself, and then obtain parameter-free topological atoms, which also have nice quantum properties. And here is an example. So I'll speak. So, I'll speak quite a lot about amino acids and in their peptide environment. So, this is a peptide bond and capped by a methyl. Here's this C-alpha. This is the methyl group that makes it an alanine. If this is hydrogen, then you have glycine. And we think right away about the transferability of atoms. There are quite a few studies. Quite a few studies, we carried out a number of them to see to what extent this carbon here inside this single amino acid has, for example, the same charge as a peptideptide or five alanins or three alanins. And then you can think of and speak of what we call an atomic horizon. So, how far do you have to? Horizon. So, so how far do you have to go for this charge not to change at all? Well, very, very little, say less than a milli-electron. I say right away that in machine learning, this has actually a different meaning. I'll come back to that. But we're not speaking about machine learning yet. I first want to review how you come to these shapes. They're non-overlapping atoms. Overlapping atoms. So there's a boundary here, a zero flux surface between hydrogen and the carbon. And I'll show you very quickly how this is constructed by means of a molecule that we don't work on at all. It's a hydrocarbon. Ian Dylan was kind enough to show me or to allow me to show this because it's all in one. So this, because it's all in one slide. So this is the outer or a outer contour electron density contour of this molecule, which with traditional bowl and stick looks like this. And the nuclei are here. That's all very well. But then where it starts, the Q10 part is here, where you start drawing the so-called gradient parts. So they actually come from infinity. So they actually come from infinity, and this one, for example, here is attracted to this nucleus. Same here. So they form a bundle that carves out a subspace that we then associate, in this case, with the hydrogen. For carbon, this pattern is more complicated, but I think you get the idea. You then have a volume for this atom, and you can also draw different types of gradient paths originating from a so-called Originating from a so-called bond critical point, emanating from here and going to that carbon, and another one going to there, recovering a bonding pattern. This is that zero flux surface that I spoke about, which again is a bundle of gradient parts coming from infinity, being attracted to this critical point here. And if you want to complete it, then you have this sharp. This sharp surface delineating the boundary between the two atoms there. You can then cap it for practical reasons, otherwise it goes on to infinity by a contour surface, and then you can add the carbon to it, which has a shape that has to look like this because of non-overlapping, the non-overlapping properties, no gaps between the atoms either. No gaps between the atoms either. They don't penetrate each other, and then you can complete the whole thing like this, and then you're back to where you started. So apologies for the speed of this, but hopefully this complements what Sheriff has said earlier. So let's emphasize then this platform, if you like. The starting point for the partitioning is row in 3D. And that can be fed in by an experimental method. I mean, high-resolution crystallography is very well developed and had adopted Q-TAME early on. For all our calculations here, all the results I'm showing, I'm using this route where I emphasize that the basis set can be anything. Now, this is important if I can just take. Now, this is important if I can just take this, expand this for a moment. So, if you work with molecular charges, for example, then you will have difficulty getting stable or meaningful numbers if your Gaussians become more and more diffuse. That's because there is then a problem with the decision making of that partitioning scheme of molecular, namely that the nucleus to which on which the On which the function is centered is the owner of all the density that that function describes. So, if the function drops off very slowly, then you can see that the nucleus starts grabbing electron density far away, which is problematic. With plane waves, the whole scheme evaporates because there's just no centering anymore. But if you start from the electron density, then you're safe against. Then you're safe against all these problems. You could also work with a grid. So, hopefully, that convinces people that this is a good starting point. And then moving on to an example of the corresponding charge changing with confirmation, then you have this histidine example here. Have this histidine example here, where this oxygen is changing color going through the rainbow spectrum from red, which is very negatively charged, to less so, and then back. Okay, so charges are important, but unfortunately, they are not good enough if you are interested in shorter-range interactions. However, However, classical force fields amber are just point charges. Then you have this kind of monopole-multipole interaction that we all know very well. But from undergraduate courses, you may know this, so dipole-dipole, and more complicated formula for quadruple-quadupole, and so on. So to actually get the exact density. get the exact density you need to go sorry the exact electrostatic interaction for given densities between atoms you you need to go to sufficiently high l values the general formula is here so that's the interaction tensor these are the multiple moments and this l is governing which term you have in this expansion which i tell you may diverge and indeed it does at And indeed, it does at very short range. I'll let you know what to do about that. But here is an example: so the water dimer, oxygen, hydrogen, the exact answer is here in kilojoules per mole minus, what is it, 415 or something, this line here. And you see that for increasing rank of expansion, this L is just is here, so this is one, so that's just monopole, monopole. So that's just monopole, monopole two, three that this answer is dancing around the correct answer, but really is within reasonable error the correct answer. But this then happens. You could say, well, I don't care about that. I can truncate it here at L equals five. That's fine. You're lucky here with L equals two. It's not too bad. But basically, you're at the mercy. But basically, you're at the mercy of the convergence behavior. The good news is that for longer, this is a quite short distance, actually, but for longer distances, these curves become very stable and you actually can get exact formal convergence. So if you then, well, generalize all this into a five-dimensional relationship. So you have an atom pair A, B. So for example, oxygen, hydrogen. So, for example, oxygen, hydrogen. Then, there are three things that you can correlate or investigate: their distance, of course, and at what rank L you will get what error compared to the exact answer. So that's a five-dimensional function which you can rearrange. You can make delta E the outcome when you vary L. So, if you then do a section. So, if you then do a section as an example here, an intersection, the OH interactions in a very small protein called crambin, then, well, let's look, for example, at L equals 2, that means one charge, one dipole. This green curve here, you see that for an error of 0.1 kilojoule per mole, which is very small, if you're beyond six angstrom, you're really always below. angstrom you're really always below that you could be more tolerant and say i'm actually allowing a kilojoule pommel as an error which is quite generous but then you're done much quicker with this curve and then you can just look at the others and have a feel for what sort of multiple moments are necessary to to get you know accurate electrostatic interaction that so that's what we started with So that's what we started with, which is different to what Jörg Baylor does with his neural net, his deep neural nets. He started with silicon, where there is no charge transfer. And the design of his strategies is kind of upside down from our point of view. He adds the electrostatic interaction later on. We start with it. But yes, so. But yes, so this is when the machine learning comes in a long time ago when we used Krieging. We actually started with neural nets, but we compared it with what is now better known as Gaussian process regression and decided that for an extra computational cost, you get higher accuracy with Krieging for fewer data points. So we never looked back, as it were. Never looked back, as it were. And we then use that to predict atomic charges, but also predict atomic dipole moments, quadrupole moments, and so on. I realize that I am going slower than I thought, but so I can't discuss this in great detail. I'm happy to answer any questions about it, but it's basically a kernel-based method with a covariance matrix. Matrix, it's completely different to neural nets. You actually need the original data to make a prediction. That also means that you can exactly predict your original data. And so here you have a formula in the end where the predictions are made for an unknown point. And this, the dimension of this matrix needs to be... The dimension of this matrix needs to be carefully controlled because this is typically 2000 by 2000, but there is technology to invert it quickly, etc. But anyway, it's a challenge that has been overcome, but it's very different to neural nets. So if we then go back to early results, then we look at S-curves, as we call them. Look at S-curves as we call them, sigmoidal curves, where you have the percentile. So, the percent. So, for example, if you read this off for isoleucine, if you take, say, one, two, three, four, one kilocalper mole here, and you look at the intersection, there's about 70% of the test set has an error up to a kilocalper mole. Okay, admittedly, that's only for the electric. That's only for the electrostatic interaction. That's what we started with inside these capped amino acids, and we looked at all 20 of them. Then you look at the mean error, which is all these are numbers that you can work with. I mean, they're not good enough eventually, but you keep pushing this down and the average of the average. And the average of the average is then about a kilocal per mole. This is so the way that these molecules have been distorted is through normal modes. We now know that you need to do bigger distortions and we can handle that. That puts more pressure on the machine learning. But meanwhile, the machine learning has improved as well. There's lots of details. There's lots of details that I can't go into, but maybe in the questions. So let's then bring in the underpinning energy decomposition method called interacting quantum atoms, IQA, which is completely building on Q-Tame. And that brings in non-electrostatic contributions to the energy as well as. Energy, as well as the so-called exact electrostatic energy that follows from a six-dimensional integration between two atoms and that then avoids the multiple expansion. So there's no convergence problems. This is true here for anything, kinetic energy. This is a beast of a thing. It's the two particle density matrix. The one particle here. The one particle here, nuclear electron, and of course, nucleus, nucleus, that should all ring a bell. But this thing here needs to be inspected. That has the electrostatic Coulomb. Then the exchange part, which we will see is linked to covalency, which is a sliding scale. And then the most difficult. And then the most difficult part is this dynamic correlation, which is going to contain dispersion. So there's a meaning to all of these contributions. So that means that as we stay close to the quantum mechanics and have atoms that are transferable, we are aiming for. Aiming for structural predictions or geometry optimizations, but also following a system in time. So, do a molecular dynamic simulation with the energy functions and the forces, see whether it stays robust, does it explode or not. But alongside, we can then follow all these chemical properties, which I think you cannot necessarily. I think you cannot necessarily do with classical force fields. Okay, so wrapping up the NRQA and showing further how it works. So this is a topological atom, another one. And you have a six-dimensional integration. So volume-volume for these inter-atomic terms in purple, only a three-dimensional integration for Integration for an intra-atomic energy. Some more details here. So you see, this is algorithmically quite complicated, but has been solved many years ago and is available in programs. If you want to have some more details, then it looks like this. I think this was mentioned in a question section with Sheriff. A question section with Sheriff and Julia about steroid effects. So, here's a way of quantifying steric effects independently of NCI. So, apologies if I possibly overwhelm you with details. So, I'm conscious of us having to see the bigger picture again. So, this slide hopefully helps. So going back to flux, so what is what? Well, the atom is coming from a cutane. The long range is electrostatics is taken care of by multiples in the spherical tensor formalism, actually. Then the short range for short range Coulomb exchange correlation is coming from IQA. And then the machine learning models. Learning models monitor the internal energies of the atoms as well as the charges. So, charge transfer effects are covered by such models. And then the classical polarization, which is actually dipolar polarization, is covered by other. So this property here is then a dipole moment. And that's again then covered by this model. This model. Everything I will say today or speak about today is about single, it's not necessarily molecules, but I should say single systems, where the model is the system. What do I mean by that? Well, you could say I can go further and take a backbone, so this whole protein here. And what you're not going to do is have a terrain. A terrain for the density of the whole thing. That would be foolish. What you do instead is you have a sauce molecule that is smaller. You grab the fragments from it and then you dress up this backbone here with these fragments. We call that knitting colorfully or gluing the models together. That's not published yet, but we have the first test cases ready. Already. So, this is then an exciting prospect for the future. Okay, so geometry optimization: one has to, with new methods, one have to start small and look at tiny things like methanol and make sure that with a modest number of training points, you actually recover the right answer. The dimensionality is quickly going up, as you'll see in a minute. As you'll see in a minute. But here we go further than normal modes. So we have torsional motion, which you wouldn't cover with normal modes. A bit bigger than is glycine. So numerically, you recover the geometrical features quite well, as you can see here, even the energy as well. Energy as well. And if you want to summarize that in a movie, then you see that in real time, if you like, you see this molecule collapse to a global minimum that has a seven-membered hydrogen bond. Sorry, maybe I should show that again. So this would... This was with 4,000 training, sorry, 1,000 training points only. And it took a long time to make this movie, actually. Okay, so Sergei will not like this, but you can go to higher dimensions. And of course, everybody's looking at paracetamol. You then have the S-curves again. This is an older method. Method. But yeah, so this S-curve is reasonable. It should still improve, I think. You can then inspect it per atom and then see that some atoms are doing very well. So this one here is probably some hydrogens are all finished. If you like 100%, hit the ceiling here, well before a kilojoule per mole. You can You can look at the monopoles. You can look at those S-curves. So, this is then a millie electron here. Again, there's a whole bunch of answers here. For carbon, you get worse results than for hydrogen, as expected. Instead of staring at these S-curves, it's perhaps better to use the charges and look at heat maps like this. Look at heat maps like this. So, you have 20 atoms here from this molecule, 20 from this, and you let each atom interact between the molecules. Okay, so you have 400 or a little bit more here. No, so 400 interactions. And then you can look at the RMSC in kilojoule per mole and see that there is a bit of a problem here, but it's still manageable, and the rest is really good. We, at the same time, I mean, this is all in-house software, are conscious that maybe one day people will want to use this. So we spend time on making clear how it all works. This is also true for the pipeline, as we call it. There's a lot of data that needs to be manipulated. To be manipulated in this environment, and you don't want to do that manually. So, this is highly automated. We strive for a click-ready environment. I'm conscious of the time. If I have another five minutes, then I'm done, if that's okay. So, more recently, then a very creative student here who Creative student here who, for the first time, actually brought in the gray wolf optimizer. I've not explained where that sits exactly, but I'm happy to amplify that. That turned out to be better than particle swarm optimization. That is in the construction of these Gaussian process regression models. And then you can again monitor. Again, monitor that for all the carbons here in alanine, where you have so-called mist plots. So you actually get a visual feel for how distorted the molecule is. And it's quite considerable compared certainly to normal modes. And again, so you have here, you know, kilojoule per mole is still sometimes under it, sometimes about. Sometimes you're under it, sometimes about there. It looks very promising, I think. Now, this is where everything comes together. We want to eventually do condensed matter, still some way off, but not too far. So I spoke about multipolar statics, convergence. I spoke about machine learning the S-curves, spent time on that, how IQA comes in. That's how IQA comes in for the various energy types. Training and sampling is a perpetual challenge for anybody doing machine learning. There's many groups are looking at that as well. I didn't speak about the forces, how they're calculated, but we do not train on forces. Geometry optimization. This is a talk by itself. So I can't. I can't really say much about it quickly. But the idea is really that there should be no need for Leonard Jones or TikTink or Scheffler or Grimm or anything. It should all come from the IQA. And these calculations take a lot of time. So we also have to spend time on optimizing the code. I didn't speak about E-Walt. That's almost a That's almost a talk by itself as well. And I did touch upon transferability, mentioning atomic horizon. Okay, so I'm sort of in wrap-up mode now. I emphasize this constantly to my group because I think this is extremely important, is that all the multiple moments, the energies come from one thing. Energies come from one thing, and that's not the case for alternatives. The approach of coal, for example, has this version coming from Tkachenko or Scheffler and the charges suddenly from the DDEC method, which has absolutely nothing to do with it. And I think that's dangerous. Modularity is another feature or advantage, I think, is that we have Is that we have clear meanings of the various types of energy, and the method is also flux is physics-based, so it's systematically improvable, which is not true for force fields in general. Even classical force fields, you can take out the electrostatic term, improve it by itself, plug it back in, and you'll find that, although you've done a good job. Although you've done a good job with electrostatics, that overall the force field is worse, which should not be possible. Transferability, I didn't speak about that, but I'll throw it in now. The machine learning that we do is acting on already partitioned information. I say that because the majority of approaches is allowing the machine learning method itself. Machine learning method itself to cut the system into atoms. And that's proving to be a vulnerability, to put it mildly. And that's why they keep coming back to, well, there has to be physics, there has to be sort of a guide to do that. And that's where they use the word transferability in the machine learning context. But of course, we have this in the physical context independently of machine learning. Of machine learning. I like to think that this is an advantage as well: that atoms are atoms or atoms. So, this is an amino acid that sits in water, so surrounded by waters. So, there is no, in this method, is no deep imprint of what is a molecule and what is a solute and a solvent. The whole machinery is about. The whole machinery is about atoms wherever they are interacting with each other. That's, of course, already said. Well, the unique here means that actually in bio-molecular simulation, GPLs are very rare. And then chemical meaning and insight I emphasized many times. Okay, my last slide other than acknowledgment. My last slide, other than acknowledgements. So, going back to the very beginning, so we started without any traditional terms. So, we're not modifying something like charm or amber. Hopefully, you're convinced that typical phosphorus don't see the electrons, they just, well, fit, say, harmonic functions to in the hook spring picture. Picture to a change in energy. But here we go back to the electron density and the second order density matrix as well. That I emphasized already. Same scheme. And we are under a kilo cal, not quite always under a kilojoule per mole, so still work to do. So, still work to do. High-dimensional training is possible. And that's actually referring to the next statement. We're making it so automatic that it's not an art, which some users of other force fields would say. And then finally, a lot of people have been working on it. So, don't worry, these are all gone here. These are still here. These are summer students, so we. These are summer students, so we're conscious also to bring in novices to almost expose them to the software and see whether they get it and then it's user-friendly enough. And thank you for listening. And I'm ready for any questions.