By a base of tensor product lines. And at the same time, you have to take a sequence of nested domains which are nested in the other direction. So these domains have to become smaller. The spaces get bigger and the domains get smaller. So you solve this black domain here for the coarsest space and then you take the blue domain for the final space and the red domain for the even final space. And then this cuffed basis is defined. And then this cuffed basis is defined by interaction procedure. So in each level, you pick the B splines with the property that the pot is contained in the level in the associated domain, but is not contained in the next spinal level. So if, for instance, for this blue level here, you take all these blue beast lines represented by 2 by 2 Celsius, but you would not take one which is contained in the red region. And there's only a little exception at the lowest level because you have. The exception at the lowest level because you have to define the support appropriately. So you have to ignore the support of the support which is outside. That's why it is shown here in the hedged area. Okay, and so this is what you get here. When you take a mesh which is defined like this, then you get the B splinter zero, or these are all the B spins, the tensor product B splines, and then you select the ones which contribute something to the spline function on these. Something to the spine function on these causes boxes. These are these B splines. And then on the next level, you pick these ones, they contribute something to these cells of size one half. And then from level 3, 2, you select ones where you contribute something on these cells of size 1 over 4 in the southern part of this domain. Okay, and then the linear independence is automatic. The linear independence is automatically guaranteed. This is actually implied by the local linear independence of the beast planes, which means the bead plants are linearly independent on every cell. And this implies a global linear independence on this. But the hierarchical basis is not locally linearly independent. It's not, no, no. No. No, just the whenever you have a sequence of spaces where you have the property of local linear independence, you can use the same procedure to give a to get a base. To give a piece, yeah, and then you can generate functions that form a weighted partition of unity by scaling them, right? So you know that when you exert the assumptions of the Debian hierarchy, you can actually ensure that these coefficients are positive. This is actually useful for geometric modeling. Okay, and then we run the same example with a flexion diffusion, and it works somehow better than for Better than for the T splines. So, this one's the final picture for the T splines, which almost global definite, and this is the final picture for the hierarchical beast plans. And it behaves perfectly locally. And then some people also wanted to use these hierarchical beast lines in isochiometric analysis. And yeah, now there are some additional results you can look into. One of the questions is the isotra completeness. It means if you have given some article. If you have given some hypothetical grid like this one here, and you define this blind space by saying you want to have all the piecewise polynomial functions with some degree and some smoothness. Then it is not true that you get all of them in the general case. It's only true under certain conditions on this hierarchy. So maybe you have to look at what we call the ring. The ring is something with a hole in the middle. You have to take the difference of the You have to take the difference of the full domain and a subset selected for a finite. So the first ring would be the full domain minus this L shape. And the second ring would be the full domain minus this level two domain. Final ring is everything. Yeah, and the answer to this algebraic completeness question is given here. So this shows the answer for dimension 2 for smoothness p minus 1. P minus one and degree p. And its answer is yes, if these difference domains admit offsets at some distance, at distance p minus one or rather this is like a generalized version of the offset because you have to do the offset with respect to the taxi cup metric. But it's shown somewhere here for degree 2, 2. So your domain is somewhere about whatever is shown in black. And this purple line is the offset, right? It's the offset at this point. The offset, right? It's the offset at distance one-half, right? Something like that. So, you don't do somehow the taxicab offset. So, here you don't round the corner, you keep it as it is. And this offset has to exist, and it must not have self-intersections. And this actually generalizes to any dimension. And you can also rephrase it differently. The answer is that it is true if the pods of the basis function at some level intersected with the three. have it intersected with this ring are connected yeah so this actually an in violation for degree 33 so here you have something this is somehow this ring and this is a part of a basis function consists of four by four cells but it intersects this part in two connected components there's one here and one here and if you have such a situation then algebraic completeness is not true okay Okay, and yeah, now about the CHP spline. So this is like a modified basis and it's based on what we call the two-scale relations for splines. The two-scale relations say that when you have a function of little L, you can represent it simultaneously at the next final level with some the Feynman coefficients C gamma, which are actually very beautiful. So this Feynman's two scale relation is a mathematical basis. Relation is a mathematical basis for many of the subdivision surfaces. And now the idea is very simple. The idea is just to truncate the function by omitting those functions which are selected at the next level. So instead of taking full sum, you only keep the parts of the function that are not selected, which are not present in the next level. And this applied to the basis functions gives you the truncated RKDP splines. And here now the same picture. Here, now the same picture as before. We have the RKPB splines on the left-hand side, and these are again on this mesh. And now we truncate them. So what happens is that these are the originally these are the selected hierarchical beast points of the L0, but if we truncate them, then we take away some portion of the support along this region where we refine to the next level. So we truncate them here, and this is in the combination. Here and this is in the combination of the level zero and level one. Now the truncated function of level zero and the function of level one. And then again, when we go to the next final level, we truncate again now with respect to these functions here. And this means this approach of these functions again becomes smaller and all these ones are somehow truncated. And then we add these functions of level two. So we get somehow functions with smaller support, but still they are not locally linearly independent, unfortunately. Independent, unfortunately. Okay, and yeah, and this has some nice properties, and the magical basis of this, of many of these properties, is a property that I call the preservation of coefficients. So actually, it is simple to see that every function in the spaces has a All the letters, and then it's you can figure out that when you want to represent this function with respect to equal piece by basis, then it just preserves the coefficient of the mother function. That means if you have a function which can be represented at all the levels and you know this coefficient of a certain mother function, then this coefficient is inherited by the resulting truncated function. This coefficient of the truncated function is the same. Of the truncated function is the same as the coefficient of the master function in this representation. And this has some nice consequences. The first one is the partition of unity property. So because you can represent the function one at all the levels, in every splint space, and the coefficients are just one because these functions sum up to one. And this means these coefficients of the mothers are always one, and this implies that the truncated B splines. Truncated these plans, sum up to one, because when you want to represent one in your space, the coefficients, all the coefficients will be one. Yeah, and this has some for geometric modeling. So you can say you want to design a function curve by using some control polygon. So this is like the control polygon here, and now we add some functions, some contributions for level one. Then when you use the HP spline. Then, when you use the HP spines, these are the HP spines, and they do not sum up the boundary. This is the sum here. And then actually, you get some strange behavior. Whereas, if you use the same polygon here, it's actually the same polygon here, but just scaled to show it better, then you get something which behaves nicely because it remains in the context holder of these control points. And this is because the trajectory functions sum up to one. And when you add another level, then it becomes even more dramatic. Here we add another. Here we add another level of HP splines, and the sum is now more or less like three. And again, you get some very strange behavior. But if you do it for HP splines in the same control polygon, you get a fairly reasonable shape. Okay, and even another level, yeah. So you can, of course, make it arbitrarily complicated. Yeah, then this partition of unity, this preservation of coefficients also gives you the crevil points. The crevil points are the coefficients. The clever points are the coefficients of the coinate functions, the x and y. And for the bees planes, you know them. They have been established in a paper by De Boer with some Greville. Do you actually know Greville? I never met him. You never met him, yeah? You know, Toscher, not Greville, yeah? Yeah, no, I'm not Toja. Okay, so and these are inherited by the THP spots. Yeah, and also you can establish a property. And also, you can establish a property which is called function stability. That means you can bounce the norm of the function on both sides by the norm of the coefficients. And this is always true because it's a basis in finite dimensional spaces. But the point is that these coefficients, they are independent of the subdomains and they are also independent of the number of levels. They are always really constants. Okay, may I interrupt you? Can I ask a question right here? Is this also true if you have very thick? True if you have very big and very small cells side to side. Yeah, you need to have a bound on this. You have quite uniform, you need quasi-uniform, not vectors. Yeah, some constants. Yes, yes. Okay, and in 2015, Caramani and Hendrix, they established quasi-interpolation operators for THB spans, all based on this preservation of coefficient property. And they can prove that they have optimal approximation power in the local. Power in the local node spin size, the local node spins. Okay, and finally, I have some computational results. We did some refinement of the unit cube, we refined it along the intersection with the sphere, and we looked at the properties of the matrices. So if you did some discretization and the mass matrix or the sticks matrix, and we looked just at the number of non-zero elements, and then when you want to use some value, you do some reordering using something. Do some reordering using something which is called the Katiel-Mackey reordering, and then you get matrix which has some bandwidth structure. And these are what you get for these cases. And actually, the THP spline somehow are able to give you matrices which have much smaller bandwidths. And this is shown here for this. This is a ratio of the number of degrees of freedom. And yeah, that's it. And yeah, I did the number of non-zero entries. So you can see that you get like 70% of the non-zero entries, and this somehow has a bigger effect on the bandwidth. After this reordering, actually, for instance, for the quadratic case, you get down to 50% of the bandwidth. And the condition number is also slightly better for the THB curves. Okay, and then we also did this again the same example with election diffusion on the unit square. Election diffusion on the unit square, and you can see it also behaves nicely as expected. Okay, and this brings me, you know, not yet, yeah, this is also showing the benefits actually for this particular example. This is for the HP splines, so you can see this catalytic key, red ordering is not so successful here, because you still get non-zero elements quite far away from the diagonal. But for THP splines, it is much better. So, here you get the ratio of the non-zero elements is like 70. non-zero elements is like like 70 percent the limit but the for the benefits it's like only 20 percent and you need the five times we can reduce the benefits by the factor of five okay and this is final example yeah just some interaction diffusion problem on the US state of Idiana so you can use it both for representing the geometry so Idiana is actually has a fairly complicated geometry down here because the border is defined Because the border is defined by some river, so you need to resolve the geometry, whereas here you need the degrees of freedom to resolve the problem in your numerical issues. Okay, so now I hit again the wrong key, but it's somewhat dramatic. Okay, yeah, that's actually final picture in this part. Almost, it is a turbine plate designed by THP's clients, and you can see. And you can see control measures of different levels of detail. And then we also looked into the surface quality of some specific part, which is called the fillet. This is somewhere down here. There's somewhere a surface between the base part of the blade and the blade. And this shows somehow this traditional method using standard tensor product splines, where you have too many degrees of freedom in this upper part. So you get a very wiggly behavior. Behavior and when you do a CHP spline, then actually it looks better, so you get a better distribution of these reflection lines. And there is actually a little problem here, but this is actually not caused by the leaf spines, but there is a problem in the data. There was an outlier in the data, and it gives you this bad behavior of the surface quality in this part. And we also looked into methods how to feed these systems back into a cast system because a cat system will not understand. CAT system because the CAT system will not understand RPGB's blinds. And so we looked at different methods to cut it into pieces. Now, you can essentially either cut everything into rectangular pieces, like spot everything as a pencil product patch, or you can use trim surfaces and then you can do with something with a lower number of patches, but you need to define trimming costs. Yes, these both of them graphs. Okay, now I have like. Okay, now I have like two minutes left to talk about. Well, I saw it later. So maybe I have to. We've got 10 minutes. Even 10 minutes. Even 10 minutes, maybe I don't want it because some people are falling asleep. So I want to talk a little bit about adult spleines. This is like the new kit on the block. So we are invented around twenty ten. They are the name Locally Refined Splines. I must say I don't like this name too much because it's very generic. Everything is locally defined. Everything is locally lofient. So it wasn't my decision to call locally lofty friends. And they have been presented and patented. And then in 2013, the first two papers about them appeared. One was written by the original team, also from Norway, and the other one was written by Andrea, who is also here. Yes, he's sitting there at the corner. And then actually, one year later, at Von Compsta, I published a paper about using ICM. A paper about using them for isogeometric analysis, and now in the last years, there were several papers analyzing linear dependencies of these functions and definement strategies. Yeah, and so what are these functions about? So you start with a tensor product mesh, and you start with the beast plans defined on the tensor product mesh. Like in the quadratic case, you have bi-quadralic beast line, and this would be the support. This would be the support of such a piece plan. And then you refine this mesh by inserting new mesh line segments like this dashed line here. And then you have to split all the functions whose support is traversed by this mesh line segment. You would split this green function actually into two functions, into one which goes from here to here, and another one which goes from here to here. And you have to split every function that is traversed by the measurement signals. Traversed by the mesh line segments. And then you can actually prove, that's a very nice result, that the set of the locally defined beast planes that you get in the end is actually independent on the order of the mesh line insertion. So when you do several ones, it doesn't matter in which order you insert them. Only the final mesh is important. Okay, and then it's now obvious that these locally refined clients or nested T meshes are also nested. Are also nested. This is actually not true for general T spikes, but for LFBINS, it's somewhat obvious. And the linear independence is generally not guaranteed, but actually we just figured out that there is an important special case which seems to have been overlooked when you look at CS-MOS splines of degree 2s plus 1. These are actually, it's a Chinese case because it was studied in many Chinese papers. Then these are actually linearly independent. Independent. Now, these are always formal basis. Okay, and in this paper in 2013, Andrea derived a very nice characterization of a local linear independence, because local independence is actually the E missing here, is equivalent to having non-nested supports. It means if two, it kind of happens that the support of one of these beast planes is contained in the support of another beast plan. Now, if this is the case, then you have This is the case, then you have a violation of local linear independence. Yeah, and currently there is some work going on on mesh refinement for locally linearly independent. Okay, I call them these RMP systems. These actually these splines of degree 2s plus 1 and smoothness s, but this has also been studied for general peace plants, general locally refined p-splines in the context of using this criteria of. Context of using this criterion of Andrea. Now there is a paper by Hendrik and by Carla about it, for instance. Okay, yeah, and now this is almost the end of my talk. So now we have a choice of different strategies for doing locally defined spines. It's like a beauty contest. You can use analysis suitable T splines or you can THP spines or you can other spines, but maybe it is more like a choice between three guys. You have a good Ziband and Zaglin. The Bart and Zaglin, and you can make your choice who is who. And I have arrived at the end of my presentation, and my conclusion is very short. So I borrowed it from Tom Licker, Tom Licker, one skilled talker, and he concluded by saying that there's always more room in the spline, depending on your application. So you talked only about a very small subset of the universe of splines. There are also PhD splines. Also, PHP splines and RB splines and DHB splines and so on. Yeah, so let's keep creating new animals for the spines. Thank you very much. Thank you very much for that long talk. Are there questions and comments? Why is the AC question important? Yeah, well, this is a question to the rest. Yeah, well, this is a question to the rest of this LGBTI community. From the approximation power point of view, it is not. No, but it still is something nice to have. Okay. Yeah. I saw there was a lot of applications to like engineering problems. I was wondering if you knew of applications to like population dynamics since those. Population dynamics, since those are also kind of vector diffusion systems, if you've seen that. Well, I did look into this kind of problems. I mean, this is not related to geometry. So I'm a geometry person. Yeah, and in this field, you don't have any, well, maybe you don't have too much geometry, I would say. You have no first fundamental forms. Yeah, there may be applications using these slides is finite. You think piecewise as finite elements to problems like that, but you aren't going to need to make use of the isogeometric principle, which is mapping the geometric domain into. And so in a certain sense, the application you're describing should be a little bit easier. But the same, it could still be variable methods, should be as clients for discretizing your PDEs, especially. Your PDEs, especially if you have somehow smooth solutions, then it certainly makes sense. But of course, the price that you pay is that maybe your matrix assembly becomes more expensive and so on. So you have to invest some more work into this. Yes. So I imagine you use all of this in computations, refine your meshes. And then my question is, how do you actually choose where to refine? How do you actually choose where to refine? So, what kind of hypostasimate do you use to tell your plants where to cut dimension? So, we are just using some error estimators from the literature. So, we looked for the effection diffusion, for instance, we took some residual error estimators and so on. So, we didn't do any research on our estimators. This is not our contribution. And so, on the practical level, anything you pick from the literature works with the so we did not find any. So, we did not find any need to make new error estimators in the room. So, now, actually, for this, let's say for this, if one manages to bring these exploits about the full potential of the totally applied spline, then you would also need error SMLs that tells you whether you should define horizontally or vertically. This is actually a new question, which is not studied in the classical literature. I somehow found one very old paper where some people in the mechanical section. People, some people in the mechanical analysis actually did it with some very preliminary methods, but this was not too successful at the time. And normally, this question does not really arise in my experience, but as a question, maybe a comment, I don't know. No, it's just a question. Hopefully it's a question, not a comment. So I was curious, earlier you were showing us it was taking 10 times as much time to assemble the basics. As much time to assemble the basics, the spline case versus like kind of standard finite element approach. Yes. And then you had these approaches for doing some factorization, quantiting all those lines. Did you get that down to the point where it was competitive with standard FBM? Yeah, so we okay, let me see. Because it looked like you had gotten a significant amount. You had gotten a significant improvement over the standard Gauss, but it wasn't clear to me that you made it. Yeah, the comparison bars is actually this. Okay, so in the way originally we just used standard Gauss. Yeah, and we used some cubic elements. And so we got down from, let's say, from 800,000 to 4,000 evaluations. And we don't take symmetry into account. So now I hit it here. Okay. So originally, our implementation at that point. So, originally, our implementation at that time was based on standard Gauss, and so it was in cubic. So, we were basically in this 800,000 blocks for each degree of freedom. And then with this method, we brought it down to 4,000. So you get a factor, how much is it? Like 200. So we theoretically, we proved it by a factor of 200. Unfortunately, the original implementation. The original implementation was based on Gizmo, and this SMT was very much optimized. The original simulation was based on the Gauss button train Gizmo, which was very much optimized because it was frequently used. So the real improvement was not a factor of 200. It was more like a factor of 10. Because there was hidden parallelizations. Yeah, there were hidden parallelization. So we did not invest the same amount of optimization to this code. Okay. this code okay yeah was that the only aspect of the like was the quadrature the only aspect of the primary aspect of the amount of time it took you to the circle or are there other factors as well well i mean you have other factors of course you first you need to be able to have a model for this specific you need to make a model that is suitable for the simulation and there are a lot of tools around for mesh generation for file elements but there are not so many tools But there are not so many tools around for mesh generation for isotometric simulation. So, this is still, and at that time, for this torrent blade, this was actually the work of a PhD student at MTU. So, he was working for three years to make volumetric parameterizations for this kind of torrents. And then we could run the simulation. But, of course, you don't always have a PhD student, you can work for three years. PhDs you can work for three years or they have specific functions, but we need to convert the two in there. But if I may add, the difference with the splanes and the standard finite elements is that standard finite elements are C0. C0 splanced, if you said, right? So then the gauss quadrature is more or less the optimal thing we can talk. So well, you can still do some calculation, yeah? Yeah, you can do all the tricks, but the number of evaluations, the number of points. The number of points is optimal for the C0. Exactly. Hold on a second. If you're trying to integrate some C0 function on a tensor product element, and you're going anything beyond 1D, that's not optimal. You can do it a bus point. Almost, certainly. Oh, certainly. Like on a square or something. If you have a density on that structure, yes, that's another thing. Yeah, no, you can do it in much less points. If you have detention, it's optimal at 1D. It's not optimal in higher dimensions. That's well known. Yes, it's staying 1D. Yeah, 1D. And it does not extend into higher dimensions. That's a very important point. There was one more? There was one more. A technical question. You said when you approximate your weight function w by a spline, this would not increase the overall error. Yes. Okay, because naively I would say the condition of your matrix is of order h to the minus two. And this is the factor which multiplies the error in the Multiplies the error in the elements when you compute the solution. Is this compensated at a certain point? Well, I mean, also when you do Gauss curtature, you are not able to evaluate these elements exactly. So you basically, you somehow, you have a quadratic form, and it approximates this by some other quadratic form. And then there is this strong lemma, which somehow tells you what the error you do. You have maybe two contributions to the error. You have the approximation error, and you have this consistency error, which comes from the Which comes from the inaccurate evaluation of this quadratic flow. And this theory also applies here. So I always thought that you would need a Mortress scheme of higher accuracy than the solution suggests. So if you are using cubic beast lines, you would expect order H2 to the 4. So I would have guessed that the accuracy of the quadrature scheme would be order H to the 6. Would be order h to the sixth degree, yeah. I think you need so we figured out so we did the theoretical analysis and we figured out we need to use the same degree for the spine project and then for historization for a second order problem where you have first derivatives in this in this form, and then of course when you when you look at it then the first derivatives converge with one or less and so in some vents we need one or more. Yeah, so there is some difference in the orders, but not uh. Difference in the orders, but not so much as you say. One order more. So there's some compensation which I don't see. I think we have a quick look into this paper and show you the derivation, but it's actually enough. And it's confirmed by your experiment. It's confirmed by the experiment. It's a theory and experiment, and somehow they match. And do you apply adaptive monitor with kind of error estimation? No, because a fixed rule gives you a certain order, but in an example, the constant could be large. So I think I remember I did it at some point I did for something, but I don't remember for what I used. But not yet. But not yet. No, no. Actually, yeah, we try to use it for trim domains. Yeah, you know, when you do trim, you have to do quadrature over trim domains, and these are the one of the approaches we try to use this kind of adaptive key. But yeah, yeah, if you were to do adaptive quadrature, then you could maybe generate some nested quadrature rules and then you have automatic error estimates. Room here to make new add new lines to the state new roles. Yeah, I think there is a coffee break since 15 minutes, huh? Okay, I think you failed to make the audience to sleep. They have too many questions. So thank you again for the nice talk. When we meet at the portal, maybe a little bit. 