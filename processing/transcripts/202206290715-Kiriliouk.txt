So, this is about approximating failure extremes. It's a joint work with Chen Zhou from the Erasmus University in Rotterdam. So, this is really a multivariate extremes talk, but I think it can be very useful for climate extremes. So, no causal inference in this talk, but again, maybe in some future work. So, the motivation here is very simple. So, the motivation here is very simple. We were looking for an easy, computationally easy method to calculate failure probabilities. So the probabilities that some random vector X is in an extreme set C. So we would like to do this for D large. So large, I will get back to how large is large, but certainly not D is two or three. D is two or three as we show in illustrations. So C will denote our failure region, and we will do this in a very simple framework. So based on NIID observations and assuming multivariate regular variation on X. So just to have an example of a typical failure probability, which is of interest in many applications. So the one that I will call C. So, the one that I will call C sum. So, where we look at weighted sums of components being large. So, where we have some positive weight vector v that sums to one. And then the failure probability is the probability that this weighted average exceeds some large value x. So, of course, this is used in finance by fixing such a failure probability. Such a failure probability, and then x can represent, say, the value at risk, or we can go to the expected shortfall. Um, and the weighted the weighted sum of components of X can represent aggregated stock returns combined in a portfolio or in climate, same flood risk management. So, if a flood may occur because of prolonged rainfall, then we're often interested in aggregating. Often interested in aggregating precipitation, whether it is spatially or temporary or both. And we would also like to estimate such type of probabilities. So I'm giving this failure probability as an example because what's often done in multivariate extremes is to separate marginal and dependence modeling. So first estimating the marginals, standardizing them to a common form, and then A common form and then focusing on the dependent structure. But of course, if we're interested in such a region, then we cannot do so because it will break down this sum structure. So in our approach, we would like to allow for not standardizing the marginals. Okay, so just a very quick recap on multivariate regular variation. If you're in multi- Variation. If you're in multivariate extremes, then it's just to set up the notation. If you're not, then just think of us assuming that the components of X are asymptotically dependent, for example. So our vector X has a limit measure nu X, which we call the exponent measure. And as we know, the exponent measure is homogeneous, and we will denote the till. And we will denote the till index of x by alpha. So, this multivariate regular variation assumption does assume that the components of x all have the same till index alpha, but this till index does not need to be standardized to a certain value. Okay, so we can use this to estimate a certain failure probability, so the probability that Probability, so the probability that X is in the set C. So even if the set C does not contain any points, any data points, well, then for large N, we can calculate the exponent measure, and then the set C is somehow drawn back so that we can estimate. So, in practice, we usually use the polar decomposition to calculate. Calculate such probabilities. So, if R denotes the radial component where we take any norm on R D and W the angular components, then we can rewrite the multivariate regular variation assumption as follows, where hx denotes our angular measure and s d minus one our unit simplex on d minus one. Okay, and so. Okay, and so then we can use the two expressions below to calculate failure probabilities. And what's usually done is assuming a parametric model on the exponent measure or on the spectrum measure, right? And what's after choosing a model and fitting it, it is common to check the model fit by comparing pairwise dependence measures. So maybe we could compare model. So, maybe we could compare model-implied pairwise dependence with non-parametrically estimated pairwise dependence by taking some till dependence coefficients. And we could see if these match. And this is often used as a measure of goodness of fit of our model. So, this gives the idea that we could also build a parametric model that already matches the estimated pairwise dependence of our data set. Of our data set. So we will do so not using the most well-known pairwise tail dependence coefficient chi. We will use instead another measure which is more appropriate for our purpose. So let's see how it is constructed. Well, from the multivariate regular variation assumption, we can look at the asymptotic margins of the vector X. Of the vector x, so the component xj is large and n goes to infinity, we see that it is asymptotically Pareto with till index alpha and the scale that depends on the angular measure. Okay, so let's call this this integral part sigma jj for the scale of x. And if we want to go to a pairwise measure, well, Pairwise measure, well, what happens if we consider the square root of the product of two components? Well, then we can calculate a similar expression based on the angular measure. So we have again an asymptotical x minus alpha and an integral now over a product of wj and wk. So we can really see this as a type of covariance measure, and we can call this integral. Measure, and we can call this integral sigma jk. Okay, so this gives us a till dependence measure, which was already introduced by Larson and Resnick in 2012, called the till pairwise dependence matrix, which I will abbreviate TPDM from now on. So, with these elements sigma j k. Okay, so this matrix was somehow, this measure was somehow rediscovered by Cooley and Thibault recently. By Cooley and Thibault recently, who used it in the context of PCA for extremes. And so they again verified that it is a positive semi-definite matrix. Okay, and it is a valid dependence measure. So two variables are tildependent if and only if the sigma j k is positive. So although our marginal components have Although our marginal components have tilde alpha, they do have different scales. So the diagonal of sigma represents their scales, and the total mass can be expressed, the total mass of the angular measure can be expressed through this matrix. So if from now on we let the norm denote the L alpha norm, then we can calculate the total mass of the angular measure. Of the angular measure, and it will give us the sum of the diagonal of the TPDM. Okay, so why do we use the TPDM and what type of model are we going to consider for it? So you may well know the Max Linear model, which is often just presented as a toy model because of its simplicity, but we will see that it can be very convenient. That it can be very convenient to calculate those failure probabilities. So, if we consider a vector y, a max linear vector y, how is it built up? So, each component could be interpreted as the maximum shock among a set of q heavy-tilled random variables. Okay, so it has a parameter matrix A, so it is a D times Q matrix. So, it is a D times Q matrix. So, it has its number of columns equals the number of independent vectors we're taking. And these vectors, well, we can generate them as Friche alpha random variables. And then the max linear vector y can be written as each component is the maximum over a column of A times the random factors z. Okay, so in theory, this model is convenient because it's been shown that it is dense in the class of d-dimensional multivariate extreme value distributions. It has been used in quite some different papers. I'm just citing a couple of them. And the drawback is that in practice, one is often forced to pick a small number of factors, so Q2 or 3, so this denseness R. Two or three. So this denseness argument is not really applicable. And this is usually due to the fact that it has a large number of parameters, right? So it has D times Q free parameters, which can be complicated to estimate. Okay, so now what's the link between the max linear model and the TPDM, as noticed by Cooley and Thibault? So first of all, the spectral measure of a max linear random vector is Random vector is just a discrete spectral measure, very simple with point masses depending on the norm of each column. So A1 to AQ with bold A denotes the columns of A. And as noticed in Couli and Thibault, we can write the till pairwise dependence matrix of a max linear vector y as A times A transpose. A times A transposed, where this A star is simply the coefficient matrix of the max linear model to the power of alpha over 2. So to standardize for the choice of alpha. So for simplicity, I will mainly consider with alpha equal to 2, but just so you know that in general we use a free alpha. So that means. So that means that the TPDM has a particularly convenient shape for the max linear model. And that exactly brings us back to the idea of our failure probabilities. So I will just go first. So is it possible to construct a max linear model Y so that its TPDM sigma y matches the estimated sigma x, right? So x is our data, we estimate its. So, X is our data, we estimate its TPDM. Well, if we can construct a max linear model which has exactly the same TPDM, we can reasonably assume that it has similar pairwise, at least, dependence properties. And then we can use the Max Linear model to estimate failure probabilities. Why? Well, because of its simple form, failure probabilities are usually very easy to calculate. So, for instance, if we take a max region. If we take a max region where at least one variable is large, well, we get the following expression for the exponent measure, or if we take the sum region where I was, which I use as a motivation, well, then the exponent measure is even simpler. And if we have the coefficient matrix A, then we can easily calculate any of those. Okay, so can we do that? Can we construct such a maximum? So, can we do that? Can we construct such a max linear model and how? Well, again, in Cooley and Thibault, they showed that as Q goes to infinity, so similar denseness result, the class of max linear angular measures is dense in the class of all possible angular measures. But in particular, if we're trying not to reproduce an angular measure entirely, but only the corresponding tail pairwise. Corresponding tail pairwise dependence matrix, then actually a finite q, so a finite number of factors is sufficient to exactly match sigma x. So yes, for any estimated TPDM, we could construct a max linear model with the same TPDM and use it for failure probability estimation. How do we obtain such a max linear model? Well, we need to find a matrix A. We need to find a matrix A, so we need to decompose the TPDM into A times A transposed. So such a decomposition is called a completely positive decomposition. And if a matrix sigma can be decomposed, we say it is completely positive, where of course a crucial part is that the matrix A only has non-negative entries. Okay, so there is a linear algebra literature on finding such completely positive decompositions. It is quite complicated and it can lead to a very large number of columns. So there is a theoretical bound to the number of columns. And we were looking for a more pragmatic approach. So our idea is to find to propose an algorithm that will find That will find decompositions in plural because they're not unique, but which may be not, which may recover an approximate matrix sigma. So to be more precise, we can inspire ourselves through the Kolesky decomposition. What does it do? Well, it decomposes a matrix sigma into A transpose, but the matrix A may contain negative elements in the case. May contain negative elements in the Kolesky decomposition. The matrix A will be a square lower triangular matrix. And we can also notice decomposition is not unique. Okay, so we're going to propose an algorithm which also leads to this two last points, but where the matrix A does not contain a negative element. So the fact that the decomposition So, the fact that the decomposition is not unique is actually an advantage because it will allow us to generate many decompositions and hence to characterize the model uncertainty behind it. Okay, so I will just show one step of the algorithm to give you an idea, to not be too heavy on notation. And for simplicity, I will consider alpha is 2. Then notation is a little bit easier. Okay, so let's just. Okay, so let's just look at our matrix A and let's think the other way around. Suppose we already have the matrix A. How does A times A transpose link with sigma? Okay, so I write A as follows. I have one element in the upper corner. A minus one is the first column minus the first element. Here I have only zeros because I'm looking for a lower triangular matrix and a minus one minus the matrix. And a minus one minus one. Well, it's just a with the first row and the first column removed. Okay, so if I calculate a times a transposed, I get the following expression, and I would like to match this with sigma. Well, some elements are rather easy. So for the first one, we can see that A11 can be chosen as the square root of sigma 11. Only positive values, no problem there. Problem there for the first column gives us that a minus one, well, it's just the first column of sigma here divided by a11. So again, no problem there. And then we see that we could express this as a recursive algorithm. So if we look at the D minus one dimensional matrix over here, well, we again have AA transposed, but one A transposed, but one dimension smaller, plus the product of A minus one with itself. So maybe we could just continue this procedure by taking sigma minus one minus one and removing this a minus one term. Okay, so if we can subtract that, well, then we're again in the situation. Then we're again in the situation we were, and we can repeat this by lowering the dimensions one by one. So, can we do that? Because all the elements were supposed to remain positive. So, this product is also a completely positive matrix. So it should be, it should give only non-negative values. And it means that if we subtract a minus one transpose. Minus one transposed from sigma minus one, well, that has to remain positive, okay? So that this matrix is completely positive. So it implies that sigma j k, so this till pairwise dependence measure between components j and k, is always at least as large as a j1, a k1. Okay, this is exactly what we're subtracting. Okay, so we can write this as follows. So if this condition. So, if this condition is satisfied, then we can write down a recursive algorithm which decomposes sigma into a lower triangular coefficient matrix, which is a valid completely positive decomposition. Okay, so I mean, but is this condition satisfied? What happens if it's not satisfied? Well, so this is the condition we wanted. So, this is the condition we wanted for all jk unequal to one. Well, suppose it's not satisfied. Well, let's define some measure d1 of the matrix where we look at the largest value of this ratio. I mean, if the largest value is below one, basically the algorithm works. If the largest ratio is above one, well, then we can do a correction in the step over here by changing. Over here by changing A11 and adding this D measure into it. And then we will have what we call an approximate decomposition, meaning that we will be able to match the off-diagonal elements of the TPDM, but we might overestimate the diagonal. So the scales of our components might be overestimated, but their dependence properties will be. Will be the same. Okay, so I can write this down in general, maybe not go over it too long in the idea of time. So I wrote it down before in the example by looking only at the first column and the first line. But I can start with any column and line, so say I in 1 to D. So, say i in one to d. Then we define this d measure, and then we can construct the coefficient matrix A by filling in its columns one by one by these quantities tau i. Okay, so this looks a bit similar as what I told you, and then we somehow peel off a layer of the TPDM by removing it, and um, we go okay, and then we can. Gone. Okay, and then we can show that by peeling off layers of the TPDM, we still remain with a positive semi-definite matrix. Okay, so to not to finish this, the intuition on the algorithm. So although I illustrate, although the result will be a triangular matrix A, of course, we can apply this to any TPDM. This is to any TPDM. Okay, so we start with an estimated TPDM sigma X. We can choose a path, which means which component are we gonna remove first, say, from I1 to YD. And our algorithm will be as follows. Well, we pick I as I1. We fill the first column of the matrix A with this vector tau I, then we reduce the TPDM by one. Then we reduce the TPDM by one dimension, and then in step J, we will fill the jth column of A, each time reducing a dimension of the TPDM. Okay, then we will find a matrix A that exactly matches the off-diagonal elements of the TPDM, but might overestimate the diagonal entries. Okay, so this algorithm, Okay, so this algorithm implies that we have to choose an order in which we do so. Well, maybe we can choose an order which is optimal, meaning maybe we can choose an order so that we never overestimate the diagonal components. So when we calculate this di, I mean, we can do it for different i and continue with the one that is smaller than one. Okay, so this is say what we call a simple approach. A simple approach? Well, when choosing the path, why not pick the lowest value of di in each step? Or we can even be exhaustive. We can in each step look at which measures are smaller than one, keep those, and for all of those, continue the iteration until finding, say, a whole tree of possibilities that give exact decompositions. Okay, so in practice, Okay, so in practice, how does this work? Well, for what I call small d say below 20, it is a very efficient algorithm which can literally obtain thousands of exact decompositions very quickly. When D becomes larger, we can still find lots of exact decompositions. It might take a bit more time. We could apply it to very high dimensions, say I tried 100 or 200, then we will not. 200, then we will not in general be able to find exact decompositions, but the approximate ones may be quite satisfactory in estimating the failure probabilities we want. Okay, so I'll skip this and I'll just go straight to an example to not go too much over time. So very small illustration on daily maximum wind speeds. So measured in kilometers an hour. So, measured in kilometers an hour in Dutch winter, extended winter from October to March, when most of the windstorms take place. So we have 35 stations. I did an analysis for all 35, but here's a small example. Let's focus on inland stations only because they have different behavior than the ones that are. ones that are at the seaside. So for now, let's look at these red points only. Then we have 18 stations. We can start with a marginal analysis to check if it is reasonable to assume an equal marginal tail index. So it is. You can see for the first, say, 19, which represent the inland stations, the estimated value of alpha falls into the 95% conflict. Into the 95% confidence intervals. So we will continue with this value of alpha and estimate the TPDM. We could also verify if the dependence between the stations is actually strong enough to apply our framework. So I could do this using the TPDM, but given that people are more used to the pairwise tilt. Used to the pairwise tail dependence coefficients chi, I plotted them here as well. So, as the distance increases between the stations, dependence decreases, but it seems quite reasonably above zero. So, our tail dependence seems a good assumption for this data set. And then, maybe we would like to calculate the probability that the maximum wind speed exceeds a certain Exceeds a certain value at at least one station. So I tried two values. First of all, the Dutch Meteorological Service issues an alarm whenever wind gusts exceed 120 kilometers an hour. So this is the first X I will try, 120. We can also look at more extreme values. So last February, there was a massive storm in Western Europe, named storm Yunis. Named storm Junis, where actually the harshest inland wind ever was measured in the Netherlands of 144 kilometers an hour. And we calculate, so these two probabilities based on 10,000 exact decompositions of our estimated TPDM, which allows us to characterize a bit the uncertainty of the estimates, right? So here I have a box plots for the 120. Plots for the 120 kilometers an hour threshold. The blue point shows the empirical estimate. While here, there were only five exceedances. So, in the 20 years of study, there was only five big alarms issued. And here, there's even only one exceedance. This is the 144-kilometer an hour record. And we see that, well, our 10,000 decompositions allow us to get somehow not only. Allow us to get somehow not only an estimate of these probabilities, but also a confidence range. Okay, I'll stop here. Sorry.