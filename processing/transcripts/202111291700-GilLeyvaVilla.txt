Thank you very much, first of all, for inviting me. So, what I will talk about today is a new gift sampler method that I have been working with, with Per Paolo de Blasi jointly. So, this is a GIF sampling measure for mixture models, which I will start by defining in a second. Finding in a second. The idea of the talk is to first of all introduce these mixture models, which you probably know, talk about some of the traditional give sampler for mixture models. Then I will talk about the order allocation sampler, which is our proposal, and I will finish with some illustrations. So, a mixture model, in mixture models, as you might know, we assume that data are exchanged. We assume that data are exchangeable, and I sample from a mixture, which this simply means that, well, this mixture consists of kernels with random components and weights. So assuming the data is IIT sampler from this mixture Q simply means that with probability PJ, I will sample from the jth component of the mixture. Component of the mixture. Here we assume that the component parameters are IID from a diffuse distribution. The weights are simply non-negative random variables that sum up to one. And this right here, this requirement right here is important. I'm going to assume that the weights and the component parameters are independent. And here m can be infinite. Infinite, finite, or random. In Bayesian number metrics, we usually consider this quantity infinite. If we consider them random and allowed to be arbitrarily large, after integrating it out, we recover the infinite case. Most samplers fit these three different assumptions on M in different frameworks. And one of the advantages of the sample, which I will advance right here. Advance right here is that we don't need to do that. We can treat them in a unified framework. So, given a mixture model, one usually defines this latent random structure to simplify things. So, what I do is to define theta i as the actual component parameter from which yi was sampled. So, if yi was sampled from this kernel, I will fix theta i equal to x. will fix theta i equal to xj this means that this theta i is a species sampling sequence which means that it is exchangeable and driven by a species sampling model whose components atoms whose atoms are the components parameters and the weights are the same right so i will start we'll give sampling methods for mister models are usually kind of have been categorized into two categories marginal Rise into two categories, marginal and conditional samplers. I will start by explaining what marginal sampler does. So the idea is to the marginal samplers integrate out this mixture model or most of it and model directly the species sampling sequence. To do so, I can encode information of the species sampling sequence into parts. Let's say In two parts, let's say the distinct values that the sequence exhibits, and how they're indexed partitioned according to the ties in them. So, for example, in this little example, assume that I observe this, then the distinct volumes exhibit are these three blocks, which is a subset of the atoms of the speech sampling process. And in this case, I will put the index case, I will put the index of theta i and theta j in the same block if they match, if they are equal. Right? An important highlight here when I do this is that labels don't matter. What I mean by this is that I don't care how are these index chosen. The reason of why this happens is because these ones being a subset of these ones are IID and in a partition ID and in a partition structure the blocks don't matter. What will matter is the correspondence. So I have to know which of these distinct values corresponds to which block of the partition, but not the label. So if I relabel everything, everything's the model is invariant, let's say under these relabelings. This is a remark that is going to be important. So anyway, with this set, how does marginal sampler work? Said how does marginal sampler work? As I mentioned, I integrate out part of the mixture model and I model directly the partition structure, which is characterized by an EPPF. I will also condition on the non-empty component parameters, which, as I mentioned, are IID from new. And given these two, I can perfectly characterize a lot of the data. And in Martino's sample, In Martino Sampler, I usually update in two steps. First, I can update the non-empty component parameters, which I can do independently, and this is easy to compute when g and nu form a conjugate per. And in the second step, I will update a partition, which can be explained in this way, but can also be explained in other ways. The way I will update a partition is by reallocating one index. Is by reallocating one index E at a time. So, in order to do this, given this partition, I will first remove E from whichever block it is in, and then I will add E again according to these posterior probabilities, to the full conditionals. Right here, I have replaced the hat theta with this prime to highlight the fact that I can relabel everything as I mentioned. Relabel everything as I mentioned. If I relabel these sets, I will also have to relabel the component parameters accordingly. And this relabeling might be necessary, especially if I is in a block in its own somewhere here in the middle, because the partition does not account for empty blocks. Right? So this relabeling right here might be necessary. Some examples of conditional sampler. Some examples of conditional sampler for the case where m is infinite are discussed in Neil and for the case where m is random, Mildersor and Harrison proposed this sampler. Right? Here I do not update M because in the case it is random, it has been integrated out. So this is a very simple marginal sample. Now I will talk about conditional samplers, which in Which in differently than marginal samples, the first and most important difference is that they do not integrate out this part right here. So to explain it, I will first encode the information of a species sampling sequence in a slightly different way. Here, I will encode it through allocation variables and the component parameters, all of them. So, in order to do that, So, in order to do this, first I will choose a labeling of the component parameters. So, here the labels will matter. And then I will define these allocation variables, ti is equal to j if and only if theta i is equal to xj according to this labeling that I choose somehow arbitrarily. And so that I can write theta i is equal to xci. The tithes or well the layer. The types or well the labels in these allocation variables define me these random sets, which, if I consider the non-empty random sets, the non-empty ones, they define me the same partition structure that my renewal samplers model directly. Right, once again, I will have a correspondence between these CIs, which in this case account for In this case, account for empty blocks, and they will also be in correspondence to how I label the atoms. An important highlight here is that when I introduce these allocation variables, labels start to matter. And the reason they matter is because I will not integrate all the weights and the correspondence will be also with respect to the weights. Now, the weights are The weights might not be exchangeable, which means that this relabeling right here, as I did in marginal samplers, cannot be done. It can be done when these are exchangeable, but they generally will not be. And why am I claiming that labels should matter? Because all the model is invariant on the relabeling of the atoms or the weights. And when I introduce the allocation variables, The allocation variables. It can be easily seen that given the weights, they follow this categorical distribution. And here, this distribution right here is not equal in distribution right here. So, actually, the labels will impact the distribution of the CIs, which have been augmented. This was the highlight I wanted to do about conditional samplers. So, how does my How does conditional samplers work? I'm going to explain the very simple case where M is finite and it is fixed, it is deterministic. So in this conditional sampler, the setup is as I explained before, I will work with all the component parameters, which I ID from new. I will work with the weights, weight, whatever distribution they have over the simplex. The simplets. Given the weights, I will consider the allocation variables following the indistribution, and afterwards, I will sample the data points from this distribution. So, in this very simple case, where I m is fixed, what one can do in a conditional sample is first update the component parameters from the full conditional, later the weight from the full conditional, and the allocation variables. In contrast to margin. In contrast to marginal samplers, I don't have to update one allocation variable at a time, so one I at a time, I can update them all at the same time independently. And I highlight here is that in order for me to sample Ci from its conditional distribution, which is given by this, I will require to know every single component parameter and every single weight, which can be problematic when M is infinite. Problematic when m is infinite because I cannot update in these steps all the component parameters and all the weights. So if I wanted to use kind of this conditional sampler for the case where M is infinite, I will require major modifications. Some exact proposals have been done by Stephen Walker, the slide sampler, and by Papas Phileopoulos and Robert, which is the retrospective sample. And for the case where M is random, I also have problems. The problem here is that the dimension of the weights actually depends on M. So I cannot just add a step where I update the model dimension because it wouldn't update because I already would have known it by P. So, in order to overcome this, Richardson and Green propose a reversible jump step, which is Step which is rather hard to implement. And recently, Frug Schnatter proposed to update them kind of in a block. But the point here that I want to make about DiM is that we need different frameworks for different cases of DiM. Another highlight about conditional sampler, which is, I have found that it's rarely. Is I have found that it's rarely mentioned. And this, of course, this is a problem when m is large. So, say m is infinite or random, but allowed to be very, very large. And it is that before updating these component parameters, I'm sorry, this was after updating, no, before updating the component parameters, I might observe something like this. That the allocation variables tell me that data points come, say, from x. Data points come, say, from X4, X7, X10, X22. But the point is that I have a lot of empty components right here in the middle from which no data point was sampled. The algorithm or the theory really doesn't tell me anything about relabeling the steps, but the problem is that if I don't, this might keep growing and growing. And from a computational perspective, I might run out of memory. So what is Of memory. So, what is advised is that we actually relabel components in a way that the non-empty components are at the beginning of the vector. This relabeling can be done in various ways. I can just simply delete this or pass these component parameters to the end of the vector. I can relabel them in order of appearance. So, now x1 will be the first one that I discover. First one that I discovered, x2 will be the second one that I discovered, and so on, or I can label them with decreasing cardinality. So the component which has more y i's will be at the first. The way I choose this, how to relabel, will actually depend on the distribution of the weights. And this is, I mean, I don't, I haven't seen any kind of guidelines to. Kind of guidelines to how to do this. And this can affect the mixing of the sampler, especially because the labels matter because of the allocation variables. So these were the marginal samplers and the conditional samplers, a very quick review. And the one sampler that I will propose here is actually a conditional sampler that pretty much works like a marginal. Works like Comartinal. The first key concept that I have to introduce before I talk about the Saddler is this concept of p-size bias peak. This is simply a random permutation of m of the first m integers that has this law conditionally on the weights. So, what this means is that I will fix alpha one equal to one, the probability p one alpha. probability P1, alpha1 equal to j with probability pj. And once I have sampled this alpha 1, I will sample alpha 2 with probabilities proportional to pj, but alpha 2 cannot be equal to alpha 1. So we'll say that alpha 1 equals to 1, then alpha 2 we can take the place 2, 3, etc. And so on. Alpha 3 will also be sampled from the remaining values with probability pks. With probability PJs. So, okay, to set up the stage for the order allocation sampler, the first thing I will do is basically consider something very similar to what marginal samplers do. The one difference here is that the non-empty component parameters will be labeled in order of appearance. So, x tilde 1 will be the first one discovered in the species sampling sequence. x tilde 2 would be the second one I discovered. The two would be the second one I discover, still the three will be the third I discover, and so on. This, uh, when I consider the partition structure, this means that the partition will now be an order partitions where the blocks are ordered according to the least element, which means that the minimum of the first block will be smaller than the minimum of the second block, and so on. Right? If I define the non-empty component. If I define the non-empty component parameters this way, it's quite easy to see that they are actually given by a p-size bias permutation of the component parameters in whatever original labeling they were. And if I just do this, this will be exactly the same as in marginal samplers and the label wouldn't matter. Just forcing this list element order in a partition just wouldn't do because. Partition just wouldn't do because, actually, as I mentioned, the labels don't matter. So, here I still have that the model is invariant under the labelings of the component parameters. And what I do to make the order matter is to not integrate out the weights. So, to not integrate out the weights, I will do various things. The first thing I will do is kind of a momentum model to assume that I have observed the whole. Have observed the whole species sampling sequence. This way, I actually can define these component parameters in order of appearance, but not only the non-empty of them. Also, the rest of them in the order in which I discovered them in this infinite sequence. And to properly define the weights that correspond to each of these component parameters, I will consider this. I will consider these limits right here. So, pj is the long-run proportion of elements of theta i that will be equal to the jth value to be discovered. Right? This then means, not surprisingly, that these long-run proportions right here will be given by the same permutation of the weights in the original order. This means, in particular, that PJ is invariant under size bias permutation. invariant under size bias permutations and as i mentioned in condition in marginal methods i have this correspondence of the blocks with the with the component parameters and now i also have as in conditional methods the correspondence with the weights in this setting of course the labels now matter because these have to be in this very particular ordering these are these long-run proportions and Proportions and they should matter. So it's not like in conditional software where the labeling was kind of arbitrary. Here, the labels actually have a meaning and they tell me the order in which they were discovered. Sorry. And I put these quantities right here in red, in gray, sorry, because in principle I will work with all of them, although in the implementation of the sampler, I will not. The sampler, I will not require all of them. I will require an extra number, a number of them extra, which is random. Okay, so before I move on, I will mention that what motivated our sampler was this very beautiful theorem for Pittman for species sampling processes, which tells me actually that theta i is a species sampling sequence driven by p. PC sampling sequence driven by P, if and only if all of these conditions hold. The first of all tells me that the distinct values that it exhibits are IID from nu, as I mentioned, and they are given by a size by speak. This second condition tells me that these unground proportions are also given by this permutation of the weights and invariant under size by springations. The third part tells me that MP, the long-run proportion of the weights, The long-run proportion of the weights in the regional orbiter and the size by speaker independence of the distinct values it exhibits. And the four conditions, which is the one, the extra piece of information that I haven't mentioned, is that I actually find a prediction rule, a conditional prediction rule for the species sampling sequence if I condition on these long-run proportions and the non-empty component parameters. And a very important highlight. And a very important highlight here is that even if I condition on all of them, I don't require all of them, right? I will at most require to sample theta i, chi i of them, or chi i is the number of distinct values in the first i thetas. Okay, so this is kind of the result that gives the validity to the sampler that tells us we are doing everything correctly. Correctly. So, how this the given the things that I said before, how does the order allocation sampler work? If M is random, I will endow it with a prior. Given M, I will sample the non-empty components in order of appearance IID from new. Then, given the weight, given M, I will sample the size bias weight. So, this has to follow. Weights. So, this has to follow a distribution that is invariant under size bias permutation. Given these weights, I can then define this what we call order allocation variables or the order partition in the list element order, which follow this rule. The conditional rule of the order allocation variables can be easily derived from this equation right here. So, what I will have is that given So, what I will have is that given the weights, the first allocation variable is equal to one, and given the size-by-weights and the first i allocation variables, the next allocation variable will be equal to j with probability p tilde j if i already observed the value j in the sequence, or it will be equal to a new value, chi i plus one with. With the probability one minus the previous waste, per chi i is the maximum of the ci's. There is a one-to-one correspondence between these order allocation variables and the order partitions in the list element order. So this can be written equivalently in terms of this partition with box in the least element order. And given the size by square. And given the size-by-squares, they follow this law right here. Here, this indicator function indicates me that this is actually in the least element order. In contrast to conditional samplers, recall instead of working with these order allocation variables, I worked with the arbitrary, with the original or traditional allocation variables, which, as I mentioned before, even to some As I mentioned before, even to sample this allocation variable from the prior, I need all the weights. Here I don't, right? I only need chi n of them. And in marginal samplers, I replace this law of the order partition with the partition of an EPPF. And later, given either the allocation variables or the partition, I can model data, sample data from this distribution. This distribution. Okay, so the way it turns out after making the computations is that to implement the order allocation sampler, I can first update the component parameters in exactly the same way I did in conditional and marginal samplers. If M is random, I will update M in a block with the weights and mean of block with the weights and and with the component parameters with the remaining component parameters these are the unobserved component parameters so that i don't have any problem with the model dimension as i mentioned earlier uh the weights are actually updated the same way also the non-empty component parameters the one thing that changes is that in the condition in the full conditional of Condition in the full conditional of m, I can find it to be proportional to the conditional EPPF given m times the prior of m. So, this is a new step. And the other thing that will be different of order allocation sample with respect to their counterparts is how I update the clustering structure. So, either the order allocation variables or the partition structure, and this will work. Structure and this will work pretty much like in marginal samplers with a very important difference that is that I have to keep these ones in order of appearance. So to update the block to which I belongs, I will first remove I from the partition and I will add I to one of the blocks of the partitions with this process. With this probability right here to an existing block, or it will create a new block. But not all the moves will be admissible, let's say. Some of them might not. And why are they not admissible? Because I have to keep this partition in the least element order and there cannot be empty blocks. Right? So this is the one difference with marginal samplers. I have to keep track of this indicator function right here. Indicator function right here. So, all of these things that I mentioned was to make this table right here to compare so far the samplers. So, it takes place in the algorithms. In the marginal algorithms, it's the space of partitions. In the order elevation sample, it's the space of other partitions. Partitions and in the conditional salt is actually in the space of clusters labels, which is bad because this leads to kind of bad mixing properties of conditional samplers whenever the weights are not exchangeable. To mitigate this, Portius Atal and Papaspilia Poulos and Roberts suggested to add a step to conditional samplers to mix over clusters. To mix over clusters labels. What about how they treat the model dimension? Well, in marginal samplers, it's very easy to treat it because we integrate it out and we model directly the APPF. In the order allocation sampler, it's also very easy to, even though we are not integrating it out by a blocking argument. And also, the case where M is infinite is very easy to treat in both cases because. Easy to treat in both cases because we don't need in any case all the component parameters and weights. And in conditional samplers, the model dimension requires special treatments for each of the cases. Another thing to check here is when do I have to manually relabel components? In marginal samplers, sometimes, as I explained, when I is a block in is in a block by its own. It's in a block by its own. I have to do some relabeling. In the order allocation sample, I never have to relabel anything because since this partition is in the list and element order and I do not consider all admissible moves, I never have to relabel anything manually. And in conditional samplers, I often have to do it because of two things. The first one is the one I mentioned in a previous slide. And the other one is due. And the other one is due to this step to accelerate mixing. Okay, so which of these samplers allow direct inference on the species sampling process P? Marginal samplers, of course not, because they integrated out. I have to make inference on P based on the clusters on the partition, but not directly. The order allocation allows it because I haven't integrated anything out and also conditionals. anything out and also conditional samplers allowed it. Now, this part right here, what I wanted to tell you about is how many unnecessary components do I sample from what I mean by this is about the component parameters and weights from which that I haven't observed, they cannot learn from the data. So how many of them do I have to necessarily sample in the sampler? In marginal samplers, if none of them Marginal samplers is none of them. In the order allocation samplers is one or less. And in conditional samplers, depending on the samplers, it can go from few to a really, really lot. So for example, when the weights have heavy tails, this actually makes them almost impossible to implement because I have to sample an unnecessary amount of them. them another thing to to ask about or to check how good is the sampler is to see how easy it is to implement marginal samplers are very easy as long as i can do them uh order allocation samplers are a little bit less easy than marginal samplers due to these admissible moves but in general that's the only difference the the one that makes them harder than marginal samplers Harder than marginal samplers. And in conditional samplers, this varies a lot. It depends on the sampler. The easiest conditional sampler is the easiest of them all, and the hardest conditional sampler can be almost invisible to implement. In addition to all of this, there are two other important things to check, right? One of them is the unvoluntary. Unvoluntary label switching, which is not this manual relabeling. I mean, on voluntary label switching. What I mean by this is when does the sampler, without me telling it to, actually changes the labels of components involuntarily. In marginal samplers, in conditional samplers, it happens very often, and this is worsened by the fact that I Is worsened by the fact that I have to often manually relabel stuff. Marginal samplers, it's a little bit less often, and in the order allocation sampler, this occurs rarely. And the reason this does is because I am consistently labeling the component, the blocks of the partition, the component parameters, in the order in which they are discovered. So, okay, just looking at this. So, okay, just looking at these things above, one can think that in general, marginal sampler or the order allocation sampler are way better than conditional samplers. But this is given that I can implement them. So another very important thing to check here is how many mixture or species sampling models can be implemented with a sampler. With a sampler. And the thing is that marginal samplers require the EPPF. So if I do not have the EPPF, I usually cannot do a marginal sampler unless I kind of automatically model or try to compute the EPPF conditionally on something. But in general, if I don't have the EPPF, I cannot use marginal samplers. For the order allocation sampler, I have something very similar, but this time in terms of the size by sweights. In terms of the size-biased weights, if I don't know the distribution of size-based weights, I will very rarely be able to implement the order allocation sampler. And the very nice feature about conditional samplers is that the range of application, even if I have to treat these cases separately, is very, very large. All right, so the next thing we want to do with the order of additional sampler is change this thing, right? Change this thing right here. With marginal samplers, I cannot do much because I have integrated out everything. With the order allocation sampler, there's something I can do. So this order allocation sampler, version 2.0, basically consists of doing this. If I don't have the distribution of size bias weights, I can model them by means of the weights in the arbitrary order and the size bias peak. So what peak so what this will do to the algorithm is change two things uh well three things i will update um the the weights in the arbitrary original order instead of these weights the full conditional of m will no longer require the conditional if f given m and this is the tricky part i have to add a step to update this random permutation which can be a little bit Which can be a little bit hard because it is a random permutation. Okay, so I will not give much details about this, but the point I want to make here is that the order allocation sampler can be made more worldly applicable. So, now I will show you some illustrations. So, I have chosen this data set, and to this data set, I implemented This data set, I implemented a marginal sampler, an order allocation sampler, and a conditional sampler for a Pittman germ mixing prior. Here, what I want to highlight is that what I meant about this unvoluntary label switching means that the component is not actually able to recognize which component is which throughout iterations. So, if I wanted to estimate the weight that the Wanted to estimate the weighted density of each component, I might see something like this. That I cannot do it well because the sampler mixes the components. In the order allocation sampler, we did not observe that. And this has very much to do with the fact that the components are labeled consistently throughout iterations. In the conditional sample, where here I implemented Where here I implemented a slice sampler, I observed something very similar to the marginal sampler. And for instance, if I want to do inference on the weights, I will usually observe something like this, like they are bimodal. These are the posterior distribution of the weights in order of appearance, which I didn't observe. So, this is the 3D view. And what about the mixing of the chain to monitor? The mixing of the chain. To monitor the mixing of the chain, I consider the integrated autocorrelation time. This corresponds to the experiment that I have just showed you. These are the results for the order allocation sampler, for the number of distinct blanks, and for the Devians. In general, I want this number to be as small as possible. Here we observe that the order allocation sampler actually compares pretty well with marginal sampler. Pretty well with marginal sampler, and they surpass the light sampler by a lot, which was expected because of this, of where the mixing takes place. Okay, these were experiments, these were not for this same database, they were for other databases, but I want to show you that we consistently observed that the order and the marginal sampler are pretty much similar and output. Pretty much similar and outperform the slice sampler. And this is for the case where m is infinite. For the case where m is random, we have observed that in terms of the deviance, the marginal sampler usually is slightly better than the order allocation sampler, but in terms of the number of components, usually the order allocation sampler is quite much better than the marginal sampler. Sampler. And one last thing I wanted to mention is how does this alpha works in this self-bias permutation works in the order allocation sample version 2.0. So here what you're seeing is a data set with estimated density, the estimated weight density of each component. Here I also did not observe label switching. Observe label switching. And here are actually this tilde pjs. So the posterior distribution of each of the tilde pjs. Here is the posterior distribution of pj. And here is the posterior distribution of each of the alphas. If, of course, these pjs I will be constructed by permuting these pj's at each iteration, as alpha indicates me. And what I wanted to mention right here is that observing this in terms of the weights is actually good because it tells me that in terms of the weights, the sample is mixing over the cluster's labels. Right? So we usually want to observe this. However, when I take into account this size bias permutation, I can also see that at the same time, and even though the sampler is correctly mixing overall clusters labels, I still don't have this label. I still don't have this label switching problem. And that's all I wanted to mention. Thank you very much for your attention and the details of this sample. You can find it on our preprint on archive. Thanks, Martha. Are there any questions? Sorry, just a quick question. I probably missed it, but uh I thought that it the mixing is better. Saw that it the mixing is better, but what about the computational time? So, how long it takes to run your algorithm as compared to the marginal one that it's about the same? All right, thank you for the question. The time is very similar. The time is very, very similar because the one difference is these admissible moves that I have to compute that can be computed actually very, very easy fast. Uh, fast. So these two run at roughly the same time. The slice sampler for this pit manjor is actually very, very slow because of the parameters that I have chosen. It is well known that the slice sampler is low for the for the pit manager, but if I change these parameters, the slice sampler can actually be even faster than the order and the marginal. Yes, thank you. Thank you for the talk. It's very interesting, different approach to this mixture estimation. I was wondering which of the methods require the use of conjugate priors on the components so that you can distribute out the parameters? Parameters, uh, sorry, can you repeat the question for me? I was under the impression that some of the methods required using conjugate priors on the parameters. The problem of the conjugate priors of the parameters is actually exactly the same in all the samplers because they model the component parameters identically. Parameters identically. So, this posterior distribution is identical in all the samples, which is at any case the part that is affected by the conjugate pair of this new with this kernel G. So, the way we do would tackle that problem in all the samplers is identical. Okay, uh, we have time for another question. have time for another question there another question people on zoom do you want to ask anything no okay if that's the case then we thank marifer again for a very nice and colorful presentation so thank you