Yeah, right. So, welcome back. Thank you for participating in the photo. So, I mentioned when we, at the very beginning, when we were just welcoming everybody, that there were some very concrete things that we hoped to achieve with Panwhat. And you're about to hear about one of them, which is. Hear about one of them, which is really close to my heart because I've personally always had a very serious interest in historical data and trying to understand what you can about epidemics of the past using mathematical modeling. And over many years, I went to a lot of trouble to collect data from all sorts of sources, and in particular for Canada. And the data that Steve is about to talk about. That Steve is about to talk about, most of it I think, was in fact sitting in my office in photocopies for many years. Very small parts of it we had digitized and done some analysis and written papers about, and that was great, but it was just far too vast a task to consider trying to digitize it all and get it organized. And we were incredibly lucky that Steve Walker was willing. That Steve Walker was willing to step out of his industry experience back into academia and take on this wonderful task and use his really high-level skills in data science to do so. So I am so pleased that Steve is here to tell us about this project. Make it race today. Okay, so I'm going to be Okay, so I'm going to be talking about what I like to call the Canmod Digitization Project. It gets called the Canmod Data Portal. And I like to downplay the portal aspect of it because I think there's lots of portals that we can plug into. And the interface, I don't think, is the important thing is getting the data that they was talking about to people to use. And that's basically the simple message, which is that data that you can play with now. Can play with now, and I'll show you some commands and send an email around with those commands if you don't have to write them down. That lets you pull them and play with them and get us sort of feedback, and it would be great to, you know, if you have complaints or whatever, working together. And that's essentially the whole point of the talk, but I'll talk a little bit more about it. So it's going to be about data from past epidemics in particular. And I mean, what does the past mean? Like yesterday was the past, but what I mean is. But what I mean is data that was from sort of long enough ago that it's got particular challenges that stem from it being recorded long ago. So long enough ago that it's not just on the internet or if you have the right password credentials you can get it on the internet. But that there are sort of other challenges with pulling it together and making it available. So I'm going to start where I have to start, which is why would we be interested To start, which is why would we be interested in this, even though I'm certainly not the person to answer this question because I'm not an epidemiologist. But this is where we have to start with the stuff and sort of why would we be interested in past epidemics and sort of the obvious answer of learning from our mistakes or just learning from the past, learning from previous cases. And just a general pandemic preparedness. If we understand what happened in the past, we might be able to tackle what happens in the future. To tackle what happens in the future more effectively and just getting better at modeling. I think Amy made a great point today, actually, which was that what made her feel confident in the recommendations she was giving was just sort of a general familiarity with epidemiological modeling, making models for different purposes. And that's what really, that's the kind of thing that is required. The kind of thing that is required. And past data are a great way to get a variety of cases that you can try to fit to and really exercise your thinking about different kinds of epidemics. So just to be concrete, there's lots of ways that you could use these data, but this is one way that I'm familiar with. So a graduate student of David's, Kevin Zhao. Graduate student of David's Kevin Zhao, or was it undergrad student? Undergrad. Undergrad student, excuse me, very bright undergrad student. Was working with, these aren't data that I'm going to be talking about today, but was working with data on scarlet fever mortality from London, a long time series that data has digitized over the years with students. And the black line is the observed data, and the red line is data fitted to a model using, doesn't really matter, the important point is just to put a picture. The important point is just to put a picture in your mind of the kind of thing I have in mind. We've got this long time series back in time, and we can fit models to them, mechanistic models. And when you fit models, then you can get things like inferences from them if they're somewhat mechanistic. So we fit the curve, the mortality curve, and then we can do things like infer, say, the force of infection. And that's where the interesting stuff happens. Because then I don't know if this is a good inference, but the I don't know if this is a good inference, but the point is you want to be able to make inferences quickly. You don't want to have barriers, and that's sort of what I'm interested in trying to remove. And so, you guys can disagree with this is too many oscillations or whatever, but the point is we can do that kind of thing. Okay, so obviously, to model historical epidemics, you need data. And the good news is that there's lots of it, but this is like a bad news. And this is like a bad news opportunity, sort of coupling, which often happens, that it can be difficult to get actual use. And that's what I've been working on. So David was talking about this. It was good that you said that because it's a perfect segue for this slide where these sort of examples of the scams that were in David's office for a long time. And just a little bit of natural history of how these things come, right? Like there's a wide variety. These things come, right? Like there's a wide variety of formats, and sometimes they're handwritten, sometimes typewritten, and they record things differently. I'll get into a little bit more of the details there about how they record things and changes over the years. But the goal is to be able to take all of these data sources and put it together into a longer time series or enable people to read longer time series. And that's sort of the challenge. And so, zooming into a particular example, this is from Septic Sore Throat, which was, I guess, at one point a notifiable. I guess at one point, a notifiable infectious disease which method the provinces had to report the number of cases in their jurisdictions to the federal government. And that was, and that's why we have these data because the federal government sort of compiled all of this information. And if you scroll down, you can sort of see how it gets messy. Like, this is some, I'm pretty sure, how I interpret these kinds of things is somebody wrote down 21 and then seven new cases came in, and then they scratched it up and wrote that in. They scratched it up and wrote that in, and were busy, so they didn't bother doing again because they had to handwrite all this stuff. But sometimes it gets less clear what's happening, and why is this 12 crossed out, but others aren't in the same column, and you have to spend a lot of time inferring stuff, and it's not the kind of thing that we found it wasn't the kind of thing that an AI could do, although you do start wondering, like, when is that going to be possible? So, all of the stuff I'm going to be talking about is digitized by undergraduates. But the point is that it's sort of a big task to sort of interpret what people meant in the past and enter it into a computer. And that's what we call digitization, because it's done manually, not by the computer. But maybe one day this could be like a training data set for an AI or something. And I'll just sort of, David actually touched on this point as well, which was good in the introduction, but just to In the introduction, but just to identify two kinds of historical data digitization projects. So, the ones that David was talking about earlier were sort of opportunistic, where you've got the sort of question-driven, you have a question about a certain past epidemic, and you start to, do I have data on it? Or maybe I can, I know someone who's sitting, who's in a European city, maybe they can go to the library there because I know they've got a copy of this or that almanac and scan it. Almanac and scan it. And you enter the data, and then you answer the question, and then you move on. But a byproduct is that now you've digitized it and you can make it available to people. And so that's a sort of a service generally to the community. And the other kind of project, which is more what we've been working on, is where you take a system, you aim for systematic coverage of a particular place and time, and so you try to say, okay, like what we're To say, okay, like what we've done is see from Canada, as far back as we can go for mortality, incidence, and population, and just put it all together in as convenient a format as possible. And there's another, I just realized that my whole slide is not on here. There should be footnotes, it doesn't matter. There's a footnote to Project Tyco, which is a similar project in the United States, where they went around to all the different data stores and went around to all the different data stores in the country and digitized it and put it in the database. And so that was a really great record of disease incidents in the US because of it. So there's this other aspect of the story, which is IDA, as we call it, the International Infectious Disease Data Archive, which has been led by David Fern, sitting here for several years because he's been working on modeling infectious diseases. Working on modeling infectious diseases and digitizing data. He's very passionate about it. And so, as he digitizes data using both kinds of projects, both systematic and opportunistic, he's been making the data available in this archive place where you can go and get data. But it's currently, that's sort of what I've been working on, is rebuilding it in something with a new format for a variety of reasons. Reasons. And today, it's a little bit terrifying, but today we are going to have a trial release, so people are going to be able to see it and see all of this warranty. But that's good, because I think I'm at diminishing returns with it. So just the goal of all this is, it's not clear, has more straightforward and more convenient access to historical and publicly available incidents from telling population data. And the hist the The publicly available part is important because we're not bothering with any kind of anonymity stuff. Anytime there's an anonymity requirement, we just say it's not in scope. It's all just publicly available stuff. It's just hard to use because of the form it's in. So there's disease incidence from 1994 to 2000, population data from 1871 to present, which I may or may not talk about. May or may not talk about depending on if there's time, but it's kind of interesting. And mortality as well, I may not talk about depending on if there's time from 1950 to 2010. But I'll start with the disease instance because it's the messiest and kind of the most interesting. So, yeah, that's what that says. So, back to this picture. These are all incidence data. So, just more. So, just more detail about what's going to be available. So, one thing about the data is things have actually gotten worse over time in this particular dimension. The government has gotten worse at notifying, or I don't know if that's the right word, but at publishing, yeah. So it's gotten monthly weekly to monthly to quarterly. But it is going to be broken down by province for that whole time period as well as at the national level, and obviously broken down. And obviously broken down by disease. And some diseases are broken down by age and sex, but none of the data I'm going to share with you are because they're messier, and I just don't think it's, I mean, you know, I'm happy if you let me know, right? But it's just, it's less together. So that's that. And it doesn't matter. Some of the slides are cut off, but I still think it should be okay. Some of the slides are kind of, but I think it should, I still think it should be okay. So, one of the David Ern had the brilliant idea, I think, to take a particular approach with this project, which is to make a parallel copy of every scan as an Excel spreadsheet using the same sort of spatial layout. So, this is an example of the same data set. The numbers are all the same. And what that helps with Poster shaking your head, John? That's brilliant. That's a brilliant answer. This is brilliant. Yeah. I agree. It was my idea. I wish it was my idea. Right. And so the point of this is that now it's in digital form, but it still has the same spatial layout. So you can put one on one monitor. Many of you probably say this, and another on the other monitor. And it's great for checking for data entry errors and the whole data provenance thing is nice because then if you suspect a problem, you can go back. And if you suspect a problem, you can go back and use this by where is it with the data entry? Is it something that weird that happened when it was recorded? Is it something you don't understand? And then, but also recognizing that people don't want Excel spreadsheets when they're modeling, they want things like this, and so we write for every one of these, we write an R script that takes the Excel sheet and turns it into this. And these things are reproducible pipelines, which is good because they still do have problems. Because they still do have problems. We still find problems regularly. But the goal of the day is just let the genie out of the bottle and we'll keep improving things and versioning them. And the idea is that, I hope this is too small, but we have a controlled list of columns. And so every data set, you're not allowed to, I'll show you this, but you're not allowed to make a new column. Unless you really need to have a good argument so that it's easier to put data together, to combine data together. And they use the same, try to use the same words that were in the original source in this format so that you can follow things back. But then that is not really what people want yet. There's still another step. It's a lot of work. Because people don't want just the archive of the historical information in a nice text. Of the historical information in a nice tidy data set. What they want is things like: I would like diphtheria incidence pleasing Canada for as long a time as possible, and everything should be perfect, which is reasonable. But that's not what that is because of all of these reasons. So there's deal with different data sources and recorded differently. Disease names change over time, place names change over time, disease codes change over time. The place names Change over time. The place names, as you'll see, it's actually useful, but it's easy. But the disease names is very hard, actually. And there's other stuff like the hierarchical nature of disease names, the way they record these things. I think they were being useful when they'll say, like, well, there's, oh, actually, now we care about different kinds of disentry. And so we're going to like partition it out. And then, oh, no, we don't anymore. Oh, but now we're going to categorize things differently so that there's not really anything comparable. So you have to work hard to give disease. Work hard to give disease names that allow you to stitch together a long data set that actually makes sense all the way through. And that's an area that I think will just keep being hard. But the point is, since we have these reproducible pipelines, where if you fix something, then you just press go and then the new version begins. And there's also these other things, which I'm not going to talk about in detail, but I've got other slides where these inconsistent time periods make it hard to get things like evenly spaced time series all the way through. Evenly spaced time series all the way through. And the age group definitions change over time over that one can be solved pretty easily. But I'm not going to talk about that because I'm not going to talk about age. So the way we deal with some of this stuff, this is the easiest case, which is location. So you can just assign one of these standardized codes for location, which I'm sure you've all seen, and just make lookup tables, and then you join that with the actual table. You get the historical name plus the thing that's easy. Thing that's easy to stitch together, but like this is what they look like. This is just a subset of what they look like for diseases. This is a very small corner of this lookup table, and it's got many columns, and it tries to categorize things about the nature of the infection and all that kind of thing. And, you know, it's a lot of work going back and looking at footnotes that I'm not really doing. It's mostly undergrads to try to get a good name that you can stitch together mostly. And so these, I'm going to send an email out with links, but you can click on these things. And well, why don't I? I should try to show it. This is one of the things where Chrome and Firefox are good at separate things. So I'm going to not click and just paste it over here. And your Firefox probably won't respond this way, but I could help you set it up so that it would. So they're good, and it formats things nicely. And this is our data dictionary. So, that, like I said before, this new data set, you have to choose a column out of this dictionary. And it goes on, and like, just scroll to show you, like, we've got quite a few columns at it and quite at the bottom. But you have to have a good argument for introducing a new one because using pre-buccanement dictionary makes it easier and to stitch together. And the different names, like, I probably Like, I've probably given bad descriptions, and you'll probably look at it and you'll be confused in some cases. But the point is that if you're confused, then no problem. You can just fix it in one place, and then all the other data, when they go to people, they get the data from the common dictionary, the description from the common data dictionary. And these are examples of our lookup tables, which you can get. So these are the current lookup tables, and they're updated. This is sort of like these links are live. Links are live, so it's an API. So if you click on it next week, it might give a slightly different lookup table that maps the historical names with the current ones. And this is the other situation. Here. This is another useful link, which a good entry point to A good entry point to what there is. There's more than this, but this is all that I've put up because I think it's the best stuff. And each one of these items in this list is a data set that you can get. So it's a CSV file plus metadata associated with it. And if I sort of, I'll just pick randomly. So there's a whole bunch of fields that you can look at, and whether they're, some of them are interesting, some of them are not. It's the kind of thing, again, Are interesting, some of them are not. It's the kind of thing, again, that we can make, that we can improve. I could talk for a while about why we chose this particular format, but it probably doesn't matter. The more interesting thing, I think, is this field here called related identifiers. And these are the aspects, sorry, are the documents and files that were used to create the data set. So, and I can, I can click on this, but you can't because you don't have access to this repository. Can't because you don't have access to this repository. But if you want to actually see these things, just let me know and I'll give you access to the repository. I just don't want to give it to everyone that we publish, but absolutely, not really hiding anything. But if I click on this and then go over here, oh, it wants me to sign in and hopefully oh yeah, there we go. Excuse me. Oh, gosh, what is it something it took? What is it sending it to? There we go. 51 and there we go. And then you get the original source. And if you looked carefully, you would see the R script and the Excel file that mirrored all these things with the full data provenances there. And if I go back to And if I go back to the presentation and move on. Oh, yeah, there's one of the useful things about this list also is each of these things is a reference. And they're sort of readable, like, I don't know, like botulism, instance of botulism between 1933 and 55. So you can highlight that name or you can copy that name, and then Where you can copy that name, and then you can use it to get those data with the command. And there are graphical, shiny things that we've produced, but I don't want to worry about working on those because the important part, I think, is just to get out to you guys. I don't want to polish them for this. And so you just, if you use this command, and again, I'll send it out, you can just attach the ID that you want and then. And then you'll be able to, well, do it. And then here's the CSV that you can get it. There's different ways you can get it. There's like a download command in the API where it'll download as opposed to just showing it on the web collection. Okay. What is this? Whoops. Okay. Um Okay, there's also a whole bunch of R stuff, and I wrote too much R stuff, and it was too much to maintain. And so I decided to show you the simplest version of the R code that we worked on, which is just a simple wrapper around that API. But it's nice because it just pulls the data into R as a data frame. And it's sort of easier to use. You have to install all these things, and it'll send an email. These things, and if I send an email. If it doesn't work, let me know. I haven't had many people using it. This will be sort of the beginning. But then you can do the commands, sort of mirror what I showed you before, right? So if I run that, I get the same metadata all kind of bound to R. So this is just an example of the kind of thing you can do where you have to put all this goggly guok in there, dollar sign, and then In there, dollar sign, and then filter, which is the key word. So you want to filter out of all of what that means is you're filtering out of all the data in this archive, the stuff that you want, although there's nothing stopping you from getting all of it and then just using your own filtering technology or doing stuff outside of R or whatever, if you don't like R. But you declare a resource type, which in this case is CanMud CBI, which is the name I gave this collection of stuff that we did. And then you say, okay, I only want Etherea in Alberta because we're in Alberta. Want Etherea in Alberta because we're in Alberta and I only want weekly data. You type that in, and then you can use whatever, because it's this long format, you should be able to just use whatever tool you want. This is just ggplot, and then, okay, this is the time series of Deteria and Virgo. And you can't see the time scale, which is sort of the impressive part. But it doesn't matter. It's from 1924 to I think it's 90 something. It's 90 something. And there's gaps, and there's all kinds of words and everything, but you'll be able to see what is useful and what isn't. And another thing is that I've tried to add fields to the historical information that are going to be useful for you as opposed to. So it's some redundant stuff, like I've attached, you could calculate the days in each period yourself so that you can compute. So that you can compute a daily case rate, so that you can compare numbers of cases across time scales. But it, yeah, so that's what this is. This is just cases in each week divided by seven, basically. It's more interesting when you want to look at different data from different time scales. And I'm so. So these are the three commands that I think may be the most useful. If all the R stuff works for you and you can install everything, each of these commands gives you those three data sets that I talked about sort of in full, in their current state. So it's not like this is going to be the final state. And it will be, if you, I would love to hear what you think about them, if you're interested in them. Like I say, there's bound to be Particularly with the CDI stuff, there's bound to be questions. But that's it. And then that's really all that I wanted to show. And I've got a whole bunch of other slides if people have questions about specific things if they want to see the mortality data or the population data. Yeah, so yeah.