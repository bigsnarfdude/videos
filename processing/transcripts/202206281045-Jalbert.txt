So this is a joint work with my former master thesis student, Zuyan Alexander, and also with a lot of engineers from the government of Quebec and also a researcher at Uranus, which is a research center on climate changes. So my talk concerns a frequency analysis of discharge at Ungage River, which is not a new topic, but in this project we had a new constraint. Okay, so Okay, so a little bit of context. So, after the 2017 floods in Quebec, the Ministire de l'Environment de l'Alutte crucial Changement Climatique launched the update of the flood zone maps. So, I didn't find an official translation for the Ministire de l'Environment. So, I will just refer to them as the government. It's much more simple. So, they want to update the flood zone maps of. Zone maps of all the southern part of the territory. So the flood zone of over 13,000 rivers sections needs to be remapped using the most up-to-date data and including climate change effects. The problem is that this is a very large territory and only 75 sections have discharge measurements. So here it's the territory that is covered by the By the remapping procedure. So, all the territory are in orange, and the darker color represent the catchment where we have at least one gouging station. So, for example, for that catchment, we have measurement at the outlet of the catchment, but we don't have measurement inside the catchment. So the observation network is very, very sparse, actually. Okay, so the government engineers have developed a method to interpolate daily discharge. So what they did is they used the discharges simulated by a hydrological model used as a special covariate for interpolating the observed discharge. So a So, a hydrological model is a physical model that routes the water as a function of the catchment characteristic and also precipitation and temperatures. So, to do the interpolation, they use six different configurations of the ideological model. So, resulting in six different sets of interpolated discharge. So, each configuration. So each configuration is plausible from a ideological point of view. So we cannot say that one is better than another one. So they all are plausible. So, and also they model the uncertainty of the interpolated discharge by the log-normal distribution. It comes from the fact that they interpolate actually the log discharge. Actually, the log discharge and assume a Gaussian error on the log discharge, and when we go back to the original scale, so this is a log-normal distribution that modeled the daily discharge. So, it looks something like this. So, for each day and for each, yeah, for each day and each reverse section, we have a set of six log normal distribution as illustrated here. So, this is the sixth configuration here, this is the density. Configuration here. This is the density of the discharge for a given day. And what we can see right now is that the six configurations are fairly consistent. The mode is around, I don't know, 800 cubic meters per second, but the uncertainty is very, very large. Actually, you know, the three of Three or four times bigger than the mode is a value that we cannot exclude from an confidence interval, for example. So the interpolation uncertainty is very, very large. And for the government, that was a constraint in the project that we have to use those distributions. So they put a lot of effort to interpolate the discharge at ungauge river section. Section: A lot of ideologists, statistician, geographer works on this. So, they wanted that we use this for doing frequency analysis of interpolated discharge. So, the goal of the project was to develop a non-stationary frequency analysis method for the interpolated discharge of all ungauged. Discharge of all ungauged river sections in the southern part of the province of Quebec. So the first specific objective was to model the extremes of interpolated discharge while incorporating their uncertainty. So the uncertainty of interpolation has to be taken into account. And later in the project, we will use this model to downscale the similar. To downscale the simulated discharge from a large ensemble of climate simulation using this model as a reference. So, we need a reference to downscale simulated discharge. So, the model developed in the objective one serves this purpose. Okay, so in this talk, I will only focus on the first point, so modeling the extreme of the interval at the discharge. The discharge. Okay, it is a project that is data-driven. So I have to talk a bit more about the data. So here it is a catchment that we use as a proof of concept. So this is the catchment of the Chosiard River in the province of Quebec. So here it is the Quebec city. In the southern part of the map, this is the... Of the map. This is the USA. So, this is a moderate size catchment of a bit less than 7,000 square kilometers. It contains 78 municipalities and 179,000 habitants live in that catchment. So, the waters goes from here and go to the St. Lawrence River, which is the outlet of the catchment. Which is the outlet of the catchment. So, this catchment contains 160 river sections where we have intermediate discharge from 1961 to 2020. So, I already mentioned it, but this is the data that we have for each of the river sections. So it is here, it is an example of the interpolated discharge at the outlet of the catchment. At the outlet of the catchment for the year 1961 and for the day where the annual maximum was not recorded but assumed. So again, I just want to put the emphasis on the fact that we have a lot of uncertainty. So what the government gave us is the log normal parameters here for each of this log normal distribution. Of this log normal distribution for each of the day, but we only retain the day of the annual maximum discharge. So with this data, we can compute a series of annual maxima. So here it is a series for a single configuration. If we had all ideological configurations, it looks like this. So again, the six configurations are fairly consistent, although some Although some years we have more discrepancy than others, but in general, it is fairly consistent. But again, the uncertainty is very, very, very large on those interpreted discharge. Okay, so just for the sake of classical extreme value analysis, if we knew the true series of annual maximas, so if we have observation, it would be fairly easy. It would be fairly easy. So, if we have the series of annual maxima denoted here by the vector y, which consists of the recorded annual maxima, then the classical block maxima model would be a reasonable approximation. So each annual maximum can be approximated by a GV distribution. So, under the Bayesian paradigm, we could use the following improper prior distribution to estimate the Prior distribution to estimate the GV parameters. So it is a non-informative prior for the location and scale of the GV parameter, but it is an informative prior for the shape parameter, which restrict the shape parameters between minus 0.5 to 0.5. It is an informative prior that is used in ideology and developed by Martin and Steringa. The problem here is that we don't have a series of true discharge or measured discharge. We have a best guess of the real discharge along with its uncertainty. So we have to take that into account. So the first attempt was to use something like an error in variable model for extreme similar to what Philippe Navo already. What Philip Navo already presented in an earlier EVA. So it was, so the title just stealed a punch, but it was an unsuccessful attempt. So the error in variable model look like this. So for each configuration here, we have log normal parameters. So for each ideological configuration, we can estimate a length. We can estimate a latent series of annual maxima. From that series, we extract GEV parameters for that ideological configuration, and all those configurations are connected with hyper parameters here. But so, what in this classical model, if I can say it like this. If I can see it like this, is that each configuration has its own GV parameters. But the problem with this model is that the shape parameters of GV distribution were very, very large. Actually, it was very near the right end support upper bound. So psi was very close to one half. And this is not consistent with the real observation that we have. with the real observation that we have where for the real observation of discharge the shape parameter is negative around minus 0.15 so this is because individually the uncertainty on the pseudo discharges is very very large okay so so we have extreme that goes all over the place and to compensate the gev just need to have large shape parameters so So a possibility would be to use very restrictive prior distribution to constrain the estimation, but instead of doing that, we try to combine differently the information from the six hydrological configuration. So this is the statistical model that we finally use. So the combination of the six hydrological configuration is pulled on the Is pull on the lower level of the hierarchical model. So at the first level, actually, we will combine the information from the six configuration to estimate the best or to estimate the annual maxima of year one here. And after the series of annual maxima are linked together with a single GV distribution. So in this model, So, in this model, the unobserved annual maxima are the latent variables that need to be estimated with the GEV parameters. Okay, so for YJ, the maximum of year J, the information provided by the sixth configuration of the hydrological model is combined with the, is combined as follow in the product. So we do the product of the log normal distribution. Of the log normal distribution. So we assume that the different configurations of the ideological model are independent, which is certainly not the case, but this is a working assumption at this moment. And we combine this information with the other maximum. So the information that are bringed with the other maximum by this GEV distribution here. GEV distribution here. So we try to relax a bit this assumption of independence. So here it is similar to have a logical end for the sixth configuration, meaning that the first configuration is true and the second configuration is true and the third, etc. So this is a restrictive assumption. Is a restrictive assumption. We also try to use a logical R. So instead of doing a logical N, a logical R, meaning that the first configuration could be true or the second configuration could be true, etc. So it leads to, instead of a product here, a mixture of log-normal distribution. But with this mixture of log-normal distribution, the shape parameters. Parameters was really too big, too. So there was too much uncertainty to combine it that way. So we ended up with that model. So this is the conditional distribution for the maximum of year J. So with all those conditional distribution, we can estimate the annual maxima as well as the G V power. As well as the GV parameters here. So we have to include a non-stationary extension. So in this case, it's pretty straightforward. We just allow the GV parameters to vary with a covariate. So in this case, we use two possible covariates, so the year as a potential covariate and also the equivalent CO two concentration in the atmosphere. Concentration in the atmosphere. So, samples from the posterior distribution of the maxima and the GV parameters are obtained by MCMC. So, here as an example, I show the trace of some parameters. So, the first one is the annual maximum of the first year, our estimation, as a function of the MCMC iteration. The MCMC iteration and also the shape parameters of the GEV distribution. So we use an adaptive metropolis within Gibbs algorithm that is very, very efficient. Actually, for a thousand iteration, it took around 0.13 seconds to achieve that on a personal laptop. So that was a constraint also because Also, because we have to fit the model to more than 13,000 reverse sections, so it has to be very efficient. So we try to develop an algorithm that is very, very efficient. Okay, so model selection has been done with the deviance information criterion and for the outlet of the Shodiarch catchment. The Chaudiac catchment. So the best model was a non-stationary one using the equivalent seawater concentration as an explanatory variable for the location parameter only. So the location parameter varies in time. And what we can see here, it is the point estimate along with the 95 credible interval. So what we can see is that the non-stationary model is Stationary model is barely significant, but it is significative at the level of 5%. And what I also want to show is that the shape parameters estimate is negative, which is consistent with the observation that we have, but the confidence interval is quite large. Okay, so this is the best model for the for For the outlet of the catchment, so we can show diagnostic plots. So, this is a bit more difficult than a standard diagnostic plots because the maxima change at each MCMC iteration. So, we have to transform or at each iteration we standardize the data to assess the quality of the fit. So, we standardize the data. So, we standardize the data to the unit dump scale. So, on the left-hand side, we see the density, the empirical density, or the histogram of the residual along with the red line. It is the unit Gumbel distribution. And on the right-hand side, this is a residual quantile-quantile plot. So, the fit on the bulk is quite good. Is quite good. Also, the fit on the upper tail is also fairly good here. So, one result of this model is that we can estimate the annual maximum for each year here. It is an example for the year 1961. So, in the background, the solid line consists of the log-normal distribution, so the interpolate discharge, and the red dotted line consists of the distribution. And the red dotted line consists of the posterior distribution of the annual maximum of this year. So we reduce the uncertainty, but it is also pretty uncertain. So it's not very precise, but we reduce the uncertainty that we have by combining actually those six log normal distribution and using the estimated annual maxima from the other years. Maxima from the other years. So we can do the same thing for all years. So here it's another example for 2020. So again, the red dotted line is the posterior distribution of the annual maximum discharge for this year. And we can construct a series of estimated annual maxima along with a 95 critical interval. So that was a result that was interesting for the government. So we can provide actually a series of our best estimate of the unobserved annual maxima, which is this series for the outlet of the catchment. So we also have an estimate of the GV parameters. The GV parameters. So, with this, we can compute the effective return level. Here it is an example of the 100-year return level. So, because we have a non-stationary model, so the quantiles evolves with times. So, and the uncertainty here, the band, the 95% credible band, includes the uncertainty that we have on the annual maximum. So we can compute those special effective 100 return levels for all the river section in the catchment. So here it is the outlet of the catchment. So the water goes from here to here. So in darker blue, this is where the discharge are maximal, because the water is blue. So we use a darker blue color to illustrate that. To illustrate that. So, and here for each river section, there is 160 river sections. We compute the 100-year return levels. So, what is interesting is that at this point, without imposing a spatial constraint, the estimated discharge and return level are specially consistent. So, the government was very happy with that. On our side, on the statistical point of view, it could be very Statistical point of view, it could be very interesting to model this spatial dependence inside a catchment. For example, with a graphical model for Xtreme, that is a very recent model that is available now. So it could be very interesting, but at the moment, the engineers from the government are quite satisfied with this kind of map. Okay, so we assume that the six iological configurations. That the six hydrological configurations are conditionally independent, knowing GV parameters. So it would be interesting to model the dependence with, for example, an extreme value copula. So I think right now the uncertainty on the annual maxima is maybe too narrow because we assume that the six hydrological configurations are independent. So maybe it is not the case. It is not the case, so we could model that dependence using an extreme value copula, for example. And the analysis was performed independently for all reverse segments in the catchment. So it would be very interesting to try to model this spatial dependence. So in conclusion, we have developed an extreme value model that satisfies the constraint imposed by the engineers from the government on the use of the observation. Observation. The model takes into account the uncertainty of discharge interpolation and provides first the estimate of the real annual maxima by combining the information of all hydrological configuration and also it provides estimates of the quantile corresponding to long return period. So, this fitted model will be used later as the reference for post-processing climate model outputs to To compute the flood zone maps for the future climate. So that's it. Thank you.