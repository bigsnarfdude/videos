Steerable master passing. Actually, I got a better understanding of what is steerable. Like just leave with meter and like steerable. You have steerable well, you can only rotate it and rotate it very easily. Then I will talk about how to build this equivalent diffusion model. And then finally, I will talk about later in the space model. In particular, we want to find the invariant representation of monitors, all called asymmetric unit representation. called asymmetric unit representation so first on molecular generation challenges and opportunities this is uh i think quite a popular uh picture about the generative model scope-based generative model you start from the data distribution so which is high dimensional quite a complicated distribution then we diffuse it gradually diffuse to some some distribution close to gaussian noise but it's for But it's for a finite time. And now we try to reverse the diffusion using there is a reverse SD. So then the key is that we need to have this score function and learn this score function. So this was my paper by publishing ICR 2021. And that's the challenge, how to learn the single function. So typically we can do delois in SQL matching or during the Or in the last last year, we had an ICL paper talk about how to compute this more efficiently using the increased differentiation. What is monitoring generation as a mathematic model? So each molecule is described by a geometric graph. We have atomic coordinates, atomic features, and the connectivity. So basically, we have each monitor is a geometric graph. And suppose we have a connection. And suppose we have a connection of geometric graphs, many, many molecules. Then we try to know a distribution of molecules. After we have this distribution, then we just sample from the distribution. Each one will be a molecule. The first thing come up with is that this distribution should be rotal translation invariant. So just suppose I have molecular coordinates and the features. Then I rotate the coordinate, but keep the features the same. But it keeps the features the same. This has the same money. So the likelihood should be exactly the same. And this has a lot of applications like material discovery, also drug discovery. We want to understand protein ligand binding and so on and so forth. So this is not very high-quality illustration of this molecule generation process. We start from somehow like the noise, complete gauss. This is complete. This is compared unreasonable. And then we do this diffusion, diffusion, diffusion. That finally generates somehow like a reasonable molecule. And then we generate many such samples. This is how this molecule generation was. This is a very simple task. It's just a lot of translation invariant. What about this one? If I want to generate materials, today one of Today, one of the quite active elements was based on this 2D material, graphene. Graphene, this one is more complicated because it's not just a lot of translation variance, but you have some inherent symmetry. So, like you have this lattice, you have lattice, like you can basically have some honey group symmetry. What's more was like each one has the sub-lattice. Each one has this sub-lattice. You look at this atom A and B. A and B actually they are different. Atom A, your labels are essentially from your left, right? You have one, left you have two labels. But at B, your left has two, right has one. So these are also some inherent symmetries. So this is a critical symmetry. But when we design materials, one very poor technology was One very poor technology was called Kiragami. This is a paper cut, a Japanese word I think. So then, based on the original principle, this graphic, we cut some atom, then this will deform, right? Because this is the long-term material anymore. This will generate different 3D structures. This was simulated by Justin, my son and Justin Baker. Basically, like crystal symmetry changes during pterogamic process. Then, what if I want you to Then, what if I want you to generate those pirigami meta materials? It's quite humanity, right? First of all, you need to have this Jojo translation invariance. Secondly, you need to have this symmetry. We don't even know what is this symmetry because once you cut it, the symmetry changes. So, that well, this programming has been quite, I think, a lot of material scientists study this. One day, my daughter and I, we were. My daughter and I, we were in a park, and then I met one of my colleagues in material science. He was actually doing experiments on this cure gaming metal materials. And then our proteins, proteins, sometimes it's quite big. And we need a cost-grain model, like we only bother the backbone. But actually, to my surprise, if you want a graph representation of protein, you want to have more sparsity, connectivity, and rigidity. Activity and rigidity. Rigidity basically means your graph needs to reflect this geometry of your property. It's a quite challenging task. Like if you usually do graph based on video cutoff means if two atoms are close enough, you put an edge there. But then we tested this. For some proteins, even if you use like eight electrons, within eight electron, eight electrons, you connect them. Still not less than connected. So this actually. So, this is actually a very challenging path. And in practice, there is something for the introducing the disorder for me. It's even harder to model. So, these are some challenges and opportunities in monitor modeling. So, let's talk about how to build machine learning models to learn things. For instance, let's talk about challenges in machine learning because we deal with geometric graph. The simplest model is. The simplest model is about this massive passing you run at work. You have more atomic features and edge features. You just keep doing this massive passing. And then let's update another feedback. And this is called a massive passing neural network, which has been very popular in quantum caption. And this one is the graph convolution neural network. It's somehow like Vietnam. It's somehow like we don't know both method pattern and special method. So this was both in 2017. But this one, the problem was, look at this. I don't have atomic coordinate information, right? The atomic coordinate is not here. So symmetry, equivariance is not here. So then we want to build a general framework. Actually, this was not built by me. This was not built by me. This has been built by the community. So, first of all, for SO3 group, we have a good understanding of irreducible representation of right. So, essentially, first we have harmonic analysis, harmonic spherical harmonics, and then rotate the spherical harmonics. It's very easy. We have these Wiggular D matrices. So, this is just some preliminary background. Then we are ready to actually one nice thing of this very harmonic is there. Nice thing of this very harmonics is they are orthogonal. So then you can lift the dimension of your features. So then you can very expressive models. So sterile model goes as follows. This is a class of neural networks that are using sterile convolution and the sterile laminarity to guarantee the equivalence. So for each point, this is atom coordinate. And then we first And then we first map it to the sphere using the sphere harmonics yl so this and the unit sphere so map to the unit sphere and then next we will have this just do this piracy we will have the initial steerable vector the steerable load features next we need to have edge information edge information we again just consider this hello as the Oh, yes, this is as the map due to the spare harmonics. Then we need after we have the initial feature, we need to do the method passing. Massive passing, essentially, to have the define a convolution upgrade for tensors. Then we use tensor product. Tensor product, together with the projection, you need to project to the original stereo vector space. So then this gives the CG tensor product. CG tensor product. CG tensor product. CG tensor product, you can simply understand this as the graph convolution neural network. You have W X times the Laplace matrix. So then just ignore the weight there. So this is the way to build the steerable machine learning model. So this can guarantee dotted translational equivalence. And just some, this is not a very rigorous, but I would try to understand the existing model based on the polling. Understand the existing model based on the Bollinger classification. You only gene variant models only propagate the gene variant patients. You don't have the parallel distance information. And then equivariant gene, when AI is one, you only have parallel distance information. You don't have high order features. And then also equivariant genes with the high order features. But when you actually do this monitor modeling, sometimes you really need this high-order features. Like that's called typon moment, which all stress tensor. These are tensor features. Equivalent genes. But the problem of now we have environment models, equivalent models has like only vector features and the tensor features. First, we need to understand this zero features. Look at this. If we only have invariant features, that means we don't have geometric information. We don't use pairwise systems. Then, how about we try to? then how about we try to try to distinguish this c-s and trans structure this is original c-s and trans basically we studied high school chemistry we know the c-s and trans asthma it's uh like this one this one was like this but we just try to generate this this this change very very long then your invariant model cannot even distinguish this one the reason was that no invariant you see this you start You see, this you start here, and you type within K4. That's your reception field. You cannot distinguish these two. So, that's why we really need a pairwise system with the term of your amateur. So, another result we established was this: this is very informal. So, when we preserve the facial dimension, then the expressive power of equivalent genes employ. Of equivalent genes employable features up to type L. Basically, it does not increase as they are growth. If you can build your invariant features or like vector features with a high dimension, so then you can essentially have the same express rate power in terms of representing a function. So, that's the two theoretical results we established in a paper that I accepted a couple of months ago. Go so then we know essentially we need a pairwise instance, right? And we don't want to involve highlight features, highlight tensors, because it's too expensive. So then the math model, actually this, if you read the machine learning for molecular model, almost everybody uses this EN equivalent graph neural network from Max Learning School publishing ICML 2021. The keys, you have this power species. This power species that every time you encode the geometry of your moniker and then update the coordinates in this way, and then updating those features. In this way, this is just one layer massive aspect layer. And then you keep doing this. This will ensure equivalence of coordinates and invariance of atomic features. So then we will use this EN equivalent graph neural network to build an EN equivariant diffusion model, load of transportation invariance. So go back to this one, this diffusion model. So this one, now all we need is to replace, to parametrize this score function using a UN equivalent graph neural network. That's all we can do. Just to build this one, parametrize this using a equivalent graph neural network. Equivalent graphene on network. So that's it's brought two things. About coordinate updates, when we do generative modeling, we need to update the network. One was this is a standard, just easy. Another one was to normalize it, to normalize it. This one was actually really used in this paper to normalize recording update. But why we need this? We need to understand this. Actually, if you write down the first panel, If you write down the photo-pound equation, write down the photoplane equation, this is easy to see. You have essentially an advection distribution equation, you have a linear term, you have an advection term and diffusion term. Then the linear term has a coefficient related to this update. So it's very unstable because linear term through expansion. So that we do some democratic study. So the normalized coding update actually has stabilized coding, but it limits the expressivity. So gene. So, instead of writing the full Planck equation, I do some simple analysis. We just consider the back of propagation. Partial loss function takes the derivative with this vector theta. So then all we need is just to analyze this Jacobi matrix. So then we want to analyze how sensitive they are. To understand how sensitive they are with respect to the coordinate update. So this actually most of the sensitive part is just a jacket. Party is just the Japanese of coordinate matrix itself. So, this is of degree two. Then, we will train this model. We want to stabilize it. So, then simply we just recognize this coefficient. We want coefficient to be small. Add this into the training. This turns out to work remarkably well. So, let me show you some results. Remember, we want to unnormalize the modeling update because it's more flexible. More flexible sometimes when you do generative modeling. Flexible sometimes when you do general modeling, it can give you diverse molecules. And so, we measured the general model in terms of three criteria. One was the lack of block connection. Another one is the atomic stability and the molecule stability. So, we see that when we do, this is our model. So, you can see we keep doing the best. But the competition of cost is essentially the same since we just add like a scalar. Like a scalar to the loss function. So, the problem of this DNA covariant diffusion model is the following. We apply this diffusion to the zoom molecule itself. Molecule, you have coordinates, atom type, and like charge different kind of features. So, they are like different quantities, right? So, we need a sophisticated model of different atomic features. Some are continuous, some are discrete, and some are even categoric, like you have carbon atom. Like you have carbon atom, hydrogen atom, oxygen atom, and also the high-dimensionality of draw data. High dimensionality of draw molecular data. If you want to model a protein, that's impossible. And then also it's a challenge to account for complex inherent molecular symmetry. So that's another challenge. So then that's why we will go to the relationship space model. We want somehow to draw. Somehow, to rule out the effect of symmetry. This actually talked about this one, alignment. So, but this one was done in a machine learning approach. This is called a disentangled representation. Basically, we disentangle the group, the effect of a group like rotation, and it's the invariant representation. So, well, then the question is, can we extend this to? Is can we extend this to molecules? We want this asymmetric unit representation of molecules. I give you a molecule. Then I want to ignore all point group symmetries throughout that. So the way to do this is that some existing ways. Okay, I have a point on the SO3 group rotation. Then we seek a gram function x x. Ram function x x and then I map it to a group entry. So then fx is the group. For each data, I just use fx inverse. So this is an invariant representation. There are a lot of ways to do this. Like the simplest way is just use PCA, right? I can find the important directions, then I align them back. So PCA, but the problem of PCA is if you have invariance image, this will be non-unique. This will be non-unique, and then another approach was publishing your paper in 2022. Just use an equivalent function to parametrize an auto-encoder. This can also guarantee you learn this in variance space. Then also optimization-based approach. This one, lack of continuity. So, this is also not a very good approach in practice. So, then this is the learning-based approach. But this is the learning-based approach actually performs in general quite well, except you have some error in the learning process. So then now if we do multiple generation, first of all we use an encoder. This encoder like disentangle invariant equivalent features. So then we do this diffusion in the lateness. After we have that, we do this deloiding process. Finally, we apply a decoder. Finally, we apply a decoder to generate the molecule. So, this was called a geometric latent distribution. So, it's a paper in ICM 2023. So, all we need is we need to find this invariant representation. So, when we want to do this, as I mentioned, when we use the PCA, for instance, what's the main challenge? The challenge lies in you have inherent symmetry. Symmetry consider water molecule, water, then you do PC, basically you have two different actually give the same result because you have some inherent symmetry. So then the challenge is we need to enumerate the ambiguity caused by inherent beta symmetry. Let's identify the asymmetric unit. Anyone know how to suppose I have an object with a lot of symmetry, like this one? Like this one, I have symmetry. How can I remove all possible symmetry? Okay, so this turns out to if this job yeah, I don't know remove symmetry basically like okay so for instance this let me give you but you don't you don't want to to add anything add anything will affect the representation itself. Anything will affect the representation itself. Yeah, cut it. Okay, so this is you cut it. But so then what we needed was if we have a geometric object, we try to represent it as a graph. And then in terms of the graph, there is a very celebrated approach called Opcroft algorithm to identify this asymmetric unit. The automatic unit. This is called a deterministic finite autonomous minimization. But then, this is you operate this on a graph. So, then we need a graph to represent the molecule. And this graph needs to carry all geometric information. So, then we need to construct a nice graph for it. So, then, this graph, we have a way to construct it. Actually, we have also tried to apply this graph to protein monitors. It turns out to work quite well. So, first of all, Work quite well. So, first of all, this we didn't construct the graph. Then, this we map this point out with the unit sphere, to the unit sphere. And we map it to the unit sphere, we just construct the convex hole to find a directly labeled graph representation of the molecule. Then, finally, I apply this hop-graph algorithm to find the elastic. To find the osymmetric unit. Let's look at the result we have. So, this is the rank one model, rank one. And then you can align this very easily. You only have one principal direction. And what if, like for this water model, because you have some inherent symmetry, then it will cause a problem. So, this line seems correct, but this one, the line is the This one, the line is opposite. So, then for this auto-encoder-based approach, you use the equivalent function to learn this encoder and decoder. The problem is you can see you always have some error. Then, if we do this as matching unit representation, then we always have perfect name. So, this is a paper just got accepted to ICML. As a summary, I talked about first of all, we try to understand this stable model. To understand this theoretical model. And then we try to use this equivalent graph neural network to parameterize the diffusion model and to improve the learning. And finally, I talk about this alignment to disentangle invariant and equivalent features. So you can find the references here. I finished it in 25 minutes. 