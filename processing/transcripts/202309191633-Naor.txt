Okay. Everybody, thanks for inviting me to this workshop in this beautiful area. So I'll be talking about work joint work with Newt Buchwinder, Arvind Srinivasan, and David Eggwood White. And I think that's a good idea. Okay, good. So I'll be talking about online bipartite matching, but the techniques I'm going to use are exactly those techniques that we use in approximation algorithms, as you will see. So I'll discuss the very basic online bipartite model. Online bipartite model, which was introduced by Karp Pasirani-Basirani. We have a bipartite graph. One side are the offline vertices. These are kind of known in advance. And then the online vertices show up. Okay, the online vertices show up one by one, and upon arrival, we need when an online vertex shows up, we need to match it, and our decision is irrevocable. So let's say we picked the red edge, now comes another guy, pick this red edge, a third guy shows up. Shows up and he's matched to, sorry, yeah. And the fourth guide cannot be matched. So the algorithm matches only to two edges, whereas opt could have matched four edges. So the competitive ratio here is just defined to be the worst case performance of our online algorithm compared with the optimal optimum. The optimal offline algorithm. This is a very basic model, and deterministically, it's very easy to get half because any maximal matching, if our online algorithm is not lazy, then any maximal matching contains at least half as many edges in the matching as the optimal matching. The nice, I mean, the breakthrough that was The breakthrough that was achieved by KDT is that they showed how to get a randomized, and of course, this is best possible. And what they did is show that you can get a randomized competitive ratio in expectation, and the factor is 1 minus 1 over e, which is about 0.632. And the idea is very simple. The natural idea of The natural idea of, let's say, every online vertex just picks in random one of its three neighbors, that's not going to work. That's going to give you basically one half. What they suggested was pick in advance a random permutation of the offline vertices. And once an online vertex shows up, then that online vertex is matched. That online vertex is matched to the free vertex which has the highest priority with respect to the random permutation. And this gives 1 minus 1 over 3. Okay, this basic result was used for many generalizations. One is vertex. One is vertex-weighted matching. And for edge-weighted matching in the Edwards problem, for a fractional version, which I will define later, 1 minus 1 over e was achieved. And it's still, for these latter two problems, it's still an open problem getting a randomized integral problem algorithm, which achieves 1 minus 1 over. 1 minus 1 over. Okay, so let's talk about fractional algorithms. So in a fractional algorithm, it's the same basic model, but now we're allowed, instead of picking just one edge to our matching, we're allowed to distribute fractions over the vertices as long as As long as these constraints are satisfied, meaning that the sum of the fractions adjacent to any vertex do not exceed one. So let's see an example. So the first vertex arrives and he just does half half. Second one partitions is fraction in some other way. Third one, fourth one. And actually what we get here is three-point, the algorithm. 3.2 the algorithm achieves 3.2, which is slightly better than what we saw before, and up is still 4, of course. And actually with fractional algorithms, we'll see the reason in a minute, we can get online 1 minus 1 over e instead of half that we can, I mean, even deterministically, we can get for a fractional algorithm 1 minus 1 minus 1. algorithm one minus one over e in contrast to when we when we impose integrality we need we can only get one half okay so Thomas what was your secret in where this flicker I think I use all the batteries in here okay so we're basically looking at this polython Looking at this polytheum. And we know that for this, we will make a new colour. Okay, fine, thanks. Okay, so this LP relaxation. This LP relaxation for bipartisan graphs is integral. So integral opt is equal to fractional, to fractional opt. Now, suppose we're given a, now we're in the offline setting, we're given a fractional point and we want to generate an integral matching from this fractional matching. And we want to get a random matching which satisfies the following. Which satisfies the following property. The probability that an edge is in the matching is equal to its fraction xe. And this should be true for any edge. Now, offline, there are many ways of doing that. For example, we can take the fractional matching and write it as a convex sum of points, of extreme points, and then pick one of them with the appropriate. them with the appropriate probability equal to the appropriate coefficient. Now why do we want this property? So this seems like a nice property to have. For example, it preserves any linear objective and expectation. It preserves fairness. You can say that fairness is one way of preserving fairness. So it's a nice property to have. To have, and we don't have it for this very basic problem of matching in the online setting. In the offline setting, of course we have it. And maybe one more concrete motivation. So I mentioned before the online edge weighted matching problem. So now instead of just matching, suppose we have weights on the edges and we want to same KBV model. Same KBV model, and we want to maximize the sum of the weights of the edges and the matching. Now, if we have to relax the problem because the way I stated it, you cannot get any competitive factor. So, the relaxation that you need to introduce is that you allow the online algorithm free disposal, which means that it can Which means that it can pick an edge and then it can discard it later and replace the vertex that was matched to the discarded edge by some other edge. If you allow that, then in the fractional setting, one can get 1 minus 1 over e. And deterministically, until recently, Deterministically, until recently, only one half was known, some version of maximal matching. About two, three years ago, there was a breakthrough result that got beyond one half, but randomized algorithm that goes beyond one half, but it's still far from one minus one over E. One way perhaps of getting one minus one over e would be for this problem. would be for for this for this problem would be lossless rounding because then we can imitate what the fraction online fractional algorithm is doing okay so let's see what what what can we say if we have an arbitrary fraction online algorithm fractional algorithm what can we say about lossless round so these are So these are my offline vertices. Now comes T1, and let's say the algorithm just gives half-half to these two edges. So this means that T1 would be, each of these edges would be picked with probability half, so T1 would be matched. Same thing with T2. Half, half, so T2 would be matched. And now, Matched. And now, so each one of them is matched with probability equal to one. And now let's make the following observation. At least one of the four pairs, 1, 3, 1, 4, 2, 3, and 2, 4, has to be matched with probability at least 1 fourth, because we know that these two guys were matched. So these are all the possibilities. possibilities. And let's say 2, 3 is the pair which has probability at least 1/4 to be matched. And now T3 will show up and T3 is adjacent to both 2 and 3. So the probability that T3 is matched cannot be more than 1 minus 1 over... 1 minus 1 over 4, but fractionally I can place half and half on the two edges which are adjacent to 2, 3. So if it's an arbitrary fractional algorithm, lossless online rounding is impossible. So the question that we're interested in, how can we still generate, how can Generate, how can we still generate an online algorithm where we can do lossless rounding per edge? Okay, so one, let's take a randomized algorithm. Any randomized algorithm defines a fractional algorithm where the fractions are just equal to the prob fraction attached to an edge. Fraction attached to an edge is just the probability that that edge is in the matching. So going from a randomized algorithm to a fractional algorithm with the same competitive factor, that's trivial. The issue is how do we go from fractional or marginals to a probability distributions or Distributions, or concretely, what conditions should we impose on the fractions so that we can achieve lossless routing? Okay, so we're going to focus on two-choice algorithms, which means that at any point we're going to just focus on two Just focus on two for every online vertex, we're just going to focus on two neighbors and choose between them, or at most two neighbors, to be more precise. Okay, well, first of all, in general, two-choice algorithms are not losslessly roundable. This is exactly what we saw in the previous example. So we need more. Example. So we need more conditions. And actually, two-choice algorithms were used in several breakthroughs in online matching that I mentioned before, Edgeweighted and also for advertising, for the AdWords problem. Okay, so let's see. So this guy, T shows up, and now Shows up. And now we're focusing on two of his neighbors, one and two. So the probability that t is matched should be equal to x1t plus x2t, the fractions that these edges have received. And the probability that each of these 1 and 2 was matched before t arrived is exactly equal to Is exactly equal to the sum of the fractions adjacent to them for online vertices t prime that have arrived before t. So the probability that t is matched cannot be, it's at most one minus the probability that both one and two were matched before t. 40. So let's pretend for a second that one and two are independent of each other. And then this means that this is a necessary constraint, that x12 plus x2t, the probability that t is matched, is no more than the probability that one, the complement of the The complement of the probability that one and two were matched at time before time t. Now, of course, one and two are not necessarily independent, but this inequality also holds with negative dependence. So the idea is we're going to impose this constraint and And turns out that this constraint is sufficient for lossless online rounding when we have a two-choice algorithm. So fractionally, what I'm going to do, there are several ways of doing it, but one way of doing it is when an online vertex arrives, we're going to pick the two neighbors which have the Which have the smallest fractional degree, meaning they see the smallest sum, each of them sees the smallest sum of fractions. And now I'm just going to do water filling on these two guys, taking into account this new constraint. And the analysis of the fractional algorithm. Of the fractional algorithm, it just uses dual, some version of dual fitting. Now, the rounding part, well, we get the fractional with fractional, the new fractional solution which is generated, as I said. And now we assume inductively that we have negative correlation. Negative correlation, and then we can match with the correct marginal probabilities. This is the easy part. The technical part, the technical work, is showing that everything we did still maintains some version of negative correlation. And so that the inductive And so that the inductive assumption of having negative correlation is maintained. I won't go into this technical part, but the conclusion is that we can get a better than half. The exact number turns out to be 0.536. Competitive two choice. It's competitive, two-choice, lossless online algorithm. And of course, it satisfies this constraint. Now, question is how can we go beyond this number? So we cannot go beyond this number, this we can prove, if we restrict ourselves to two-choice algorithms. choice algorithms. We can take this constraint and we can general, in principle, we can generalize it to any number of neighbors. And with this generalization, we can show that we can get a fractional algorithm which achieves 1 minus 1 over e, which is best possible for fractional algorithms. However, this solution cannot However, this solution cannot be rounded. So it seems like another constraint needs to be added to the fractional algorithm so that we can get the 1 minus 1 over a, which is the best competitive factor. Let me mention another number. Another nice implication of this work. So, there is in online algorithms a model called advice model. And it's a non-deterministic model. What it says is that an online algorithm is equipped with an advice string. And the question is: given an advice string of certain number of bits, by how much can the competitive factor be? Can the competitive factor be improved? Well, for matching, if we tell every online vertex to which vertex, offline vertex it should match to, well, the advice would be of size n log n. Any randomized algorithm is an has gets an advice which is just the random uh random string. Random string. The question that came up several years ago was: how many advice bits do we need for online branching to break this factor of one half? And Penna and Borodin showed in 2019 that you need at least log-log n bits. Now, previously it was known how to achieve How to achieve going beyond one-half using log n-advised bits, but exponential time, or n-advised bits with polynomial time. Now, if you take our algorithm, well, we can, it turns out that it can be implemented using log log n bits and And since log log n is a lower bound on getting beyond one half, we get here a sharp threshold for going beyond one half. And let me just say something on how this is achieved. So there are two parts. First, we show that using O of one constant-wise independence, this is And twice independence, this is sufficient for implementing our two-choice algorithm. It yields pairwise independence between the alpha vertices. And so this means log n, log n bits. And to get down to log log n bits, instead of using constant wise independence, we use epsilon bias. I won't get into the definition, constant wise. Into the definition, constant-wise independence. And then to construct such random variables, log, log, n bits suffice. And so we get this the same bound. We lose little O of 1 and the 0.5366 bound. So to summarize, Summarize So, just to summarize, it would be very interesting to extend our understanding of online rounding. Lossless rounding is one interesting direction. Direction. As I said, going beyond two-choice algorithms is very interesting. The question is: what further constraint can we impose on the fractional solution so that we can go beyond two-choice algorithms? Maybe is there a notion of online integrality gap? Not really sure what this means, but in What this means, but in offline algorithms, we know that if we have some LP or a nonlinear program, the integrality gap of the program gives us a bound on the approximability that can be achieved that way. Can we come up with some notion of online integrality gap that would integrality gap that would be uh that would be interesting. So thank you. We have time for a question what is the notion of negative dependency? Yeah, standard one. For some of the analysis we need uh a stronger one where we condition One where we condition on certain events and subsets of events, and we need that when you condition on a larger, let's say you condition on subset B, which contains subset A, then the probability when conditioned on B goes down compared to the conditioning on A. So some condition on A. some sub condition was that but it's basically the s the the standard uh negative so that there are like many different style of negative st the the the bas the basic one it's not negative associ association or anything like that 