Magic, but there is a magical mystery. Okay. An awful lot of particle physics results are presented with asymmetric errors. Going to the latest Morion's talk, from the ATLAS talk, there's 24 plus, 7, minus 5 femtobars appeared. CMS, you have a whole lot more. We don't have to go very far to find them. So they're widespread. To find them. So they're widespread. And they are not handled correctly. The experts do not know how to handle them. I'm confessing my ignorance, or at least my past ignorance. I know Glenn doesn't know because I asked him. I know Louis doesn't know because he asked me. Three may not be a particularly large sample, but I stand by this statement that the experts don't know. It's important, it's widespread. So this makes it a really So, this makes it a really appropriate and useful topic for a workshop like this one. So, before we get too spread out, some ground rules to govern the talk and discussion. I want to ask this in a frequentist way and get a frequentist answer. I know there will be Bayesian approaches, there are, and when we have a frequentist understanding, we can put it down together with the Bayesian one and compare, and we will learn a lot because we ought to reject this language. We ought to reject this language of frequentist versus Bayesian. We have a lot to learn from each other. We can put the two together and see where they agree and where they differ. But until I have a proper frequentist understanding what's going on, a little voice saying, look, you know, you really could look at this entirely differently, is not going to be helpful. And there are lots of functions, parsols, ignoring normals, what have you, which are known to be asymmetric, of course. Asymmetric, of course, and they're not part of the problem because if you know what the function is, you know all there is to know about it. It's not like this simple plus so much minus so much sticking out of nothing. We're working in what I could think of as a fairly large n region. If n is large, then everything is Gaussian and all the distributions are described by two parameters, a mean and a standard deviation or what have you. The distributions with The distributions we're dealing with are sort of skewed up, messed around Gaussians, but they are described by three parameters, the third one in some way telling you about the skew, but they don't need more than that. There are people who go around dealing with their systems, their asymmetric errors by adding the positive ones and the negative ones separately in cropature. This is unjustified, this is wrong, this is crazy. This is crazy. This is so crazy. I am not going to insult your intelligence by telling you why it's crazy, although I do have a backup slide if that's necessary. There are, I think, two sources of asymmetric errors. They come from two rather different areas. One, I call these systematic and statistical. I would not go to the wall to defend this terminology. It's meant to be descriptive and how. It's meant to be descriptive and helpful rather than prescriptive and legal. I'm sure you can find statistical errors which have this pattern, systematic ones with that pattern. But let's not quibble with the names for the moment. I hope we can find better ones later. One is in this one parameter at a time systematic investigation, which we do a lot of. We've seen this before. You have some Newton's parameter mu, you have some results, which I've got theta hat. Which I've got theta hat, I estimate of the wanted parameter theta, and you calculate the result at the target value, central value of Newsom's parameter, and then you adjust the Newsen's parameter up by one sigma and down by one sigma, and it moves up and down, or possibly down and up. And if it moves by different amounts, you have to quote different asymmetric errors. The other reason. The other reason you get asymmetric errors is what I call statistical when you're doing a maximum likelihood estimation. You find the estimator from the peak value. You read off the one sigma errors from the point to which the log likelihood falls by plus or minus a half. If this is a nice parabola, those are equally spaced. If this is not a nice parabola, but something a bit paraboloid, then they're not evenly spaced. Then they're not evenly spaced, and you have to quote again asymmetric errors. Let's look at both of these, come on in a bit more detail. Firstly, the symmetric, the op-at, you have got just three points. Yes, it would be nice if you had more. And if you can get more, then you can fill in this dependence, or perhaps you could generate a random normal sample in new. Random normal sample in Î½ and read off the sample properties in theta hat. But for the sake of the argument, and this is very common, getting these points is expensive in terms of computer time, and we only have the three of them. So what can we do? We want to know how theta hat depends on nu. We know the distribution for nu. We want to pull out of that the distribution for theta hat. And with just three points. With just three points, you can do piecewise interpolation, just have two straight lines, the red lines, or you can fit a quadratic. The coefficients are quite easy, going exactly through all these three points. I'm putting to one side the question of errors on errors for the moment. I'm assuming we know these three points precisely, and I'm assuming that we have tried and failed to convince ourselves that these three points would really fit on one straight line. Really fit on one straight line if we didn't look at them too closely. Okay, so we can either do it with two straight lines or with one parabola. If you go with two straight lines, then the normal distribution for nu projects onto a split normal distribution for theta hat. The standard deviations are sigma minus and sigma plus. The areas of the two sides are the same. Sides are the same. This is a split Gaussian. Some people call it bifurcated Gaussian, but it's clearly wrong. It's not sort of split like that, it's spit like that. I like to call it a dimidated Gaussian. Dimidated is a term from heraldry when you combine two shields by spitting them down the middle and sticking them together. I have failed to convince people that this is what we ought to call it so far. It does seem to me, though, that when you have a word. To me though, that when you have a word which doesn't say exactly what you want it to mean, it seems a shame not to use it. Is there something implicit in assuming that the two areas are the same? For new, this part is the same as that part because it's a normal, and so this part is the same. Yes, but my question is really, could you have chosen a different setup? You can choose anything, but I think if you want to keep with these three points, then you have the same areas above and below. I'm not saying this must be the way to do it. I'm saying this is a sensible way of doing it, which is not obviously wrong. Uh not sensible in our opinion? Um if you have a good reason for doing it, then perhaps we could look at it. Yeah, sorry. That's zero there. So it wasn't zero ones. Yeah, but like it's whatever. Oh, sorry, yes. I mean this this is stage two and this says zero. Sorry about that. Okay, with the apparatus, one model or the other. One model or the other, you then know what the distribution or a plausible distribution for theta hat is, and you can do all the appropriate combination of errors and so forth stuff, not infallibly, but honestly and adequately. And the details are given in this treatment. Looking at the statistical, as so-called, now the problem is that you've got three points again, but this time you may. Again, but this time you know the middle one is a maximum. You ask, what can I draw that looks vaguely like a parabola and goes through those three points? There's a whole lot of choices and the best results are forms inspired by Bartlett and you have a parabola which is still like x minus x0 squared over sigma squared, but the sigma squared or the sigma changes linearly with x. Changes linearly with x as you move across. And the parameters, sigma and sigma prime or v and v prime, are readily available, are obtained from the sigma plus and sigma minus. What do I mean by best results? If I take something like a puzzle, like a log normal, like the solution of a simple small number maximum livelihood problem, where I know the shape. I know the shape, just take those three points, feed them to either of these guys, and get the shape back again. Then the shape I reconstruct is plausibly pretty much the same as the one I started with. Again, it's not manifestly correct, but it's honest, it's adequate, and you can go on because you now have a model for what the right gload is and do all that stuff. It is and do all that stuff, and the details are in this preprint. Now, you will have noticed the date on this preprint and the other one is quite a long time ago. So, what's happened since then? I published these preprints. They've had a fair amount of use in the community. I never sent this to a journal because I knew that there was something missing, it was something incomplete. If you like, If you like, I had a fear and a hope. The fear was that if I stand up and say there are this sort of OPAC-type asymmetric errors, there's these maximum right asymmetric errors, then somebody would say, ah yes, but what about these errors we have and these errors we have? And there are all sorts of different ones. And I would have to go back and think about those. And the other hope is why should we have two different treatments for asymmetric errors? Treatments for asymmetric errors? Why can't we just have one? What makes the difference between one and the other? Can we not find some duality, some unification for a unified system of asymmetric errors rather than two separate ones? Now, neither of these has been realized. I have not, alas, woken up one morning and thought, hey, if I write down the formulas of the asymmetrical system. The asymmetric for systematic errors this way, then it will look like the statistical ones. On the other hand, I have not come across any other source of systematic errors, asymmetric errors, rather than these two, so my fear is reduced, though it's not gone away. But I've also come across other questions which sort this all out. There are other questions, what do we mean by an error? That's a rhetorical. Do we mean by an error? That's a rhetorical question. Please don't try to answer that question. When we say the error, do we mean the variance on the estimator? Or do we mean the 68% central thing? When I talk about an asymmetric error, am I talking about an asymmetry in the PDF? That is fixed theta, it's a function of theta hat, or an asymmetry in the likelihood, that is, I fix theta hat and look at the variation. Theta hat and look at the variation of the function of theta. And what do I mean by handle the errors? Are they combination of errors or combination of results and chi-squares and goodness of fit? And can one but regarded as a special case of the other? So let's start looking at those. What is an error? A statistician, I didn't ask a statistician what an error was, I asked Wikipedia what an error was, but I'm sure the statisticians will agree that the error is just the difference between the observation. Between the observation, theta hat here, and the true value, theta. Businesses have no time for this because this is entirely conceptual. You can never say the error is so much unless you know what the true value is. And if I knew what the true value was, I wouldn't be doing the measurement, would I? So, physicists, I think perhaps originally people used to talk about likely errors and probable errors and root measure. Probable errors and root mean squared errors, but the qualifications got dropped off, and now we just talk about the error on a measurement as being the root mean square difference between the between the measurement and the true value. So it's the root mean square of the root means the square root of the expectation value of the Expectation value of the statistician's errors. But we also nowadays, particularly when we talk about errors, use them to refer to the 68% central confidence region. Theta lies between theta hat minus sigma and measurement plus sigma with 68% confidence. These two are, of course, equivalent for Gaussians, but not for non-Gaussians. If you've got a non-Gaussian distribution, if it's asymmetric, Gaussian distribution, if it's asymmetric, it is non-Gaussian, and then I think people probably prefer definition two. We want our statement to be a statement about the true value, not about the mechanism by which I did the measurements. And on the other hand, if you want to add errors in code, which you very possibly do, then that only applies to variances. That only applies to variances. You can't add confidence regions in quadrature in the same way if they're non-Gaussian. Adding in quadrature works even for non-Gaussian distributions. And in a typical analysis of the sort we're familiar with, at some stage, possibly in section six or section seven, there will be a table of systematic errors, maybe four or five, maybe as many. Maybe four or five, maybe as many as two dozen, where people have buried this, that, and the other, the good, the bad, and the ugly, as best they can in an honest way, and added them all up in coditure to get the systematic error. So there is no correct answer. You can use this definition, or you can use this definition. What's important is you don't get confused and you know what you're using. Which you're using. So, definition two is sort of symmetric, right, in terms of the um no, no, this could be a theta minus theta minus and a theta plus hat plus theta plus. I should have put those in. The asymmetry can be in the PDF or in the likelihood or both, and now we have to look at confidence belts and confidence regions. Confidence belts and confidence regions. And I find it, yes, so in these pictures, this is the theta axis, the true value axis. This is the data, and the data here is encapsulated in theta hat, the estimate of theta we've made from our data. And running across this way are the PDFs, probabilities of getting particular theta hats for particular. Theta hats for particular values of theta. And running vertically, you have the likelihoods, you fix the results and look at the likely probability of a particular true value giving you that. The proportional Gaussian is, I think, a very useful pedagogical distribution. Suppose you've got something which measures in a Gaussian Which measures in a Gaussian way, so sort of a symmetric PDF, to 10%, so plus or minus 10%. So if you measure, say, 100, that could have come from 90, and that's a bit over one standard deviation, because if it's true value of 90, then that's 90 plus and minus 9. It could have come from 110, and that's a bit less than one standard deviation, because a true value of 110 gives you 110 plus and minus 11. 110 plus and minus 11. So you're back, yes. So this is an expanding cone, and the upper sigma plus is longer than the downward sigma minus. So you have an asymmetric likelihood, even though the PDF is. And even though the PDF is symmetric. If the PDF is asymmetric, as it is down here, where you've got a long positive tail, then that also introduces an asymmetry in the likelihood, but it has the opposite side. If you're for a particular theta value, you may get a large positive fluctuation. Positive fluctuation, then that gives you a positive skew. Then, if you've measured a particular theta value, theta hat value, this could have come from somewhere a long way further down and be a large upward fluctuation. This has a negative skew. So, you can have asymmetries in PDFs or in likelihoods, and they are not linked in any. And they are not linked in any obvious regular way. So let's suppose we're working with PDFs. We'll get to likelihoods in a minute. If you're working with PDFs, you are probably working with, you're probably doing combination of errors. You're probably adding things in code because the standard combination error is error. Because the standard combination of errors formula that we've all learnt back in primary school, or maybe they didn't include the correlation term in primary school, is a statement about PDFs. This sigma f squared is the variance of whatever this f is. As I said before, if you don't have Gaussians and things get messy, it is still true that variance is bad, and so does the bias, which we do have to worry about, and so... Which we do have to worry about, and so does the unnormalized skew. These are the semi-invariant cumulants of the L. So, so does the unnormalized kurtosis and so forth. But we're going to stop at skew. If you've got trying to combine errors from two, say, dimitated or distorted Gaussians, and what you have to do is convolute the two of them. When you convolute a Gaussian with a Gaussian, you get a Gaussian, but that's a peculiar. You get a Gaussian, but that's a peculiar and nice property of Gaussians. If you convolute a dimidated Gaussian with a dimidated Gaussian, you don't get a dimidated Gaussian, you just get a general sort of blob. And the same is true of the distorted Gaussians. So you can't go directly to the sigma plus and sigma viruses. But what you can do, what I would suggest, is when you're adding in curvature, you don't just add the variance, you also add the skill. You also add the skew from each contribution, and because they're asymmetric, the skew is not zero. You add all these up, and then you can find the sigma and the sigma minus that give you that particular variance and skew for either the divided or the distorted Gaussians. So the table of systematic errors gains a few columns. Errors gains a few columns. You might not print them all in section 7, but you have sigma plus, sigma minus, the variance, the skew, and the bias. Add them all up, get the total variance, total skew, and go back to find the sigma class and sigma bias and keep sigma back. Translation table is here. It's different whether you're using two straight lines or the quadratic, but if you know sigma plus and sigma minus, then this is. And sigma minus, then this tells you, does the, having done the integrals of the Gaussians, it tells you the variance and the skew. You add up the variance and the skew, find a total, and then work out from that the sigma plus and the sigma minus of the total. And suggested strategy would be to choose one of the models to work with and then use the other as a standard check. If you're You can obviously, if you're using sigma plus and sigma minus in the diminutive form, the chi-squared for a particular x from this measurement is either the difference over sigma plus squared or the difference over sigma minus squared, depending on whether x is bigger than or less than u. And if you're working not with these two straight lines, but with the parabola, then it's a bit more messy and approximations have to be made. And approximations have to be made, but you can get a reasonable equivalent form. This tells you, the value of chi-squared, whether x is compatible with this measurement mu. It doesn't tell you anything about the goodness of fit. You can't use it via Wilkes theorem and so forth, like a likelihood function, because these sigma pluses and sigma minuses are. Of pluses and sigma minuses are not constants. You can, again working with PDFs, combine results. Suppose you've got a set of measurements and you want to combine them to find the average. And you do not have to check for compatibility. We could be, for example, you've measured the height of all the students in your class and you want to find the average. You want to find the average. Quite how you manage to measure the height of a student in an asymmetric way, I'm not sure, but let's suppose you did. And there is no requirement that all student heights should be compatible. You can still find a meaningful algorithm. And so we don't have to worry about chi-squared. And we can frame this question as: let's write this average as some sort of weighted sum and find the best weights to make. Find the best weights to make the result unbiased and minimise the variance. And the usual messing around gives you the standard weighting formula and the variances are given by sigma plus and sigma minus slightly differently for the two models. So we've reduced the combination of results to the combination of error. And again, if you want to do this, we've Do this and work with one model and use the other one as a sample check. If you're working with likelihoods, then what you're probably doing, what comes naturally, is combining results, because the likelihood, the two measurements is just the, the log likelihood is just the sum of the two individual likelihoods. If you've got a set of here three measurements which were given to you as sigma plus sigma minus. Sigma plus, sigma minus in the central value. You've parameterized them either in this form or in that form, so you know what forms look like. You can then find the sum, find the maximum, and the delta log L equals minus a half equal max. And again, work with one model, use the other one as a standardity check. So combination of results is straightforward if you're working with likelihoods. Goodness of fit is essential here. Is essential here and comes naturally because we're working with likelihoods and you could use Look's theorem and there are n minus one degrees of freedom, one because we combine. And if you want to combine errors using writing fields as a basis, it's rather more difficult. You can't just add things in quadrature. What you have to do is say, let's suppose f is just x plus y. I'm leaving out the differentials to make it simpler. You can put them back in. Let's take. Let's take x minus y. I don't need anything as a Newsen's parameter. I just profile along the line of constant x plus y and read off from that the delta log n equals minus half point. So you can do combination of errors, but it's now a profiling problem and not an adding variance problem. So why use different functions? We've done approximations to parabolas, we've got demidated distorted Gaussians, and surely the Gaussian or approximate Gaussian is just the exponential of the approximate parameter, but it turns out that if you try and use one to help with the other, it just gets messy. Bringing it all together, we are getting towards the end. Good. The answer to your question. Um the answer to your my question, um what is an error and are you working with PDFs or likelihoods um are linked. If you're working with likelihoods then you don't know anything about the variance of the estimator. What you know about is the 68% central confidence region. If you're working with PDFs, then you know all about the variance of that PDF, but you Of that PDF, but you don't know anything about the confidence region. These p theta hat theta, L theta theta hat, are of course mathematically identically the same equation. They're not e to the theta minus theta hat over 2 sigma squared. They're some smeared version of that, smeared perhaps in both directions. But we're treating one as a function of theta. Treating one as a function of theta and treating the other as a function of theta habits. So, almost final slide. Yes, there are only two sorts of asymmetric error, I believe. I'm satisfied with that. It's not really fair to call them systematic and statistical. We can instead talk about systematic asymmetries in the PDF and asymmetries in the right view. In the likelihood. And if you're dealing with PDFs, then when you say error, you would better mean the variance of the results. And if you're dealing with likelihoods, then you'd better mean that the error is the 68% central confidence. And people are nodding their heads there and they're saying, yes, yes, it's obvious. Why did it take him 20 years to work that out? And okay, my work here is done. Hindsight is a marvelous thing. If you're working with If you're working with PDFs, you are probably combining errors, and you should do that by adding up the quadrature and the skew in a little table and working back. If you are working with likelihoods, you're probably finding results, and you can do that by adding up approximations to parabolas. It's numerical, but it's quite simple. Good instead is probably irrelevant to certainly. Probably irrelevant and certainly difficult for PDFs. It's vital and straightforward with likelihoods. You are probably not combining results with PDFs, but if you need to, if you want to, you can. If you work at it, if you write it as a weighted combination. You're probably not combining errors, because you did that up here, if you're working with likelihoods, but you can if you work at it, but you have to do it by profile. But you have to do it by profiling. I'm not incorporating it. So that's where I think I am. I am truly grateful to Louis and Omath for asking me to give this talk, so I have been forced to get my head thoughts in order. And the ideas I got would really benefit from talks, discussion, exploration with other practitioners and experts. Practitioners and experts, that's you. Discussion, helpful criticism, unhelpful criticism, and further examples, other ideas, filling in bits of details and working on this together would all be very welcome. Chats over coffee, discussions with your experiment to see if everything is taken care of. If anyone wants to help me write this paper, which has got to be written now. Paper, which has got to be written now, I think, then offers of collaboration will be very helpful. There is plenty of work to go around, and there should be plenty of glory to share at the end of it all. And so, I see the chairman is standing there. I am hopeful that the 2023 Burrs workshop will go down in history as the one which cracked asymmetric errors in the company. Wait a second, let me press the magic button to sign.