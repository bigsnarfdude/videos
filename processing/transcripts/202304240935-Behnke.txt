All right. Good morning, everyone. For those who have never seen me, I'm a statistician, so I'll try to reduce the number of physics blasphemies as much as possible. That said, it's a pleasure to be here, to see many familiar faces from FISTAT 2021 and beyond, and some new ones. So, in FISTAT 2021, as Nick and Dola have already discussed, the goal was to The goal was to understand how systematic uncertainties are dealt with by particle physicists. And the contribution a statistician can give in this direction is dual, I believe. In one sense, what we can help with is to try to spot some issues that may arise in the methods that typically physicists use, some problems that we can spot very easily, but maybe they don't tell us about. Easily, but maybe they don't have a statistical background necessarily, therefore, it might be a little harder. Or also propose new solutions that they may not be aware of and that can potentially be helpful for them to address their problems. Now, in both workshops, we are having quite a few contributions from physicists and statisticians. A big portion of statisticians will be talking, giving formal contributions, but we will expect more also But we will expect more also from the audience that in this fight there are no really formal talks, both audience, virtual audience and people in person. Now, before we look into the problems of like the methods that deal with systematics, let's try to understand what these systematics really are. And it turns out it's not that easy to define them, actually, in the sense that, well, first of all, what do we want to do here? There will be some Want to do here? There will be some quantity of interest, can be a perception, can be signal intensity, and we want to provide an estimate. So let's say we have a parameter theta and an estimate for it. I'm going to call it theta hat. Now, if I give the estimate itself, it's useless because there will be some error, of course. So we want to quantify the error. But the sources that contribute to the errors are essentially three. One will be the statistical uncertainty. One will be the statistical uncertainty, something that we know, we don't know, but somehow we know how to deal with it in a variety of settings, because there are about two centuries of statistical theories we can rely upon, so very good. Now, systematic uncertainties is still something we know we don't know, but we don't always know to deal with it properly. And that's the reason why we're here. That's the motivation for the entire workshop. And finally, we will have some. And finally, we will have some irreducible error. It may be due to variables we did not measure, hidden systematics we cannot even think of being there. And therefore, there is little we can do with that. It's beyond our control. We can just accept that some small error may always be there. Now, but when it comes to give a formal statistical definition to systematic uncertainties, I think it's a little difficult. Difficult. So, statistician in Triestat 2021 proposed this definition, which is we can think of statistical uncertainties as those that dissipate as you increase the sample size, basically sources of variance, whereas we can think of systematic uncertainties as sources of uncertainties that do not dissipate with the sample size. So essentially, Essentially, sources of bias. And this is a definition, actually, that even ChatGPT will give you if you ask something like that. But then the more the five-star workshop went along, the more I myself as a statistician realized that this discrimination is a little bit too reductive. At the end of the day, what physicists call systematic uncertainties is way more than just bias, okay? Way more than just bias, okay? So, first of all, there are systematic uncertainties that are essentially the uncertainty around the bias correction that we make. Then we have uncertainties that, it's not true they don't dissipate with the sample size increasing. Indeed, you may have an effect of the systematic increasing with the sample. They are negligible for a small sample, but they increase as you increase the sample. Indeed, I was discussing offline via email with Tom that Email with Tom that there are situations where there is some sort of publication bias, where people stop the experiment and publish a result as soon as the systematic dominates the statistical uncertainty. So, really, very much depending on the sample size in some sense. And also, with our discussion with Louis, it came out that there are systematics that actually decrease with the sample size. That's basically you're talking about the systematic effect relative to the total. Relative to the total uncertainty going up, or are you talking about truly the systematic effect if absolute size goes up? So, that's a good question for Tom. I don't think I understand that. So, these are sentences from your talk, actually. I think we are open in the situation that we stop the experiment because our systematic. Yeah, so that's sorry, I misphrase at the time. Sorry, not to refer to this symposium. Yes, so there are other situations where we can think, well, in general, we can think of systematics having a statistical component, and indeed systematics and statistical uncertainty may be therefore correlated. So it's not something we can look at them separately at the end of the day. Now, since providing a formal statistical definition to systematic uncertainties as intended by the physics community is not that simple, I think a better way to think of is just to look at some examples. So some examples of systematic uncertainties may arise from the Monte Carlo simulations we perform. We may have systematic uncertainty due to mismodeling. Due to the uncertainty associated with the universal unit, Due to the uncertainty associated to the parameters estimated in previous studies. Then we have systematic uncertainties due to the instrument, the calibration of the instrument, and then those uncertainty with the use of CATs, for a machine learning solution to do CATS, for instance, or at various stages of the analysis. Now, for what concerns mismodeling, which took a big portion of PHI stats. A big portion of FiStat 2021, and we will also have some discussion tomorrow in this workshop. Sources of means systematics related to means modeling may arise from approximations. We don't quite know our model for the database, so we use polynomials, let's say, or some sort of mathematical approximation for them that will, of course, have its own error. Even see if we just take a polynomial and we have two truncates. And we have to truncate the order somewhere. Then we may have systematics due to the choice of the models for the signals, for instance. Maybe I want to detect dark matter, let's say, that has never been the technique. Nobody knows what it's made of. So it's hard to write down the correct distribution. All we can do is to rely on theories, and if the theory is incorrect, the model for the signal will be correct. And another source of systematic uncertainty due to mismodeling arises in. Uncertainty due to mismodeling arises in the context of modeling the background distribution. Now, for the statistician that may not be familiar with the word background, what is meant with background in physics is essentially the signal of all the sources which are not the one I want to detect. Since there are so many objects that contribute to the background, the background distribution may be particularly hard to model and therefore is easy to encounter this problem. Encounter this problem. Now, systematics related to mismodeling and indeed the situation where we have effectively biased in our model. So it kind of fits our initial simplified definition, let's say. So in FaceTAT 2021, a few solutions were proposed. From physics side, mainly there were parametric solutions. There are many more non-parametric holes in literature, but that's what we have seen mainly and twice. That's what we have seen mainly at FISTAT. Statisticians mainly focused on semi-parametric or non-parametric approaches. And so, regardless of which approach ones would prefer to use, a question that arose during the discussion from Larry Westerman was, well, when we have these many methods essentially to quantify this bias, can we somehow take the difference as a sort of measure of the systematic bias affecting our Of the systematic bias affecting our model. And then, while talking to Luy, it came out another question, which was: okay, if we do use this difference to quantify the systematic bias, then we have to keep into account that difference itself will have some uncertainty, so it will introduce some other source of uncertainty as well. So, now, this is in some sense the first open question from FISTAT 2021. I will have I will have a few others throughout the talk that I hope will help simulate in the discussion throughout the workshop, maybe at lunch, dinner, or if you want to talk about something more like how your pre-pear went. So now another aspect that caught a lot of attention in FISTA 2021 is how do we deal with nuisance parameters since effectively within the physics. Effectively, within the physics community, very often systematic effects are taken, either introduced into the model through nuisance parameters. Now, how you introduce them into the model, it could be by using the posterior from previous experiments or the prior if you're using a pragmatic Bayesian approach, or you simply multiply the likelihood of your main experiment by the likelihood of auxiliary experiments that were used to constrain your Use to constrain your parameters. Now, once you do that, do you marginalize or do you profile? In principle, it would be natural to marginalize if you're following essentially a Bayesian approach and profile if you're following a frequentist approach. But then there were also hybrid approach proposed by Bob, for instance, which suggest frequentist approach for the main analysis and Bayesian for the auxiliary experiment. For the auxiliary experi for the auxiliary studies. Now, again, we have a bunch of possible ways to proceed, and I wonder if it is even possible to reach a consensus on which approach to use when. And probably, I guess not, I don't know. And if we cannot reach a consensus, how can we then somewhat compare or combine the results from various experiments if that is Various experiments, if that is at all possible. Now, on the line of combining different results, we have to keep in mind that a substantial challenge here is to take into account the correlation across various systematics. Nick mentioned this aspect as well. Correlation considering the result of various different studies. And a big help when it comes to combining results from different studies is the fact that. Studies is the fact that we have access to the likelihoods now. So there is the possibility to use the results of somebody else, really see what has been happening. And this is all wonderful, I think, for the physics side. But then as a statistician, if I look at the plot that Nick provided, for instance, I wonder, first of all, can I even make sense out of such a complex model if I don't really have much of a background on the topic-specific background? On the topic-specific background, if I don't really know much about neutrino experiment, let's say and I would like to work with a neutrino expert and can effectively assess these likelihoods. Because as a statistician, I'm used to think of likelihood as really functions, okay, with parameters. Whereas physicists often, when you talk about likelihood, really what they mean are templates or basically software that will generate some values when you have a set theme. Values when you have a set of inputs. And if these software is something that me or my students have to spend one or two months learning, it is a little bit of a challenge here. So I wonder up to what extent a statistician can effectively assess this likelihood on its own. Now, I'm asking this not just for the sake of the conversation, really with the final goal of our analysis in front of me, just thinking clearly where we want to reach. Thinking clearly where we want to reach. And what we want to reach is essentially either detecting signals or checking the validity of some models. So at the end of the day, we want to find, we want to perform inference in one way or the other. Now, if you have a lot of computational resources, you can do your confidence interval or construct your tests and simulate everything. Great. But if your model is very complex, maybe using asymptotic results is what could be possible. Results is what could be particularly useful. Now, when we deal with asymptotic results, there might be some issues that come up. And this is something that Alessandra brought up at FistTart 2021. So it's not really just self-promotion since she cited one of the papers at co-authored. It's really an important point here. So whenever we use some Whenever we use some, let's say, very popular tool of statistical inference, such as likelihood ratio test, let's say, and we know it has a chi-square distribution, that chi-square distribution is there under some assumptions. If I look at a model as simple as this, where I have a density, a distribution for the background with some nuisance parameter phi, and then I have a model for my signal. Suppose the nuisance parameter here is the signal location. Here is the signal location. Is this my belt? So, if I have my signal location here, this is a nuisance parameter, whereas the parameter of interest is the relative intensity, this eta. Because if I test eta equal to zero is equivalent to test if I have a signal or not, or greater than zero means I do have a signal with positive intensity. But now But now I already have some troubles in terms of these regularity conditions behind wheels. First of all, eta goes between 0 and 1, so if I'm testing eta equal to 0, I am on the boundary of the parameter space. So I don't have my price priority. Second of all, if my eta is equal to 0, it doesn't matter what's the value of theta, I always have the same model, the background. So I am in a problem of non-identifiability. In physics, this problem is This problem is essentially what comes up with the look as a wear effect. Sorry, cannot really limit it for eta equals zero. They're limited by the condition that the density must be non-negative. So eta can be less than zero. Yes, but the density is by definition not a negative. No, the total thing. So here I'm talking about relative intensity. So I want this all thing to integrate to one and then positive. To integrate to one ending positive. Yes, and that means that eta can be less than zero because what you get is the fun. If it's a physical parameter, it's like a cross-section, we don't be sure. No, no, this is not a physical parameter, this is a physical parameter. It's a big difference between the statisticians and the physicists. Physicists normalize afterwards. Statisticians think something. So the statistician thinks of this as normalized quantities positive, well non-negative. So it is normalized, but we are assuming it is non-negative because that's definition of probability density function, let's say, or probability mass function for statisticians. Right, exactly. If eta were negative epsilon, a small number, it would still be a legitimate statistical model. Instead of having a bump, you'd have a dip. Yes. Yes. But so to me, it's really the interpretation, right? The null hypothesis is I don't have a bump. So a negative value is part of the null. The null hypothesis is still that point mass at zero. But it's not so much because that's a density, but because that's what you're actually interested in the testing. Yeah, I'm talking about this at every fiscal meeting. You have to distinguish between the model primarily. Between the model parameter and estimates of the model parameter. Write down the model. Before you do anything, write down the model. Every parameter of the model has a range. And Poisson means have to be non-negative or even positive. And so the confusion always arises because model parameter absolutely has the physical constraints. The sample from the distribution of a Gaussian synodon model parameter can be negative, but that's not. Can be negative, but that's not the model parameter. So, as I understand this, she's talking about the model parameter, and therefore it is constrained to be based on it. Yes, thank you for translating. No matter what I'm talking about, I put a slide on this. And on there, like it's a great way of So I'm saying a second kind of ether is completely reduced result. Yes, but I'm saying if I want to detect a pump, it is is still this is a legitimate constraint. I claim that's a horrible thing to call it. It's a sample from a distribution. Gaussian centered at zero, you sample from it, you get a negative number. I don't like to call that an estimate of the model per hour because The model parameter, I think, chose this. Stop calling it new hat. If I can just add another viewpoint from statistician, actually, ether, a negative value for ether, is not allowed because when you act by maximum likelihood, you have to constrain parameters to get up around space. So ether is. So either is defined to be positive or actually between 0 and 1. So it depends. Okay, you can, if you treat the likelihood of the log likelihoods, choice versus maximum function, you can maximize it and get whatever type of values you get out of them. But the maximum likelihood estimator has to satisfy the constraints which are of course the name. So if I've got it right and your question is about having an eggs value, It's about having a negative value. So if my student negative value estimate for e cap, it's wrong. That's on my slide. That's exactly what I have on my slide. The imaginary magnitude estimate is zero when your sample is negative. Yes, it can be zero, can be one, can be any value between zero and one, but you can't be anything. We don't know how to generate in our toilet. We do where we love money carols. We don't know how to generate those. How to generate Poisson data from Poisson mean as negative. So it's just not in the model. People talk about Bayesian consideration. There's no prior. I'm sorry, I think we need to continue. Yes. I mean, we will take up from the discussion this time. So anyway, let's assume that this is okay. Now, another problem that can come up is when actually the nuisance parameters are on the boundary of the parameter space as well. And this is a much more tricky scenario, I would say. One highly cited paper in statistics literature, Self-Elian, 1987, deals with this problem. But even this paper with dozens of citations, the asymptotic is actually incorrect. Like the examples are fine, but the proof of the main theorem is... Fine, but the proof of the main theorem is not really valid. And so even statisticians don't quite know how to handle this problem very easily in a natural way, let's say. Even if you do simulations, you might encounter issues indeed when you have parameters on the boundaries because you may have simulation results that are really inconsistent with what you want. Now in general, In general, citing Richard from FISA 2021, statisticians need to see abstraction at the level of mathematics to be confident that they are given a valuable contribution. And what I relate to this sentence and I really liked it very much is that when I see a model like this in sort of a simplified form, I can kind of spot where possible problem may arise. But if you have a likelihood in a software, I'm not quite sure. In a software? I'm not quite sure I can. And so, also, I even wonder: how do you even check these conditions in your complex likelihood? Because at the end of the day, if you are not checking them and you're still using something like chi-square distribution or some other variation of that with their own regularity condition, and you report a p-value under this assumption that they're then invalid, what you have is basically just a falsity in terms of result that you're reporting. So, we really need That you're reporting. So we really need to be careful there. And I think, in this respect, this is where statisticians can help as well. But I think what can help bridging the two community here is, when possible, providing simplified models that then hopefully, if I solve the problem for the simplified model, you can extend it to the more complex scenario. And that is because, from my experience, when I try to convert important statistical issues arises in physics into Issues arise in physics into statistical questions. Very often I end up with fundamental statistical problems. So not something not that triggered like an entire PhD thesis, as it was the case for me. So in this kind of situation, if a statistician wants to allocate at best his resources, should not probably invest three months of the PhD student or a postdoc to learn a software to figure out how to compute the likelihood, but maybe something that is more fundamental from a statistical perspective. Fundamental from a statistical perspective. In this respect, also having some sort of realistically simulated data really helps a lot rather than again having to learn software to just open the data. Data challenges are incredibly useful in this respect. And in fact, I know people also in the audience who have published a paper in statistical journal proposing some new methods and to show that they're mixed words to use the data from the data challenge. So they're really, really valuable. So, they're really, really valuable for us. That said, I wonder if there is anything else we could do to somehow make sure the communication stays up between the physics and the statistics community. And this is a summary of these five questions that already raised some discussion, apparently. And that said, I thank you all for your time and for accepting our invitation. Well, then you want to ask the question. Well, maybe partially.