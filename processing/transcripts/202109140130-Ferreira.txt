So, I think now is the time, and I'm very happy to introduce Professor Fernando Ferreira from Lisbon University. He is a well-known proof theorist with many, many descending scientists, I can say. So, please. So, please. Okay, thank you very much for the introduction. First of all, I would like to thank the organizers for inviting me for this meeting. It is a pity that the meeting It is a pity that the meeting is not taking place in China. When I got the invitation, I thought China is very far away, but I've never been there, so I will accept. But it turns out that we are still in the pandemic situation, and so the meeting is online. Also, I got more time than I asked for. So I have decided to speak about the theme that I originally submitted, but also other things. So it's a motlay of things and maybe some polemic. Polemic thoughts. But yeah, so let me start with my talk. So this is about this meeting is about new frontiers in proof and computation. So I will speak about proofs and computations to start with. I'm going to state many things. many theorems they will not be in in in in the most um uh they they are correct but they are not they are not um optimal so that we don't get lost in details um so uh the first is is very well known suppose you prove an existential prove an existential statement exist x a of x a can be of any complexity provable in first-order intuitionistic logic then there is a term t such that you can also prove this this can be proved in several ways for instance catilumination or something um the second theorem suppose eiding arithmetic which is the intuitionistic version of piano arithmetic proves the center arithmetic proves the sentence for every x there exists a y a of x y a of arbitrary complexity then there is a closed term of godel's calculus of primitive recursive functionals such that for every n a of n t of n is true um this one can be proved using um godel's uh dialectica interpretation for instance um instance. So of course, I mean, intuitionism is there is a philosophy, an intuitionistic philosophy whose main claim is that when you prove the existence of something, you have to construct it. So this second theorem says that you prove the existence of You prove the existence of y in terms of x, so there must be a construction of y from x, and indeed there is. Of course, it has to be expressed not in the language of HA because the provable total functions of HA exceed the primitive recursive functions, so it has to be. Functions. So it has to be stated in a terminology that goes beyond the language of HA. And one of these terminologies, not the only one, is using Gödel's primitive recursive functional. So T technically is a closed term from the type of natural numbers to the type of natural numbers, and it gives rise to a recursive function. To a recursive function that to n gives Tm. So the first theorem, of course, because we are in logic, let's say, you can do everything inside the language. But that's what we expect of intuitionistic reasoning that you have so-called existence properties. You have so-called existence property. If you prove the existence of something, you should effectively get it. Oops. So there are many generalizations of these results. The technique that I mentioned of God's function interpretation also shows this is just a curiosity, but well known to people in proof theory. In proof theory, that you can add to the second theorem Markov's principle, which is this principle that I wrote here, where B0 is quantify a free formula. Well, I draw attention to this because one thing is intuitionism and intuitionistic reasoning. Another thing is computational extraction. I mean, you can have computational extraction. And you can have computational extraction in theories in which the reasoning is not totally intuitionistic. Markov's principle is not acceptable by intuitionistic people. But nevertheless, I mean, it doesn't change the fact that you have this computable. That you have this computable witness. But we will see that even non-effective principles like Wickonig's lemma within the intuitionistic setting can be enjoyed now in second-order extensions, of course, and still allow some kind of computational extraction. Okay, maybe the concrete results that are Maybe the concrete results that I'm presenting here, they descend from the work of many people, namely, for instance, Ulrich Kohlenbach in his book. He has results that are not exactly like these ones, but are close. So if you go to classical logic, you have You have the situation changes. You have the Airburn's theorem, which is the main theorem, I would say, that L be a language of first-order logic without equality, with at least a constant symbol. If you prove the exist x A0 of X, A0 is quantifier-free. If you prove it in first order classical. If you prove it in first-order classical logic, then you can find the finitely many closed terms such that this quantifier free sentence, this finite disjunction, is also provable in first-order classical logic. Actually, if you see it in the right way, it's a tautology. But so in here, you see a difference to intuitionistic logic. A0 is now restricted. It's not the matrix is not of arbitrary complexity. It has to be quantifier free. That's a first difference. And a second difference is that in the conclusion, you don't just have a zero of t, you have a finite t junction. And this is not an artificiality, it has to be like this. It has to be like this. These things, T1, TN are so-called urban terms, I guess. And of course, it may be proved in several ways, for instance, by cat elimination against technique, but also other ways, which I will mention maybe in the next slide. Next slide. So now, if you go to classical arithmetic, suppose PA proves the sentence for every x there exists a y, a0 of x, y where a 05, then there is a closed term, t of Gaudel's calculus of primitive recursive functions such that this is true. So in EI, of course, the matrix is quantifier-free, it's not unrestricted, and the finite. And the finite disjunction, let's say, disappears, but you can always make it disappear if you have definition by cases which you have in this situation. Actually, my particular way of seeing these things is that all these theorems are variations of a single theme. single theme so um um all this theorem here can be proved um using uh goddull's function interpretation or more directly using a variant that works directly for the classical case that is due to schoenfeld and this theorem here and erban's theorem can be proved with this technique With this technique. Okay, why do you, this is, everybody knows this in this meeting, but let me remind this. Why do you have to restrict the matrix A to quantify FE and not just allow any matrix whatsoever? It's because the result is false otherwise. is false otherwise. So suppose that pa plus for every x there exists a y of x y, can we find a computable function such that this happens? And in general, no, you cannot find. If you use the law of excluded middle, you have this that PA proves this sentence here, where T is clean is T predicate. T predicate, but a computable witness would decide the alting problem. Of course, this uses the fact that you can prove this uses the law of excluded middle or LPO the bishop's principle of omniscience. Omniscience. So, of course, this does not happen in intuitionistic logic, and that's why in intuitionistic logic you can allow arbitrary matrices. So, well, this business of effectiveness and the law of excluded middle, the use of law of excluded middle mathematics and the And the breaking of effectiveness. Of course, this is all well known, and from the proof theorist side, from the mathematical side, people just use the law of X through the needle with no second thought. But there are some mathematics and some mathematicians that are attuned. Attuned to these problems. For instance, in number theory, there are a series of finiteness theorems, theorems of the following form, that given x, the set of solutions to a zero of xz, a zero is decidable, is finite. And the way you express it is this way. And this is not pi. Pi zero two sentences, pi zero three. So it happens that there are very important theorems like for instance Roth's theorem on Diophantine approximations, in which it is not known if you can get the Y effectively from the X. The proof really uses the law of excluded middle and you and And apparently, you cannot go around it. So, and these are deep problems in number theory. Another example that was spoken years since the first talk by Shu is cauchiness to prove that a certain sequence. prove that a certain sequence a n is Cauchy. If you express this is again pi zero three sentence and in general you cannot express the n in terms of the k recursively. But as opposed to the example of the finiteness theorems, when you cannot get When you cannot get effectively n from k from the proof that you have, it is usual, and maybe Ulrich can correct me if I'm saying something wrong here, usually you cannot do it because it's not possible to do it. It's probably not possible to do it, which is different from the finiteness theorems that I mentioned. Theorems that I mentioned. So let me give an example of the case of Browder's strong conversion theorem that was mentioned in the first talk by Xu. And it was, I think, in the discussion afterwards. I think in the discussion afterwards, Urich mentioned that you don't have this effective rate of Cauchiness. Where does the non-effectiveness come from? Of course, the proof of Browder's theorem, the original one, is very abstract. It uses Tikhonov's compactness theorem in the form to prove that a boundary of the theorem is the same. A bounded closed convex set of a Nilbert space is compact for the weak topology, and this uses Tikhonov's compactness theorem. So it's a very abstract proof. But surprisingly, it's not where the non-effectiveness does not come from the abstraction, does not come from the abstraction, the abstract machinery. The abstract machinery. It actually comes in this particular proof from the non-effective existence of approximations of infima. For instance, approximating the distance between a point X and a set S up to 1 over K plus 1 is the statement that I write here. So given K, you can find Y, the good approach. You can find why the good approximations are that it's a good approximation. And you have to prove this, you have to use you have to go by reduction. And it has to be, there is no way around it. Otherwise, the proof will be effective and then one would have an effective rate of conversions. Effective rate of conversions, which we know that we do not have. So it's proving this that turns this as a non-effective argument. Of course, mathematicians do not see this because they just take the infimum and taking the I'm taking different is arithmetic comprehension. If you have arithmetical comprehension, of course, I mean, you have excluded middle and everything. So, but in here, you don't need arithmetical comprehension. You only need these approximations, but these approximations require classical logic. Okay, so okay, let me just review something of. Something of course. So, if you have a pi zero three statement, usually you don't get the n in terms of the k effectively, but there are some things that you can do it. And so that's what people do, for instance, as Columbach did in the analysis of browser's theorem. You cannot get an effective rate of conversions, but you can get a rate of metastability. And I'm just going to briefly explain. and I'm just going to briefly explain what this means. So suppose you have for every k there exists an n, for every m abava. If you look at this part, inner part here, there exists for all. If you negate it, you have for all that exist. And if you use some choice, you get the, well, actually, you don't have to use choice here, just it's called choice, but it's. Okay, you don't need the actual choice. There exists a G, so this is equivalent to this. And then if you negate again and plug in here, that means that this statement here, it is equivalent to this statement here. And that's the metastable version. And there is a computable functional such that that gives dn in terms of the k natural number as g function. So it's a function. So g is like an oracle. There is a computable functional f and it can be defined in this way. Of course, in proof mining, you get concrete bounds for this. So, this is maybe the epolemic slide. I don't know if I should use the word slide, but oplemic slide. Well, there seems to be a growing consensus among logicians that God's incompleteness theorem results. Results do, do not have much relevance for present common mathematics. That in practice, the use of abstract machinery for proving common elementary mathematical theorems can be removed. I mean, this is defended by Angus McIntyre in his contribution to the Goddess centenary volume. And of course, there is also some evidence. Of course, there is also some evidence in the areas that the group of Columbach works, in which one can see that the proofs can be finitized, even though the original statements use abstract machinery, like, for instance, in Browder's theorem. At the end, we'll give two examples of removals, one discussing Browder's result, the other on Browder's result, the other on new potency and Zones number. Let me just say that this consensus is not universal. So I spoke about intuitionism and then classical logic. Let me now speak about semi-intuitionism. Now suppose you have Eiding arithmetic plus Markov's principles, plus the lesser limited principle of The lesser limited principle of omniscience, which is this principle here. This is, of course, a law of classical logic. You can just prove it by if you negate this, then you get the negation of this. And negating this is to say there exists something that's such that not, and there exists a y such that not. And of course, this pair will refute this part in here. Of course, this is. This part in here. Of course, this is classical reasoning. But yeah, suppose you prove for every x there exists a y A of x y, a arbitrary complexity. Well, you cannot get a precise witness, but you can get a witness among finitely many witnesses. So there is a closed term of God as Carcass of Primitive. Those terms of Gautas calculus of primitive recursive functions such that for every n there exists an m less or equal than t of n such that a of n m is true. So you have a of n zero or a of n one or blah blah blah or a of n t of n so you one of these will give you the witness. Actually, you can also, oops, you can also add what I called bounded contra collection, which is this, which is not intuitionistic valid. Well, people know the collection. Collection is this. Notice that AE is unbounded. In E it is bound. In here is a bounded, in here it is bounded, like in here or quantifier free. This, of course, is a constant of HA. You can prove it by induction on y. But not this, which is the counterpositive restricted to quantifier free formulas A. But nonetheless, you can edit. You can edit. And so you could put here also plus PCC collection. Actually, this one is a consequence of this one in the present of, yeah, I don't remember exactly, but this one is a consequence of this one, maybe already in HA, in HA. So, I mean, yeah. I mean, yeah, you don't get precise witness, but you get a witness among finitely many. So I put here this threefold table. So in intuitionism, if you prove there exists an x A of X, you expect to effectively construct a term T such that A of T. such that a of t. In classical logic, you have Ehrman's theorem. If you prove the exist x a of 0 x a of 0 means quantifier 3, then you can have finitely many terms or weaknesses and you know one of them works. You don't know which one, but one of them works. And in semi-intuitionism is something in the middle between intuitionism and classicism. And classicism. So the statement is exist A of X like in intuitionism, but the conclusion is like in classicism. I think that this is very robust. This kind of semi-intuitionism is very robust. Okay, so my Okay, so my talk is entitled Accumulation into Finite Sets. Let me just see what time is it. Okay. Accumulation into finite sets. So let me briefly say what I mean. Well, the two of the Well, two or three years ago, I came with a combinatory calculus which adds to the usual combinatory calculus. Actually, I did it together with my co-author, Gilda, in the framework of logic, and then I did it for arithmetic afterwards. But yeah, it's for arithmetic. So you have the ground type N for now. Type n for natural numbers. And of course, if you have types, you have the arrow type, that's the usual thing. But we also have a star type, which sigma star, the interpretation should be the non-empty finite sets of elements of type sigma. And you have terms in this combinatory calculus that are given by the combination. That are given by the combinators, by the arithmetical constants, including the recursors, but also the star constants. This gives single terms, this gives union of two sets, this gives index unions. I'm not going to go into details, but there are some conversions which are associated with these set theoretical inequalities. Of course, the conversions should be. Of course, the conversion should be read from left to right. And I'm using ESF theoretical notation so that people understand better what are these equalities and where these conversions come from. So with this conversions on top of the usual conversions that you have in the usual combinatory calculus, this star combinatory. This star combinatory calculus enjoys the properties of confluence and strong normalization. So, as a consequence, from a closed term of type n star, you can normalize it and what you get in the end is basically finitely many numerals. There may be repetitions, but you can read off effectively finitely many numerals from what. Many numerals from a post and of n star. So, this is maybe the most technical slide. Okay, when I introduced this calculus, it was to prove Erban's theorem using a functional interpretation, which had been done before by Uri Kohlenbach, but instead... By Ulrich Collenbach, but instead of this star combinatory calculus, you have definitions by cases. Yeah, we just basically accumulate witnesses into finite sets. That's why accumulating into finite sets is the title of my talk. And you have to accumulate because in the end, in the Armen's theorem, in the end, you have finitely many weaknesses, but you have to do this along all logical rules. Along all logical rules. And you can do it using this star combinatorial because, of course, you have to define the function interpretation. And I originally defined it for logic. Now I was thinking about defining it just for arithmetic. But why not define it for arithmetic, second-order arithmetic, for WKL? Notice that I don't put WKL not because I'm Because I'm allowing unrestricted induction, but of course, you can put WKL not if you restrict the recursors. So this is a technical, it's what I call a cumulative interpretation. It's like in Schoenfield's interpretation, so to a formula A of second of language of second order arithmetic, you assign formulas. You assign formulas in a language that I don't specify, but it has all this, it's a finite type language of the form for every A there exists a B. And this is supposed to be a Bayes formula. I'm not going to describe it synthetically. So these are just for second-order bounded formulas. order bounded formulas. What are second order bounded formulas? Well, they are closed under first order bounded quantifications and second order quantification. It's a peculiarity of these things that second order quantifications, second order set quantifications are considered bounded quantifications. And then if you have already interpretations of A and B. Already interpretations of A and B, you define it in this way. You see that the only clause that raises types is the clause of not because of this for every F. And you see that there are similarities between clauses five and six, because this is for every X letter. Because this is for every x layer, so equal to z. So I have primitive bundled first-order quantifiers in the language because it's more expedient in this way. When you define the interpretation here or for every x, it acts in the same way because they are like bounded quantifiers. So they just go inside. So, with this interpretation, and notice that I should not only here you have this for every F, but here you need the star type. So, if A is of type sigma, A prime is of type sigma star. So, and why do I say accumulation? Because, well, first of all, if you look at the clauses, the existentials are all of type sigma star. So, there is a crucial monotonicity property. a crucial monotonicity property. If you have A of A B and B's containing B prime, then A of B prime. And then you can prove a soundness theorem, which I'm not going to read it. The conclusion is, I put it set theoretically. Of course, you can also have the conclusion in a theory, but I chose to do it like this. Like this. And so the corollary is that if WKL proves for every X exists A0XY, then there are close terms. There is a closed term of type N to N star such that this is true. Actually, you can internalize this argument working with a modification of the model of Of the model of hereditary recursive operations, and you get a conservation result. So, this is correct in PA because of this. Friedman's conservation result can also be obtained by restricting the term couples, working with restricted induction instead of just a full induction. So before I go to, this is known, before I go to semi-intuitionism, let me describe briefly second-order arithmetic in the intuitionistic setting. So it has intuitionistic logic and it has this axiom and And there are some principles that are important. This is collection. Now, if you replace this bounded quantifier by for every x, this is collection one type up. It's called intuitionistically fun. Intuitionistically fun, which is classically false. It's a classically false principle. And you have the reciprocals. So, but the reciprocals must have quantifier F3 formulas or bounded formulas. So this is the reciprocal of this one. And this one is the reciprocal of this one. This one is just Wick Koenig's lemma. It's a counter collection principle, as this terminology of collection and counter collection was introduced by me and Olive a long time ago. So suppose this theory, intuition, plus delta zero comprehension axiom, you can also have delta one plus Markov's principle by a principle of independence. A principle of independence of premises by Wikoig's lemma, by fun, which is classically false, all the others are classically true. Suppose that this proves this sentence, where A is a first order formula, first order, then there is a closed term such that this happens. In particular, this is true. I mean, this is an observation because since fun is false. Since fun is false, but it only proves first-order a true synthesis. That's the corollary. So, this theory, although it has the fun principle, if it proves a first-order sentence, that sentence must be true. Of course, the theory also proves fun and fun is false. So, we have to restrict somewhere you restrict the first-order sentence. I mean, I think this is. I think this is intuitionistically known, at least if you, not precisely the result as I'm stating, but the gist, let's say, that fun does not perturb first-order synthesis. It doesn't, at least in terms of truth. So, So moreover, if you restrict to for all that exist, this is just conservative over HA. Let me say something about accumulation here. In here, the types the existential variable is dA. In classical logic, this type. In classical logic, this type star, so it was a set. In intuitionistic logic, this type here is not a star type, but the end star type, something of this form. It ends with a star type. And you can define a relation here by defining it in this way. And then you have this monotonicity property. So this. monotonicity properties of this accumulation if if if a witnesses something bigger also witnesses okay let me just do a brief deterrent on on admissibility uh actually before i published papers on arithmetic i had published also papers on admissible admissible set theory and i'm not Set theory, and I'm not going to explain it, but yeah, of course, the witnesses are not collected into it's a cumulative interpretation, but witnesses are not collected into finite sets, but in countable sets, which are coded inside some trees. And basically, if you have q patec with infinity and you prove for every x there exists a y, where phi is bounded formula in the sense of set theory, there is a There is a closed term of a calculus such that, well, W is gives the three ordinals up to omega one. So the countable ordinals. So if you have this, so you can witness the y in terms of the x in this precise sense. For every x in la, l is the. X in L A. L is the constructed holier A. L A is supposed to be L of the ordinal associated with A. For every X in L A, there is the Y in L T of A, such that these are. In particular, if you just have there exist, you get this with alpha smaller than the Bachmann our ordinal. So this can be done with a function, a cumulative function interpretation. One can also analyze second-order theories, class variables, with the principle of 3 pi one on reflection, which is a kind of Wick Koenig's lemma in admissibility. And you can also analyze very natural semi-intuitionistic admissible theories. I believe that. I believe that Gerard Jager is interested in these results now from the point of view of ordinal analysis. This thing was published like seven or eight years ago in the Journal of Symbolic Logic, and this analysis of semi-intuitionistic admissible theories is in Pfefferman's volume. So how much time do I have? How much time do I have? I should have watched in here. Yeah, you have including this discussion 17 minutes. Okay. Okay, so let me speed up. Actually, I'm not far from the end. So from logic to mathematics, this is an interim slide. Of course, I went to second-order arithmetic. But one should go to finite order arithmetic because you can express more things in finite order arithmetic. In order to have mathematics, I mean, all these theorems like Erman's theorem, they are based, they can be extended to universal theories, and everything can be put in the framework of universal theories in a sense. Because if when you have recursors, then you can. If when you have recursors, then you can prove induction using so-called characteristic principles. So, recursors are there to have terms and to witness existential statements, but they allow for induction. And barrel cursors, they allow for numerical comprehension, unrestricted numerical comprehension. This is, every time I say this, I point, this is an amazing thing due to Spectre in the early 60s. In the early 60s. Of course, you can also have numerical comprehension using Pfefferman's new operator, but that's not as sophisticated as bar recursors, because you do not only get numerical comprehension, you get more. You actually get also some kind of functional comprehension. And that's another thing. Comprehension for functions is a completely different. A completely different sort of animal, let's put in this way. Let me make some. This is maybe the second polemic post slide. I mean, if you are naive, and maybe I am naive, and one believes that ordinary mathematics can be formalized in second-order arithmetic, that means that extensionality is not needed in mathematics. Needed in mathematics. Because in second-order arithmetic, you cannot express extensionality. So, but of course, if you go to one type IA, you can express extensionality. And the simpler case of extensionality is this one. If f and g are extensionally equal, then whenever f has a property. whenever f has a property g has the same also shares that property that's but this principle is does not have a dialectical function interpretation you cannot do proof mining in its presence the so-called bound function interpretation actually refutes extensionality in the presence of feffermann's mu operator extensionality can be stated as a universal statement which kind of flattens the thing Flattens the sink. Is the presence of μ a dividing line for mathematics? I mean, Sam Sanders is going to be speaking after me, and he probably will have will say something about this. Certainly, if you have mu, I mean, there are many things that you cannot. There are many things that you cannot do, of course. There are also things that you can do. Let me finish with these two examples. In order to interpret the bowel collection principles, oh, let me restart. So in secondary arithmetic, it was enough to work with finite sets. But if you go to finite type arithmetic, finite sets are not sufficient. not sufficient you have to accumulate not uh are not sufficient for for to interpret bound collection principles you you have to go from accumulation in in finite sets to accumulation below a bound and there is a systematic way of doing this it's so-called the bounded function interpretations and within the framework Within the framework with an abstract type for a Hilbert space, bounded collection can be taken, can be stated in this way, where B is bounded and closed, let's say. And if you read this, it just says that if B is containing this union, then it's containing infinitely many. The sigma ends have to be open sets, actually, logically open. I mean, this has to be. Actually, logically open. I mean, this has to be a sigma one formula. A sigma formula, sorry. A sigma one formula. Actually, sigma formula is enough. This has to be a sigma formula. If it is not a sigma formula, you get into trouble. This was observed first by Ulrich Kohenbach, and he has a recent paper on this. So, and of course, this is a Heiner-Boreal principle. Oh, it is false in infinite-dimensional Hilbert spaces, but nevertheless, it's present and the collection piece can be false. However, if the Bayes theory is universal, followed by bounded, they only prove for all the exist sentences. This is a consequence of a general conservativity result. Friedman's result is just a very particular case of this. So the difference is that in Friedman's So, the difference is that in Friedman's result, you get, if you prove something with Wikoenig's lemma, you can prove without it. And we Koenig's lemma is set theoretically true. In here, if you can prove something with, let's say, this Einéborell principle, something, I mean, for all, from for all exists, if you prove with this Einaborell principle, you can also prove without the Ainéborell principle. But the difference is that the Annaboral principle is false. But, okay, so but the gist of the result is the same. By the way, this bounded collection principles in this particular setting are so-called uniform bounded principles introduced by Columbach before the boundaries function interpretation. In Browder's strong convergence theorem and sequential weak compactness theorem, which is used in the proof, can be replaced by Einer-Borough compactness theorem. And the mining of the Einero argument is trivial because it's a bundle collection, it trivializes. So actually, this part of the argument, which maybe it's the most abstract part of the argument given by Brother, is trivial. Other parts are trivial. Trivial other parts are not, but other parts are not, but but this part which is the most abstract actually trivializes. So, um, I have one more example, but uh I don't think I'm going through it in very detail. Um, I'm just uh well, just say that suppose you this is about new policy and the use of Zorn's lemma. Of Zorn's lemma, an element of the ring is called nilpotent if there is a n such that e of n equals to zero, and an element is nilpotent only if it lies in every prime ideal of r of course the left-to-right direction is obvious. The right-to-left direction uses Zohn's lemma. Basically, if he is not new potent, you will just... if e is not new potent you just consider e square e e cube baba ba it's never zero so you consider all ideals that do not intersect this e to the ends there is at least one the zero ideal you take a maximal among them and it's actually a prime ideal so you use zorn's lemma to use to to to get the maximum among them for instance you have a a very concrete application of this a very concrete application of this very well known if you have two polynomials and their product is one then these coefficients must be new potent and there is a very nice abstract proof of this because by the cruel theorem it's enough to show that each ai is having is in every prime ideal of r of course the equation Of course, the equation also holds in the quotient ring are of x, and this is an integral domain. So we may conclude that they have degree zero, and this means that the AIs are in x. So for every prime ideal, the AIs are in x. So the A's are newpotent. As Fred Richman, a famous constructivist, said, I mean, this proof does not give a clue about the index of new potential. About the index of new potency of the AIs. I mean, you know that AI to the k is equal to zero, but can you compute k? This gives no clue. It's a very abstract proof. Of course, there are known ways to compute this. My last slides, I'm not going to, one cannot analyze Kruhl's theorem. cannot analyze cruel theorem, one can analyze something which is weaker than cruel theorem using so-called bounded collection. But that's enough to, for instance, to get this application and to extract bounds for the indexes of new potency. Thank you. Thank you.