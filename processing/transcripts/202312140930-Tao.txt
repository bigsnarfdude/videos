So today I'm going to talk about this homogeneity of pursuit in ranking inferences based on pairwise comparison data. This is a joint work with Tracy at Harvard University. So in this work, we mainly deal with the ranking problems. We know that ranking problems is everywhere in our daily life. For example, in the sports and gaming, we would like to rank the teams based on Good teams based on the pairwise comparison results among them. And another example may include a recommendation system. For example, when watching the Netflix, the website may recommend some movies to us based on our personal interests. And this is based on the ranking procedures. And other examples includes the journal ranking, university ranking, etc. So I will start with a motivated example. Start with a motivated example. Here we consider the NBA basketball team ranking. This is for the 2022-23 regular season data. So we observed the 30 teams, the results of 30 teams. And the data is a win and loss results. So it's a binary data. And for each pair of teams, we observe two to four comparisons between each pair. And famous model for the ranking problems is the Bradley-Tavalius model. If you remember the professor Jantin's talk at the first, so the model assumes that each team has the latest score, theta i, and the model assumes that the results of comparison between each pair depend solely on the differences of their score. So the probability can be written as a sigmoid function of the differences of the beta. Of the differences of the beta. So we use basically use the MLE to estimate theta, and then based on theta, we can rank the teams according from the highest to the lowest. So although this relatory model has been studied very well in recent years, there are still some challenges for this model. The first is that there may be some potential overfitting due to the insufficient comparisons. Insufficient comparisons. Because based on the literature, we know that the estimation error for each estimated theta is proportional to 1 over square root of n times L. And this error can be quite large when L or N is not very large, it's small. For example, in our NBA data set, we have N equals 30 and L equals around 3. So the estimated genera for each individual is about 0.1. Individuals about 0.1. But we can see from the estimation that some estimate scores, their differences are much smaller than 0.1. So it's intuitive to think that we might not trust the ranks based on these estimated parameters if they're very close. So their differences may be due to the estimation error instead of the real ability in each item. Items. And another thing is that sometimes we are more interested in dividing these teams into groups, such that there is within each group, there is no significant differences, and between groups, there are significant differences. Because in reality, sometimes we would like to know the first tier or second tier of the teams instead of the explicit ranks. Such, for example, in the university ranking, there are a lot. University ranking: there are lots of universities, and sometimes we are more interested in the tiers instead of the exact rank. And also, such group structure can sometimes preserve the privacy of the data and make the ranking more reasonable and fair. So, for today's talk, we propose a model which is an extension of ETL model with group structures, and we also provide a method that can We also provide a method that can simultaneously do the estimation parameter estimation and the group clustering. And also, for theory, we can achieve a faster conversion rate than the previous literature. And I would also like to mention that for this kind of problem setting, and actually in the literature, there are many works that focus on constructing confidence intervals. Constructing the confidence intervals for the individual theta i. So, this is a such confidence interval is also a way to a way for the uncertainty quantification. And we may think of an ad hoc approach that is we can first construct confidence intervals and group i and j if their confidence intervals overlap and otherwise do not. So, this is a very straightforward method. But such methods But such methods sometimes cannot work in practice. Because, for example, in the figure on the right, we can see that this is a ranking of the cargo ships. We find that all the confidence intervals, they overlap with each other. So if we use this method, we can only get one group based on these confidence intervals, which may give us no information. And sometimes, and in practice, in this method, we also need to connect. To we also need to compare each pair of items, so we need to like do the square of times of comparisons between each between each items. And we sometimes also need to do like the Barfronic correction or other adjustments in order to do this method. So, this straightforward method does not cannot work very well in practice, and I will introduce my arm. Will introduce my R modeling method in detail later on. So let's first take a close look at the BTR model. As I said, suppose there are n items and each item is assigned with a latent preference score theta i star. So the model assumes that the probability of i douche j is proportional to the exponential theta i star and otherwise the same. star and otherwise the same. So we can write this probability as a sigmoid function of the differences of theta i star and theta j star. And we also assume that for each pair, each observed pair i and j, there are all independent comparisons for them. And in our model, we assume that all individuals can be divided into k groups, which is a partition of one to n, and we can write it as the 1 to n and we can write it as the C h g k star. So here we have k non parameters. We also assume that in the theoretical for the simplicity in theoretical analysis, we assume that each group has the same number of individuals. But in applications, actually, we can allow for different numbers in each group. And we can also see that when k equals 2, it will reduce to the standard BTL model. Standard DTL model. And our goal here is to conduct the estimation and inference of the present scores from the pairwise comparison data. And I will briefly introduce here the first assumption, which is normally assumed is that the maximum the difference of maximum minimum value of this four, we call it a dynamic branch. This kappa is a big O1. This curve is a big O1. And we also assume that the sum of all the scores equals zero, just for identifiability. Because we know that if we add the same number to all the scores, the model will not change. So we need some identifiability condition. So let's first consider an an oracle case where we know the exact group partition. The exact group partition G. So, if we know that which individual belongs to which group, then we can write the explicit log likelihood function as listed here. Then, by minimizing this negative log likelihood, we can get this Oracle MLE. And we have got this result with this Oracle MLE that we have proved that. That we have proved that if the above assumption holds, then the convergence the convergence rate for this MLE is square root of k plus log n over L times N. Actually, we can compare this result with the previous result that is for the standard ETL model, zero convergence rate is square root one over L. So if we assume that the K, the number of group K, is small n, then we can see Is small n, then we can see that our conversion rate is faster than the previous conversion rate. So an intuitive explanation here is that if we assume the group structure in all the items, then that is we need to estimate the k parameters. So for each score, for each parameter at k, theta i, we have more information because we can use the whole information in this group instead. Whole information in this group instead of the one individual. So, based on this more information, we can get this faster convergence rate here. And however, in the reality, we do not know the group partition G, so we need some method to estimate it. And we propose here the panelized MLE method. That is, we add some voted compute penalty to the likelihood function and do this panelized MLE. So, here's an illustration. So here's an illustration that we can use, for example, this GAD penalty or the MCP penalty. And this figure shows the differences of these penalties with the PASO penalty. And we can see the main differences is that for after a constant A, its penalty will become a constant, which is different than the vessel. And for this penalty, For this penalty, we may first think of two possible ways for the penalty. The first is the fuse penalty with a pilot estimator. Here, the pilot estimator means that first we estimated the scores to get a preliminary estimator, zeta pre. Then, based on this, we can construct a rank statistics call that can rank the zeta pre from the lowest to highest. And then we make And then we make use of this linker dragon mapping poll and put it in this fuse penalty here. So from in this likelihood and penalty, we can say that it only shrinks the adjacent coefficients in the order of pause. So there are only n minus one penalty here. And when the lambda tuning framed the lambda When the tuning frame lambda is large enough, we will get a piecewise constant results for the theta. So, in this way, we can get a homogeneity structure in the theta. And another way is the famous total variation penalty that it uses no prior information. It just penalizes over all pairs of items. So, you can see that there are about You can see that there are about n square number of penalties. So, there's the issue with this total variation penalties that it contains too much penalties and it may result in a much larger estimation error. And also, it may be very computationally expensive. And the problem with the first use panel is that for this S data hat to be consistent, we need this. Be consistent, we need this assumption that we need to assume that Paul is consistent with the order of the true true score with high probability. However, we can see that this assumption is quite stringent and it may not hold in practice. So, an intuitive way is to use less information from this poll rank this ranking statistics and more acc more in the likelihood. Penalty terms in the likelihood. So, here we propose a method that's called the CARS penalty. This is enlightened at this paper pure Fan and Wu in 2015. So, I will explain what the Cars penalty is. So, here first, we have this delta, which is a predetermined parameter, and use this delta, we find all indices I2 to RL, such that I2 to RL, such that the gaps between the adjacent coefficients of beta pre, their differences, their gaps are larger than L. Then, based on these indices, we can construct the segments, which is B1 to BL. So this large L, the number of segments, depends on the number of delta. And we call such mapping the older segment. mapping the older segmentation loop syllable and based on this loop syllable we can construct this hybrid pairwise penalty so this penalty consists of two parts the first is the within segment is the between segment penalty and the second is the within segment penalty we can see that for the second part it's panelized over all pairs and within each segment and for the first part it panelized For the first part, it penalizes the pairs that between the adjacent segments. So we can see that this penalty takes advantage of the order of the segments, but at the same time, it can allow some flexibility of order shuffling within each segment. So this can give us more freedom and flexibility in the penalty. And when the L equals 1, it will reduce the used penalty. And when L equals 1, and when L equals when L equals L equals 10, it is abuse penalty, and when L equals 1, it will reduce it to the total variation penalty. So that explains the word hybrid here. So it's kind of like a generation of these two penalties. So let's see how assumption two can be relaxed. Here I'll use a figure to illustrate. So let's say the that's that dog. Say the red dot represents the true group one, and the plus single represents the true group two. So there are a total of two groups, and the y-axis is the preliminary estimators of the scores. So we can see that for the preliminary estimator, the order is not consistent with the true ranking. For example, like the number 17 and number 18, their ranking is not. is not consistent. So the assumption two may not hold here. But if we apply the, if we use the order segmentation, that is, we segment it into a total of 10 segments from B1 to B10, then we can see that each segment is strictly larger than the previous segment. But within the segment, we can allow flexibility. So we do not require We do not require the consistent ranking within each segment. So, this is an illustration of why this card penalty can relax the assumption of the fuse penalty. So let's formalize our procedure here. So, first, we get this preliminary estimator, theta pre, and then based on the tuning parameter delta, we construct an order segmentation as. construct an order segmentation as described before. And then we use this hybrid pairwise penalty together with the likelihood function to get the estimator into the hat. And I would also like to mention that when delta equals zero, it will reduce the field's penalty, which is a special case of cost penalty. Of course, kind of thing. So, for the theory, I will briefly show the theory. Sorry, I forgot to mention for the computation part, I choose the front-tuning parameter based on the cross-validation method. And since this kind of is concrete and the likelihood is not convex, I use some techniques such as the projected. Techniques such as the projected gradient sense. And I also approximate this likelihood using some linear approximation to do the computation for this minimization of this likelihood. And in the theoretical part, we develop the properties of this card penalty. That is, under some regularity conditions, if the half-minimum gap between groups. If the half-minimum gap technique group satisfy this condition, and the tuning parameter also satisfies this equation 6 and 7, then with high probability, we can derive that this card's objective function has a strictly local minimizing data height. And this data height is actually equals to the oracle MLE of this algorithm letters. So we can see that this data hat has the had us has the exactly the same convergence rate as the original MLE, which is a square root of k plus log n over Ln. So in this way you can obtain a faster convergence rate. And we also studied the special case which is the field's penalty and similarly it can get derived that the regularity conditions differs a little bit but our conclusions is The conclusions is the same, and we can get exactly the same congress rate as the rock, as the ROPLA. So lastly, let's see some real examples. This is the examples that I show at the beginning. This is the NBA Festival team ranking. And so at the end equals 30, and each playing team, each pair of teams play at least. Each pair of teams play at least two games and at most four games. So, this is a summarize of the NBA regular season results, just based on the win and loss results. We can see that, for example, the Milwaukee Bucks, Boston, Celtics, and Denver methods, etc. These teams may rank the highest because they have won the most games. So this is the preliminary TMAP ranking based on the estimated score. We can see that the differences between each, between the adjacent estimated scores are quite big. So we may assume that there's explain some group structures in this set. So we applied our cards to estimate the group structure and here's the results. We can see that We can see that we group all the 30 teams into the paid groups, and also the Milwaukee, Boston Celtics, etc., they are indeed ranked highest. And also, an interesting thing to notice is that for the NBA basketball regular season, we normally will choose 20 games to enter the playoff season. playoff season. So this 20 games is actually just the first four groups in this group structure. So it may give us some conclusion that our group structure may have some real insight in applications. And we also do some prediction, calculate some prediction error. We divide the whole The whole data set into the 60% or the 80% training data and use the rest for the testing data to calculate the prediction error. So the figure on the left is the 60% and inside the first box plot is the prediction error of our method. And the second box plot is the prediction error of the standard ETL model. So we can see that. Our model. So we can see that our model can indeed reduce the prediction error. Especially when we use more training data, the differences in the prediction value of the month increases. And another very interesting example is the journal ranking. It uses the this math stats data set, which is collected by Tracy and Professor Zee's group. And Professor Zin's group. And here we got the citation data. That is, we got which paper in which journal cites the papers in other journals. And we intuitively, we may think that the papers in one journal tend to cite those papers from journal with a higher prestige. So we may get a rough ranking of the journals based on this citation. Ranking of the journals based on this citation network. And here we consider the 33 statistics journals and we exclude the probability journals which the citation between probability and statistics may be very low. And here the data is approximately from the 2004 to 2014 is about 10 years of the total citations. So this is a preliminary So this is a preliminary, sorry the title is wrong, preliminary journal ranking for these 33 journals. You can see that the first, the top four journals indeed rank the highest in this ranking. And among it, there are some differences that are quite small. So there may exist some group structures. And then we apply our method and we Our method, and we clustered all the journals into 11 groups. And here are the results of this journal ranking. Yes, and there may be some insights from this ranking, and I will not go into details about it, but you can see that the AOS and JRSB is Is that scroll in the first group and barometrica and JASA is the second group. But actually, the data may change nowadays because we use the 2014-2015 data. So the ranking now may change a little bit. And so the last real data is a very good idea. Real data. I mean, I don't have time to go into details, but here we rank the Netflix film ranking. We use this data set and we extract 100 movies with the highest number of reviews. And here listed the first 15 movies right here. And similarly, we divide the movies into the different groups. And there are a total of 19 groups in each NBCD ranking. Reach in BSCB rankings. And we can see that the movies are listed here: the finding remote, the sixth dance, load of brains, etc., which is indeed quite famous. So, so, yeah, so you can see that this, our group structure may have some interesting findings for the real application. And to summarize, in this work, we explore the homogeneity of the. Where we explore the homogeneity of scores in the BTL models. We assume that there is a group structure in the preference scores. And we introduced the TARS penalty to estimate scores and the group structures at the same time, which have these several advantages. And we also study the statistical properties of TARS. And finally, we use some data sets, including squads, journal ranking, movie ranking, to demonstrate the Demonstrate the efficiency of our model, and we can see that the prediction ability may be improved by our method and models. And sometimes we have more insights in the real data of applications. And it's still an ongoing project, and I'm still working on the ranking inferences, which may give us a sharper confidence in the estimated case scores. Case scores and ranks, which I will include later in this page. So that's all. Thank you very much for listening.