Thank you, David. Hello, everyone. It's really a great pleasure for me to give this talk today, even if it is at a long distance. I would like to thank the organizer for this invitation to speak about, as David said, about model-assisted estimation through random forest in fine population sampling. It is a Sampling. It is a recent work done in collaboration with Medi Dagdoug from the University Bourbon-Franche-Ponte also and David. And in fact, it is a part of the PhD thesis of MEDI, PhD that will be defended very soon at the beginning of July. So I will start just sorry. For it, I'll start by giving you what has motivated this study, and I will present you then this new model-assisted estimator and give you some finite population sampling and asymptotic properties, as well as some true variance estimator suggested in our paper. I will finish with a small simulation study done on real Then on real data and a short conclusion on further work. So nowadays it is not unusual to have a very, very large data set and this is due in part to the existence of smart metals, smart phones, which are capable to record and to send the information at a very fine scale, as for example, Scale as, for example, every minute or every second. National statistical offices have also access to a variety of data sources and they can have on the sample data, they can have a very large data set. In this condition, traditional parametric or non-parametric estimation method may prove inefficient, and we have suggested in our paper a new class of model. Uh, a new class of model-assisted estimator based on random forest algorithm. So, I will start as usual by defining you by giving you some notation. We consider the finite population denoted by you and from which, sorry, from which we select sorry, Camilla, did you move the slides? We didn't see the slides moving. I have just moved the I have just moved. I have just moved my computer. Sorry, the slide is now on the first page. It is on the fifth. You don't. No, I see the slide moving. You don't. It is not moved. Sorry, do you see the where I No, I think it's on the first page. No, I'm on the fifth. Maybe you can reshare. If it is not shared anymore, you can stop sharing and reshare. It is not working. All right, Lon, what with this slide? That's good. Now we see the slide seven. Yes, and now do you see? That is good. That's good. Okay, sorry. So you in fact, it was only talking. It is not. Do you need to go back or it's okay if I continue from here? I think this is good. This is the internet. Okay, sorry. So we consider the population U and we select the sample as usual from U. We select the sample as usual from U according to a sampling design with first order and second order inclusion probabilities denoted by pi k and by pi k l and the goal is to estimate the total of survey variable variable y the total T y denoted here as usual so with full response we have the we can estimate this total by using the unbiased or responson estimator But if we have auxiliary information denoted by x1 up to xp, we can improve the Horvies-Thompson estimator by considering, for example, the model-assisted approach. In this approach, the relationship between the Y variable and X1s is modelized in this form. In this form, where here the regression function is considered for the moment in a general form, it has an unknown but smooth expression, and the residuals are supposed to be independent and zero-mean variables. The model-assisted estimator is given this expression, as everybody knows. It is the sum of two terms. The first one, it is the finite population totals of the estimated prediction hat M. And the second one is the Horvies-Thompson estimator for the estimated residuals YK minus hat M computed in XK. Here, hat M is some design-based estimator of M, of the unknown M. So several estimator have been suggested over the 30 or 40 years, starting with the well-known Greg estimator, the class. Greg estimator, the class of Greg estimators obtained when the regression function m has a linear form in beta. This estimator works well if the relationship between the y variable and the x variables is close to a linear one, and also if the number p of auxiliary variables is small compared to the sample size. If the second, if the last If the last condition here is not fulfilled, then the penalized model-assisted estimator, such as RIGH panelized regression or LASU, and even more, the model-assisted estimator based on dimensional reduction by principal component analysis should be used instead. To escape from the linear relationship, we can use non-parametric models and in And in this case, M remains unspecified but smooth function. And from the seminal paper of Preton Opsimar in 2000, many non-parametrics models have been suggested for the univariate case as well as for the multivariate case. In the second situation, however, the number P of auxiliary variables should be relatively small. As I said at the beginning, As I said at the beginning, nowadays we face an emergence of statistical data sets, but also we face emergence of statistical methods called largely as machine learning methods. And all this has been born to respond, to meet the increasingly need to modalize situations. Situations that are more and more difficult. If we look at over the last year such the statistical methods, we can class them into, for me, in my opinion, into three large classes. The first one are based on begging, what is called begging methods, which consist in Which consists in producing a very large number, a very large B number of predictions and combine them to produce a final prediction given here that it will be better than a single model can do. The second class is the class of methods based on boosting. In this situation, the method and the method is different. We start with a weak feet or a large number, which Weak feet or a larger, which is most of the time with a very few terminal leaps. And we improve it at each step of the algorithm by considering a model built on the residuals obtained from the previous one. And finally, we have the deep learning methods, which are rather different from the philosophy. The philosophy described before, but these methods are requiring extremely large data sets of unusual unstructured data such as image or a sound. So I will the random forest estimator that I will talk about belongs to the first class. It is a backed estimator, which is Which is consists, in fact, in producing a B, as I said, a large number of estimation obtained now by means of trees. To obtain different trees, a randomization mechanism is introduced, and Breman suggested in 2001 to bootstrap the individuals from the original sample. Original sample and also at each split from the tree to consider only a part of the original auxiliary variable. In this way, he obtains rather different trees, uncorrelated, and the final prediction by random forest will be very efficient. Many random forest algorithms have been suggested. random forest algorithms have been suggested since 2001. They become very popular over the last 20 years, especially due to their predictive performances and a better ability to handle large data set. However, the theoretical properties have been proved only recently in case of particular random forest algorithm. So I will just show you quickly how it works, the requisition. How it works the regression, the production of M by using regression trees. And I will show you, I will give you it directly in case of a survey sampling setting, as it was suggested by Meckenville and TOS in 2019. We start with the sample data, XK and YK, and we split the regressor space into disjointed region or nodes. region or nodes A, A1 up to AJ. And to do this, several criteria exist. The cart one is very popular and we stop when some stopping criteria is fulfilled. Suppose now that the region have been determined, then the prediction of M in some X points, X points that will belong to the That will belong to the nodes, for example, Aj, then will be equal to the weighted mean of y values computed for individual L that will belong to the same nodes as the point X. So it is in a post-ratified estimator, but with post-strata that have been built on the sample data. Then just three. Then, just sorry, just a small example of regression tree. So, if we have 30 units and two actually variables, x1 and x2, so we want to split the plan, the into disjointed regions such that into each region where we must have at least four individuals and no more. individuals and no more than seven seven elements. We start with X1. We split in two regions such that the obtained region will be as homogeneous as possible with respect to the Y variable. Then we continue. These two large regions will be split at their turn into two another one by using now X2. Now, X2 again to obtain the region as homogeneous as possible and will continue until the region have the desired number of elements. We have here the corresponding tree. And if we want to do the prediction, if one units that belong to this final leaf, then we'll do the weighted mean. I'll do the weighted mean of all units that belong to this leaf. Now we'll do a large number of trees to obtain a forest. And in order to be different in this tree, as I said before, we introduce some stochastic procedure. In order to obtain for each tree, Three, the disjointed region. The algorithm that we have considered in our paper is a little bit different from the one suggested by Rayman in the sense that we select without replacement our individuals from the original samples. And next, at each split from the tree, we consider, as before, only a pair of predictors which are selected without replacing. Which are selected without replacement from the initial p variables. And then we compute the prediction by using this tree and we average them to obtain the final prediction by means of random forest. So once that we have obtained this prediction of M, we can plug in into the model assisted estimator. Assisted estimator. The operation is almost finished. And I will give you now some properties of this new estimator, some properties that have been inherited from the random forest estimator. For example, it is also a backed estimator, namely it can be obtained by averaging now B model assisted estimator. Model-assisted estimator obtained by means of origin trees. We can also write it as a weighted sum of y values with weights here, wks, which are depending on x on the sample as usual, but are depending now in an implicit way on the y variable, since the y variable was the one that has been used to obtain homogeneous. To obtain homogeneous split region in the cart criterion, for example. If we want to, in fact, this wide dependency of the weights has a lot of consequences. And even in the statistical literatures, sometimes there are algorithms that are not, that gives weights not depending on why, they are sometimes used. Sometimes used. In our situation, we have suggested, for example, to build our random forest algorithm at the population level by using some variable y star, which is known on the whole population, and correlate it with y. If we don't have this and we want to estimate at the same time several population totals, then we can use the model calibration technique to obtain Calibration technique to obtain weight that can be used to estimate, as I said, several TY. Finally, this weight satisfies some calibration, some calibration equation on the population size. And the last finite sample properties, it's something it's a new properties with respect to the other non-parametric estimators. Estimators and it is related to what is called in the machine learning literature the out-of-bag elements. More exactly, recall that the first step of our algorithm, we have considered only a sub-sample of the original sample. Only n prime individuals participate to construct the prediction hat n. So there is some is some individuals there are some individuals that do not participate for this hat m and there are even more than that some elements do never they never participate at the at the prediction of m for these elements the their weights are always the orvest thompson weights and we can show that the model assistor and the forest estimator can be written in this Forest estimator can be written in this form, in this equivalent form. The first term is unchanged as before. The second term, in fact, it is an average of Horvie-Thompson estimator, but computing now for the residual of YK, but computed for the elements that are out of back. So the elements, so this term, this section. So this term, this second term, it can be viewed as a correction term which protects from overfitting. And it is specific to this kind of estimator, to random forest estimator. In some situations, this second term may be zero, such as for example, if we consider the whole original sample, and in this case, the estimator has the projection form. Matter has the projection form, but the overfitting cannot be avoided in this situation. So I will continue now with the asymptotic properties and the variance estimator. We consider as usual the asymptotic framework of ESAC if you are, which allows us to consider sample and population sizes going to infinity. And we have some. And we have some classical have become classical now assumption about the y variable, the sampling fraction, the pi k and pi k L. And we consider some supplementary condition, the fifth here, which is some kind of extension from the condition considered by McEnville and TOS in case for regression tweets. For regression trees. And in this situation, in this asymptotic framework, we can show that the sampling error can be bound in this way. Here, in zero, n zero is the minimum number of units that we allow to have in the terminal LIFA. So if this number is also going. If this number is also going to infinity, then our estimator is asymptotically unbiased and consistent. And if we have this particular relationship between in 0, n 0 and n, then we have the classical n minus half rate consistency. rate consistency. In order to obtain the asymptotic variance, we need supplementary assumption here, the sixth one, which is verified in our case, in our simulation we have found that it is working. It is in a case the general the Condition of in a survey sampling setting, the same as Bu and Scornet have considered in their classical affinity population. And it means that asymptotically, the random forest partition obtained in the sample are converging to the random forest partition obtained built at the population level. At the population level. More research is needed to obtain and to prove this condition, which we consider as an assumption for the moment. In this setting, we can obtain then that the random forest estimator is asymptotically equivalent to the difference estimator. And as usual, we can derive the variance estimator. I will give you now some simulation, the result of a part of our simulation study. Simulation was realized on a real data, Irish, and in fact extracted from the Irish Commission for Energy Regulation, which can be obtained on request. From this data, we have considered a population. data have considered a population of more than 600 6 000 irish households and companies for which the electricity consumption was recorded every half an hour for 30 days for 14 days sorry so we have more than 600 covariates which are highly correlated considering this data we have generated Data: We have generated two survey variables that I will give you a little bit later, and the goal was to estimate their total. We have compared several strategies by using by selecting 2500 iterations of consider a sampling of 600 and It and five model-assisted estimators plus the Holvis-Thompson one. In the paper, we have considered a lot of estimators, but we have considered here only five. The first one is the GOAC estimator, the classical estimator based on a linear relationship. The second one is based on the tweet. Based the tree, the Grecian tree algorithm, based on cart algorithm. The third one is the random forest estimator obtained with 1,000 trees. A minimal number of elements in each terminal node equal to 5. And we consider the effect one of the four choices for the number of selected variables. Number of selected variables at each split, which is square p. The fourth algorithm is the XJ boost, which is one of the very performant algorithm based on boosting. And the last one is the nearest neighbor predictor with five neighbors. The general The generated y variables are the following. The first one is linear in x1, x2, and x3. And the second one is a non-linear relationship with respect to the same three x variable. To compare all these estimators, we have considered as usual Monte Carlo measures. We have computed the relative bias and the relative efficiency with respect to. Relative efficiency with respect to the orbit-tonson estimator. We have also considered the we investigated also the impact of the high dimension of the performance of our estimator. And in order to do that, we have introduced more and more correlated noise variables. The most was at the end, 400 variables. So we wanted to see. So we wanted to see how is the, especially the random flow rate estimator, how it is robust to this scenario setting. The sampling designs were the first one was simple random sampling without replacement and the second one was a stratified sampling with four strata built on the contile of X1. Of X1, and we have choosed to use the X2 optimal allegation. We have here the inclusion probabilities in the fourth data and with the corresponding weights. We are in effect in an informative setting. The correlation between the survey variable Y1 and the inclusion probabilities being about 0. Being about 0.4. So, we wanted to test the performance of the, especially of the random forest estimator in this setting, because most of computer software, in our computer software used to implement the random forest, are not Are not built in order to consider sampling weights, and it is not so immediate to modify them in order to be able to put sampling weights in the estimation of M. So we wanted to see if we can use the original random forest estimator with this informative. This informative design, and if it is not adapted, what should we do? Here, the first plot gives us the relative efficiency on the y lab for the first, for estimating the total of y1, which was linear in the x in the 3x co-variable. covariable and on the x x here on the horizontal we have considered an increasing number of noise variables we started with zero so we don't have noise variables then five then up to 400. the the estimator here the the estimator in blue the blue one which is the best Which is the best up to 200 noise variable. It is, in fact, the linear regression Greg estimator, which is the best as we expected because it is well adapted to this situation. However, when the number of auxiliary variables is growing, then its efficiency is starting to To in fact is less efficient and it is here outpassed by the random forest in green curve or the ex-Jabousta. Even for this linear relationship, in fact, these two estimators are rather stable when the number of auxiliary information is getting. Information is getting greater and greater. The estimator that is the worst in this situation, it is the one based on nearest neighbor estimation, as it is expected, because it is not very adapted when the number of auxiliary variations is large. For the second survey variable, Y2, Y2. Here we have a non-linear relationship. So we don't expect that the classical Greg estimator is working well and we can see that its efficiency is very bad. And in fact, the relative efficiency is explosive for more than 100 variables. The one that it is relative. It is relatively stable and is conducting well, is the one based on random forest estimator here. And lastly, the first two plots were obtained for simple random sampling without retracement. And consider now for the stratified sampling, which I recall you that it was an informative. It was an informative sampling in this situation. So we had, in fact, it was a surprise, yes or no, because as I said before, the algorithm do not take into account the sampling weights. So the random forest estimator, again in green here, we can see that when the number of noise variables is starting to be large, then Starting to be large, then its efficiency is very bad. While the other one, XJ boost, is, for example, is stable. So we wanted, we looked closer in order to find the answer, what makes it happen. And we have computed separately the square biased here in blue and the blue and the the variance in orange in the orange curve to compute to derive the mean square error here in green so we can see that for a small number of noise variables we have a low bias and relatively larger variance but when this number is getting larger the situation Getting larger, the situation is reversed. The biased, the square biased, will be larger, and the variance while the variance is remaining the same. So, what happens if our explanation is that the process that selects without randomly square p variables at each split, when we have a large number of When we have a large number of variables, it may happen that the variable variable design variables that are very important for our estimation may not be selected by the random process of selecting the variables. So these important variables do not contribute the estimation of M of the large M, giving a large variance like in this situation. Like in this situation. To cope with this situation, we have suggested to force the algorithm to consider these important variants at each split. This can be done, for example, with ranger computer, our computer, if this option exists. Or we can consider all the variables, but in this case, it is not a random forest and a real. A random forest and a real random forest algorithm. We have so we have conducted this new simulation study and we can see here in green, the green curves, that when we force that the design variables are considered at each split, then the bias is greatly reduced. So we have almost no bias. So we have almost no bias in this situation. The situation is a little bit worse when in blue here, when we consider in fact all the variables. Again, only five individuals by terminal notes. And the worst situation are the orange one when we leave the algorithms to choose what the The variables and the design variables in this situation may not be chosen. The red one is the one obtained with a larger number of individuals in the terminal node. But we can see that it is not enough to diminish the bias. So the The conclusion is we have done proportional to size, also proportional to size sampling design. That in survey sampling setting, this kind of machine learning algorithm should be employed carefully and especially in this situation because they can give some unexpected Unexpected results. The second part of this simulation study concerns the variance estimation. For this part, we have considered only simulated population with several auxiliary variables simulated also, and this survey variable Y considered in this non-linear expression. The goal now was to build The goal now was to build a 95 confidence interval for the total of this y variable. So we consider on the horizontal, we consider the Horris Terms of variance estimator as presented at the beginning for the estimated residual when we consider different number of units by terminal nodes. So we start with a smaller We start with a smaller quantity up to a larger one, and we can remark that for when we have a small number of units by terminal nodes, the confidence, the coverage rates are rather far away from the desired one. And when this is Since N0 is growing, the coverage rates are getting better, but we need to have a really large number of units by terminal nodes to have the 95%. So it is not very in fact, we try to understand what happens here, why we don't have. We don't have the desired rates. And the problem here is that it is a more general problem. It comes from the fact that this Orvis-Thompson various estimator with non-parametric models, especially non-parametric models, may do a large underestimation of the residuals. Take a small example. If we consider a random forest album. If we consider a random forest algorithm with only one unit by terminal node, then the prediction in xk in this k point will be exactly the real values y. So in this situation, the estimator will be equal to zero, while the real, the asymptotic variance is not equal to zero. So we have two underestimated the variance. In a larger context, a non-parametric context, Obsomer and Miller have noted a similar problem and they suggested a cross-validation method to select the bandwidth in their situation. And inspiring from this construction, we have suggested a new A new variance estimator based on K-forward cost validation. Sorry to interrupt, we have four minutes. Okay, thank you. I just, it's my last slide. So for people that are familiar with the cost validation, the K-fold, in fact, we split our sample in K-fold. We leave our We leave out one fold and we construct the feed the model on the other k minus one fold, and then we predict the residuals for the individuals from the kth fold. Starting from these residuals, in fact, these are the new residuals. We obtain this new variance estimator, which conducts very well in this situation now. We have Situation now, we have whatever the sample, the number of units in the terminal nodes are, the variance estimator gives us the desired or almost the desired convergence coverage height. So just here, I will stop, in fact, and I suppose that my time is finished. And just to tell you that at this moment. Just tell you that this method has been also tested for the imputation for IQ null response, which is also part of MEDIS thesis. And this new variance estimation is at the moment under further investigation. So thank you very much for your attention.