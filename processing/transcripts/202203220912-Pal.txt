So what the idea behind this project is to study large-scale optimization problems that have a different kind of commutation symmetry than we are used to. So what is the usual symmetry? The usual symmetry is exchangeability between particles. So here is a typical problem. So we want to take limits of large-scale optimization problems. So consider particle symmetry. So consider minimizing the falling interaction energy. Falling interaction energy vn of x, which is the average of the squared error between the positions of n particles. Now, how would we do this? How would we try to optimize this? Well, one thing is that, okay, you just do gradient flows. This is a Euclidean gradient flow. If you wish, you can start your initial particle configuration as IID from some density rho naught. You take the gradient of Vn, this is the potential, and then you flow your particles according to this potential. According to this potential, the thing to note here is that there is a limit to this as n goes to infinity, and this comes from symmetry. What is the symmetry? The symmetry is that the function vn is permutation symmetric. If you permute the coordinates x1 to xn, in any you change the labels, this remains the same function. And that essentially means that the function vn is actually a function of the empirical distribution of the process. Empirical distribution of the process, empirical distributions of this particles. And so, if you have a function on Euclidean spaces that is invariant on the permutations of its inputs, then it can be extended to a function on its empirical measure and perhaps by lifting it suitably to a function on the space of all probability measures. And that's the idea. So, for example, for the very simple function vn, we know what is the function on probability measures is simply the variance. And for both, there is And for both, there is terrible. Okay, so there is variance is the function which is on the all measures. And then I can use Washisting gradient flows, which means simply doing the gradient flow on the space of probability measures. And this would be the limit as n goes to infinity. One may also add a noise term, and this is very frequently done. You add a square root of beta times independent Brownian motion. And then this Sd or the rather. And then this SD, or rather the law of this Sd, it captures another great influence on the Vashes chain space. And as the audience here knows, this is the variance plus beta times the entropy, which in statistics is called the entropy regularized optimal transform. So gradient plus beta. Now, this is all well known. The purpose of this talk is to introduce a different class of problems, which comes with a different class of symmetries. Different class of symmetries. And the symmetry comes because the problem is not over particles but over dense, unlabeled, weighted graphs. So here is a very particularly canonical example. So let G be a finite simple graph with n vertices. And what you do is that you look into the number of triangles in this graph. And you take the density, which means you divide it by n cubed to normalize it. Well, this can be. Well, this can be also thought as a function on the adjacency matrix of this graph because you have an adjacency matrix A and for every choice of three vertices in the graph, you look into the product of the adjacency matrices. So you look into its either one or zero, if it is just a simple graph, whether the triangle exists or the triangle does not exist, and you just sum them over all such possible choices. Actually, the same definition in terms of this adjacency matrix. In terms of this adjacency matrix, also works if the graph is weighted. And here, by weighted, I mean that the adjacency matrix is any real symmetric matrix. For every edge of the graph, you attach a real number, weight, and then you do the same sum, a same formula, extend z, and that's the edge density formula. Now, this number, as you can easily imagine, the number of triangles has a symmetry of a graph, which is the following. Okay, sorry? When you do this weighted graph thing, you're not. When you do this weighted graph thing, you're not counting the triangles anymore. You're weighing the triangles with the edge weights, that's correct. It's the product of the edge weights. It's a product of the edge width, that's correct. And why does it matter that it's, does it matter that it's symmetric at this point or not? Well, the adjacency matrix is a symmetric matrix, so you want to keep it symmetric, and that's important for our limit. You don't want to look at directed graphs for limits? You don't want to look at directed graphs. The theory is not there yet. So you have this sum, and so it has a This sum, and so it has a particular symmetry which is different from particle exchangeability. What is it? The symmetry is that no matter how you label the vertices of the graph, it doesn't change the triangle count. And so the graphs can be labeled in any ways, the vertices can be labeled in any ways, but the edges themselves cannot be permuted in any ways. You if you permute all the edges in some order, that destroys the triangle curve. But the vertices can be labeled in any ways, and that preserves the triangle curve as a different class of symmetric. As a different class of symmetry. Here's another example which often pops up. It's called the scalar entropy. So if you have a graph which is an adjacency matrix and here you really want to take weighted graph where the weights are between 0 and 1 for every edge, you look at the scalar entropy function, which is p log p plus 1 minus p log 1 minus p, and you just add them all up over every edge weight and 1 over n square. You might think I'm arbitrarily choosing these examples, but no, these are actually. Choosing these examples, but no, these are actually very important examples that come from lots of areas, including statistics, computer science, extremal graph theory, and so on. So, these are established canonical examples. So, that's another example. Same deal. This is simply, you know, this one is actually particularly simple. This one actually does happen if you permute all the edges. It still remains the same. But this is a very special example. For most of these problems, that symmetry doesn't hold. So here is a problem that is taken from the theory of exchangeable random graphs in statistics and statistical physics, which is that you consider minimizing the triangle density plus the entropy over all possible sets of graphs with some edge widths which are constrained to remain in some intervals. These kind of problems have a long history. Dakonis and Janssen, Chakachin Varden, for example. And Chattachi and Vardhan, for example, study large deviations of random graphs, talk much more about Lovash's work, which is very closely connected with what we're doing, Leubetsky and Jaw, and there's also a textbook by Euphesia, which covers the whole gamut of such problems. So this is the symmetry I was talking about. The symmetry is the following, that the graphs are unlabeled. So this graph has the same number of triangles no matter how you label the vertices. No matter how you label the vertices, that's exactly the same. Which means that if you look into the adjacency matrix, it's a function of adjacency matrix that is invariant under permuting the same permutation on both the rows and columns. You are not free to permute the rows and columns independently. You must apply the same permutation for rows as it is for the columns, and then the function remains invariant. So, this goes back to this idea of exchanges. Goes back to this idea of exchangeability. I often say that, you know, the theory of optimal transport is very closely connected to the classical problem theory of exchangeability. So, this is actually another example of exchangeability. So, there's particle exchangeability, which is a symmetry of particles. This is a higher-order exchangeability of arrays, in particular two-dimensional arrays. A long history of work by David Aldis, excellent recent work by Tim Austin. Austin and you can find a survey by Austin online. Another of the reasons why computer scientists are interested in this is because it comes up in neural network problems. I won't quite go into what neural networks are, except it suffices to say that it's an optimization problem on a sequence of bipartite, large bipartite dense graphs, weighted graphs. So it's some sort of multivariate extension. Some sense of multivariate extension of what we're doing. Sorry, when you talk about dense graphs, what do you mean dense? So it's in a particular way. So dense would mean that the number of, so take for example, Erdoshini n, one half. Now that's a dense graph because every particle has how many edges? Well there are about n over 2, so that's the number, approximately the same number of vertices. An example of a sparse graph would be Erdos showing n, 1 over n. So you mean dense in contrast to sparsely? That's right. Dense in contrast to sparse layer. That's right, yes. And that has to do with the topology, as we'll see. So, there's a neural network is a problem of large-scale optimization on very large networks. If you have a single hidden layer, there has been some very recently, very well-cited papers by Main Montanari and Shiza and Bach, which deals with a single layer neural network and shows that optimization on a single neural network is given. On a single neural network is given by Vashisting gradient flows. There's a limit of that. If you want to go to multiple layer neural networks, you can't reduce it to Vashisting gradient flows. There are no particles anymore. But it does have a symmetry and there's a symmetry of these graphs. There's exchangeability of these graphs. It doesn't matter how you label the nodes, it gives you the same result. Okay, so what do we need if we want to develop a theory of gradient fluid? To develop a theory of gradient flow or a Vashi-stein-like calculus from particle systems or measures and lift it to graphs or the limits of such graphs. What do we need? Well, just like in particle systems, we need a common embedding that contains on unlabeled graphs. Graphs are discrete. You can't do calculus on discrete spaces. You need something continuous. You need to have something embedded in a continuum space. On that space, we need a suitable topology of graph convergence. Of graph convergence, and this is the topology which on measures is a weak topology. It gives you a natural topology of measure convergence. We need a topology for graph convergence. We need a metric for which we can define gradient flows and should be completion under this metric. For measures, this is the Vashestin 2 metric. And we will see here this metric is sometimes called the Gromov-Washestein metric. We'll come to this. And then we need a theory of differentiable structure to define gradient flow of this space. Define gradient flow on this space. The reason they are on quotation marks is because we neither have a differentiable structure, nor is this a gradient flow. But thankfully, the theory of curves of maximal slope on metric spaces is now well defined. So we don't really need all the differentiability to do what we are trying to do here. So let me first explain the embedding part. Like, what do I mean by limits of large graphs or matrices? So we start with what definitions are So we start with what definition of kernel. So a kernel is a measurable function from 0, 1 squared to minus 1 to 1. So which is symmetric. So we always want to preserve this symmetry. So it's just an arbitrary measurable function which is symmetric from 0, 1 squared to 0, from minus 1 to 1. This minus 1 to 1 is completely arbitrary. I could have chosen minus 3 to 5. That wouldn't make any difference. I just want it to be contained in some compact interval. That's important. Internal. That's important. Any symmetric matrix can be converted into a kernel. And this is an example as you see here. This is a symmetric matrix. I divide by 1 over 16 to make it between minus 1 and 1. What I do is that this is a 4 by 4 matrix. What you do is that you divide the unit interval into 4 by 4 piece and then you took a piecewise constant function whose height is given by, or height or depth is given by these numbers here. So that's the kernel embedding. So that's the kernel embedding. Now, weighted graphs, their adjacency matrix and their kernel embedding, they can be recovered from one another. And this is an example as you see. This is a graph, a pure graph. It's a particularly nice graph. Its adjacency matrix looks like this. It has a graph 1 connects to 8, 1 connects to everything below it on the right hand side, 2 connects to everything below it on the right hand side, and so on. So this is a particular form of its adjacency matrix. Form of this adjacency matrix, if you embed it into the kernel, you get this kind of sawtooth function out of this kernel. However, as I said, one should not distinguish between two graphs which are simply different because you have labeled the coordinate vertices different. Which means two matrices, if they represent the same graph in the sense that if you apply a permutation to its rows and columns, they become the same, then they should be identified. Then they should be identified. It's very similar to taking Monge maps, right? You know, for measures. That's basically what we are doing here. So we say that two kernels are equivalent if there is a measure-preserving transform from 0, 1 to itself, such that you can obtain one kernel by applying the same measure-preserving transform to the x and y coordinate and go to the other kernel. The measure-preserving transforms are simply, I'm just doing the infinite, the limit of. Infinite limit of commutations. That's all I'm doing. So if there is one measure-preserving transform that transforms one kernel to another, I call this to be equivalent. And this is the space of graphons, which is defined by Lovash and Schlegetti, that look at the equivalent classes of kernels up to this isomorphism, up to this identification. And that's my space of limiting graphs of unlabeled things, of exchangeable graphs there. So for finite label graphs, the current. So, for finite label graphs, the corresponding graphons are simply equivalent classes model of graph isomorphisms. You just identify the vertices in any way, that gives you the graph one. You should compare this with the measures which are given by two different push-forwards. They give you the same measure, but they're coming by two different push-forwards from 0, 1 to the real line. Again, please stop and ask me questions if any part is not clear. Me questions if any part is not clear. Now let's look into symmetric functions. So here is again the function that we looked before, the triangle density. So this is, we already talked about in a graph, looking to the number of triangles and by n cube, if the graph is weighted, you weigh every triangle with a product of edge weights and you rescale it. This has a limit on the space of kernels, but also as we see on graphons. On the kernels, take any kernel W, you just any kernel w you just took into this this triple integral w x1 x2 wx2 x3 wx3 x1 and you integrate over x1 x2 x3 over 0 1 cube and that is the right embedding of this function versus kernels but now you see that this function is actually invariant under taking this measure preserving transform and therefore it's actually a function over the equivalent class and therefore it's a function of the graph. And so this can be obtained by vertex by permitting vertex, it doesn't change the function. And that's the kind of functions that we're looking for. So I want to show some examples of graph-1 convergence. So here is a half. So this is the sawtooth function of the adjacency matrix we saw a couple of slides ago. You can imagine what happens as n goes to infinity, it goes to a kernel that looks like this. But this is a bit misleading. But this is a bit misleading because I'm choosing a particular kernel. Things can look quite different if I have the same graph on, but I look at a different kernel. So here's an example. This is a complete bipartite graph and you can see it has a graph on representation which looks like the checkerboard. So with this from this kernel. However, I can also relabel the vertices. Instead of saying 1, 3, 5, 7, 2, 4, 6, 8, I can do 1, 2, 3, 4, and 5, 6, 6, 6, 6, 6. I can do 1, 2, 3, 4, and 5, 6, 7, 8. And all I'm doing is simply permuting the rows and columns of the checkerboard so that it becomes a block matrix. So these are both the same graphons, although they look quite different as kernels. Alright, okay, so now that's the embedding. Now, how do I put metrics or topologies on this? So there is a topology and there is a metric which has different things here, just like, well, in the measure case. Like, well, in the measure case, it's similar because the topology generated by Bashi Stream 2 and the weak conversions they are the same topology, at least for bounded measures. It's not true here. The topology and the metric are quite different. So, again, recall that two kernels are equivalent if I can obtain one from the other by using the same measure-preserving transform on both the x and y coordinates. Now, there is a general recipe on how to obtain metrics or norms. metrics or norms on, or sorry, rather metrics because Graphons is not a linear space, metrics on Graphons, where you start with any natural metric on the space of kernels or matrices and you take an infimum over all possible measure-preserving transforms. Again, this should remind you very strongly of Washistein kind of metric because again, very similar ideas that are being played here. So, again. Okay, so we start with any norm on the kernel space and I take an infimum over all possible permutations of one kernel to another and I take the infimum of this norm. So two of these metrics are useful for us. One is the one that is used by graph theorists, which is called the cut metric. This is a rather unusual metric if you have never seen this before. It was originally introduced by Alan Fries and Ravikanan in 1999, but it really got But it really got its importance from Lobash and Sheggetti's work. So the metric on kernels is given by this. You look at any rectangle and you integrate the kernel over that rectangle and you take the supremum over all possible choices of rectangles. Now, why is it particularly important? Because there is already an existing notion of graph convergence that comes from graph theory and combinatorics, and this metric captures that convergence. Convergence. It was shown by Lovash and Shageri that if a sequence of graphs, and by this I mean actual graphs, converge to a graphon in the limit, that would, the idea is that, well, not only if you look into triangle density, if you look into density of this shape, if you look at density of those shapes, for any finite graph F, if you look into how many such shapes are in the graph, then those densities would all converge to the integrals. Would all converge to the integral. So that's the notion of graph convergence, and this captures that. What is particularly important for us is that the space of graphons given by the metric induced by the cut norm is compact. And this is a highly non-trivial result that comes from Shemerody's regularity lemma. But it is very important for us because we need a topology, just like with convergence, we need a Proverbs theorem. We need a topology for which we can say when do we have compact sets. Say when do we have compact sets. So that's what the Schenerich's regularity theorem tells us. So, an example of this, a rather trivial example, if you want to set your mind on this, is that take the Edoshmini n, one half. So, there are n many vertices. Any two vertex tosses a coin independently with head or tail equal probability. If there is head, you join an edge. If there is tail, you don't join an edge. So, everything is like equally like uniform distribution. Now, this sequence of random graphs, all Of random graphs almost surely converges to a graphon in this metric, which is simply a constant graphon, half everywhere. And you can imagine this, what you're seeing it is you're zooming out of the thing, and what you're seeing is that the densities are basically the same everywhere. So it goes to a constant one. So that's our topology. This metric gives us a topology. However, our gradient flows would be defined with respect to a different metric, which corresponds to the Washistrim 2 metric. Which corresponds to the Washistring 2 metric. What you're going to do is that you're going to choose the L2 metric on the space of kernels. You choose the L2 metric on the space of kernels, and then you take the infimum over all possible measure-preserving transforms. Then that gives you what graph theorists call the invariant L2 metric, and what people from machine learning call the Gromov-Lashistein distance. I have no beef with either of these names, but people call by very different names. Call by very different names. So, invariant tentometric and Gromov-Washist, they're basically the same thing. And what they do is that you have two graphons or two kernels, and you try to permute the rows and columns of one so that you minimize the L2 distance between the two. Now, the reason why this is important, and this is actually what we prove in our paper, is that if you look into this invading L2 metric, it turns the space of graphons as a geodesic. Of graphons as a geodesic metric space. So, not only is it a complete metric space, but there is also a geodesic structure. Given two graphons, there's a unique geodesic that joins this. So, this is a very closely related, and so it allows for a notion of geodesic convexity, and it's very analogous to the Washes-Tin2 metric over measures. Somewhere I should mention again this famous paper by Bosch, Scheis, Lovash, Source, and Western Bay, where they look Where they look into various different kinds of metrics, including the cut metric, different nodes of convergence, and they consider the invariant metric. Alright, okay, so let's try to understand what gradient flows are. So again, I'm going to do a lot of technical crimes here because, again, gradient flows exist on RT, but they don't quite exist on arbitrary metric space, but I will still call them gradient flows. So, what are the new? So, what is the reading flow on Rd? You guys are familiar with this, but let me just quickly go over this. So, I have a function u on Rd to R, and I want to look into this kind of problem. So, this Cauchy problem, u prime t is minus of radiant of f of u of t with some initial condition. Now, if I want to do this over arbitrary metric spaces without any differentiable structure, you see the first thing, the problem is that there is no notion of gradient. That there is no notion of gradient on metric spaces. So, how do we get away from this problem? So, well, the textbook for this is Ambrosia, Gigli, and Savari, but I want to give some idea of it. The idea is that, well, you take derivative of f of t. You take derivative of f of t, you get this is by chain rule, u prime t gradient of f of ut. And then if you apply the simple inequality 2ab is greater than or equal to minus a square minus b squared, you see that. square minus v square, you see that this is greater than or equal to minus one half norm square of u prime minus one half norm square of the gradient of f of u t with equality only when u is a gradient flow. And so an alternative way of defining gradient flow in R D is that the curve is a gradient flow if it satisfies this inequality, but this direction is reversed than what you get from the usual inequality. Now, what is the advantage of Now what is the advantage of doing this? The advantage is that this formulation depends not on actually the derivatives but on the norms of derivatives and these norms of derivatives can be defined on metric spaces. And so this again as I said it should look into the textbook by Amru Shujiki El Savare. So there is a notion of a metric derivative. If you take, so imagine that you have the space of graphons, you have an absolutely continuous curve, and this is the Absolutely continuous curve, and this is the metric which is the L2 metric, the Gromov-Washistein metric. There is a notion of metric derivative which tells you this absence which characterizes this absolutely continuous curves. So that's a speed of the curve. You can't figure out what is the velocity, but you can figure out what is the speed. So that's the speed of the curve. There is no gradient, but there is something called an upper gradient, which kind of acts like this absolute value of the gradient of f. There is some things there. And actually, in Something is there. And actually, in our case, we can actually, for all the examples that we can compute, we can actually compute things like something that we call Frechet-like derivative. So there is a notion of derivative on the L2 of 0, 1 square, some kind of Frechet derivative. And you can modify that to get something like a Frechet-like derivative on these equivalent classes. So that's this Frechet-like derivative. And so there is an, again, there is a curve is called gradient flow, more technically a curve of maximal slope. Maximal slope if it satisfies this. So, yes, although there is no differentiable structure on the Grafan metric space, there is a well-developed theory by now by which you can talk about what it means to be a gradient flow. Whether it exists, whether it's unique, these are separate questions, but there is a theory for this. So here's our first theorem. Again, going quite rough there, the exact statement is in our paper. is in our paper. If f as a Fresh-like derivative, which I will describe in a few slides afterwards, what is a Fresh-like derivative, it's some notion of derivative that you can define. If the function is geodesically semi-convex, again as I said, the space is geodesic, it has geodesic, so you can define, actually I should really say it's generalized geodesically semi-convex, not geodesically semi-convex. Then starting from any graphon, there exists a unique reading flow curve for F. Now the curve Now the curve, remember the space of Graphons is actually bounded. It has a boundary because you don't allow the entries of the matrix to go between away from minus 1 to 1. This is important for the topology. So what it does is that inside the space, it flows according to this pressure-like derivative. But when it hits the boundary, you have to constrain it. Because if the boundary is taking you away from the box, you must project it back. And so there is a full description of it in our paper. Description of it in our paper, exactly how it flows. But yeah, this is, as you can imagine, once you hit the boundary, you should suppress so the curve doesn't exit the boundary. You can do it by putting indicator functions. So that's our first result. So let's try to compute some of these Fresher-like derivatives. They're quite natural. So again, let's go back to this triangle density, which we have been studying so far. The triangle density is simply this. It is simply this. Given a kernel or a graph on W, or a kernel representation of a graph on W, is given by this triple integral, x1, x2, x3. And then take this triple integral. Its specialized derivative is three times this integral of, its specialized derivative is another kernel or another graph, which is given by this integral of wxy, wxz, wxz, wzy, dz. This is an example of what we call in our paper to be a potential energy. Just like in the theory of optimal transport, there is potential energy, interaction energy, internal energy. There is also the same classes of functions that come up here. Potential energy, interaction energy. So this is an example of a potential energy. We also give other examples in the paper. So H, H, Delta, H, H, the triangle density is what you call a potential energy. Actually, any well triangle density is a linear function. Density is a linear function. It's a particular way I can define potential. What I'm trying to understand is the first line or the second line that you're calling a potential energy. Oh, for time, the density is a potential energy. Yes, yes. That's right. And so it has a particularly simple formula for its derivative. Yeah, that's a potential energy. Okay, so here is the scalar entropy function, which I said is particularly simple to understand because it only depends on the edge weights. So this, remember, this is the scalar. So, this remember this is a scalar entropy function. This, as I said, this comes up in the large deviation studies of Edershen-Urano graphs. So, you take every edge rate wxy, apply the entropy function, and you integrate. It has a Freshenite derivative, which is the log of the edge rate over 1 minus edge rate. Now, the reason it's very simple to understand is the gradient flow is just this. It's an ODE, but at every point x, y of the, so you're looking into the xy of the, so you're looking into the unit box, every point xy, how is it flowing? It is flowing according to the negative of that function, log of w over 1 minus w. As you can imagine, where does it flow? Well, if you are above 1 half, the derivative is negative. If you are below 1 half, the derivative is positive. So every coordinate just monotonically, well, not monotonically, but every coordinate just converges to exactly 1 half. And that's exactly what the gradient flow is. It goes to a constant 1 half. As you would expect, one half as you would expect constant one half would do this this integral function so this is as I said this comes up in the theory of exponential random graphs that people want to minimize these kind of functions where you have some density counts of some densities plus the entropy and okay so my grad students have done this This animation, if I click on it, I can show it. So, which one do I? If it's here, right? It's okay, it's all right. Okay, there is an animation. If you, I can't seem to quite click on it, but if you click on it, you will see the gradient. You will see the gradient go up here. Oh, okay, good. It does. Okay, click on it. Okay, good. So you can see, for example, what is happening to this. So it will converge. It's maybe not very clear because the color gradient, they did not pick very well. But it does converge to something which is supposed to be the right value. Which we actually, for most of these problems, we don't know what is the right value. So we don't actually know what are the optimizers. A couple of slides ago, you had a convention. slide about a convexity hypothesis. So these are semi-convex. So entropy function is convex and all homomorphism density functions are semi-convex. Alright okay so good. So you might wonder how did we actually do this animation? What is the justification of this? The justification is again we want to say something like this. That if you do finite dimensional gradient flows on large graphs then that is converging to this gradient Then that is converging to this grading flow on the space of graphics. And that's the theory that we want to graduate into. And that's exactly what happens. To understand this, let's try to understand, okay, so suppose I have a function on the space of graphons which has this following rigging flow. Again, as I said, I'm simplifying this because this really isn't the formula. Once you hit the boundary, you need to constrain yourself, but let's assume it is. I will come to what this fresher-like derivative is. Just like Just like any function on probability measures can be seen as a function on empirical distributions on particles, similarly any function on graphons or kernels can be seen as a symmetric functions of matrices. So you have a, this function will induce a function on the matrices, on n by n symmetric matrices. And n by n symmetric matrices is an Euclidean space, so you can run a regular Euclidean gradient flow, right, for every coordinate of the matrix. For every coordinate of the matrix, well, up to symmetry, you can run as Euclidean gradients, gradient flow. The question may, the natural question is: are these two curves related? Can I recover that curve in the limit by some limiting procedure as n goes to infinity? So that's a natural question to ask. So here is my definition of what I mean by the Fresher-like derivative. So suppose I have a function, suppose I have a function on graph. Suppose I have a function on graphons. I call it a pressure-like derivative if it's another, it's a symmetric measurable function phi. So, given a function f on graphon, a symmetric measurable function phi is the Fresher-like derivative. If it in some sense, it's like the approximate linear, the first order linear part of F. The point problem here is that the Graphons is not a linear space, so you can't just do actual Feshet derivative. Do actual Feshi derivative, what you really do is that you approach for given any u, so given any w, you approach u from the space of graphics. So you can only approach from one side of it. And then you take this, and if this condition holds, then this is a fresher-like derivative. So I hope you'll agree. This is a very natural sense of taking a derivative. So one might ask, like, okay, how is this fresher-like derivative related to the Related to the Euclidean derivative? The answer is very simple. The Euclidean derivative multiplied by n squared taking the limit will converge to this again in the space of graphons. Now to see that this is not very different from measures, let's do a comparison chart. Let's see an example where this is actually true. You see what's happening. So here is a function, not space of measures. What is a function? The function is on empirical function, on empirical measures. You look into the empirical measures. Empirical measures, you look into the empirical mean. So, what is the Euclidean gradient of fn? Well, it's clearly 1 over n, or every coordinate. So, what is the corresponding functions on all probability measures? Well, integral of x d mu. It's a simple function. What is the Waschisting gradient of that function? The constant function 1. And what you see is n times gradient of fn goes to the constant function 1, in some sense, whatever that. function one in some sense, whatever that would be. On grounds, however, it remains the same but the scaling changes. n becomes n squared and that's because of n squared entries. So here is the simple example of edge density. So on the set of graphs or weighted graphs, you simply average out all the edge weights, a n of ij. So that's the edge density. What is a Euclidean gradient with respect to every edge density? Edge density, every coordinate, it's 1 over n squared times 1. What is the function on graph forms? Well, it's a single integral, it's a double integral of w xy dx dy. Its fresh-like derivative, if you compute, it turns out to be again this constant 1. And what we are seeing is n squared times the gradient is going to be the same. So the difference is only in scaling, but otherwise the structure is the same. Yeah. So uh in in the mean case I mean the f, I'm sorry, are we on Rn? The f is a vector, right? So in the mean case, I'm just take this to be taken. So take every xi to be this real value. And so now this is a function which is 1 over n 1 over a summation of xi. 1 over n, 1 summation of xi, that's a symmetric function on Rn. And so that gradient on Rn is a vector 1 over n times a constant all once. Right, so somehow, again, the thing that I'm sort of confused about is... So when you're talking about the Wasserstein gradient, you're talking about it as a vector field rather than a one. Exactly. It's a function one. So that's a very So that's a very important point. So the Washes in gradient, as you know, it's an element in L2 of the measure. So it's a function there of thing. And what this Fresh-like derivative is given a graphon. So imagine that the graphon is representing a large weighted graph. This Fresh-like derivative is another function on that same large-weighted graph. So for every weight, you get another value of the function. So it's like. So it's like, so they're both graphons on their own accord, but they're actually coupled in the sense that they both can be embedded on the same graph. We call them coupled graphons in our view. All right, so okay, so let's consider again this gradient flow on the on the on the okay, so Okay, so I wrote something rubbish here. So, what I want to say is that, okay, so let's again consider the Euclidean gradient flow. If I look into the Euclidean gradient flow, I can imagine that because the gradient is being scaled, so the actual gradient flow is just given by this Fresher-like derivative, which is n squared times the Euclidean gradient. But you can imagine then that, well, if I look into the Imagine then that, well, if I look into the Euclidean gradient and if I speed things, speed time up by n square factor, then as n goes to infinity in the sense of graphon convergence, it would converge to the gradient flow on the space of graphons. And that is exactly what happens. So it's reasonable to expect that the gradient flow in graphons can be obtained by taking our scaling limit of Euthydium gradient phase. So here is our next theorem, which is that, well, take Theorem, which is that, well, take a function which is a function with gradient flow double t. So, again, what is this class of functions? Well, these are symmetric functions and matrices. So, what do you mean by symmetric? It means that it doesn't change if you permute the rows and columns of the matrix by the same permutation. And so, you extend it to the space of graphons and you look into its gradient with respect to this. And so, you get this curve, Vn of T, which is given by this. And as all. This and as always, you have to make adjustments at the boundary. Even the Euclidean graduate descendant, when it hits the boundary, you don't want it to escape. You want to keep it inside the boundary. Keep it inside the cycle. And now you scale time, and instead of thinking of these as matrices, you think of these as graphons. So if you say think of matrices, they don't have scaling limits. If you think in terms of graphons, they do have scaling limits. And so using this exchangeability, one can show that if the One can show that if the initial starting point converges in the cut metric to the starting point on the space of graphons, then the entire curve will converge uniformly in the cut metric over compact intervals to this radiant flow of graphons on the graphons. The whole thing will converge. Of course, it doesn't say that, I mean, it's probably, you won't even expect, it doesn't say that it's converging all the way to infinity. We can't really say that. Infinity, we can't really say that whatever this is converging to, that's also what the limiting curve is converging to. That's a separate argument. I want to show a particularly striking example of how this can be used in computational advantage. So there is a famous theorem by Turan, which says that the n-vertex triangle-free graph with the maximum number of edges is a complete bi-type graph. So if you look into all graphs which have n vertices and Vertices, and if it is triangle-free, which means that the graph does not contain any triangle, then the graph that maximizes the number of edges is a complete bipartite graph. So you separate it out and then left side goes to everything on the right side and vice versa. If you want to see as an optimization problem, well, how will you do it? Well, what you can do is that you can run a gradient flow of the following. A gradient flow of the following function. This is 10 times, or rather, some large constant lambda times the density of triangles minus the density of edges. When you try to minimize this, you're minimizing the number of triangles and you're trying to maximize the number of edges. And actually, what you see here is that this will be the limit if you run the program. And this checkerboard is already something that we have seen. We actually recovered that this is actually the byproduct. Actually, the byproduct, complete bypart graph. So, one application of this would be: if you want to have a conjecture of how the optimization of graph problems look like, when you run this program, run this reading flow and check whether it actually gives you the answer. So, here, the answer will depend many ways you can your initial starting point determine which sort of equivalent description of the means of the starting point. Possibly, that's possibly true. Yeah, because there could be many checkerboards, as I say. But no matter which one you say, that's possibly true. But actually, I don't know exactly how, I mean, obviously these are done by grad students, not me, but I don't know exactly where they started with. Well, that's a good question I should ask them. With all the simulation it's open, how do you adjust to the exception? I think for most of these cases, they were just doing random graphs for simulations, the initial condition, randomized initial condition. In fact, I think just yesterday I got an email from Gabriel Pere, who does these things on the bromo phosphorylist. And I think Gabriel has libraries to do this. We have some established methods. Okay, so alright, so I'm okay, good, I'm almost done here. So I want to say a little bit about where we are doing now. So we are talking, so yeah, so our ongoing work, which hopefully should be over soon, is that we show the convergence of stochastic gradient descent with add or added noise there, with or without added noise. There, one of the problems that what you would imagine is that, okay, so you have gradient descent, which converges to reading flow, but as we Is really flow, but as we know for the particle systems, you'd have a Brownian moments. What happens to those kinds of limits? So I think we have some answers for that, and that's what should be out soon. I hear soon as a heavy, heavy tail. So we started doing this for the optimization of multiple layer neural networks, so that's one of the goals, at least on the CS part of things. But I also want to see that this is a But I also want to say that this is an interesting class of mean field interactions, because typically in mean field interactions, particles interact with everybody else. But here you have a different class of symmetry where you only interact, you cannot interact with everybody with everybody else. There is a graph structure that must be maintained. So there are probably many more interesting such problems where this limit can be obtained. So I'm happy to get suggestions for problems like that. Alright, so just to conclude, so optimization on graphs is hard because it's a discrete set of points. However, Graphons, which is a limiting space, was developed by Lovash for analytical tractability. Students should use that. And the thing that we do is that using exchangeability, it is possible to do Wachenstein-type calculus on this space, on the space of graphons. This space, on the space of graphons. So it's an exciting space to work on, and then it has the other use of it is that it's numerically very good because it's approximately what you would see if you're doing it on matrices as well. Thank you. And yeah, so the paper is on archive. Thank you very much. Right, thanks a lot, Sume. Are there any questions? Martin? Martin? Yeah. Okay, thanks, Shulin, for this very nice talk. Well, I can't see Martin. So there's some kind of obvious question. Is there any curvature? I think, as far as worked on it, it behaves into very similar to the Bashistin space. I have to see this negatively curved space again. It's the same problem that we. Again, it's the same problem that we had: is that if you just have Jurassic convexity, we can't work with it, but if you have to use generalized Jurassic convexities, it inverse that direction of inequalities. So it's a negatively curved space. I'm saying this correctly? No, it's positively curved. I'm saying this wrong, right? What is it? What is it for the Russian space? It's positively curved space, right? Yeah, sorry. So it's also positively curved space. It's a submersion of a flat space. It's a submersion of a flat space in the flat case. But it has exactly the same property. I mean, the thing to remember here is that the only thing that... So what are we... So I mean, the idea is that the flat space is like L2 or whatever. And the submersion is basically when you quotient out symmetries, you can only bring things closer together. Yeah, that's exactly what is happening here. That is exactly what is happening here. So, yeah, so. Yeah, so as Robert said, really you should think of measures. If you want to put measures in this framework, well, any measure, if you just, let's say any measure, this is an exchangeability language, is given by IID samples from that measure. So take an IID samples of this measure, that gives you an arbitrary permutation of these values that come up, and you compress all those values as a function on 0, 1. So that's like your, every measure is this. Every measure is given by a function from 0, 1 to the real line, which is just its randomly arranged histogram. So that's generalization. So that's Robert says. This is a flat space, which is this L2 space. And now we are just quotienting things out. So I am only bringing things together. The problem, one of the major problems here is that there is no notion of Monge maps. In fact, I've convinced myself that there cannot exist any Monge maps. That they cannot exist any Monge maps. I mean, it's like we cannot characterize. So we can show that these measure-preserving transforms feed that map one graphon to another graphon, they exist. So there is an infimum which is actually attained, I think, at least in a slightly generalized sense, it is attained. But we cannot quite characterize them, like how do they look like? And I don't think they look like anything like what you'd expect like maybe means of complex. What you'd expect, like gradients of converse function. But maybe they have some weird characterization, we don't know. But in the absence of such Mange maps, one cannot do this kind of Brunier type of analysis. You have to always carry over these labelings of 0, 1 square with you. But you can still do a lot. Alright, Simone has another question. It was very nice. And It was very nice. And so I have so one is an observation that as you said you're losing L2, so it's kind of flat metric with the but with the permutation inside and it seemed to me very similar to the description of the Buserstein metric as done by Danbo when desc describing the Blastein gradient in the in the Basestein space. in the in the past time space where you embed the L2 of 01 whatever value in RT yeah and then you look at the measure as the mark that the time ending so it seemed a bit sillier so maybe well so thank you so I don't know this reference if you can send it to me that would be I'd be really happy about that but I'm not surprised at all I mean but as I said Surprise at all. I mean, but as I said, the point I'm trying to stress here is that there is a common structure in both these measures and the graphs, and this is this exchangeability which allows us to do the same thing. And that's why it gets these very similar structures. But yeah, if you can email that reference, that would be great. Thank you very much. Yeah, yeah. And I say that it's similar, but maybe it doesn't apply, so beware. But the other question that I have was about Instead, that I have was about the metric structure that maybe you can have on a graph. So, this seems to be this setting of graphons, seems to be adapted to these dense graphs. That's correct. Instead, if you have sparse graphs or you want to retain geometric structure of the underlying space on which the graph is so there is something, I don't know, in between, or right, so there are lots of very. Right, so there are lots of variations of graph forms, so for dense graphs, for sparse graphs, for in-between. As far as I understand, sparse graphs have an entirely different limiting structure. This is the all this Benyamini-Shroum limiting structure which looks into local neighborhoods. They don't quite fit here. So if you take a sparse graph, let's take Erdogani G n, 1 over n, which is very sparse, its graph found limit is simply the zero graphon. It doesn't give you anything interesting because it's Doesn't give you anything interesting because it's just farce. But if you see this, I think Yufe's book probably covers all of this, but also some of the references here. Lobash has a great book which also covers this. Lobash's book is called Large Graph, something like, I forget. It's in our paper. It's a wonderful book, the textbook, and he also gives many different notions of convergence when the graph is not dense. But this is a very active research area. I believe either this year. I believe either this year or next year there's a whole science program on this thing. This graph convergence, there's various modes of graph convergence. Right, so I was just thinking, I mean, somehow you could do this on the cube instead of the square with permutations of all three axes. So that theory, so the main problem is topology. Otherwise, everything should work exactly in that. Otherwise, everything should work exactly in the same way. Because Tim Austin's work, for example, when he generalizes Aldis and Hoover's, he does it for arbitrary n-dimensional IVs, so for tensors, or exchangeable tensors. The problem is that for graphs, we need compactness to do the Washistin analysis. We need to show that this backward Euler estimates, they have convergence and so on. And this compactness is here provided by Shannaridi's lemma, but I don't know of any such lemma for higher order things. And that's the problem. For higher-order things, and that's the problem. If you can find a topology which gives us compactness, everything will go through. Or at least some characterization of compactness. I guess it would be relevant to hypergraphs or something like that. That's right, it would be related to hypergraphs or densers. Random hypergraphs. Exchangeable random dense hypergraphs. Exchangeable hypergraphs and density. Yes, exactly. And there's a lot of optimization problems on hypergraphs. That's true. So it's quite similar to the measure. So if you take two graphons, there is one arrangement of rows and columns such that it exactly matches the norm. And so just do what Robert's here is. We just draw 1 minus t of this plus t. It's very simple. It's just exactly the same idea. And then you also have the same proof of this thing, that if you have any absolutely continuous curve on the graph on space, Curve on the graph on space, we can find some representation of it on kernel space, which is actually absolutely continuous in L2. So, same kind of like, you know, thing. So, so any intuition? Exactly. If you have any intuition about gradient flow, just drop the measure part, think in graph on terms. Chances are that it's going to work out. Exactly. The only thing that doesn't work out is because we are hamstrung with a lack of... Is because we are hamstrung with the lack of marshmaps, so we don't know the existence of marshmaps. You can still control those types, yes. Back to sparse or dead. So when you were on your previous slide, you had mentioned neural deaths, applications with several layers. So if I understand correctly, you would have to have a fixed number of layers to make the network sort of deeper, right? Correct, exactly. So that's so if even the voltage chain calculation. So even the Wascher Stream calculus, the way that it enters in a single layer neural network, or rather actually a single hidden layer neural network, is that they take this depth, well not the depth, the breadth, the width of the network to go to infinity. And there, because it's a single layer, there's only one graph, like basically one set of edges. And so everything is just permutable. But once you put multiple layers, you actually have this symmetry and not particle symmetry. I think that's closed lists. And we have the coffee break coming up. So let's thank again all the speakers of today's session. And yeah, see you in the next session. Thanks a lot. When is the next session?