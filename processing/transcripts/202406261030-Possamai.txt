Yeah, so today I want to talk about Stacker Bear games. It's great. Basically, 50% of my work has already been done because if you didn't know about those, you would do by now. And somehow what I want to talk about, this is something we've been working on. It's been a while now with Carmilo, Emma, Nicolas, and myself. And this stemmed from kind of trying. Stemmed from kind of trying to understand why some of the methods that have been developed for principal agent problems in continuous time, which are very specific type of cycle bear game, why those seem to be working but not directly applicable for more standard cycle bearing games themselves. So we started like delving a bit more into the literature, trying to understand what people had done. Then there was a long time trying to understand. Then there was a long time trying to understand where what we had done was kind of fitting. And I think we now converge to the fact that, well, you will see there are at least four type of definitions for equilibria in this literature. And it looks like we're bringing up a fifth one that is actually different from the others. So let me tell you about what it is that we're doing. So yeah, that's for Stacker Bear games. You know what's going on here, leader, follower. I'll just skip that. Leader follower, I'll just skip that. Uh, okay, so the setting we're working with: this is standard diffusion setting. Uh, you get the state process X, can be multi-dimensional if you want, and you get volatility and drift that are controlled by both the leader and the follower. Uh, leader chooses the control alpha, follower chooses the control beta. I am not telling you what this set scale graphic A and B are. This is where. Are. This is where things start getting very different depending on the definition you're taking. But for us, somehow the game is going to have a very standard form. Alpha is chosen by the leader, goes and announces, I'm going to do this. Follower is like, okay, if you choose alpha, then I'm going to decide what my control beta should be by maximizing my criterion, which has just some terminal reward and some running reward. And some running rewards. The Cf here does not depend on alpha, but that's just because it's lazy writing. You could have alpha here at the same way, you could have beta down there. Doesn't really matter. Now, because it's Teicherberg and the leader has a strategic advantage over the follower by deciding first what he's announcing, he can anticipate reactions of the follower. For simplicity, assume given alpha, you have a unique optimal response beta star. Optimal response beta star, then what the leader does is just maximizing over his controls, anticipating best response from the following. I don't think I have this anywhere in the slides, but let me just tell you a bit what's going on if you have several optimal responses here. You have two strands of literature: you have the optimistic people and you have the pessimistic people, and you also have a bunch of people in between. The optimistic people say, The optimistic people say that, well, if there are several of those, the follower is indifferent between them, so he's very nice. And he lets the leader decide which one is the best for him, which means that here you would have a soup over beta star. Okay, so that's the optimistic way of thinking about this. We can deal with that. That's not a problem. The pessimistic one is, well, I have no clue what I is going to do. So I'm just going to take an inf over optimal responses here. Over optimal responses here, there. Okay, we also can deal with that. That's really not a problem. And if you look at that's more like bi-level optimization literature in operations research and optimization, there are a bunch of things where people are looking at some form of epsilon equilibrium and they define different ways of dealing with potential non-uniqueness here. That is a practically extremely relevant topic at the relatively theoretical level we're working with. It's really Theoretical level, we're working with it's really just a matter of deciding what you want to do. And I'm telling you that putting a soup here on inf doesn't change much my life. I can show you later on where this is going to pop. All right, so there I still haven't said anything, that's just the model. Now, different types of equilibrium. So everybody here knows about games, and you know that somehow this always idea of infinity. This always idea of if you play mesh equilibria, whether you're using open loop controls or closed loop controls, can make a huge difference. Here, it feels like it's even worse somehow. If you look, so this definitions, this is coming from a paper by Ben Susson, and I don't remember Webb's. And they are identifying using the literature in the deterministic case four types of equilibria that are coined by. Types of equilibria that are going like this. So basically, here Ft is filtration generated by the underlying Bohnian motion. So the open loop is what you expect. You just take things that depend on your initial position and whatever history of the Bohnian motion you've observed. Adapted feedback, this is where you use this. You don't know the initial position, but you know the current position. So this one is special. It leads to a really different type of equilibria than the Equilibria than the other ones, it's those three here. This is what called global advantage of the leader of the follower, meaning that he has an advantage over the whole duration of the game, while this one is an instantaneous advantage because you only learn, I mean, you don't, you only know the value of Xt there. Then you have adaptive closed loop memoryless, which is X0, XT, F. And then finally, adaptive closed loop, where you can use the whole trajectory of X. you can use the whole trajectory of x instead of just the current map. This is dealt with in the literature by some form of dynamic programming approach, but as I said, it's very different from the others. This one and this one, this is typically tackled using stochastic maximum principle. And this one is not tackled because people don't know how to deal with it in general. And there are some examples where it works, but in general, And there are some examples where it works, but in general, just yeah, like it's an open problem managing to deal with that at the same level of generality than the others. So obviously, when we saw this, we were thinking, okay, let's do that. That seems to be the most fun one. I can only tell you that's not what we're doing. We are sitting kind of somewhere there, not here, not there, but kind of in the middle. What we are doing is Doing is following Dr. Witch chapter in Hifeen's book, saying, Well, why should I use information generated by the Bonnyan motion? Because the Bonnyan motion is just modeling choice that I'm making. I'm just saying that X, which is a quantity that I observe in real life, has some dynamics where there is this Bognian motion. But it's just something I invent in my modeling. It's not something that I can, in general, observe. If you just do control and you don't do games in general, it doesn't matter. To games in general, it doesn't matter, right? They just say I take my controls depending on W, or I take my controls depending on X. At the end of the day, if your problem is nice, you get an AGP equation, and that's the same thing. The values are the same. You can also go and ask Jinfeng. It's not always the same. You can book up examples where you take very bad terminal conditions and you take typically no Markovian problems, and it can happen that the values are different. But yeah, you need to work hard for this to happen. But yeah, you need to work hard for this to happen. So it's more something that is not expected to be there. In for games, you expect that things are going to start being different depending on which information players have access. So what we were thinking is more along the lines, yeah, I don't want to use W, I want to do something that feels more closed-loop to me when I think about Nash equilibria, and I just want to use the path of X in my definition, which obviously means that if you know a bit more about Means that if you know a bit more about this, we're going to be working at some point in weak formulation, but that's going to come. Once you've said this, this is the standard definition of cyclical equilibrium, saying that for any choice of alpha, beta star is an optimal response, and then alpha star is an optimal response for the leader, given that beta star is big. So, this is just the standard thing we're using. It works for one, three, and four, doesn't work for two, which has to be different, but I'm not going to talk about it. Two, which has to be different, but I'm not going to talk about. All right, so that's a little bit the state of the art when you're looking at the literature, as I said. This is known, this is known, this is known, nobody knows how to do this. We are kind of going with something that is going to be different. And yeah, I can tell you it is indeed different. We can check this on examples, but I want to tell you a bit more about. Want to say ipa star, beta star, beta star should be operative rather than beta star, alpha star. Where where equilibrium definition of equilibrium because you yes, yes, yes, yes, beta star of alpha. Yes, yeah, it's uh you want to know beta star for any potential alpha, yes. All right, so I've already said all of this. Uh, now, why is this linked to principal agent models and why when you know about principal agents, you When you know about principal agent, you get a bit frustrated because at the beginning you don't really understand why you can do things in this setting but not in standard cycle bare games. So you've heard about this again, so let me skip that part. And then let's go to the model itself. So in the most standard case, again, you have a process X, you have two players, the leader who's the principal, the follower who's the agent. Leader plays first and announces his controls. And here, These controls, and here, in the simplest case, the control is this random variable ψ, which is the payment that the leader makes to the follower at the end of the contract. So this is xi. While if you remember before, there, this was what I call gf, I think. Yeah, right, so you got gf here, now this becomes xi. Right, then give us. Right, then given ψ, the follower chooses his optimal effort beta by maximizing this quantity until then that's pretty straightforward. Now, imagine there is an optimal response beta star to this. Then, what the leader is maximizing over this control, this anticipating the optimal response. I'm changing the way this is written because I'm just emphasizing that this is supposed to be written in weak formulation, but doesn't really matter. It's really the same thing here. It's really the same thing here than what you had before. So, from the purely control point of view, at least to me, this looks like a harder problem. This is harder because maximizing over random variables, what is going on there? Is it calculational variation? How does this p star here or the beta star depend on xi? It seems even more complicated than the other one. Just turns out, I agree, this is simpler. So, why is it simpler? It's simpler, uh, well, I will show that in a second. This is simpler because you have a huge degree of freedom that is going on here. So, this is just me telling you that once you're there, you follow this idea by Sarinkov, which we extended with Nizarani, Sharima talked about this. You can prove kind of that if y here is the value function of the follower, okay, then there exists some process z, some process gamma. Exist some process z, some process gamma, such that at the terminal times, the value function of the follower has those dynamics where h here is the Hamiltonian of the follower. So this is the second order BSD point of view of this, which, well, I don't know, we haven't thought about whether we could apply MASA in use into this. I don't know yet. But regardless, this is something that's. Regardless, this is something that's true. And why do you like this? Because y at capital T is psi, is the payment. Okay. And then what you are just saying is like, oh, great. The problem of the principle now, I can replace psi by this, which is the terminal value of a forward SD, which is controlled by Z and gamma. And I transform this here into a control problem. And then, okay, it's not an easy control. And then, okay, it's not an easy control problem. There is a bunch of the genera I see popping up, but still, I can write down an LGB equation for this. And once I have it, this is a simple maximization that you need to do a posterior. So you look at this and you're like, great, I'm just going to do the same thing. I'm just going to do the same thing with the standard cycle berger. The problem that you end up with almost immediately when you try that is like, what's xai? There is no xai, there is no payment in the cycle berger. No payment in the Stackerberg game, and I told you xi is the terminal reward of the follow that's completely exogenously given. I have no control over this, this is not my choice. Regardless, I can start the same reason. So, let's say I'm trying to do this. I took what I have before, I fix an alpha chosen by the leader. Given alpha, there is some Hamiltonian functional associated to the problem of. To the problem of the follower. And I can mimic what I showed you before, right? This y alpha here, which depends, of course, on the choice of alpha. This is the Hamiltonian. And there is some Z and some gamma like before. That's fine. I can do this. My problem, of course, is that I cannot just say I take Z and Gamma, which are whatever I want. That's not allowed. Before I could, because before. Before I could, because before I was just telling you take psi, it's whatever random variable that you want. I'm just representing it under this form. It's kind of like a non-linear Martingale representation theorem, if you want to take it backward. But xi was free, so z and gamma were free. There's kind of like this one-to-one correspondence there. But here, psi is not free. There is no, but still, this is true. And what you realize. And what you realize once you think about this is that what is actually going on here is that I'm just not allowed to take whatever Zn gamma I want. I have to take the Zn gammas such that in the end, it should be GF here, this guy that I could call forward ends up exactly where it has to be. Right? I can always do that. Now the question, of course, becomes, okay, are there more than one? Are there more than one that can do that? And yes, the answer is yes, because that's the alpha here that's giving me extra degrees of freedom. So, somehow, what you're suddenly saying is like, okay, I am the leader. I choose alpha. Once I choose alpha, I kind of decide on a pair Z and gamma here, which is such that when I define this control process here, I choose. I choose Z and gamma in such a way that this ends up with probability one at capital T there. If I can do that, you can prove that the value that the follower gets is exactly the immuno value of this thing here, and that the optimal response would be whatever maximizes the Hamiltonian there. So it's kind of what you expect. It's kind of what you expect. So, the difference with what I've shown you before is that, yeah, I don't have freedom over xi. I have a constraint. xi needs to be equal to this quantity here. So, what do I have here? Well, it's what's called a stochastic target constraint. There, I stop asking things to G-Feng. I go behind, I start asking things to Bruno. And what I get here is that from the point of view of the principle, I transform the problem into a new. Problem into a new control problem with an extra-state variable y as before, but with stochastic target constraints in the sense that now y and x need to satisfy this thing here. So that's what's going on. In the way we understand the problem, we transform the bi-level optimization problem into just one-level optimization problem, which is kind of a standard control problem, except that it has this stochastic target constraint. Has this stochastic target constraint over there. Another nice thing there is that people have done a bunch of the work for you. So stochastic targets in and of themselves, this is Mete and Nisa that developed this, then afterwards with Romivald and Bruno. And the paper that is more interesting for us, this is Bruno, Romivald and Sylvia Ambert, on what do AGV equations look like when you look at optimizing something like. Look at optimizing something like this under a constraint label. Now, we have general results for the way these things go, and basically, what's happening is that, okay, how do you deal with a constraint like this? The most standard target problems you can think of are super hedging problems. Super hedging problem is a stochastic target thing. You're controlling your wealth in such a way that at the terminal time, it ends up above some terminal. It ends up above some terminal payoff. So you're super edging over half space. In super catching, you have this nice property that basically being above your terminal payoff at capital T, this propagates backwards in time as being above some other function at any point in time, meaning that this, what they call the reachability set, stays a half space at each point in time. The other way around, sub-hedging is the same thing. You're in half space that's like this, and it Space that's like this, and it propagates backwards like this. So now you have an equality constraint here. The intuition is that if life is nice to you, this is being greater or equal and lower or equal. So you should have kind of like this intersection of two half spaces. And in this example, this is exactly what happens. The constraint at n capital T basically propi gets bypassed like this, and your domain is some. And your domain is something like that. At the terminal time, capital T, those two things stay there. But if you want y to be equal to g, at the end, it should stay between two curves, w plus and w minus, that you can characterize using the non-results in stochastic target problems. So we prove that under reasonable assumptions. Let me show you a bit. Let me show you a bit how these things look like in a simple model, linear quadratic. So, yeah, like linearity in the controls, there are quadratic costs. What is going on there is that you, okay, so that's what the model looks like. That's the dynamics of x, that's the dynamics of y, that is your constraint. We're taking gf to be the identity, so you have y that needs to be equal to x. That needs to be equal to x. It's kind of the best you can hope for here in the sense that these two functions, w plus and w minus, you know the pd is that they satisfy. So there is some constraint on the gradient here that's there to match volatilities. But those equations are simple enough that they have explicit solution. Linearly, it's not too surprising that you get something like that. Get something like that. You can verify that those are indeed W plus and W minus, which is not the nicest thing to do because, in general, the characterizations you have for these things are in terms of discontinuous viscosity solutions. But here it's one of the simpler cases where actually things become continuous. So I know how these things work. Then I look at the paper by Bruno, Romuel, and Cyril. This is telling me the domain over which I need to solve my PBE. So I need boundary conditions. I need to know what is my value function on the boundary, but they tell you exactly how to do this. So you have a specific that is telling you what is the value function on the boundary. Here we are again are lucky. We can compute explicitly in this example the value function on the boundary. Again, linear, I mean, this is reasonable, that this is what you expect. You expect. And there, this is where you stop. Like, I mean, you start being lucky. In the end, you have your PDE over this domain, you know, the boundary conditions. You write the standard PDE you can think of in the middle, and then you do numerics because that's the best you can do here. So we did that. And in this example, so remember, you have the open loop case. You can do it explicitly. There is an explicit solution, and that's easy. The adapted feedback, I told you I was not interested. Back, I told you I was not interested in it, but you can also solve it explicitly there. It's actually equal to the open loop one. Then you have the ACLM, which is adapted closed-loop memory less. This one is a pain. This one, we don't know how to get the solution explicitly. If you take a subclass of controls, we get it explicitly. But it seems not clear how to get it in general explicitly. It's different from the other two. It's different from the other two. Then we have ours, which is different from all those three. And then there is the last one, the AF. No, that's not AF, that's AC, adapted closed loop, where I told you we don't know how to solve it in general, except that in this example, we know how to solve it, and we can prove that it's again different from all the others. So that's what I will finish with to give an idea about how you can. Yeah, so that's the PDO role. The PV overall, how you can, in general, try in some cases to tackle the open case. So, we thought about this because somehow now that you've made the link with principal agent problems, you can start thinking about what Emma already mentioned, which is first best, second-best problems. Second-best problem is the one where you don't observe, like the leader does not observe the effort of the follower and is trying to provide incentives in an indirect way. The first best is when Indirect way. The first best is when the leader basically chooses the effort of the follower. And in the ACL case, you know a lot. When I say you know a lot, you know the path of x, you know the path of w, and you choose alpha. If you know the path of x, if you know the path of w and you choose alpha, you know beta. You know exactly what beta is. I mean, you just take xt, you subtract the integral here. You subtract the integral here, you take the quadratic variation, or no, not take the quadratic variation, you subtract the integral of w, and then you differentiate with respect to t, and then you get beta. So you know what beta is. And the idea in this principle agent literature is that when you can infer exactly what beta is, you can design contracts that are going to punish the follower if he's not choosing the control that you would like him to choose. The control that you would like him to choose. Now, punishing means that you really can penalize enough, which means that the set of controls have to be large enough, but kind of in here you can solve things by hand and show that if alpha, I don't remember, I think it's alpha needs to become negative enough, that for any choice beta tilde made by the choice beta tild made by the leader, it til made by the leader, he can implement some alpha that is going to make beta til the only optimal response for the follower. Which means that when you can do this, the problem of the leader becomes what? It chooses alpha and beta. So it's just a standard control problem. So it's not always true that you can do this, but in the cases where you can do it, this ACL ring is just a standard control problem. And in the example, you can solve it. In more general cases where you can punish This is where you can punish enough. This works when you don't know how, when you cannot punish enough, we are like everybody else, we have no idea how to tackle the problem. But at least this example shows us that you have now five different notions of equilibria, which fail to be the same in certain examples. So, yeah, I mean, ours is what it is. It's more inspired by the principal agent literature. But yeah, of course, we could be discussing for a long time. Because we could be discussing for a long time which one is the best, if any. That's just one movement. And I'll stop here. Thank you. Yeah, we have time for questions. Everyone, for the boundary conditions, you, I mean, if you really want to have a tag, you recognition that it is absorbed. Understand that when you touch it, you watch, yeah. So, do you have general results? Here it works, yeah. In our case, it works. At least in the example and the nice things, it works. Otherwise, it's a bit unclear what is going to go. The volatility of x is not controlled, yes. Okay, so maybe you would have responding a technique. Yes, yeah, so it's like it's a much nicer setting than the very general one. Yeah, how the formulation. Yeah. How the formulation will look like if the leader is with us machine? If the leader is as we say. So if the leader, yes, I never said that. So basically, you need to look at what is going on in the Hamiltonian for the leader. So if the leader is optimistic in the Hamiltonian, you should add an optimization over optimal responses. Okay? And if it's pessimistic, now basically you interact. Pessimistic now, basically, you interface what you have as like a fictitious zero-sum game against somebody else. And then the value, I don't know if it's the upper or it should be the lower value of that game. And you write the Hamilton, Jacobi, Belman, Isaac's equation that's associated to this. So, in this sense, it's really doesn't matter. If you want to just tell the level I write AGV equations, you can do it. If you want to put TPP and things like that, If you want to put TPP and things like that in that setting, you can still write it down anyway. Yeah, it's really just put the info or put the soup depending on what you're interested in. In that sense, it's flexible. I have one particular plateau in mind: that is the agent does not control the dynamics of X, but the agent can control when the contract is terminated. In which sense he is not a Not a fix. So if you just tell me this, then there is like no moral hazard there in the sense that, again, if the principal is allowed to make contracts like, okay, I give you 10 in case the contract stops, you do whatever you want, but if the contract stops at T, which I like, I give you 10. If it stops at 10, I like, I give you 10. If it stops at any other time, you give me $1,000 or $1 billion. If I can do this, you look, okay, this is what I'm being announced. So what is my optimal stopping? Well, it's when I get something positive, not when I have to pay 1 billion. So in this sense, if you have one agent and the principal is allowed to contract on the time where he decides to stop, then actually the optimal stopping from the point of view of the agent is equivalent to optimal stopping from the point of view of the principal. Equivalent to optimal stopping from the point of view of the principle. The only way you can make these problems meaningful is when there are more agents, or when somehow you have to design contracts where the principal is not allowed to do the things that I said. It's again this punishment contracts I told you you can observe when it stops. If I can observe it and I can contract upon it, then I can punish you. And in the end, I'm the one choosing. So that's typically in this. So that's typically in this literature, you only see problems where the principal is the one stopping, not the agent, because of what I expect. Again, unless like Shaoloux has papers like that, the one that Medi was presenting is along those lines. If you have more agents, then the problem becomes meaningful because you cannot kind of stop everybody whichever way you want to. Yeah, let's scientists leave.