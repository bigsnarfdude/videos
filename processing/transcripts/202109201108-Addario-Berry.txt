So T C just would be the set of combinatorial trees whose size is n and whose plot sequence is this particular. It's this particular sequence. Okay, so so what I want to state for you now are some bounds, tail bounds on the height of a tree which is uniformly sampled from one of these collections. So I'll call that a random combinatorial tree. I'll call that a random combinatorial tree. It should be uniformly distributed over TC for some C. Okay, so just to make sure the definition is clear, if C was the sequence 2, 0, 0, 3, 0, 0, this is telling you the number of children of vertex 1, 2, 3, 4, 5, and 6. So that means that the set T C only has. The set TC only has sort of two tree shapes in it. There's the shape where vertex one is the root. So vertex one has two children, vertex two, vertex four has three children, and the other possibility is that vertex four is the root and vertex one is the child. Okay, so these are the two tree shapes that show up in TC. There's four possible labelings of this one. And there's six possible laymen to this one. Okay, so that's clear? Okay, so then here's the first theorem. So for any positive epsilon, there are constants little a and big a. So with the following holds, so if I sample a random combinatorial tree, so the sizes and width With at least epsilon and least. So just to be perfectly clear, what I mean here, I just mean that, so T is a uniform sample from a class T C of this corner. And the sequence C then has to have length n and at least epsilon n of the entry. And at least epsilon n of the entries should be zero. It has to have at least that many leaves. Okay, and I want to, under those assumptions, so c has at least epsilon and zero entries. Under those assumptions, I want to state bounds on the height of the tree. So then for any x greater than one, the probability. The probability that the tree t that's height greater than x squared n is the post a e to the minus little a x squared. Okay, so these are this is a sort of sub-Gaussian tailbound for the height on the order square root n that holds uniformly over any class where there's not too few leaves. Okay? Leaves. Okay, so first, a couple of comments about the theorem are in order. So let me say, what do I want to say first? Well, maybe it's first we're saying that this result is sort of for any fixed epsilon, the result is tied up to constants. Yeah. So for any for any fixed epsilon zero so I'd say up to the values Values. So we don't claim optimal constants in the result. Though in fact, we can say a little bit more. So the proof tells us that in fact, so little a we can take to be of order epsilon squared. So we can be a little bit. Epsilon squared. So we can be a little bit, we can make the dependence in the theorem a little bit more explicit. And so let's say that, so this is so this sort of sub-Gaussian tail is the behavior of the temperature. Is the behavior of the limit tree just the Brownian continuum random tree? Okay, so when I say the limit tree here, what I mean is, so there are many natural ensembles of random trees, which have a common scaling limit, the Brownian CRT. And this object, so it's some compact random metric space whose diameter, which is for trees equivalent to the height, has these sort of sub-Gaussian capital. Has these sort of sub-Gaussian Kelbans. But there are other, there are other, so I use the word the, and it's important that I put it in a very promise here, because there are other models of continuum random tree other than the Brownian one for which other sorts of tail behavior is possible. Okay, so for other CRTs, there's different. There's different so the let's say this ejection also gives access to that so what I mean is Okay, so what I mean is there's some process which I haven't told you yet for how you use this bijection to prove bounds on the height. Okay, and it's there's the more assumptions you want to make about the degree sequence, maybe the more you can say, the more precise things you can say as a result. Okay, so this is one form of bounds. But if I, you know, if I told you that this sequence, for example, had one vertex of degree n minus one and all the other vertices were leaves, then you can. Other vertices were leaves, then you can make much stronger statements. The treatise has to be a star with height two. Okay, so that's a silly example, but the point is that, in principle, knowing more about the degree sequence may allow you to make more precise bounds here. And so far, it seems like for all the bounds we've tried to prove, this is really sort of the right tool for getting sharp results. Okay, so I want to state Another result, not about what happens when you make more precise assumptions on the degree sequence, but about comparing the heights of different trees. And maybe just before I do that, I'll make a third remark rather than state. I'm rather than spending all my time stating results. So the last remark is so the theorem, we call this theorem theorem one, so yields results for For other models of random trees, such as the family trees of branching processes and simply generated trees. And the way that the way that you use this theorem to deduce results for such models of random trees is essentially by saying that if I show you the family tree of a branching process, okay, and I tell you how many vertices, it has n vertices in total. I tell you how many vertices are leaves, how many have degree one, how many have degree two, and so on. Half degree two, and so on, then sort of there are simple symmetry factors that allow me to say that's equivalent statistically in terms of the shape to a random random combinatorial tree. Okay, so because of that, family trees of branching processes and simply generated trees can be used as averages over random combinatorial trees. And so as long as those averages satisfy the conditions of this theorem with sufficiently high probability, then we can pass results from this model to other models. Model to other models. Okay. And so I'll leave it at that. I'm going to now tell you about relationship, stochastic relations between the heights of different classes of trees. But first, are there any questions about what I've said so far? And you're also getting post-stable things? Yes, yes. So Okay, maybe one, maybe one, before I go to comparisons, one more theorem about DNA trees, family trees of branching processes. So this is, let's look at this isn't quite an answer to your question, but it's in that direction. So let's. So let's. I'm going to look at being an atries now. So let's let V be an offspring distribution. That means a random variable supported on 0, 1, 2, and so on. And I'll let TN be. B. So, this is the family tree of a branching process with offspring distribution B, conditioned to have n vertices. Okay, so if B is subcritical or critical and has infinite variance, okay, then Then we can use this injection to show that the height of Tn is little of root n probability. So the previous theorem said that under an assumption about the number of leaves, the height was sort of not likely to be much bigger than the square root of n. But now this theorem says that if you look at this distribution of randomness over tree, Distribution of randomness over trees, then in fact, the height will be much smaller than the square root of n typically. Okay, and similarly, if the expected value of b is less than one, but b doesn't have finite exponential moments. So, in the language from Rick Kenyon's talk, this says there's no way to tilt the distribution to become critical. Tilt the distribution to become critical, okay, then also is n to zero probability. Okay, and these statements have analogs for what are called simply generated trees, which are a slight generalization of branching processes. And those results answer some questions of Svante. So, it was really nice to hear your voice on the Zoom, Svanta. I'm glad you're here. Okay, so let me see. Miguel, I'll go for another 10 minutes or something. Is that okay? Yeah. So, this last thing about comparing height. This last thing about comparing heights is in a slightly different direction. I need one more definition for it. Okay, so a path vertex. So a path vertex is a vertex with exactly one child. And um And so I want to define a class of common real trees. Well, so this is sort of a union of a bunch of the classes I was considering before. So if I'm given integers L which is positive and P which is non-negative, I'll write PLP plus, I guess it's a But I guess it's a set of sub-binary combinatorial trees, L leaves and P path vertices. Okay, so sub-binary means all every vertex has either zero, one, or two children. Okay, so if there were no path vertices, Okay, so if there were no path vertices, this would just be a binary tree. Then, if there's L leaves, then there's L minus one internal nodes. Okay, but path vertices just sort of subdivide edges. And so this instantly implies that there's L minus one nodes with two children. Okay, so be perfectly concrete. So for example, So for example, set B31 has three shapes in it. There's this shape, and there's this shape, and there's this shape. Okay, right, three leaves, two, one path vertex in each of them, and there's like Each of them, and there's like six factorial labelings of that one, and half as many for these two because they have symmetries at one of the leaves. Okay, so um so the third theorem of state Says that if I take any random combinatorial tree which has no leaves P. Panther diseases and And I let B be a random tree from this class BLT. Okay. The height of T is stochastically at most height. Okay, so this says that. So, this says that among trees with a given number of leaves and path vertices, the stochastically tallest one is the one where all of the other internal nodes are just binary. And this sort of formalizes a heuristic which says that the best way to make your trees tall is to make is to have small degrees, and the best way to make them short is to have large degrees. Okay, in the extreme, if you have n vertices, the best way to make your tree tall is to have all half vertices except Tree tall is to have all path vertices except for one leaf at the top, right? The best way to make it short is to have a single vertex of degree n minus one and a bunch of leaves. Okay, and so this somehow says that if I fix the number of leaves and path vertices, then you should make all the remaining degrees as small as possible. And in fact, so this is essentially a consequence of a sort of so it turns out that there's So it turns out that there's a partial order. There's a stochastic, there's a partial order on degree sequences, which yields a stochastic order, partial order on the heights of combinatorial trees. So that's the last theorem I'll say. And I'll say a very brief word about one of the proofs. So I need to define something. So if I'm given a child sequence, See, so I'm gonna, what I'm doing now is sort of setting up a partial order on child sequences of trees with n vertices. So I've got some sequence, and I'm going to modify it in two locations, i and j. Okay, so how am I going to do that? Well, first, I need that ci is bigger than cj. So, what I'm going to do is just even out the degrees. So, if this was, maybe this was a four and this was a zero. So, this corresponds to some node with fourth order and this was a leaf. Okay, then I'll just make this a three and this a one. Okay, so you can sort of heuristically imagine taking a tree that one of the subtrees that was attached to as a child of i and moving it over to now be a child of j. Okay, so this goes to C prime. to C prime the sequence C1 Ci minus one C j plus one all the way up to Cn okay and so here and four says that if E is the random treatment T C and T prime And T prime is a random tree from class T C prime, then the height of T is stochastically at most the height. Okay, so evening out the internal degrees, right, that's what we're doing here, stochastically increases the height. Okay. Okay, and if you combine that with the fact that just suppressing path vertices also stochastically decreases the height, right? If I take a just sort of imagine taking a degree sequence and removing all of the degree one vertices, and that has a very simple combinatorial effect on the tree. It just sort of contracts these sort of paths and edges. Okay, then you can use that to. Okay, then you can use that to deduce theorem three from theorem four. Okay, so maybe in just three minutes, I'll give a proof heuristic. So I should say the results about the very first result I stated about sub-Gaussian tailbounds can also be deduced from theorem three by because effectively all you have to do now is prove Effectively, all you have to do now is prove results for sub-binary trees with a given proportion of leaves, and then you can go that that immediately implies results for other trees with a given proportion of leaves. Okay. Actually, so that's not the way we actually prove theorem one in the paper because we need this other technique, which then yields, is more robust, yields results for families of trees where the, with like the branching prop. The with like the branching process with infinite variance and things like that, okay. So, in a sense, there's two there's two proofs of those initial psychiocentes. Um, and I'm going to tell you about the one that doesn't involve the stochastic inequality now. The basic idea is just a Our geometric decomposition coding sequence plus the birthday paradox, if you will. So I'm just going to explain how you would prove tail bounds on the height of a uniformly random labeled tree. So imagine taking a uniformly random light. So imagine taking a uniformly random sequence in n to the n minus one. So that's going to build me a Cayley tree, a uniformly random label rooted tree with this vertex set, and asking how I can understand the height starting from this sequence. So let's split that sequence up, V into a first piece of length x root n, and then a second piece. And then a second piece of length 4x root n, and a third piece of length 16x root n, and so on. Okay, until I split up the whole sequence. Okay, now what I want to do is if you remember how the bijection worked, as you read along this sequence, you can construct the tree as you go. Each time you see a repetition, that finishes a branch, and then you start growing a new branch. Okay, so we want to understand how the tree height increases as along this process. So you have some initial height at time x root n, then it grows some after this amount of time, and it grows some more after this amount of time, and so on. So if we were going to end up with a final height for the tree coded by the sequence, which is bigger than, say, 2x root n. 2x root n, then something has to happen. Either sort of you get height bigger than, so x root n from the first part, or you get an increase in height, which is more than like x over 2 root n in the next part, or you get an increase in height, which is more than x over 4 root n. X over 4N to the third part, and so on and so forth, right? So I'm just saying what I'd like to say is that, so I can prove a bound on the probability I end up with a tree of height bigger than 2x root n just by a union bound over a bunch of events that have to occur along this sequence. Either you get this contribution from the first bit, or the tree grows at least by this much in the second bit, and by this much in the third bit. Sum up all those values. Bit sum up all those values. Okay, and now the birthday paradox step is just to say about how long it, if you if you imagine that at this time here, you're growing some brain. Time here, you're growing some branch. And think about how long it takes before you see a repetition, which means that that branch gets cut and you start growing a new branch. So, how long do the branches that you grow in a given one of these geometric intervals tend to be? Well, if V is a uniform element of N, then, or my sequence here, My sequence here. What's the probability that the k plus first entry is not among b1 up to bk? Well, that's just 1 minus 1 over n to the k, which will approximate as e to the minus k over n. And now if k has the form or to the i root n. Or the i root n right, then uh and this is like so this is like uh sorry um uh yeah that's right and this is like e to the minus four to the i over root n okay so that means that the sort of the probability The probability along this sequence that the next thing you see causes a cut is doesn't cause a cut is looking like this. Okay, and the probability that there's no cut from sort of time k to time k plus l by the same logic will be about like e to the minus l times four of the i. L times 4 to the i over root n. And so now we're only, what we're asking is not to see a branch which has length like x over 2 to the i. Okay, and so by sort of the geometric growth here is beating the geometric tail. Is beating the geometric tails of the increase in height in a way that these bounds remain semi-ball. Okay, so that's there's a little more to it than that because you have to deal with the sort of maximal inequality and things, but that's the basic story. And the general idea of now looking at the statistics of repeats in this sequence to bound the height of the final tree is very robust. You can see if you have a node of large degree, it'll show up repeatedly very quickly, and that should tend to decrease the height, for example. A match would tend to decrease the height, for example. So, this is really sort of analysis of repeats in this sequence ends up being our whole methodology for all of our cells. Okay, I'm over time. Thank you so much. I'm sorry for going long. Questions? And also, if there's anyone on Zoom who wants to ask, you can either unmute or ask them to chat. The chest. And one, I think, one thing that I'm wondering though is for the stochastic domination that you mentioned. Yes. So do you get some explicit correspondence, some explicit coupling of Greece to the Greece distributions? So that's a. Oh, now I hear an echo. Yeah, so there's a sort of rearrangement that you can do, which basically consists of sort of turning all of the channels. There's a bit of a case analysis of it. Yeah. In one case, you cut all the others. In one case, there's an idea you don't cut, but yes, you get. there's an ideal colour yes you get you get some correspondence between different between the sequences for different degree sequences it always increases the height then that's the that well i mean then we then we prove a lemma which says massively increases the height which in principle means there is a coupling where it always increases the height but the lemma but it passes by an auxiliary lemma which is a little bit implicit so i'm not sure exactly what we haven't reverse engineered to see what the coupling is that you would get for the trees from that Swanta, you've got your hand up. Okay, yes. Okay, hi, Luigi. Can you hear me? Hello? Can you hear me? No. I already know. Can't hear me? Right. So the harder part is actually what happens when you have many more leaves. So the point is that when you have fewer leaves, the extreme is this class. This class B L P, okay, you can where you have L leaves and P path nodes and everyone else's binary, you can think of it as just taking a binary tree and then randomly subdividing the edges. So if you have a binary tree with A leaves and then you subdivide to get a tree, subbinary with And notes. And then roughly, you're subdividing each edge n over 2k times. So there's got 2k edges here, and you want to end up with a tree with n vertices. So, and I'm imagining here that k is small compared to n. So that effectively just gives you a sort of deterministic, quasi-deterministic stretch of the tree vertically. So when there's very few leaves, the sort of there's very few leaves the sort of the picture is that you would have height about root k times n over k but when there's when there's very many leaves so you can imagine a model where you had a node which had like n minus the degree n minus liblo of n and then a bunch of path nodes for example and now there's sort of an extreme value problem you have a bunch of little trees growing off of a central hub and you want to know how big any of them are And you want to know how big any of them can get. So, in the model where the tree is very star-like, the picture is much less universal. But in a model where it's very clock-like, this sort of more or less tells the story, I think. So, Smart has a question. Yeah. Yeah, okay. Okay, so well, so this was a very interesting talk, I think. First of all, I want to comment as. I think. First of all, I want to comment, as I'm sure you know, Luigi, and maybe not all others, that this is very similar to a situation with random graphs, where one has the model I like with a random graph with given vertex degrees, precise as your coordinatorial random tree. And then there are other models that can be regarded as mixtures or randomization or whatever you want to call them of them when you take averages and can get results from this. Can get results from this more refined result. So I see that this model is, therefore, has great potential of being useful, giving more precise results. So I wonder, I guess that one thing you want to do, maybe have done, is to prove convergence to the continuum random tree under suitable conditions involving moment on the degree distribution and so on. So, is that something you, and maybe also come? And maybe also convert into stable tree under other conditions and so on. Is that something that works well with this model? I don't know if you can read him this faster. Yeah? You can. Okay, so if you. You can. Okay, so sorry, so I didn't realize. Yeah, so there's a doctoral student of Nikolai, Afiovanovinovi, found more or less the same bijection independently. So he was his goal in developing it was really to understand the sort of pea trees and inhomogeneous random trees. So he has a Random trees. So he has a preprint coming out which proves convergence results to various families of Leby trees under various assumptions on the degree sequence. So that work I think is quite nicely complementary to ours because my sort of first priority in this was really what things can be said without assuming any that you're in some sort of so non-asymptotic results where you're not assuming that the degrees are well behaved or anything like that. And that's really the focus of our work. And then Blanc and Adie's work says when you do make those These work says when you do make those assumptions, what can you get out in terms of convergence? Yes, I realize that. And I appreciate that aspect of your work that for height, you don't need any assumptions because it turns out that the bad cases when you have irregular degrees or verses of large degrees, they turn out to give small height, as you have shown. But that leads me to another question in the opposite direction. Question in the opposite direction. Suppose you are interested in a width instead, then you certainly need some conditions to rule out such bad cases. But can you use this model to prove some results under reasonable assumptions for the width? So far, I haven't found any way to make this set of tools useful for studying the width. The only way it's useful is this. Useful is this. If you can sort of statistically, it turns out that the smallest labeled leaf is like a uniformly sampled leaf. And so, you know, if just knowing about the first branch tells you what sort of quite a bit about typical behavior in the tree. So if that first branch is very short, then that sort of means that most nodes are very close to the root. And that means that the width has to be large, actually, because Actually, because if most notes are fit in a small number of levels, then one of the levels has to be large. So we've used that idea, but that's a sort of indirect route, and it's the only thing, only way we know how to use this sort of tool for the studying the width. Okay. Okay. Thank you very much. Very interesting talk. Thanks. Thanks. Well, thank you, Rudy. Thanks very much. Thank you, Reggie. Thanks very much.