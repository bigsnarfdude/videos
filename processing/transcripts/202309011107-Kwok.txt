Martin Gandler on time parallel uh time parallelization, parallel and time balance. This is another one of these communities where there are multiple points of view. There are the people with an ODE point of view, and then there's a lot of fruitful interaction with people from the linear algebra community. So it's one of these results that try to give the same methods, you know, both perspectives, and see what This and see what useful information we can get out of it. Okay, so parallel and time methods. So, this is a linear ODE system that we all know and love. It could be a discretized PDE or whatever. And then when you do time stepping, well, you can do one step at a time, or you can write all your time steps down and in your vector of unknowns, and then you get this lock lower triangular Lower triangular linear system. Okay, so to be concrete, if you use backward Euler, then this phi zero here is going to involve a linear solve, one per time step. So obviously it's blockler triangular, so you just do for a substitution and then you call it a day, and that's perfectly fine. It's order end in terms of solution time, or n in terms is the number of blocks, but of course now your algorithm. Blocks, but of course, now your algorithm is completely sequential. Okay, so there's no parallelism there. Now you have a giant cluster with many, many thousands of processors and you want to do something useful with them. So you want to parallelize the solution. There's parallelism in space, obviously. Here I've deliberately cited very old papers, not because the field hasn't moved on it. hasn't moved on and these things haven't improved in the meantime. But it's just to show you that this is starting to become mature technology. There's a series of conferences that is still going strong, but this is like starting, this started in 1988. It's been what's forty years now? Thirty five, forty years. So these preconditioners have had time to mature. Have had time to mature. And if you look at PET C now, FETI DP are implemented, BDDC balancing going on. It's all in there. So it's quite mature technology. There's obviously also multi-grid, which goes back even further. And then there's the geometric version, the algebraic version, etc. And it's been you know, we know what to do for for many different types of problems, problem multi-wave, etc. Problems, not multi-wave, etc. Well, it's not to say that there aren't still research problems in there, but you know, there's been quite a bit of progress, okay? But this is in space, okay? So now, if you think about a time evolutionary, a time-dependent problem, you have a dimension in space, and then sometimes because of CFL or whatever, you know, you want to integrate over many, many time steps. At some point, if you have too many processors, If you have too many processors, you can only squeeze out so much efficiency by parallelizing in space. Time is still your bottleneck. So more recently, there's been a lot of interest in looking at parallel and time methods. Can we do useful work by working on different parts of the time interval in order to get more efficiency out of it? So initially, it's not quite obvious how one would do this because, you know, One would do this because you know, causality, you know, you need to know your initial value in order to look at, you know, to see what happens later on in time. Okay, but there is one of the first methods that shows that parallel in time can actually be useful is the para-real method, which falls under the multiple shooting item in this list. I'm going to say more about that later, that's why I didn't put many. About that later, that's why I didn't put any references in there. Multi-grid in time, I'm also going to say something about it. And of course, there are other ways of parallelizing in time. Some people in the audience have to give a word from them. So this is obviously a very incomplete list. I'm not trying to cite everybody, otherwise we'll be here all day. If you want more references, there's a full webpage on parallel and time methods with references and a number of papers and citations over the Papers and citations over the last twenty, twenty-five years. Just to say that this is not only for initial value problems, there's also a lot of interest when you have PDE-constrained optimization problems where the PDE is time-dependent. And there's a lot of work on there as well. And if you want to survey, there's a survey up to 2015 by Martin Gander, a more recent one by Venom and Jacob Schroeder in 2020. And then here, In a 2020. And then here, the motivation is really real-time computation. Yes, we have an order-end algorithm, which is the obvious Ford substitution one. But if you're trying to predict the weather for tomorrow and you have a lot of processors lying around, it's better to use 100 processors and have a speed up of five, so efficiency of five percent, than to for the forecast computation to take more than twenty four hours, in which case your forecast is completely irrelevant. Your forecast is completely irrelevant. So you're not going to get as good of an efficiency as parallelism in space, but it can still be useful in some circumstances. So as I mentioned, para-real is one of the methods that really kicked off a lot of interest in the past 25 years or so. And it falls under multiple shooting. So what do I mean by that? So suppose I want to integrate my system. Suppose I want to integrate my system from 0 to capital T. So the point of view of multiple shooting is to say that if I know my solution at some intermediate time points T1, T2, T3, etc., then I'll be able to figure out everything in between. I'll just integrate from the known solution. And then this will be completely parallel as well. And once I know something about T2, I can integrate this bits completely. You integrate this bit completely independently of the first bit. So the big goal here is to figure out what the solution is in these intermediate points. And the criterion here is basically one of continuity. You want what gets propagated from zero to match what you used to start at T1. So here my propagator is F. It basically takes the initial value from T and I minus 1. From t at i minus 1 and into the value at t1. So you want the propagated value to be equal to just like that. Oh, by the way, here, I don't index the f's here depending on the interval, but obviously if you use variable time steps or there are different right-hand sides, then these f's will all be different. But in order to write in the notation, I don't do anything with it. Alright, so how does the para realize? Alright, so how does the para real method work? Okay, so we have this decomposition of the interval in time. So you need to have some initial guess for the intermediate point. So the idea is to, in order to speed up the process, what you want is, in addition to the integrator that you're using for your exact, for your discretized solution, you also want something. Solution, you also want some sort of a coarse approximation, G. And that one doesn't need to be very accurate at all, but it does need to be cheap, okay, because that's the one that we're going to apply sequentially. So you use the course propagator G, you propagate the initial values to all the TI points. And from then on, you can integrate very accurately in parallel over each of the intervals. So F is Over the intervals. So F is the expensive one, but you can do it in parallel. So you get however much speed up as you have processors flying around, essentially. And now, of course, unless you're very lucky, the solutions are not going to match up on the boundary, so you need to correct. And the correction looks a little bit like a deferred correction. You see here, if my method has converged, and this value at iteration mu plus one matches the value at Mu plus 1 matches the value at Î¼, then these terms just drop out, and you just have the exact matching of the conjugation. But in the meantime, before you get to the solution, these corrections are going to be non-zero. So the way you generate the next iteration is by a recurrence you see on the i on the interval. So that work is sequential. Okay, so you just step over until the end. But hopefully, this is cheap enough that it's not going to be a significant. Enough that it's not going to be a significant amount of the computation, and that now you have an updated iteration, iterator, and just continue on to a convertion. So, if G has negligible cost, then the speed up is going to be n divided by the number of iterations you need to take in order to get to an acceptable level of accuracy. Okay, so n is the work that you need to do if you integrate sequentially. If you integrate sequentially, then you have the number of iterations that we use. All right, so if you look at the literature, typically, you know, when people do speed up analysis, they mostly neglect the time spent in the course propagator, assuming that it's cheap enough. But if you have many, many time steps, because that's sequential, that's still your bottleneck. So it's not going to be true if your integral. It's not going to be true if you're integrating over many, many intervals. And that poses a problem in terms of scalability. You can't just slap on more intervals when you have more processors. So what do we do? Well, the bottleneck is the coarse problem. So can you apply pair recursively to solve that in parallel? Yeah, so that gives you a three-level algorithm. So even the Romans back then knew that two-levels sometimes smoke. Knew that two levels sometimes do. You need a third level. This is the Pont Ugal at an aqueduct in the south of France that they use to transport water to the nearby Roman city of Ni. Okay, so here is my version of the Roman aqueduct. Okay, so I have a very fine discretization here, and then I have intervals. Previously, there's only one level of intervals, so it will be these. So it will be these, and then within each of these, I have the fine propagator that does the fine integration over many time steps, and then I have the G that integrates, you know, that takes very few time steps, that is not as accurate. And then I've written down the same recurrent as before. The main difference here is that I don't use G to step from here all the way to the end. So each of these G's, I'm the only. These g's, I'm only going to apply it to basically a small number of sub-intervals up to here, for example. Okay, so this pararial is going to be completely independent from this pararial. Okay? So I do that, and I also don't iterate it. This is not an in-reliberated iteration. I only do it once, and then I'm going to get some final answer here. Final answer here: if you want to, you two, two, etc. And then now I pretend that, so now I run the second parariel, this time on the very coarse, the ultra-coarse decomposition, and then it looks also exactly the same as a para reel, except that I have replaced the fine salt by what I just earlier. Okay, so the way I wrote it down here is: you know, one pair of your propagation up to One period of propagation up to here that is in parallel, albeit with slightly less parallelism than before. And then whatever I get after that one single para-real propagation, I throw it up to the ultra course and then I run a para real on that. So you can count up the number of integrator applications per iterator. Per iteration of this method. So if I assume the cost of FG and H or C F, C G, and C H, then the total cost is this. So the N, M, and L means, you know, I have N fine time steps in one intermediate interval, M intermediate interval in one course interval, and then L for courses interval. So if you want to look at the If you want to look at the sequential, like if I don't do any parallel at all, I just step through sequentially, I have n times n times l tiny time steps, and that will be the cost of the completely sequential one. And this is the cost of one iteration of para Rio. There is the number of iterations missing here, so this would be multiplied by the number of iterations. See that this is what the speed up is going to be. If I neglect communication. Is going to be if I neglect communication cost. So, this is something that this is one possible way of generalizing para reals with three levels. It's something that I wrote down a few years ago when I was participating in the Copper Mountain conference on multi-grid. And basically I just wrote it down on the back of an envelope before I started analysing it. So I the first question is, does it converge to the right solution? Converge to the right solution? And the answer is yes. For any choice of FGNH, three-level period converges to the fine solution at most this many iterations. So convergence to the exact solution. Zero error other than random. It's this many. This many is way too many. It is not useful. You're not going to get any speed up out of that if you're using this many iterations. By the way, two-level parallel has the same. Level pariel has the same thing. It converges in n iterations where n is the number of intervals. But if you're using n iterations, then you might as well step through everything. You're not gaining anything. So the whole point here is to choose your course representations carefully enough so that you actually converge to something acceptable with fewer iterations, much fewer. Okay? So that's the idea. So that's the idea. And so now the other methods that you can parallelize very well is of course multi-grid in time. Everyone knows multi-grid now. This is not the only way of parallelizing it in time. There are other multi-grid approaches, but I'm mostly going to be concentrating on what is known as the M-grids method by Rob Galbru and collaborators. And it's essentially an algebraic multi-grid method. So the two-grid version is: I have my finite. Version is: I have my find and my course grid. I fix some blocks that I'm going to relax on. So, for example, this will be a block that I apply, you know, whatever relaxation that I want. And then, you know, between blocks is where the residual is going to be. So I take those as my course representative, as my course point. I restrict to the course grid. I solve the problem on the course grid, for example, with LU. Grid, for example, with LU, and then once I have that, you know, I have the correction that I interpolate back at the correct. Okay, if you want to do multi-level, then it will be here, actually. Let's next slide. If you have three levels now, and then you apply the core solve recursively, you solve the problem of the coarse grid on the intermediate grid recursively. Multi-grids, you keep cycling through the grids like this. And every point here. This and every point here involves a choice. Okay, so there are many choices available. Luckily, there's a parallel implementation X-braid where you can play with all these different choices of the local relaxation, how many times you do the relaxation, how much do you course in, etc. How use like we're between levels, etc. etc. Okay. Now, this is the way we usually write We usually write m grid for a linear problem. Everything is in terms of correction and residuals. For nonlinear problems, there is the full approximation scheme. You insist on working on actual solution values instead of corrections. So there are some extra terms here relative to the correction embry on the right-hand side, so instead of just this. Side, so instead of just this, you have this right-hand side, and then when you correct, you know, you actually computes the correction for you. Okay, so if you write it down this way, you can apply it to nonlinear problems without finding the wrong solution. They will converge to the correct nonlinear solution. And when the problem is linear, then it is very clear that it's equivalent. There are just some extra terms. This distinction. This distinction turns out to be important. So, fast forward to 2019, when I was at Ikea, I met Stephanie, I was telling her about this three-level parallel that I just wrote down on the back of the article. And then she drew some pictures like, this is equivalent to endpoint. And then she explained, you know, here are the steps, here are some arrows, you know, the data is going like this. And then I'm happy, I'm convinced that this is equivalent to M. This is equivalent to m grid, and I was easily convinced because we already know that for two levels there isn't groups results. Now, of course, one of them has many, many choices to make, and the other one has like one recurrence that you write down. So, the recurrence you write down is equivalent to an M-grid for one particular choice that you make. It can't be equivalent to all of them. Okay, so it's equivalent to the one where you just do one F-relaxation. One F relaxation, and you cycle through this using a V something. Just go down. Okay? And of course, you know, you can't be equivalent if you don't start at the same point, so you also need to fix some initialization before you can prove equivalence. And there are two ways of doing that. You either show that they generate identical iterates, or you show that they have the same error property. Both proofs exist in the literature. One of them is written. In the literature. One of them is shown by Martin Gambit. So you see, it's funny because there's a proof of equivalence in 2007 before Mgrid was invented. Okay, but that's because the Mgrid paper that systematically generalizes multi-grid to time, to initial value problems, didn't show up until later that there was already some sort of multi-grid interpretation by the Gandharan problem development. When the gambling function of all appeared. There's more stuff. If you use FCF relaxation, there's another way to interpret it as parallel, still two levels. So now, I gave a talk about the analysis of convergence for three-level para-real, and then Martin heard the talk and said we should really write down this equivalence proof properly. And that was a nightmare because there are a few difficulties. So, first of all, they don't work in the same space. At no point in Parallel do I mention the find solution. So, you need to fill in the find solution yourself in order to show that equivalent iterates. And then there's a question of how do you date it where you started the wrong point, like nothing is going to be equivalent. The second one is: n-grid works with. The second one is mbrid works with correction, whereas para-real does not work with corrections. It's really solution values. So that turns out to be the crucial point that allows us eventually to prove equivalence. It's to go through the FAS, the full approximation scheme first, because otherwise there's just no hope. You start at the wrong point. You calculate all the corrections. There are too many corrections that accumulate. We just couldn't think of it. And then also it turns out that's the way the blocks were decomposed. The way the blocks were decomposed relative to the picture in Gandrin Famili is just off by one. It turns out that you can do it either way if you date, the fine solution carefully enough. And that works on two levels, but on three levels, it didn't work. Only one of the versions can work. So here's a theorem. It's basically one of those things where once you know the answer, Answer, the proof fits in like half a page. And this is like actually shown on an eight-page paper, the DD proceeding. So it's really not that hard. And it's a lot of details, but it's mostly notation and writing. So I'm not going to show you the proof on the slides. There is no points, but it's funny how things that looked obvious when you write them, you put them down in little arrows. If you don't get it exactly right, it's kind of. Exactly right, it's kind of not so easy to have a formal proof where you can be sure about everything. Okay, and then oh, there's another thing which that differs between two and three levels. Usually in multi-grid, when you course it, you do some relaxation before you course, and then when you interpolate back, you do some relaxation. In two-level multi-grid, M-grids, and two-level para reel, you still get the same equivalent. You still get the same equivalence because somehow para-real ignores what you do in the interior of the intervals. So you can keep the post-relaxation, it just does nothing, essentially. Whereas in three levels, that's not true because if you go inside, you're filling in the intermediate points, and the intermediate points are used for the fine integration. So somehow there's, if you touch it twice, if you make up. Touch it twice, if you make updates twice, one's going up and once going down, then that's going to affect the equivalence, so it's no longer equivalent. It gives another method. I'm going to show you some curves on what it does. I mean, so how much time do we have? A couple of minutes. Oh, okay. Alright, so. Four a couple. Four minutes. Okay, thanks. So, once you have the equivalence for the FAS, you can, you know. For the FAS, you can subtract consecutive iterates to see what the correction form is, and you can work out the iteration matrix or what they call the error profit regulator. And then this is what people like in the multi-thrid community because then they can take powers of this thing and look at the norms and then we get convergence estimates. And then if you look at what the iteration matrix is, it's very classical. It's very classical. You know, you do a relaxation on the fine grid, and then you calculate the residual, and then you go up to the coarse grid where you restrict, and then you apply your coarse correction, and then you come back. And then the coarse correction is recursive. It has exactly the same format. Okay, so all this can be worked out, and it has been worked out in that very short paper. So now the question is, why do we care about these equivalence results? It's because you get different perspectives, and then, you know, it's Perspectives, and then you know, it's beneficial to both points of view. So, the first benefit is you can analyze n-groups differently. The classical way is the linear algebra way. So, you look at the matrix and then you look at the norms of the powers of the iteration matrix, etc. And then for linear problems, you can actually get rather tight bounds out of it, but there are some additional assumptions, like the propagators at the different levels need to come in, otherwise it's hard to. That there was. It's hard to do the analysis. Whereas the parareel analysis classically comes from the ODE community. So they have very different assumptions. They talk about Lipschitz constants and then truncation errors. They try to bound things like this. So for example, they said the coarse propagator has to be Lipschitz continuous with the constant beta. This is essentially the truncation error if you think of f as the continuous. Error: if you think of f as the continuous solution and g as the discretization. So that has to have a certain order, p. And then what they do afterwards is they take this recurrence, they subtract the exact solution, they rearrange the terms, and then they figure out the recurrence for the error, which is sort of a double recurrence. There's a recurrence in u and a recurrence in I. But then they solve that by writing down the generating function, for example, something like this, and then they substitute it, and then. And then they substitute in, and then there's some algebra, and then at the end, you get the error in the transform space. And then, if you expand it in the scalar series, the coefficients give you the actual errors. So that's how they do it. You can do the same thing for three levels. Very hairy, very, very hairy. But you can get a thing like this. So, in particular, you get these finite termination criteria by looking at the. Termination criteria by looking at the solution and say this is a polynomial in this number. So a polynomial means it has to stop in the expansion at some point. And that's when you know that the error goes. The other benefit, so in the other direction, how we get para-real benefits from this mgrit insight is now mgrit is a lot more general, there are a lot more knobs you can turn, so now you all of a sudden have all these new methods that you can try. All these new methods that you can try and see what happens. So, for example, if you put the relaxation by the post-relaxation back, which did not exist when I wrote down my naive parareel, three-level para-real, then you see that there's an extra step, so there are these half steps, and then at the end there's this extra step, which contains another propagation by G. So, this here, this is the residual, so this doesn't need to be computed. Residual. So this doesn't need to be computed twice. So each iteration requires only one propagation by F over a short interval, but it requires two propagations by G. So that's why there's a two here. And then now you can compare how well throwing in that extra propagation, how that affects the speed up. On the one hand, it could lead to fewer iterations. It could lead to fewer iterations, but on the other hand, the iterations are now more expensive. So, let me look at this diffusion equation with periodic boundary conditions. It's 1D, but it is known that this is a hard problem for parallel and time because there's no dissipation and the signal always comes back once it leaves on the right and comes back on the left. So, it's very easy to get wrong. And in fact, if you just run two-level parallel, Two-level parareel, it is well known that if the diffusion coefficient is large, then you know it converges very nicely. As you get more and more dominated by a vection, it sort of plateaus and you get no progress until the last iteration. So this is over eight intervals. And then the last inter so essentially you get the cost over sequential is one, so you get no speed up at all. Okay, that's one of those things. So let's run three levels. Okay, so just look at the number of iterations for now. This is the two-level one. Over eight intervals, it should converge to the exact solution in eight iterations, and it does. And then if you look at the three-level version, there's one without the post-smoothing and one with the post-smoothing. So this is the one that is slightly more expensive for iteration. It reduces the number of iterations a lot. Of iterations a lot. But now, if you take cost into account and you work out everything, so again, you get no speed up in terms of cost from the two-level one, but the three-levels one now is switched. So, actually, it does reduce a number of iterations when you put it in the post-relaxation, but not enough in this case for my particular choice of time-stepping scheme to be worthwhile in terms of wall clock time. Of one clock time. And then the other thing here is: here you actually get speed up for three levels. This is 0.5 and not 1, which you can't do for 2 levels. So there's some merit to understanding better the 3 level method. And then there are different variants that you need to study. I mean, this is one particular problem. I mean, obviously, your mileage is going to be problem-dependent, and you can try out or not or see what happens. I'm basically done. I'm basically done. This is the short eight-page paper that has been accepted, and it's now posted on the Domain Decomposition Conference website. The actual Springer version will come out a year after, because it always took a long time. Ongoing work studying what happens for wave and advection-dominated problems, because that is known to be hard for parallel type methods. And further applications for example, Jemima in her talk. Jemima in her talk shows something very similar to what I mean from a data assimilation application, so maybe we'll talk something similar. Thank you. Thank you very much. We won't go on forever, but we should ask some questions. Thanks, the question. So, yeah, I've asked Stefan Gutt all this question, so I'll see if you give Stephan go to all this question, so I'll see if you give me a different answer. So I do time stepping and I solve the question diffusion equation, I know this adaptively, and I know pretty well that's a very healthy time stepping, very small time steppers to start with, diffusion dog and anything. Then you have a regime, you didn't go very far in time, by the way, then you have a regime with parabolic smoothing, and then maybe a wave propagation regime, and then you go from steady state. How does para real kind of work? So you pick time steps rather than uniform ones to start with so that you kind of reflect the real behaviour with time. So for a thing like this, my understanding is correct is that you don't have to have units on time steps, but you need to decide on your time step ahead of time. Is there any way you can actually, because with propagation in time, using time step, you can do a lot of error estimation in time. Recognize whether you need a small step to carry on or a large number. But how can you incorporate that information to it? I'm not sure. If you have a model for the error for a particular time step size, then maybe you can incorporate that into your system and try to do paragraph. But see, the problem is that you're trying to do useful calculation of different chunks at the same time, right? Chunks at the same time, right? So you're trying to do calculations before you know what the solution is. Whereas the adaptivity requires like you're doing a posteriori error estimate, so you need the solution first. So maybe you can do some sort of adaptive mesh refinement, but once you have something at the end and then check the error, estimate and it's not good enough, then maybe fine then. Uh yeah, I I I haven't really thought about I haven't really thought about that too much. I know that for uh optimi instead of optimization on the PE constraints you have an adjoint equation anyway, so it's really hard to do that in both directions. It's a good question, I don't have a good answer for that. He was pretty dismissive of time, step basically says you're living in the dark age. Basically, says you're living in the dark ages, then we'll figure it out, so speak. Wouldn't the answer be: we're going to live with very small time steps in order to let the hardware do its thing for us? Computers are so beautiful these days, you don't need to worry about doing proper adaptive error estimation. Isn't that what Hackbush said, but for elliptic problems? Like, just throw in a ridiculous. Problems, like just throw in a ridiculously fine grid, and then we'll use like hierarchical H matrices, and then the problem will go similar viewpoint. And it wasn't evangelical at all, which I really liked. Everybody's done. So, um final thank you to everybody then. Final thank you to everybody then. So I hope everyone has a safe journey back to wherever you're going. So I'm going to wrap it up. I hope you want to stay, John. No, I think just thanks all for coming and having spectrum. Yeah, yeah, just if you're gonna get Alice, we've got one. Well, I can't use it with someone else. What's the question? Yeah, I don't know if it's too much. But I also study. No, I think it's a good idea.  It doesn't approximate the final thing just by the time. And so it really, really. So what we ended up doing is the first thing is all the information back from all the people on the characters here. And one way to do that is to say, like random credits. And so that is one thing that I've been finding and doing is that But special websites is not because it's being done. But it's just a second microwave normal fine level. But I think that I'm not sure. It's still not accurate enough. So you have to kind of add some correction terms that you can get by the Taylor series approximation to get like one order extract. And if you do that, then you can get fast over and smaller. So we find that it's much, much more sensitive. What you do on the corporate is much more sensitive. Then there's some work that can also or standard medicine find level method of mind. Still use stimulating. You haven't really found the family way. As long as you're then correct to stimulate granj. And um we haven't found a way to do method of lines of all files. That's what's just linear extra actually 2D codes as well. But then now Oliver gets started to get some results on the non-linear function. And theorizing the non-linear equation. Theorizing the non-linear equation I was still using also starting more participants or something like that.