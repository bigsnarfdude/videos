It's very happy for me to here to present our work on this project called OpenCAE Poro. This is mostly software project for simulating multi-phase and multi-component POROS media flow. To make my topic more aligned with the theme of Aligned with the theme of this conference, I will show a little bit about the linear solvers we developed for structured grid problems. Okay, so here's my outline. I will first introduce the project itself and then show a little bit about the parallelization efforts we do to improve scalability and efficiency of our code. And efficiency of our code. And then we introduce our collaboration with Professor Jing Chao Shu's group from CAUST on AI-enhanced methods for this kind of problems. And then show a little bit about our future plans. Okay, so for the introduction, the background for the forest media flow, we have various applications in energy, carbon neutral, environment. Carbon neutral environment protection, so on and so forth. So, our goal is to provide an open source software package for these types of important applications. So, we want to give a quite general framework for simulating porous media flow. Okay, so since this is a Since this is mostly a multi-phase problem, apparently we will deal with very complicated interfaces with moving or free boundary, free boundaries. But in our simulation, mostly in our model, we are dealing with field scale models. So the spatial resolution is mostly like meters. So we cannot resolve. So we cannot resolve the minimeter or even narrow meter scale interfaces like the lab scale simulations or experiments. But instead, we are looking at quite large scale in spatial part. But still, we will see a very complicated interfaces which is moving in time. Okay. Okay, so our basic model is just mass conservation combined with flow equations. For the moment, we are mostly looking at the linear Darcy's equation, then the EOS equations, capillary pressure models, so on, and so forth. This is the basic models. And then to use it in different applications. It in different applications. We sometimes need like a thermal equation to introduce ISO, I mean a non-constant temperature case and combine with the geomechanics, for example, with the Biot equation and absorptions and desorptions of the components to the rocks. To the rocks. Sometimes we need to use non-Dars equations. And sometimes we need to couple the Rosmedia flow with free flows like Navy Stokes or just the Stokes. And as I mentioned, we are looking at several application fields and try to make a quite general framework. So this is a framework for the So, this is the framework for the problem setting. And here's the model equations. The model equations are quite standard, the mass conservations for different components and the Darcy's flow, the linear Darcy's relation between the velocity of the fluid and the pressure head. And then we have saturation relations and Relations and composition equations. So, also, we need to use space balance equations coupled with the equation of states. Okay, so this equation is very standard and also quite general. And when we are dealing with different applications, so we need to couple them with. With other equations. For example, if we are dealing with porous media with fractures, we sometimes need to use multi-continuum models. And sometimes we couple with free flow equations like Navier-Stokes. Sometimes we need chemical reactions to deal with. To deal with the chemical reactions between the fluid and the rocks. Okay? So we developed a software and we tested with benchmark problems from SPE, the Society of Petroleum Engineers. So this is some sample results between our software and commercial software. And commercial softwares, and you can see that the results are quite close. So, in this package, we are developing mostly we have several numerical methods included like impact, which is pressure implicit and other variables explicit, fully implicit, which is using implicit coupling. Uh, coupling for all the primary variables, and also AIM is an adaptively implicit method, which we will determine where we need implicit variables and where we don't need to solve them fully implicitly. So, for the discretization part, what we are doing is quite classical. Classical, basically, we are using adaptive mesh refinement and finite volume, lowest order finite volume method for the space and backward Euler for time, Newton's method, and Kryloff, multigrid, and then solve the equation of states by cosine Newton. Yeah, general, basic stuff. General basic stuff. Okay. Then so, then what we are trying to do now is try to improve the efficiency of this code and improve stability. How to improve efficiency in this context? So what we can do is we can use better modeling. Better means smaller and more computable. So what we can do is we can do So, what we can do is we can do the upscaling and to make our original model smaller. So, the typical model in the industry we are currently dealing with is a size of like 10 million to 100 million size, the grid cells. Then, to make it more efficient, we can do the upscaling. Do the upscaling. Okay. And also, people are dealing using reduced order models to make the model more computable. And then we can use better solvers. For example, we can use more efficient parallel algebraic multi-grade solvers to deal with the linear equations. Because if we are using fully implicit methods or even with adaptive implicit method, With the adaptive implicit method, the linear equation solver takes most of the computational time in our simulation. And then we can also do the better implementation. We use better computers to improve efficiency. For example, we can do heterogeneous computing to use MPI coupled with CUDA to improve the computational efficiency. Okay. But there's still But there's still many challenges we are facing currently. For example, we have model uncertainties, even with very large geo-models, we are still facing a lot of uncertainty. For example, the porosity and permeability coefficients are difficult to obtain. And usually we need to. Usually, we need to run our simulations many, many times to adjust the coefficients. And also, we want to have larger throughputs for the simulation. And also, there's a trend to use digital training in the developing fields, which requires very fast response, even real-time response. So, for these kinds of challenges, Challenges, even with high throughput supercomputers, it's very difficult to handle them. So in the end of the talk, we are trying to use AI-enhanced numerical simulation algorithms to improve the efficiency of this workflow. Okay, so maybe the AI-enhanced means we use. Enhanced means we use AI algorithms to improve efficiency to get the model. And eventually, we need to do some full-scale simulations, but we want to reduce the number of simulations that we need to do. Okay, so I just very quickly to go through the motivations and the background for this thought. The background for this software project. And now I show a little bit our efforts to do the parallelization. Okay, so this is a workflow for one time step of numerical simulation for this porous medias flow. So basically, what we need to do is construct a Jacobian matrix J for the Matrix J for the fully implicit method, okay, or AIM adaptive implicit method. So then we need to solve this Jacobian system. This is a very large and coupled algebraic linear system. And after we solve the primary variables, we need to solve the equation of states to get secondary variables. So there we need to solve many, many nonlinear systems in each. In each grade cell. So, this part of the computation is mostly local, and here we need to solve global systems. So, in terms of parallelization, this part is challenging because we are assembling the global Jacobi systems and we solve global Jacobian systems. We need to find efficient linear solvers and preconditioners. And the preconditioners. This part usually takes more than 17% of the CPU time. And then in this part, we need to solve many nonlinear algebraic equations, but they're small and local. So this part is usually considered easy for parallelization, but unfortunately, But unfortunately, this part is very difficult for load balancing because those nonlinear problems, it's very difficult to balance the node. Balance the load. Okay, sorry. Okay, so here I list of challenging problems, which is not important. Okay, then here I show Here, I show some demographs where we have four wells where we inject water or like CO2. And then if we have very, how do we say this is like uniform or constant permeability fields, then the Then the interfaces are quite simple. But if we have a heterogeneous medium, the interfaces are difficult to obtain. And then once we have very complicated interfaces, the nonlinear problems we are solving at each time step is very difficult to converge. Okay. So the So, the multi-phase flow for anisotropic and heterogeneous medias will lead to very complicated flow fronts and increase the difficulty for solving the nonlinear systems. And we are trying to obtain a better initial value, initial guess for the nonlinear equation solver. What we can do is by standard. By standard practice, we can either reduce time step size to make the time step smaller so we can use the previous solution as a good initial guess, or we can do an extrapolation, or we can solve some approximate problems to give a better initial guess. For example, we can use two-grid method to solve a cost-grid problem and to obtain an initial guess. Initial gas. So these things we all do. Plus, we introduce a new method called adaptive coupling DDM, which is idea is the following. We have a front, which is a two-phase interface. This is the water, and this is oil, for example. And since the interface, we are sharp interface, we are introduced. Interface will introduce problems in our parallelization. We try to couple these boundaries where the interface goes through to for in the domain decomposition or additive Schwartz method, then we get initial guess, better initial guess for the nonlinear problems. Okay, so here for Here for domain decoration method, what we are trying to do is not the regular domain operation for the linear system. What we do is we solve each domain separately. We don't communicate with each other. We just solve the local problems independently and to obtain an initial guess. Okay, so this is a very easy way for obtaining the initial solution. The initial solution. But since the nonlinearity around the list interface is high, if we do these three subdomains separately, the performance is not good. So we try to couple them together and to get this so-called coupling DDM. Okay. Then these colors basically means how do we couple. Basically, it means how do we couple the parallel processes together? Okay, so this part is mostly for parallelization. And we show the scalability of this new method compared with the standard for the implicit method. So, if I say FIM, which means we just use a parallel solver to solve this FIM discrete system. Discrete system directly, and this ADDM is just using the previous method I introduced to obtain a good initial guess. And we can see for a large size problem with more than 12,000 CPU cores, we can obtain 50% speed up. And we can also see the scalability is also improved. This curve. Is also improved. This curve, purple curve, shows the weak scalability for the method. Compared with FIM, it's much improved. Okay, so if we combine the methods I just introduced with our simulator, what we can obtain is a This kind of weak scalability performance, we can solve a maximum of 3.5 billion unknowns with more than 3,000, actually 350, 3,500 CPU cores to maintain relatively good weak scalability. But if we look at these shaded areas, shaded bars, that's the linear solvers. That's the linear solvers. Okay, that's the linear solvers in our simulation, and we can see where as we increase the number of CPU cores, this part still increases. What we want to see is a flat curve where we have perfect weak scalability, but for the moment, we still don't have it. So, what's the problem? The problem is here in the linear solvers, we use Here in the linear solvers, we use two main components. One is the edge brick modigrid solver for the pressure equation and a block IU method for the global system. And these two parts contribute to this increase of CPU time, war time. Okay, so to improve parallelization parallel scalability, Parallel scalability, we need to improve these two parts. Okay, one way to improve scalability is instead of just solving the pressure equation with AMG, we can do a monolithic AMG which deal with all the variables together with AMG method. Okay, so with this method, we can slightly improve the scalability. Slightly improve the scalability. Okay, for here, I show a very small problem using one CPU core up to 64 CPU cores, and we can see the parallel speedup is better with this BMG compared with the previous linear solvers I used in our numerical test. Okay, but this is still not enough because. Is still not enough because the scale of the machine is still small. And then we try to improve the numerical performance with the multi-grid taking the structured grid into account. Okay, so if we take the structured grids into account, although our numerical grid Numerical grid is not exactly the grid structures, the structured grid as we consider in numerical analysis. The topologically, the method, the grid still has some certain structured information. So here, what we compare is we compare with the hyper. With the hypery, that's the Lawrence Livermore code, where we compare the structured MG, structured MG code with SMG from Hypery and also BUMA MG from Hypery. And we can see if the structured information is used well or wisely. And we also use mixed precision computation, which is more and more supported. More and more supported in our modern computer systems, we can get much better speed up when we scale from like 100 CPU cores to 4,000 CPU cores. So, this is for x86 architectures, and this is for ARM CPUs. Okay, that's Okay, that's we call it a semi-structured grid. And then we can also use a geometric multi-grid for structured grid. Okay, this is exactly structured grid. We can use a GPU to improve efficiency further. Here are two numerical tests. This is 2D. We go from a milling. Go from a milling to like 250, 100, 250 milling announce. And also for 3D, go from like several 260,000 to like 1 billion unknowns. So this is on one GPU, one A100 GPU card. 100 GPU card. And the numbers, what we are showing here is the computational time. So basically, with like 0.1 second or less than one second for 3D, we can achieve the discretization error with four multi-grid methods. So, but as I mentioned previously, I mentioned previously, the poorly structured grid can be used, but in very limited situations. So we still need to improve efficiency for general grid or unstructured grid. So another front we are doing is try to solving linear systems. Solving linear systems using adaptive procedures to improve the efficiency of algebraic model-grade method. For example, what we can do is for different linear systems, we can choose parameters for AMG method adaptively. Okay, so here we use one simple example to show what we're doing is solving this 3D ICF demo example, which is the future. Example, which is the fusion energy simulation. And the number of unknowns is like 6006 million unknowns. And we choose different parameters, SATA. This SATA is a parameter from HyperI. It's called, okay, the name is not important. Some kind of parameter SATA. And if we choose. Theta, and if we choose it differently, we get a very different number of iterations until convergent. For example, if theta is 0.5, which is recommended for 3D problems, and the number of round iterations is reaching 500. But if we choose a it mu a smaller uh smaller one, and then we can get uh uh much better uh convergence performance. Uh, convergence performance because this kind of theaters are tuned using a Poisson's equation, and this ICF is quite different. So, so what we do is we introduce auto, so-called auto-MG, where this theta is choosing using a neural network, and this neural network is trained using like about 100 training data from ICF examples, and then we using 10 examples. We are using 10 examples for testing. And these examples are all from ICF, but they are from different sizes. The problem size are different. And then we can see the AutoMG performance is very close to optimal performance. It's like 10% slower than the optimal performance. Okay. Of course, we can also use. Of course, we can also use neural networks for solving linear systems to improve efficiency. For example, the Jing Chaos group is using MGNet or MGCNN or some kind of related methods to improve the linear solver efficiency. And another front, which I'm going to introduce here, is using AI for PD scheme just. PDE scheme, just to use the neural network to solve the reservoir simulation problem. Okay. Of course, the very popular strategy nowadays is so-called neural operator learning, where we try to learn this linear or nonlinear operator directly. The classical model-based numerical Physical model-based numerical methods usually require a priori information or a priori knowledge for the underlying PDE. And sometimes our model might not be accurate enough. For example, for the petroleum reservoir simulations, the coefficients of the Darcy equation is very rough. The information we can obtain is very limited. Okay, also the problems sometimes is too complicated and we need to solve them many, many times to adjust the model coefficients. Okay, the database neural operator learning is based on infinite dimensional regression, and we treat input and output as elements in functions. Output as elements in function spaces. So basically, what we are trying to do is we, for example, solve this equation and we are given this parameter kappa and we try to learn this operator go from kappa to u. Okay, u is the velocity field and for example, okay, and kappa is the permeability, for example, or mobility. Okay, there are many neural operator architectures we can use. For example, FNO is a popular scheme. It's published in ICLR in 2022 and the citations of this paper is more than 2,000, I think. And another approach is the Jing Chaos group proposed. The Jing Chaos group proposed MGNO. It's another neural operator architecture. And we are trying to compare these two architectures in our study. Okay. So the setting of the study is the following. So what we are trying to input is the permeability field. Then we use either FNO. Either FNO or MGNO to learn this operator go from this permeability to the pressure field or the saturation field. And these two things are, of course, time dependent. And to make it more realistic, when we train this neural network, we use partial. We use partial information, which means, for example, we can use the first 20 years information to train the neural network. And then for the testing, we use the next 10 years for the future time for testing. Okay, so this is just to make it more realistic and as we usually use it in the field. Okay. Okay, so here's the data set size. Data set size is 2000 times 25. 25 means we have 25 time step information and 2000 is just 2000 different sets of permeability choices. And then the resolution is fixed for the among this, we list we we choose 1600 for training okay then for testing we we use the rest 400 test data sets for the for the test but we use 60 time steps and 35 of them has not been seen by this training set okay of course then we can do a regular study for A regular study for the training. For example, we can use different loss functions, different architectures, different optimization methods, so on and so forth. But that's not our target here. So we just compare these two neural architectures. Here is from K to pressure P, and here is from K to saturation. From k to saturation. And this is the summary of the number of parameters used in this network and the resulting relative L2 error and time for inferences per sample. So there's not much to see, but just showing these two methods are comparable. And then we look at the numerical results. Okay. Okay, so these two columns, we are dealing with a time equal to 10 and a time equal to 20. And these two has been seen by the training set. Okay. And the last four columns are the new time steps from 30, 40, 50 to 60, which has not been seen by the training. And here is the input and list. Is the input and this is some sample results from FNO and sorry, this is just FNO. We compare it with the direct numerical simulation. The first line is using classical simulation and this one is predicted by FNO. And the last row is differences. And we can see the difference. And we can see the difference is like from 1% to 3%. At least it's acceptable, particularly for the pressure field, because usually when we compare different commercial softwares, the difference is like 5% between different models and different simulators. So 3% is a very good. Is very good. Also, this red box shows the part where we are interested in because we are trying to use the data from the first two time steps to train the model for the future prediction. Okay, this is MGNO. The results are very similar. Are very similar. The relative error is slightly smaller, up to 2%, but I would say they're quite comparable. Okay, the difference comes from this predicting from permeability to saturation. Okay, we can see this is the FNO result, and the differences between microsimulation and predictions from neural networks are quite. From neural networks, they are quite large. The relative error goes up to like 30%. Okay. Okay. I think the difference comes from the pressure field changes very smoothly and very little in time. Okay, even in this case. But the saturation field, we can see a very sharp interface and this interface is advancing in time. Is advancing in time. And since this happens, the neural network does not do a good job. This is the MG angle. It's slightly better. Relative error is up to like 10%, less than 10%. We can still see a good interface prediction, but still it's much. But still, it's much worse than the predictions for the pressure field. Okay, so this is our efforts to using neural networks to improve the efficiency of numerical simulation, or we can reduce the number of numerical simulations so we need it in our whole workflow. In the future, what we are trying to do is What we are trying to do is to apply it in real applications where the models are very difficult. For example, we can have Darcy, Darcy, List Darcy is for porous media and least Darcy is for fractures and then coupled with Navier-Stokes or Stokes equations for free flows in these vacancies or caves. Caves. We can also use a coupling of free flow and the fractures and matrix and also power elasticity to handle the fracture deformation. Okay, this is some other examples. Because of the time, we just skip this and stop here for a question. Here for your questions. Thank you.