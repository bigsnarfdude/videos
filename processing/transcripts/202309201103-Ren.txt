Thanks for the introduction and thanks to the organizers. Today I'm going to talk about baby PIH, which is a parameterized interpoxibility of a kind of mean CSP. This is a joint work with Glencat and Sandeep from UC Berkeley. So in the following, I will first provide some background on parameterized complexity, then I will define a constraint satisfaction problem and introduce parameterizing approximation. And introduce parameterizing approximation hypothesis. After that, we'll stay our setting and show our main large baby PIH. Finally, I'll give a quick overview of our strategy. Let's start. So, in the parameterized world, each input instance x is associated with a small parameter k, which is an integer. So, the complexity now is measured as a function of both. Is measured as a function of both n, which is an input size, and k. We say a problem is fixed parameter tractable if it admits an algorithm that runs in f of k times polyn time. But f could be any computable function like 2 to the 2 to the k. So let's take a look at these two examples. The first one is k-vertex cover, where the input is an undirected graph. Is an undirected graph, and the goal is to decide whether there is a vertex cover solution of size k. Second one is multicolored k-click, where the input is an undirected graph G, where the vertex set is divided into k parts. And your goal is to decide whether we can find one vertex from each part so that they form a click. Although both of the two problems are anti-complete, you know, the first one. We know the first one, the k-vertex cover, admits an algorithm that runs in 2 to the k times polygon. Therefore, it is in class FPT. For k-click, people do not believe it has time, has algorithm better than m to the little of k. So the trivial brute-force algorithm just runs in n to the k time. So we have identified k-click as the We have identified K-click as the complete problem of class W1, where you can think of W1 as the parameterized analog of NP. And class FPT is inside W1 and is widely not equal to W1. Okay, so let's recall the constraint satisfaction problem. During my talk, we'll consider RT2. We'll consider RT2 CSP, but the input consists of a set of variables which is denoted as x, and sigma represents the domain of each variable. And phi consists of some RT2 constraints. The goal is to decide whether there is an assignment from x to sigma that's satisfying all the constraints. In the approximate version, the CSP value is defined to be the maximum fraction of constraints that are sigma. Of constraints that are simultaneously satisfiable by some assignment. Okay, so 2CSP is MP complete, for example, from the reduction from a three-color. And the PCP theorem tells us even constant approximating to CSP is still in PR. So being parameterized to So in parameterized 2CSP, everything is the same, except now the number of variables x becomes a small parameter k, while the alphabet size becomes as large as the input size l. And we are wondering whether there is a f of k times polynomial time algorithm for this variant of CSP. So parameterized CSP is still W1 complete. For example, For example, we can easily reduce from multicolor cake leak the exact version of multicolor cake leak. And as an analog of the PCP theorem, the analog of PCP theorem is called PIH parameterizing approximability hypothesis, which states even constant approximating parameterized to CSP remains W1 hard. In contrast to PCP theorem, To PCB theorem, we still do not know whether PI actually is true. So it remains as a hypothesis. So let me show you some implications of PIH. Currently, starting from W1 not equal to FPT, we do have many proximity results for many prime price problems. For example, for Klee. For example, for Klee, K-Star cover, et cetera. However, now these inapproximability results are obtained in a rather problem-specific way. So we have many different techniques for many different problems. For example, for take league, we have aircrafting codes and size and sets. For K-Stack cover, we have also two contrasting frameworks. One is the distributed PCP framework. One is the distributed PCP framework, another is threshold graph composition. Furthermore, for some, also very important problems like K exact cover, even 1.01 approximability is not under W1 or equal to FPD. So we have to the previous slide and explain what those approximabilities are and actually not proximate. Because A is the K is not the solution. It's not the solution, since the number of variables. So maybe I should mention the reduction from multicolor cake leak to parametrized to CSV. In multicolor cake leak, we have k groups of vertices, and you need to pick one vertex from each group, such as they form a click. So we can think of each part as a variable here. Variable here and the the vertex we can pick is just the domain of uh each variable. So if we can find a click, which means we can find k vertices that satisfy all the constraints between them, then we can find a satisfying solution for 2 CSP. That's why the k there becomes the number of variables here. So what if p So what if PIH is true? Then we can take parameterized gap, gap to CSP as a starting point, as what we do in the NP world with the BCP theorem. Then we can just use the canonical reductions from gap to CSP to all the other problems. So this can not only unify the whole landscape of parameterized inapproximability, but can also give us new But can also give us new inapproximity results. For example, a constant in approximately for K that out. So how likely is DIH2? So far, we only know under a very strong hypothesis called GAP ETH, parameterized to CSP is hard to approximate. But starting from GAP-free hypothesis, even from ETH, which is Even from Eth, which is stronger than W10 equal to FBT, we still do not know how to prove that. And proving PIH, or even prove the constant inapproximity of parameterized 2 CSP under ETH, remains one of the major open problems in this theory. And now let me introduce our setting and our main result. Setting and our main result. So we consider a variant of CSP. Actually, we consider the list satisfaction of to CSP. Now, instead of assigning each variable a single value, we allow to assign it a list of values. And a constraint on a pair of variables x and y is said to be satisfied as long as there is a pair of values u and v, where u and v v v v v v v v v v Of values u and v, where u comes from x list and v comes from y's list, such that you will satisfy this constraint. So here is an example where there are three variables and three constraints. The alphabet is 0, 1, 2, 3. The three constraints are shown as above. In general, as a normal CSP, it is not satisfiable, but it is least. But it is least satisfiable by this assignment, this module assignment. You can see each constraint, for each constraint, there is a pair of values from the list of these two endpoints such that they satisfy this constraint. Is it clear? We say a two CSP is R list satisfiable if there is an assignment with maximum list size at most R that satisfies all the constraints. Now our goal is to list satisfy all the constraints while minimizing the maximum list size. Two quick remarks. If a cis key If a cis P is fully satisfiable, then it's equivalent to being one list satisfiable. And as R increases, it becomes easier and easier to R list satisfy an instance. Second remark is, if it is R list satisfiable, then by picking one element, one value from each variable's list uniformly at random, we can in expectation satisfy one of the ask. Expectations satisfy 1 over R squared of a fraction of constraints. Therefore, the CSP value, the normal CSP value of this instance should be at least 1 over R squared. In the recent work of Caso and Cosette, they proved the following baby PCP theorem. That is, for any integer, constant integer r greater than 1, it is not hard to distinguish between 1 list satisfiable and not even r. Satisfiable and not even our list satisfied. Why do we call this preview PCP? Because this statement can be implied by the normal PCP theorem by setting the sun plus parameter epsilon to be something smaller than 1 r squared. So Bartle and Kodi actually gave a short commentator. Gave a short combinatorial proof for this weaker version of PCP. And they also showed it was enough to prove NP hardness for some promised CSPs, like 2 plus Epson set, starting from baby PCP instead of PCP. In this work, we prove the parameterized variant, which we call baby PIH. That is, assuming W0 equals LPT, distinguish between one is the satisfiable and One list satisfiable and not even R is satisfiable cannot be done in F of K times Boolean time, where K is the number of variables and N denotes the alphabet size. Why is two pathways on set? What is 2 plus X1 set? Oh, 2 plus Epson is a set formula and it's guaranteed there is an assignment which satisfies half minus. Half minus epsilon fraction of literature in each class. I missed something about the double-prosic result. This follows, we knew that, right? Because it follows from your what is this the result? So so they gave a alternate proof of this this weaker version. Combinatorial I'm notorial. That's not. I mean, for me, this is PCB. Uh you mean you should which is PCP? Is the proof surprisingly simple? Does it evolve in PCP machinery? It does not involve PCP machinery. It's just like two, three pages of combinatorics. It's a known result, but it's much simpler. I mean Known result, but it's much simpler. I mean, it's a very clever proof, but it's elementary and short. So I should read this paper. Okay. So baby PI is at first itself interesting approximately result for the list satisfiability of CSP. Also, it may be a baby step towards finally proving PIH. And the lack of proof of PIH makes it more appealing. PIH makes it more appealing since it may already be enough to get some applications of PIH from baby PIH. Unfortunately, for the last point, we do not have any natural problems, but we prove that something stronger, which we call average BPIH, is sampled by PIH and BBPIH. So that is enough to prove the constant approximability for K exact cover, which is not. K exact cover, which is not known before under W1 not equal to FBT. Okay, now let me show you the reduction and the high-level idea of our proof. Even a 2CSP instance pi, we use the simplest and maybe the most common construction, direct product construction. So the TY is. So the t-wise direct product of an instance pi, denoted as pi to the power of t, is defined as follows. The variable set is all the t size subset of x. And the alphabet of each size t subset is the older partial satisfying assignment for this set of variables. By partial satisfying assignment, I mean assignments that satisfy all the constraints. That satisfy all the constraints induced by this subset. And for every two different sets, we just check the consistency between between the consistency on their intersecting variables. Very simple. So for any constant R, we want to show there is some Show there is some large enough t depending only on R, such that if pi is satisfiable, then so is pi to the power of t. And if pi is not satisfiable, then pi to the power of t is not r least satisfied. Know that our reduction time is n to the power of t, which is n to the o sub power of 1. So it is actually a polynomial time reduction. Reduction. So we have a unified proof for both baby PCP and baby PIH. Sorry, I could you so the new variables are T set suborgent variables? Yes. What are the constraints? Constraint just to check whether the whether the two uh size T subsets are consistent on their intersecting variables. It just a repetition of the omitter. Reputation of the original. And also it checks the local constraints in each flow. Yeah, yeah, that's that's that's that's in the alphabetic script. R is the list size we want to prove partners to. Can you say something about what's the dependence of R? T is roughly R to the power of R. So here's an example where I want to show a three list satisfying assignment for pi to the power of four. Suppose there are ten variables in pi, there should be ten choose four variables in pi to the power of four, but due to the space limit, I only draw three of them. So for each site 4 subset can take a sight. Subset can take a satisfying assignment on those four variables. And for every different sets, we just check their consistency on the intersecting problem. Where the consistency is checked in the least satisfying sets. Great. So uh we we want to finally show that pi is itself one least satisfied. Pi is itself one list satisfying. So, what do we do? We just do induction on the list size R. We repeatedly extract some satisfying assignment for the smaller instance pi to the power of t prime. T prime is smaller than t. So, suppose we have a satisfying assignment for pi to the power of t, then based on this assignment, we construct assignments for construct assignments for pi to the power of t prime. And our goal is to decrease r to r minus 1 in each step. If we end up with the one list satisfiability of some pi to the power of greater than 2, then we are done, since every constraint has been checked locally in the alphabet of each set. And being one list satisfi means these partial satisfying assignments are completely. Partial satisfying assignments are completely consistent. There is a global satisfying assignment. So, how do we do that? For each smaller set, S, we want to pick a larger set T. We want to pick one of its superset. And then we inherit the whole list sigma of T to be the new list sigma prime of S. By inherit, I mean we take the restriction of sigma of t on the set, on the smaller set S. If we just do this, then the list size will be unchanged, right, since we inherit the whole list. And our goal is to, for each S, select the set superset T in a clever way so that the list size could be decreased by one. So here uh here I want to extract satisfying assignment for the smaller instance pi to the power of 3. So for each smaller set we pick one of its superset and inherit the assignment for the su for the bigger set to the smaller. Why didn't you cross out one of them? Sorry? Why is one of them been thrown out? thrown out? Oh yeah, we cannot throw out directly. So that's a key point. That's what we do case analysis. So we need some analysis to make sure the one we discard does not help. In fact, we can only discard the assignments that are never used to meet any consistency requirements. Requirements. Suppose we have the following bipartite direct product instance, where the definition is similar. We have T-side subsets on both sides, and for each set we have R values. But now we're only checking consistency between the left part and right part. Okay, so we observe that on the right side, among the three sets containing x1, x1 is never assigned by. X1 is never assigned value 3. So we can safely remove all the assignments on the left, which assigns x1 to 3. So how can this always happen? So it might not be possible that we always have a variable xi and a value p such that on one side xi never equals to p, and on the other side, in every set containing xi, xi is assigned p somewhere. Here, the secret is we Here, the secret is we can keep the superset at our own. So we for example on the right side, we can only inherit from such a t that 3 xi is not assigned with 3. And on the other side, we can only inherit from such a t that xi is assigned 3 somewhere. 3 somewhere. So, if this is true, then by discarding this rubbish assignment, we can safely decrease the list size by 1. And thus, we can do induction on the left list size. This captures our proof idea. And the actual proof And the actual proof has more uh technical details and case analysis, so I will not go into more details here. In a high level, one more thing. So, right now, you haven't reduced the max list size on the left-hand side, so you need to make sure that everything is hit here. For every cell, we need to reduce this size level. So, we first proved the theorem. We first prove the theorem for the bipartite R comma 1 case, where for each left set it has r values and each right set has only one value. Then we prove for the bipartite R comma Q case where R Q are both constant. Finally we get the non-bibatite R case which is what we want. So a few takeaway parameterize the inapproximability hypothesis Parameterized inapproximability hypothesis is the parameterized analog of PCP. And in this work, we prove baby PIH, which says it's W1 hard to distinguish between one list satisfiable and not even our list satisfiable. Proof idea is to do induction on the list side. As an open question, I wonder whether we can prove average baby planet. The difference is that in the no case, now we allow assignments with Allow assignments with average disk size at most R. So from the colour side, we need to rule out this larger class of assignments. If average BBP, this is much like the label cover problem. So if this is true, average BBPI actually is true, then we can get constant in approximate breaking of K that cover, which is not known before. That's it. That's it, text. Any questions? Will the average basic DIS also imply in a proximity for the other two problems? Yeah, yeah, yeah. There so this this is not a complete list. So there are many other problems. Many other problems. The average maybe PRS have implications for all those problems? So I'm not sure, but so it's just like some covering type thing. At least for KSA cover and K exact cover, it works. And I It works. And I guess there are more problems that it works, since it's just something I label covered. Can you say a bit more about the combinatorial core in the autocorrelation? Sounds like some kind of Ramsey argument. Okay. I think it's a weird question. So for each smaller subset, we can choose what superset we inherit from. So we pick such a set that X1 is never assigned. It's like a larger graph of finding the substructure. Yeah, like this. So originally we have a K prime, a T prime hypergraph. We want originally we have a T hypergraph. Then we want to find a T prime sub hypergraph. 