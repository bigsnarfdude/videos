An additional talk which is not on the schedule at 4 p.m. It's the last talk which will be given by Igor Krasovsky and tentatively, as far as I understood, it will be about random matrix, both Freud and Sample. So at 4 p.m., even though it's not on the schedule, there will be one last talk by either person. That's the most speaker of today will be Jean. He will tell you about uh know About non-me failure matrices. Oh, okay. Thank you. Thank you for inviting me for this workshop. Today I'm going to talk about some random matrix models related to this Anderson conjecture and this quantum chaos conjecture. Of course, we cannot solve them, but we can find some models which are related to these two conjectures. To these two conjectures, keep the spirit of these two conjectures, and we try to understand those random matrix models first. This is John Work with Birchen Stone and Yang Fan. Birchenstone is my PhD student. Yang Fan was my student. Right now he is in Cheng Kwan University. Let's quickly let's review the Anderson conjecture. You have seen this it's for a couple of times. Couple times. The way we understand is, just we look at this model, we say, okay, we have the Laplacian and this V is the potential. But since we talked about the random matrix, so look at them as a matrix. You can see this is a linear combination of two operators. One is this Laplacian, another one is V. If you write V in the matrix. V in the matrix form, you notice this V is a diagonal matrix because V is the multiplication operator. So if you write it in a matrix, it's just a diagonal, right? It's the quickly say just the V V X equals to V X V. So this V is a mo is a diagonal matrix. So look at this two operators. If you look at eigenvectors of these two operators. Eigen vectors of these two operators. For delta, what we have is e to the ikx, and this is the extended function to the whole space. And for this, V, it's the opposite. This is a delta function. And the only delta function is the eigenvector of this diagonal matrix. So now, when we look at this Hamiltonian, this linear combination of extended Combination of extended operator and localized operator. What is the sum of this? What's the linear combination of these two operators? This is Anderson conjecture. As you know, Anderson conjecture says if d equals to 1 and 2, the eigenvectors of this h is always localized. So d equals to 1 and 2 always v wins. And the localization length of the eigenvector is proportional to lambda to the minus. Proportional to lambda to the minus 2, the localization length of eigenvector, just if the eigenvector looks like this, then the middle part is the length, roughly speaking. The definition is not very important in this talk. But on the other hand, we know there is a phase transition for high dimension. High dimension. When lambda is small, lambda c, when lambda is small, this localization length is infinity, which means the eigenvector of this h is really extended to the whole space. So when d is larger than or equal to 3, sometimes delta wings, sometimes v wings. Furthermore, you'll have the conjecture talk about the The conjecture talks about the eigenvalues. If you restrict this H in a box, then in the delocalized region, you find out the eigenvalue, the eigenvector gap match the GOE statistics. And in the localized ver uh region, the gaps of eigenvalue match the Poisson distribution. Uh so roughly speaking, uh in this talk we In this talk, we focus on different things. There is a phase transition. On one side, the eigenvector is localized, which means it looks like this, and the eigenvalue match the Poisson distribution. On another side, the eigenvector are delocalized and match the GOE distribution. So, this is what we learned from the Anderson conjugate. We learned from the Anderson conjecture. Maybe you know there is a very similar conjecture in the random band matrix. First, let's look at the definition of the band matrix. First, this is matrix in D dimension. And just instead of Hij, we use HX and XY. XY are the in the lattice, D-dimensional lattice, sometimes had an L, that is the size of the Head L, just the size of the matrix. Random band matrix, it's very easy to understand. First, it's a matrix, so it looks like this. Looks like this. Second, it's band. The band means this type of structure when x and y, this h x y equals to zero, when the distance between x and y is larger than w. Here the distance is just the regular distance in the letters. Distance in the lattice. So most of the entries are zero. It has a band. So matrix band and a random. A random just say the entries are centered. The mean is zero. Most they are independent to each other up to a symmetry, which means the x hxy equals to hyx, symmetric or Hermitian. Or Hermitian. Otherwise, it's just their ID vendor variables. In most cases, we just choose Gaussian, standard Gaussian. The distribution is not very important in some proof. In my talk, I will not emphasize the distribution because distribution is not very important for the proof. But the main keyword is random, a band, and a matrix. So it's random, it's an ID entries, and band, it's a band and matrix. Of course, it's a matrix. For this, For this random band matrix, it's easy to see when w is very small. Let's say w equals to 1, it's just a diagonal matrix. If it's a diagonal matrix, the eigenvector of the diagonal matrix is localized, it's clear, right? And on the other hand, if w is very large, w is very large, w equals to l, then you definitely will believe the eigenvector is still localized. But actually, it's not that easy to it's really really not easy. It's really not easy. It's really, really not easy. It's really not easy to prove. I think the first paper for delocalizer was published in CPN, so the story is not easy. But you definitely believe when the band is narrow, it's localized. When band is wide, it's delocalized, roughly speaking. So let's look at the conjecture. We have the similar conjecture. The conjecture says when d equals to one, the same story, the localization length is always you can say this. You can say this eigenvector is localized with the localization length and w to the squared, w squared. So, which means if it looks like this, w, then the eigenvector, the localization length of the, if band is w, then eigenvector is w squared, yeah, of course, this means if l is very big, if l is less than w squared. Less than W squared, then of course the localization is just L. It was predicted more than 30 years ago. For high-dimensional case, again, there's a phase transition. People believe for small W, you will see localization, but when W is large enough, you will see this eigenvector is extended to the whole space in the high-dimensional case. So it's parallel. Dimensional case. So it's parallel to what you have seen in the Anderson model conjecture. This is here, this is a lambda to the minus two, and here's the phase transition. And when lambda is small, it's localized, when lambda is large, it's delocalized. I think in today's talk, I will definitely always mess up which side and which side. So the spirit is just one side is this, another side is that. Okay, again, the conjecture is this. If it's localized, then if you look at egg value, it's possible. Then, if you look at eigenvector value, it's Poisson. If you delocalized, you look at eigenvector, eigenvalue is GUE. And this is one of the most important conjecture in the field of random matrix. I use this question in every single proposal I wrote in the last 15 years. But on the other hand, this is This is not the main topic of today's course. So I will not talk about the details of the results on this conjecture. I show you some work, show some main results. We have upper bounds on the localizing length, lower bounds on the edge. bounds on edge and this supersymmetry setting and we have eigenvalue in the for the GOE results for the Poisson results. This is the main result so far. This afternoon Charles Smart will give you a talk about his new works I think better than this one here. But definitely I think he will talk about the details on About the details on this topic. So, if you rely on me to give the background, use the lunch time, you change some size. So, I'm going to skip this because this is the main, this result is related to today's topic. So, let me remind you: the conjecture says when D is larger than 3, the localization length. The localization length is infinity. If W is big. So the conjecture is for any D larger than 3, a localization length is infinity for large enough W. This is the conjecture. So what we proved is we proved when 2 to 3 years ago, we proved when D is larger than equal to 7, the localization length actually is larger than W to the NH. Than w to the any fixed power for large enough w. So it's not infinity, but it's already a very large. That's what we did. More precisely, if what is n there? N is just for f any fixed uh large number. Fixed large number, yes. Uh no and for any fixed fixed And for any fixed, fixed is the same. Fix and learn. Because if it's correct for two, then it's correct for one. More precisely, what we did is slightly stronger than this. What we got is if the box, the matrix size is less than the W to the N, which means you will see. Which means you will see this eigenvector is delocalized in this matrix. It's not just delocalized, it's in the sense of QE, which means if you have eigenvector, if you have eigenvector, oh, one thing I forgot to say, in my talk, most of the results are only correct for bugs. So sometimes you will see the bug word bugs. Sometimes maybe I'm I missed that some place, but most of them just. Place, but then most of them just for bulk. If you have a eigenvector, a bulk means just if it's spectron, it's just not middle, just not edge. If you ask me what is edge, I say not in the bulk. Yeah, if you look at the eigenvector, you do some local average, just you do a convolution. Just you do a convolution of some function, smooth function. You do some local average, and you find out it's just flat. It's really flat. And we did some works on the eigenvalues, but this is much weaker than this one. This is what we did for the high-dimensional case. Okay, so which means if for large D case, Large decays, you have a band matrix, then you will see the eigenvector is almost extended, let's say almost extended. Now, let's after we know these results, we look back to the Anderson model. We find Anderson model, if you look at it in the form of matrix, it's also a diagonal matrix, another it's a band matrix, it's really narrow band. It's really narrow band. It only has three lines, where delta is the neighbors and the V is diagonal. So, whether we can do something for the Anderson model with the methods we developed at here, of course, the answer is no. So, we try to see what can we do for the Anderson model. So, we tried this model. This model just says this is the Anderson model, delta plus. Model, delta plus lambda v, v is diagonal matrix. Now, instead of diagonal matrix, let's replace this diagonal matrix with block matrix, block, other blocks. Of course, it's very, very long, but I think block. The block for these blocks, the bandwidth, the size of the block is W. And of course, as you can see. And of course, as you can see, if w equals to 1, then it's just the regular Anderson model. And we want to understand this model, just delta plus lambda times V V B. And for each block, they are random matrix. The exact distribution is not very important, but for this model, I just they are random, they are independent random matrix. Matrix, yeah, that's a random matrix. Do you assume any symmetry? Oh, yes, it must be symmetric. Yes, it must be symmetric. Otherwise, this is not advanced. In our work, we just choose it as weakened, independent of weakness. But if you look at the proof, you will see the assumption of weakness is not that important. Why do we choose this model? Instead of band matrix, just the band matrix, we choose the block matrix. We choose the block matrix because this is very important. This is important. Yes. And because of this reason, the eigenvector of this VB is localized because there's no interactions. So all the eigenvectors are living in one block. So it's either just living here, living here, living here. So what we can say is. Here. So, what we can say is the localization length of VB is always W. But the localization length of delta is infinity. And now we look at a linear combination of these two operators. We want to know actually some randomly. We want to know who we are with. It's kind of like the spirit of the Anderson model, just we have a localized operator, we have a We have a localized operator, we have a delocalized operator, and we look at the linear combination of them. And we find out, we guess, that there's also a phase transition when D is larger than or equal to 3. And actually we have more details about the coefficients. So let's let's look at the uh conjecture. Oh, so it's it again, just say this one versus this one, lo delocalized versus localized. Woo well wait, right. Versus localized. So, what are you doing here? So, you add something to this matrix or what? Oh, I just look at this matrix. No, Laplacian. Identity matrices on the algorithm. This is regular Anderson model. This is the model we want to study. Just the delta. This is the original one. I mean, this is a. What is the VB? A V B is just this matrix. But maybe it's just this matrix. It's a matrix. Okay, so this is the conjecture we find out. For this model, if let's say this blocks are independent wigner matri uh standard uh wigner uh matrices. Oh when I say standard matrix, I just mean the regular one, the the spectrum is from negative two to two. The spectrum is from nectar 2 to 2. And then I write in this way. This time I move the lambda to here because I want to use this to match other models. So I write a lambda here. Usually you write lambda at here. So I write it here. If you look at this H, our conjecture is the phase transition occurs at here. When lambda is much larger than this one, Than this one, this model is extended. And when lambda is much less than this one, this model is localized. Actually, the localized region is easy to, it's not hard to prove. We give the, I think, almost the final answer because that's what you look for. Well, lambda, for any dimension, if lambda is much less than this critical number. Critical number and the localization length is just W, which means this is the H block. I did not write the lambda on this picture. This is the H block. If you add some lambda, you want to add some lambda, there's a picture to show you as well. So when you add lambda, lambda is kind of at here just to connect two blocks like this. Of course, there's some red dots. Right, like this. Of course, there are some red dots inside, but they're not important. Important part is you will have some red dots connect to blocks. So you don't care about the distribution? Distribution is weakener. Distribution is for each block, first is symmetric. Second, all the entries are independent ID random variables with the mean zero and a variable. Variables with the mean zero and a variance one over double. Yeah, I also have let me write down the weakness matrix. I meant that it's not that you're not concentrating on GUEs for case. Oh, that's what. No, no, no. That's what I meant. You don't care about distribution. No, I care, but I haven't proved anything. Ah, okay. Ah, okay. We can, we can, we definitely can. We haven't proved uh uh proved the result for the eigenvalues. Could you just remind me what L is? The left scale L? But this is the localization length. Oh, that's what you call localization length is the block size, right? Localization length of the eigenvectors of this H, this H. So let's So let's say let okay, let's say this. Let's look at this. So when lambda is zero, you only have this H block. Well, H blocks look like this. And if you look at eigenvectors of this matrix, the localization length of all eigenvectors of this matrix is W because it's living in the block. Now, when you have some lambda, then you will see this system is mixed, like say something mixed with each other. So you will see the With each other. So you will see the localization lens were getting bigger and bigger. And the story is: we believe if it's much less than this one, it's W. And when it's much larger than this one, it's infinity. It's a big jump. That's what we believe. That's a conjecture. I think Nevson, I should put detail here. This is a conjecture. Yeah, yeah, that's our conjecture. Our conjecture bit, right? Aha, oh. So when you say Nyang and Yin 2024, it's not theorem, it's conjecture. No, no, no, this is Rizal. This is Real. See, the number are different. Okay. This is Rizal. Oh, I see there's a four there. Sorry. So what we did is we proved the local the wind direction. Local the wind direction, just for NAD, and this is much smaller. This is this we use the fractional moment methods. And on another side, we proved is when D is larger than 7 and the lambda is larger than this number. This number doesn't match the conjecture. This is 2, this is 4, and this is the localization length is very long. As you can guess, the method we used is the one we developed for the band matrix. So this one. And this one. But we haven't submitted our approval. The main reason is both of us get six in the new year. So let's wait one more month to see the paper. But definitely we will finish in this year, but just not right now. So we have this one. So one size, this. So, one side is localized, another side is almost infinity. This is a correct threshold, we believe. And actually, I will explain why this is correct later. Okay, so now let's review our band matrix. The band matrix looks like this. So, the whole thing is banned matrix, right? But I just picked the matrix. Band matrix, but I just pick the middle part out and then the other part. And this is this block-anderson model. Black Anderson model is a band block matrix plus delta. Delta is the red part. And the random band matrix is a block band matrix at the non-diagonal block entries. Look like this. So if we can prove this one, and we proved the lambda is very small, this is still extended. Let's look at the lambda. Is still extended. Let's look at the lambda. The lambda is really small. It's negative here. You see, they are negative here. They only connect a few entries between two blocks, and the number is small. There's a negative here. So only with very small connection, you can connect different blocks into a whole system. This is very amazing phenomenon. And so we ask ourselves this question: whether we need so many entries for the band matrix. The answer is. For the band matrix? The answer is no, we don't need that. You have the middle part block band matrix, then add some non-diagonal block entries. The non-diagonal block entries of the band matrix can be much smaller. And this matrix is still delocalized. Actually, you can see the conjecture is you can reduce from here to here. But we did not, we tried some calculation. We can definitely reduce this number. We can definitely reduce this number, but we did not write down anything. The reason is, I think this model is not interesting to anyone. But just to give you the feeling, the band matrix is not necessary to say it's band. We have the core and something to connect the different blocks, then that's already enough. Let me be fast. So we still need to talk about the quantum chaos. The quantum chaos has some similar conjecture. If you look at this, is the billiard. If you look at this is the billiards, the billiards, this is the disc, this is this is this is sinite. If you put a ball in the billiards and hit it, you will see the trajectories. And this one, these two are beautiful, these two are ugly. That's already enough for this talk. I just, you don't need to know the details. This is beautiful, this is ugly. And the beautiful. This is ugly. And the beautiful one, people call it the integral system, and the ugly one, people call it the chaotic system. There's an interesting people find something very interesting for these two cases. If you look at the Hamiltonian, just the net cap delta, in this different billion, in different billions, you calculate the the eigenvalues of this operator, just just the energy level. And Level. And you look at the gap of this energy levels. You find out if it's a beautiful one, then the gap matches the Poisson distribution. If it's the ugly one, it matches the GOE distribution. I think you have seen this twice, one on Anderson model, one on this one. So here, the same story, as the two conjecture says. As a two big conjecture, says if this billet is integrable, which is the beautiful one, then the eigenvalue is Poisson. And if it's the billiards chaotic, then the eigenvalue is GoE. So it's in the quantum chaos conjecture. And furthermore, if you look at eigenvectors here, this is for the eigenvalues. If you look at eigenvectors of this delta, for this case, the eigenvector is also beautiful. And at here, the eigenvector is. Adhere, the eigenvector is ugly. Of course, I don't, it's not, it's if you choose one to as your painting for your house of a house, I think maybe you choose this one. It looks beautiful. But let's say this. But if you, I, let's call this one is called a localized, and this one is called a delocalized. Of course, right now it doesn't look like it's localized or delocalized. But let's just, right now just let's use these two terms. These beautiful ones, let's call it localized. And this is every Let's call it localized, and this is everywhere. This is called delocalized. Actually, I will explain why we call this localized. This is delocalized. How do you exactly associate the shedding operator with this two billions? Go back to the previous slide. I'm an elder computer. Just this H is negative delta. The Laplacian, nothing special. With the direction boundary condition. Defined on the on a different shape, just on the desk. Then, why is the trajectory kick C? I mean the trajectory of the failure. Why is the failure dynamics related to this shooting operator? Oh, why the looks like that? There are some reasons. There are some reasons. But I think in this talk, maybe I answer this after talk. In this talk. Talk in this talk. So, roughly speaking, if you ignore some details, roughly speaking, we have seen a universal conjecture on these three topics. In the quantum chaotics, in the random band matrix, and the Anderson models, we all seen that's a phase transition for high-dimensional case. On one side, eigenvector is localized and the eigenvalue is for some, on the other side, Is Poisson, on the other side, eigenvector is delocalized and the eigenvalue is GOE. We have seen the random banana matrix and the Anderson model, block Anderson model. Let's take a look at the quantum chaos model. We try to understand it first, not prove, we did not prove anything here, but let's just try to understand it. This is what we try to understand. What do we try to understand? So, in the disk, if you have this minus lambda and the versus just lambda plus v. This v is like you change the shape. You say you just you have a square, then you have a square, then you make a hole at the heel. You change the shape. Change the shape, you can just instead of change the shape, you can say we add some perturbations. But first, let's look at delta. Let's let me go here. Delta. So look at the delta. If the delta, if look at delta in the unit disks, this is the formula, and we know the eigenfunctions can be written as the product of two parts, r part and the theta part here in the polar system. We can write in this way because the angular momentum is conserved in this story. This subspace, all the functions you can write as. You can write as a function of r times e to the n c the i for all this type of functions. And this is an invariant subspace of delta. Because this is what we're going to have this way. So as long as we can accept this, because this is disk, this is rotation invariant. Rotation invariant means angular momentum is conserved. Angular momentum is conserved, which means we have invariant subspace. invariant subspace we have once we have invariant subspace we can write the delta into this way just the delta n delta n is the restriction of delta in s n we can write it in this way right because you have a you have a h and this h has many subs invariant subspace so you write it in the subspace like this way so once you write in this way this looks like this in each subspace it's a block Space, it's a block. Of course, it's not finite-dimensional blocks, but it's a block, right? So it's a band block. So this is very general for the integrable model. Because for integrable model, we always have one conserved quantity. If it's the disk, then the rotation invariance. If it's square, then this x-direction momentum. X direction momentum is conserved. So at least you have one conserved quantity. Once you have one conserved quantity, you will have some environment subspace, then you can write in this way, you will write the H into. So for interpretable system, you can always write it into the blocks version. And once you write this one, then you ask yourself what is the eigenvalue gap because you know the eigenvalue know that eigenvalue of h is the union of eigenvalue h alphas. So if you say, okay, I have a spectrum window, I want to know how many eigenvalues I have in this small window. And this, you can have one eigenvalue from here, or maybe this eigenvalue comes from here, eigenvalue comes here, here, here. So it's so it's a union of the eigenvalues. And because this H alpha, they are kind of like H alpha's they are kind of like independent to each other. Of course, they're not independent, but you can think about just whether I have an eigenvalue in this window for n equals to one has nothing to do with whether I have eigenvalue in this window when n equals to three. So it's roughly five minutes or five minutes. I'll try to be fast, I'll try to be fast. So, and once you have, now, if you look at the chaotic system, the If you look at the chaotic system, the chaotic system is like you add some perturbation. You pull a hole, you change the shape, you say it's a disk, then you just add a little bit more. So you change the shape, this changing shape is more like, say, you add something to connect the different blocks. And what we believe is, yes, if you do that, you will make this system extended, delocalized, then you change the Then you change the, it's like what you have seen in the band matrix model and the Anderson model. You change the, you change the localization length, and then you will see the income value changes from Poisson to GOE. So, roughly speaking, just we believe these three things are similar to each other in the following way. Just say you all start from You all start from blocks band matrix. If you add a delta, you get the block enters the model. It adds the off. You off diagonal blocks entries, you have the band matrix. You add some perturbations, you get related to the quantum chaos model. Of course, for quantum chaos models, there are many different models we can use. Let's say this is either just say we use the based on different The based on different basis you use, you can say the blue part is random, the red part is deterministic, or the red part is random, the blue part is deterministic. There are different models. But here, we just focus on some simple case. We just want to know how large a perturbation will cause the phase transition. Just how many red distinctions, blue disc things will change the phase of this matrix. So we start. So we start with a very simple model. First, we just want to know how many perturbations we need to add to mix these two blocks. Just let's say we have two blocks. If there's no red dots, then they're separated. And when we have red dots, they're mixed. When the red dot is large enough, they're mixed. So let's say we only have finite independent blocks, just finite of them, and we add some A's. AI deterministic perturbations, how large the A is where mix different blocks. Where A is zero, it's clearly still localized. Now, when A is very large, you would believe this is mixed. And my question is just say, the whole system could be very large, the band band. But just say, let's say only we have two blocks. How many connections we need to mix these two things? Just just two or maybe finite up. Just two or maybe finite up. Let me just jump to the answer. The answer is what we proved. We proved the answer is if they are this H are independent Wigana matrix, so this is a Wigana matrix, and A is deterministic, as long as, and we'll say the operator norm of A is much less than one, so it's a perturbation, as long as the Herbert-Schmidt norm is much less than one, then the eigenvectors are local. Aigam vectors are localized, localized. When I say localized, it just means if you look at Aigam vectors, you will find out one in blocks such that this eigenvector is basically supported on this particular block. It's like the independent case. And if you look at eigenvalues, the eigenvalue is the Is the eigenvalue limits match the union of D copy of sine kernels? Just say it's the union of the independent sine kernels because all of them are a Wigner matrix. Now, when D is large enough, then it converts to Poisson. On the other hand, if the Herbert Schmidt norm is much larger than one, you will see this eigenvector are delocalized in the following sense. Just the weight of the eigenvector on each block. On each block are the same. It's really flat on each block. And if you look at eigenvalue limits, it's just the mesh. The whole, instead of the copy, the union of the copy of sine kernels, independent sign kernels, it's a one big sine kernel. So what we proved is just for this model, when A is, we proved the thresholds for this A. Thresholds for this area. Her minimum much less than one, what you see is localized and Poisson. On other sides, which we prove is delocalized and geo-equal. So we only need this much to mix two blocks. And more general, the story actually is this. The story says if you have some blocks and A, how large is A? The A, the stretch. The threshold is just one over the normalized density eigenvalue density at the E. Just if you want to know at energy E, what is phase transition, is this one. We know for Wigan matrix in the bulk, the row E is order one, so that's why in the previous results, what you have seen is this is the threshold is one. Now, okay, so I think I gave all my results. Give all the all my results. But next question is, now is the future question. The future question is: say, let me say a few words. So how about a band block matrix? Because here we only have finite blocks. So if you have a very long band block matrix, you add some A, how many A can you add to mix all the systems? They're independent systems. You add a little bit of A. Systems, you add a little bit of A to connect them, the whole system will be connected to each other. So now the question is: if we have a band block matrix, which means the number of the blocks would be much larger than one. So it's very, very long, very, very long. What will happen? We find out this interesting conjecture, which means, let's just jump to this sentence, this conjecture. So if A is large enough to mix To mix the blocks next to each other, it's enough to mix to the whole space for the large-dimensional case that these larger than 3. So that's why in the previous Anderson model, you see what I said is when it's small, it's localized in the W, but when it's large, it's immigrant to infinity. And now, let's, if you look back to our previous conjecture, we have this block NSM model, this is blocked. NSM model, this is block, this is lambda. We have this condition, which is lambda c is this. What's the meaning of this one? If this meaning actually, it just means this equals to one. I forgot to say this equal to what. It's just they say this equal to one. So I forgot this equals to, actually equals to one. I missed. But this condition is just a Herbert-Schmidnum of A. Because you think about lambda is like A here, like A A here. So, okay, so maybe let me just stop here. I just say we did some works on these three topics, and we try to use universal way to understand that. That's what we get. Okay. Do you have uh Devoner and Nenami? Actor. Devner and Minami. Debner and Minami Debner estimates and Minami estimates. I mean if you already have we focus on the eigenvectors. But it's basically I mean you did with Gutnier, we did the same thing with GOE. So once you have localization, I mean it depends how you obtain the localization, but if you obtain it for example by methods of fractional Methods of fractional moment method, then you get Vegner and Minami almost for free. Not precisely for free, but almost for free. So we did the, I'm sorry that I kind of said that, we did the localization part with the precise one over square root and what, one restore W in in constant for the block matrix G U G O E and G U E. G-O-E and G-U-E. So you deal with Wigner mattresses, which is including, but it's much more. We only work with the GUAGUY space. Okay. I will check it on that. Yeah, maybe we will talk later. I will show you. Because then, I mean, if you have the localization part, you almost immediately have vector and binami. Okay. Okay. I don't know that. Not for me. That and not for actually, I actually, this is the first work I did on localization. That localization. I did all I did in the past 15 years on the delocalization. That's the first part. This is the first time, so I'm not familiar with this story. Okay, maybe we'll catch up later. Yeah, I have a question actually. When you speak of the localization length here, right, for which energies or for which Energies of for which vectors, I mean, how many, for example, for how many vectors, right, within your whole system, do you have such an estimate? All the equations. All the eigenvectors in the bulk. So it's a spectrum is the spectrum is from negative two to two. And as long as the eigenvalue is not close to negative two and two. Okay, but and then the second one, the localization for the bulk and the somehow. As I said, I always focus on the bulk. Yes, yes. So both of them are just for the bulk director. Most of them are just for the bulk of vectors. Because you see, I mean, the problem I have with Bloch-Anderson model, right, is that W gets large, right? So the part of the physical space that is affected by your perturbation, which is the reflation in this case, is rather small, because this one stays size one. Your Laplacian just connects, it has, if you look at the rank of the reflation that you add, it is of size one over W times the Of size one over W times the length of the total length of your system, right? Because the Laplacian space of rank one, right? Because they're just these few red dots you're thinking about. And so actually the size of the Hilde space you could move around by such a small rank is not very big. So meaning that the Laplacian does not affect a big chunk of your Hilde space because it has small rank. Yes. Okay? And so this is why I was wondering if there was This is why I was wondering if there were not some spurious eigenvalues, right, which were essentially attached to these parts of the Hilbert space, where this result was not true, meaning that the L equals W, right, it looks to me that the big part of the Hilbert space which is covered by the blue squares, right, that's fine, there you can do whatever you want because the Laplacian doesn't see them, right? And there is a small part, which is actually, if you let the number of the squares increase, it's small with respect to W, but it's not. To W, but it's not, and it's more respect to the total size of the system, but it can get large as well, right? If you have an increase in number of squares, where actually you have another behavior. So, and well, I don't know. No, but then you need to... There is a precise order of limit in which you need to add. So you suggesting kind of taking first the blocks go to infinity. But this is what he does, because he looks at W very large. He just lets the blocks go. But it depends on. But it depends on the size of the box. I mean, they go simultaneously to infinity. Sure, both things go to infinity. No, you're mixing the order of limits. W goes to infinity as the size of the box. That's the main distinction. So it means that the small one is still of the size 1 over w because the squares are much larger than the dots. Look at the blue squares, and you have a few dots, right? The number of places that you modify in your Hilbert space, thanks to Laplacian, is over the 1 over w. One over W. But they all go to it. I mean, you have to. No, no, this is not in the picture. Or do you let the chunk of Laplacian also go to an era? No, no, no. I mean, connecting to it. So you could imagine the following. Every second block in blue, you replaced by just a stretch of the Laplace. And so then it takes up the same dimension in space. So you can have faraway blocks, right, that are connected by some stretches of the field of... That are connected by some stretches of the free Laplacian. And then the Laplacian and the blocks, the blue blocks, play the same role in physical space. And Laplacian. That's not true because the Laplacian inside the Bignum matrix doesn't have the same effect at all. If you take the Bignum matrix, cross the Laplacian at all, it is still localized and you get the usual thing. You see, you should look at in it's just like imagine you have a potential. It's just like imagine you have a potential, right? Imagine you have a potential, okay? You have a stretch of periodic potential, then a stretch where it's random, then a stretch where it's periodic, then a stretch where it's random. And here, what you have is a very small stretch where you have the Laplacian and a long stretch where the thing is random, right? And the problem is that they're not of the same size. Asymptotically, when you let W go to infinity, because the Laplacian plus Wigner is just like Wigner. You can forget about the Laplacian in there. In the blue block, right, where you add the Laplace. In the blue block, right, where you add the Laplacian. I cannot speak about Wigner, but the Laplacian doesn't play any role. No, you won't. I'm sorry. You will. The Laplacian doesn't play any role. I mean, the things that they have the same behavior as the Wigner alone. It depends on the coupling constant. Whatever the coupling constant. And moreover, it's more here. But whatever the coupling constant, it's going to be the same behavior. It's not going to change what's happening inside this block. This is why the only thing that I think is important is the relative size of what's happening outside the blocks and inside. What's happening outside the rods and inside the rods. But the coupling constant is in terms of the size of the block, that's the point. But the coupling constant is small here. Because W plus infinity. Precisely so, but it's in terms of the size of the block. The more so else. So actually the retraction that you add is even smaller. Are you complaining about the localization part of the delocalization? I try to understand how much of it comes from the fact that actually Of it comes from the fact that actually in Hilbert space, right, if you look at the rank, so imagine take just a blue block and add the Laplacian to it, right? You get something which is again like a Wigner matrix. It's going to have the same delocalization properties like the Wigner matrix. And so you have these blocks that are essentially IID. Of course, there are finite blocks, there's only eigenvalues, and things live there, and there, there are, well, Things live there and there they are, well, delocalized because it's a Vega matrix. And then you couple these by just in dimension one, right, two points. In dimension seven, two to the seven points, right? And you'd like to understand, but these blocks