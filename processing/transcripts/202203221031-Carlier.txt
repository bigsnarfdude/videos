With Piero Fiseke and his PhD student Daniela Vogler from Munich, TUM. And it's indeed related, it's not exactly optimal transport, but it's related to exchangeable laws of random variables. It's quite classical topic in probability theory, the Habit and Savage theorem. And there are some motivations which I'm going to describe also in terms of multi-marginal optimal transport with symmetric. Optimal transport with symmetric costs, as we saw in the previous talks. Okay, so let me start with some basic, very basic, and classical stuff. So imagine that you have a family of random variables, Z1 up to Zn with values in some abstract space, let's say a Polish space. And it is called exchangeable whenever for every permutation of the For every permutation of the components, every sigma in the group of permutation of n letters, the law of the initial vector is the same as the law of the permitted Z sigma 1 up to Z sigma n. So there are plenty of examples. Of course, if the ZI's are IID, it is an exchangeable sequence, but there are many examples beyond independence. For instance, if all the viruses instance if all the variables are the same and z i does not depend on z or if the z i are a deterministic function of iid variables y i plus a common noise u it's also exchangeable and there are plenty of non-independent exchangeable laws and a good reason for this is linked to convexity in fact the notion of exchangeability in terms of law of the the the vector The vector is invariant by convex combinations, but of course, mixtures of independent is not independent in general. Now the classical Ewitten Savage theorem is about infinite exchangeable sequences. So now I give myself a sequence, Zi of random variables. So the index i now is a set of integers. Now is a set of integers, and it's called exchangeable whenever you take a permutation of finitely many components, the law remains the same. And now the celebrated Ewitt and Savage theorem, which is a generalization of an older result by definity almost one century ago in the case of binary variables, says that more or less law of exchangeable infinite sequences are mixtures of independence. Of independence. So, in a nutshell, if Z is an infinite exchangeable family, then its law can be represented. You can find a probability measure alpha on probability measures on the state space X, such that the law of Z is a mixture according to this measure alpha of a product measure lambda to the to power infinity, let's say. Okay. Power infinity, let's say. Okay, and the sampling measure alpha is in fact unique. And a classical interpretation of this result is to say you may view lambda itself as a random variable and to say that given lambda, conditional on lambda, the law of Z is the law of, so conditional on lambda, the ZI's are independent. So more or less, you can interpret exchangeability. Can interpret exchangeability in terms of conditional independence or something like this. So now, in the case of a finite exchangeability, so finite length sequence, it's not true anymore that exchangeable laws are mixture of independent. There is a very simple example. So just take n equals 2, take x, which just has the pair 0, 1, and take The probability law, which is given by putting one half on one, zero and mass one half on zero, one, then this law cannot be represented as a mixture of independence, because if it were the case, you could write mu as an integral on the unit interval of a mixture of the Dirac at zero and Dirac of one. And since mu gives zero mass to zero zero and to one one, integrating this relation, you would find that. integrating this relation, you would find that alpha should be both concentrated at zero and at one. So it's not possible. So you see that finite, there's a difference between finite and infinite exchangeability. But of course, you may expect that if n gets large, well, exchangeability for a sequence of lengths n is close to being a mixture of independence. It is actually a very classical and very nice result of Jaconis and Friedman. Nice result of Giaconis and Friedman. So, Giaconis studied this a lot since the 70s, which says that if you have now a finite exchangeable sequence, you can find a probability of a probability alpha such that the corresponding mixture of independence is one over n close in total variation norm to this exchange, the low mu of Z1ZN. Okay, I mentioned also that it is a I mentioned also that it is a classical topic. There are many textbooks and lecture notes about it. For instance, there is a very nice lecture note at Saint-Flur by Aldus in the 80s and a more recent book about 10 years ago by Olaf Kallenberg on the topic. And there are also known results, exact representation results for finite exchangeable laws that go in two directions, basically, in terms of mixtures. Basically, in terms of mixtures of laws of own sequences, so drawing without replacements, something you can find in the book by Callenberg or by some work by Kends and Seckele. And there are also some results which still see exchangeable laws as mixtures of independent laws, but allowing alpha to be a sign measure. So the interpretation is quite. So the interpretation is quite unclear. You lose uniqueness of the alpha. And I will not follow this viewpoint in the sequel. So what I would like to do in this talk, to present this joint work with Gerrot and his students, is to give a definity Erwitt and Savage-like representation, but in terms of some polynomials of empirical measures, and also to emphasize the and also to emphasize the role with the convex geometry and the extreme points of the set of symmetric laws or their k-point marginals. So just a few slides of motivation in terms of optimal transport. So it was mentioned in the talk by Aurelien Alfonci. A problem that has received quite a lot of attention in the recent period is the multi-marginal Multi-marginal coulomb cost case. So you look for a probability measure gamma with fixed one-marginal rho. So rho is typically the density of one electron and the unknown gamma is the density of a cloud of n electrons. So as to minimize this total Coulomb cost, subject to the constraint that, well, since the cost is symmetric, you Well, since the cost is symmetric, you may assume that gamma is symmetric and the first marginal of gamma is fixed. So, let me mention also other works, including some works from people in the audience, like Nassif and Jung and Kim, together with PASS, which we have studied other multi-marginal with or without symmetries of the problem. So, it's quite active topic. Active topic now. And so I'd like to mention two extreme cases depending on the end, the number of electrons. So as you may guess, so dividing by the number of terms in this sum, in fact, you can see this Coulomb problem is the same as looking for a probability measure on R6, mu. On R6, Î¼ with first marginal row, which just minimizes the Kuno cost. But there's a condition that should be the two-point marginal of a symmetric probability measure on R3 to the N. So the dependence with respect to n is hidden in this constraint. Okay, so let me consider two extreme cases. First is n equal to just two electrons. So it was noticed So it was noticed in the beginning, so I didn't mention, but there were two seminal papers on this problem: one by Boutazzo, Gori Georgi, and the Pascal, and at the same time by Gero Friseco, Codina Cottar and Claudia Kupelberg. And for instance, the paper by Boutazzo and his co-authors, they observed that there is a sort of provided the one electron marginal row as. As the density is absolutely continuous, there's a sort of Brunier or Mackengenbo result saying that the optimal plan is concentrated on a graph of some map T, which depends on the gradient of some potential, which solves some sort of dual formulation of this OT problem. And so you see for n equal to, we are in a very correlated We are in a very correlated regime where the position of the second electron is a deterministic function of the position of the first electron. On the other side of the spectrum, if n goes very large, if n is infinite, a case which was considered in a very nice paper by Cotard Friseco and Brandon Pass, it was observed that you can use the Erwitt and Savage theorem in the sense that your measure of mu just should be Measure mu just should be a mixture of independent measure of lambda tensor lambda, an average of such measures. So the problem corresponding to n equals infinity is written here. So you look for a probability measure alpha and probability measures whose barycenter is rho, which minimize the average with respect to alpha of these quadratic functionals. Of this quadratic functional. So lambda gives the Coulomb cost, Lambda tensor itself. But in fact, for instance, writing this in Fourier, you can see that this quadratic function is in fact convex. More or less, it is the H minus one square normal. So in fact, you don't have interest. The best way to do, and under the constraint, that the average of That the average of alpha should be rho is to take the direct mass at rho, which says that for infinitely many electrons, in fact, the optimal coupling is just a rho tensor itself. So it's drawing the position of the electrons independently. So it's quite striking that you have this constraint for two electrons, there is a deterministic coupling, and as n gets large. and as as n gets large uh the positions of of two electrons should should should be uh should be independent so you must you may ask yourself what happens if n is very large let's say 100 or 1000 but not not infinite uh how does this uh independence uh show up and is very much related to to to a non-asymptotic version of errit and savage and also you consider here in the coulomb case You consider here in the Coulomb case, you see that electrons interact just pairwise, but you can imagine K particles, K body interactions, meaning that there are K particles interacting. So you may be interested not only in the two-point marginal of gamma, but maybe it's marginal on more components. Okay, so that was a long introduction. That was a long introduction, but the rest will be quick, hopefully. So, I'd like to emphasize the role of some polynomials of measures in this representation, and then we'll deduce a sort of representation a la choke or a la Ewitten Savage for extremal symmetric clause. So, okay, as I said, I'm going to work on the Polish space. Polish space. So P of X will be the set of Boreal probability measures on X. I will fix a large integers n larger than 2 and a possible subset of positions of indices for the particles K less than N. And the probability measure on XK, I will call it a K-plan. And given an N-plan, so probability measure on XN, Plan, so probability measure on Xn, I will denote by m k of lambda the law of the k-first components. Since I will deal with symmetric laws in a second, it does not depend that there are the k-first ones, it could be k other components of the marginal on k components, not necessarily the first ones. Given a probability on xn and a permutation And a permutation sigma. Of course, I will define the corresponding permuted n-plan gamma sigma, just look at its action on a test function. So I just take the permutation of the components. And a measure on xn will be called symmetric if it is invariant by this permutation. Of course, in case it is not, I can symmetrize it. So Sn of gamma will be the average. S n of gamma will be the average of a permutation of these gamma sigmas. So the set of symmetric measures on Xn would be just the image by this symmetric symmetrization operator of the probability over Xn. And as I said in the beginning, so think of Coulomb cost. I was interested in k equal to, but more generally, I can be interested in k less than n. And I will call And n, and I will call a k plan, so a probability measure on xk, we'll call it n representable if it is the k-point marginal of a symmetric measure over the big space xn. Okay, so p and rep xk will be those symmetric measures on xk which can be extended to a symmetric larger system of n components. Okay, so in terms of So, in terms of this is just exactly what we saw in the introduction, but in terms of laws rather than in terms of families of random variables. Okay, so to see the structure, let me start with something from scratch, from almost nothing, and almost nothing is just a direct mass. So, let me take a point in Xn, so a point X, X1, Xn in Xn. I take the direct mass at this point. I'm going to assume. At this point, I'm going to symmetrize it, and now I'm going to look at the hierarchy of the marginals. So I will denote by Î¼ k. So Î¼ k is the k marginal, sorry, the k point marginal of the symmetrized Dirac. Of course, starting from the bottom, from mu1, mu1 is easy to compute, it's just an empirical. Easy to compute. It's just an empirical measure. It's one over n, the sum of the delta xi. So now let's see what's the structure of the other marginal. So mu2, the second margin, the marginal on the first two components, and so on. So it's an easy computation. This is on the bottom of this slide. U2 is just 1 over n to the n minus 1, the sum of Dirac of Xi XJ. rack of x i x j with i different from the n x i being different from j so it's exactly uh up to a prefactor and to a correction on the diagonal it's exactly a term which is homogeneous of degree two so lambda tensor itself so the independent coupling with a prefactor and there is a negative correction which is on the diagonal to remove uh the ties uh the the ties The ties, the ties I and J. Okay, so note that lambda transform itself is quadratic, it's homogeneous of degree two with respect to lambda, whereas the second term is just linear. I can do the same kind of computation for mu3, so the marginal and the first three components, and still playing with inclusion and exclusion, I get a similar expression. I get a similar expression with the main driving term being lambda tensor itself three times. There is a correction with a minus sign, which is concentrated on a two diagonal. So identity sharp two lambda, or more generally, identity sharp q lambda is to the q is a measure on x q which is defined this way. So it's concentrated. Is defined this way, so it's concentrated on the diagonal of xq, and there's a last term which is concentrated on the free diagonal, and it's totally explicit. So when you see this and you do this elementary computation, it's very tempting to say that there's a general formula to try a sort of polynomial on that, saying that all the marginals of this symmetrized Dirac should be expressed. Should be expressed as a polynomial expression with respect to lambda, which combines tensor products of lambda itself to a certain power with identity to a certain power applied to lambda. So terms which are of degree p in lambda and terms which are linear with respect to lambda. Of course, the computant torics become trickier after k equals Trickier after k equals 4. For instance, if you want to identify terms which are homogeneous in degree 2 in the expansion for mu4, you see that you should distinguish, for instance, the term identity to the free lambda tensor lambda, which is homogeneous of degree two, with two terms. The two terms identity identity squared lambda tensor itself. Okay, so just to see the action on test functions, the first guy is concentrated on four apples of the form xxxy, whereas the second one is concentrated on pairs on four apples xxyy. Okay but still it's quite easy to see that the Anza. It's quite easy to see that the Anzatz is still justified because there's a recursive formula. In fact, it's quite easy to see that the mu k plus 1, so the marginal when you add one point, one component, can be expressed as n divided by n minus k mu k tensor lambda. And there's a correction to make, a correlated correction, which is Which is this sum where Rj is just a sort of repetition operator. So Rj is a map from Xk to Xk plus one, where you repeat the J's component at the end. Okay, so now you see that since we have seen that the mu k for k equals two and three satisfy our polynomial and zats with this recursive formula is certainly true for higher higher values of k. And And okay, to give a non-recursive formula, I need a little bit of painful considerations on partitions. I will give a formula, but once we see the formula, the take-home message is forget about it. It's good to know that it exists. But I'd rather like to stress some comments on this. So if you give yourself an integer j, An integer j. A partition of j of length nth is just a collection of integers p1 to pn, which are sorted by a non-decreasing order and which sum to j. Of course, you may extend such a partition of j by adding zeros after the last non-zero guy. And the range of a partition The range of a partition will be the values take the non-zero values taken by this partition, and the multiplicity. If q is in the range of this partition, the multiplicity of this partition will be denoted by cardinality of p minus 1 q. So, for instance, if you look at partitions of 4 and you consider 2, 1, 1, you see that 1 is in the range as multiplicity 2. So, now here's the formula. It's quite awful, but it is quite what you. Awful, but it is quite what you expect. So, again, I take a Dirac mass, I symmetrize it, and I look at the K-point margin of this guy, where K between 1 and N. And it expresses as a polynomial expression with respect to Lambda. Lambda again is this empirical measure. So So, okay, the main term is an independent one, it's homogeneous of degree k, and there are corrections which alternate signs. There is one over nj decaying coefficient with respect to j and there's a symmetrization sk and pjk is a is a polynomial which is homogeneous of degree k. Of degree k minus j, which is given in this form. So, lambda p, so p is a partition of j of lengths less than k minus j. And this lambda p is just you have blocks of size. So you got linear terms, which are tensor products of lengths, the member of the partition plus one. And at the end, you put all you can being independent. So what remains. Being independent, so what remains is lambda tonsor the remaining coefficient to have homogeneity k minus g. So it's k minus g minus n of p and there's a prefactor like this. Okay, and the proof is not particularly, it's just brute force and brute force computation. Let me give you the formula for Fn5. You can use this formula to expand it. It even though the coefficients are quite nasty, if you look at the contribution of the correction of degree k minus g, the sum of these coefficients can be computed. It's given by this formula. This is called the Stirling numbers, which are the J-symmetric function of the first integers and can be computed. And can be computed. It is very much, this representation is very much in the favor of Diaconiston Friedman, because if you truncate, you only retain the term lambda to the k. The remainder is 1 over n in total variation. And of course, you can go higher if you truncate by keeping the p first correlated terms, the error in total. The error in total variation is in one over np plus one. So you mess yourself at this point: is it worth the sweat to do all this computation? So let me show you a picture, which the bars represent the relative weights of the different terms. So the first bar corresponds to the mean field or the independent term. This is for k equals 4. And for various. four and for various values of n. So you see without any surprise if k equals n equals four, in fact the second term is dominating. But still for large values, for instance for n equals 10, it's still more of the same order. And what's probably even more surprising is that even for n equals 100, the effect of the second of the first correction term, okay, the term which is homogeneous. Okay, the term which is homogeneous of degree k minus one is still visible. Okay, and it's still a few percent of the coefficient of lambda to the k. Okay, since my time, of course, once you have this, it's easy to deduce an Ewitt and Savage-like theorem because if you consider the set of M-quantized probability measure on extent, which are just the empirical. Which are just the empirical measures. So, this is this set, which can view as the image of x to the n by the map lambda, which sends a point to the corresponding empirical measures, something we saw in the talk by Sumic. In fact, the formula we saw before says that for a direct mass, so if gamma is a direct mass, then it's similar. Its symmetrized version is this polynomial f n applied to lambda of x, which is an empirical quantized measure, which is of course the integral of f n n lambda d alpha of lambda, where alpha is the direct mass at lambda of x, which is a push forward of gamma by lambda. So you see that this expression, the Sn of gamma and the right-hand side, given that alpha is a push forward. Given that alpha is the push forward of gamma by lambda, it's linear with respect to gamma. So, this formula extends to all discrete measures, and by density, believe me, it extends to all gamma. So when you symmetrize, you can always write this representation formula, which is very much in the spirit of Ewitt and Savage, which says that these polynomials That these polynomials, these awful polynomials we saw, are exactly the basis which serve as a representation for n-representable k-planes. So there is a probability measure. Again, if a measure is n-representable, it can be obtained as a mixture of these polynomials for a certain probability measure on alpha. On alpha. Do I still have one minute left, Martin? Yes, that's fine. Okay, and the last thing, okay, you recover, of course, Erwitt and Savage and Jaconis and Friedman and also it's like a Taylor expansion if you want this FN case. Just what I want to see is that these polynomials that we see, that we saw, sorry. That we saw, sorry, evaluated that empirical measures, in fact, they are exactly the extreme points of the set of n-representable measures, and they are also so-called exposed. So an exposed point of a convex set is a point for which you can find an hyperplane. You can find an hyperplane which is touching the convex set at this point and this point only. And in the paper, we have a construction on how to expose this Fnk of Lambda, where lambda is a quantized measures. And so what it says, I don't have time to comment too much on this, that there is a very particular structures on this hierarchy of convex sets and of extreme corresponding. And of extreme corresponding extreme points. In fact, this fnk of lambda is a bijective parametrization of the extreme points, not depending on k actually. You do not create or destroy extreme points when you take higher or lower marginals. And there's also a nice rigidity that by construction, this map FMK. This map FMK, which describes the extreme points, which are also exposed, as I said, as a linear left inverse, because by construction, the first marginal composed with FNK corresponds to its mu1 and mu1 was a lambda. So it's these highly non-linear polynomials when restricted to empirical measures, they have a left-hand. They have a left inverse, which is just the first projection, so a linear left inverse. And that's time for me to stop, I guess. Thanks. Okay, thanks a lot for this nice talk. Are there any comments, questions? Audience, can I ask from the room? Sure. Sure. Giovanni, this is very nice talk. So, John, do your coefficients have anything to do with the event sampling formula? Sorry. As the evil sampling, yeah, absolutely. Absolutely. Yeah, it is connected to the event sampling. In the paper, we give some details about it. Yeah, that's right. Absolutely. Sumik, I cannot see you, but that was you, right? Sorry? That was you, Sumik. Sorry, that was you, Sumik. Yes, that's right. Yeah, so the paper is on the archive. I can find it. It is. Yeah. Are there any further questions or comments? Okay, so it does not seem to be the case. So, since we are also already running late, let's thank Guillaume. Other questions which pop up can be asked later, maybe during coffee break. So, and we'll now have a very short technical break, I guess, to switch on the