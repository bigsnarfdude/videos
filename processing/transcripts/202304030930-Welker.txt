Okay, so we have our next speaker this morning with Wilkeswell, and he'll be talking about the Garland method and its extension and potential new application. Great, thank you very much for the invitation. Thank you very much for the chance to speak here. So, my title was chosen ambitiously. And I don't think I talk to live up to that because I just realized. That because I just realized that the talks are actually 25 minutes, but for that, um, so I will spend probably quite a lot of time explaining the original problem, assuming that what if you don't know what this is. Is this true? Okay, good. Okay, so everything which is new, the extension, this joint work with Eric. And this joint work with Eric Best and COVID. So, what is Garland method? Gerald method emerged from geometric group theory. So, the setup is simplicial complexes. You just heard about it. So, we have a simplicial complex over some ground set. The elements of the simplicial complex are usually called the faces. The dimension of the face is the cardinality of the face minus one. And the dimension of the simplest. And the dimension of the central complex is the maximum dimension of one of these faces. That one was a quick one. I think that is so now what Darling method will do is it will show that specific homology groups of this solution context are trivial. They're zero. And it can do that only if the field. The field of coefficients is a field of characteristic zero. It will not work on a finite field of integers. It will not tell you much about it. So, okay, what we do is we set up the usual setting for defining simplicial cohomology. There's a specific reason why we use cohomology instead of homology, which will become clear later, the way it's set up. So, the ice simplicial homology chain group, the co-chain group of delta for the simplicial complex delta, is just a direct sum over all phases of dimension I of the simplicial complex. Choose a basis element and take the real vector space with these basis elements EC. Now, after fixing some orientation on the ground set or on delta, we have the simple co-boundary. Sim uh co-boundary operator going from the i's to the i plus first co-chain group of delta, sending e sigma to a linear combination of e toss, where tau is, where sigma is the phase of tau, and then we have, depending on the chosen orientation, we have a coefficient epsilon sigma tau, which is something plus minus one, and of course, yeah, no. Yeah, no, because I don't assume that's a full face, it could also be C. I think so far, again, it's very basic. Now, there is one thing which has been studied a lot by many people here, actually. This is the Laplacian of for a simple complex. Laplacians exist in every dimension. So, what you do is So, what you do is you look at your boundary operator or your co-boundary operator, and you choose on your co-chain groups your favorite scalar product, choose your standard scalar product, and you look at the co-boundary operators and their adjoints. And then what you do is you look into some dimension, say i. To some dimension, say I, and then with your co-boundary operator, see so, like here, you have your co-boundary operator, you go up in the I plus first cold chain group, and then you go down again with the essentially the boundary operator, the adjoint to the co-boundary operator, and you add to that the same going downward. So you first go with the adjoint of the I minus first co-boundary operator. The I minus first co-boundary operator downward to the I minus first co-chain, and then you go up again. And you add those up. And the components are sometimes called the positives, the one going up and down, and the negative is Laplacians. Yeah, just so things. And they are interesting for computing homology because the kernel is an easy exercise. You can give your Exercise you can give your linear algebra students the kernel with not telling about homology but saying like this isomorphism between the kernel of Li and this quotient defining the cohomology group of delta. Okay, so that's one ingredient you will need the actually the positive Laplacian will play an important role and for that For that, but before we can continue, we need another few definitions that definitions that we probably also all know. So, what is the link of delta? The link of sigma in delta for a phase sigma of delta. So, that's all tau in delta with trivial intersection with sigma and the union of the two is in delta. So it's like geometrically, it's the triangulation of intersection of your geometric realization. The intersection of your geometric realization of your simplicial complex with a small sphere built around C. And we need two easy definitions. We call delta locally I minus one finite if the link of every sigma in my central complex where sigma is of dimension i minus one is finite. Well, I mean, if you don't want to worry about infinite complexes, just assume every complex I'm talking about here is. Complex I'm talking about here is finite. There will be some subtleties, but that will be an important condition we need. And we need delta to be locally connected at dimension i minus one, which means again these links for phases of dimension i minus one are connected simplicial complexes. Okay, so these two conditions will appear in the Garland method and now Garland method. And now the Garland method, which goes back to problems with geometric group theory in the 70s or 60s, and then was picked up again for mostly in the infinite case by Bauman and Svienkovsky and Suck. So what is the condition? We have delta locally I minus one finite. So all the links are finite, locally connected. And I mentioned I minus one, all the links are connected. Links are connected simplicial complexes, and then we look at all those link complexes for the dimension for which we have applied our conditions. So sigma is of dimension i minus one. We look at all those link complexes. We look at the positive Laplacian of these link complexes, and that's a self-adjoint operator. So it's as a, as we all know, it's a spectrum in the reals. Spectrum in the reals. And we look at the smallest non-zero eigenvalue of this Laplacian, this pass or this operator. And we do also some normalization where we normalize by the degree. So there's some normalization here going on, which you can also build actually into the adjoin that is scalar products you are using, but for the standard scalar products. For the standard scalability, you have to normalize here. So, if you look at all those eigenvalues, and if all those eigenvalues are bounded from below, are larger or equal to, or bounded from below by i over i plus one, the dimension we are looking at, then the i-th cohomologic move of this simplicial complex is this. That's kind of a local to global situation. You know something about spectral properties. Properties of the upper Laplacians, the positive Laplacians of the links, and then you use something about the homology of the global. Okay, so this is the method. And if everything is, if delta is not finite, then just use L2 for model. So this is the classical method. And this has And this has some requirements of how this thing happens. So now the question that Eric and I were discussing was: I mean, first of all, if you ever have looked into the proof of this method, The proof of this method, that's really a mess. I mean, this is the lengthy calculations. I mean, these are great mathematicians who did that, and I don't know how they came up with these proofs. I mean, this is really amazing. So we wanted to understand a little bit better conceptually why this is true. But we also wanted to understand if this is restricted to simplicial complexes, or if we could go beyond that and talk about. In talk about cubicle complexes, ohedral complexes, or generally just gene complexes, and see if this is applicable in this situation. Whether we can get rid of the local finiteness. That's maybe something that in a central complex situation where we deal with central complexes, say, also interesting in community. Interesting in community of algebra is not such an big restriction because most of the exhibition complexes are finite there anyway. And then also about the local connectedness. And in some sense, our result, I mean, I'll probably not be able to finish our generalization in or finish a description of our generalization in detail, but what I can say is we can get beyond cypressial complexes, we can get beyond local connectedness, then we won't have. Then we won't have a result that the homology is zero if we drop local connectedness, but it will have a bound on the size of the homology group. The same holds if you go to non-centric complexes. There will be a certain combinatorial bound on the highest cohort. But global finiteness, we don't know how. Okay, so what is our experience? Okay, so what is our extension? So, how do we describe that? Okay, so this is actually now essentially homological algebra. This is very general. I mean, we start with three Hilbert spaces. So, Hilbert spaces essentially we need, because we need a scalar product. We need adjoints of our maps. Before our adjoints were the adjoints of the co-boundary operator, which you could have also just picked as the boundary operator. But in a more general context, Data, but in a more general context, it's not clear what is the depth in the other dimension. So we pick three of them and we pick subspaces, subclubble spaces, and then we create a commutative diagram of that type. So we have a three-term complex of Hilbert spaces. So there is a map A minus between A, capital A minus, capital A zero, and map A plus between capital A zero and capital A plus. zero and capital A plus. These are bounded maps of Hilbert spaces, where we have adjoins. And of course, it's a complex, so composed the two give us zero. So this is, and then here we have the subspaces. So we project down. So this is the downward maps are the projection maps, and which we call pi. And these projection maps make Make these p's are essentially induced by the projection maps and the maps in the first row to make it trivially a commutative. And then also the subspaces have one particular requirement. The first subspace, it must be similar. Okay, so how does this resemble what I have talked about before? So about chain, co-chains and chains. and uh so the okay so they in the what i've talked about before the a minuses and a zeros and a pluses they are the uh the uh oh actually now i wrote the chain co-chains okay so we lift this minus one here this zero here and the one here so these are One here. So these are the first few coaching groups of the link complex. So for our link Laplacian, we needed just essentially this is a graph Laplacian, and we need just the first few coaching groups of that. And the P's, they are, it also should be coaching. They are, it also should be coachings, the coaching groups we are interested in. So it's the I minus first coaching group of delta, the i coaching group of delta, and the i plus first coaching. Now, and the maps between the a's, they are the component-wise differentials. So this is the differential, you fix a sigma, then you have a differential going from c minus one, the whole boundary to c0. The whole boundary to C0 of the corresponding link for the same signal, right? And we just sum this up. Is that clear? The more mysterious thing is, okay, if you look back, you have here the ps should be subspaces. That's not so obvious how this thing here is a subspace of this. So, how the I minus first coaching group of delta Group of delta can be embedded here. That's a bit more mysterious. For the minus, actually, it's less mysterious because for the minus, this one, this is the C minus of the links. So this has just one generator and this corresponds to sigma. And here, C minus one of Ci minus one of delta also is one generator for each sigma of that dimension. So therefore, That damage. So, therefore, the isomorphism of A minus and B minus is kind of more obvious. How you get those here and those here as subspaces that embedding is more complicated and actually meets also the assumption we have in our drawing. So, that's not so obvious to create this. To create this, how to create with these definitions, these definitions, this situation we have. Right, so this is the situation we create from our original situation. And this thing here we call a Gaudin structure. That's these A's and these B's together, they give a Gaulin's form. And now the Garland method, in some sense, is trivialized, not completely, but assume you have such a Garland setting, then you define two numbers, alpha G and beta G for this Garland, I understand structure, for this Garland structure. These A's and B's that are commuting give a commuting diagram. give a commuting diagram. So you have some alpha g is some infinum over alpha in the kernel of alpha minus adjoin. And then you apply alpha a plus to this phi, take the norm squared, and beta g is the supremum of all phi in kernel b plus, normal phi is one a plus of phi. So this in principle are two operator norms to strictly do some subspace and or like Um, or like the operator norms. You have a here similar to an operator norm. The first one is essentially the spectral gap, the smallest eigenvalue of the A plus, A plus adjoint. The second one kind of encodes the combinatorial structure of B. And then our Garland lemma, which is essentially the Garland method, if you know how to set up the Garland structure. How to set up the garbage structure properly is that if the alpha is less than the beta, then the B complex is exact and there is no homology in the B. So, and if you remember how our B should look like, this is exactly the three co-chain groups, Ci minus one, Ci and Ci plus one. And if this is exact, it means the isocomology group is three. True, okay. So now, but this is not this proof from this definition. This is again something a student can do easily once you know that it's the student should prove that. What is more complicated is what I had on the preceding slide. How do you get this garland structure set up such that this situation, already the original situation, appears? Original situation appears in this. This is like the complicated thing, and it's technically involved. How much time? Five minutes. We have still five minutes. Okay. So, oops. So for that, we essentially extract combinatorial data from a general chain complex. Say, Say we have our complex P that we are interested in. We have it get like think of this code chain complex that we were just looking at. And then you choose the basis, the natural basis in the simplicial complex case, of course, are the n minus one simplicity, the i simplices, the i. One simplicis, the i simplicis, and the i plus one simplicis. You consider, you order them by inclusion, you get a postet. The poset has three ranks: one at i minus one, one at i, and one at i plus one, this special case. There we have three posts, three posets. And then we have some conditions of these postets, of this postet, and this is a graded positive with three ranks. First is that below every element of this. This is the middle rank, there is a fixed number of elements in the bottom row. So, for example, in this intuition complex case, if this is an I simplex, then in every I simplex, there are I plus one, I minus one simplices. And then there's the same condition for the top rank. There is a fixed number of elements in L. Elements in S minus below that, that you can also easily compute in a simplicity complex. Then one has a condition that if I have an element in the top rank and two elements below it, then below the two, there is a fixed number of elements that is below both of those. So in a simplicial complex, this is an I plus one simplex. These are two O-dimension one figures. Are two O-dimension one phases and they contain exactly one O-dimension two phase. So this number is one. And then you need your information from your complex. These are these numbers that assign to these edges essentially coefficients in the boundary operator. And these must satisfy a certain condition, this one. And in addition, this one we could relax, but it will become more technical. It will become more technical. In addition, we want that between a top and a bottom element, there are two middle elements. That's, of course, satisfied with cheapness. What? Cubical things don't have that. But I didn't say that this always comes from this situation. It doesn't also work because you also have the condition that S greater than B is connected for every B. So that will also not resolve the situation as a initial case if it's not connected. Initial case, if it's not connected. So, this poll set you create out of your simplicial cubicle or whatever complex or your just chin complex, you create it with a certain trick. And once you have this created, then one can prove the following. If you have such a girl in poset, so the poset that has these all these conditions. Conditions signed before, then there is a Gaulin structure. Well, if not some Gaulin structure, intimately related structure, don't tell now what the construction is, such that the corresponding beta, well, there were these two invariants alpha and beta, so that for every gaunt structure, so that alpha is larger than beta, then the middle homology is zero. But here you can bound your beta. So, when alpha is larger than this number, it's also larger than beta. And if you think of the simplicial complex case, n0 was i plus 1, n 0, 1 was 1. So this is i plus 1 minus 1 is i over i plus 1 was exactly the bound from God. So this leads, yeah, this leads to a simplistic, yeah. Okay, so now. Okay, so now we can do this for also for cubical complexes, also for asymmetrical complexes that are not locally connected, but then the poset we create has additional data. It's larger than just the face. So, like for the cube, as Andre mentioned, there are faces that don't Faces that don't intersect at all, like you have a big cube, you have two opposite faces, they don't intersect. So, they have an empty intersection that would violate this last condition. So, what you do instead, you add in the positive additional elements. They give you additional constraints because now you have to look also at coefficients that in this case don't. Coefficients that in this case don't come naturally. You have to find them. You can find them. And you have to control the spectrum. Also, for this additional graph-like structures you have. And if you do that, then with the next proposition, you can apply the Garland method to these components. So this is highly technical, and I won't be able to debate this in this last one minute, or I don't know, minus one minute. Minute minus one minute. So, so this is just to give you a rough idea. And I thought that this conference would be a good idea to present this because I also think that this method has a potential for complexes in, say, commutative algebra. There are a lot of complexes there. Of course, most of them are over a polynomial ring or a regular ring or whatsoever. So this works over. So, this works over a field, and it's also works only over a field of characteristic zero. So, what you would do is like go into great or whatsoever. And indeed, the classical coalitive method can be easily applied to like Koso-type. But maybe there are other complexes from which you are able, or one can is able, to extract these carlin posets and actually control them in a nice situation. And then the result, as I said, is. Then the result, as I said, is not necessarily a vanishing. Well, the result is a vanishing, but not a vanishing of the homology of the complex you are interested in, but a vanishing of some other complex, the complex related to the scale deposit. But then, by a short exact sequence, you can get a bound on the dimension of the homology you are actually. Okay, so that's about it what I wanted to say. It's a very rough and Say it's very rough and technically.