Okay, now let's get back to the last talk. We have our very own Jean Bernoulli, Robert Lomanana, talking about the sparse parity matrix. Okay, thank you. First of all, I want to thank the organizer of the conference for inviting me. For inviting me in this wonderful conference. And I want to mention that this work on the sparse parity matrix is a joint work with Amin Koyauklan, Jun Li from Qato University, Oliver Cooley, and Mihan Kank from Teu Grads. So what are we studying here is that our Is that our problem, our model can be viewed as an optimal Bayesian inference problem? So, more specifically, we are given an n by n matrix A over F2, where each entry is one with probability t over n for a fixed parameter t positive. Now, the ground truth x is a vector that is Is a vector that is drawn uniformly at random from the colon space of A. The observed data is then the matrix A and the vector y hat, which is equal to A times X hat. And our goal is to recover the crown truth X as best as possible, given the matrix A and the vector Y hat. A few lines. A few lines of linear algebra showed us that the posterior of the crontroof S, given the data A and y hat, is just the uniform distribution on the set of solutions of that random linear system. And that is the reason why this inference problem is optimal, because we can assume that the statistician or the person that case. Or the percent at case that we take from the posterior distribution. Moreover, we introduced this quantity R bar A, which is just the expectation of this overlap conditioned on the matrix A and the vector y hat. I will just refer to this as the condition. Refer this as conditional overlap. Now, this concept of overlap is intrinsically connected to the replica symmetry answers that we have from statistical physics. So, what do we mean by replica symmetry here is that this overlap Overlap R X X R is concentrated in its conditional overlap. Moreover, we have a more stronger assumption, which we call strong replica symmetry, which means that this conditional average concentrates in a deterministic value. And here we just take the expectation. Now, Now, in general problem, Piasian optimal inference problem satisfies the strong replicasymetry assumption as it was explained by Jean-Barbier in his talk two days ago. But here we provide an example where the problem is replica symmetric but fails to be strongly replicated. But fails to be strongly replica symmetric for an infinite range of parameter t. To introduce this more correctly, let alpha lower star be the smallest fixed point of this function phi t and alpha upper star the largest one. So our main research reads as follow for t less than e we have Less than E. We have strong replica symmetry. First two fixed points have the same value and the value at which, the deterministic value at which we overlap concentrate is given by this one plus alpha low star divided by two. And for degree turban E, replica symmetry. Replicasymetry condition still holds, but the strong replica symmetry condition is not. So indeed, what we have is that alpha lower star is strictly less than alpha upper star. And with probability half, this conditional overlap will concentrate at this value 1 plus alpha lower star divided by 2. And with the same process, Two and with the same probability, it will concentrate in a different value here. Our problem is not just a problem about inference, but it is also a constraint satisfaction problem. So it is at least superficially related to the random cake source that, which is a problem that has been extensively studied. That has been extensively studied before by people like Acleoptas, Moloy, Montanary. So to be concrete, the random cake sourceat ask for the solution to a linear system a k times x is equal y over f2, where this a k is a random n by n matrix where each row contains exactly k1. exactly k1 and the vector y is chosen chosen uniformly and independently of this a k so it is well known that there is an explicit shaft threshold of m over n at which this cakes or stat problem admits solution and beyond which there is no solution anymore so this is Anymore. So this is proved by detail and sorting. So moreover, this Kaxo satisfiability threshold coincides with the threshold where frozen coordinates appear. What do I mean by frozen coordinate? So it's coordinate I in N such that in every entry of a solution X, it takes x, it takes the same value. And the precise number of chosen variables in this random case problem concentrates on a deterministic value. And a nice overview of this can be found in the book by Mesar and Montanary in Information Theory. So, how does our problem compare to this cake source? Of this cake source. So, first of all, our linear system always has a solution because the vector y is taken from the colon space of A. And here, instead of looking at the aspect ratio of the matrix, our main parameter is this density t of non-zero entry. And what And what was very striking is that for T criter, the number of frozen variables fails to concentrate on a deterministic value. So we do not observe this typical 0, 1 law that we observe at the phase transition. To make this more formal, let's call a coordinate. A coordinate i frozen if it takes the value zero for all vectors in the kernel. And we can do that because the set of solutions that we will have in our problem is just the translation of the kernel. The set L A will be the set of frozen coordinates, and F of A will be the fraction of frozen. Will be the fraction of Rosen coordinate. So recall this fixed point alpha lower star and alpha upper star that we had for the function phi. So the result about freezing is as follow. So if T is strictly less than E, then the fraction of frozen variable is about alpha. is about alpha lower star, which is equal to alpha upper star with probability one. On the contrary, when d is greater than e, with probability half again, it concentrate to the lower fixed point, and with the same probability, it concentrates to the larger fixed point so. So it's kind of easy to go from the freezing result to the main theorem. So first, a simple inference algorithm such as Gaussian elimination can get the frozen variable i in L A of the ground truth right. But any other inference Any over-inference algorithm can only get about half of the unfrozen variable, right? So this conditional overlap RA bar will be about one minus fraction of frozen divided by two. And the main theorem will just follow from the result on freezing. Now I want to talk a bit about I want to talk a bit about the method that we used to prove this result. So, first of all, the formula for venuity of A is known and it was found by Amin, Koyaw Klan, Ergur, Gaw, Heterish, and Rolfjen. So it reads as follows: Vernility of A over N. of A over n as n is tending to infinity tends to the maximum of this function psi t for alpha and 0, 1. And this function psi t is somehow the beta free energy when we consider the system as a Boltzmann system where the Gibbs distribution. them where the Gibbs distribution is just the uniform distribution over the kernel. So what should be intuitive in here is that this maximizer alpha should asymptotically coincide with this fraction FA of frozen variable. But the proof that is used there, which is based on some Based on some Eisenman Simstar argument, where we try to find the change in the nullity when you delete or you add some row in the matrix fails to imply this fact directly. Nevertheless, it's interesting to look at how this function is. To look at how this function phi and this function psi behaves. So, what we found is that we have this function psi, which is the beta free entropy, and this function phi will somehow be related to a message person algorithm which Algorithm, which we called warning propagation, that Oliver Cooley talked about yesterday. So the fixed point of this function phi t is in one-to-one corresponds with the stationary point of this psi t. And now when t is smaller than e, like the blue graph here, we just have Here, we just have one fixed point, and that one fixed point will correspond to a unique maximizer of our function psi. Then, when we get to a value greater than e, we get three fixed points in here. So, two of them are stable, meaning that the derivative at voice points is less than one, and one is unstable. Is unstable. So the two values that are stable will correspond to the maximum, the unique local maximum of our function phi here that is achieved at the same height as we see here. And now the unstable fixed point here will correspond to a minimum of that function. That function. And in one part of the proof, this function phi will play a role, and in another part, this function psi will play another role. So the first step of the proof is a concentration result. And in order to get that, we turn to a graphical model. A graphical model mainly the matrix A can be seen as the factor graph G A where one side of the factor graph is corresponding to the rows of the matrix which we call check node and the oversight will correspond to the colon of the matrix for the variable. matrix for a variable which we call variable node and now there is an edge between two of these vertices in different class if the corresponding entry in the matrix is one which means with probability d over n we will have an edge between say a0 and v1 so the graph that we get here is just the erdoshreny random graph random graph that is bipartet and the edge probability is the over n and we know that this random random graph has a poison dilocal structure and it has very few short cycles so what we do is then use warning propagation to track the frozen variable in Frozen variable in our system, and this warning propagation algorithm explained by Oliver Curry is a process similar to the billing process for the random graph. And the update rule that we use will correspond exactly to this function phi. And at the end, we prove that this fraction of frozen variables. This fraction of frozen variable will be one of the fixed point of that function phi. So this is the bit where we use that function phi. Now our result is a result about the stable fixed point. So we somehow need to exclude this unstable fixed point. So if we just If we just observe the nullity formula, which gives us the dimension of the kernel, and knowing that any entry in a vector can only take two values, the number of solution would be approximately two to the maximum of the function as given by the formula in Unity. Now, Now, if we suppose that instead this fraction of frozen is about this unstable fixed point, then we will also get that the expected number of solution is about two to the phi function applied at this unstable fixed point times n. But this will be But this will be somehow in contradiction with venuity formula. So we conclude that this portion of frozen variable being near this unstable fixed point is unlikely. Now, the last step of the proof is saying that the end stable, the The unstable, the two stable fixed point are equally likely. So, what we found is that the Warneil propagation algorithm will predict that about alpha n proportion of variable or frozen, about one minus alpha upper star n proportion of variable or unfrozen, and the rest. And the rest are uncertain, and we call this uncertain part slush. Now we have found that this F of A is concentrated around alpha lower star and alpha upper star. So the only possible outcome is that either the slush is entirely frozen or it is entirely unfrozen. And frozen. To see that, we considered the minor AS of A induced by the variable and the constraint in the slush part of the graph. So if this minor has fewer rows and colons, then the slush would be unfrozen. If it cannot completely freeze, you have a bunch of. Freeze, you have a bunch of free variables. On the other side, if this miner has more row than colon, then the slush will freeze. So now the warning propagation algorithm that use and the matrix A, they are invariant under the transposition of our matrix. So this minor AS will This minor AS will be over constraint as often as under constraint. So then we conclude that this slush part of the graph will freeze with probability about one half. So that's how we get to this half and half probability at the end. Now I want to give Now I want to give two further research directions. So it will be interesting to see if our technique apply to problem related to uniquely extendable CSP. What do I mean by that? It's CSP where you have one clause, this clause has exactly k variable in it. Variable in it, you know the value of the k minus one k minus one variable, then you only have one possibility left for the last variable. So this warning propagation technique that we use, it would be interesting if we can use that for this extendable CSP. Extendable CSP. The next problem is that about matching problem on bipartite random graph because the graph that we looked at here is then just the biparted F-tor-Shrini random graph. Thank you for your attention and I welcome any question. I will try to answer them as best as I can. Also frame as best as I can.