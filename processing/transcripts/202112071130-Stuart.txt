Good afternoon, everybody. My name is Georgia Stewart, and I will be, as Norway said, talking about oil spill source location using Bayesian techniques. This is work that is joint with my research group at UT, Victor Chen, Samuel Estes, Eric Velseth, and Clint Dawson. So, a little bit about me. My PhD is from UT Dallas, as I said yesterday, and I'm currently a postdoc at UT. And I'm currently a postdoc at UT Austin in the Computational Hydraulics Group at the Odin Institute for Computational Engineering and Sciences. Primarily, my work there has been an oil spill source location, though my background in my PhD is in seismic inversion. So I've done several applications in geophysics. In two weeks, I will start a new job as a research scientist back at UT Dallas in the Cyber Infrastructure Research Computing Group in the Office of Information Technology, where my focus will be on high-performance computing workflows for uncertainty quantification. Uncertainty quantification. And I will also be at AGU next week if anybody wants to meet for coffee or something. So let me know. Today I will be giving a problem overview of the oil spill inversion problem. And I will be talking about how we model oil spills with Lagarangium particle tracking. And then I'll give you an overview of the package we've been working on for doing LPT, which is Pyopatra. Then I'll go into the Bayesian formulation. Then I'll go into the Bayesian formulation and end with a numerical experiment. So, quite a bit of this will be repeated from talks that you've heard earlier with the Bayesian formulation, but I hope to give you another application for inverse problems that's not seen very often. So what is our problem? So there are many oil spills every year that occur around the world. Many of them occur in the Gulf of Mexico or in the Red Sea. Those are two of the most. The Red Sea. Those are two of the most common locations for oil spills. And sometimes we get oil spills that we detect on satellite imagery that we're not quite sure where they come from. So, for example, in the recent hurricane, Hurricane Ida, which went over Louisiana a few months ago, there were several oil spills that occurred off the coast of Louisiana that we picked up on satellite imagery, and it was not immediately apparent where they came from. So, for example, I have here a picture of the oil slick. You can see Of the oil slick. You can see the oil slick off the coast of Louisiana by a wildlife refuge. And this is one of the ones where it wasn't immediately apparent where it came from. So our goal is to use satellite imagery of oil spills and then transform that into a format that we can then use for an inverse problem to find that source location. So this problem includes a little bit of image processing, it includes Bayesian inversion, and it includes Bayesian inversion and it includes trying to find novel techniques for setting up the likelihood function. The image processing part is primarily work by Victor Chen, the student that I am working with. So I'm not going to talk too much about that, but I just want to make sure to give him proper credit for the satellite processing. So our four problem in this inverse problem is oil spill modeling. Now oil spill modeling is its own field. Oil spill modeling is its own field. There are many ways of getting increasingly complex models of oil spills, but one of the simplest and most computationally efficient ways is called Lagrangian particle tracking. So with LPT, Lagrangian particle tracking, oil spills are models as collections of discrete particles. These particles are propagated independently according to some ocean model. Now that independent propagation is really helpful because it makes it easy to parallelize on a high performance computing. To parallelize on a high-performance computing machine. So that makes it very computationally efficient when we're doing something like Markov chain Monte Carlo, and we need many, many, many realizations of the forward problem. LPT codes, including mine, which I will show you in just a moment, are based off of a variety of ocean models. So some ocean models include HICOM, ADCERC, MOHID, POM, and MITGCM. Those are all different ways of modeling the currents and turbulence that occurs. And turbulence that occurs in the ocean. So the LPT is not modeling the ocean by itself, it is taking an external model and then it is using that to update the location of the particles. So with Lagrangian, particle tracking is actually quite a simple concept. So we look at the change in location of the particles, so dx dt, dy dt, and dz dt, where x is in the longitudinal direction, y Longitudinal direction, y is in the latitudinal direction, and z is in the depth direction. So we have two terms for x and y. The first term are the drift velocities in the longitude or latitude direction or depth. And these drift velocities can incorporate all sorts of things. They can incorporate currents, wind, and wind forcing as the primary two, which my LPT code takes into account. More complex ocean models may take More complex ocean models may take into further things that impact the drift velocity. And then we have the second term, which incorporates turbulence or random diffusion. So in the ocean, there are structures below the size, the size of the currents and the size of the cells that we discretize our model with. And we can model that sort of turbulence, which could come from ocean waves, for example, or randomness in the wind, all sorts of things. Randomness in the wind, all sorts of things as a random diffusion. So our LPT process is a random walk process. And then finally, in the Z direction, which is depth, we also have buoyancy of the particles. So different particles, different types of oil have different buoyancy properties, which determines how quickly they react to the surface, what shapes of droplets they form on their rise to the surface, and so on and so forth. So that's why the Z direction has That's why the Z direction has buoyancy information as well. But at its heart, we're looking at the combination of currents, wind forcing, and diffusion as our primary factors. So in order to enable this research, I have written a new code called PyPatra, stands for Python Ocean Particle Tracking. If you're interested, the source code can be found on my GitHub, and we also have Docker images. And we also have Docker images for ease of use if you don't want to recompile the code yourself. I will warn you, though, that the code and documentation are very much a work in progress. So if you would like to use an LPT code, just send me a message and I'll walk you through it. A little bit about Pyopatra. So right now it can currently read HICOM and MoHED meshes. So those are two ocean models. They're both structured meshes and soon it'll be able to read ADSER. And soon it'll be able to read ADSERP, which is an unstructured mesh. Even though HICOM and Mohid are structured meshes, Pyopetra formulates, when it reads it in, it formulates it as a triangular unstructured mesh just for the flexibility of being able to use AdCERC as well. Pyopetra is written in C ⁇  with Python bindings. Currently, that Python binding is required to read the meshes, but I'm working on a pure C implementation. But once you have the mesh built, Once you have the mesh built, you don't have to rebuild the mesh every time in order to run the inversion. You can do the forward problem over and over and over again for an MCMC process, keeping that mesh intact and just resetting it every time. So the mesh building, even if it's like a 20 gigabyte mesh, that only has to be done once, and then you can do the forward problem as many times as you would like. It's parallelized using MPI, using the shared memory functions for large meshes. So you can, this is. For large meshes, so you can. This is supercomputer-ready, it will duplicate the meshes across nodes but not across processes. And currently, I have 95% test coverage on both the Python and C ⁇  codes. So here's an example of the propagation of the particles in a ocean field. This example does not have wind forcing. I'm leaving wind out of this simulation. Of this simulation, but for example, we want to drop 100 particles in this eddy. We can watch the particles flow through the eddy, and you'll notice that the clumps of particles kind of fans out as it goes, and that's the impact of the random diffusion. So if there was no diffusion, they would not, it would be completely deterministic. They would stay together. But because there is that random element, they start to fan out even though they're being carried by the current. By the current. And this current is in the Red Sea. And I want to note that in this inverse problem, the starting location for the forward problem is very influential. So I can place a clump of particles right here in kind of this column location. Column location. Oops. And my thing has broken. Darn it. This worked perfectly every time until I went to go give my talk. Okay, but you got to see one flow of particles. Okay, so the problem is we need some way to compare density information from an observed spill that we get from satellite imagery with the output of the Lagrangian particle tracking. So the output of the LPT is a collection of discrete particles. Is a collection of discrete particles. So it's not immediately apparent how we compare it with the satellite image. So our proposed solution is to leverage ideas from comparing probability distributions. So we can turn our satellite image into a density map that shows the density of oil in different parts of the ocean. And we can also turn the output of our LPT code into a density map. And then we compare the two using ideas from comparing probabilities. Using ideas from comparing probability distributions. The satellite imagery density map is still a work in progress with Odin Institute CHG student Victor Chen. And we have so far for this explored the Wasterstein metric and the Bhattacharya distance, and we're working on the Jensen-Shannon at the moment. So I will talk briefly about what the Wasterstein metric and the Battacharia distance are. So the Wasterstein metric is the The Westerstein metric is the minimum cost for transforming one probability distance into another. I actually like the computer science term for this metric better, the Earth Mover's distance, which is W1, because I think that's more descriptive. You can imagine a big pile of dirt and you're trying to turn that pile of dirt into another pile of dirt of the same volume. And how much energy does it take to move that dirt to the new location? So this is the So, this is the Wasserstein metric. We have WP and we're working on a radon space, and we have the set of all company couplings of marginals, mu and new. But we have a problem with this. It's a little bit complex to calculate, especially when you're dealing with two dimensions or more. The oil spill inversion problem is either two-dimensional or three-dimensional. Dimensional in the just in space. And you might have a time component as well. And you might take into account that you might invert for like the wind forcing coefficient or the diffusion coefficient. So it could be up to like a six-dimensional problem. So what we do instead is we use an approximation called the Slice-Wasserstein metric. So the Slice-Wasserstein approximates The slice Faster sign approximates Faster sign by calculating projections of the probability distribution in whatever dimensions it's in, and then calculating the one-dimensional Earth mover's distance along those projections. So the Earth mover's distance, also known as W1, is much more computationally effective and easier to compute than a true two or three-dimensional Bosterstein metric. So we do use the slice Bosterstein. So, we do use the slice Westerstein to approximate the Westerstein metric. The downside to this is it is a stochastic process. You are taking random projections over the probability distribution. So you do get a variation in the output of the objective function if you're using the slicefoster stein as your objective function. So that's one thing to keep in mind. And the other thing to keep in mind is that it is not a cheap process. Is not a cheap process. So on a supercomputer, Linstar 6 at TAC, when you're doing the inverse problem, most of the time in the calculation is spent calculating this objective function. And it requires a lot of inter-process communication to collect all the particles from all the processes, which results in quite a computationally inefficient. A quite a computationally inefficient objective function. So, another option is what we call the Bhattacharya distance, which instead of measuring the amount of energy it takes to turn one distribution into another distribution, it measures the overlap between probability distributions. So, the downside to using Bhattacharya over Busserstein is that if you have, say, two distributions that are very close to each other, but disjoint. Close to each other, but disjoint, the Battachario will not reflect that they're close together. But the Vosterstein will, because it will take less energy to transform to close distributions than to further away distributions. But the Bhattacharya does not reflect that. So there's pros and cons to using both. But the nice thing about the Bhattacharya is that it scales really, really well. So here I have a scaling picture for the Slice Wasserstein and the Bhattacharia. Slice Wasserstein and the Bhattacharya. And you'll notice that the Battacharia distance scales perfectly. So, as you double the number of processes, the computation time halves. Whereas the slice master sign, we hit a turnover point where it starts to get more expensive the more processes you add to it. So there's a trade-off there. But I will note that the slice of Esterstein doesn't really turn over. The slice of Esterstein doesn't really turn over until around a thousand processes, so you can still get quite a bit of scaling there. So let's move on to the actual inverse problem now. So you've seen Bayes's rule several times in this conference so far, so I'll go over it pretty quickly. So we want to get the posterior distribution, which is the distribution of the model theta after the data D is incorporated. And to do that, we look at do that we look at um the we look at Bayes's rule which says that the posterior distribution is proportional to the likelihood function times the prior distribution so the likelihood function measures the misfit between the observed data d and the simulated data for the model theta and the prior distribution is what we know about the distribution of the model theta before the data is even taken into account and as noted in an earlier talk Bayes's rule also has a denominator the model evidence but since we are using this in a But since we are using this in a Markov-Chin-Monte Carlo framework, we don't need to calculate the model evidence because it is constant for all models and it is divided out in the Markov-Chin-Monte Carlo process. So we just look at the numerator. We formulate the likelihood function using a Gaussian with our objective function of choice and the exponent. So my So, my background is in the seismic inversion, and this is a very common objective function formulation for seismic. You would see something like the L2 norm in the exponent instead of like fly spots or time or battacharia, but the idea is very similar. We use the uniform prior in our examples, though the prior can take many forms. We're going to sample from the past year using just the plain Markov chain Monte Carlo method. So we start with a random walk sampler. We give an initial point. Maybe it's the map point of our prior. It may be it's some random point within some constraints that we know about possible source locations. It could be a variety of things. After we get that initial point, we solve the forward equation to get the simulated data. To get the simulated data. And then we determine whether we're going to accept or reject that new source location. If we reject the source location, that's the same thing as accepting the previous source location again. So even if we reject 10 models in a row, we're accepting the previous model 10 times. We're still adding information to our posterior distribution. However, if we accept that new source location, we update the step in our markup chain. We update the step in our markup chain, and then when our random log sampler draws the next point, we're going to step from that new point location. So, in the experiments that I'll show, we are considering only two dimensions. So, it is trying to find the latitude and longitude of the source location. For our numerical experiment, we're going to use information from the Deepwater Horizon Oil Spill. From the Deepwater Horizon oil spill that occurred in 2010. It was located off the coast of Louisiana, and it was one of the largest, or I believe the largest, marine oil spill in history. We had oil leaking from April to, I believe, September when it was finally, the well was finally declared dead. And here I have a picture of the Deepwater Horizon oil spill with some cleanup efforts. Those tiny little things are boats, so you can really see the extent of the oil spill. It was huge. It was huge, and we are still working on environmental impact from it today. So we assume a two-dimensional mesh with particles released every hour for eight weeks. We start the chain within the satellite contour, which you'll see on the next page. And then observed data is generated from my Pyopatcher code with particles released at the true location and then transformed into a density map. Transformed into a density map. So we're not using the satellite contour to generate the data for this inverse problem. We're using a purely synthetic setup. So here is the results using the Busterstein metric. So here we have the coast of Louisiana. You can see New Orleans is here. I have the satellite image of the extent of the oil spill in gray. Gray, and I believe this is after eight weeks. And then we have the starting location. This is the first step in the random walk sampler. So maybe where we believed the oil spill started, but weren't sure. Again, it's a uniform prior. So I'm not putting a Gaussian around the starting location or anything. And then we get the true location with the orange X. With the orange x. And then you can see that little red smear there, which I've blown up in this box here, is the posterior distribution of the source location. So even though we started quite far away, we zero in on that source location quite well. Now, this is a completely synthetic example. So we're, and we're committing the inverse crime of using the same solver to generate the data as we used to solve it, but it is showing that the It, but it is showing that the Wasterstein metric is effective for as a likelihood function for comparing distributions of particles. And then I also wanted to compare it with the Batticharia. So notice how kind of small and tight the posterior distribution is for the Bosterstein. The Batticharia does similarly, except for you notice that the black box showing the extent of the posterior distribution is much larger for the Batticharia. Is much larger for the Vaticharia than it is for the Vassar's diametric. So the Vaticharia doesn't quite narrow it in as much. We get a wider posterior distribution, which I found interesting. But the Vaticharia also is more computationally efficient, scales much better, and is cheaper to calculate. So there is a trade-off there between limiting the posterior distribution and having a really computationally efficient method. So, that concludes my talk. I wanted to give a thank you to CAUST, King Abdullah University of Science and Technology in Saudi Arabia, in particular to our collaborators, Omar, Olivier, and Ibrahim. And also, thank you to all the organizers of the Women in Embers Problems session. And I really appreciate you inviting me to talk today.