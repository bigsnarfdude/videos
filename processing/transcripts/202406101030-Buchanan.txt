Oh, yeah. Let's continue to the second part of the tutorial. So, in this part, we're going to talk about kind of a way to take these concepts from the previous lecture, make them more scalable, turn them more towards the settings of interest with modern high-dimensional data sets, see how they work. So just kind of picking up where we left off and summarizing a little bit at a very high level kind of this paradigm that we were sort of advocating around compression on road optimization in the previous lectures. So we have this kind of three-step framework to derive these white box networks. So the first thing you do is you have to design an objective function where you have this property that the optimal points of this objective function are. The optimal points of this objective function correspond to good representation. So, this is how this looks for representation learning, but you know, the same thing is true in more general applications as well. We saw it for Lista. So, there's this design problem of designing the underlying objective function. We've talked about the rate reduction in the previous part of the tutorial. We'll talk more about how to modify that in helpful ways in this part of the tutorial. Once you have that objective function, you get your network architecture. Architecture by incremental unrolled optimization of this objective function. So I'm using the word incremental here to sort of foreshadow a little bit that we're going to be sort of in this part of the tutorial advocating taking some liberties in the way in which each iteration of this optimization algorithm actually corresponds to kind of Corresponds to kind of a perfect gradient descent step on the underlying objective function. I'll say more concretely what I mean by that later on in the talk. But basically, we're going to adopt a perspective which is less sort of dogmatic in guaranteeing that kind of each layer of the network needs to be a gradient ascent step or descent step. And rather, that it's okay to take some approximations for the sake of efficiency in the construction of each of these layers, as long as kind of the underlying procedure. As long as kind of the underlying procedure should be optimizing this underlying objective function phi that we designed. And when we actually test the networks that we have so obtain in practice, we can actually observe on the empirical side that each layer is actually performing optimization of the underlying objective in the way that we desire. So this is going to be a little bit of a shift for the moment for this sort of desire that we have to have scalable networks away from this rigorous interpretation of each layer. Rigorous interpretation of each layer as a gradient descent step, and more towards this interpretation of each layer, kind of doing an incremental step of optimization, maybe an approximate step of optimization on the underlying objective in a certain way. So objective function, architecture from unrolling, incremental optimization. And finally, we just need to have some procedure once we have this network for how to learn its parameters, how to train that network. So there's an optimization procedure here of this objective function. Procedure here of this objective function phi that gives us the network. And then we have this other optimization procedure. So I'm going to call it like backward learning, or we can just think it's backprop, which is separate and depends on a specific choice of task for training. And that determines how you actually learn the parameters of the network that you, the white box network that you've obtained from data. So these are the three steps kind of of this paradigm. We've seen examples of those for Lista. For Lista, very quickly for Radu. We're going to see in more detail in this lecture how they look for a network that can actually scale. So I'll talk about each of these in this part of the tutorial and then show some results for the network that we obtain. And first, we can start by talking about this design of the objective function. So we talked a lot, kind of if we review, we talked a lot about this sort of information gain of information gain objective function delta r in the specific setting where you're given labels for the data you're trying to represent in the previous part of the tutorial. So just reviewing the same notation here, these normalization constants, et cetera, are not important. What's just important to remember here is this is sort of what we were talking about in the previous part of the tutorial. So do you have the sort of representation z, matrices pi, which tell us matrices pi, which tell us how to partition each column of the representation Z into these different disjoint subsets of the overall data set, such that each sort of element of the partition is one class. And then given those matrices pi, we define these sort of coding rate approximations, which allow us to sort of identify and also try to transform once we unroll the low-dimensional space. Transform once we unroll the low-dimensional structures and data distribution towards a representation space. How does that shape? Yeah, that's a typo. But let me think for one second. I think there's no so what I think it looks like. I think it looks silly, but just this doesn't depend on K. So this is equal to capital K. I don't know why it's, but this should be weighted more than this, basically. So it's like, it looks like a typo, but actually the weighting is correction. So once I replace this sum, because this doesn't depend on lowercase k. So this is just the scale factor of capital K. This with capital K. This with capital K over 2. I'm sorry for writing this. I should remember to fix this, but actually, this should be kind of average. This should be kind of an, sorry, an average coding rate. So we wrote it differently in the previous lecture. I didn't write it this way. I had some extra terms that correspond more to the exact form of the upper bound. This type of upper bound is working in kind of a setting where you have a large number of samples m. And so I've taken away the pre-factors. But in order for these to be scaled in a commensurate way, this should kind of be averaged over the class. But sorry about that. That's confusing. But it actually is. This way leads them to be correctly scaled. You just have sort of the overall objectives, you know, be scaled by a factor of capital K. That's true. Yeah, yeah, yeah. So this is also assuming balance classes, I think. Yeah, that's a good point. What are they doing? But they are yeah, yeah, but but it should be scaled by each of these should kind of be normalized by this actually, but if they're all the same yeah yeah sorry about that thank you for I'm glad we caught that so we talked about kind of this maximal coding rate reduction principle in the previous part of the tutorial. Uh, part of the tutorial. So I checked the appendix. Like what I was saying was right, a correct characterization of that theorem. But at a level of a principle, what we said for that result was just that the optimal points of this rate reduction objective delta R as a function of Z in this labeled setting correspond to sort of configurations of the column of Z, where, according to the partition that's imposed by the labels, the The representations are orthogonal across different classes and sort of isotropic or as isotropic as possible within classes. So, this gives us a structured representation Z via, so at the global optimal points of this function, the information gain delta R, at least with suitable normalization. So, this maximal coding rate reduction principle, we talked about how this kind of inspires us to actually obtain neural networks. To actually obtain neural networks by unrolling this objective function delta R. And this is how this Redu net that we alluded to very briefly at the end is actually derived. What we're going to talk about now is how to kind of go beyond a lot of the restrictions of this previous formalism that we talked about. So, in particular, what should we do in order to get rid of this reliance on the label matrix pi? We don't really want to have. Matrix pi. We don't really want to have that in the construction of the network. How can we also, I didn't talk about any results for RaduNet, but RaduNet actually sort of has a lot of trouble to scale to large data sets just due to certain computational inefficiencies associated with its gradient in its exact form. So we want to think about how to sort of improve scalability vis-a-vis that problem as well. I'm going to talk about kind of three simple steps that we can take that are sort of, I'll present them as kind of that are sort of, I'll present them as kind of like modifications of this function, delta R, that'll actually lead us towards an architecture, sorry, towards an objective function that's actually more scalably on rule. Okay, and so the first one, we're going to kind of take a cue from the popular transformer neural network architecture that we alluded to in the introduction and change the way that we actually seek to represent the distribution of the data. So recall, kind of in the design of the Vision Transformer, In the design of the Vision Transformer, that the way it processes data, for example, processes an image, is as a sequence of tokens. So it's not processing the image as a single sample, a single vector, high-dimensional vector x, but actually as a sequence of tokens that come from that single high-dimensional sample x. So what that looks like in the case of a vision transformer is that at this pre-processing layer, we actually We actually sort of, so in the case of visual data, I'll just focus on this example for the sake of simplicity. We actually perform a tokenization operation, which in the typical vision transformer or a transformer for visual data is very simple. All we do is take this input image X and divide it up into non-overlapping patches of a specific size. And then we sort of raster order, flatten those patches, and then apply a lung. Those patches, and then apply a learnable affine transformation to that resulting matrix. And that then gives us for a single image. So now kind of the role of the number of columns of Z is changing. We should take note of that. But now for a single image, the representation that we start with when we process it with the transformer deep network is going to be a matrix Z, where the columns should still be thought of as embeddings of the input data. Of as embeddings of the input data, but now they're kind of embeddings of its patches. So that affine map transforms the raw patchified input into this initial embedding. Does that make sense? Ah. Okay, no. So this is something that's like a typical implementation trick for transformers, which is to, so this will eventually be a completely low. This will eventually be a completely learnable parameter. You can't really see it on the projector, but these are highlighted in blue to denote that these are going to be learnable. This is called the class token. And for some reason, I think I don't have deep insight into it, but it's kind of an interesting question. But in applications, it's often useful to take a transformer, which kind of learns a representation for the distribution of a patch. I'm sorry, for an image, you know, kind of the distribution. For an image, you know, kind of the distribution of its patches within the image. Eventually, if you want to use that to do classification, it's useful to like add an extra dummy token, which when you take the final representation, the way you actually obtain a class prediction is just to kind of map that dummy token towards a set of class probabilities. So, this Z class is exactly that dummy token. It's a learnable parameter at the input, but it gets a At the input, but it gets appended to the entire sequence of tokens. So all these tokens depend on the input. This one is just learnable, you know, like an additional offset or something like that. But when we process this whole sequence of tokens from layer to layer, we're going to process that additional dummy token with them. So it'll actually learn something, or at least absorb something as it goes up the network about the entire distribution. But that's all that token is. So it's not actually a label or any side information, purely learnable parameters. That information, purely learnable parameters. Well, then, once we have this tokenized input, the rest of the network is just like a typical deep network, a stack of layers, each of which performs some specialized processing of this representation, this embedding of the image as its token. And so each layer in the transformer is composed of these two kind of ubiquitous blocks. These two kind of ubiquitous blocks. The first is this self-attention block, which kind of projects this sequence of tokens, computes global pairwise correlations between the different tokens, and then uses those after some normalization to actually recombine the tokens in a certain way and generate the output. So this is kind of a matrix to matrix mapping, where actually the way each Actually, the way each component of the input sequence of tokens kind of enters the output is determined in a certain way by that token's correlations with all the other tokens in the sequence Z. So that self-attention layer is kind of a ubiquitous, famous hallmark of the transformer architecture, which gives it, it seems, a lot of interesting capability. But as we talked about, it's still, you know, in the black box design. Many competing ways to understand what this self-detention. Ways to understand what this self-attention operation is performing. After self-attention and a skip, an appropriate LN here is just denoting some form of normalization. The second component of the transformer block is just to compute kind of a multi-layer perceptron operation on the result of that self-attention block. Again, with appropriate normalization and a skip. And when we do that, that gives you one check format. And when you stack several of these together. And when you stack several of these together, you obtain this mapping that kind of transforms this embedding of the different patches of the input image towards a representation. And then you can use that representation to process in different ways. So the change that we're going to make kind of based on this or inspired by this structure of the transformer and its success across so many different domains and applications to the basic rate reduction is that whereas in the previous framework, we were thinking about computing the rate reduction. computing the rate reduction on something like maybe a set of samples from the underlying image distribution or maybe a batch of samples. So each column of the representation Z was some representation of an image. So instead what we're going to do is take this Q from transformers and seek to learn a representation for the distribution of tokens within an image instead of just sort of the image itself. Just sort of the image itself. So that means that kind of at the level of modeling, we're going to transform the input image into its collection of tokens, and then compute the rate reduction, or here just, okay, not yet the rate reduction, but compute these sorts of upper bounds on the lossy coding rate of the data for this token set rather than computing it for a set of images. So this change is sort of at the level of notation, it's there's. Of notation, there's no change. But at a conceptual level, what this is actually computing is very different kind of from this global coding rate kind of of a set of images. And so this is going to be helpful, maybe in different ways. You can think intuitively that now we're kind of learning about how to represent associations of patches within an image. And there's sort of, you know, we can sort of group patches in different ways with this embedded. Different ways. With this embedding, I'm not going to talk about the role of the embedding at the pre-processing of the transformer very much at all. But that embedding, oh, I guess we'll hear more about that later in the workshop. But that embedding actually has, in an interesting way, sort of capabilities to also preserve information about where each patch in the image came from. A priori, that information is not here whatsoever, but the role of that embedding is actually to play that a little bit. So actually, you end up. Bit. So, actually, you end up through this type of a transformation to the rate reduction, having the ability to sort of compress identify low-dimensional structures in this distribution of kind of appended patches and sort of the position as well. Potentially much more flexible for working with image data. So, the next modification that we're going to do here, so we're representing tokens now instead of images. We're going to talk about for the next modification ways to sort of get around. Ways to sort of get around the need to have this label matrix apply. And so, for that, we're taking a cue from a lot of these other white box architectures that have been proposed in the past: convolutional spark coding networks, scattering networks, et cetera, wherein you include as kind of a component of the underlying objective function, and then of course the unrolled network, a model for the distribution of the data. So an explicit model for the distribution of the data. So we're not just going to assume anymore that we're given labels. That we're given labels that somehow tell us how to divide the different images up into different classes that are supposedly structured differently from one another. We're going to introduce sort of a learnable parametric model into the mix, which is going to end up being a parameter of the network we unroll. So let me talk more concretely about that. And kind of our task here is just to work such a signal model into a delta R objective so that we can unroll it. So explicitly, the way we're going to come at this is kind of Come at this is kind of proposing a parametric model for the distribution, the marginal distribution of each token within one image, sort of as it's being processed. And so what we're going to ask is that we're, or what we're going to seek is to kind of transform this distribution of tokens towards a mixture of Gaussian, a mixture of low-rank Gaussian's representation. So we want it to be the case through processing of the data. Through processing of the data, that this distribution of tokens is eventually transformed towards this statistical model, where, like, so marginally speaking, each tokens distribution is coming from this mixture of Gaussians. The mixture of Gaussians has sort of learnable parameters that are encompassed by these underlying covariance matrices. But so, this is the specific signal model that we're going to seek to transform the data from. And this sort of And this sort of makes sense with respect to these desiderata for representation learning that we've talked about earlier in the lecture, earlier in the tutorial, as sort of a target for representing the high-dimensional data. But now this is a specific parametric signal model. And what we can do is try to actually compute an approximation of the lossy coding rate with respect to the signal model. Good. So we do that. Of course, that's kind of intractable in general. The exact lossy coding rate or even good approximations. Yes. Absolutely. given s and alpha yes yes yes exactly and si is random too so it's it's supposed to be this is maybe a cumbersome way just to write that zi is a mixture you know it's drawn from a mixture of gaussians where each component uh has its covariance like ui or yeah ui ui Or, yeah, UI UI transpose, let's say. So that's the underlying probabilistic model, basically. And this is just some way to write it in terms of like specific random variables. So if this alpha i is Gaussian and this Si is random, and they're both independent of one another, this should give rise to exactly the same probabilistic model. I guess so, yeah, yeah, yeah. Yeah. I think, yeah, it depends on the way in which we're interpreting the role of this model. And yeah, sorry for the confusing presentation. Yeah, that's right. Yeah, yeah, let's say uniform. But I think, yeah, let's say uniform. They should be uniform. That's what I mean by this subscript R. I think we'll just assume it's uniform for simplicity. Just assume it's uniform for simplicity, but I won't say that the prescription we end up with for the objective and the network will really depend on that in like an interesting way. Let's just say that it's uniform for simplicity so that like, you know, when we take gradients and things like that, eventually all the mixture components have the same weight. So normalizations are all the same and things become a bit easier. Yep. Yes, absolutely. And we'll think about it in that special case where P is less than D. And actually, so in all the models we'll look at today, except at the very, very end, we'll also be working in the case where the number of mixture components. The number of mixture components is equal to the. Yeah, yeah, yeah, we'll consider that special case. In that sense, yeah, multiple UKs. Oh. Oh. He is bigger than V, right? Yeah, okay. Well, we'll consider this specific, the low-dimensional case where P is small. So these are degenerate Gaussian. So we'll be considering this model as kind of, you know, the target for our representation, basically. That's the role that this statistical model will play. Now, once we have that, by the way, thank you for the question. Stop me if there are more. Sorry. I just, I don't have a good time control. So we need to figure out, at least in the compression framework, can we actually compute a reasonable approximation? Compute a reasonable approximation, at least to instantiate the same sort of program that we had in the first part of the tutorial. Can we actually compute a reasonable approximation maybe to the cost of coding some observed data with respect to the signal model? And the answer is kind of no in general, because based on the specific arrangement of these subspaces, UK, so I don't have a quantitative claim of hardness or anything, but it can be very challenging to actually compute the exact cost. Actually, compute the exact cost of coding the overall data. Moreover, the statistical model as we wrote it is sort of underdetermined, too. We've only sort of asked that the marginal distributions of the tokens satisfy this certain distribution distributional assumption. But we haven't said anything about their joint distribution. That's apart from the specific configuration of the UKs as well. So, what we'll try to do is just work with sort of think about this case. With sort of think about this case, like René said, where we have sort of low-rank Gaussians in this statistical model. E is small. And also, the number of components in the mixture is not too large either. And maybe even especially, we'll have EK is equal to the ambient dimension D. In that case, it makes sense to think a little bit about, and this is also in some sense analogous to the case with the pi matrix, but we won't talk about that in detail unless anyone is interested. But we'll think interested. But we'll think about this case where all these UK's are actually mutually orthogonal. In that case, because of that mutual orthogonality, we sort of have a partition property, and we can actually exactly compute the same tight upper bound for the coding rate as we did before. Just by projecting, computing sort of volume, packing spheres, and then adding up the individual costs or approximate costs of coding across each of those different projections. Across each of those different projections. This is in general always an upper bound on the true cost of coding just by virtue of, well, you guarantee to sort of represent everything in this way. You just maybe miss out on certain efficiencies based on correlations of the tokens of the subspaces U. So because it's an upper bound, it sort of makes sense in some sense to actually use this as an appropriate stand-in for this R compression, this cost of coding with respect to the different low-dimensional structures. Respect to the different low-dimensional structures in the data. So, if we make this substitution now, as I said, so this is very much similar to the previous R compression term, but now we're sort of projecting the overall kind of set of tokens into these low-dimensional bases, and then computing the same sort of volume measurement of those projections, adding those up. So, with that, we can actually use that as a stand-in for the previous R compression. For the previous R compression term that we defined in the case where we had this partition of the samples induced by the labels, and now write down a notion of information gain for this setting where we have this specific parametric model for the data. I would say yes, but I would say also one has to be a bit careful due to this distinction between kind of samples and token. As a function, as a function, yes, as long as we have these constraints on the sizes that we talked about before, it's a strict generalization. If the constraints on the sizes change, weird stuff can happen. As Josh said, you know, the statistical model itself can be very... Model itself can be very underdetermined, even in worse ways than it was already. And that can lead to weird properties of the geometry of this book, but at least under these assumptions. Yeah, because you can just sort of relocate this guy into the middle. And then UK, UK transpose, when it's orthonormal, is an orthogonal projection matrix. Yeah. Good. So, that is the second simplification or sort of modification that we'll make. And the third is simply we're going to try to sort of remove kind of an undesirable symmetry. So one thing we note is that in this notion of information gain that we've defined, there's a symmetry where if you rotate all the orthonormal bases uk by the same rotation matrix, and you also rotate the representations by the same ratio, The representations by the same matrix, you get the same value of the objective function. Towards like kind of representing the data in a way that's convenient for downstream processing, maybe also maximally interpretable. It makes sense here to kind of break this symmetry. In some sense, maybe in like a slightly blasé way of saying it, you can always do PCA on your feature representation later. But eventually, we're going to be kind of taking this objective function, unrolling it. This objective function, unrolling it, optimization or optimizing it from layer to layer. And so this symmetry can sort of play weird effects on you across the iterations of the network. From that perspective, it makes sense to get rid of it. And the way we're going to do that is just to ask that, at least in an intuitive sense, out of all the possible equivalent configurations equivalent under this symmetry of the underlying statistical model and representations, we pick the one that is sort of sparse in the standard basis. Sparse in the standard basis. And so to enforce that in a soft way and in a conceptual way, we'll just subtract off this additional L0 package. And that gives us a new objective function analogous to the delta R, the information gain objective function for supervised setting we had previously, that we will call sparse rate revision. So we've added, sort of converted to the token setting. We've introduced this learnable model for the data. And we've sort of added a sparsity promoting regularized. added a sparsity promoting regularizer, the objective function, in order to break this symmetry. And what we expect, so it hasn't been proven, but what we should expect intuitively is that there's a similar maximal coding rate reduction principle for this sparse rate reduction objective function that we saw for the original delta R with the labels y, at least under nice conditions. So nice assumptions on the matrices uk and appropriate normalization. So that means So that means that by optimizing this objective function, we expect no theorems, but we expect that the configurations that we get are going to correspond to those configurations that are optimal for delta R, but happen to be also small, sort of for. But it's maybe slightly technical to prove that, and it hasn't been done yet. So this objective function is now going to be suitable for us for deriving a non-ruled network. So I'll talk about that next step now. So I'll talk about that next step. So here, we just want to sort of perform some sort of approximate gradient descent on this sparse rate reduction objective in order to obtain kind of each layer of the neural network that we're eventually obtaining. And so what happens here is that maybe two things are significant. Now, kind of when you unroll this network and sort of learn the parameters from layer to layer independently, now at the conceptual level, it's sort of like Now at the conceptual level, it's sort of like each layer of this network, each FL corresponding to each distinct incremental step of optimization, kind of has its own local signal model. That's maybe a design choice, but it's also sort of standard in unrolled networks. You don't usually force the parameters of each layer to be the same as they would be in an unrolled numerical optimization solver directory. But conceptually, what this ends up corresponding to is now that because these local signal models can change across the These local signal models can change across iterations of the network, you can maybe have interesting effects. This is totally just an intuitive picture, but in terms of the way in which nonlinear structures in the input data distribution can be linearized as you're trying to transform that distribution towards sort of the desired compact and structured representation that we expect. By virtue of the fact that these subspaces now can change from layer to layer, it's not just fixed as a single representation, sorry, as a single. Sorry, as a single sort of collection of subspaces. And the question then for kind of deriving the network architecture, you know, same as we had previously, but the design question here is how should we sort of go about optimizing this objective function to obtain the resulting architecture? And this becomes an interesting problem because there's sort of a large design space here that one explores by using ideas sort of that are classical from numerical optimization, high-dimensional data analysis, different optimists. Data analysis, different optimization methods, alternating schemes, different approximations, momentum type methods. I won't talk about any of these, but the most basic in this talk. But it's interesting now that you have all these different well-understood ideas to play with and how you decide to optimize this objective function and thereby kind of decide what this architecture of the network looks like. So, I'll talk about just here, very simple sort of prescription for how to do this, which is just to do a For how to do this, which is just to do a form of alternating gradient descent on the overall sparse rate reduction objective. So we're sort of partitioning it up into two distinct components, one that intuitively performs compression and another that intuitively performs sparsification, and then taking kind of alternating gradient ascent steps for the form of restarting on each of those individuals. So first let's talk about how to optimize the compression term. The compression term. So, what we'll do there, just sort of flip things around and take gradient descent, a single gradient descent step with approximations for the sake of efficiency on that compression term. The interesting thing that falls out of this calculation, which we alluded to a little bit at the end of the previous tutorial, is that after you introduce appropriate normalization and take sort of a first order Taylor approximation to Approximation to the matrix inverse that arises in the gradient of this compression term. What you end up with is a layer that has the form of a standard self-attention layer in a transformer-like architecture with two key differences. So here, maybe the key difference is that from the conceptual perspective, whereas we had distinct projections in the transformer architecture of the tokens, when we Former architecture of the tokens when we compute the self-attention operation, kind of each of the query, key, and value so-called components of the self-attention block had a different projection. Within this derivation, when we take a gradient descent step on the compression term, we end up with a self-attention layer where each of these projections has the same, is equal, essentially. And they actually correspond to the And they actually correspond to the underlying subspaces in the local signal model for the distribution of the data. So, this kind of gives an interesting interpretation of how this self-attention block in a transformer-like architecture connects kind of going in the opposite direction here to this underlying compression penalty in the objective function that we unroll here. Function that we unroll here. Yes? Yeah, yeah, yeah. Yeah, I don't think it is valid. I agree. But what's interesting is that when you actually empirically evaluate these networks, whether the resulting layer is actually doing a reasonable job of decreasing this underlying compression function. underlying compression function? What you get by this procedure actually performs okay. Maybe it doesn't perform as well as if you performed a valid approximation. I don't know. I see what you're saying. I would say maybe there's no connection on this slide, but I would say that the empirical evidence that I'm sort of alluding to, just the fact that you do okay at decreasing the value of this compression function through this type of a layer that we obtain through this procedure. Through this procedure. Dual K means maybe you don't decrease it maximally. We know the gradient would lead to a sort of a maximum local rate of decrease. But if you still decrease it, it means there's something to be understood here. That's what I'm trying to say. Yeah, precisely. Yes. Yeah, yeah. So I try to be careful. I'm definitely not complaining, claiming that here. So talking about how sort of the perspective that we're taking in this work is more less on the rigorous side in that each layer is exactly an iteration of gradient descent on the underlying objective function, but rather that certain approximations are being made. We're not taking extreme efforts to justify them. And we're evaluating the suitability of those approximations toward the overall. Approximations toward the overall representation learning goal more on an empirical side. When we actually obtain the resulting network, do the layers kind of perform the functions that we designed them to do? That's kind of the overall paradigm that we're taking with this work. So the point is well taken. But I do absolutely, yeah, want to, I think, make it clear that we're not claiming that this derivation is sort of a rigorous. Rigorous connection between gradient descent on the subjective function and the transform number. And so the point is well taken in this. But I think at the overall higher level, understanding the role of the network in this representation learning framework is a very worthy goal here. And we sort of get that out of this derivation as well. So that's kind of for the compression block. For the sparsification block, I'll talk through this kind of quickly because I'm running out of time. But so for this block, we take a very restrictive approach to how we optimize this sparsification term. In particular, we don't perform a full sort of unconstrained gradient descent, but sort of gradient descent in a subspace. Of gradient descent in a subspace of directions, where that subspace kind of depends on an additional auxiliary parameter that we introduce for optimization. So, that parameter is going to be an orthogonal dictionary D, assumed orthogonal. And when we sort of seek not to optimize this term incrementally over sort of all local directions, but just among directions that sort of correspond to this type of an optimization problem. What we see is that optimization. What we see is that optimization actually doesn't change the value of this underlying expansion term R. As a result of that, kind of the local optimization becomes equivalent to just a sparse coding problem with respect to this additional parameter D that we introduced. And for that sparse coding problem, what we propose to do is do a very simple, very local approach to solving it. Just relax it to a lasso type problem. Relax it to a lasso type problem with an underlying matrix variable, and then perform a local step of optimization on that lasso objective by one iteration of gradient descent, proximal gradient descent. And so here too, just for the sake of practical performance, it doesn't turn out to be extremely significant on the practical side, but we're adding an additional non-negativity constraint here, which wasn't present in the problem. We're only optimizing. No problem. Only optimizing over those Z's, which are also not negative. This is more than anything, so we see performance differences slightly in practice due to this. But I think more than anything, it's just due to trainability. Because once you add that non-negativity constraint, the usual soft threshold nonlinearity becomes a reloop. And so this overall step of Prox gradient on this matrix Lasso objective becomes much like one iteration kind of of Lasso, of Prox gradient. Lasso of Prox gradient on LASSO that we talked about in the previous lecture. So, because this is sort of performing an affine transformation with respect to this introduced parameter D of the output of the previous step of compression optimization and then applying RELU. We call this kind of the ISTA block and it plays the role in this transformer-like architecture that we're deriving of the MLP that we see in the usual transformer network architecture. In the usual transformer network architecture. So we have kind of a fine mapping, nonlinearity. But here, the difference from the usual transformer architecture is that the skip connection is actually inside of the nonlinearity. In the transformer, the skip is on the outside. And that leads this type of an architecture to be sort of explicitly promoting sparsity of the feature as we go up and down. So that's sort of a very practical difference from the usual ML. A very practical difference from the usual MLP block in the transformer that actually leads the representations to be more structured and sparse as we iterate the layers in the network. Yes? Yeah. So D is like just an auxiliary auxiliary parameter. Auxiliary parameter that we're introducing to optimize this objective in kind of a constrained space of direction. So I know the objective is not differentiable, but if you ignore that momentarily and just think about maybe taking a gradient descent step on this objective, think that we're sort of not taking a gradient descent step in the entire ambient space, but only with respect to a restricted subspace of directions. And that subspace depends on this. And that subspace depends on this additional parameter D that we've introduced. That's true. Oh, I see. Like you optimize the delta R so then even that's efficient. Then, even that what is the new station? I thought I arrive at some P. Yeah, so right. If you think about it, I mean, from that perspective, D is just a parameter that's being introduced. Well, the D is eventually going to be learned. I mean, once it's a parameter, we just learn it when we train. Once it's a parameter, we just learn it when we train the network. So it's not explicitly related to the previous Z or anything like this. So I think one is observed that um it's not it's it's not at L plus E at L plus contact. B and L for it will be that yeah, so it's not that L and E to find yeah, but but even in the HP there is a linear relationship. How am I deriving this? Well, yeah, I don't have it written on the Well, yeah, I don't have it written on the slide, but what I said out loud, I'm claiming that this is equivalent to what I said out loud, that this corresponds to kind of optimization in a subspace of directions in the ambient space. It's, I mean, because this is sort of like, I mean, yeah, if you relocate this D because it's orthogonal, this is sort of saying that F of the input Anyway, what I'm saying is that you can obtain the same type of problem that we obtained down here through this perspective of simply performing gradient descent in a subspace of direction. We can maybe talk offline about it after, so we, you know, for the sake of time. I think I'm already well over time. I think I'm already well over time. So let me try to wrap this up quickly. So, what we've talked about now are kind of two distinct ways to perform alternating approximate gradient ascent on this underlying objective function. When we interleave them, we end up with this neural network architecture, which is kind of like a transformer. We call it crate, where the first block performs kind of a self-attention type operation. Self-attention type operation where the query key and value matrices are all set to be equal, different from the usual transformer. And they have this role corresponding to the underlying signal model as relating to the kind of the learnable subspaces that model the distribution of the data. And then we have this MLP block in the Cray model after that, which explicitly sparsifies the data by sort of taking a prox gradient descent step on a matrix lasso. Step on a matrix LASA type objective. So when we interleave those, we get this crate architecture and sort of the way we actually train this architecture in practice. The parameters that are learnable now are going to be these orthogonal dictionaries D that we sort of introduced in the sparsification step, as well as these matrices, UK, the orthonormal bases UK, that kind of model the signal. So the forward pass of a crate network is kind of operating. Pass of a CRATE network is kind of optimizing this underlying sparse rate reduction objective. But when we actually train it, say through classification, through this standard class token approach, the back propagation step that learns those parameters is actually sort of a distinct mode of optimization. So it's important to separate between the two of those when thinking about kind of what this architecture is actually doing to the distribution of the data. And just presenting very quickly, I won't keep too long, but some I won't keep too long, but some very basic experimental results. So, what we see actually to sort of Rene's criticisms earlier, these are sort of weak verifications of the underlying concept, but at least non-trivial verifications. So we actually compute in a trained CRAPE model, sort of trained on ImageNet 1K with a certain variant of the embedding dimensions and the sizes of the parameters. So after training on this data set, sort of on a random mini batch of data, how well does the Batch of data, how well does the forward pass of the network actually compress these samples? So when we actually perform this MSSA layer, is it actually kind of reducing this compression term that we associate to that layer's representation? And are we actually sparsifying the representations in the network as we sort of forward propagate? And what we see is that after training, there actually is a reasonable downward trend to optimize this compression term and also to spark. This compression term and also to sparsify, sort of verifying in some sense this approximate derivation that we gave earlier in the lecture. And so one small point to make is that, you know, towards non-triviality of this verification is that, so these plots are showing the same sorts of results at random initialization and also after training. And the Git representations for a data set like ImageNet that actually do reduce the sparse rate reduction non-trivially. reduction non-trivially. So you see here that you do actually have to do some learning. So those aren't just a consequence of random initialization or implicit bias, it would seem. So actually some sort of learning is going on here that would be interesting to understand at a deeper level. So in terms of performance, one of the interesting takeaways that we get here, which is maybe a benefit of the derivation that leads us to this great architecture, is that, so unlike in typical vision transformers, Unlike in typical vision transformers, the penultimate layer representations in these GRAPE models can actually be used to directly segment sort of patches in input images. So just by sort of saliency, looking at large magnitude activations in the attention matrices of the penultimate layer for different images, the different heads of the MSSA block, which corresponds to the multi-head self-attention block in the transformer, actually segment sort of different. Transformer actually segments sort of different parts of the overall image. In an interesting way, when you look at sort of object classes like animals or something like that, where there are semantic relationships between different parts, even though the classes are different, legs, heads, what have you, we sort of see that different heads in these MSSA layers correspond to segmenting different parts in a consistent way across different objects from the different classes. So there is this kind of interesting Interesting segmentation behavior with somewhat of a semantic flavor that is emerging in this model. And again, that doesn't really emerge without additional engineering tricks in vision transforming. So the performance of these models is also reasonable and scales reasonably well. For the sake of time, let me skip the other results and just highlight that in some ongoing work, so some work is done by collaborators here to actually show. Collaborators here to actually show a bit even more promising scaling behavior of these crate models through a suitable modification to the MLP block. So I won't talk in any depth about the details, but I'll just say at a high level, we talked about how this ISTA block in the CRAPE model presented earlier kind of uses this orthogonal dictionary D. So by sort of making some modifications to accommodate both not just one iteration of Prox gradient, but potentially a multiple. Of Prox gradient, but potentially a multiple, as well as an over-complete dictionary, not just an orthogonal one. So it's possible to add on the practical side a little bit of capacity, a little bit more that wasn't present initially to this MLP block. What we then see for that model is pretty promising scaling behavior. So I'm not showing the previous results, but now these models actually, as you increase model size in a controllable way and increase data size as well, you can actually obtain reasonable scaling trends. Reasonable scaling trends for the CRAPE model, even compared to the highly engineered VIP model. So, at the largest scale, actually, the performance keeps increasing in a nice way. So, this plugs well maybe for performance on the practical side for these models. And with that, let me just wrap up. So, for this tutorial, I talked about kind of one approach to try to use ideas from high-dimensional data analysis via Data analysis via unrolled optimization, compression, to design deep neural network architectures where we have some mathematical understanding for what each layer is doing to the data distribution. We talked about some very brief experimental results showing that the performance here can be pretty promising. And I think the parting message that I want to leave us with or want to sort of depart on is that kind of this approach to thinking about deep neural networks through the lens of model. About deep neural networks through the lens of low-dimensional structure, it's not just sort of an exercise in looking back into the past, looking at the old days where there was so much mathematical interaction between theory and practice, but also an exciting and quite promising avenue for future research to actually try to build even better deep network architectures through better schemes to optimize the rate reduction or other derived objectives. Also, better ways to Also, better ways to interpret the behavior of each layer within the overall neural network using ideas from mathematical analysis, high-dimensional data analysis. So, very promising avenue of research towards these goals. So, I hope we will all try to think about the role of low-dimensional structure in data and models in our own research on the math of deep learning. And look forward to everyone else's talks. Yes. Yeah, sorry about that. I can go to the table. Yeah. So this one is like I can show the table too. This is a more recent result. That's why I touched on this a little bit. Touched on this a little bit, but it's maybe a little bit harder to read off. But so standard numbers for like vision transformer performance. And I think this is going to be kind of transfer learning performance slightly. So these models are going to be pre-trained on this bigger ImageNet 21K data set and then fine-tuned on ImageNet 1K. And the resulting performance will be plotted here. So what we end up seeing here, kind of the diamonds are performance of a typical like Or performance of a typical like VIT model. So we're not going to quite the state of the art here. Like the size of the data set is not state of the art. The size of the model is not either. I mean, the size of the pre-training data set. I think state of the art here is probably like 91%. Who's really counting? But this type of a number, 85%, is fairly typical for a big DIT model in this setting. And so we see for the different crepe models here. So this is the original one. These are a little bit maybe under tune. Little bit maybe under tune relative to the table previously, but these should be getting, you know, around the 75% getting up here. This sort of modified model in this current work is actually scaling very favorably. And when I say scaling, I mean both like the kind of the first order behavior of this curve, as well as like the absolute values of these performance metrics. They're doing a reasonable job of following the VIT curve. Of following the VIT curve. So getting into the 80% test accuracy top one for image net work. It's the ImageNet 21K pre-training days. So it has even more, yeah, for this one. So all the table I have too is also for this transfer learning setting. So that's why I just focus on this. So once it's written, couldn't the best way in TV that you take That you take collection of time models and DIT is actually by infrastructure. Yes. Yeah, that's true. I think the ideal evaluation, right, would be something that really does test its ability to do unsupervised learning. But this is a different evaluation, I agree. So it could be unfair to create. But at the same time, the evaluation setting you're proposing, I think, is much more challenging, much more interesting as well, but more challenging. So it would probably require additional, you know, both kind of at the design level, like how to evaluate these models, methodologically. These models, methodologically speaking, in order to actually get a good evaluation. Right. So these refer to different configurations in terms of like number of parameters in certain ways of the underlying model. And the slash 16, 32, 8, this is the patch size. So when we tokenize, do we use tiny patches? Do we use tiny patches or big ones? Normally, B stands for base, L stands for large. And we don't have, normally there's like S for small or T for tiny. And these are just rough sort of, you know, what they actually induce is like what's the size of the embedding layer? How many heads? What's the depth? Yes, just different ways to set the Ah, so this is the this plot is from a recent work. And crate alpha is the crate model with a modified a modified ISTA block. So that ISTA block that we talked about in the tutorial was derived from that restricted optimization of that sparse part of the overall objective. And so this crate alpha is just Is just with that block replaced by a more powerful version, which I think is an additional level of tenuousness between that block and the original objective function is introduced as well, as you could imagine. Thanks, everyone. Thanks for your attention. Sorry for overtime.