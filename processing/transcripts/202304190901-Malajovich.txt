He's going to speak about expected number of limiters, opinions and methods. I would like first to thank the organizers for this fantastic setting, for the meeting, and in particular to Antonio, who will be who may be watching us, who explained to me uh Vitaly's uh theorem, which is uh quite useful Which is quite useful for this talk. So the striking theorem dating back to the 1990s, Duke of Hauslan and Schuklane, says that random systems of real polynomial equations have an expected number of rules that is the square of rules. Number of routes that is the square root of what they should of the number of complex routes that are expected. So, Pezou's theorem says that the number of complex protected routes is the product of the degrees. The theorem by Koflan and Schultz Meil are more subtle. We are counting real rules. So, there is not a generic number of rules. Is not a generic number of rules, there will be an expected number of rules. And the metric structure on the space of polynomials is quite important for the theoretical to be true. So, since I like also normal basis, I will give the following notation. I will denote by the same symbol S D R the space of homogeneous real polynomials. Homogeneous real polynomials and the ensemble of homogeneous real polynomials. We put an inner product structure on that face. And that inner product structure defines what this unit Gaussian. The covariance is the identity with respect to that method. So I will write down the polynomial system like The polynomial system like this. X is X0, X1 is XN, say n dimensions. The D choose A are multinomial coefficients. This sense. Write down some notation somewhere here quickly. B to the A, that is B That is B d C of A N is just B n. So the sum of A i is equal to B, and this is equal to the coefficient of X c A in the sum X. Right too small, X okay. X n. Xn B. So those are the nominal coefficients. So with this metrical structure that makes this monomial in one normal basis, we can treat the F A as the unit identically distributed unit function. Distributed unit versions, and we obtain a random polynomial in the costland plate ensemble. And the theorem is that the expected number of roots in real projective space is the square root of the product of the dr. And then later on, Maurice Rojas got a generalization of that. The generalization of that same theorem for multi-homogeneous systems of polynomials. So you divide your variables in several groups and the system is supposed to be homogeneous in each group of variables. Then he also relied on some special coefficients that look like this but are suited to the multi-homogeneous structure. Structure. And this was in the end of the 90s, was in Park City Conference. And then we all started to wonder if it was possible to get a more general result, like the square root of the Bernstein number, for sparse polynomial systems. Uh and uh got some sort of partial results in uh some papers with Maurice, but nothing uh nothing like that theorem. But some sort of local theorem that was not uh actually a good answer. So uh today I will speak about uh I will speak about uh two CRMs and this is a sort of uh slow moving uh subject. So I think I should maybe write down the CRMs and then explain. So I want to compare the expected number of real roots to the expected number of complex roots. And uh before going further, let's go to the to CRMs. And let's go to the C two CRMs. One is a recent, the other is ten years old. So I take uh kinematic. Um equals zero or it is zero divided by this window is only we can see it. There is a window here. Please, if you are watching this, just click somewhere here. So let W be a measurable subset of R minus zero to the N. So I'm removing Removing points with one coordinate equal to zero, I assume F1 F and spaces of real polynomials and one. E1, En are numbers. And then I say they are the expected number of roots in W of F1 to the V1 to the PN. So as again, again, as before, VFI are spaces with an inner product and don't then they An inner product, and then they become some ensemble with unit Gaussian. And I will say what this is. But the first UN, this is that's a square root of D1 DN E of R F one F N. So those are the two. So those are the spaces I defined here. And those are powers of that space. So I need to define what this power, what is the multiplication between spaces of polynomials. And this is actually a more general more general idea. So the t i's will appear again, degrees, because it means they are numbers. So I am taking a space. So I am taking a space and then take its power. I see I like the number of times you repeat like the space is F, F n. So there is an operation of product between spaces. So those spaces are spaces of polynomials or analytic functions defined in some region, say W. W the spaces are spaces with an inner product and they have a reproducing colour. And that multiplication was defined by Aaron Sign in the 50s, maybe older. And what is it? What is this price? And so I will explain that. And under that, Under that operation, we recover some sort of square root of basic number. So if I have two spaces, say P and F. Uh and those are species of functions functions from F and G, I prefer. F and G, function from some W into the reals would be onto the complex, the idea is the same. I can define a tensor toggle. So this belongs to F, this belongs to G, and the tensor product of two functions goes from W. From W to R takes F X Y onto F of X G of Y. This is the tensor of two functions. We can take the tensor of the two spaces Of the two spaces and the linear pen of B F cross C have in our products here. So we define atom sign. Then Aram sign defines an inner product here adjust to the product of the inner product. Then there is a reproducing kernel. The reproducing kernel can be constructed by using an orthonormal basis. So for instance, in F you define K X Y as a product of As a product of uh let's say sum of s equals x equals y where you choose an orthonormal basis. I'm working mostly with polynomials so I don't have to worry about convergence. I span these issues. What do you mean by potential product in the magic space? So in the if you go to the So if you go to the transformer product, you get just the product of the two kernels. So this is the kernel of F, the other is the kernel of J. You need to separate the variables. The functions are neither. Complex conjugation, yeah? This is complex. Oh, I don't need that. Sorry. I was thinking also about the complex case. In the complex case, there is complex conjugation. In the complex case, there is complex conjugation here. Let's simplify everything to the real case. So, this is much simpler. You just restrict it to polynomials? Simpler, uh, but this is a more general uh construction that will help us to find the coefficients. So, uh this is a tensor product. Uh this is a tensor product. Now we need to go uh back to the uh ordinary product. So so so okay s so okay so but there's no sort of like norm of the tensor part. It's just the usual it's a linear span of those symbols. Yeah, but then f times g is f plus g But but it's not necessarily complete with the specific an inner product. It's not the user product. Okay, that's just the ordinary test part, not the completion of it. No uh the spaces are finite dimensional. Oh, they're finite dimensional. Yeah, if you put the You have all science papers, you have all the sophisticated theory, but we don't need that investment. Okay, okay. So, just with just some linear algebra. So, let's do some linear algebra. Maybe it's necessary to do some functional analysis in the general case, but we just need some linear algebra. There is an operator that takes h of x y y into h of x x let's call it delta delta is a projection it can we can define delta going from the tensor space to a space of functions on so the end the image of this is simply so the image of this is the usual products of functions and finite sums of them the usual tensor product Sense of the usual tensor point to try to get two linear functions, high-dimensional basic functions. Okay, let's put it more clearly. Delta will produce the product, but we still need to get the matrix of the product, the inner product here, on this space. So the trick is very simple. This is a linear operator, it's a projection. So what are on sign did? So what Aaron Sein did is that he defined the product as the orthogonal space to the kernel of delta in here. So we go from F cross G into a new space, let's call F G and this F delta. Uh this is delta projection and here for instance then you get the the the orthogonal the orthogonal there is an isomorphism from the orthogonal to the kernel to the image just writing it information does this have a simple formula if you have like a vector space of polynomials? Yes, it's confusing. Yes. It's by powers of linear force. Yes, so you just. Let's go back to the basic example. I think you should make the definition of just define it. Then we know what you mean. Is it defined? So fg is the space of functions obtained in the image of delta. So what fg formula can? So but you form the tensor function can be or a image. So uh instead of looking at the fun the function of xy y becomes x is a variable and we define the metric structure to the isomorphism because this is a linear map yes the image of a linear map Is the isomorphic to the orthogonal to the kernel in at least in linear algebra. So we're just using the rank theorem. So let's look at the basic example. So we start with two of those spaces. Two of those spaces H E R and H E R. Okay, so those are spaces of polynomials of degree D and E, and they had some orthonormal basis, say basis say alpha bet A X A beta B X B I still don't know alpha and beta which have written here then we want to go this is the product to H plus E of R. E of R and uh we will obtain a basis that I will call it X C X C So what are the sub agents? Those are multi-indices. Those are multi-indices? And they need these numbers, in fact. Those are bases. So, so, those are the automatal basis. The orthonormal basis of monomials, okay? Monomials. Yes, this is the particular for monomials. And what happens now is that if we want this construction, we need to be gamma C squared equals sum of alpha A square delta B squared where C is A plus P. Is A plus P okay? So what is the kernel kernel kernel of delta is the span of Uh A plus um sorry not of yeah of X E Y plus X minus X A dot Y P dot plus B equals A plus plus B dot okay so this is the span of the projection so the Delta goes from colonial C. Monomials in X times monomials in Y to the monomials in just one variable. So this is zero if you identify X and Y. No, because this is this this is something that those are the monomials. So this is What I need now is to find an orthogonal vector which will be something like x a y b some c a plus b and here I need to put some some coefficients. Some coefficients. So if I just put alpha A Letter B I get also normal, but it does not work out. What's the same as the case? This is the orthogonal one. No, no, sorry. I need to make it orthogonal with respect to the inner product of the potential product. So I need to put alpha A to the square times beta B to the square. Because there is no coefficient here, so I need to put the coefficient here twice. And the norm of this vector happens to be sum of alpha e alpha p to the square per root of those are big. Because the basis because the alpha A Beta B X A Y B are normal systems so from this equation you recover unit vector and you recover the coefficient m as it r. I apologize. I'm going to ask a simpler question. I don't understand what the domain and codomain of this delta are. Still, this functions are functions in X and Y. X and Y belong to the same domain. And then the image are functions of just this same domain. Oh, so they're functions on the domain. Their functions on the on the on the domain crosses all, and this is the fallback of the diagram. Yes, okay, yes, and this is symmetric. So, if you trace, if you look at this formula, gamma c is the coefficient of xc into the product. So this works perfectly for Multinomial coefficients. You recover the multinomial coefficients from this equation. Sorry, Guigory, but it's still difficult to follow the logic. I mean, you route to gamma c, and then you say lemma, gamma c square is this. So then you but first you find is this a definition of gamma c or was it? Okay, the logic is the following. You start with a space, whatever it is with the basis. Another one with a basis. Then you want to produce the basis of the Aronsign product. And there is another condition because those are monomials, so you can write a monomial like. Monomial like you want to have autonomous basis with respect to the product. Yes, this one. So you start with two use two normal basis, right? Yeah. So then if you definition, then if you have let's let's tell the hypothesis. I did not tell all the hypothesis, but for the monomial basis, if you have a monomial basis, If you have a monomial basis, whatever monomial basis you have here and here looks an orthonormal, then if you want orthonormal here, you need this equation to hold. I think I need to move a little further. So this gives a definition of a product of Product of reproducing kernel spaces. In fact, it's about analytic functions and reproducing kernel of analytic functions, but I am just looking at polynomials and the same code for complex number. Now, let me go. I will not write the other theorem. I will just move. Veronese embedding. So, Peter mentioned Veronese embedding for sparse systems. And yes and no. What I want to say now is that we have This definite space is like producing kernel spaces. Okay, so in the case of polynomial of polynomial systems, this k so the simplest example is k of xy is Y is sum of xiyi to the degree. This is for homogeneous polynomials with the whale metric. This falls in a much more general segment. So instead of writing down the Veronese embedding as a monomial basis, We can also write the Meronese embedding from our W into our function space, take X into X, that's a polynomial. I will project device here over the reals or over the complex. So Or already complex, so we need to projectivize. And this can only be done if the kernel does not vanish. So this is another hypothesis I did not mention before. And it does not matter in this case. By the way, because we removed all those zeros. Otherwise, sparse system evaluation. Parts system required, may have points for which the kernel vanishes for all polynomials. That's called a base point in some papers. This embedding is also known as Kodi embedding in some communities and this is also a convenient way to do things. So let's uh go to the real uh number of rules. To the real number of rules, I need to make a big picture now. So we have our solution set. So if that's a retirement, then k of dot, x is. Of dot, x is a linear function that evaluates one of your analytic functions at the point x. Because that's to replace. I see, so it's actually, it is that simple. It is really just evaluation. Yes, maybe there is a complication if you start separating the dual and the primal. You have to do it. Yeah, but I'm using interproducts, so I'm just working everything in the product. So, but there is something that is still simpler that will come out shortly. So we have a certain variety that is a set of words which f dot x equal to zero for all variables. Okay, the solution variety. This is also called the solution variety that's for solution. So are we is that the variety? It's innocent variety all the points is rapid. Is where f evaluated x to zero is something that something else is f is this where f evaluated x vanishes or is it something else? Yes, I see. It's the most happy way to write that down I've ever seen. Okay, you win the most popular way to write this down I've ever seen. Okay, let's put this on. So there are two canonical protections. So by one Pi 1 and pi 2. If I want to tell you the number of real roots, I need to integrate over this probability space the number of three images of pi 1. Okay? So we will do that using the co area formula. And for the co area formula, I will need the implicit. I will need the implicit, the local implicit function. But using the implicit function theory, I will go G the local implicit function. Okay, so if you don't really like my notation, let's take f equals zero and differentiate. So this means that d f That E F of x x dot plus f dot of x is equal to zero. Okay, so if I want x dot in terms of f dot the f of x f of x minus 1 but now I want a dot so instead of getting stuck I will write down like this k dot x first equation R of matrix K N dot X dot x star times everything here times f dot so I have the evaluation so next step so this big object is the chain So this equation, what does it mean? So this is the derivative of the implicit function to go from where to where. So I have f here f dot and I want this x to dot and this is And this is okay. So, the tangent space of the solution variety is something else. Yes, it's between tangent spaces. Okay, so it's the tangent space. Yes, because it's defined by this equation, you have a differential. So, we have the analysis. So, now I need to compute the normal Jacobian. So, the normal Jacobian will be computed by dg By DG. Sorry to interrupt, but I mean you wrote the theorem over there, but wouldn't it be better first to define all notions that appear in the theory? Is it now all defined? I got spaces of functions. That is the note expectation. This is the expected number of roots for the random system in this Random system in this, but there is no this. Ah, but there is no. It's different because it's not both phase. It's not both space. It should be both. Sorry. Okay. Okay, that was the same thing. And then this is to be compared to the complex equation where you have exactly the same except the square root. Now I understand. Square root now I understand okay so we are we are working on the third real so the normal Jacobian so here I have a space of a huge dimension and go to a space of a small dimension there is the kernel and the orthogonal to the kernel we need to get rid of the kernel to compute a determinant so dg times So dg times dg star will do the job. It will be a square matrix. And then we get the determinant of this thing and we will take it minus half or plus half whatever it it takes. So what happens here is we we have this operator V G V G star. G V G star is equal to. You want an absolute value around this. But before we continue, I will need to rescale this equation. Okay, here I will divide by diagonal. By the answer of one over k i dot x angle. So this is for each row in this system. So but it's essentially the same thing. So what I will do here is then to Do here is then to get the f of x. Sorry, this I will call this delta okay, so it's the f of x minus one. times I need to divide by itself so that's pot X divided by its norm and then the transpose of the same thing again so I have the inner product of k with k k with k so I will get df of x minus 1 times the diagonal mating kx divided by k x x f of x minus one so this simple This simplifies. I'm using the reproducing kernel property. So the norm is the square root of kxx, and the inner product of k dot x is itself is the node to the square or kxx. So when we do the integral So you might type the DF with this, right? Just DF, not just F dot. The whole thing. The whole thing. But then X dot is nothing to do with the C, because then you can just erase the G from C. Yeah, that's just a scalar. Yeah, so I think what it means, you're manipulating the tension space of. You're manipulating the tension space of the function space. But the norm that depends on the structuralistic calculator. Yeah. So then it works. Alright. Okay, well the calculations will work at the end. And I need to write down the results actually. So I'm going to do what? Yeah, maybe, but you are right. In my equation, what I should write here uh let's put del let's call it delta. So there is a delta over here. So we need to get rid of uh whatever is in between. We Is in between. We don't have to get rid of those details because then we obtain that the normal Jacobian is the determinant of the big matrix one big I dot X. X from D of I Know X and let's jump to the actual calculation. So the expected value. In w of O equal F1 of N So this is the integral over those that ensemble, let's call it F of the number of pi 1 minus 1 F the n of And of F, this is a unit number. Okay, so this is the definition I did not write in the very beginning. So let's take the formula and f of x will be the fiber and now. And now we need we obtain so we obtain the determinant of that thing So there is a minus one over somewhere in between, but I don't care. We obtain some determinant. And we'll actually write this as an expectation. And there is a constant due to the integration we started with the Gaussian in some diagnostic. Started with a Gaussian in some dimension. We got the dimension by n. There is a factor of pi over 2 before the exponential of minus x to the square over 2 in the Gaussian. So this will spill out. I don't care about that. We get the expectation of a determinant. This is what's important. And the rules here, the row is. The rule is one over k of x in move the derivative of i, but I can take the derivative inside here, the k dot x. Uh can I for this is the derivative with respect to x of Derivative with respect to x or with respect to the second argument. So, this expectation is an integral on the function space fiber. So, this is an expectation over the function space. That condition to be in the fiber. Condition to be in the fiber. So, expectation, so this I write the row as depending on x. Okay, the row depends on X. There is a certain expectation and this is where Vitalis theorem, which actually goes back to Vale 1976 or maybe earlier, and it's a And it says that this thing this thing here equals, so we have the x times the mid column of some C one of x C n C of X it's a mixed volume. Maybe quick clarification for those who don't know, I mean the vital evolves without this integral over W. Just the expectation of the absolute value of a determinant. Expectation? Should be the absolute value of the determinant in the expectation. Indeed, it can be written under certain conditions as a mixed volume if something gets to be done. Yes, and then there is an a description of the CIs through their support function. So the support function H of C I H of Ci of X equal to the expected value where R has a certain distribution, which is what we will obtain here of each do more. So, this is the support function of the segment. Apply to U. This is a function on U. So now I had a certain space and I got to. Certain space, and I go to the power of that space. So, when I take a power of a space, I multiply the polytog by a certain power so here. So here it turns out, going through the calculation, that is the square root of the Ti, and this gets out to the mixed volume linearly in each variable. And each inner expectation is multiplied by the square root of the Vi. Over the complex Over the complex, it's much better. But instead of having this expression, we get an extra product of color forms, and they are multiplied by the di, not the square root. So over the complex, we get product of the di's, over the reals, the square root. This is independent of whatever we are looking at. We are looking at. So, if we look over the complex, we can take one set over the reals, another set. It has to be different. So, one of them does not need to be the real part of the other. But this relation will still hold. So, the the Kostland Suxmeil theorem is a particular case. This is more general. Uh this is more general, but at the same time it gives us less information. We do not have a priori information for every space, just for spaces that are powered of the previous space. And if we take and multiply two spaces, that's dual, over the complex we get a clean result. Over the reals uh we don't. The complex one is in my paper of twenty thirteen in the journal of F in the Journal of FOCN, where there are more details and more general cases. The paper on the recent, the most recent result is restricted to polynomials by convenience. And thanks for your attention. So basically there was pipe. We haven't told that corrected much. So maybe Talk directly much, so maybe I have a quick question.