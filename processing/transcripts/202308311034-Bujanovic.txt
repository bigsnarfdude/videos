So, this is a joint work with Luca Drubisic, Daniel Kresner, and Heysen Lam. And so, in this talk, we are going to discuss how to exploit structure in order to achieve much faster computation, but by using randomized vectors. The structure that we are going to exploit is going to be the one in which computing matrix vector products is much faster when the vectors Products, it's much faster when the vectors have a chronicer product structure. Such situations come in place when matrix A is a short sum of chronic products. And such matrices arise in many applications, typically from the PD discretization, but we saw this morning that there may be other sources of such matrices. So in order for a randomized algorithm to exploit this structure, the algorithm would need to use The algorithm would need to use random vectors of the form chronicle product of two vectors, and not, let's say, the default random Gaussian vector. So, this is going to complicate the analysis because the default random vectors, Gaussian vectors, allow for a simpler and elegant analysis compared to other distributions. There are many, many concentration results available, and the main problem. Available, and the main property that comes into use in all proofs is that the Gaussian random vectors are unitarily invariant, which is not going to be the case with our vectors. So, our main question here is what is the effect of replacing unstructured Gaussian random vectors by rank 1 random vectors in which the components, the vectors, are again Gaussian or in some cases radona random vectors. So, we are going to discuss We are going to discuss two different problems. In the first part, I'm going to talk about trace and normal estimation by using rank one random vectors. And in the second part, I'm going to talk about subspace embedding. And with the first time, I'm going to show some applications to identify mutations. Okay, so in the first part, we discuss phrase norm estimation. Our goal is to compute either a norm 2 or the F norm of a matrix A or phrase of symmetric. Trace of semantic positive semi-definite matrix B exclusively by using matrix vector products. And there are many known results that use random vectors. So already using a single random vector is enough to get a good first estimate for a norm of a matrix A. And if you use more vectors, let's say K of them, an average or k products, then Products, then we are going to get probable good estimates for the trace. And all these results rely on the fact that the random vectors that we use are either Gaussian or rather machine-handed vectors. So let's see what happens when we replace them by random rank one vectors. Okay, so before that, let's see one interesting application where it is much faster to use rank one vectors in computing the norm compared to using ordinary. To use in ordinary classical method, and that is the estimation of the Faschet derivative norm of a matrix function F. So the Frechet derivative norm, a classical way to compute the norm, is to use a power method in order to compute the largest eigenvalue of the Ka transpose Ka matrix, which is a linear operator associated with the Operator associated with the Foucher derivative. This requires computing, evaluating the matrix function f on potentially very large matrices several times. This algorithm may be slow. But on the other case, on the other hand, if the matrix that in which the point in which we evaluate in which we evaluate the fresheterio Evaluate the reactive is a rank one matrix, then there is a specialized algorithm, which is a special version of an Arnold method, which never needs to compute the function f on a large matrix, but only on the small projected matrices. So if there is a way to estimate the norm by using only random rank one vectors or random rank one matrices, then this method is going to, the alternative method is going to be much. The alternative method is going to be much faster than the standard approach, at least when we need only a rough approximate of the cachetary. Okay, so this is our setting in which we are going to work. We have a matrix for which we want to compute the norm. It's an rectangular m by n matrix, and n is factored into n tilde times n hat. We are going to generate random vectors, x tilde and x hat. And you're going to approximate the norm by evaluating matrix vector products with this vector. So the difficulty in analysis compared to the ordinary Gaussian case comes for several reasons. One of them is that the entries of this vector, x, are not independent. The other sources, as I mentioned already, the distribution. The distribution of chronically random rank one vectors is not invariant under orthogonal transformations, then also the random variable, which is the square of the norm, is not sub-exponential, unlike in the Gaussian case. In that case, we can use some of the shelf norms, all of the shelf bounds. And there are also no concentration results or tail bounds. Results or tailbones known for this random variable, and its probability density function or cumulative density function is very difficult to compute. So just a broad idea on what is used in that case in the blues. We take eigenvectors of the matrix A transpose A and we reshape them into a matrix Q and then we either analyze what happens with the Analyze what happens with a random variable which is called a Gaussian chaos of order two, for which some results are known, some concentration results are known, or we somehow make use of the single variable decomposition of the matrix Q. And the analysis that we do suggests that the performance of random rank one vectors is affected by the rank of the matrix A, but also by the rank of the dominant right singular vector. And right singular vector. In this sense, when you take the S D order of that vector. Okay, so I'm going to compare now the results that we can obtain for ordinary unstructured Gaussian random vectors and the results we obtain for the rank on Gaussian vectors. So first we just approximate the norm by using a single matrix vector product, a times x, and we have some parameter theta. And we have some parameter theta, some penalty theta, and we want to know when is theta times norm, a times x, an upper bound for the two normal matrix A. And in the Gaussian case, classical results tells us that the probability that this does not hold falls with one over theta. And when we do the same analysis for the rank one case, we also look for one over. But multiplied by this logarithm factor. So it's slightly worse than in the case of unstructured vectors. So this bound is actually tight. This is the probability that the above inequality does not hold on the x-axis is tau, which was actually seen on the previous slide. Sorry about that. And this is our bound. And this is our bound, the failure probability. And on the graph, there are several empirical probabilities for several different matrices. So the bound is necessarily pessimistic because there is a matrix for which the bound is almost achieved. And this matrix is a rank 1 matrix which has a rank 1 right singular vector. We have a rank 1 matrix whose right singular vector is not rank vector. Right singular vector is not rank one, then this failure probability is slightly less. And as we decrease, as the singular value decay of the eigenvalue decay of the matrix A smoothens out, as they do not drop exponentially, but let's say quadratically, then these matrices behave better, better, and better. So the bound The bound or the upper bound is more and more likely for those matrices, but unfortunately our probability bound has to be pessimistic because of these ugly edge cases. Okay, so we can also produce other types of bounds. So we can bound, let's say, the probinius node from the low, and in that From below, and in that case, the probability is exponential in this parameter theta. And our bound again, let's say, follows the slope of the empirical probabilities of several matrices that we okay. So, if we want to increase, actually if you want to reduce the probability of failures, we are going to repeat experiments. So, one way of repeating experiments is So one way of repeating experiments is simply by taking the maximum of the several samples that we take. This is going to increase the risk of overestimating the norm, but on the other hand it's going to decrease the probability of failure. So if you take seven random random vectors and if you take this factor theta equal to ten or twelve, depending on the case, then we get a success probability of over ninety nine. probability of over 99.9% regardless of the matrix of the matrix that we use. Okay, but the more common way of using random vectors in order to estimate the Frobenius norm or equivalent to the trace of the matrix is due to Hutchinson and it is averaging over case samples of this really like portion. Ocean. The nice thing about this is that it is an unbiased estimator for the trace of the matrix, meaning that the expected value of the estimate is equal to trace of the matrix, whether we use rank one vectors or whether we use the ordinary vectors. Okay, and here we compare the ones for that are known in the literature for the unstructured. For the unstructured Gaussian vectors and power bounds that we obtain for the rank 1 case. This is what the bounds look like for the unstructured vectors. So the only thing that actually we need to look is the behavior of this exponential part. So the probability of failure of this upper bound is exponential in the number of samples that we take, in the epsilon value, that was the penalty. Epsilon value, that was the penalty theta before squared, and in something which is the stable rank of the matrix. And when we do this for do this analysis for the rank one vectors, then we also get the same dependence, exponential dependence on the number of samples, the square of the parameter epsilon, but somehow we lose this um stable rank vector in in our In our case. Analysis for the lower bound is a bit more difficult. And as you see, the bound for the classical Gaussian case looks the same, the number of samples, the stable rank, and epsilon squared. And the one that we are able to obtain does have a number of samples, epsilon squared, the the stable rank not of the matrix B. Not of the matrix B, but of the matrix A, where B is A transpose A. But unfortunately, we cannot avoid the M hat factor, which is think about the square root of the matrix dimension in the denominator. This is going to slow down a lot this factoring. It seems that it is the consequence of the analysis. It is not something. The analysis. It is not something that we see in reality. Okay, so the performance of the rank one vectors for a concrete example. So we take a matrix which is of dimension 2500 and the source of the matrix is the final difference discretization of the convection diffusion equation on unit squared. And we want to compute the norm of the Frobenius norm of the A inverse. Frobenius norm of the A inverse squared, which is a trace of A minus transpose. So here we see that computing this and using random random vectors here is much faster than using unstructured vectors because applying A inverse to a random vector x is actually equivalent to solving Sylvester equation with the right-hand side X. And this is much faster than X is a lower. Faster than X is a low-rank matrix. Okay, so what does this graph show? So on the x-axis, there is a number of samples. So let's say we take 20 samples and then we look at the blue line. The blue line corresponds to rank one Gaussian vectors. And at minus 5, it means that when we take twenty samples and compute the average over twenty samples and repeat this ten thousand times, And repeat this 10,000 times, then the trace of the matrix was never underestimated by a factor of more than minus 5. And similarly, the line above says that the trace of the matrix was never overestimated by the factor of, let's say, 5. Ideally, these lines should be as close as possible to minus 1 or plus 1. And we see that the Gaussian. The Gaussian case or the Rademacher case gets close, relatively close to plus or minus one. But the same thing happens for the rank one case, the rank one vectors, but it goes a little bit slower, so their performance is slightly worse. Okay, so this is the first part about the norm and risk estimate. We will move on to the second part, which deals with Part which deals with oblivious substance embeddings. So, in randomized NLA, a very useful tool is our random embeddings. What are random embeddings? We have a long vector x of size n, and we want to find a linear map which is going to map this vector into a vector in R L where L is as small as possible. As small as possible, so that the norm after the transformation is preserved up to a factor epsilon. And a famous Johnson Linear School property says that there are probability distributions D from which we can sample these linear maps M such that this property, this preservation norm, is preserved for all vectors from Rn up to Rn up to a probability with a failed probability delta. Okay, so the question is: what probability distributions are the ones that you need? We can ask for more. We can ask for preservation of norms of all vectors belonging to a certain k-dimensional subspace. So there are, again, Distributions of random matrices such that for any orthonormal V, the norm of all vectors from the subspace V after the transformation with omega are preserved, regardless of the subspace V we choose, with the probability larger than 1 minus. probability larger than 1 minus alpha. And this is particularly nice, this property, because it preserves, it keeps the pseudo-universe of omega times phi small. It's going to be smaller than 1 over 1 minus epsilon. So if we take embedding with epsilon, let's say 1 half, then we have a guarantee that with the probability larger than 1 minus delta, the pseudo-inverse of Time. The pseudo-inverse of omega times 3 is going to be smaller than 2. This can be important. Okay, so what are the distributions that give us this oblivious substance embedding, as they're called? So the classical result is take the random matrix G, which has random independent Gaussian entries. The matrix has L rows and N. L rows and n columns, and as soon as L is larger than constant times the dimension of the subspace that we are interested in, times epsilon minus 2, then this map is going to be an oblivious epsilon subspace embedded. And this has many, many applications. Okay, so this is the case where we use random Gaussian entries. Random Gaussian entries. So, what happens in the case when we use rank one random vectors? So, this problem has already been studied in the literature, so structured random emphatics, in particular tensor random projections, where they consider rank 1 tensors of order D and show that they have this general property that I talked about. And by using techniques from one of these papers for the general case, specifically for our case of two tensors, for our rank one vectors, and also applying the approximate matrix multiplication techniques from Coin-Melson G-Query, we are able to show that a random matrix whose rows are rank one random vectors is going to be oblivious. Is going to be a oblivious subspace embedding. As soon as we take L, rows, where L is proportional to k to the power 3 hubs down times epsilon to the 2 to minus 2. And this improves the previous first known results where they have this factor of k squared or even k to the power of 4. Okay, so. Okay, so what is the application? What is the possible application of this? So the application that we had in mind is contour integration for eigenvalue problems, where we want to compute eigenvalues, k eigenvalues, which lie inside the contour gamma and we do so by computing the integral of the residual times a random matrix which has L columns. Matrix which has L columns. So the question is how big L should be. If we compute this integral, it should be larger than K. If we compute this integral exactly, then the exact eigenvectors belonging to these k eigenvalues are going to be in the span of this method. But we cannot compute this integral exactly, so instead we compute it by using the quadrature formula, which in turn reduces to using a rational formula. Using a rational filter function and applying it on our matrix omega. And the role of this rational filter function, when we are evaluated on the eigenvalues inside the contour, it's going to be approximately one. And when we evaluate it outside on the contour, it's going to drop very rapidly and be approximately zero, essentially. So what we want to know is how the How the span of C, how close are the eigenvectors that contribute to the span of C? And this is the result that we can obtain. So we can bound the angles, the tangent of the angles of the wanted eigenvectors with the span of C by this quotient, which depends on the rational filter. So this eigenvalue, the cable's first eigenvalue, is outside. First eigenvalue is outside of the contour, so this is going to be very small. This one is inside the contour, so this is going to be approximately one. Then we have this factor that basically can be bounded by the norm of the random matrix omega, which is something about which can be bound by the size, by the dimension of the matrix omega. And then finally, we have this factor, which we already Which we already mentioned, which is something that determines the number of samples that we need to use here. So if we use the random rank one vectors, then we have a guarantee, since it is a subspecies embedding, oblivious subspecies embedding embedding. We have a guarantee that this is going to be smaller than, let's say, two as soon as we. Smaller than, let's say, 2, as soon as we take L proportional to K to the 3 halves. Question. So the normal V transpose orthogonal omega depends on V orthogonal. Yeah, so it doesn't have B involved. Okay, so here the vectors are B voholmo. Right, but the norm is not B norm. Yes, yes, but this. Yes, yes, but this matrix is orthogonal in the ordinary sense. So the eigenvectors might be orthogonal, and if you add a factor p one half, then this matrix is going to be two more orthogonal. And the same, but the measure you're using for the norm on the right is not the b norm. Yeah, it's not. Yeah, so I mean, you can basically plot this one out or so the b orthogonal is b norm. So the V orthogonal is V orthogonal, or the small Vs are B orthogonal? The small Vs are V orthogonal. But the big V's are too orthogonal. Are orthogonal? Are orthogonal? Yes. So you can control V transpose omega pseudo-inverse, the normal time you can control it, but the other norm you cannot control, which is large. Yes. V perfect transpose omega, which is large, this one can draw with ten or something. So it can draw. So it can grow a bit in matrix dimensions. And we've seen terms like that in Matania analysis we've done in the past. And part of the reason is that at some point you use some of the activity to split out terms. And mainly you might want to investigate VPEC transpose or mega omega transpose V. So you may use... Yeah, I think you might want to investigate that. Yeah, so th that is possible when when you use the ordinary gauss uh random vectors. Because somehow they they this this somehow they they this this disappears magically, right? But in our case, as in the case of some other, let's say, SRFT distributions, this simply cannot be done. So we have to move over here and we have to hope, or we have to, yeah, we have to hope that the rational filter is going to swallow up this step. So there's a lower one, you are saying, somehow, that these stem cannot be. Yes, yes, yes, yes. Yes, yes, yes, yes. So there there is probability bound which says that this cannot be larger than the norm of the microphone. So does EJ uh play a role? Sorry. Okay, so this yeah we cannot take that into account. So we simply have for now we have to switch on to analyzing the the rightmost expression and th this is too complicated to analyze with uh rank one right here. So yeah. Okay. So I don't know if I have any more time, but maybe just a quick line. So. Or minutes? Couple minutes. Yeah, we started later, actually. Okay, so this is the motivating example. So, I mean, why would we actually want to use in the first place the the rank one vectors here? Why not use the oil two vectors? What is the benefit of using rank one vectors? So this is a motivating example. So this is a motivating example. We take a Schrodinger equation with some potential B. We discretize this on a unit, well not unit V, on a square by using finite differences with n points on each axis. And this leads to the matrix, which is a short sum of chronic products. And this is a huge matrix. It has n squared times n squared dimension. So we take, let's say, smallest or a few smallest eigenvalues of this matrix and we reshape their eigenvectors into Their eigenvectors into matrices. And then we plot the singular value decay with matrices. And what we observe is that the singular value decay is extremely fast. So it's basically exponential for many of the, or for I wouldn't say all potentials, but for many of the interesting potentials. Okay, so the target that we want to compute, which are the eigenvectors, let's say belonging to the smallest, few smallest. Belonging to the smallest, few smallest eigenvalues are objects which are low rank. So we want to use a technique that somehow keeps computing low rank matrices and in the end the final approximation that we get is this low rank object that we need to compute. So how do we compute the approximations for the eigenvectors? We compute them by doing quadrature. And the quadrature for each And in the quadrature, for each of the quadrature nodes, we need to compute something like this. So we need to compute, we need to solve a linear system involving matrix Z times identity minus A. And if you look at a single column on this matrix omega, and if we look at the vector which is the solution of this linear system, and if we look at the vector, And if we look at the singular values of the reshaped vector, then the single value decay of the reshaped vector is exponential, is very fast, if the right-hand side here was of rank one. And if it was unstructured, then nothing happens. Obviously, there is structure in the equation that we are trying to solve, and when we matricize this equation, Matrix size this equation, we clearly end up with something which is a uh multi-term Sylvester equation with the right-hand side which is of rank one. And there are many papers about these equations in particular and especially when there are only two terms, and there's a supporting theory saying that the solution of such equations will be numerical over Numerical overline. Maybe not for this very general case, but for some simple cases. So now we want to solve this equation, and it is not a very simple problem, unfortunately. So the method that we try to use is preconditioned by such step. So preconditioned by such step can be reformulated so that all the vectors that we use in that method are stored as factors. Stored as factors of low-rank matrices, where the matrix is actually a vectorization of, is a matrix of the vector that we are analyzing. And all the computations in this method can be done efficiently by using these factors of these factors. We need to have a preconditioner as well, and the preconditioner that we use Conditioner that we use are a few iterations of the SQS ADI, meaning that we just ignore these other terms and then use this recommendation. Okay, so this is the main idea. It works for some cases and it does not work very well, for some other cases. There are challenges, so the convergent depends on the nodes, so there we go around the contour and the convergence. Tour and the convergence depends on the node. It also depends a lot on the potential. Different potentials have different convergence rates. And also there is a slight issue with the ADI precondition. So the paper suggests always use, let's say, fixed number of iterations, five iterations is going to work. That's that's not real the case. But the uh potential opportunities of these methods are pretty big because we can use them for non symmetric We can use them for non-symmetric problems, and we can use this as part of other control integral integration methods. So, just one sentence to conclude. There are also other methods, eigenvelling methods, that can exploit the fact that eigenvectors are of low rank, and they can exploit this if we start them with a random rank one vector, and not with the unstructured vector. For example, the L V. For example, the LBPCG method by Gnas. I'm not definitely going into these details. So, even in the block version of this method, we can get convergence of all four smallest eigenvalues. And during the process, the largest of the ranks that we observe is, let's say, 100. And our problem is something 3,000. Problem is something 3000. The discretization in each of the x and y axis is 3000. So it's kept relatively low rank. Okay. So these are my conclusions. So we see that we can use rank 1 vectors for purpose, let's say, of normal trace estimation, but also for the purpose of oblivious subtle embeddings and then eigenvector computations. We see that the performance is slightly. That the performance is slightly worse than when we use unstructured vectors, but the gain in speed can be substantial. And theoretical analysis is definitely much more challenging than when we use unstructured numbers. Okay, so that's it. Thank you. I still need one with the parts. Yeah, so when you move from a random vector, you reinterpret it as a matrix, and instead you use two random components of a random one matrix, you lose a factor. I mean, and you showed the experiments, hey, it's really there, you lose something. How does it depend on the dimensionality? So if I use a D dimensions, Cp rank one tensor, instead of a true extremely Instead of a true extremely normal and one vector, do I lose something in terms of dimensionality also? Or okay, so in case of let's say trace in normal estimation, it seems that if you use order D, rank one vectors, that their performance and the bounds get worse exponentially in D. So there seems to be not a very good idea to compute a norm and phrase by using A norm and trace by using like very high-order and quantum tensors. In the case of subspace symbetics, I mean these papers that I mentioned, they immediately discuss the case of all the DNA contensors. And there also you can see that, okay, maybe not for the embedding, but for the general property, the order of the tensor also appears in the Of the tensor also appears in the exponent, in the bound for L. So you're going to need a number of samples that is exponential in the order of tensor.  