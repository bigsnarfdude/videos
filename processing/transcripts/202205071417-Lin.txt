Assistant research group and the this project called adaptive Christmas start in 2020, so that two years ago. So we I have other students when they do their final year project, they also contribute. Even now, my PhD student also is still near Hongxin Yan is still working on that. Wong Xin Yan is still working on that, but PP's two students, Michael and Philip, they already graduate. So in this talk, we will first talk about why we develop adaptive Chris Master. And also, as you know, we already have many Chris Master can be used for a Can be used for assessment. Why, or what's the new? Anything innovative? Yeah, so we are told to talk about our novel question selection mechanism to using artificial intelligence. Another one is to talk about our new infrastructure for the system. So after my slides, Supern will. Yeah, supernatural will demonstrate our current progress. This project starts from many years ago. I asked myself, what is the best learning environment? Or in Azabasca University, what's the best online learning environment? So we should, the best learning environment is not about right. Bright classroom or birds chirping among the flowers. We argue that the best learning environment is we can allow learners work on their very interesting project or subjects with a balance between the challenge and also their skills. There were good balance. That they were a good balance. Also, they can get the instant feedback from the instructor, from the teacher, or from the system, from the agent. So just like computer games, playing computer games, or we call one-to-one tutoring in very ancient time. But one-to-one tutorial. But one-to-one tutoring is very costly. It's not practical, right? So in 1984, educational psychologist called Benjamin, Benjamin, Benjamin Balloon. Yeah, he found. Yeah, he found that one-to-one tutoring using mastery learning led to a two-sigma improvement in student performance. And one-to-one instruction, when compared to the standard model of instruction, substantially not only increases the mean of the performance. Of the performance also decreases the standard deviation. Yeah, this is what one-to-one tutoring system. But you know, this is not a practical one-to-one and not scalable. So he, in his paper in 1984, he asked, how can we achieve this result in conditions more practical or more? More practical or more scalable than one-to-one tutoring. So, this is called the famous two-sigma problem. So, one way to realize one-to-one tutoring, we can use according as IT technology. IT technology and especially World Wide Web, and also artificial intelligence technology. There are some famous researchers like Peter and Julita from, yeah, one Peter is from University of Pittsburgh and Juliet from Canada, University of Saskatchewan. So they are trigger from Yeah, children from the hyper medium are using hyper medium for learning. So they start the research about called adaptive learning. They propose this name is called adaptive learning. What is adaptive learning? Adaptive learning is to deliver dynamic delivery of learning experience. Experience address the unique needs. This needs can be a different speed, different pace of learning of different course material of individual. Because then the pace or content and the performance can change. So we need just in-time feedback. We need to know. Feedback: We need to know student performance. Also, we can provide recommendations for accommodation. Also, and then in this way, we can optimize the learning path for each learner. So, this is called after that, more and more research about adaptive learning. adaptive learning so as you know the uh adaptive learning the foundation is called assessment yeah we need to know our students uh so the ability to make this assessment so we can do granularly and also dynamic any any time we should so that we can know no student of performance so this is a disadvantage So this is a foundation. Also, and then what is of, yeah, this is called formative assessment. So as we know that there's usually we have two types of assessment. One is called formative. Another one is called summative. Yeah, so formative is like people, according to Paul Black, say formative is like when the chef tastes the soup and summative assessments when the custom tastes the soup. They are very different, right? So formatively try to help students rather than only evaluate the student. Evaluate the student. So, thought it should be an integral part of instruction in using the real time for guiding a learning process. And students also should be actively involved. They can test their self and they can actively check themselves. Theirself and also the feedback should be constructive so that students can close their gap once they identify those learning gap. But the assessment is very time consuming. It's also boring, right? It's always the test. Also, waste of student learning time. Time. So, how can make assessment more efficient become very important? Otherwise, students always being assessed very few stressful or boring. Then we have to see how we can realize formative assessment. Yeah, traditionally we use our exam, our assessment always use classic test for CTT. CTT is we assume after the test, the student can get a score. And this score usually consists of a true score that we want to try to know, but never accurate. And also error, that's always. And also arrow that's always any test cannot precisely to get student level on so C D D is based on test is test oriented. So the unit of analysis is the whole test. The student get a score or the overall score. But without considering the items or students' latent trade. So So, this kind of test cannot help us make a prediction how their performance or individual, which their weakness and half they live. After that, since several decades ago, people, some researchers talked. Some researchers try to propose some method to do called adaptive. So, adaptive is the purpose is to select sequences that's different interaction patterns and try to optimize the number of, so reduce actually is try to reduce the number of questions. Reduce the number of questions so that we can assess students accurately and also using as little time as possible. So this is called adaptive for formative assessment. And the most famous technology or approach for adaptive assessment is called item. called item response theory and based on this theory people call computerized computer testing called CAT yeah proposed by Lord in 1980 so this approach is of now it's very popular it can be used to in many applications yeah it's a very It's a very useful tool for like psychology or education or medicine. But there are some limitations for item response theory. Most significant weakness is a lack of ability to make a granular cotton. A granular cognitive assessment of a knowledge and mastery because the different questions can measure only one common trait. Another one is IRT is based on a question bank with parameters. Those parameters should be maintained, should be calibrated value. Um, very often, and that's very, very difficult to maintain. So, because this limitation, we feel to do adaptive assessment, IRT is not enough, not good. Okay, then Chris Master at the first is another for adaptive. Of not for adaptive assessment. In 2009, I and with some of my students try to build a game-based learning. So we simulate the TV show called Chris Master and build a system so that students can compete like a TV show game-based. Game-based. And then we realize, and we are using 3D virtual environment deployed in Wonderland. So that's the early version, but not adaptive. Now, yeah, as we said, we need adaptive to help instructor and Help instructor and student estimate their knowledge, skills, and detect their learning weakness and gap. Also motivate and promote their learning. The methodology we are using for adaptive engine design is first we use called domain modeling. Domain modeling use a theory called knowledge space theory and also student modeling. So we build students, we try to model students, their progress. Also use a Brayson network and optimal question sequencing. If we use some mechanism. Mechanism or algorithm like a battle of a newly banded model and top two Thompson assembly model. I'm going to talk about later. So first is model a domain. I use my course, data structure, that's a computer science course, fundamental course. Course as a testbed. So, first, I model the whole domain using a graph. The two models, they are pregnancy relationship. So, this is the first step. And on the next layer, we need to model. We need to model for different knowledge components. We need to model according to our pedagogy. We need to model the learning objectives. Here is one knowledge component called graph, graph algorithm. Graph algorithm. So you can see there's a LO is a learning objective and 11 learning objectives. So these learning objectives also can be modeled as a graph. So for example, here on the right-hand side, you can see the sequence according to our They're sequence according to our pedagogy, their different learning objectives. And then based on the knowledge space theory, we can automatically construct or call this called learning paths. So student can, a learner can use, can use different learning paths, can be generated. This is showing how to. This is showing how to from learning objective one to go go to different branch and then and then master all the learning objective and also the then we have a lot of questions so this these questions and also uh what is the relationship between our questions also those learning questions also those learning objectives so we can we use a matrix called Q matrix proposed in 1993 by Thompson Okar so the Q matrix is actually very very simple here I use the example here Example here. For example, in for the case question, math one plus three, they need objective addition, but not a subtraction. And the second question, they need subtraction. Yeah, so this is very easy to model our questions and their corresponding. And their correspondent learning objectives. So the our mechanism, the learner, when learner facing an adaptive question master. So quiz master, at any time of learning, if a student wants to know what's my weakness right now, after I learn several learning objectives, I need to know. Objectives. I need to know if I can go to the next knowledge component or next unit or not, then can come to ask Chris Master. And Chris Master can identify first what to be assessed and then identify the weakness. What is the weakness learning objective? So we use an algorithm called best arm identification. That's the Thompson sampling we are using. And then after identify the weakness, the learning objective, then we recommend a course material to remedy the weakness and then students can learn. Can learn and can come back to the test again until all the learning objectives reach a certain threshold level. So such an idea is called mastery learning. So master learning is every student should reach a certain level. Level and to go to the next step about the question selection algorithm. Some people, some researchers, they already proposed some approach. For example, here is a called upper confidence bound based. Based several years ago. This approach is the first one to try to tackle this problem. But we found that there's some drawbacks. First is they cannot use prior knowledge. So it's, yeah, this is a big weakness. Another one is. Another one is a deterministic. It's that the sequence will be fixed every time. This is also not good. And there's some parameters to be turned. Also, the efficiency is still not ideal. So we try to use Thompson sampling. And this Thompson sampling is one of the Thompson assembly is one of the multi-arm bandit algorithms, which belongs. This topic is belong to machine learning. It's one of the machine learning called reinforcement learning. And after that, we found that the Thompson sampling is not still the efficiency. It's not that good. So we found that there's another model called TOP2. That there's another model called Top 2 Thompson sampling. It's improved, it's better. So, this kind of Thompson sampling, Thompson sampling is proposed by Thompson in 1933. Thompson is a Canadian scientist. And this kind of approach, so not only defining the means of the Of the easy learning objective proficiency, but also build up a probability model so that we can accurately estimate those proficiency. Also, based on a level of confidence, so this confidence increased as more samples are collected. So, we feel the advantage of a Thompson sampling-based approach is we can use. Based approaches, we can use prior knowledge. So, for example, student, if they already do a test, to do an assessment previously, and now they want to do the similar test, then our system will know which one he did good, which is not good. So, those previous knowledge we can, the agent can use. And so that's we increase the efficiency. Efficiency. Also, very easy to set up. No hyperparameters to be turned, and also very easy to be maintained. No need to calibrate item parameters. Also, very important, it is significantly in shorten the number of questions. Yeah, we already implemented the idea. Um, the idea uh, so the and show them we will show you later, and uh, here, yeah, for example, here we if we want to identify um five five in blue and learning objective in graph algorithm. Then we can after student doing some questions, we are then we can easily. Then we can easily see which part student is still weak, and then we can recommend a question of a course material. So, this is already done. But further, this is to be done. After the weakness, we can further because this, for example, here, learning objective seven is the weakest. But the learning objective is they are prerequisite. They are prerequisite. There's a four prerequisite here, learning object two, three, four. Also, priority Q and relaxation. They have prerequisite knowledge. We know from domain model. So we can further to test what's because student did. Because badly in performance. So we need to further know if which properties is because the knowledge or the lack of a knowledge of a prerequisite. So we can further to test their prerequisite to see which prerequisite maybe student is still weak. So that we can give feedback. Okay, so that's about our methodology. And then at the architecture part, we are using called distribute model. So Chris Master is here in the middle. We have a, it's about how to assess. It's about how to assess. So, we have a student model and adaptation engine, and we also log student performance. We use JavaScript and MongoDB, and it is, right now it's a secure, secure system. And based on this, we can student can Yeah, students can use either directly with login to Adaptive Master or can log in from a learning management system using we call LTI. LTI is a standard, it's a protocol for learning, learning, what's the name learning. Learning technology in interoperability. This protocol. So it's a standard already proposed for many years by international organization. International organization. So, with this LTI, we can this different system can communicate securely. And then also we are building called open question repository. So, so that our question bank, before we co-op. Bank, before we call question bank, it will be in a separate and can be open and so that educators can maintain can maintain the maintain the repository. So that means not only one repository can be made, any learning system, any repository, as long as it's repository as long as it's LTI enabled can be can be integrated with our quest master so become a useful tool so this is the our proposed architecture for for for Christmas okay um I already yeah we already argued that the need and the benefit of to to have a To have an adaptive, formative assessment and design and develop a novel adaptive engine using machine learning. Also, we proposed an implementing integrated system architecture. Now, I'm inviting Supan to demonstrate what we already, right now we have for the adaptive. For the adaptive Christmas, the new version. Hi, everyone. I will go ahead and share your screen, please. Right, yeah. Oscar, could you turn off your screen sharing? Okay. It won't allow me to share it. Doesn't mean it. Let me see. Sorry, I'm not a very fancy course presentation. Where's the zoom? Maybe I think if you move your mouse to the very top of the screen, like to the edge of the top edge of the screen. Okay, maybe. So now you are still showing my screen, huh? Yeah, maybe close your okay, you did. So now we can have Supon, right? Yes, yep. I will share the screen. My screen is being shared. Can everyone see it? Yes. Can everyone see it? Yes. All right. Welcome to this demonstration on QuizMaster. There are three main aspects I want to demonstrate. The first is the adaptive portion of QuizMaster. The second is the static portion. And finally, the administrative portion of QuizMaster. Currently, QuizMaster is hosted on AWS. And within the next two or three weeks, Next two or three weeks, we should be able to allow public testing of QuizMaster. I will go ahead and log into a test account I created for this demonstration. So this is the adaptive portion of QuizMaster. We are using top two Thompson sampling to identify a student's weakest learning objective or weakest learning objective. All weakest learning objectives, ideally with the least amount of questions needed. Currently, the confidence interval is set at 90%. The basic idea behind TOP2 Thompson sampling is if you get a learning objective incorrect, there is a higher probability that you will be asked questions pertaining to that learning objective rather than. Objective rather than learning objectives you answered correctly. Currently, we also have a peer exploration phase of two questions. And that simply means for each learning objective, a student will be asked at least two questions. So I will go ahead and start a quiz with three learning objectives: a depth first search, breadth first search, and dikstras. First search and the extras. My plan is to try and answer most of the depth-first search and breadth-first search correctly while skipping all the Dijkstra's algorithm. So our first question is a depth-first search question, which I will answer. Here we have a dijkstrus, and I will simply skip the question. And skipping it marks the question as incorrect. This is our second depth-first search. We have another dikes for us. We have another dijkstras, so I will skip over all the dijstras questions. Here we have a breakfast search, so I will try and answer that one correctly as well. And here is another depth of a search. These are all the extras, so I will skip them. Since I've already answered a couple of depth-first search questions, I will skip this one. And here's our breadth-first search. So if I skip this one, the quiz will finish. Quiz will finish, and over here we can see some usual information like the score, also the total number of questions, and how many questions you answered correct. Below here, we have the three learning objectives we chose, depth-first search, breadth-first search, and dijras. For each learning objective, we have some feedback given. The feedback will depend on how many questions you answer. Depend on how many questions you answered correctly versus how many questions you got incorrect. For both depthfest search and breadth first search, I answered some questions correctly while I skipped some. And since skipped questions are marked incorrect, it says you could do better. But for Dijkstras, I skipped everything. So the feedback is, I don't think you studied. And here we also see a link to a resources. In this case, it's a In this case, it's a YouTube video. However, only resources are shown for your weakest learning objective. In this case, it was Dijkstra's, since I skipped all the questions. So if I open this, it takes us to a video on Dijkstra's shortest path by William Fisset. Other learning objectives will also have such resources and Have such resources. And if that is your weakest learning objective, they will be displayed for the student. One other point to mention is that the number of questions and the order in which the questions are displayed will change for every quiz you do. And this is in contrast to a traditional practice exam you would write, which is the next part. Which is the next part I would like to demonstrate. The static portion of QuizMaster. The static portion of QuizMaster is like a traditional practice exam, meaning the number of questions will always be the same, and the order in which the questions are asked are also the same. So, here, each quiz will always have five questions, and the order will be a depth. And the order will be a depth-first search, breadth-first search, crucials, prims, and dijkstras. And now the quiz will end. And if I were to start this again, it'll be the exact same order and number of questions. So we have our depth-first search, breadth-first search, cruscals, crims, and dijkstras. So this is one way in which the adaptive portion. The adaptive portion of QuizMaster differs from the static. And the final portion is the administrative portion. This is mainly for educators to help manage the question bank. So I can choose the first unit here, and we can see all the questions in the question bank. Currently, the formatting Currently, the formatting isn't the prettiest. However, all important information can be seen here. We can add a question, change a question, delete it, and preview it. So for example, if I would like to see how this question would look for a student, I can simply click the preview button and it will show us this question. I can also add a question by filling this form rather than. form rather than having to use the command line or interact with the database directly our question will appear at the very bottom our test question i can also change the question by changing to test question two it now says test question two and finally you can delete the question as well and the question will no longer appear in in the question Appear in the question bank. That wraps up the demonstration. We could open it up for questions, unless Dr. Lynn would like to add something. Yeah. Thank you. Thank you very much. So we can ask some questions. I probably will start like I mean, a simple question. Are all questions with four options? Or can you set up the questions to f for example f five options? For example, five options. IBCDE is that a faith? I mean, it's kind of probably possible, but just asking. Yes, yeah. Right now, the multiple choice questions will always have four, but that can be changed in the future. Okay. And you said you used AWS for your and what is the learning management system you mentioned that you are using? Sorry, could you repeat that? You said LMS learning management system? Right. The version demonstrated here isn't using a learning management system. It's directly logging in to the QuizMaster web application. The application. You developed an application because you mentioned in the yeah mentioned in the presentation that there is one invention. There is learning management. Yeah, what would be your question? Yeah, yes. Right now, the Moodle learning management system also Bright Space, also Convest, those are very popular learning management systems. Also, all LTI enabled. So we already test, we can connect the Christmas with them. The QuizMaster wisdom. So that means students can, if you log into Moodle, and also link to the QuizMaster. Yes. All right. Thank you. That was a good question. Other questions? I have a question. So when you write a new, add a new question in the database of questions, you have to say for each question what learning objectives it serves, how difficult it is, this kind of stuff. This kind of stuff? Yes, we but the only learning objective should be specified. The difficulty level can be specified or we can let the system learn in the future. It can be based on the answers. Okay, because for example, if we need some system like that, we know for That we know for some questions how students answered, but if we, of course, create new questions, we may not know. But we have a lot of data and we know how students exactly answered for the specific choices from A to D between. And we can probably use this data somehow. Of course, later if others submit that are not participants in Market Group, but other groups, this may change. But other groups, this may change. And this will be interesting to see how it will change. So, are you essentially categorizing the problems into different categories? So, for in terms of the kangaroo, you would have logic and grade one to two. And if the student can do well in logic, they give extra questions. Is that kind of the main idea how this would be implemented? Basically, logic, there might be different objectives. We aim to achieve to a question, right? To a question, right? There could be different aspects of logic of algebra of geometry. There are in geometry, you have so many that you may set up. Essentially, you're categorizing them for essentially, yeah. Maybe the problem would cover certain objectives, so you can start with the objectives and then go and check what the problems cover, which objectives they would aim to achieve. I have a question not related to one. I have a question not related to one cargo. So, if I want to use something like this for my cargo tasks, can this be integrated in Moodle? Sorry. The question is, if used for a class like calculus class, can this be integrated in Moodle? Yeah, yes, certainly. Yeah, because we could use LTI protocol. Yeah. How difficult is it to write the actual problem? So, for example, even the diagram, do you have that?