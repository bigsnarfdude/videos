Chetti from the University of Florence, who's going to be talking about ARIMA models and multivariate Bayesian structural models for causal inference on sales data. Take it away. Thank you. Thank you. So first of all, thank you for having me here. I'm very glad to join this workshop. And the way I've thought about this workshop is to give a practical overview of what are the main issues that can come Many issues that can come up when analyzing sales data for policy evaluation. So, I'm gonna jump straight to the empirical problem that we were facing. And then also we'll describe two different approaches we came up with to solve all the issues that we faced back then. So, on October 4th, 2018, a couple of years ago, the Florence branch of this Italian supermarket chain. Branch of this Italian supermarket chain decided to lower the price of several stock brands in many other categories in order to, you know, they hope to increase their sales. We focused on the hookies category and among them 11 store bands were selected for this permanent price reduction ranging from 5 to 23 percent. The thing I want to stress here is that we do Want to stress here is that we do not know how these Levant stock brands were peaked by the supermarket chain, so we couldn't rely on the randomization assumption. So we set up an observational study, and we were asked by the Sweden Market Chain to assess whether their price promotion affected the sales of their storebund products. On top of that, the supermarket chain also identified 11. Market chain also identified 11 direct competitors, which were cookies with the same characteristics as the store brand cookies they picked. And they were hoping, you know, that by introducing this permanent price reduction, they were not affecting too badly these air competitors. And just to give you a practical overview of what this data set looked like, I included this figure here where on the top row you can see Top row, you can see the store brand cookies, and in the other row, you can see the competitor brand cookies. And those two are literally the same product. So they even have the same shape. So and same ingredients, same flavor. Even the packaging looks almost the same. So it kind of, I mean, we were expecting a negative effect on competitive cookies stemming from this intervention. Stemming from this intervention applied on the storeburn cookies as well. And so, our tasks, so the task that the supermarket chain assigned us was estimating the overall effects of this price policy change, both on the property theory brands, but also with a special focus on the direct impact generated on hypothetical goods as well. May I ask a question? Yeah, sure. Does the price reduction? Yeah, sure. Does the price reduction bring the store's cookie price below the competitor's price or equal to the competitor's price? Well, actually the store price was already below the competitor price because usually store-burn products, the store-burned products sold by the supermarket chain were already cheaper than the other ones. And with this intervention, it was they were a lot more cheaper. So that's why we were sort of expecting. Why we were sort of expecting a negative, a huge negative impact on the other competitor products. Thank you. Great. Thank you for your question. And so this task actually was not so easy because there was, I mean, we found many issues that could complicate the estimation. First of all, there's a correlation structure, which comes from the time series. Structure, which comes from the time series data. And then sees data are seasonal typically. And we also spotted all the effects because I think we can all agree that when, I mean, people maximize grocery purchases before huge important holidays such as Christmas or Easter or the alike. And finally, we have a user defense problem because what Let me state that more formally. So, interference typically arises when the potential outcomes of one unit are influenced by the treatment assigned on other units as well, which is precisely our case, right? Because we are expecting that a negative impact on competitive products stemming from an intervention which wasn't directly applied on competitive products, but instead on top-brand products. That on top-brand products. So, in some sense, to infer the effects attributable to the policy, we should net out, let's say, the portion due to other factors. Because typically, what people do, it's comparing the sales before and after the intervention. And then if they observe, let's say, 50% increase, they are happy and they say, okay, this intervention has actually boosted the sales of this product. But it may not be the case because. It may not be the case because maybe there was already an increase in trends for the products, which has already started in the past. So the last 50% increase you observe, it's just natural, so it cannot be attributed to the policy, right? So there are these issues that can complicate the inference of causal effects in observational settings. And the first approach we came up with is based on a Came up with is based on ARIMA models, and it was a joint work with Flobita Cippellini and Fabitze Amniamali from the University of Florence. The first thing we did is setting up a COSM framework under the Rubin-COSM model comprising the assumptions allowing the postulation of potential outcomes in the Euro-COSA contrasts, which are basically our causal estimates of interest, and then the assumptions for the identification and estimation of these estimates. Estimation of these estimates and the attribution of the uncovered effect to the intervention. That based on that, we used an inferential methodology exploiting a LIMA models that essentially involves predicting the outcome in the absence of the intervention and contrasting it with the observed outcome. This is very much alike with causal impact, I mean, the causal impact approach by Kay Boderson, but instead. But instead, it's based on IRIMA models, whereas causal impact uses Bayesian selection of conservation model. And the way we thought about this approach is, I mean, why we did not use causal impact and we set up this other model, other approach. For a very simple reason, because we were asked to do this analysis by the supermarket chain, and they were not Confident with Bayesian inference. So they do not use Bayesian inference very much. I would say at all. So we just wanted to give these people something that they could understand. So that's why we thought about alumodus. And basically, we thought, okay, if this works with Bayesian structural transmodes, it can work with. Optimized models, you can work with ARIMA models as well. People working in this market chain are much more familiar with ARIMA models, so let's give them something that they will understand. And the second reason was ARIMA models are quite common in the econometrics literature and even among predictioners. They are implemented in many, many statistical softwares like R or Stasa. They all have their already Have their already implemented function to do animal forecasting. So that's why we structure this approach based on animal models instead of Bayesian structural time series models. So the way I think about it, this frequent is alternative to causal impact. But now let's go to the assumptions. First of all, I will denote with WIT a random variable describing the treatment assignment. Up to now, I've Up to now, are there any questions? If not, I have a question. Do you feel that the estimate will be very different between causal impact and the CRIMA model? No, I think that this should be quite similar, except that maybe from my experience, causal impact, since it's Malaysian, so it's a lot more uncertainty in there. Uncertainty in there ends up with wider confidence intervals. Instead, ARIMA models have, you know, usually end up with, you know, shorter intervals, so it's much more likely to observe significant outcomes. Yeah, that's my whole review. Thank you, appreciate it. You're welcome. So So, we will maintain the following assumptions: single persistent intervention assumption, because there is, I mean, stating that there exists a single point in time such that before that point in time, no unit is treated, and after that, all units will be treated. And in particular, we have only two potential paths being. The units are always treated persistently or always assigned to control persistently. Assigned to control persistently. The situation which is best represented here. So, this is one of the treated products. As you can see, the intervention state is denoted by the red vertical bar. After that, the price, this is the price evolution of this stock and cookie. After this point in time, the price goes down and stays at this level throughout. So that's why this intervention is persistent. Intervention is persistent. And you can see also the difference between this price policy change and a simple promotion, temporary promotion, which you can see here, this drop in price was a temporary promotion. After a short period of time, the price bounced back at its original level. Another assumption is the temporal interference. So in this first paper, we still rely on SUTPA. On SUOTPA. So we still rely on the absence of interference assumption. Why is that possible? Because we set up, we considered every product to be treated. So the first way in which we address the interference problem is by considering all goods to be some way, in some way treated, so affected by the intervention. So we did two parallel analysis. With the store brand, we did this kind of analysis. We did this kind of analysis. So, the intervention was defined as the permanent press reduction on their price. But we consider also the competitive brands to be treated with a different definition of intervention, being the relative price increase with respect to their perfect substitute. And this is quite obvious because after those goods were discounted, the other competitive goods were immediately The other competitive goods were immediately perceived to be more expensive. And not only were they perceived to be more expensive compared to the direct substitutes, they were. If you can observe a relative price increase compared to the cookies. So that's how we dealt with this interference problem in this first work. The third assumption is covariance treatment independence. So we included covariance in our model in order to Variants in our model in order to improve the prediction accuracy of the counterfactual outcome in the absence of the intervention. And then the non-anticipating treatment assumption is another very important assumption telling us that the probability of each unit to receive this treatment doesn't depend on whether the other units get the treatment or not. And most importantly, it depends on past treatment. It depends on past treatment outcomes and past COVID rates. So, the way I think about this assumption is the time series equivalent of the uncomfortedness assumption in the cross-sectional literature. So it's actually the assumption that tells us, okay, you can attribute the uncovered effect to the intervention. Under this assumption, we can define the causal estimates of interests. I'm sorry to interrupt Fiameto. I'm sorry to interrupt Fiameto. I did not hear what you said about the assumption that you're making. What is equivalent to what type of assumption? Ah, sorry. This is equivalent to the absence, I mean, unconfoundedness assumption in the cross-sectional literature. So typically, people assume that the treatment is unconfounded in the cross-sectional setting. The equivalent of this assumption in this time series. Assumption in this time series setting could be this non-anticipating treatment assumption. So the treatment is not confounded, let's say, by future interventions, future outcomes, and future covariates that you may observe. Thank you so much. You're welcome. So the first causal estimate that you can define is the point causal effect. This is just the difference between the sales under treatment. Under treatment and says under control. This point effect can be defined for every time point after the intervention date, so we can get also the kinetic causal effect. This would amount to be the total number of units sold due to the policy change throughout the whole post-intervention period. And also by dividing that By dividing that for the social number of time points in the post-integration period, you can get also the temporal average effect. So, this would be the social number of units sold on average on a daily basis thanks to the press policy change. Okay, so under those assumptions, we can set up our inferential procedure, let's say. So, here Let's say. So, here there is our potential outcome time series, and we assume this time series to evolve as some kind of process. In this very simple case, this is an ARMA EQ process. You can tell it by the lag polynomials. Then there is this White North term, which means even variant sigma square epsilon. But what makes this different from unusual Armard? Is different from a usual ARMA model is the addition of this component. And this component is let's say an intervention component, but actually it's much more than that. You can see here there is an indicator variable taking one when the unit is assigned to treatment and zero otherwise. So that means that if you substitute here w equal one and w equal zero and you make the difference between You make the difference between the two, what you get is actually τ, which is then defined, as you can probably see here, as a proper causal effect. This is the point causal effect we saw before. So that's why we call this approach causalima, because there is the addition of this causal component here that you are not going to directly estimate, and we will see how in the next few slides. In the next few slides. Then you can also consider much more complex versions of this model. This is, I think, the most complex one that you can set up. This is a seasonal process. These are the seasonal components and then there are the referencing operators because possibly YCW is a non-stationary process. But then what's important is still this intervention component and the Component and the reason why it is so important is stated in this other slide. So, the equation for actually after some algebra manipulation can be quite simplified by defining a transform variable where you take a transformation to, let's say, restore stationarity. Then, Zt will be the stationary part of the model, but most importantly, what you can see from Importantly, what you can see from the simplified equation is that in the absence of intervention, so if w is zero, you can define the case step-ahead forecast of your sales conditionally on the information set up to time t-star, which is our intervention date. So, this head S, if you read it here once again, is the states that you expect in the absence of intervention. So, basically, this. So basically, the forecast of your sales in the post-intervention period is the closest testament you can get to the sales in the counterfactual outcome where no intervention is applied. And you can exploit this guy here to set up an estimator for your point effect. So your point effect at a specific time point T star plus K after the intervention date of applying the intervention. Applying the intervention instead of not applying the intervention is given by this difference. So the observed C is in the intervention case, which you actually observe, as I said before, minus the forecasted C is you would have had in the absence of intervention. And yes, under the hypothesis that the intervention effect is zero, Intervention effect is zero for all t in the post-intervention period, you can set up some hypothesis tests on the point effect, on the cumulative and temporal average effect. So you can actually test whether the difference you are observing between the observed and counterfactual time series are relevant. So if those differences are significant. So to sum up, in order to estimate this effect with this model, we need With this model, you need to follow three steps: estimating the RIMA model only in the pre-intervention period. So that's why I told you before that we do not directly estimate tau ti from days, because we stop our estimation period, let's say, up to the intervention date. And then, based on the model you learn in the intervention period, On the model you learn in the post-intervention period, you perform this prediction step and obtain an estimate of the counterfactual outcome during the post-intervention period. And then, by comparing these two, you can estimate the causal effect and then test the resulting differences with the hypothesis tests I showed you before. Everything is implemented in air package, which for now is available on GitHub. So, everyone, I mean, we are I mean, we are actively working on it, and we believe this approach can be used in all those settings where causal impact can be used. It's, as I said before, a frequently self-nacetic. Yeah, and most importantly, it can be used when there are no control series available. The only limitation of this approach in this setting is Approach in this setting is that we do have some interference, and we are not able to fully understand this interference, fully account for this interference between products. Because, for example, we are not able to account for the interference standing the other way around. So from competitive brands to store brands. And we solved that, I mean, hopefully, using with this other approach. This is a joint work with Yabur Bojinov from Harvard. With Pradyawor Bonjinov from Harvard University. And essentially, what we did here is grouping the units into pairs, where in each pair we have one store and started companies or brand, and we modeled those pairs jointly. We included, therefore, our prior beliefs regarding the interference and the interactions generating between these units by By including our prior knowledge in the variance coherence metrics, regulating the dependencies between the units in the payer. And so that, I mean, in my opinion, this is the correct way to deal with interference in these settings here. So what we did is extending our causal framework, introducing more formal causal estimates, capturing the impact on Estimates capturing the impact on both the unit that received it and the other units in the same group. And to do the inferential part, we derived the multivariate version of causal impact, which we implemented in the causal and the SCSR package. So in that, in this instance, we did use multivariation models because it was so much simpler to, and I think even more. And I think even more correct to include your prior beliefs in the business governance metrics. But we will see that in the next few slides. So about the assumptions, yes, we extended the COSA framework, but essentially all the other assumptions stayed the same, only we partially relaxed the absence of interference assumption, which now becomes the partial template interference. As you can see here, the ATP partial. As you can see here, basically partial temporal interference means that we are allowing interference within each pair. So these two products can interfere with one another and should interfere with one another. Those two products, the same because they belong to the same pair. What we do exclude, I have a very simple stemming from arising between units belonging to different pairs. And this is essentially an assumption about consumer behavior. Consumer behavior because we are saying that people won't stop buying cream cookies just because chocolate cookies are cheaper or vice versa. Causal estimates are the same, just written in a more formal way, because this time we have a payer assignment. So we have, yes, a vector of zeros and ones. In our case, these In our case, these the dimensionality of the group. In our case, this is just two, but I included the generalization because, in principle, I mean, you can set up groups as large as you need. I mean, then it's up to the power of your laptop to do all the computation computations. But in theory, they can be as much as you need. Much as you need. The causal estimates in principle are the same, but just reason in a more formal way. So, this is the point of fact comparing the sales under the assignment W. In our case, the observed assignment is 1, 0, meaning that the store is assigned to treatment and the competitor is not. And WCL is a counterfactual assignment. In principle, I mean, there are this time in a multivariate world, there are World, there are lots of counterfactual assignments. We are mostly interested in the assignment 0, 0, where neither of the two units is assigned to treatment. And then there is the human limit and the temporal average effect. About the inferential part, as I was saying before, we used this multi-variant patient structure of LCS models. And why is that? First of all, because they are flexible. Why is that? First of all, because they are flexible, you can add all the components that you need to describe your data. So let's say you spotted a pre-trend, so you may want to include a trend component, or you spotted seasonality, you can include a seasonal component, and so on and so forth. Most importantly, they allow to exploit these variance governance metrics, which is the Which is the matrix of dependencies between the units in the paper. So by setting the prior distribution on that, we are including our prior beliefs regarding the dependencies between the units in the pair. And that's how we model the interference. And we, yes, basically dealt with the interference issue in this setting. Yes, this is another parameter that we have. That we have. Essentially, the CT is a variance, governance metrics regulating the dependencies between the states. This would be mostly, I mean the states are usually assumed to be independent. So this would be a diagonal matrix that can also be redefined in this way because actually the dependencies between the units is the same, but by But by means of this matrix C, actually the states can, every one of them can have its own prior variance-covariance matrix. Yes, now just to give you the idea of the results we found, I'm including here the results of the empirical analysis. So we had actually So we had actually instead of 11 cookies, 10 cookies, because we had quite enough missing the initial app for one of the considered products. So we did the analysis on 10 paves. We used a trend per seasonal model which was selected based on posterior philippic checks. So we tested a bunch of models and the trend plastisonal one was the model that That I mean, Sue said very well. I mean, our data with respect to the other ones. So we selected this model. We included covariates to improve the prediction of current counterfactual outcome. Among these covariates, we included such holiday dummies, and even a set of, let's say, synthetic controls selecting among the one category. On the one cafe that was completely left out from this policy change, and which were wines. So, wines were the only products completely left outside this price policy change. All the other categories had at least one product within this price promotion. Yes, so that's, I mean, our committee set mostly. And also, we included the unit prime. And also, we included the unit price with a specification here because the competitive brands were not directly affected by the price promotion, so we could use its price. But in order to respect covariance treatment independence assumption, we couldn't use the stored price, so the discounted price, right? Because that's the covariance in some way absorbing the intervention. So we basically included the most likely. To include the most likely price that we would have had for the store cookies in the absence of the intervention. Meha, a quick question. If you can go back. Yes. You're using wine as a synthetic control and that's great, but I imagine that wine sales might be associated with Saturday and Sunday dummies and also with the holiday dummy. Is that a problem? um did not find uh so much association um we tested we we we did some um some checks uh about the i mean on the correlation metrics and uh we did not find uh i mean that they were so much associated so we did not have the beautiful problem you're welcome uh and And actually, the results, I mean, those are the results of our analysis with the multivariate region structural models. We can spot the presence of three effects on products four, seven, and ten. The way we should read these numbers is that, for example, product number four, in a one-month period following the intervention, this product, thanks to the permanent Thanks to the permanent price reduction, we was able to sell on average 47 units more per hour because actually our these were I mean daily data but we have average sales counts for each I mean we divided the total number of units sold for that day by the total number of hours that the shop stayed. Number of hours that the shop stayed open because we have a Sunday problem where this market chain stayed open just for the morning. So we normalize, let's say, our counts by the social number of opening hours. So 47 units sold more on average thanks to the fairmine price reduction. Interestingly, I mean, I think one of the most interesting results. One of the most interesting results here is the absence of effects on all competitive brands. So that was actually the result that this market chain is open to have because they observed some effect, some positive effect on their property theory brands, but the other ones were not impacted so badly. Because I don't know if I mentioned that before, but the supermarket chains sells. Sells both their property brand and the other ones. So they didn't want to have too harsh effect on the other ones, and they did not. Meaning that there are other factors driving the sales of these competitive brands, which can be interesting to find actually. Those are some plots of the effect, of this positive effect, and then the comparison between the observed seeds with the Observed sales with the forecasted sales. Those are the posterior peditic checks we ran, like the density comparison, so comparing the posterior paedic mean with the empirical density, and then yes, some vertical diagnostics. But all in all, just to conclude, yes, advertising campaigns are Advertising campaigns are quite important for many firms, and managers are often involved in the assessment of the effectiveness. Our two approaches can be used to estimate the possible effect of interventions under the mean potential outcome framework, so we can actually state that we can attribute that uncovered effective intervention holding those assumptions. And most importantly, both approaches can be used in observational settings where you can. Settings where you cannot rely on randomization, and also in those settings where you do not have suitable control series. Otherwise, if you have many control series, you can just use synthetic controls or differencing differences. And in particular, the first approach based on animal models can be used as the frequency alternative to cause an impact if you have a single-term series of even. Term series or even many non-interfering time series. Whereas, in more complex settings where the units do interact with one another and there might be substitution effects going on, then the other approach based on multi-meditation time series is for sure a better choice. So, yeah, that's enough for my side. Thanks for your attention. And I'm happy to. Welcome. I'm happy to take questions. Thank you so much, Fiameta. This is fascinating, and I'm delighted to see time series being used seriously in this domain. It was always sort of implicit that you would want to be able to use time series tools in studying computational advertising, but I don't think we've had a talk yet that's really taken that approach. One question is: Do you really need the complexity of an ARIMA model? Would an autoregressive model? RIMA model? Would an autoregressive model be sufficient? Well, actually, RIMA models are very general, but for sure, you can also have the authoregressive part. So you can set up one 0-0 model, the authoregressive, one process. Yes, that's for sure. The important part is that you do what we did before, so doing Before so doing the estimation up to the intervention date and then do the forecasting and comparing the observations with the forecasting, but the model that you use in the pre-intervention period, it must be suited to your data. So if an autoregressive one model is fast, use it for your data, that's perfect. Yes, that was sort of my question. How complicated was the time series model of the data required in this application? Of the data required in this application. In this application, not so complex, not so complex. So it was at most like two, zero, one. Every time series, we did like the way 11 products, and I should say 10 because of missingness, but we estimated 10 independent products, sorry, 10 independent models. 10 independent models. So, on each TEM series, we selected the best model based on some information criterion, or either the academic information criterion, or the vision information criterion. And so, this model that we selected was the best for that particular cookie. And in some way, it was in yes, one zero zero model, so not very best in one. In other case, it was a two zero one modus. 201 model, so but not so complex actually. Thank you. Does anybody else have a question or a comment? I am curious to know why this chain of grocery stores did not do an experiment in which they lowered the price in one store for a couple of months and then evaluated the effect there. That would have been, I think, a fairly clean way to test. I think a fairly clean way to test what's going on. Yes, I agree. I mean, of course, as a statistician, you would hope to have firms experimenting a lot so that you can set up a causal framework where you can rely on randomization and whatever. I think this market chain, I mean, it was, it's a huge market chain in Italy. It's a huge market chain in Italy, but they are not so familiar to causal inference yet, so they are not experimenting as well. And I think, I mean, this is quite common in Italy. Firms are not yet adopting a causal approach for their problems, which is sad because they just rely on pre-post-analysis, as I was seeing before. It's just they introduce the intervention, they compare what happens. Compare what happens afterwards with what was happening before the intervention, and that's the causal effect. That's the effect. I mean, it can be an effect, but of what? So it's not even causal. So many firms share this initially yet. Whereas I see that Twitter and Amazon and Google do run a lot of experiments, which is nice, which is good. Which is nice, which is good, actually. Yes, but Italy is not unique. I think that many American firms are not really thinking about causal inference in anything like a very sophisticated way. But I do think that most firms would probably do some sort of experiment before they implement a sweeping change in the price structure for all of their goods. I mean, it just seems to me to make sense to try it out at a small scale before you commit. Yes. I mean, yeah. Yes, I mean, actually it was, this wasn't an experiment, it was, as you correctly are recalling, you know, a huge price policy change affecting all the stores, I mean, without any sort of experimentation. Or maybe they just run a couple of questions to their consumers and ask. Consumers and ask them, would you be happy if I reduce the price of those products on a parameter basis? I don't know, maybe they did some investigation of this kind, but they did not for sure experimented anything. And then still, this was, I mean, not, this experiment was in Florence. I'm sorry, this was not an experiment, but this was run not so much on a large scale. On a large scale, so this was quite reduced because it stayed in Florence or yes, and in Tuscany at most. In other Italian regions, they didn't do this price policy change. So it was huge.