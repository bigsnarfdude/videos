At least that. Yeah, before I start, yeah, this is my first time here. I'm delighted to be here and really happy that people from two neighboring countries can come together. And I can't believe I have to say this, but it's great that Canada and the U.S. are friends, and I hope they still be friends. So sure that time is relived. Okay, this is joint work with my student Yong-lyung, who will be looking for a job next year. So a lot of this is her work. A lot of this is her work. I'm happy to chat more offline about anything. If you have questions, please stop me. We can go, let's see how far we go, and I can take some extensions offline otherwise. Okay, so what is the motivation? I was talking to a former colleague of mine who was doing some work with a freight marketplace. So for those of you who don't know, in freight transportation, drivers really hate what's called deadheading. So if I have to pick up a load and point So, if I have to pick up a load in point I and take it to point J, I would much, much rather have something else to pick up at point J to pick to come back with, or something roughly approximating that. And that's the idea, the motivation behind this model. So I might be, if I'm a broker who runs some kind of a freight transportation market, before I offer these loads up to the drivers, I might want to pair them or more generally. To pair them or more generally bundle them into attractive routes that involve less deadheading, because if I do that, the drivers will be more likely to want to take these bundles or these pairs and will offer me a lower price for them. And so you can look at here at the example of how I would calculate the reward of matching these two loads is: I would look at what do I save, the red, by matching these two loads, minus the additional distance that I have to drive, which is the yellow. I have to drive, which is the yellow. Okay? And there's a similar thing you can think of in a ride-sharing marketplace. So here I have drivers, I have riders, and I'm the platform again. And in this case, the drivers want to avoid detours as much as possible. I want efficient use of my resources. And so I might want to do something like instead of doing this zigzagging, right, or assigning the trip to two different. Or assigning the trip to two different drivers, I can pair for willing riders, I can pair them together and optimize this kind of joint, this ride that has some elements where the two riders are riding in the car at the same time. And then if I do this, I can offer the riders a lower fare for this trip. So this is the kind of thing that we're talking about here. These are the motivations that we have in mind. So let me first give you the model and then talk about some related work. So the idea here is I'm going to have a bunch of The idea here is I'm going to have a bunch of types. And these are types of nodes that might arrive at any given period. I'm going to be operating in discrete time. And for the purposes of this talk, this is going to be Bernoulli's. So every type has an arrival every period independent of all the other types with some probability P, P I. And I have what's called a sojourn period. So the sojourn period here is: I am the platform manager and I quote the arriving. I quote the arriving nodes of each of the types, a maximum waiting time. So, tau periods is the maximum you will wait in the system before you get service. So, you can imagine this either way. What I mean by the maximum waiting time is imagine back to the examples we had before. I will just give you service by yourself if I can't pair you with an appropriate match within that waiting time. But I would like to pair you beforehand. I'm also going to assume. I'm also going to assume that this network is complete, so I can match anybody with anybody, and I'm just going to embed the compatibilities in my reward. So, any two different types have a certain reward for being matched. And think of this as, in the examples before, how much distance did I save by pairing these two loads together? This kind of thing. Specifically, in some applications, I might actually like to pair two nodes of the same type. In some applications, that's not as appealing, but in some applications, That's not as appealing, but in some applications it is, so we allow that as well. And we're going to look at optimizing the long-run rewarding system. So here's a little cartoon of what this might look like. I have three possible node types, and I have a sojourn of three periods. So at any given time, I have a system state that looks like this, and I have to decide what to do. So I maybe make some matching decision, and after I make a matching decision, those notes disappear. And anything that's been waiting. And anything that's been waiting for too long, if I don't match it, it disappears as well. Then the process moves forward, I move my to the next period, and then some new stuff shows up, and I decide what to do again, and so forth and so on. Okay? All right. So what are we going to look at in this model? We're going to look at the two, to my mind, two of the simplest things you could think of to do. One is batching, which is wait for a given amount of time. In this case, For a given amount of time, in this case, the sojourn time, batch arrivals over the sojourn time, make the best possible decision I can make with the stuff that's available over the sojourn time, and then repeat. Versus greedy, which would be kind of what it sounds like, at any given time I look at what's available, I make the best decision with those availabilities, and then I keep going. So, what has been done with these kinds of models in the past? To my mind, the two most related works are probably these. Mind the two most related works are probably these two. So this one is in continuous time and has Poisson arrivals and they don't really have a sojourn time. You can think of it more if this is a waiting time or an impatience. Like a node shows up and they have a certain amount of time that they're going to wait in the system before they decide to leave. Now, this is not known to me as opposed to in our model, I know exactly how long everything's been waiting, and I can make decisions based on that. And then there's this other paper which does something kind of like what we do, but they do it with adversarial arrivals. And they have a fixed sojourn time. So they actually have a quoted sojourn time in the system. And both of these papers do algorithms that have constant competitive ratios. And then these other papers are, in some sense, also non-bipartite matching, but the assumptions are a little bit farther afield from what we're doing, but there's also some stuff here. There are other works, and of course, once you. There are other works, and of course, once you look at bipartite matching, there's many, many, many papers, as you will see that this workshop. So, I apologize if I didn't list your paper, but this is to my mind the most related stuff. Okay? So, what we are doing instead is, if you think especially of these two papers, they've got these constant competitive ratios. What we're going to do is look at the performance of these two kinds of policies as I let my sojourn time grow. And the punchline is what I want is to. To convince you all that I really don't need a very long sojourn time for both of these policies, when appropriately designed, to give me good performance. So just to nail down exactly what we're talking about, for the batching policy, I'm going to do something like this. Like in this case, suppose the sojourn time is five. So I accumulate arrivals over five periods, and then I look at the window of time of those. Of time of those, everything that showed up. And then I'm just going to make a solve a maximum reward matching problem offline, just with whatever's there, execute those matches, clear the system out of everything regardless of whether it got matched or not, and just repeat. This is what a batching policy does. The greedy policy is a little more like: I'm looking at what I got. Okay, maybe nothing to do here. Move forward one step. Look again. What can I get? Okay, maybe match something. What can I get? Okay, maybe match something, so forth and so on, as every period goes by. Okay, so the first thing to notice, right? Yes. Is there any reason to match something before it's about to expire? No. And we're going to use that a little bit later. But in this model, rewards do not depend on how long stuff's been waiting in the system because I'm assuming, or only in the sense that the reward is either what it is or zero if I wait too long, right? So you're exactly right. We'll come back. Right, so you're exactly right. We'll we'll we'll come back to that. So didn't I understand correctly that one difference of the model with those that uh know the orientation is that uh if you don't match enough, then this kind of go off and you go to the next class. But it's not the case that you might have a spillover from the previous class. No, everything that you did not match goes away. Yes, that's one of the key differences with some of those other papers. Exactly. Yeah. Any other questions? This is great. Just to make sure everybody has got the basics down. So the next thing to notice, which is a pretty basic So, the next thing to notice, which is a pretty basic observation, is that a greedy policy, as I stated it to you, is actually not very good. So, here's a simple example of why. So, imagine that I have three types, and what I have is essentially a decoy type. So, I have this node that shows up all the time. Every period, one of these nodes shows up, and this node has a very small match value with anything else. But if I match the other two types with each other, I get a very large match value. A very large match value. But then the greedy policy is always going to waste any arriving node with one of these decoys and not kind of wait and see if they can get something better. So in some sense, the only time I'm going to get the high-value match in a system like this is when I'm lucky and I get the arrivals of the two high-value matches at the same time. And otherwise I'm wasting everything. So you can see that I'm going to do significantly worse than if I had been a little more strategic. You know, a little more strategic. So, a simple greeting policy is not going to work. What we're going to propose is a slight modification of this that uses what we call a reserve price, but you can probably give it a bunch of other different names. And I'll get into the details of that. Okay, so this is basically what we're going to analyze. For the batching policy, it's exactly what I described to you. And for the greedy policy, it is going to be a greedy policy that solves a max reward matching problem, but that Solves a max reward matching problem, but that includes these things called reserve prices that I will define in a second. And very crucially, it executes the matches with only the nodes that are about to expire. Now, in some sense, we're not really using that in our analysis, but that's just, you get that for free. You may as well wait until something's about to expire because, in this system, at least there's no penalty for doing so. So, this is the setup. The concept of a reserve price here is essentially imagine that I add Phantom. Essentially, imagine that I add phantom edges, phantom nodes and phantom edges, and these edges are going to have a reward that depends on a couple of different things, including how long the node has left in the system. And when I solve my maximum reward matching problem, I include these edges. And so these edges prevent me from making silly matches like the ones I showed you before. That's the basic idea. So, in this example, then, if I calibrate these rewards. Calibrate these rewards here, or these phantom edge rewards, then I prevent myself from making the bad decision and I kind of save everything until I get the right arrivals. Okay, so here is in one slide the main results. And so everything here is a function of the sojourn time, meaning how do these policies perform as I let my sojourn time grow? So if I look at the performance of the batching policy compared to The batching policy compared to the optimal that I can do in a period of sojourn period of tau. I'm off by a factor of one over root tau. And for the modified greedy policy, I'm off by a factor of one over tau. Now, that's not probably terribly surprising, especially to this crowd. As I make stuff wait longer and longer and longer, I'm clearly going to get to optimum. Maybe the question of how fast is is a slight question, but I'm clearly going to approach optimality. But I'm clearly going to approach optimality. To me, more practical conclusion is the next two statements, which is that if I'm willing to be a little off optimal, then I will converge exponentially fast. And so for the rest of the talk, I'm going to try to give you some intuition as to how we get there. The last thing I'll say is this first result is tight in the sense that there exist instances that will not converge faster than this rate. For greedy, I suspect that that. Greedy, I suspect that that is type, but I don't know for sure. And the one thing we will need here is: so, for this talk, I'm just going to do everything with Bernoulli's, but I do allow general arrivals, so I can have multiple nodes of the same type arrive in one period, as long as the support is bounded. Okay, that's the only assumption I need. And I do need independence. What if each type has a different subject to do? Interesting question. I guess if they all grow. If they all grow, so that will complicate our model a lot, but in the sense that since I'm making everything be a function of the sojourn time, if all of those sojourn times were to grow at the same scale, I would basically do everything with the smallest of them anyway. But think here more of like the sojourn time is like this time that I'm quoting, this guarantee that I'm quoting, that I'm quoting to the nodes that are arriving. Like you will not wait longer than this. But there could be situations in which that could be by. There could be situations in which that could be by class as well. So there would be extensions that could be possible. So we're going to use the LP that you would expect for this, which is just a simple fluid relaxation with, just note here, this is because I can match nodes of the same type. So outside of that, this should look like you would expect. And some other papers that I pointed to before have something that looks roughly by this. Here, this is what I want the matching frequency to be for any given pair. Frequency to be for any given pair. So the analysis hinges on doing everything for just a single pair, and then we're going to do some randomization to extend that analysis to the general case. So this is probably something Siva would give to his students as an exercise in his class, based on what I was hearing yesterday. So if I have Bernoulli arrivals, then if I look over a sojourn period of tau, the total number of arrivals for that type over the sojourn period. Arrivals for that type over the sojourn period is a binary. And so for two types, if I have this A and I have AI and AJ, then if I look at that LP and I imagine just an LP that just has the two types, then this is the optimal solution of that LP. And this is what the batching policy does over a sojourn time of tau. And so the gap between these two quantities is, if the two are equal, is one over The two are equal, is one over root tau, and then it gets this exponential function if they're slightly off. And the proof uses concentration inequalities that are, I think, more familiar to everybody in this room than to me. So this is a pretty basic result. This uses a Hofdings inequality. And maybe there are a couple of things to point out here. One, this is how we get the approximate optimality result. So basically, So basically, if I already get, if I'm lucky and the two arrival probabilities are off by more than epsilon, then I will get the exponential convergence for free. But if they're not, I will just randomly discard to get that difference to the epsilon, and then I'm off by it most a factor of one minus epsilon. And maybe forgetting the proof here, the intuition is the following. So why do I get this difference? The intuition is. The intuition is: if the two probabilities are exactly the same, then in order for me to get that probability as a frequency, I need to be lucky with both types. I need to have the arrivals of both types not deviate too far from the mean. But once there's a slight difference between the two, then I really only care about the lower of the two. The higher one, I kind of, even if I'm unlucky with it, it doesn't really affect my frequency that much. So that's why I get this difference. Does that make sense? Does that make sense? So, this is the first kind of randomization step that we get to get these two results that I showed you before. For the greedy policy, now one thing that we're using here is in the case of a single pair, the greedy policy is actually optimal, right? The greedy policy is whenever possible, make a match and break ties by picking always the thing that's been waiting the longest. So, the analysis here is a little more complicated. Analysis here is a little more complicated, but not, I would say, complicated without being illuminating. I have to build a Markov chain that tracks what is my state of my system. Now, the Markov chain for a Bernoulli case, this is a binary vector. And because I'm running a 3D policy, one of the two sides of the vector is always identically zero. Because if I ever have something available to match, I will match it. So I'm really just looking at either one side or the other has stuff waiting, and the other side is always empty. The other side is always empty. And I just look at the stationary distribution of that Markov chain and calculate how often am I going to actually execute a match. And it ends up being something similar to before. When I have equal probabilities, I have something that depends on one over tau. When I have slightly unequal probabilities, I have this factor right here, which is the important factor. And so I do something similar, right? If they're off by enough, then I get exponential convergence for free. And if they're not, then I will randomly discard the smaller of the two to get that gap. The smaller of the two to get that gap, and I get exactly this and that. Yes? Why is the focus on a single pair? Or, like, how do you, in practice, choose which pair to look at? Imagine right now that the universe is a single pair. Okay, okay. Yes, there's nothing else here. There's just an I and a J, and I'm just looking at what happens in this case. Yeah, thank you. So now we're going to go to the general case. Okay, so, general case. So we're starting with these single-pair results, and then we're going to. Results. And then we're going to look at what the policies do for the general case and define a kind of randomized in-between step. And the randomized in-between step actually inherits the performance guarantees of the single item care or the single care case. And then I'm going to show that what we defined before dominates the randomized counterpart. So you're going to sample from the exact relaxation? Is that? Exactly. Yeah. So in a manner in which anybody here probably would. Anybody here probably would this is the natural thing to do if you've got this kind of a relaxation. I look at the optimal solution and then I essentially this is an asymmetric quantity. I look at what proportion, the relaxation is telling me what proportion of my arrivals should be matched to this type. And I'm going to assign those arrivals to that, we call it a subtype. And so instead of just having type A, I'll have subtype ij. And if I do this, and then for arrivals of type J, I have subtype. For arrivals of type J, I have subtype JI. And once I do this, everything is a bunch of different separate single-pair instances. That's basically it. So in this case, this cartoon shows you, like here, this is a type I, but it's subtype K, and this is a type J, but it's subtype I. And so this tells you not everything will be compatible. But that's okay, because what I do now is I will achieve for every one of these subtypes, I will achieve. Every one of these subtypes, I will achieve the same convergence rate as the single item case, and I will just get the worst possible convergence rate over all of the pairs. And so if I do this randomization, then I've got a version of my policy that will have the same convergence guarantees for the general case with this simple randomization. Okay? You are using linearity of the expectation and the fact that they're just information accurately. Yeah, so essentially what we're doing is we're saying, So essentially, what we're doing is we're saying, when I define this probability, and this is independent of the arrivals, so then when I look at the matching frequency for this specific subtype, it basically will become this number. Probably says this. Okay? Any other questions? Okay, great. So that gets us to the general case, but it doesn't really. This is not a practical policy. This is not going to perform well in practice. We tried. And so, how do you get to the general case? So, first, for batching, there's really nothing to do. If you look at any batch, then the subtype matches I made over that batch form a feasible solution for the maximum reward match that I could perform over the nodes in that batch for the batching policy. So, clearly, I will do at least as well. So, there's nothing to do there. For modified greedy, I have. If I'm greedy, I have to do a couple of things. So I have to calculate what I call this reserve price for the node and give it to the node. And then I will, at every step, define this max reward matching. And the reserve price is going to depend on the subtype. So I have an arriving node, I assign it a subtype based on the randomization that I showed you before. And now I say, okay, how many periods do you have left? What's the probability that a competitive The probability that a compatible subtype will arrive before I expire, and that's in expectation how much I expect to get from this node. So I will not match it unless my match sort of exceeds that amount. I put that into the matching problem, and then you can do essentially an argument from expectation showing if I chose this reserve price edge, that's because the available, I prefer to save this node rather than use it right away. Save this node rather than use it right away. That's essentially what the argument says. So, then you, quick question. Do you shrink that expectation a little bit when you add these down nodes? Like the reserve price is exactly equal to the expected. It's exactly equal to this. So the reward of the match times the probability that a compatible subtype will appear at any time over the next. So you don't have to shrink it by a button minus up to once again. No. And then back to the question about weighting. Question about waiting. So, this will re-optimize whatever is available at any given time. Now, in principle, the greedy policy would simply execute everything right away. So, I could just re-optimize and execute immediately. But, of course, I will do better by holding off and only executing the stuff that needs to be executed because something's about to expire. Because if I do this, then I iteratively can improve at every step. So, this is a heuristic, it's just better performance empirically, but it's clearly a much better performance. Clearly, a much better performance. Yeah. But is it obvious that it inherits the guarantee from the other one? Because you might now be making a matching that is actually not the matching. Yes, so it's a lot harder than the other case. You cannot make an argument on a single cycle path because there will be cases in which doing this will, you will regret it later. You can only make an argument on expectation. So, this reserve price, in some sense, tells me I should expect. Some sense tells me I should expect to get at least this much reward from this node. And so, if the matching that I construct matches it to something else, then I know that I could have, I had this available reserve price match with a dummy node, and I preferred something immediate that was better. So that's roughly how the argument goes. Yep. So, just to clarify the interaction between the subtypes and the reserve prices, this is effectively when a type I node arrives, I randomly assign it a reserve price. Yes. Yes. Exactly. So I need to. Yes, exactly. So I need to, I still need to assign subtypes, and based on those subtypes, I will need. So, in the sense, I do have, even though the reserve price is deterministic and what I'm solving is a deterministic matching problem, there is some randomness in how I assign that price. So, that's a key point. So, here the subtab is only for forcing the price, right? You're not forcing. Exactly. So, once I, what I'm doing here is I'm essentially mimicking. I'm essentially, I'm mimicking the behavior of the randomized policy, but I will allow myself to match with anything. That's basically, I'm trying to get the best of both worlds. Other questions? Couldn't you modify the greedy by some estimation of the dual value as a shadow pressure instead of using it? Maybe. I don't know. Let's talk offline. Yeah. Okay. So in the remaining time, first let me give you, before I go into some experience, First, let me give you before I go into some extensions, let me give you an illustration of how this looks in practice. So, we constructed a couple of different case studies. I'm going to show you one with the New York taxi data. So, using the taxi zones, we constructed an instance, a ride-sharing instance, with Monday to Friday commuting time arrivals. And we tested a sort of a straightforward greedy policy, and then the two policies. Greedy policy, and then the two policies that I described, and also the offline match. So, simulate for the entire rush hour and solve it offline after I've seen the whole thing. And I'm going to, the time unit that we picked is 10 seconds, and the sojourns are essentially like a minute or a minute and a half or two minutes or this kind of thing. Okay? So, here's what stuff looked like. I'll know two things. One, to our surprise, it is not the case that the modified GUI universe. That the modified greedy uniformly outperforms batching, which I was very surprised by because I would have thought modified greedy would be just better. But in this instance, it's not uniform. Also, the LP starts kicking in and beating the offline simulation even for a sojourn time of two minutes, which I thought was kind of interesting. And no surprise here, this is how the average waiting time for the mold. And so this is Modify Greedy. When you wait and delay, Greedy, when you wait and delay until one of the nodes is about to expire, you clearly are making nodes wait longer. The batching is essentially just, for the batching, it's just the average length of the social node. So let me then give you a couple of extensions and wrap up. So the first extension I'll say is one thing that I did not mention, but a referee was actually the one who noticed this, and I'm very glad that they did. Ride-sharing is a situation in which I highly prefer same-type matches, right? If I'm doing ride-sharing, Same type matches, right? If I'm doing ride sharing and I luckily enough have two people who are both at point A and both want to go to point B, that is my ideal match, right? So it turns out, in situations like that, where I prefer, this can be made precise, but in situations in which I prefer same type matches, I get actually much better convergence. And this essentially goes back to in the same type, in the single pair case, the same type match convergence. But same-type match convergence is just much faster. And the LP only wants to do same-type matches. So when I'm doing my comparison of my policies against what the single-pair policies do, it's only against same type matches. So I get much, much better performance. And then let me talk briefly about impatience. So impatience here is, imagine that even though I quote you a sojourn time, You a sojourn time, you might still not want to wait the whole sojourn time. So I'm going to quote you a period of tau, but some people are just going to leave early anyway. But my contention here or my assumption is that my quote to you or to an arriving note of a sojourn time is going to influence how long we wait. So, in that case, we have the following results. Essentially, our results still hold for batching if our departure process. If our departure probabilities are very heavily influenced by sojourn time, and for the greedy policy, slightly influenced by sojourn time, meaning at least that they decay in the limit. And these are both in the sense best possible, in the sense that there are instances where I have, if my departure probability only decays at a rate of 1 over tau, then my batching policy is no longer optimal. And if the departure probability If the departure probability is constant, then my greedy policy will not converge to the fluid LP. So we are working right now on extensions, and my sense is that this will require stronger relaxations, kind of like the Awadashi Touch paper has a stronger relaxation for a similar model, but is the kind of thing that we're looking at right now. That's all I got. Thank you. Any other questions? Maybe one or two. First, could you foresee modification to the batching algorithm for it as well as the greedy? Because it's sort of like it's making order T correct matches, order square root of T incorrect matches, I guess, whereas the greedy greedy incorrect matches, rather than order one incorrect matches roughly, those sort of lines up in the issue of what we have. Interesting. Interesting is there's something you basically have these sort of cheese privileges. I don't think so, because at least I think of this like if I look at the gigantic matching graph, what the matching policy, the problem it has is I have like a line, every T periods I have, every top periods I have a line, and all the edges across that line are infeasible. Edges across that line are infeasible for my batching policy. This goes back to like, unless I assume that my model has stragglers of some kind. But essentially, I have this big batching graph, but anything that crosses every one of these thresholds is infeasible for my batching policy, whereas the greedy policy will be able to use a lot of those edges. Yeah. So you can. Yeah, so then would you do it like T over tower over two time, but then keep stuff? Yeah, we thought a little bit about that, but not a lot. So, yeah, that's a great question. The other question I had was this seems to be sort of more based on sort of central limit and behavior, and weights are kind of sort of like a dense graph. Would there be like a sparse version of this? Interesting. Well, the results are independent of the topology and the arrival process. But I can imagine situations in which you could sharpen this, maybe if there's imbalance. I don't know. That's quite possible. Yeah, I guess in nature's time, maybe let's have lots of time for offline questions. Alright, okay, so next we have Wei Mao Yiu from Chicago Fool's from Chinese Ugo Hong Kong to tell us about data-driven matching for impatient and heterogeneous demand and supply. Oh, wait, I haven't stopped the recording for the previous month.