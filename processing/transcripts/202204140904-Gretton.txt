Hope to be interactive. And yeah, all right. So the task that we're looking at is model criticism. And I'm going to basically motivate this with a simple example. So let's say that we are looking at the city of Chicago, which is broken into regions here, and we're interested in modeling crime. So here, what we've got as a scatter plot are all of the Scatter plot are all of the crimes that have been committed in Chicago. And let's say that we've done a very basic statistics course, so we've fit this crime model with a mixture of two Gaussians. The question that we might then be interested in is, well, is this a good model? So, you know, plainly it's got some limitations here. But in a way, like the fact that it's not a good model is Not a good model is almost irrelevant. We kind of know in advance that our model is going to be wrong if our problem is sufficiently complicated. So asking, you know, does a model, you know, is a model correct or not, is kind of meaningless, right? As you get more and more data, probably you're going to find a mismatch between your model and your data. So a much more interesting question to ask is which of two candidate models is closer. So here I've got a mixture of two components, a mixture of 10 components. I've got a mix of two components, a mix of 10 components. And in this instance, one might hope that more components fit better. Of course, the mismatch also depends on how much data you've seen. So if you've seen a small number of points, then it may matter less to get this fine detail than if you've seen a large number of points. So we can refine our question even further. We can say, given how much data we've seen, is it possible to decisively tell that model P is better or worse? That model P is better or worse than model Q. So, this is, I think, a much more interesting question to ask. So, what we're going to look at here is a relative test of goodness effects. So here we've got model P and model Q. Hopefully, one of these models is going to be closer to our underlying data R, and we're going to look for some notion of divergence, which is going to then hopefully give an ordering of our models. Okay. Okay. The other point that I want to emphasize is that we're going to be interested in Bayesian models with relatively complicated latent structure here. So here I've given a sort of scary looking model, which is a hierarchical latent Dirichney model for text with a Chinese restaurant process prior. So I'm showing this just to look intimidating. It's a sort of iceberg model where the observed A sort of iceberg model where the observed data is just one node among a very sort of large and frightening amount of latent structure. But ultimately, what we're interested in is looking at how well these kinds of models fit data and comparing different models with latent structure. So if you're a Bayesian, these models with latent structure are sort of the interesting models. There's very few models that one might be interested in which don't have this latent structure. Okay, so this mode. Okay, so this motivates the title of this talk: this relative goodness of fit tests for models with latent variables. I'm going to be using as a mechanism for measuring the goodness of fit, a kernel stein discrepancy. So, this is a measure of discrepancy, which is derived from an integral probability metric, but modified using a Stein operator to give a Using a Stein operator to give a measure of goodness of fit for models that we have in closed form up to normalization. I'm going to talk about a statistical hypothesis test, in particular, hypothesis test of relative goodness of fit. So, you know, does model P significantly outperform model Q given the amount of data we've seen? And yeah, this I'll demonstrate on a couple of examples, including later Dirichlet allocation. Okay, so you know, as. Okay, so as I say, I'm happy to take questions throughout the talk. So please interrupt me if anything isn't clear at any time. Okay, so let's start with, you know, before we go into relative tests with multiple models, let's just look at a single model. Let's say we've got a model P, we've got data drawn from Q, AID, and we're going to work with the assumption that all models are wrong. So we're not going to be particularly interested in the case that the model and data match. The case that the model and data match, but we're only going to be looking at the behavior when they don't. Okay, so let's start by looking at the integral probability metric that we're going to be using as the basis of our model comparison. So at a high level, an integral probability metric looks for a function which is a well-behaved function in some sense that maximizes the difference in expectations under the two things we're interested in. So here, Q and P are both samples. So here, Q and P are both samples in this case. And we're looking for a well-behaved function that maximizes the difference in expectation under P and Q. Okay. And you can sort of see that this gives a nice, I guess, hierarchy of levels of departure from P being equal to Q. So in this case, with the data we've seen here, because the function is well behaved, we're saying that Q and P are closer together in some sense, or in the sense described by this IPM. Described by this IPM compared with this case where P and Q are further apart. And far apart is sort of determined by the way in which we've required our F to be well behaved. So an example of an IPM, for instance, is the Wasserstein distance, where the Ellips norm is bounded. We're going to be interested here in using a reproducing kernel-Hilbert space, and that's indeed the class of functions that I'm using in this illustration. In this illustration. So, just to tell you what that is, a reproducing kernel-Hilbert space is a Hilbert space of functions which are linear combinations of features. So, that's pretty straightforward. So we've got here a linear combination of features. You can see that these features in this illustration become less smooth as we go up the dictionary features. And the norm of this function is the sum of the squared coefficients that define it. So, the key point here is that we might be allowed. The point here is that we might be allowed to have an infinite feature dictionary, and indeed that's the case that we're going to be interested in. So, how do we express these functions of infinitely many features? We don't want to have to write down infinitely many coefficients. But in the event that these coefficients are a linear combination of finitely many features, so which is to say, you know, a linear combination of features of finitely many points, then we can express these functions as linear combinations of. These functions as linear combinations of kernels, where the kernels are the dot products of the features. So, basically, you know, as long as our coefficients that define our functions can be expressed as finite combinations of features of individual samples, then our function is a finite combination of kernels, which are dot products between features. I'm illustrating that here with a case of the exponentiated quadratic kernel where I've got these red bumps here. These are basically kernels centered at these x i's. Basically, kernels centered at these xi's, and the green function is what I'm getting out. So, this exponentiated quadratic kernel, by the way, is the kernel that gives these feature dictionaries here, which are Hermite polynomials with a decaying envelope. All right. So this is the class of well-behaved functions that we're going to be using to define our integral probability metric, which will distinguish our P and our Q. So here's a nice example. So here's a nice example. Here, P as a Gaussian mean zero, Q as a Gaussian mean one. You can see this is a function that maximizes a difference in expectations. And you can sort of see that it becomes a little bit more positive where there's red, it becomes negative where there's blue. And this is a smooth function, which means that E P of F minus EQ of F is large. So you can see that that's a nice way of distinguishing P and Q here based on this well-behaved function. Behave function. Okay, so that's all well and good. What I want to make sure is that this thing is zero only when p and q agree. For that, we need a rich enough dictionary of features. The reproducing kernel-Hubert spaces that are based on the exponentiated quadratic kernel, so the Gaussian kernel, are rich enough. We're also going to be using the inverse multi-quadric kernel later in the talk. That's a rich enough class of functions. But as I've mentioned earlier, But as I've mentioned earlier, there are other classes of functions that one can use to define IPMs, such as the Wasserstein distance, which is based on the one-Lipschitz function. All right. So this is a measure of difference between distributions, but it may be a little challenging to compute this if we're comparing samples Q from a model P. The challenge arises because we The challenge arises because we can't necessarily compute the expectation under our model of our function in closed form. We may not have a normalizable model, for instance. And in that case, this might be a challenge. So there are ways to address this. One thing is, well, even if I can't normalize my model, I might still be able to sample from it using Markov-Chain Monte Carlo. That's perfectly valid. But, you know, But this is a little bit, I think, missing the point in many cases, because if I do that, I'm not taking advantage of what I know about my model. I've got a lot of information about our model structure, and maybe I should actually take advantage of that and use that to do better than I could just by sampling from it. And that's going to be actually what we're going to look into. All right. So let's say we don't want to sample from our model. We want to kind of Our model. We want to compute an integral probability metric or something derived from it without sampling from the model. And we're going to, you know, I emphasize, we don't want to have to assume that we're able to normalize our model. So to get rid of this EP of F, I'm going to use the non-German stein operator, which I think everyone here is extremely familiar with. In 1D, I can write it as one on P d dx of f times p. You can see that I don't need to. You can see that I don't need to be able to normalize p to compute this because the normalizing constant cancels. And rather straightforwardly, if I take the expectation under p of this, it's going to be zero subject to appropriate boundary conditions. So if I add this operation to my function p, its mean is going to be zero when I take the expectation. Okay, so now let's bug that in, right? So rather than using an integral problem. Right, so rather than using an integral probability metric based just on a function, I'm going to apply this sign operator to my function. I'm going to get rid of this second expectation, thanks to the property of the sign operator, and then I'm going to just be left with, you know, I'm looking for a function in a well-behaved class of functions that maximizes EQ of AP of G. So let's look at what this looks like. Here's our two Gaussians again. Here's our G that maximizes this. That maximizes this. And what you can see is that q, you know, as q approaches p, this g function approaches zero. And if q and p agree, then this g function is going to be perfectly flat. Okay, so this is our kernel stein discrepancy. So basically I'm using my kernel class of functions, but I'm going to apply this stein operator to these functions so that their expectation is zero under p, and I can get rid of this model expectation. Model expectation. All right. So a question that arises, which comes in the title of this slide here, is, can I still compute this expression straightforwardly just using kernel functions, either expectations of kernels or sums of kernels? So basically, can I compute a simple test statistic without having to worry about the fact that I might be defining my functions in terms of infinitely many features? My functions in terms of infinitely many features? And the answer is yes, but it takes a little bit of algebra to get there. Okay, so let's first of all start by rewriting the Stein operator in this classic form here. So I'm rewriting it as f d dx of p plus d dx of f. And I want to basically be able to write my Stein operator applied to my function in my reproducing kernel-Hilbert space as the dot product of the coefficients that define. Of the coefficients that define my function and these stein features, where the expectation of these stein features under p should be zero. So, just as I wrote my function of x as a dot product of coefficients and features of x, I want to express my stein operator defined on my function of x as a dot product of f and stein features of x. So basically, I need a feature dictionary expression for my f when I've applied the standard radar. My f when I've applied this time operator. So let's see how we're going to do that. Basically, the main trick to use is that the derivative of f can be written as the coefficients f and the derivative of the features. So this is a rather straightforward operation. And in particular, the derivative of the kernel can be written as the dot product of the features and the derivative of the features. So if I'm taking the derivative with respect to my first argument of my kernel, I can just take the derivative of these features. I can just take the derivative of these features and take the dot product with these other features. So basically, you know, the sort of punchline of this is that even with infinite feature dictionaries, a dot product is linear and I can take the derivative inside the dot product. So using that fact, my Stein operator applied to my function is, you know, d dx of p times f plus d dx of f. Well, I know that f at x is a dot product of f and the features of x. And then I know that the derivative of f. And then I know that the derivative of f can be written as the dot product of the coefficients and the derivative of my features. And so that is my dictionary of Stein features. It's basically derivative of p times features plus derivative of the features. And that gives me these signs here. All right. So I've defined my stein features. And as you can see by their form, the expectation on the p of these stein features is zero, as we've required. We've required. So now let's compute in closed form our kernel stand discrepancy using those stein features. Okay, so here is our KSD. I'm going to assume I've got two independent samples from Q, which are the data that I'm comparing my model to. So X and X prime are two independent samples from Q. So here is our expression for the kernel sign discrepancy from a couple of slides ago. I'm looking. From a couple of slides ago, and looking for basically a smooth, a well-behaved function which maximizes this expectation EQ of the stein operator applied to g on x. All right, now first step, use the result from the previous slide. I can express my function g of x as the dot product of the coefficients that define it and these sign features. Okay, next step, and here I have to be a little bit careful. And here I have to be a little bit careful. I'm going to take my expectation inside my dot product. Now, this in infinite feature space does require a little bit of caution. In particular, I need to make sure that this expectation is finite to do that. The expectation of the squared or the expectation of the squared d dx of p has to be finite. But as long as this is true and we're going to return to that condition, I can take my expectation inside my dot product, even though it's a dot product. Dot product, even though it's a dot product over infinitely many features. And then I have a rather elementary operation. I'm looking for a unit vector which maximizes this dot product. Well, that's just going to be the unit vector in the same direction as this vector here, which means that I just get the norm of my stein features as my statistic. Okay. So, you know, what does this function look like, this witness function of my stein discrepancy? Function of my stein discrepancy. Well, in my crime data, my witness function, which is sort of given as a gray heat map here, is going to have its largest amplitude where there's a mismatch between my density, my model, and my data. And so it's going to reveal that indeed, you know, there is a mismatch along Lake Chicago here because there's no pirates in Chicago, so there's no private in the lake. And so you're kind of using this witness function g as a way of seeing where it is that my data. Way of seeing where it is that my data and my model disagree. Okay, now I said that I would give a little warning about this condition here. I can't just assume that it holds. So here is a counterexample. Let's say that my p, my model is just a standard normal. So d dx.p is minus x. Let's say that my data come from a Cauchy distribution. Then I'm requiring that. Then I'm requiring that the expectation of x squared under my Cauchy should be finite, but it's undefined. So, here is an example of a case where the condition fails. So, be careful if you've got a P which doesn't have an integral under my Q, basically. Okay, so given that we know from here, From here, that our statistic is just the norm of the expected sign feature, I can compute that in terms of kernels. All right. So here is the expression. Remember that x and x prime are independent samples wrong from q. Hp is going to be given by this expression here. In this expression, I'm looking at the case of the Stein operator in D dimensions, capital D dimensions, rather than in 1D. capital D dimensions rather than in 1D. So my Stein operator is this expression here. These k1s and k2s are just the vectors of derivatives of my kernels with respect to their first and second argument. And then K12 is going to be the derivative with respect to the first and the second argument. So this is going to be a D by D matrix. I'm just going to take the trace of it. Okay, so the key point here is everything can be computed with constant. Everything can be computed with kernels or derivatives of kernels, so I don't have to worry about the fact that I've got an infinite feature space. And, you know, also importantly, I don't have to be able to normalize my model because I've got the derivative of my P and my P as my Steinoff radio. Okay. Now, under what circumstances is quantity zero only when my model and data agree? Well, as long as I use a rich enough feature dictionary, so that's to say my Our feature dictionary. So that's to say my kernel is C0 universal. And as long as this expectation here is finite, then my kernel stand discrepancy is zero only when P and Q agree. Okay, so basically expectation of Q of the derivative log P on Q norm has to be finite for this to work out. Okay, are there any questions so far, by the way? Because I feel I've talked for quite a while without pausing for questions. Pausing for questions. I have a question. So, is there a simple way to program the KST by the MND or the MMD by the KST spell? Right. So, KST and MMD are sort of, you know, they are quite fundamentally different objects. So, I can't bound one by the other. Yeah. Yep, this is the short answer. Yeah. Yeah, and in particular, you know, the MMD doesn't require conditions like this to ensure that the MMD is zero only when team degree. So, like, there is when we introduce this Stein operator in the definition of the IPM, we're sort of complicating matters in a way that is a little bit, you know, makes it difficult to sort of bound one by the other. Yeah. Any other questions? Any other questions? Okay, I'll continue. So, in the examples that we're going to talk about later, I'm also going to be talking about model evaluation for models with outputs or X's that are in a dispute domain. So, for example, topic models generate distributions over words. So, plainly, Words. So plainly, words are discrete-valued, and so I need to define a sign operator and a sign discrepancy in the event that we've got a discrete domain. Okay, so let's say that we've got a discrete domain vocabulary size capital L and there are D letters or D dimensions. In that case, we have a Stein discrepancy which looks extremely similar, as you might expect, to the Stein discrepancy on the previous slide. The sign discrepancy on the previous slide, where rather than using a derivative, we use a difference operator. So, this difference operator basically takes each of our letters and increments the letter by one. So, if I've got an A, it goes to a B, B, it goes to a C, and so on, but it's cyclic, okay? And then this inverse one decrements the letter by one, all right? What kernel do we use? Well, in this. What kernel do we use? Well, in this case, I'm going to use to stay Hamming distance kernel, which basically looks at the number of mismatches. So, this is the kernel that we're going to use here. Yeah. And under some circumstances, we can also ensure that this is zero only when P and Q agree. We need that P and Q are supported, like they have support over the entire domain. And we need that the gram matrix, the kernel matrix. That the gram matrix, the kernel matrix, is strictly positive definite over the domain. So, if those two properties are true, this lack of rank deficiency basically ensures that you can always distinguish distributions, then you've got a discrepancy for discrete domains as well. Okay. So, yeah, so we've defined our kernel stein discrepancy. Now, let's say that we're going to compute That we're going to compute our standard discrepancy from data, and because we're doing a statistical test, we need to look at the distribution of our statistic when, you know, rather than having a population expectation, we've got a finite sample expectation. So remember in our population definition that we required x and x prime to be independently drawn according to q. What that means is that our finite sample estimate is going to be a u statistic, which is what I'd expressed here. Which is what I'd expressed here. And in the event that P and Q disagree, which they always do because all models are wrong, this u statistic has an asymptotically normal distribution, which I've shown here. So it's Gaussian distributed around its mean. The mean is not zero because P and Q disagree because all models are wrong. Okay. In the event, by the way, that P and Q are the same, we don't have a normal distribution anymore. We don't have a normal distribution anymore because the statistic becomes degenerate. So then the asymptotic distribution is an infinite sum of chi-squared, so it's a little bit messier. But we won't require that for this talk. Okay, so remember, you know, all models are wrong. So we're interested in a relative test of goodness of fit, where on one hand, we've got the model P, on the other hand, we've got our model Q, we've got our data R, and we're interested in knowing which of P and Q are closer. And q are closer. And so, what we're going to do is to have a hypothesis test where we compare the KSD from P to R with the KSD from Q to R. The null is that P is less than Q, the alternative is that P is greater than Q. So, null is that P is better. The alternative is that Q is better. And this is the hypothesis we're going to test. Now, because we've assumed that all models are wrong, we have a very simple joint. Have a very simple joint distribution of the distance between P and its population, Q and its population. Basically, these are going to be jointly asymptotically normal, which means that if our test statistic is the difference between the discrepancies, then the distribution of that difference is also asymptotically normal. And a statistical test is going to be pretty straightforward. So, yeah. Okay, so yeah, so a relative test is a pretty elementary thing to do in this case. Right. Any questions? I guess, so why not the reverse thing? Like the case is not symmetric, so. Right. I mean, so in a way, like, you know, the ordering, you kind of have to ask the question to get the ordering. So it's like, if your question, if I'm asking the question, is P better than Q? Better than q, then that determines the ordering of my difference. So, in a way, like you know, you need to sort of, let's say, construct a hypothesis in order to test it. And so, you have to decide on which of the two you think is the better one. But I mean, you're using the case D or R with respect to P or Q? Oh, oh, sorry, sorry. Yeah, so R, just as a recollection, P and Q. Recollection, P and Q are my two competing models, but R is my data. Right? So I'm always doing P to R and Q to R. Have I understood your question or have I? Yeah, yeah. Okay. So we've, you know, now defined our statistic. We've introduced the notion of relative testing. But we're interested in But we're interested in defining a test for latent variable models. This is going to, in fact, add a considerable amount of complexity to our discussion. Okay, so in our latent variable models, we've got some z which is hidden, and our x given z is given by this integral. And this is going to be the case both for p and for q. Okay, now remember what our standard operator looks like. Now, remember what our standoff rate looks like. It's in this form here: f delta p on p plus delta f, right? Now, the issue here is that I need to know p of x up to normalization. But for many complicated models, I don't have that in closed form. There's a few exceptions, but by and large, you know, I can't do this integral in closed form, even up to normalization. So I seem to have run into a little problem here. And, you know, this problem. Problem here. And this problem is all the more severe because if I'm a Bayesian, almost every model that I'm interested in is going to be a complex related variable model. So, in a way, without being able to deal with this case, I've ruled out, in fact, the vast majority of interesting use cases for my statistic. Okay, so a few years ago, we proposed this as our approach. This is a terrible idea and it doesn't work. It doesn't work. So let's. I actually presented this, I think, in ICML back in the day. So apologies to anyone who was in the audience at that time because this is awful. What I might want to do is say, well, let's just sample from my Z, and then let's do this integral numerically. Okay. And I've then got the numerical integral here, the numerical integral here. I plug it in, I get my discrepancy, and I'm done. All right. Doesn't work. Doesn't work. Okay, basically, the issue becomes that I've got this ratio of derivative p on p, and this is very numerically unstable. So basically, in almost all interesting cases, this is going to have really catastrophic variants, and the test is going to have very poor, very low power. And without either sampling a gigantic amount of these samples and also being And also, being, I think, in a very simple scenario, this is just not going to work. So, even something as simple as, let's say, probabilistic PCA is just going to fail in this case. So, don't do that. Yeah, we actually, yeah, so this is a bad idea. Here is an idea that works a little better and it uses a really nice trick. Okay, so what we're going to use is the fact that the score can be written as the x. fact that the score can be written as the expectation over the posterior of z given x of the conditional score of x given z. Okay, so the proof is actually very simple. So I'm actually going to show you, so this isn't a mystery, s is going to be derivative of p on p and I'm going to have one on p the integral of derivative p of x given z dpz. All right. First step, divide and divide and multiply by p of x given z okay p of x given z on the numerator p of x given z on the denominator and i've taken the p of x inside here and now i've just got my my base rule here p of x given z p z on px so i've got the expectation under my posterior of sp of x given z okay um so this is the conditional score um which in most interesting cases i can compute up to normalization all right All right. So, how do I then estimate my score function? I draw z from the posterior given xi, and I, you know, I do this integral. So now we no longer have this catastrophic thing where I've got a ratio of two quantities that I'm computing from sampling. I just take my z from my posterior using my favorite MCMC method. Then I compute this expectation for each of xi. So is that? Right. So, is that clear to everybody? Because this is key to making everything work. Without this, everything breaks. Great. Okay. So I'm just going to give a bit of foreshadowing here. There's a few parameters here I need to care about. One is how long do I do burn in? The other is how many samples do I take? These are very delicate questions, and we're going to have a look empirically at. To have a look empirically at some answers to those questions later on in the talk. But keep these in your head as things we have to be a bit careful about. Okay, now let's say that we've got our latent variable models and we want to now compute our kernel stein discrepancy for these latent variable models. So here is our stein discrepancy. Remember, it's this integral of this HP quantity. It's a u statistic. This should be a double sum. So sorry about the typo. Uh, so sorry about the typo. Um, now, uh, what we're going to do is to replace this HP statistic with our statistic I'm going to write as h bar p, where h bar p is the Stein kernel, where we've replaced our score with this approximate score, where we've used Markov-Chain Monte Carlo over the posterior of the zi's for each of the x i's. All right, um, so this is our new statistic using an approximate. This is our new statistic using an approximate Steinkernel. And once again, we're interested in a relative goodness of fit test, right? Where the null is that p is better than q, the alternative is p is worse than q. And what we're going to do is to use as our statistic the difference in the stein discrepancies, you know, where I've computed my stein discrepancy using Markov-Chain Monte Carlo with T steps of Bernin and M samples. Okay. Okay, so you know, once again, I need to decide on a threshold for this statistic where I will accept the null if I'm less than that threshold and reject the null when I'm over that threshold. For this, I need the asymptotic behavior of my statistic when I've done Markov chain Monte Carlo to estimate or to approximate my Stein kernel. All right, so lucky for us, the Lucky for us, the Stein statistic, when I've used Markov-Ti and Monte Carlo to approximate it, is also asymptotically normal, okay, with some mean, which is the difference in the population stand discrepancies under P and Q, and a variance, which is going to be a function of our, well, a variance of our statistic. Now, one has to be a little bit careful here. Has to be a little bit careful here. In particular, I need to make sure that I've done enough steps of burn-in that this quantity here is small. In particular, when I multiply it by root n, I need to have done enough burn-in that this thing is vanished. Okay, now, this is something that I, you know, unfortunately, the study of this quantity as a function of how much burn-in I've done is a very challenging topic. I think it's a topic that even in the last year, In the last year, there's been research on that. So, I don't have a simple answer to guarantee that this is true. This is something that, as a practitioner, I just need to be a bit careful about. I also need, you know, because I've got a central limit theorem, that there's going to be the usual conditions on moments, higher order moments, that these are finite. So, subject to these conditions, once again, I have asked. Once again, I have asymptotic normality. Okay. But you know, we're going to look at this Burnin issue a little bit later in the talk. All right. So, you know, this is our statistic. The level alpha test, I'm going to, because I've got asymptotic normality, I'm going to be able to reject it when I exceed a threshold. The threshold is going to require an empirical estimate of the variance. Of the variance, which I'm going to get by jackknife. So I do that, I get my test. All right. Any questions before we go on to experiments? Okay, good. Okay, so let's look at some experiments. I'm going to start with probabilistic PCA. The reason for starting with this is that I'm able to compare in this case the In this case, the relative test with a closed-form estimate against the relative test where I'm using Markovte and Monte Carlo to estimate, to sample over the latent state. So I've got a low-dimensional or 10-dimensional normal as my latent, and then my X's are this latent multiplied by some matrix A with unit covariance. Okay, so this is probabilistic PCA. PCA. Now, this is my data. With my models, what I'm going to do is basically I'm going to perturb this A by E11. So this is a matrix with A1 in the first entry and zeros everywhere else. So it's like one, zero, zero, zero, zero, zero, zero. Okay. And the delta is going to tell me how far I've moved away from my ground truth. Okay. Now in this example, I'm going to perturb p by two. To perturb p by two, so model p is worse than one, so we're under the alternative. Um, we're going to use the inverse multi-quadric kernel. Um, so this is for various reasons a good kernel for this setting. So I think Lester has got quite exhaustive analysis of why that might be. The sampler we're going to use is a no-U-turn Hamiltonian Monte Carlo sampler on the latent space, which is a very good sampler. Now, here, what we're doing is we're comparing the rejection rate of the maximum mean discrepancy against the rejection rate of the kernel stein discrepancy, both when we've done the interval in closed form and when we've sampled using this no-U-turn sampler with 500 steps after 200 steps of burn-in. Two things you see here. First of all, the KSD and the KSD with latent sampling are indistinguishable in this case. So, you know, for this number of Uh, for this number of samples and this number of burn-in, uh, we've basically satisfied all of the requirements we need to satisfy for this to work out. Um, second thing to notice is that we're vastly better than a maximum mean discrepancy. Okay, and in a way, this is not surprising because we've got a low-dimensional latent structure, but a very high-dimensional observation. The Stein operator is allowing us to take advantage of the fact that we know something about our model. Something about our model. We know that it has low-dimensional structure. And so, in that way, we're basically like solving the easier low-dimensional problem that we should be solving rather than the difficult problem of comparing samples in very high dimensions, where the difference between the distributions is very subtle. Okay, so here is, I think, a very nice demonstration of how well we performed. How well we perform. The point I also want to emphasize here is that the key to making this work is using this Markov-Chain-Monte Carlo approach in getting the stand discrepancy. So if we use the ratio of expectations with samples from the latents, which we did in our earlier attempts, this didn't work very well at all. All right, so this is in the KSD, MRV fails terribly. ASD, MRB fails terribly. So now, you know, that was a nice toy example. Now let's look at a more interesting example, which is a topic model. So this is a less trivial example of a Bayesian model. In this case, we have data from archive articles from stat theory, and we've got two models, which are latent direct layer allocations, or LDA models trained on math probability theory and stat method. Math probability theory and stat methodology. So in this case, we might expect that Q is a little bit closer than P in this example. So just a reminder of what latent Dirichlet allocation is doing, you're basically sampling a theta, a distribution over topics from a Dirichlet distribution. You've got beta, which are distributions of a word per topic. For each word in the document, so there's n words per document. You sample a topic and then you sample a word from that topic. Topic and then you sample a word from that topic. Okay, and there's M capital M topics. So, you know, in this case, we're in a discrete setting. So we're going to be using the discrete Stein discrepancy. Okay. So in this example, there are basically 100 possible topics in our LDA model. We're going to just We're going to just sub-sample our documents to 100 words. And so, and there are L words, L possible words in our document. We're sub-sampling because we're using a bag of words representation. So we're just sort of ensuring that our model is tractable. We're going to use an IMQ kernel again on the bag of words representation. And for the case of the KSD, we're going to run Macovchin Monte Carlo 5,000. Run back of China Monte Carlo 5000 iterations after 500 steps of run in. And here, you know, here's our rejection rate for KSD as a function of how many samples we look at and how many samples we observe from R. And as you can see, we're very decisively rejecting the null in this case compared with MMD, which is doing very poorly. Okay. Okay, so that seems to be a pretty unequivocal victory for MMD. So now let's look at an even easier problem where we compare not against math and stat, but stat and computer science. So computer science is way far away from statistics. So this should be like a shoe-in, right? But oh no, it works. But oh no, it works catastrophically badly. So MMD finds it extremely easy, right? It's right up here, it's rejecting all the time after very few samples. And KSD is doing terribly. So even after 300 observations, I'm barely above chance level. So this seems unfortunate because I've taken a problem and I've proposed an even easier example. Even easier example. And now it's doing extremely poorly. So, you know, what myth happened here? The issue is that the Stein operator that we've defined is a function of a ratio of p of x plus one. So that's, you know, basically I'm incrementing my letters by one on p of x. Okay, so an issue is that. So, an issue is that if we've trained our LDA model on computer science, but we're evaluating it on stats, there are going to be stats words that happen almost never in our CS data. So basically, P of x is going to be very tiny for these words. And so this ratio is going to explode. And so my variance of my statistic is going to be huge. So is that problem clear to everybody? So basically, I'm not allowed to sort of find myself in a scenario where P of X is vanishing for particular words and my SP blows up. But if I'm in the scenario where I've got stats documents, but I'm evaluating them on a CS model, that is indeed what's going to happen. And so my Stein discrepancy is going to be a vector of these things. Discrepancy is going to be a vector of these things for each of the words in my documents. And so, the more of these things blow up, the more my variance is going to blow up. And so, the worse my test is going to perform because I just, you know, I've got such a huge uncertainty over my statistic. So, this is kind of interesting. So, it's not just a case of like, I'm absolutely certain that my Stein discrepancy is going to do better. I need to be a little bit judicious in the, you know. In the uh, you know, in taking care that my stand operators are well defined for the problems that I'm looking at, and where I've been careless, for instance, in this case, I'm going to get a test with very low power. So in this example, I'd be much better off using MMD because MMD is going to see right away that there are words in CS that never appear in R. And so I'm going to reject the null trivially and immediately. Okay. Is that clear? Are there any questions about that? Okay. So I hinted earlier on that we needed to take care in the design of our Markov chain Monte Carlo sampler and that the number of steps of burn-in and the number of samples that we take are going to influence the quality of the tests that we get. Of the tests that we get. So, unfortunately, it's a little challenging to actually figure out rigorous theory, except for in some very toy cases, for these quantities. But what we're going to do is to look at the practical behavior and get some sense of how to set these parameters as practitioners. Okay, so remember that we required, you know, as a very first step that the As a very first step, that the bias induced by the number of steps of burn-in has to vanish. And the amount by which it needs to vanish is going to depend on n, right? So I've got root n times this should go to zero. So if that doesn't happen, then the theory breaks. You don't have asymptotic normality anymore. So this is already a point at which you have to be careful. Unfortunately, we can't. Unfortunately, we can't sort of, except in some toy cases, figure out how quickly to make T grow with N. So, this is just a sort of warning sign that one has to be cautious of. Okay, now if you have a poor sampler, then that can cause you to reject the null just by dint of the sampler being poor. So, let's say, for example, that we've got Let's say, for example, that we've got exactly, you know, P and Q are exactly the same model, but we sample from P using a bad sampler and we sample from Q using a good sampler. In that case, you will have a mismatch between P and Q just because the samples from one are very far from where they ought to be. And so we've got a, you know, we have a mismatch, but it's in a sense a valid mismatch just because the expectation of one of our statistics is being taken. Statistics is being taken with Z's, which are sort of confined to only part of the space that they should be because of this poor mixing. So here is the example where we've drawn P using MALA and Q using this no-U-turn sampler. And basically, once we've, there are two things that we're doing here. One is that we're increasing the burn-in size, and the other is In size. And the other is that we're increasing the number of samples that we've taken after burn-in. So you can see two things. First of all, we should be rejecting at chance, okay? So as you increase the number of burn-in steps, eventually you are rejecting from chance. Even, yeah, we're rejecting from chance, you know, regardless of how many samples we take. But basically, like the number of samples after burn-in mean. Samples after burn-in mean that we reach that chance level sooner. So, if I've taken a thousand samples after burn-in, then we're rejecting at the right level after fewer steps of burn-in because we've just run our sampler for longer. Yep. So, you know, basically you have to burn in for long enough and you have to run your sampler for long enough to get the right level in this example and not reject just because one sample is worse than the other sampler. Than the other sample. Now, this is in the case where we're comparing with a reference from which we've just drawn 100 samples. Okay, what happens if we draw 300 samples? Well, if we've drawn 300 samples from our reference, then we are much more sensitive to differences in P and Q because we've got a greater number of samples from R. So, in that case, for the number of samples, For the number of samples that we've drawn and the number of steps that we've burned in, we're actually never rejecting at chance level. So basically, as N grows, we need to burn in for longer, and we need to make sure our sample is mixed to avoid this pathology where we're basically finding differences in the models just because our sample is no good. So, yeah, the sort of the The sort of take-home message here is that you have to be careful about your sampler design, and you have to be careful about taking sufficiently many steps of burn-in to avoid artifacts that you get from poor choice of either of these quantities. Okay. So, yeah, so I think that's most of the things I wanted to talk about today. I wanted to talk about today. The two references that I am referring to here are, on one hand, the kernel test of goodness of fit. So, this is the test in the event that you don't have latent variables. And then this kernel-stein test for converting latent variable models, which is the case that I've described in the second half of this talk. So we're going to be shortly putting up a revised version of this. So the version that's online now is actually very much behind. Very much behind what we're currently doing. So, I advise you not to read it yet, but to read it when we put out the new version. So, yeah, I'm happy to take further questions. Thank you for the great questions. I I have a speech but we can't hear I'm not sure I if you're asking something I can't hear you easily you will ask now just one second I think by the way yeah Gezina also has a question or her hand is raised but uh sorry go on so uh So, is it a problem that you mentioned for this discrete kernel style discrepancy? Many because of this ratio between Px plus 1 and 1? Right, yep, yep, absolutely. I was supposing that if you could replace that, I mean, replace that style operator with some, you know, other versions of it, if you can remove this ratio, then it would be fine. Yep, yeah, I agree. So, I think. I agree. So I think, you know, this is something actually we only discovered in the last couple of weeks when we were running experiments and we were trying to figure out why we were doing so badly. So absolutely. Like, I think one needs to think about stein operators for this discrete case that are better suited to this problem. And it may then be that you get much better power. So, you know, I would welcome suggestions here. So we're just using, obviously, it's further back in the. It's further back in the talk. This sign operator that has been proposed for text, but I believe that there may be better dispute stand operators for the setting we're considering. We have some suggestions. Oh, can you send them? Please send them to me. I would really love to hear them. Yeah. Perhaps Gesina, would you like to ask? Okay, thank you. Yes, actually. Yes, actually, just a side remark: there are many, many discrete operators which one could choose, and one doesn't have to make it circular, which might be another problem. Yep, yep. If you do the Reg Young construction, make it wrap around, sometimes it's quite unnatural. Okay, no, but again, you know, I welcome suggestions. So please email me with suggestions for improved operators. As I say, we found this so late in the Say, like, you know, we found this so late in the day that we didn't have time to investigate deeply alternatives. But, you know, part of the benefit of this workshop is that I can receive helpful suggestions about how to do this better. So I really welcome any ideas. Yeah. Yeah. So my real question is about the estimated KSD. If you go back, because it's so it's really it's estimated by bootstrap samples, right? So this is a no, well, maybe without. Well, maybe without the relative KSD. So, even just the original one. So, let's not think about MCMC samples because that makes it more complicated. Yeah, that one. Yep. So, for this empirical statistic, this is something that classic Stein people love to look at, right? Look at bound on the distance to normality in terms of the parameters. Right. Yep. And so I think it might be quite possible. So, I think it might be quite possible to work something out here, which gives you something about how the variable convergence depends on N. Right, like to use to trade off with the G later. Once you know how good your NCMC is, yeah, no, that I think would be very interesting. Um, let me just, uh, I've uh misjudged my earphones. Um, yeah, no, I think. Um, yeah, no, I think that would be very interesting. Um, I think, uh, yeah, um, so one, yeah, you're sort of proposing like there's various scene results that one can use to sort of determine how far from normality as a function of n and then to look at, is that right? Well, yeah, you might not necessarily want to have various scenes, so whatever distance you have, which agrees with what you can analyze for your MCMC method. Got you. Yeah, I think a challenge. So, yeah, I think a challenge that we found is that the burn-in distance and sort of how quickly burn-in happens seems, as far as we're able to tell from the stats literature, to be very challenging to prove things about. So, I think that the challenge perhaps might be around that issue of finding how quickly MCMC methods are mixing. How quickly MCMC methods are mixing when we're trying to sort of make sure that this bias isn't too severe. Yeah. Yeah, I think for some MCMC methods, there are some results which give you the mixing rate, but these might not be the best samplers, of course. Right. Yeah, well, that's the other thing. Unfortunately, you know, we really have to use good samplers because if we don't, we have very poor performance. So that's something I really have to underline. Like this, no U-turn sampler was essential in making this thing. Sampler was essential in making this thing work. Yeah. And you know, even like a Mala sampler, which is reasonably well designed, already wasn't good enough. Yeah. Okay. Thank you. Cool. Thanks. So you know that MMD is not Minimax optimal in the classic settings. So I think they make it minimax optimal. Is something similar possible to do? Is it possible to do something similar? Possible is it possible to do something stolen from ASP? Right. So, when we're talking about minimax optimal, are you referring to these adaptive tests that are minimax optimal against SOVALIF classes? Is that right? Right. Yeah. Oh, sir, could you maybe go closer to the microphone? I'm having trouble hearing you. Yeah, he didn't adapt to once all the regular response either, yeah. Regularized ones either, yeah. Oh, sorry, could you repeat that, please? Either the adaptive ones or the regularized tests, right? So, um, the tests that I'm aware of for the MMD, which is minimax optimal against solo alternatives, is an adaptive test, which is basically using an aggregation procedure to optimize over kernel bandwidth. So, this is you know, Right, so this is for MMD, right? Now, for KSD, we have an aggregation test as well, but we don't have minimax optimality. So that is an open question. I would be very interested to explore that, but we don't have an answer. Yeah. Thank you. I have a question. So, when you say the performance of the MCFC method better, does it mean that the Is it how how does the bias come into this picture? Is it that the fast mixing is important or the bias of the sampler is more important? I would say both in the sense that what we want is, you know, think about what we're doing here is we're computing these expectations over the posterior of z. Now, if our sample is mixed poorly and z is confined to some part of its space, then plainly you're not getting a good estimate of this posterior expectation. Expectation. Likewise, if you've mixed poorly, sorry, if you're not burned in sufficiently, then obviously your Z given X isn't the stationary distribution of the Markov chain. So again, you're in trouble. So we've sort of done it by eyeball. We've made sure that our mixing has happened sufficiently and that we've run our samples for long enough to sort of. And that we run our sampler long enough to sort of cover the support. What we did find is that the mixing, because no U-turn sampler is very good, the mixing time actually wasn't very important in our case. So basically, we could pretty much ignore it, in fact, and we could just run our sampler more or less with very little burn-in. Sorry, yeah, the burn-in isn't important. Sorry, yeah, the burn-in isn't important for no neutron sampler, it burns in very quickly, so you can, yeah. Um, yeah, but this is obviously a very empirical observation, right? It's not a proof of anything, but it's what we found in our tests. Thank you. There is one more question, Yichen. Okay, thanks, Aka. So, I'm African. So, I'm going to ask a more difficult question. So, let's say your test is definitely working and you will get another C that shows that some maybe say P is a better suit to the data comparing to U. That is what you. Do you really confident about this result? Because, you know, TSE tests tell you that it's going to be, but it's not really the case in practice. Sorry, could you go closer? Sorry, could you go closer to the microphone? I'm having a bit of trouble. Oh, sorry about that. Okay, so let me repeat. So let's assume that you do have problems with, let's say, high variants and your TSD statistic tells you that he is a better face or that's your status. It tells you. Do you think this is really reflecting what is happening in many other places? Because I think this is very, very. Because I think it is very relevant in the sense that at least then case a good objective in terms of, for example, fitting models in. There was a chime that interrupted the question. But I think, so your concern that the KSD is somehow missing some matches between the The model and the missing information about mismatch, is that right? Yes, something like that. So you can really have that you're going to fit the ground two data distribution with student tree and your P and P rules are actually one of the five. Then you can tell you where these things P or Q is a better bit or not, but is the state parameter. Is the state parameter is really close to the graph parameter. So, is the question to do with the fact that you're fitting both models from data? Is that have I understood? Oh, sorry, did you answer? I think you've moved away from the microphone again. Yeah, maybe Ichen can type the question in the chat. Yeah, if you type in the chat, yeah, I think that your microphone is. I think that your microphone is acting up a little bit. So it keeps, you know, if, in a sense, the question is, I mean, I can answer, I think, what I think the question may be related to, which is that, you know, indeed, we have the issue that our models are fit from data. So two things arise. First of all, we can't fit the models using the same data we test on because that will create correlations that are bad and will Are bad and will mislead us. So we have to obviously fit the model on HELDAR data. But I guess the other point is that we do need to, let's say, make sure that we fit the model well. So if we don't fit the model well, it may be that you get a mismatch or you make a decision which is a function of how well you fit your model and not a function of whether the model itself is good or bad, if that makes sense. Good or bad, if that makes sense. But I'm looking at the chat, so if the question appears, I will try to answer. So it's 23 minutes from now. Yeah, yeah, thank you. Yeah, well yeah, thank you for your invitation. No, that's just that we said that, but it didn't work quite now. It's like two hours before     Yeah, I use the microphone. Okay, I think I'm just going to use this one first. So anyway, how did I remote working? I don't think that the clicker actually worked at this point. Just the laser. But I don't think the clicker is actually working for some strange reason. But that's the USB for it. But I don't know why. What I was going to do is to avoid some technology from yesterday, I'm just going to send this thing to this laptop. And then I'm just going to. And then, like, I'm just gonna open the slides here. Is that okay? Yeah, that would work just fine. Okay, so since there's a mark, I've got your hairdrop. Because then your slide will go there for sure, but do you want to use the board as well? Well, most of it is going to be the slide. Okay, perfect. Yeah, I don't know how to say it. I can just leave it like that because they'll see. Like, obviously, they'll see that. Well, and then as soon as you share the slide. And then, as soon as you share the slide, we can just share that screen that will have the slide and it will work by charm. Otherwise, if you want to use your laptop, you can do that as well. I think it's not about the computer, it's about the keynote versus the PDF slides. Oh, yeah. I just want to use this computer. So, what if I just remove the uh I mean I'm I'm I'm trying to airdrop it, so what uh account of this computer? An account of this computer. Can you click on the phone sign? Oh, yeah, just first, that's it. Well, then, if I do this, then I have oh, okay, okay, so Uh Bill.