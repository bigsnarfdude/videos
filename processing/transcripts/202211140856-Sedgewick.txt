Should turn off mirroring, I guess, but I'll do it this way. The title of the talk is I perceive a memory-efficient alternative to high-turn off log. Seeing my dirty laundry here. No, anyone through that one? Sorry for the slow start. There we go. Okay. Okay, let's see if this thing works. Yes. Uh okay, so that's my title. Uh start recording. I did but it's not Okay, so uh that's my title. Uh Hyperbit, a memory efficient alternative to hyperlog log. And the title is chosen wisely because anyone searching for hyperlog log is going to find this. If you leave hyperlog log out of the title, nobody will find it. It's joint work with Jeremy and Svante, and I label it as a work in progress. What that means is that Svante needs That means is that Svante needs to check the math, and Jeremy needs to check the code. And I haven't, neither one of them has really seen this yet. And it's kind of a long story. Some of you have heard some of this story before in my talk in Poland, the Flagellais lecture. And so that's where this work kind of started for me. So this will be kind of a half survey and half new stuff, and it's a work in progress. And it's a work in progress, and this is a workshop, so that's where we are. What's wrong with it? So, as all my talks are dedicated to Philippe, and this problem really is one that originated with Philippe in the 80s and really carries his spirit really throughout. So, I have to start with a little bit of background, a little bit of context. The first thing is this idea of algorithms. The first thing is this idea of algorithm science. I wish I had thought of this term like 40 years ago, but I've only been using it, I don't know, for the last five years or so. And it's really what Knuth conceived of and what many of us have been doing. It's like prime objective of our research and writing for many years. Knuth called it analysis of algorithms. And that's a pretty accurate term, but a bigger community has kind of taken it over. But a bigger community's kind of taking it over, and I'll talk about that later. We have our analysis of algorithms meeting, and that's fine. I'm just saying, I wish we had called it algorithm science, I wish CNU text. I think it's a more descriptive term. So the idea is that for most of the stuff we do, the first thing we do is implement the algorithm and run it on realistic input to see if it's going to be of any use in the real world. There's lots of algorithms out there, and we're going to study the ones that are going to be. And we're going to study the ones that are going to be useful. Then, the next step is to develop some kind of mathematical model describing the behavior of the algorithm, and then use that to formulate some sort of hypotheses about how the algorithm is going to perform, and then run some experiments to test it. And that's how we understand how algorithms are going to be effective in the real world. And then we iterate. Maybe there's parameters we can learn about best values of the parameters. We can learn about best values of the parameters and so forth. But it all goes together, and this is like true in many scientific endeavors. This is basically the scientific method applied to the study of algorithms. And the benefit of this is that for doing this for decades, people coding up, like creating, say, the C class library or Java library, they look to our writings for their code. Writings for their code. And so we have tested and know which algorithms are going to be effective for them and really has played a big part on that software infrastructure. There's drawbacks. One of them is there might not be a good model or it might be unrealistic or it might be too detailed. And the other thing, I know this is challenged to a group like this, is that sometimes the math can be hard or too hard. Can be hard, or too hard, or too hard for us to get things done. So that's a context of algorithm science. I think I grabbed some more. Now I mentioned there's this other thing going on in computer science, and I call that the theory of algorithms. And that started with AHAC Penelman in the 70s and then CLRS. And this is And this is widely caught, and many people are active in this. It's really a mathematical basis for studying algorithms in contrast to a scientific basis. And what they do is they analyze the worst case costs. Then they don't have to worry about the model, really. And they use O notation for upper bounds, so they don't have to do detailed analysis. And with that, then they can classify algorithms by these costs. Upper bounds on the worst case costs in a classify algorithm. Some have quadratic, some have exponential, or n log n or whatever. And the benefit of this is it's kind of a quick and dirty study of algorithm performance for designers. And some people call it an age of algorithm design. But I want to emphasize this is really theoretical because there are big drawings. Because there are big drawbacks to this approach. First one is the analysis is not suitable for scientific studies. If you say an algorithm is big O of n squared, it says very little about what that algorithm is going to actually do in a practical application. And therefore, the algorithms are not implemented and they're often just not useful in the real world. And I put a whole slide on this. And I put a whole slide on this because people often overlook these basic facts. And actually, when I present this slide and talks, I very often get challenged on it. Of course, they tell us about what goes on in the real world. Okay, so that's that. Now, what about analytic cognitorics? So now I'm preaching to the choir. So, just to summarize, the drawbacks of the theory of algorithms approach is: first of all, you're studying. First of all, you're studying the worst case. In the real world, we don't study the worst case that much. Worst case is all the molecules line up and this building collapses and a mountain moves or whatever. We don't go on that basis. And you can't use big O upper bounds. Throwing away the constant using upper bounds, you can't really use that to predict or compare algorithms realistically. The drawbacks of what CNU does is that sometimes it can be much too detailed. Too detailed. And the excessive detail required to really study an algorithm over time has become a bit of a liability. In the 1970s, it was amazing that on the Cray-1, the world's fastest supercomputer, we could develop the world's fastest sort, and we could predict its running time to the microsecond. It was amazing. And there was a lot of math involved, there's a lot of good modeling, there's a lot of good. There's a lot of good modeling. There's a lot of good hacking. But you don't really need to predict to the nearest microsecond in most applications. To the nearest minute or the nearest hour, something is usually okay. So anyway, that's a drawbacks. And really, the origin of analytic combinatorics was to be a mathematical basis for scientific studies that could be useful as a basis for algorithm science. So with analytic combinatorics, So, with analytic connotorics, we get a way to develop models. You have to fool around with the models a lot. And analytic comotorics is a toolbox for developing models. And then we also have the universal laws that let us get by the details of the analysis very often. And then a side benefit, again, in preaching to the choir, is that this is useful in many sciences, not just algorithm science. And we're finding people all across the spectrum using analytic combinatorics effectively. Analytic combinatorics effectively in studying the real world. So that's the context. And I think this context really comes through in this problem that I'm going to talk about. Okay, so the problem is cardinality counting. So you've got a huge stream of data items, and you want to know how many different values are present. And so throughout, I'll use this reference application. You've got a web log of all the people that visited your website, and you want to know how many. And you want to know how many different uses there are. These are people that this is six million strings from a particular day in, I don't know, 2015. So that's my reference application. How many different strings are there in that? Now, this problem, there's lots of useful applications to that I'll talk about in a second. But the state of the art since the 70s is you just sort. 70s is you just sort and then you just go through and skip the equal ones and count. And that's like a Unix sort. That's a Unix command to count the number of unique visitors in the web log. And it's built into a lot of systems like SQL and other things, sort, then count. So you can go faster by an order of magnitude by just using a hashtag. Magnitude by just using a hash table. The sorting solutions are going to take n log n. The hash table can be linear if you do it right. But the problem is that often the stream is huge compared to the amount of available memory. So I can't fit all the values into memory. How can I possibly count? Well, the answer is you can't count exactly because you have to save them all to really know because the last one might be. Know because the last one might be equal or not equal to what you've seen before. But what you can do is change the problem to be cardinality estimation. So now we want to just estimate the number of different values in the screen. And so I'll just frame this as the practical cardinality estimation problem as the one that we want to talk about. So first thing is you only get to make one pass through the stream. You're a little computer sitting on the big Little computer sitting on the big wide internet and the stuff's flowing through. You want to get to see it once. You want to use as few operations per value as possible. They're streaming through really fast. You don't have much time to process them each. You want to use as little memory as possible. And you want to produce as accurate an estimate as possible. I can quantify these a little bit in a minute, but that's the problem that we're trying to address. And Philippe formulated this in the 80s. I'll get to this. So there's a lot of applications like the one I just said. Actually, one of the original ones had to do with database application, the order in which you do joins in the database, it matters, and you want to do this estimate before wasting a lot of time. And so many, many applications where you don't really need an exact count, but you'd like an approximate count. Be like an approximate count. So, like these days, we think of you're a big commercial operation, like, I don't know, Visa or MasterCard or something. You might have lots and lots of accounts, lots of values, or you're running a network switch or something like that. You've got to think big. So, in terms of quantifying this, we'll talk about 10% accuracy, 99%. 10% accuracy 99% of the time using just thousands of bits of memory. It's really amazing that we can do this. And I'll take you through the story of how we got there. I was just looking for an image last week, and I found this post on Facebook 2018, 2018, not that long ago. Computing the count of distinct elements in massive data sets is often necessary, but computationally intensive. Okay, not bad. Say you need Got that. Say you need to determine the number of distinct people using Facebook in the past week using a single machine. So they said, okay, we'll do that. We'll just do an SQL query on it. This would take days of time and terabytes of memory. So people still wrestle, the people who are not here in this audience, still wrestle with this problem that comes up all the time. And I'll come back to this in a minute. The time, and I'll come back to this in a minute. Okay, so now I'll talk about Philippe's solution, which is called probabilistic counting with stochastic averaging. And this is 1983. That's like almost 40 years ago. Now most Facebook guys didn't know about it. And so this is a fantastic paper. It introduced a problem and introduced the whole idea of a streaming algorithm where we do different types, not just this one operation. Just as one operation. And it's the idea of keeping a small sketch or a little bit of information about big data. And there's detailed analysis and full validation through experimentation. And this algorithm is fine. At least Facebook guys should be using this one. And it's extremely simple, and I'll explain it to you in a minute. The bottom line is, you know, teach this to your students in half an hour. It's a really great. Half an hour. It's a really great example of the effectiveness of both algorithm science and analytic commentator. And you know, Facebook should be using it. Okay, so the starting point is some really simple integer functions. So the first one is r of x. That's the number of trailing ones in the binary representation of x. It's also the position of the rightmost zero, and I'll give plenty of examples. And I'll give plenty of examples. So, capital R of x is 2 to the little of r of x. So, this one's just got 1 trailing 1, or the 0 is in position 1 from the right. And so, capital R of x is 2. That's the, sorry, went the wrong way. Here's a couple. Here's a couple, here's a better example. So this one's got five trailing ones, and two to the fifth is 32, or in binary. It's like you take the trailing ones and turn them all to zero and add a one to the left. It's capital R of x. Okay? So those are very simple integer functions. Not only are they simple, in particular, capital R of x Is really easy to compute. So you take x, then you take not x, so complement all the bits. Add 1 to x, so that brings us 1, 0, 0, then those are all the same. And then you just and those two things together, and you get capital R of x. So those are three machines. Those are three machine language instructions to compute capital R of X. It's extremely easy to compute on any machine. It's also possible to compute little R of X with only a few machine language instructions, but you need to study Knuth Volume 4A or Hackmem from MIT. And there's also P of X, which is the number of ones in the binary representation of X. ones in the binary representation of x. People have studied these functions really intensively because initially they were pretty important in cryptography, I think, is the real reason. But anyway, everyone here believes the capital R of X is easy to compute, and I'm asking you to believe that the other ones are not too hard to compute either, if you know what you're doing. So the algorithms are based on these three integer functions, but mainly probabilistic counting is just Mainly, probabilistic counting is just dependent on capital R of X, which is so simple to compute. Okay, so what's the first step is to hash the values. So hashing is something that people have known about since the 60s. And what we want to do is transform each value in the stream into a random, say, 64-bit value. That's called a hash function. That's called a hash function. And that technology is very well understood. In the 20th century, 32 bits seemed to be enough. Now, 64 bits seems to be enough. And it probably is enough because 2 to the 64th is a pretty big number. So that with very high probability, if the values are total random, they're 64-bit. They're only going to be equal if the values, original values are equal, so it doesn't really impact the cardinality count. Really impact the cardinality count. And it's hashing is built in in modern systems. And you can use, again, fast machine code operation. So the state-of-the-art is called the Merson Twister, and it uses, again, only a few machine code instructions to convert our values into so-called random 64-bit patterns. So, what it does is it allows us to do Is it allows us to do cardinality estimation on binary integers that look random, not strings that have like dots in them or numbers or URLs or anything like that. So they look random. They're not really random. I'll get to that in a minute. Except for the fact that some values are equal. So we're going to do cardinality counting on strings of random 64-bit integers. 64-bit images. Okay, so the first hypothesis, getting back to algorithm science, they're not random. They're computed by a deterministic process. They're just not random. But we're going to adopt the hypothesis that they're enough like random that we get the same behavior as if they were random. This is a scientific, you can't prove this mathematically. You have to start with some ground truth. You have to start with some ground truth. And in the real world, we've gotten by for decades with the idea that if you hash, they're close enough to random that for the data that we have, it's going to work out. And what we really have to do is run experiments to validate any hypotheses about performance, just to double check. But that's what we do in algorithm science, is run experiments to validate any hypotheses. Experiments to validate any hypothesis. This is a really basic one. And for hashing, I mean, 40 years ago we used to discover, oops, that was a bad hash function. But nowadays, it's pretty rare, unless you have what's called a bug. It's not really the hash function that you thought, you're not going to have a problem with a bad hash function. And so I just want to make that point that what we do is run experiments to validate. Is run experiments to validate hypotheses. And this is a basic one for this problem. And it's just distinguishing from what goes on in the theory of algorithms where they worry about other things. And I'll talk about that in a minute. Would it be a dual problem? Instead of taking the last few repetitive ones, the first few, yeah, you could do that. The first, yeah, you could do any, yeah, because they're supposedly all independent and random. There's lots of different things you could do, and I'll get to that. Okay, so here's the algorithm from 1983. So we're going to maintain a single word sketch, and what we're going to do is for each data item in the stream, we're going to compute big R of X. Big R of X. So that's 2 to the number of trailing ones. And what we're going to do is use little R of sketch to estimate log N. And I'll see why that might be reasonable in a minute. Or big R of sketch is 2 to that power to estimate N. And it has to be refined with a correction factor, the analysis. Correction factor that analysis will tell you. And here's just a quick what it might look like, say, if n is a million and we get a new data item. So this data item has little r 4 and we're saying we're around a million somewhere. So big R is 16 of that data item. So it's just a one with four zero. It's just a one with four zeros after it, all the rest of them are zero. And the algorithm is to bitwise OR that with the current value of the sketch. Up here's the current value of the sketch, there's that value, so we're going to bitwise OR them. Now, most of the time, when n is that high, sketch is going to have a lot of ones at the end. So most of the time, this operation isn't going to do anything, usually. Isn't going to do anything, usually. It's only in the relatively rare event that it's got a lot of trailing ones and it's going to change the sketch. The leading bits of the sketch are almost surely zero. The trailing bits are almost surely one. And what we're using as our estimate of log n is the position of that rightmost zero in the sketch. So this would be in this. So, this would be in this example the estimate of n would be about 2 to the 20th, which is about a million. So, that's the algorithm. And again, just to be sure that people don't get lost at the beginning, this is with 32-bit values. These are things that can happen. So, again, with high probability, there's sketch has a lot of. Sketch has a lot of ones at the end. If you don't have as many ones at the end, there's going to be no change. And there's also a very low probability that you have way more ones that are in the sketch. So in this case, we added a one to the left of the rightmost zero. And that also doesn't change anything. That's a low probability event, but it can happen. Event, but it can happen. And then here's one where the sketch changes, but again, it's too far to the left. And here's one where the sketch changes and increments a little r by one. What's the probability of this event? That this thing should have 11 ones. It's 2 to the minus 11. It's a low probability event. But if you have millions of items, it's going to happen. So that's another thing that can happen. And you could also zap out a zero in the middle of a string and even increase by more than one. But all these very low probability events, most of the time nothing happens. It's only when you're fooling around where in is that things start to happen. That's just the intuition of the algorithm. And the thing about the algorithm is Is it's extremely simple. That's the code of the entire algorithm. That's computing big R. And this is, well, it's using a little bit of advanced stream processing, but everybody can understand what it says. It says, you get a stream as input. For every string in the stream, you bitwise OR the sketch with R or the hash of it. Or the hash of it. That's it. And then return R of the sketch. So you get single word sketch or all the values, the R values, and then return the big R. So I call this a really early example of a simple algorithm whose analysis isn't. And the story of this algorithm is there was a guy at IBM Research. There was a guy at IBM Research who had this algorithm for a database application. And Philippe went to visit him. And he says, that estimate seems low. How low is it, Philippe? And he hit on the right guy to ask. Because what you can do is just run experiments and figure out what the correct factor is. So he even said, I think it's about 75%. And Philippe, we And Philippe, we haven't found direct evidence of this quote, but we all know that this is what he would have said. You don't have an algorithm if you don't have a good analysis of this. And there's a mathematical analysis that proves that the correction factor is 0.77, and there's an oscillating function of n that we're all familiar with, and that's not the topic of this talk. It was a tour de force in the It was a tour de force in the 80s. In the 90s, we understood it to be a tri-parameter. It's the highest null left of the right spine, a tri. And by now, it's kind of standard analytic combinatorics to compute this correction factor. So that's the correction factor, and we can run the algorithm. And so the hypothesis is the expected value is going to be n. The expected value is going to be n if you take a do a lot of experiments. So, this is 20 trials with 100,000 variables. And first thing you notice, and even they notice, results only within a factor of 2. So why is it only within a factor of 2? Well, it's obvious it is always returning a power of 2 divided by this factor. So, those are the only numbers you can get. So, you've got to. You got to do more experiments on the same stream to get more accuracy. So, this is called probabilistic counting. The actual effective algorithm incorporates what I'm calling stochastic splitting. They call it stochastic averaging, but I think it's more descriptive. So, we want to perform n different experiments on the same stream and then average the results. And then average the results. Now, there's things you could do. So, do you want to do m independent hash functions? No, that's way too expensive. You don't want to do that. Because m is a big number. Don't remember, you've got billions of them. So if the thing takes a day, then it would take a thousand days. You don't want that. M-way alternation, like put one in each stream and just but and that's also unsatisfying because it could really get it wrong for certain inputs. For certain inputs. So, this one would say each of the streams has only one, that's probably not what you want. So, the alternative that Philippe developed is called stochastic splitting or hashing. And what we do is use a second hash function to throw them into the streams. So, that way, equal values get thrown into the same stream, and it's like a fair way to divide them up to Way to divide them up to help determine the number of distinct values. All right, let's see where we are. And then, so we're going to use probabilistic kind in each stream, and that gives us n sketches, toodle the little, and sorry, I should have edited that out. Saying compute the average number of trailing bits in the sketches, and it's the same actual correction factor. Correction factor. Yeah. They said capital M has to be big, but I don't understand why it would have to be big to get. It depends on it. So we get there. To get the kind of accuracy we're talking about has to be fairly big. Not million big, but thousand big. We'll get there. Okay, so that's stochastic splitting, and there's the And there's the code. It's a little more complicated, but really not very much more complicated. So we're going to use a second, we're going to have an array of sketches, use the second hash to say which sketch we're going into, use probabilistic counting on each stream, and then go ahead and average them. Probabilistic counting with stochastic splitting. Stochastic averaging, they call a PCSA. Okay, so again, the accuracy depends on M. And like you said, we want to analyze that. And that's another tour de force, Flangela. And just paraphrasing it, so it uses 64M bits, so 64-bit words, and it gets the relative. And it gets the relative accuracy close to 0.78 over square root of m. So m is 1,000, square root of m is 30. You want to be within three standard deviations, 10%, it's going to be 1,000. Okay? And again, that's using Mellon transform, very precise asymptotic estimates. His uniform bounds did an awful lot for 19. Did an awful lot for 1985. And so, again, our hypothesis is that this is going to work for the hash functions we have and the data that we have. And they did in 1985 reproducible scientific experiments. Internet is not in widespread use. What did they do? Well, everybody was using Unix, and there was online documentation of Unix. Online documentation of Unix, man pages, and they ran it on all the man pages in Unix. So everyone else could try this thing on their machine running it on all the man pages. It's kind of amazing that they did that. And you know, anybody can code this one up. And like I just ran it, it's less than 1% larger than the actual value just for my little thing. Just works. Does that affect? Works. Is that effective? Yeah, absolutely. So it's definitely an effective approach. There we go. One pass through the stream, not very many machine instructions, and it's validated. Now, maybe there's better ways to get use less memory. To get use less memory. And maybe there's other operations that can be supported. There's a lot of things. And as you pointed out, you have these random things. There's many other statistics on these random numbers that you could try. They try and analyze this one. So that's PCSA. And actually, Jeremy wrote a very nice survey article about the development of this in 2013. In 2013, it'll appear in the collected works when the collected works come out. And I'm sure Mark will have more to tell us about this. Okay, so what's my time here? Oh, I'm good, I think. I'm only up to 1980. So there's a better solution. So, first of all, the theoreticians get a hold of the problem. And there's this seminal paper by Alamati Sinsgetti. That one, the great Since Getty that won the GÃ¶del Prize for foundational contributions. And they tried to formalize, they didn't try, they formalized this from a much more abstract point of view and talked about lots of other streaming computations. And they have a theorem that kind of what it does is it replaces the assumption about the hashing function with what theoreticians have been doing is. Theoreticians have been doing is assuming the existence of a random bit. And then from that, you build up everything else to make sure that if you, your hash function produces independently random bits the way that you want. But that's what they do. There's still an assumption. It's still not random. And you still want to use any of these algorithms, still have to test them. So no, actually, no impact. Actually, no impact on practical cardinality estimation. It just changes the hash function. It's a very weak accuracy estimate with big O bounds and didn't implement the algorithm, wouldn't implement the algorithm, nobody would, because there's too many operations per value to compute a hash function like that. Okay, but still, introduce the problem to the theory community. The problem to the theory community, and they ran wild with it. There are many, many papers about cardinality estimation and other streaming algorithms, but very, very few of them are actually implemented. Like if you're at Facebook and looking for an algorithm, you're not going to have to search through that and it wouldn't be pleased. So, but anyway, after a while, and And Philippe improved his algorithm too. I'm leaving out some of the story, but there was a theoretical algorithm proven that says that you can get by with, so our sketches, like I said, are 64 bits. That's like log n. And you can get by with just log log n bits for each screen and still get the 1 over n square root of n accuracy. Than accuracy. And again, there's no, there's big constants hidden. It's not implemented at all. That's a theorem. And meanwhile, Philippe and his colleagues developed an algorithm called hyperlog law. And so it achieves these same bounds, but it's also practically useful. And that's 2007. So it's a pretty easy variant, and I'll just talk to you about it. Easy variant, and I'll just talk to you about it in a second. It just uses a much smaller sketch. So we have these 64-bit sketches, and basically, what Hyperloglog does is just save the R values. Rather than save the whole sketch, it just saves the smallest R value seen so far. So that's how it gets the log log n instead of log n. Now, that screws with the analysis. The analysis, and you're going to have to, and also they take a harmonic mean rather than a regular mean just to knock down the standard deviation a little bit. So very little extra expense, and again, full analysis validated with experimentation. Didn't put out that algorithm until they had that. That's hyperlog log. Hyperlog log and I won't go through the detail of it. It's really a simple variant of the other one. Rather than keeping the whole set, it just keeps those R values. And there's a theorem that goes along that says that it's going to use log log n bits. I like to think of log log n as six in the real world. Okay, maybe seven, not if you want to argue it, but not definitely not eight. And I have to say, I usually don't jump on the bandwagon of trying to cut out a local van factor. It's like six to me. But so that's the memory use, and that's a significant difference for sure for hyperlog log. And there's a theorem. And so, on the basis of the theorem, we can say. Basis of the theorem, we can say we're within three sigma. One key thing about the theorem, they say it's approximately Gaussian. The statistic is approximately Gaussian. And we have probabilists in here, and you can think about that more deeply than I can say. Why does Lee say approximately Gaussian? And there's no proof that it is Gaussian, but maybe there's a limiting, whatever. You guys can worry about that. That approximately Gaussian. So that's enough for me, and even enough for them in the paper to say, put in this line for the practitioners, say, use this algorithm, you're going to be within three sigma of the count 99% of the time. And so if you take m equals 1024, then you get that. Okay? Get within 10% of the count, 98% of the time. Percent of the count, 98% of the time. Okay, so that's hyperlog log. Let's see. So here's a more extensive validation. So I take my input file and every 10,000 data items that I read, I'm going to stop and do an estimate. And so up. And so up to a million. And so this is the exact value, the exact cardinality, and this is what the algorithm produces. So there's one gray dot for every experiment. I do 100 trials. And then a red dot is the average of the gray dots. And it's the average. Sorry about that. So, if you just take, like, then another validation is just do a histogram on a million and fit it to normal, and it fits. So that's hyperlog log. And as M increases, the thing goes narrower. It absolutely fits. It absolutely fits for every value round. Okay. What? That's a great fit? Well, they're all maybe a million is not enough. I do a billion. It's approximately normal. So, anyway, that's definitely an issue, and that's why this is a good place to present this, for sure. That for sure. Okay, so the Facebook guys did read this one. They implemented Hyperloglog and they got their job done in 12 hours with less than a megabyte of memory. And a megabyte of memory to those guys has to be like really absolutely nothing. So they could have used 64 megabyte and used the even easier algorithm. But anyway. But anyway, and it's not just Facebook. Actually, Hyperloglog is used lots of places. Google, there's all kinds of startups based on HyperlogLog. Amazon announced that it implemented. This is a 20-line algorithm. Maybe announcing that you were able to implement this 20-line algorithm is not the greatest. So, anyway, that's. So, anyway, that's where I go. That's calling Philippe an algorithm scientist because he really was a prototypical algorithm scientist. So, this is, now I want to talk about what's past hyperlog. So, the theoreticians continue to work. The theoreticians continue to work on this and even proved matching upper and lower bounds on this problem. So any algorithm that gets big of one over square root of n must use omega of n bits. And then there exists an algorithm that uses exactly that. And again, no implementation, no validation. The constants are The constants are undoubtedly huge and implicit in the big O notation. And so it's still open, was still open, if we can really beat hyperlog log in practice. And as I said, usually, I know, like, okay, log log is good enough for me. So, what would a better algorithm need to do? Well, still make one pass through the screen. Use Use not very many machine instructions, not very many bits. Okay, I've always said that. And it was really Jeremy during, we would meet by walking, and during a walk, you know, he's explaining, well, I only learned all of this stuff from him, and he's explaining to me, and he said, really, there should be a really easy algorithm that doesn't use very many bits. Small constant times n bits. What does that mean? What does that mean? It's really got to be not very many at all. Because hyperlogo only uses 6n bits. Nobody's going to care unless it uses like 2n bits or 3n bits even without blowing up the other parameters. And you have to do experiments, it has to prove out that it actually does work. It seems like quite a challenge. Jeremy told me just last week that after Just last week, that after five years or ten years, he gave up on the idea. So I presented most of what I told you and a lot of other stuff in Craco for the Plajolet lecture in 2016. And I had all the slides ready. And on the plane, on the way to Craco, it's a long ride, I was thinking, what's an algorithm that uses only a few bits going to look like? It's going to look like. And so I had this idea and I said, okay, how am I going to, it might be a nice finish to the talk. How am I going to explain this algorithm? Well, what I have done all throughout my career has been writing code for what I want the algorithm to be. And if I can't get that code to be simple, then I give up on the idea. And if that code's simple, then I can see if it works. Like, see if it works. And all my books, all the code, it's like that. So if it's gonna be too much crappy code, I'm not gonna put it in my book. So I had the same feeling about this. So I said, well, I'll explain it with code. And I put this slide up. I put that in the talk as the last slide in the talk. And I'll explain this one to you quickly. Oh, I'm going to run out of time. So, okay, I'm going to do it really. So, okay, I'm going to do it really quickly. I think we'll see what I do. So, the idea is that I'm just going to keep track of an estimate of log n. So that's this parameter t. And I'm going to have two sketches. And the first one is 64 indicators of whether pink rent teeth. And the idea is if I get a value that's got an R. A value that's got an R value that's bigger than T, I'm going to set the bit saying maybe I should think about incrementing t because I just saw a bigger value. But I need more evidence in order to increment t. So I have 64 of these values and I won't increment t until I get half of them. Half of the streams, I found an R of X that's bigger than T, and that's when I'll increment T. And that's when I'll increment T. Now, but I don't want to throw out everything about what I've seen because maybe there's bigger values. So I'll also keep track of whether t should be incremented by 2. And then when I get half the bits 1, I'll reset the sketch to the sketch 2. That's like where I would have been if I. That's like where I would have been if I had started with that one, with the new value, and then reset my second sketch to zero and go along. So that was the idea. Basic idea is when in half the streams I've seen something that indicates that I should increment t, I go ahead and increment. But I don't throw away all the information that I saw in the stream. If I saw some really big ones, I keep track of those too. And this algorithm actually had a name before it existed. It had to be hyper-bit-bit because you're going to use two bits per stream. So, anyway, that's the name. And so I put this slide up as the last slide of the talk and explaining this is what such an algorithm might look like. But I didn't even compile this or anything, because it's just for talk. anything because it's just for talk. And a number of people came up to me could you show me that last slide again? And so I had to post the slides like here I'm going to have to I'm going to not go through these examples because I've got to have a different one coming later. I don't have time. So like I didn't know anything about the bias vector. I didn't even know if it even worked and people asked. So when I tripped back I said well I got to post these So on the trip back, I said, well, I've got to post these slides. Maybe I should compile the code, see if it compiles, and maybe I should even run it. Actually, what happened was I wanted to watch the last season of the Americans, but it didn't download on my computer. So I had nothing to do. So I said, okay, I'll get these slides posted before the end. So I got the exact values, hacked for a while to figure out what the bias should be. To figure out what the bias should be, and it works. It gets within 3%. Does seem to work amazing. So now there's stuff to do. And now I'm going to have to rush through the new stuff. Sorry about that. And well, time flew by. I was actually working on a book. And so there went 2016 and then 2017 comes and goes. comes and goes. And 2018, we had Canoe's 80th birthday celebration. And I gave pretty much the same talk. And Svante was there and his attitude was, well, you haven't figured that thing out yet. And at the conference dinner, he handed me a piece of paper with the analysis on it at A of A six months later. Months later. So, okay, so that's living right. And I'll just flash through the analysis quickly because I want to do the analysis in the other one. And then I want people to get bored and lost. It took me a while to understand it, but let me just rip through it to get the result. I thought I'd have more time. It's got the amazing result that so. So, when you get to the point where half the bits of the first sketch are one, how many bits are there in the second sketch? And it's cube root of one over cube root of two. Absolutely amazing that the analysis can get you to that point. And we can validate that in everything else. It's all fine. So, okay, so this. Okay, so there's still more. I'm going to do pretty much the same thing for later. But here's the validation. It's just way too high. So, and Svante said this in the little first line of the little bit of the paper, this analysis is no good because you're forgetting about the bits past, even the really high values. You're forgetting about a lot of them. There's just way too many values. There's just way too many values that get recounted later on, and you're ignoring them. So it's no good. It's going to be high. Now, you could do hyper bit, bit, bit, have a third sketch and carry that one on, but you're still going to have the same problem, and it would be more expensive. So now we've got to estimate the number of recounts in hyper-bitpit. Hyper-bit bit. And again, time flies on, or not so quickly. I hate to be using my valuable time on this, but I'm not doing this correctly. So we all know what happened in those years. It doesn't seem like such a long time now. And then, like last spring, I actually had an idea. And so, how are we going to estimate this number of reds? Going to estimate this number of recounts. And that's the final frontier. And the idea was: we need to estimate the values that we forgot. Why are we even bothering to keep track of them for one stage? Why not just estimate it for that one too? Just estimate for every stage. And so there's a much simpler algorithm that maybe we can analyze, and that's what Hyperbit is. So it's got an estimate of log. It's got an estimate of log n. It's got a sketch, which is n indicators whether to increment that. It goes through, and when it finds an r value that's bigger than t, it sets the sketch bit. And then when half of them are 1, it increments the estimate of log n. That's hyper-bit. Now there's some kind of bias factor. It's going to be a function of the number of bits left in the scheme. Of the number of bits left in the sketch. We don't know what kind of function that is. And the experimental, I ran this and experimented with it. It was kind of inconclusive of what it would be because I had no idea what this function is. But it seemed worthwhile to try to analyze this algorithm before trying to analyze hyper-vibbit, because then maybe that'll help us figure out what to do. So then I go back to some. Then I go back to Savante's proof, and I do have time to do that, I think. I'm not going to go through the example because I think we'll all pretty much get it. Okay, so the analysis. It's the same as what Swante did at the beginning, but it's much simpler. So in a data stream with that many distinct values, the probability that a given item has more than 10. An item has more than t trailing ones is 1 over 2 to t plus 1, right? So that's easy. The probability that no item has more than t trailing ones, or that the sketch bit is 0, is about, that's like a plus 1. And so that's the probability of the corresponding sketch bit is zero. And then we can work from that. Where are we? Where are we? So we're going to call a phase every time that t is incremented and the sketch is zero. So after we've added v values per string, then the number of zeros is binomally distributed. And so the expected number of zeros is m times that value. times that value in the sketch. And so we're going to stop when that is equal to m over 2 or e to the minus 0, 2 t plus 1 equals 1 half, or b equals 2t plus 1 natural log of 2. That's when we're going to stop. That's the number of values in each phase as a function of our estimate of log n. And so in the And there's m of them. So the expected number of values in phase t is that: m times log 4 times 2 to the t. Elementary probability, I think. Okay, so now we want to estimate the recounts. So the other idea is instead of thinking about the whole future, just think about the next phase. How many values are going to be? How many values are going to be recounted in the next phase? And then when I get to that phase, I'll worry about the one after that and so forth. How many are going to get recounted in the next phase? And the answer is easy. It's half of them. Because either the next bit over is zero or if it's one, if it's zero, then if it's one, it's going to be counted again in the next phase. If it's zero, it won't. So half of them are going to be counted again in the next phase. So we just have to compare. So we just have to compute the number of values that are going to be recounted, and that's the number of values that would generate half of half the number of ones or a quarter of the number of ones. And that's easily computed using the same idea. And so we get natural log of 4 thirds instead of natural log of 2. So that's the number that will be recounted. So then you subtract that off from the ones that get the ones. Off from the ones that and get the ones that won't be recounted over there, 2 log 3 minus log 4. Again, relatively elementary arguments, and that gets us to the last phase. And the last phase is, depends on the proportion of zeros in the sketch. And again, following through the same kind of logic, it's not too bad, there's three observations. So, first of all, we've only got First of all, if you've only got beta percent, or if you have beta proportion of zeros in the set starting with mid equals one, then you can calculate the number of values that got you to that point. So you want to do that. The recount estimate from the last phase is much too high. It assumed that you were going to go all the way in the next phase. So you add that back in and then replace that estimate. And then replace that estimate with the number of values needed to generate beta over 2 ones, or 1 minus beta over 2 ones, which is easily calculated. And then you can get a total expected number of values that have to be accounted for in the last phase. So, adding them all together, We get a theorem that gives the expected number of values from hyperbet. But after it completes p-phases with beta in sketch, and that thing can be immediately translated to code. It's the final accounting for hyperbit. And so that's that little factor. These things are easy to check because they say, okay. More than an hour, but I still have four minutes, I'm thinking. These things are easy to check because it's got to go from whatever it is to twice that because you're going to be incrementing T resetting the things obviously got to be continuous. So now here's the memory used for probabilistic counting and hyperlog log, and that's for hyperbit. And you've got to add the log log n for t. add the log log n for t. So this matches index lower bound with a constant of 1. So that's a pretty good constant to get at. I suppose you could do it with fewer than n bits. I don't know. And okay, so similar accuracy hypothesis. We're going to conjecture that it's approximately Gaussian and so that definitely So, that definitely requires some math, it requires the power of people in this room. And then we're going to say that we have a solution. That's the same experiment. This is hyper-bit-bit running on the same experiment. And that's the same, not quite fit. But for every value of m, it goes narrower and wider, so really looks like a Scoutsian with. It's about tune with standard deviation one over m. Okay, so that's hyperlog log on the left, that's hyper bit on the right. Seems like you get the same accuracy with one-sixth as much memory. And that seems like a win. And again, maybe it's not optimal, and maybe it is. So, anyway. Maybe it is. So, anyway, to finish what's next, complete the analysis somehow. Maybe a look at hyper-BIFIN. Maybe it's a good thing to use a real thing, then an estimate. And maybe it's not that hard. And there's parameters, like, why are we stopping when it's a half? Why not stop when it's key root 2 or something? I don't know. And so maybe we could fool around with that a little bit too and got to run some more validation before. More validation before pulling a paper off to the validated things. And also look at algorithm science for other streaming algorithms. I think people are studying streaming algorithms of all types and it might be worthwhile to do. Okay, that's my talk. Thank you. If you have questions, no one. So um this might be a little bit of a long one, but I'm wondering, so if you look at the sort of the original, so the original idea with Mark, and uh you if you decide you're willing to do a long offline computation every time sketch changes, then that doesn't really hurt. That's true, that might be okay. Yeah, so in principle, what you have is the information that you have. That you have the information of the sequence in which the bits were filled in in sketch and how much time passed each time. So there's a question, I guess, which is whether that single run already contains, if you're willing to do this offline analysis each time, already contains enough likelihood information that you can make an estimate. You don't need to have two that. You can just compute some, you know, for example, you want to. Compute some, you know, for example, you want to take the hyp a hypothesis test whether whether it's you know the big number n is correct or one point one times n. And so you have 30 eras in which you're gathering information about that. And so taking advantage of the fact that 30 or 60 or whatever is a big enough number, maybe you have enough to get the accuracy you want. It's all there. That's a good point. And you don't have to go through the data balances. Yeah, I think that's and then when you're not leaving on the table leaving on the table you know maybe you're maybe you're looking at counts of things after the you know the bit change and maybe you're not maybe you know i don't know exactly the yeah no you definitely have that feeling when you're watching the thing going in the second half of the computation like nothing's happening right right not much is happening that the first half already all those low bits that change that aren't doing a whole lot for your estimate yeah are all contributing to these likelihood factors and generally And generally, it's true if you could get every paired hypothesis test within probably you get the whole thing. That's a projection of the estimate. Yeah, that's a terrific question. Yeah, thank you. Other questions? Okay, so let's take the speaker again. Can you hear me? Yes, absolutely. This is Janzo here. I don't really have a question on this, but I just want to say that I was very glad to hear this. Very interesting, and I will be interested to see details. And it really gave me inspiration to try to see if it's possible to prove as a project normality. Not that I will do it before dinner. Not that I will do it before dinner, but in any case, later next year or so. Yes, well, I have a preliminary draft of the paper that I've been waiting to do this talk to complete, and I'll send that to you. And I agree that, I mean, the original thing is only, it's like a multinomial. I mean, yeah, I agree. It might be possible. So I definitely intend to send this to you. Thanks very much. Thank you. Okay, so there's a half-hour coffee break for 28 minutes now. And then at 10:30, we continue with the talk of Calvin Rivera Lopez. Calvin, are you here? He was here yesterday. Okay, so you exist. He's working on his talk. Let me modify. Hopefully, we're going to territory. Sure, we should. I have quite a bit of conversation.