I'm very excited to share our recent work on MBImpute. It is an imputation method specific design for microbiome data. So why do we want to have an imputation method for microbiome data? The motivation lies in the existence of excess zeros in microbiome sequencing data, regardless of technology. Data. Regardless of technology, being the 16S RNA sequencing or the whole genome sequencing, there are always a lot of zero-rate counts in the microbiome data, which is usually summarized as a sample by taxa count matrix. There are three types of zeros. The biological zeros means that the taxa are truly non-existent in samples, and these are true zero abundances. But in addition to those, we have technical zeros. These are zeros in These are zeros introduced due to artifacts in the preparation of sequencing experiments and sampling zeros, which are results from limited sequencing depth. Especially for WGS data, there could also be zeros due to the contamination of house DNA that makes the bacteria species receive fewer rates than they should have. That effectively reduces the effective sequencing depth, which also results in. In depth, which also results in sampling zeros for bacterial species. Because of those, the analysis of microbiome data becomes very challenging. And we propose MBIMPU with three goals. The first is, of course, to recover likely non-biological zeros in microbiome data and therefore reduce data sparsity. And our goal is also to output an imputed data matrix, a sample by tax account matrix, in the same Tax account matrix in the same format as the input matrix. So, because we can reduce the sparsity, the many downstream analysis that couldn't work for highly sparse data will now become useful, as we'll see later in my slides. So, what is the input of MBImpute, or what are the data resources it can leverage? There are three matrices. The major one is what we call the microbiome data. It is a sample by text. Bond data. It is a sample by taxonom abundance matrix. So, here by abundance, we mean that the read counts, which are non-negative integers, are log transformed. And the matrix is M by M. So, it has M rows corresponding to samples, like patients or microbiome samples. And M is the number of taxa. You can consider this to be a number of species or the number of phyla, depending on the level of taxa you define. Depending on the level of text that you define. And the second matrix is a sample by covariant matrix, which is usually called sample metadata. Here we have the same n rows corresponding to the n samples and q columns corresponding to the number of covariates. Example covariates include age, gender, and body mass index of people of those samples. So the third matrix is a taxon by taxon pairwise. Is a taxon by taxon pairwise distance matrix and its dimension is m by m. m is the number of taxa. And this is extracted from phylogeny information like a phylogenetic tree. Here we just encode the distance between taxon J and J prime by the length of the path connecting them in the phylogenetic tree. In other words, if they are closer in phylogeny, they have a smaller distance here. Here. And the distance is a non-negative integer. So, this is a graphical summary of the three matrices I just mentioned. This is the matrix that needs imputation, a lot of zeros. And to do the imputation, we want to use the matrix itself and also borrow information from the sample metadata matrix and also the taxon-taxone phylogeny matrix. So, the idea is that we hope to leverage the We hope to leverage those Q covariates to borrow information from similar samples that share similar covariates. And also, we hope to have the taxa closer to each other in the phylogeny to borrow information from each other in the imputation. Okay, this is the first step of MB impute. That is, we want to identify which taxon or which abundance values are Or which abundance values are likely that need imputation. So, specifically for taxon J, we fit this gamma-normal mixture model to its abundances across the n samples. So, this is a gamma component. This is a normal component. The reason we want to do this is because looking at this slide, the gamma component captures the concentration of abundances near zero because gamma cannot be negative. Cannot be negative, the density cannot go to the negative part, so it's truncated here at zero, and also the normal part is used to describe the distribution of those non-zero concentrated abundance. So, our expectation is that without this zero mode, we should have a normal-like shape for the abundance distribution. But with this zero, then fitting a normal distribution alone will not describe the data. Will not describe the data well, and the gamma component can capture this zero inflation part. So, we want to check whether indeed we need this gamma-normal mixture model or a single normal model is sufficient. So, to do that, we perform a likelihood ratio test to compare the two models. One is the gamma-normal mixture model, one is a single normal model. And under the now hypothesis that the Now, hypothesis that they fit the data equally well, then this likelihood ratio test statistic will follow a chi-square distribution with three degrees of freedom. Then doing this, we can obtain a p-value for every taxon j. If the p-value is small, then we will decide: okay, we will go for the gamma-normal mixture model. Then, using this mixture model, we can calculate a posterior probability. Posterior probability for taxonom J in sample I. So specific to this Yij value, about how likely it is coming from the gamma component given its value. And this is the posterior estimate. We basically plug in the parameters we estimated from fitting the gamma normal mixture model. So you see pj becomes pj hat, alpha j becomes alpha j hat, and so on. So this dij will give us an idea Will give us an idea about how likely this value yij needs imputation. And our decision is based on dij. If dij is greater or equal than one half, then we decide that yij needs imputation. So this graph shows that indeed, and this is from real data, it shows that indeed for many taxons here, taxon one, two, three, and four, we observe this zero inflation and the gamma normal mixture model fits better than a single. Her model fits better than a single normal model. Please note that the data comes from colorectal cancer short at CRC, and we will use this data set again in our real data application. Then we come to step two of MB impute. Given that we know the values that need imputation, like this one in square, how do we impute it? So the idea is that here, given this matrix, we want to borrow information from similar samples. Information from similar samples and similar taxa, and also use information from sample covariates. And also, we have the phylogeny to use. So, combining these information together, we will impute this value. This is the diagram. And then in detail, what we do mathematically is that based on the DJs we calculated in step one, we will define sample taxon pairs into two sets. Into two sets. One set is based on the pairs whose dij is small, smaller than one half. Then we think, okay, these values are likely coming from the Gaussian component, not the gamma component. And these values do not need imputation. So we call them omega. And for the values whose dij are greater than one half, we think they need imputation, and we call them omega c. So c stands for complement. So omega and omega. So, omega and omega c together, they describe the other pairs in that sample by taxonom matrix. Then, we assume this model for the relationship between yij and this yi dot means the i-th row in the matrix, y dot j means the jth column in the matrix, and xi dot means the ith column in the sample metadata matrix, and the error term. And the error term. So, what this actually means is that we are assuming a model, a linear model, and here this y dot means that we want to borrow information from similar taxa. Okay, given the sample, we want to borrow information from other taxa. This one means that given the taxa, we want to borrow information from similar samples. And this is to borrow information from sample covariates. So, you can see that we allow each. You can see that we allow each coefficient vector to be specific to taxon j, specific to sample i, and specific to taxon j. So we actually have many parameters here to estimate given this model. And one technical note is that we do not want to use yij itself to impute itself, right? That's not useful. So that's why we specifically set the jth element of kappa j to zero and the i th element. To zero and the ith element of tau one to tau i to zero, so that we can avoid borrowing information from yij itself. So, this one can make the model clearer. So, you see that yi dot is actually this row, y dot j is this column, and x i dot is this row. So, we will use these as our predictors and we will learn their coefficients for the imputation. And this comes to a high-dimensional model because To a high-dimensional model, because if we do the calculation, the number of parameters is this large, while our training sample size is the number of entries in omega, so it's smaller. M times n is less than this, it's almost m squared plus n squared. So we need some penalization, as we have seen in previous talks. So this is our loss function to be minimized. So the first one, the first term is the squared loss, and the second term, the penalized. And the second term, the penalization part, it has two parts. One is that we want to penalize this kappa J, right? Because for kappa J, J goes from 1 to M, and each kappa J has M minus 1 entries. So this is a lot of parameters. So is cow I. This one has n times n minus 1 parameters. So to do the penalization, here we use the L1 penalty and a special thing for the cap. And a special thing for the kappa J J prime is that here the coefficient is about borrowing information from J prime to impute J. And so here we leverage the phylogenetic distance between taxon J and J prime. So that's DJJ prime. And here we basically the idea is that if their DJJ prime is small, it will be penalized less. And to allow some flexibility, we raise this distance to a power. Raise this distance to a power Psi because we don't know if the phylogenetic distance is meaningful in its original unit, which are integers. So we raise to the power. And this power, Pai, together with this penalization parameter lambda, will be chosen by cross-validation based on prediction performance. Okay, with this optimization, we can do imputation. Here, I'm going to show some results quickly. So, first, we do a lot of simulations. So first we do a lot of simulations to confirm its performance with ground truth being known. So we evaluate the two accuracy criteria, the mean squared error. That's for how well the impeded values mimic the true values. And the second is correlation, Pearson correlation, between the impedive values and the true values. So what we did is that we compare our method, MVIMPUTE, against the raw matrix without imputation and also solve. Imputation and also soft impute, it is a general imputation method that is that is very popular. And also, we have several popular methods borrowed from single-cell RNA-seq field, because in that field, imputation is a very important problem. So, comparing to those methods, we can see that we are doing the best in MSE and the best in correlation, because MSE, the lower the better. For correlation, the higher, the better. We also examined the really Also, examined the really for each taxon, we examined its mean divided by SD. And we look at this statistic and its distribution. So this is because we are doing simulation. We have the complete data without missing. This is the data with missing and the imputed data. So you can clearly see that the MB imputes imputed data mimic the complete data the best. And this distance is just to calculate the distance between two distributions. So it shows that we are doing the best. So, it shows that we are doing the best. And we also use a scatter plot to examine that phenomenon. So, here, S D, taxonom S D standard deviation is the x-axis, meaning it's the y-axis. We also see that the shape of MB imputed data mimic the complete data the best. Well, other methods didn't do as well. So, this is another result, which is very meaningful, the differential abundance analysis, that is, to identify the taxes. That is to identify the taxa that have differential abundance between two conditions is very key analysis in the microbiome field. So here we use the 16S rNN sequencing data simulated from a third-party simulator called Sparse Dosa. And here we evaluated the DA tax identification accuracy in three measures: precision, record, and F1 score, which is the harmonic mean between precision and record. Harmonic mean between precision and recall, and we compare for each of the widely used DA method, including Wilcoxon, T-Test, NCON, zero-inflated negative binomial, negative binomial, DEC2, metagenome C. We compare the accuracy of their use alone and using MBIMPU as a preceding step. So you see that with MBMP as a preceding step, the accuracy of all those methods are improved. Improved. Okay, now we come to real data. So, for the real data, the first thing we examine is whether the imputed data look reasonable. So, this is from some type 2 diabetes microbiome data shown as T2D from two papers, two studies. And this is the raw data. We can clearly see the zero inflation. And after we run MBIMP, the data looks like this. So, what's encouraging to us is that you see the distribution. Is that you see the distribution for the non-zero part in the raw data are very well preserved? And if we compare with the general method soft impute, we see some changes. Soft impute actually introduced some spikes here, while MBImpute doesn't have this issue. So our hypothesis is that soft impute is a low-rank matrix factorization-based method. So this spike is probably due to that nature, but this is subjective. That nature, but this is subject to our future investigation. The second result is again about the DA analysis. So, what we do is that for the T2D data, we have two studies and for the CRC data, colorectal cancer data, we have four studies. And we evaluated the consistency of the DA taxa identified from each study. So, our hypothesis is that if the DA taxa identification is accurate, Identification is accurate, then we should have some consistency across studies. And here we also compare each DA method result with having MB impute as a preceding step. So encouragingly, we see that with MB impute, the consistency are much improved for all the methods. And this is another check is the cross-study classification. So what we do is that we use one. We do is that we use one study to identify D8. And then, based on the identified D8, we predict the sample conditions like T2D or not, CRC or not, in another study. So here we show that using the DA textile identified by MBIMP with MBMP as a preceding step, then for all those DA methods, the classification accuracy in another data set, in another Another data set in another study becomes better. Okay, so the second to the last result is about how well the non-zero abundance correlation is preserved. So here we can see in the raw data, without imputation, we have a lot of zeros here. And what we do is that after the MB impute, you see the non-zero part is very well preserved, but the zeros are gone. And compared to the correlation, to the correlation calculated on raw data based on the non-zero abundance we preserve it very well but you see if we don't look at the if we just look at the whole raw data then the correlation is much driven by zero the black line so that's why this shows why it is very important to do imputation but you may wonder why couldn't i just throw away these data points and just use the non-zero abundance yes you could but the the loss of that Could. But the loss of that is that you will sacrifice the loss of some samples. While here, we rescued all the samples. So, with our approach to calculate the pairwise correlation between two taxa, you use the same number of samples and in your data. Well, in this approach, if you throw away the zeros, you will get a different sample size for different pairs of taxa. So, that's why we think it's advantageous to do MB impute. So, lastly, we show that using We show that using MB impute, we can run some methods that cannot handle high data sparsity. So, that is the, for example, it is the Bayesian network analysis. So, that one actually requires Gaussian data. So, if we don't do imputation, it cannot be run. And here is a very preliminary analysis showing that using MBMQ, we can build those Bayesian networks from two studies of T2D. And here, interestingly, we see that this edge between the See that this edge between the purple and the red two ruminococcus in eubacteria, this only exists in the control data, but no longer in the T2D data. Although this is a preliminary discovery, but we think that allowing microbiome data to construct Bayesian network is a promising, interesting application. So, this is the summary of what we said. So, MBIMP is a method for imputing microbiome data. Imputing microbiome data, it can correct non-biological zeros it identifies to reduce data sparsity. It can utilize sample metadata and taxophylogenetic information. We found that it outperforms imputation methods developed for other types of data, and we showed its application in deep A analysis and network construction. This is the R package, and this is the bioRxive paper. So, I want to thank the first author of this work, Ra Chen, for doing this fantastic. Work Ra Chen for doing this fantastic job in developing this method, and my former student, Vivian, who also helped us with this work. So, this is the last page about the references of the methods we compare against. Thank you very much. Great. Thank you so much for that great talk. There are a couple of questions in the chat from Toby. Do you do anything to ensure the fitted gamma component is close to zero? And I think, yes, that's something we. Yes, that's something we actually. Yeah, I think that's a very good question. So, based on this graph, yes, so we actually did the use the EM algorithm to fit gamma normal mixture model like this. But as you said, if the gamma turns out to be having a very widespread, right, not concentrated around zero, then in that case, the matrix model may not actually fit better than a single Gaussian. Actually, fit better than a single Gaussian. So, my hypothesis is that those will be excluded or reject, or will not be excluded, will be excluded by the likelihood ratio test. So, in that case, we will not fit a gamma-normal mixture model for use. And from Hong Ku, the next question, is different sequencing depth an issue among samples, or is the different sequencing depth issue among samples considered in the model? Yes, that's a very good question. It's actually what we did. It's actually what we did in practice is that we encode the sequencing depth into sample metadata. So essentially, the sequencing depth, sequencing depth itself is a sample covariate. So that's actually handled by this gamma normal mixture model. And also, if you look at here, sorry, if you look at here, so we actually have this, I forgot to say that in the normal mean, you see that it's actually a linear function of the sample covariance. So that's why. Are the sample covariates. So that's why it's incorporated here. Thank you so much. And with that, I'll ask if you wouldn't mind to stop sharing your screen. Thank you again for a great talk.