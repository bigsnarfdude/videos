Approximation of the Kerman feature. This curious one, you know, the Kerman feature is very nice. It introduces the time evolution of the statistics represented by Gaussian, when the model is liner, when the operator is liner, etc. And this algorithm reproduces how the intensity evolves along the forecast and analyzed state. So it takes the form of Take the form of a linear formula, a linear algebra formula, where we can see the mean and the covariance metric that evolves along the time. So it is very, very simple in its form. However, we cannot, it is quite difficult to implement for a large system. And so we need approximations. And so the first approximation that everyone knows is to use Ansemberg. is to use ensemble Kerman feature. So in this approximation we estimate the covariance matrices from an ensemble. Here we will introduce another approximation that is inspired from what we done in dinational data estimation where a very important part of the work was to design covariance matrices. So if you have a model of covariance matrices For instance, a diffusion equation or wavelets, I don't know. Then, if you are about to write equations for the parameters, like here are represented equal parameters for equation with diagons and the L2P. If you are able to write equations for the forecast as well as the update at the analysis at the analysis time, then you can reproduce the camera. Then you can reproduce the Cambod feature cycle. So, to do so, we first start with a covenant model. As I introduced, oh, okay, maybe I want to start with a presentation of the market. Since it is a new approach, I guess everybody don't know what is a total figure manager. Since it is new, we can revisit many, many aspects of data assimilation throughout this year. And actually, this talk will give you This talk will give you some of these aspects. We don't go into details, but just to give you, to provide you some ideas. So we start by a covariance model, and the covariance model we want to use here is a one based parameterized by the variance and by the local anti-p. So the variance for an error field is defined as a usual way. The expectation. The expectation of the the square is the error field, so uh if uh the expectation of the field is zero for x given. And the anisotropy is defined from the second-order Taylor expansion of the correlation function between a point and a point in its neighbor. And the entropy is characterized by what is called here the metric. What is coherent? The metric function. Or another way, the aspect tensor that corresponds to the inverse of the metric tensor. So in one dimension, this corresponds to the lengthscape, which is quite usual. And an interesting formula rating the error and this tensor is given by this expression. Okay, so To illustrate what it corresponds to, in one dimension, if you have here the correlation function, the length scale is defined from the second-order theoretical expansion of the correlation that corresponds here to a parabola. And this is the length scale. In 2D, if you have correlation functions, the shape of the correlation function at the origin can be featured by ellipsis, and this ellipsis is directly repelled. is directly associated to the industry. So now since we have a covariance matrix, we can define how it can be updated when you have observations. So I don't want to detail this algorithm, but one important important formula is that if you have this kind of angotropy for your forecast, For your forecast, so the expectancor for the forecast. And if you have an observation at this point, then the analysis and the 2P stand as the ratio between the variance of analysis divided by the forecast variance times the anisotropy of the forecast. So it is reduced here by these fraudsters. But very, very simple. It's very, very simple. But of course, this is an approximation, and in this algorithm, we can assimilate observation in a sequential way, as it is done in, for instance, on some common future. So, the second-order version that you have here takes this form, so it's a bit more complex, and it has some drawbacks, and we discussed that. So, to illustrate what happens in this situation, so here we consider the case where the background covariance matrix or the forecast covariance matrix is given as a homogeneous and isotropic formation. And we have an observation at the middle of the domain. And so, what happens? All the ellipses will be modified due to the assimilation, and we have this kind of formula. We have this kind of formula in the application here, in action. You see that the ellipses at the middle are much smaller than the ellipses at no distance from the observation because of the reduction due to this ratio. If the stretch is a a little if you consider uh an observational uh uh standard deviation uh lower than in that case, Lower than in that case, you will see deformation of the LETs that cannot be represented by this leading order term. So some illustration here in a simple test case. So we consider a flow in 2D with an observation network and here represents the analysis variant stream. So you can see where. You can see where the observations were, and here it represents like airplane tracks, for instance. So you have many observations at the box. So the analysis error field shows you implicitly what was the original correlation functions. And we compare here what is the result of the Kerbin filter versus what is from the result. Is what is provided by the parametric element future for the first order, so that formula, and for the second order. So the cost associated to this computation is awful, but here it is very, very low because we stand on approximation of analytical solution for that. Again, for the anisotropy, so it's not easy to So, it's not easy to show every ellipsis. So, to reproduce what occurs here, we consider diagnosis and what occurs. So, it is the variation of the local land scale. And so you can see that where there were observations, the land scale will be smaller. So, now we can consider the forecast type. The forecast step, that is the second step in the camera future. And to do so, if you have a dynamics, then the PKF forecast step, if you consider a Kovan's model based on variants and isotropy, is provided by this set of equations that correspond here to a second-order Kalman filter here, because we consider the redirection of the variance onto the mean. Variance onto the mean here. So this is not so difficult to compute most of the time, but it is quite boring. And so we introduced a tool to facilitate this. And this tool is called CPKIF. So CMPKIF can perform for you, if you give this set of equations, it can perform for for you the PECKIF dynamics and it can also perform for you uh automatically numerical codes. Automatically, numerical codes that help you to study this problem. So, this is an illustration. For instance, here, you provide to Simplica F, that is a Python package, the Burger equation. So, this is what you need to enter, and that's all. Then you press enter, and boop, boop, poop, you've got this equation. It's very simple. Okay, so what is this equation? Okay, so what is this equation? Well, you see it is exactly like in machine learning. We don't know exactly what is inside. So sometimes we recognize things. For instance, here we recognize the Bergla equation plus this term that is due to the redirection of the error onto the mean. So here, in that case, u stands for the expectation. Then you have a system that is coupled. Then you have a system that is coupled. Here you have the variance. Here for the equation of the variance you have U. Here for the uh dynamics of the NH2P you have the variance and you have also. For the variance, for instance, we can recognize the attacks of the variant by this term and also producing a producer that is often put in turbulence, for instance. In terms, for instance. Then, do you understand this term? No? Me too? I don't know. But anyway, it's happened. But we can recognize other terms like the attacks here and also the stretching due to the deformation of the elexis due to the flow. So this system is nice, but you cannot compute anything with it because there exists some terms. It because there exists some terms that we don't know here. So there is a closure problem. And the closure problem is not related to the non-linearities, it is due to the diffusion term, actually. Well, so we can consider it is zero. But if you do that, then the problem is that you have a negative diffusion here. So the system is unstable. So n you need to provide a closure. We have been very lucky because we We have been very lucky because we proposed a closure that stands for this expression. And computing this system with the closure, we have been able to reproduce the time evolution of the mean in the Berger equation for the variance. So here we start with an homogeneous variance. And at the prime beginning, the variance is 10. Prime beginning, the variance is damp because of the diffusion, except here where the front is forming and that introduces a spike of uncertainty. For the length scale, so that corresponds to the anotropy in one dimension, you can see that the length scale will increase. This is due to the diffusion, with some oscillations here that are quite difficult to understand. So we could see that. So we could see that we could think that this oscillation are maybe due to an error of the diagnosis. But actually it is not because this oscillation is exactly captured by the dynamics. So it is very nice because we are doing physics of uncertainty. We predict a dynamics and we see that it corresponds to actually what we should do. Nice. Okay, but most of the time we Okay, but most of the time we don't know. We we have been really lucky to find something. And the aperture for the PKF could have been stopped here if we didn't have found this kind of closure. But now, when you've got this kind of difficulties, you don't know terms inside an equation, what are you doing? The planning. Okay, so we introduce a way to We introduce a way to. Okay, so you understand? With CPKF, you give the equation and it provides you the dynamics of the ancestry. And then, because you are there, you press enter and you have the numerical code. But since you can obtain this numerical code, it can be NumPy, it can be also other things. And especially it can be TensorFlow, PyTorch, or what else. Or, what else? Why? Because we consider a finite difference spreadization of spatial DVD that exactly corresponds to convolutional neural network. So it's very natural to transform, to translate partial differential equation into neural network. And for the time scheme, it's very easy if you understand what is an error scale. Actually, people in machine learning call that result. Machine learning calls that residual network. So we have all tools to transform our partial differential equation into a very simple architecture. So this has been done. So again, you consider this very nice set of equations and you propose, for instance, a closure, but with coefficients you don't know. So this is one way to do. One way to do, but of course there is another one. And if you enter this kind of with uncertain coefficient, you can generate here some layers with your favorite framework. Here it was the PRS with the concept flow. And then train from data sets. So for our problems, the difficulty is to obtain a data set. Well, actually, you need to compute on. To compute ensemble of ensemble. By this way, for each ensemble, you have an estimation of the dynamics of the uncertainty, and then from that, it corresponds to your data set, and you can train the term. Here, you saw we used the closure we considered that could be interesting, and actually, it was. Most of the time, you have no idea about what to take, so you can. What to take. So you can consider all terms that are homogeneous to the term you consider. That can be a way to provide a dictionary of terms that could be interesting. Or if you don't have this kind of idea, you can put an MLP, for example, or something like that. Since we translated all our dynamics into a machine learning framework, we are ready to train this kind of Uh trend this kind of uh of problem. So it's very simple. And for this illustration, we found uh a coefficient that was not so far from the theoretical world and we were able to reproduce dynamics or the uncertainty for the kernel chip. So now another point that is of interest is that is most of time we have regional models. So it would be nice to see if it is possible. Nice to see if it is possible to formulate a parameter declaring filter with bounded domain. So, is it possible? We can consider a simple architecture here on a domain that is bonded at 0 and 1. And so for this very simple equation, the application formulation consists of three equations, one for the mean, one for the variance. One for the mean, one for the variance, and the other one here for the lanotropy. And so, if you have a boundary here at x equals 0, and you want to consider a GLD condition, for instance, solving, introducing GLD condition in that problem is very easy. You have to specify the variance you want and the anotropy you want, and that's all. So, compared. And that's all. So compared with an Sub-Kerman filter, for instance, it's quite easy to handle this kind of equations. So, but does it trend? Well, actually, yes. We specified, for instance, here, so this corresponds to the type from 0 to 1.5, the classic time of detection, with the variance here and the length scale here. So we specified an evolution of the variance and we specified an evolution of the landscape. An evolution of the NENSK at the inflow, and we can see here that the AKIF is the battery produce the NKIF. The problem for that was to find an appropriate ensemble of forcing for the encave, so to validate the for the valid the encave. It's not so easy to create ensemble of a boundary condition Sample of a boundary condition so that the length scale, so the time scale for the signer at the entrance, will correspond to the given length scale. And so actually it has been necessary to develop some processes, and especially we have created an ensemble of forecast where the local time correlation has been set in a corner. as we have set in accordance with what was the we wanted to obtain inside the domain. We've done the same thing for this time a diffusion equation to consider the theoretically condition and again we were able to reproduce a large part of the ensemble vector and we And we also consider the case of Neumann boundary condition. So, that case, what is interesting with Neumann, is that the length scale near the boundary are infinite. And this is because if you look at some sample here of error inside the domain, if you are near the boundary, then you know what. You know what will be the signal at the boundary. So you have an infinite correlation. Correlation is one, sorry. And so the landscape is infinite. And that was what we were about to reproduce with the PKIF. Okay, so now the PKIF is able to apply for the forecast step, even if you have. Step even if you have boundary conditions. So now it will be interesting to see what happens here with recycle. So again, we consider our favorite transport equation with this form. It's not so easy in that case, but if you compact the term, you recognize the addiction of the variant, the addiction of the NHGRP, plus a term here that is very simple to understand. Here, that is very simple to understand. If you have a correlation in a flow with a shear, then what happens? This correlation is moved like this, and so you will have an X trophy appearing. So, that's why there is a source term in the dynamic of the anode troopy. For the illustration, here we have added some very small diffusion. Diffusion and so, what happened? Here we have the time, and we produce here the analysis. And along the time, we were able to reproduce exactly what the command filter provides. This is for the anisotropy that is diagnosed here for a ratio. Ratio. And again, for the variants, so you can see that the pre-caffe is able to reproduce the time evolution of the variants along the different cycles. So maybe just to point that there is, you see here, some difference between what is proposed by the installer german vector and what is proposed with the PKF. And that's an important point. Often, Important point. Often we consider that ansible method is a reference. And yes, it's true. We don't have something else. But sometimes, and actually most of times, this is not exactly the reference because it is polluted by model error. And actually this difference here is due to model error. So we can understand this. Understand this in view of the PKIF. That's an important point. If you want to solve this equation, then actually what you solve is not the addiction, but it is the addiction plus terms, plus terms, because we consider centered formulation, center finite difference for the special divided. That corresponds. So you want to solve this equation, but the numerical Solve this equation, but the numerical solution we obtain will be the solution of this equation. And if you do that in one dimension, for instance, so this is the mean here, variance in the NC, the mean is not affected by the model row, but you can see here that the variance predicted by the ensemble chemistry is in late. Is in late compared with what is provided by the PKF. So the PKF is wrong or the NKF is wrong. But actually, if you increase the resolution of the NKF, then this time the ensemble claimant filter corresponds to what is provided by the PKF. That's nice. So the PKF can be used to introduce an estimation of the material. This is exactly what. This is exactly what we observe also on the correlation. I won't detail this. But I want to go back to this dynamic of the intensity and of the model. So I explained, I introduced a little what is a modified equation, but I recall this. So you want to solve this equation, and so you propose a very simple numerical scheme, here, an Euler, a print scheme. But when you But when you integrate with this scheme, actually the numerical solution is the solution of this equation with here a diffusion term. So this is not exactly the same addiction that you prescribe and plus here a diffusion term that corresponds to the effect of the discretization. And because of that, you can That you can so, this is the effect when you integrate nature. If you consider a high-order scheme, time scheme, and space for the dynamics, and what is obtained with the Euler scheme. So, you can see that because of the diffusion term, the maximum of the concentration will fall because of the diffusion. So since we know exactly what is the dynamic of the uncertainty here sorry, what is the dynamic of the numerical solution, we can plug this equation into CPKF to provide the dynamic of the uncertainty as it really is and as it is really obtained from an ensemble Kerman filter. So this is the equation that the ensemble Kerman filter The ensemble chemical filter actually is performing. And from that, we can diagnose what is the evolution of the model error. And so you see that model error is not homogeneous in time, of course. We have to, for instance, here increase the variance at some positions at the variable. This issue also This issue has also been observed for real models. We shall perform life simulations that show that lots of variance occur for our system. And this can be studied, as I said, from view in the view of the parameter kernel, so that you can introduce some correction to Correction to ensure the correct transport of the virus. This view is related to what Shea present us some days ago. It is Shea present the dynamic of the covariance. So to obtain the dynamic of the variance, the diagonal. And here from the modified equation, actually, we try to obtain directly the We try to obtain directly the dynamic of the variants. So it is a nice, there is a nice connection which is to that. So this is nice. We presented some results in Univariate, but in real life we have multiple fields in interaction, so it would be interesting to extend the formulation in the multivariate plane. And this uh is what we done. This is what we done considering a very simplified air quality model, right? With a lot careful TR for the chemical schemes, with the transport here, so the antection. And this very basic model verifies some of the aspects we want to reproduce from a real chemical transport model. We have two species, so it is multivariate. There is non-linear dynamic. Uh there is non-linear dynamics uh because of the interaction between the two spaces and uh we consider continuous feed. This is very important continuous feed. I go back to this uh later in the confusion classic. So, okay, so again you plug this equation in CPKF and path you've got this one. So thanks to CPKF it's uh we don't need to go in detail. And you In detail. And you can see here that we have some closure term, the unclosed term, and we propose a proxy for the post-correlation functions that is based on the parametric lambda filter parameters for this formulation that has variance and the the anisotropy. And what are the results? Then compared again with the NKIF. With the NKF, we have been able to reproduce the time evolution of the mean of the standard deviation as well as of the band scale plus the close correlation between the two species. Then we have considered an equivalent, but this time for GRS, that is a simplified chemical scheme, where we were able to reproduce a part. we were able to reproduce a part of uh the dynamic of the intercity and especially uh you so it is quite difficult to to observe here but you have uh access to the uh to the slide so it you could here but actually I can see that the results correspond between the NKF and the BKF okay so it's time to conclude So, the interest of the PKF is that you know, ENKF ensemble came and filter is a very, very simple algorithm. You have a model, Laurenstrason 63, sorry, your ocean, your chemical transport model, what else. You can take this algorithm and poof, it runs. But you know, it's not, life is not. But you know, it's not, life is not always this simple, and it is costly, so you don't need to think about what you do, run, run, run millions of. Okay? This time, but yes, okay, but what is the difference between Lorentz 63 and equation of the ocean? Well, it's not exactly the same equation. And actually, And actually, this difference, you have ordinary differential equation in that case, in one case, and partial differential equation in the other case. And this is never taken into account when you consider the interval current filter. So you lose something, and you stay in linear algebra. But continuous is very nice. We can do many things we can't do with linear algebra. So consider this time that we have our. Consider this time that we have partial differential equation, that is continuous field, differential field, we are able to simplify actually the dynamics and the PKF relies on that. So to conclude, the PKF is able to reproduce the time evolution of the uncertainty. We have proposed some assimilation of the parameter. In the case Of the parameter, in the case we got the variance and the antotropy, and provide some tools to help the exploration of this formation that can be generate code or differential equation as well as helping you in machine learning and and physics. Okay. We tested on boundary. On boundary domain, on bounded domain, but there is a lot of work to do, especially related to the multivariate system. And so, especially for meteorology, it would be nice to look at what happens with shallow water or what else. And there are, I guess, many applications we don't have in mind, and so maybe it will be And so maybe it should be about that. And uh in particular, I think it could be very nice for sensitive analysis as well as as targeting because we can at low cost predict what is the evolution of the in sarcity and so what is the effect of the observation and so okay, thank you very much. Hope this tour was not awful. I want to go behind your bus. Thank you, Olivia. Nice, all of you. I have a very specific question. If you go back to slide 29. Oh, sorry. So the second order closure doesn't give a result on a length scale, right? Yes. So I'm wondering, is this because the correlation curvature changes sign in the second order approximation? Do you remember what causes the NINs that you're getting in this case? Yes, actually, here. So you have a metric tensor that is a symmetry. Tensor, that is a symmetric and positive definite tensor, for the forecast. And with this simplified relation, since here you just have a fraction, so it is a ratio between the analysis variance error and the focal variance error, you do not change the nature of this symmetric matrix. So here the analysis. The analysis entropy is still a matrix. But actually, here there are other terms, and I don't know exactly if this is due to numerical errors or, well, I'm not known for the moment. But sometimes we can have, yes, negative spectral value. Spectral value, yes. And then if you calculate the correlation like NIN, because yes, I'm not sure. So maybe this formulation here is more robust in application. Why? It is not the true update of the what was nice, and thanks Risha. I thought we were not able to compute the update at the end of today. The update of the M should be and to be sure you should. Okay, and oh, yes, we can do a precision about how to use the neural network to estimate the parameters. In fact, we need to know the difference form, the exact form, so that you will know the truth. Several terms. So here we have proposed a little dictionary of Switzerland that were related to the closure, the analytical closure we obtained. And so we just need to find three terms, and that's all. And that's all. But in real life, where you don't know, you have no idea about what it is, there is two possibilities. First one is to consider alter that have the same physical dimension to what you are looking for. And then the dictionary is this time much more important, and then you estimate the dictionary. So So it corresponds to the idea when you want to merge all parts of the physics, the information coming from the physics, as this kind of torch, and machine learning. But since you are in your favorite framework, TensorFlow, Kieran, SpriteTorch, you can, in place of this turb, consider a usual neural network, maybe a unit or whatever, plug in this term and In this term, and this will be taken into account in your implementation in the dynamics. So you can do what you want. And it's just a translation of the differential equation into data. So this approach is rather than taking a model discrete type. Taking a model, discretize, making an ensemble of it in order to have the covariance properties. Model. From there, you calculate the covariance PDE revolution and then you discretize. That's the basis, right? And it turns out that for chemical data assimilation, where transport is very important, that leads to a huge simplification compared to the Huge simplification compared to the enzyme will come. So Sina will talk about that. But basically, it's a way also to permit to have more of the conservation laws into the covariance dynamics explicitly written rather than try to get the ensemble to create that for you, which does not always work. Yes, and the cost associated to the approach is very low compared to the ensemble calm stator because you just need to solve three equations. So it is equivalent to a cost of three models, more or less. Okay. Thank you very much.  I haven't subscribed to Text. Oh, but I had a couple of technical questions. I was wondering if you had time. Maybe not. If we could chat. I had very specific questions. I didn't think there were anything. Yeah, sure. Well, yeah, yeah, sure. Yeah, sure. Well, yeah, yeah, sure. I'll just be able to do it. Yeah, but the discussion session will be much longer. And then uh China is not data, so just So I'm gonna take so Nietzsche. I'm gonna take just three minutes. Yeah, can we ask?