I'm Koisi Joseph Jahini, a postdoc from Argonne National Laboratory since January 2022, under the supervision of Dr. Stephan Wilde. And today I'm going to talk to you about one of my recent PhD research supervised by the Professor Sebastian Legigabel from Polytechnic Moreal and Michael Cooperas from Media University. So I'll be talking to you about constraint stochastic blood boss optimization using the progressive barrier and probabilistic estimates, which recently appeared this year in the Mathematical Programming Journal. Programming journal. Here are the main points of the presentation. I will first talk about the general framework of blood boss optimization, followed by a presentation of the MAS algorithm whose stochastic variant is the Stomach's PB algorithm developed in this research, followed by a presentation of the progressive barrier approach used to handle constraints, and then present the Stomach's PB algorithm itself, followed by a presentation of some convergence results, and then finally some numerical results. What is the blood boss opposition? Is the blackboard subposition? So, according to the written free and black box optimization book by the professor's audience, here it is the study of designs and analysis of algorithms that assume that the objective and constraint functions are given by black boxes. And a black box is any process that, when provided, an input returns outputs, but the inner workings of the process are not analytically available. And the most common form of a black box is a computer simulation. In a deterministic framework, a blackboard optimization problem is mathematically for. Optimization problem is mathematically formulated as the minimization or the maximization of an objective function f subject to constraint CJ of s less than or equal to zero, where f and the cj constraint functions are given by black bosses and whose derivatives are not available. Now in some situations the values of f and the cj constraint functions can only be computed numerically but with some random noise thus leading to concern stochastic blood boss optimization and they are denoting by fetal zo and ceteth j the noisy compatible versions of Noisy comparable versions of F and CJ, especially we assumed that F is the expectation of FO and CJ the expectation of C teta J, while the teta J are independent unknown variables whose distribution is supposed to be unknown. And here the subset G of Rn consisting of variables for which no consent is valid is called the feasible region or feasible set. So from here we have to bear in mind that the Stomach-PB algorithm developed in this work is not applied directly to F. Applied directly to f and the cj functions since they are not numerically available, but put the noisy variants f theta 0 and c theta j for minimization in order to find a solution for this deterministic problem. One of the most common ways to model a noisy function f theta is to define it to equal a deterministic function f plus a random variable whose distribution can possibly depend on x. Here is the graph of a deterministic function f and the one corresponding to a realization of each. Of each node invariant and two-dimensional illustrations of feasible domains here in white, respectively with respect to the deterministic constraint and corresponding realizations of the Leonosian variants. So, here, for example, the Stoman's PB algorithm will be applied directly to F theta 0, C theta 1, and C theta 2 for minimization in order to find a solution for this problem we have here. This must be the algorithm developed in. The stomach PV algorithm developed in this research is a stochastic variant of MALDS. And MALDS is defined as MeSH Adaptive Direct. It's a diaspora algorithm developed for deterministic black box optimization, which achieves descent by moving in the directions of best points, making use of positive basis or positive spanning set. At each iteration, the mass algorithm generates trial points on a description MK of the space of variables called the mesh. space of variables called the mesh. And in this definition, xk is the current solution, delta km, the so-called meshes parameter, while the columns of the matrix D form a positive spine set. Each iteration of mass is composed of two steps, the set which is optional and the pole the most important and on which relies the convergence analysis. During the pole step, trial mesh points are evaluated inside the region delimited by a frame FK of extent the FAKT, the so-called frame size parameter, and an iteration. And an iteration of the Muslim group is called successful whenever a child mesh point with an objective function value lower than that at the current solution is found. In this case, the head point here, Y, we become the current solution at the beginning of the next situation, where both the mesh and frame source parameters are increased. Then 12 mesh points are generated again inside the frame around that solution. And if none of them improve the value of the LDT function, then the iteration is called unsuccessful. Is called unsuccessful, and in that case, the coin solution is not updated. While both the mesh and frame sets parameters are decreased, and so on. Now, how are we going to ensure decrease in the adjacent function f in a stochastic framework where the exact deterministic function values are not available? So, here the information is provided by estimates f0k and fsk of the unknown f function values respectively at xk and enabling points. Let's call it xk plus direction sk. Plus a direction escape. But here, a simple decrease in these function estimates should not necessarily lead to a decrease in f, not necessarily. That can happen sometimes, but not necessarily. However, making use of so-called epsilon accurate estimates introduced, used in the storm paper, for example, we proved that for a decrease in F to hold, then a sufficient decrease in this epsilon accurate estimate is required. And according to Estimates is required. And according to this definition, we can also obviously see that the accuracies of these function estimates F0K and FSK are dynamically improved by the sequence of frame size parameters, which in fact is supposed to converge to zero. Now you can be wondering how these estimates F0K and FSK are made available. Well, we consider, for example, a random, let's think about the deterministic estimate F0K. So we consider a random estimate. Consider a random estimate L0K defined to equal the mean of PK random samples of the noisy objective function. While the sample size PK, depending on the, for instance, parameter, the data Kp can be, for example, chosen using the classical THF inequality, so that once we replaced the deterministic estimates F0, K and FSK with the random variance, then both inequalities hold with some high probability. Hold with some high probability. And then we simply define the deterministic estimate at zero k like a realization of an observation of the random estimate at zero k. Now let's talk about feasibility. The constraint version of the MALS algorithm, whose stochastic variant is the Somat PB algorithm, uses a progressive barrier approach to handle constraints by introducing the so-called constraint variation function H. Constant valuation function h and here according to this definition we can see that h is a non-negative function with h of x equals zero corresponding corresponding to physical points that is those in the physical region d since in that case cg of s will be less than or equal to zero means that the max c i is zero and then h is zero the progressive barrier approach into uh introduces a non-negative barrier test h k max such that the corresponding sequence is non-increasing then Is non-increasing, then any sale points which is outside the set X here or whose constant variation function exceeds the value threshold is rejected. Now, it's very important here to observe that H, even in the Stocacy framework, is defined the same way using the CJ confirmed function values, which are unavailable. And the good question is: how are you going to ensure decrease in that function edge? Since in fact, we need to impose. Since in fact, we need to improve feasibility and we also need to derive results related to that function h. So, how are we going to do that in a stochastic framework where the constraints are also noisy? Well, at a given integration, assume that we want to check for decrease in H. So, let's consider two points xk here and a neighboring point. Then we do something really easy. For example, we For example, we use epsilon accurate estimates of the CJ and all constant function values, and with those estimates of the constraint values, we introduced so-called epsilon reliable bounds on the unknown H-font values. So, what we do is the following: once we know that USK, the epsilon-reliable upper bound, is strictly Is strictly less than L0K, then that means that the value here is strictly less than this one. So that is something extremely simple. So we ensure decrease in H, even though we don't have access to the exact H function values. And we can also observe something else. Since H is a non-negative function, then whenever I u0k here is equal to 0, then equal to zero, then that means that h of xk equals zero. But in that case, we cannot claim that xk is feasible since this conclusion is derived using inexact information. So we just say that xk is epsilon feasible. And if xk is not epsilon feasible, we say that this is epsilon infeasible. Here is an illustration of the epsilon feasible notion. We consider the only constraint function here in green of the function here in green of the disk of the two dimensional version of the disk problem whose corresponding feasible region is the yellow disk then we noisy it using a random variable uniformly generated distributed in minus 33 and in each box we generated 3000 points that can be possible mesh points at a given iteration and for each of these points except those of the first box we computed the corresponding epsilon reliable upper bounds using TK noisy Blabos evaluation. Using PK noisy Blavoss evaluations. So here the red points correspond to feasible points, but with respect to the noisy constraint. And that's why many of them are outside the two feasible region in yellow. And for various values of PK, the blue points correspond to epsilon feasible points that can be used by this Tomato-PB algorithm. And here we can observe that the larger PK, the more epsilon feasible points become feasible. And it's very important. And it's very important to mention that here we used pk equals 1000 and even 150 only in order to show that epsilon feasible points become feasible asymptotically while such a large values, such large values of PK are not necessarily required in practice in order to obtain satisfactory results with the stomach BB algorithm. So far, So far, we saw how to ensure decrease in H using the epsilon reliable bounds previously introduced. More precisely, we just saw that once we have just look at the green inequalities first. So once we have USK, which is strictly less than L0K, then we show decrease in H using the green inequality here. The problem is that this The problem is that these simple decreases do not really help to prove the convergence of the sequence of frame size parameters to the zero, which is in fact very crucial in the covenants analysis. So that means that we need something, we need more sophistication, we need something more powerful than simple decrees, and that powerful thing is a sufficient decrease. So we introduced estimate. So, we introduced estimated violations of the unknown H1 values respectively at XK and enabling point. Then, by using the previously introduced epsilon reliable bounds, we prove that for a sufficient decrease in H to hold, then we need a sufficient decrease in the estimated violations. So the red inequality here is the one we used in the algorithm to check for decrease in H and to improve feasibility. All these results allow to define the iteration types of the stomach PV algorithm, which can be either H-dominating, F-dominating, improving, or unsuccessful. And here is an illustration of H-dominating iteration. So at the beginning of this iteration, I assume that the only solution is epsilon infeasible. Then we compute the barrier threshold to be equal to the corresponding epsilon. Equal to the corresponding epsilon reliable power bound so that while improving the epsilon infeasible points, then the barrier threshold will be pushed towards zero, thus improving feasibility. Then we generate all points in a frame around the quantum solution and compute estimates of unknown function values. So assume that all of these points are epsilon infeasible. So this one, for example, leads to sufficient decrease not only in the estimated valuations, but also in the estimates of the In the estimates of the objective function values, meaning that a decrease occurs both in H and F. So, for that reason, the iteration is called H dominating. And this point here in green will become the epsilon feasible solution at the beginning of the next iteration, while both the mesh and French parameters are increased. There are two types of F-dominating iteration. So, assume that at the beginning of this iteration, the only solution is epsilon infeasible again. Infeasible again. Then we generate our points in a frame around that solution and compute estimates of unknown function values. This point is rejected because the corresponding epsilon reliable upper bound exceeds the barrier threshold. Now for this one and for the first time in the algorithm, we found the point with u k equals zero, meaning that that point is epsilon feasible. So we keep it like our first epsilon feasible solution for the beginning of the next situation. Solution for the beginning of the next situation. And in that case, the epsilon infeasible solution is not absurd, where both the mesh and from size parameters are increased again. Now, regarding the second type of F-dominating iteration, here we assume that at the beginning of this iteration, we already had both types of solutions. I mean, an epsilon feasible solution and an epsilon-infeasible one. So we generate a grain tile points in frames around those. Points in frames around those points and compute estimates of unknown function values. Here, something really interesting happens is the following. By exploring around the epsilon infeasible point, we found a point with uk equals zero, meaning that that point is epsilon feasible. So, we are no longer going to compare it with the epsilon infeasible point, but with the epsilon feasible one. And here you can see that a Can see that a sufficient decrease occurs in the estimates of the adjacent function values, meaning that we have a decrease in F. So that point will become the epsilon feasible solution at the beginning of the next iteration, while the epsilon infeasible solution is not updated and both the mesh and frequency parameters are increased again. Now, if the iteration is neither F dominating nor H dominating in bed, at least one of the That at least one of the previously evaluated epsilon infeasible points leads to a sufficient decrease in the estimated violations, then we have what we call epsilon feasibility improvement because we at least improve feasibility. So the iteration is called improving. And if the iteration is neither improving nor dominating, then it is simply called unsuccessful. And in this case, and this is the only case where the above the mesh and frame set parameters are decreased and no. Are decreased and no incommon solution is updated. The convergence analysis of the Stomach PV algorithm strongly relies on the theory of Stochacy processes. So I'm going to explain quickly how the method results in Stochacy process. First, we call that the objective and the constant functions are noisy. So at each iteration, we compute estimates of unknown function values. Let's just talk about the 10th iteration. So we run our algorithm the first time and we compute f 10 0. If we run the algorithm a second time, the third time, 1000 times, we are going to obtain possibly different values of f 10 0. So those values of f 10 0 can be considered as realizations of random only, which is the big f 10 0. So now think about So now think about an algorithm which uses random polynomials, F0K, FSK, and so on, in order to decide for updates. So obviously the behavior of such random pointers will influence each iteration of the algorithm, which will thus result in the sequencing process. So in our theory, since the estimates are no longer deterministic but random, their accuracies need to be quantified probabilistically. So the estimates F0K and FSK are. The estimates F0K and FSK are required to be beta probabilistically epsilon, and the bound, the random bounds, are required to be alpha probabilistically epsilon reliable. And this is the definition. So we say that the estimates F0K and FSK are better probabilistically epsilon accurate if they are epsilon accurate following the deterministic definition, but with some probability at least better conditioned on the past history of the algorithm, since they are no longer deterministic but random. And we see. And we say that the random bounds are alpha probabilistically epsilon reliable if they are epsilon reliable following the deterministic definition, but with a probability at least alpha conditioned on the past history of the algorithm. Ideally, we would like these parameters alpha and beta to equal one, but such a requirement should be very difficult to meet in practice. So the very natural question here is how can these parameters alpha and beta be chosen in order for the algorithm to converge? Algorithm to converge. So the choices of alpha and beta are made in this theorem, where we also prove that the sequence of random frame size parameters converges to zero with probability one, or equivalently, the sequence of random mesh size parameters converges to zero with probability one. A result which means that the mesh used by the Stomach-PV algorithm becomes infinitely fine, which is known as a zero-to-order convergence result. And we also prove that the Stomach-Pv algorithm generates a subsequence of epsilon infinity. Subsequence of epsilon infeasible solution that converges to a Clark stationary point of H with probability one, which is a necessary optimality condition. And we derived a similar result for F. Regarding the numerical experiments, they are conducted on 126 stochastic problem instances of 42 constraint problems additively perturbed using mean zero and uniformly distributed random variables. At each iteration, At each iteration, the estimates are computed using NK-Noisie Blair Boss evaluations and making use of available samples from previous iterations. Then, four variants of stomach PB corresponding to NK equals one, two, three, and four are compared with MANS using a progressive barrier approach, where the constant variation function H is defined by means of an LP norm with P equals one as in the stomach PB format and P equals two. From white and t equals two. These figures show data profiles comparing the variance of the SOMASPB algorithm with those of Mars for various noise levels quantified here by the parameter sigma. The horizontal axis shows the number of noisy blood boss evaluations divided by n plus one, while the vertical axis shows the proportion of problems solved. Here, all the algorithms were applied. All the algorithms were applied not to the f function but to the anoisy variance f theta zero. So all the points visited by the algorithms were recorded in order to compute the true f function values and those function values are then used to generate the data profession. Here you can observe easily that all the curves of corresponding to the stomach PV variance are above those of mass towards the end. MADS towards the end, which means that generally the SOMASP algorithm outperforms MADS in a stochastic framework with stochastically noisy objective functions. So thank you very much for attention.