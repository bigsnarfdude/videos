Good workshop everyone. So today I want to talk about something I think that's sort of transpired throughout the workshop and you know has been it's something that comes up in many places when you talk about the statistical problem, which is the relation between search and decision. So for example in the quantitative click problem you can formulate it in two ways. So here I'm talking about planted instances of problems. So one is the distinguishing problem. One is the distinguishing problem where I have a random graph, and then I have a random graph with a click in it somewhere, and the task is to decide which I'm looking at. So it's the distinguishing problem. And then there is the recovery or the search problem, which is to find the planted object in the click. And, you know, like, so we call them distinguishing versus recovery. Maybe in statistics, we call them hypothesis testing versus estimation. It's the same problem. And what I want to talk today is about how these two. Today is about how these two problems relate to one another and why we can expect them to relate to one another at all. And you know, sort of we come from cryptography, and from cryptography, we're actually more interested in the regimes in which these problems are hard, right? And the reason that this is interesting is because sort of hardness, there are two different notions of hardness here. There is the hardness for search, which is hardness for finding the instance, which is like in the cryptographic setting, it corresponds to having a one-way function. Having a one-way function. And then there is the hardness of distinguishing, which is like: if I cannot distinguish two distributions, then I get some unpredictability. I get a bit, which is deterministic. I know which, like, it's determined by the instance, but you as a computationally bounded observer cannot tell whether it's like the null bit or the planted bit, which one I picked. And this is the notion of pseudo-randomness, and this is what you really want for many of the cryptographic applications, as we saw in the other talks. In the other talks. And I think what's really surprising is that, you know, like if you think about these two notions, like getting computational hardness seems like something that's much, much easier, right? Like if I take a glass and break it, then putting it up, like gluing the pieces together is hard, right? But pseudo-randomness is something like, you know, like, as Tolstoy would say, like, every pseudo-random, you know, everything that's not pseudo-random is not pseudo-random in its own ways. There are like a million ways in which you can try to distinguish a string from a random. Distinguish a string from a random string, right? You can look at this bit, you can look at the correlation, you can look at the properties, and so on. And I think it's really surprising that these two things, you know, these phenomena are related, that they, you know, there are a lot of instances where we either have both or we have neither, right? So that's what I want to explore. And so what's sort of, so motivation is, right? So if you look from the hardness perspective, you know, I want to prove that something is pseudo. I want to prove that something is pseudo-random. Well, I would like to reduce it to proving that it's hard. Why? Because hardness is a lot of the time easier to analyze. And even within these communities, we have tools that deal specifically with hardness of search, like analysis of solution space, overlap gap property, and stuff like that. And it would be great if we could say, well, because the solution space looks like this, we can say that we also get pseudo-randomness for freedom. Right, and I'm actually cheating a little bit because the examples I'm going to talk here are trivial from the perspective of solutions-based analysis in the sense that all the problems I will talk about today, at least, are in the injective regime, where somehow we have so many constraints that the solution is uniquely determined by the constraints. But one can somehow, you know, like if we do the five-year plan, we can think about extending it to also problems in which we have non-unique solutions. But I think it's a really fundamental question to kind of relate the complexity of search and the complexity of distinguishing. And why do we even believe that this is possible, right? And somehow, like, complexity theory gives us unrealistic expectations here. So why does complexity theory give us unrealistic expectations? Like you open a complexity theory textbook or go on a complexity zoo, and they only talk about decision problems. Like there is very little that they talk about search. And the reason it is like that is because one of the things that you don't even mention often is that That you don't even mention often is that, like, in worst-case complexity, decision and search for NP, like NP-hard problems, are equivalent, right? And a lot of the problems that we deal about is like if we formulate it in the worst-case setting, they become NP-hard, right? So then we can say, well, I can reduce to SAT, and SAT has this third-to-decision reduction where, you know, by fixing bits, I can always recover the solution. So I can solve it that way. So that's kind of, you know, you can interpret it as evidence as to why you might expect decision answers to be related. Decision answers to be related, but as we know, a lot of these things that work in the worst case breakdown in the average case. So, I think what's more important is there is this other kind of two results in complexity theory. Especially the first one is this basic theorem that was first proved by Hasta Dimpayazzo Lebrid and Louvi that says that one-wave functions are equivalent to pseudo-random generators. And what this means is that if I have any hard search problem, Hard search problem, then from it I can build a pseudo-random bit. I can get something that's indistinguishable from random. But the reason this does not apply directly to ours, I think, is because you have to work pretty hard to engineer this pseudo-random generator. It's not a natural thing that you would come about. And in, you know, in sort of if you kind of look at different notions of hardness, there is also this theorem by Disson and Witherson, which the reason I mentioned is more because. Reason I mentioned is more because the techniques that are used in proving that theorem I think are quite relevant for trying to understand the connection between hardness and randomness in the statistical inference setting. And I'm not going to formulate this theorem like it's kind of if you read the theorem, it's a bit heavy to read it, but essentially like one consequence of it is that if I have a function, say that you know you have to evaluate it sequentially many times to get the answer, and that's the best way you can do, then out of that you can get. Well, then out of that you can get pseudo-random bits. But the notion of pseudo-randomness that they get is quicker in the sense that actually I have to work very hard to get my pseudo-random bits. I have to work harder than it takes to distinguish them. Right? But it's some kind of evidence again that we can get the randomness out of ROS. Okay, so what about statistical inference problems? Okay, no, one more thing I wanted to say is that at the heart of both of these theorems is this kind of super simple but really, really remarkable. Really, really remarkable fact called Yaus Lemma. And what Yaus Lemma says that if I have any distribution x and some y which is some label y which is correlated from x, and the problem of distinguishing this from x together with the completely random label uncorrelated with x is equivalent to the task of predicting y from x. Okay? So, you know, like we kind of don't even say this, but in fact this is a like really, really subtle result. Like really, really subtle result. Like, I've taught it many maybe ten times and every time I did it, I can't remember how it's proved. It's it's one of those things that you have to scratch your head to remember how to do it. And like one kind of piece of evidence why it's so remarkable is that it really only works when y is a bit. That if you try to extend it to larger domains, you're going to fail. So it's really, it is, it makes everything go through. So it kind of relates the problem of telling two things apart. The problem of telling two things apart to the problem of finding something, right? At least distinguishing the stock market from random doesn't help you make money. Maybe a little bit, right? Sorry, so what's the exact standard type distribution? So you have a joint distribution of example x, it can come from any distribution, and y is just a single bit which is correlated with x. So if I replace this bit with So if I replace this bit with a random bit which is uncorrelated with x, suppose I have some way of distinguishing these two, then I can predict y. Just from x, without seeing the random. You mean like there's an algorithm? There's an algorithm that with probability whatever one half plus whatever what the distinguishing advantage is, outputs y. Yes. From a larger domain, can you try to distinguish whether x and y are come from a product distribution? You can, but the advantage shrinks by a lot when you You can, but the advantage shrinks by a lot when you do that. So, yeah, so you have to pay a price over large domains. You get very, very little advantage. Good. So, okay, so what about statistical inference? So, what I'm going to try to say is like, try to draw some lessons about, and a lot of what I'm going to say about is known. I'm going to try to say one new thing, but it's actually not been written down. So, this is the audience to ask if it's really new or not. But I will, but so I'm going to do it on these four examples. To do it on these four examples, I'm going to look at dense LPN. Good thing that Ayush and Rachel defined everything for me. So this is dense X4, dense random linear equations modulo 2. So here I'm looking at the planted problem. The right-hand side is obtained by planting an assignment and maybe adding a little bit of noise, but between friends we don't need noise, right? This is hard enough already. So the second one is like the sparse version, where I have random linear equations with just say k variables per equation. We just say k variables per equation. I'm going to talk about click also, planted click, and in the end, maybe say a few words about Poltrice function, which if you don't know, I'll explain it, but it's not so important. It's a kind of a generalization of this LPN. So, okay, so how does decision, how do you relate the hardness of decision dens from dense LPN? And this is a very nice idea. I think, like, we not reminded me that this was the first time. Be not reminded me that this was first done by Michanchio and Mo in the context of LWE, but it's essentially the same argument. So I just want to show you this proof. If you don't remember anything, I think this is what you want to get out of the talk. So how does it work? Well, we want a reduction, right? We want to say that if I can tell a system of linear equations when the right-hand side has a condit solution, P and when it has a completely random solution, R, then I want to find the assignment X. So how do I do it? So, how do I do it? Well, suppose that I have some distinguishing advantage delta, so there is some distinguisher D that has advantage delta. And this is the first trick. Actually, we already saw it in David's talk when he talked about Lindenberg. The way that you do it is that you slowly go from one distribution to the other. So I'm going to somehow replace this one with a zero, replace this zero with a one, and so on. And the point is that at some point, like the distinguishing advantage will have to jump by. advantage will have to jump by delta divided by the number of equations, right? So what does this say? This says that there is a specific equation here so that when I made this change, the distinguishing advantage went up by a factor of delta divided by m, divided by the number of equations. And the point, and now the point is that because the right-hand side is random, I can totally eliminate all the equations that come after it, right? Because the right-hand side is completely. Because the right-hand side is completely uncorrelated with the left-hand side, those don't give me any information, so I can kind of sample the right-hand side on my own. So, what do I end up with? From the distinguisher, I actually come up with this, well, that's exactly the setting of Yao's lemma, where like condition on these first so-and-so many equations, in the next equation, on the one hand, I have a distribution which gives me the true value under the planted distribution. On the other hand, I have a completely random bit, and I can distinguish these two. Random bit, and I can distinguish these two. So now it tells me that now I can predict. So, what can I predict? I can predict the value of a random XOR given that I have some XORs as training samples. So, that's what it tells me. So I get the predictor. It doesn't tell me what the value of XOR is exactly, but I can query it. I can say what should the value of X1 plus X3 plus X4 be? And it gives me a guess. And I know that this guess is going to be correct at least. Guess is going to be correct at least a delta over n fraction of the time. Okay? So now, well, the task we have remaining is to turn this guesser that guesses with some advantage delta over f to a solver, something that always gets the right answer. And this is a coding theory problem, right? So we have some kind of very noisy encoding that I can query at different inputs, and it gives me a prediction of the output, and I want to get it at every input. In this case, In this case, the code that we have is the Hadamard code because I can query it essentially at any XOR. I can take any equation and I can query what is the value of this equation and I can get a prediction for it. And the Goldberg-Hevin theorem tells us that the Hadamard code is list decodable. So what it allows me to say is that given this predictor, I can get kind of a list of not too many guesses. Like the size of the list is the inverse of the list. Of the list is the inverse of the advantage squared, it turns out, so that I know that among these guesses, one of them must be the solution that I'm looking for. Okay? So that's, so that's, so those are the two ideas that go in. Right? You do the reduction and then you decode. And yes. I'm really sorry. Like, can you just precisely state the claim that you just. Sorry, precisely state. Okay, so there are two parts, right? The first one is going from a distinguisher to a predictor. So, what is the predictor? The predictor is something that gets a sample, like as a training data, a sample of at most M equations, and gives a prediction for a new equation. And this prediction is correct with probability delta over m over the choice of input. So, the input here is the equation itself, the sample. Okay, so that's. Okay, so that's that makes sense or how did I predict the new bit? Using Yaoslan. Because you could distinguish. So there was a use of Yaoslan. Yes, that's crucial. That's where it happened. So that's really where you go from randomness to hardness. And then the next part is you just kind of want to get a full solution. Now I have something that gives me a signal about the solution. I want to get the full solution. Like, I do decode it. Okay? So that's. Okay, so that's it's beautiful, right? And there are two great things about this reduction. The first thing is, like, for this community, is that it's sample preserving, right? I need the same number of samples for distinguishing that I need for prediction, maybe even a fewer. And the second great thing about it is that there is no restriction on the advantage, right? Even if I have something that distinguishes with tiny advantage, I get something that, you know, by working as hard as like inverse proportional square with the advantage, I can get. I can solve. I can extract this image. Okay, so that's kind of the ideal, right? That's what we want to get. But what's so special about the dense LPN or dense XOR? So now I want to look at sparse XOR and say sort of what would happen if I try to apply this reasoning to, say, KXOR. Okay? So the distribution I'm going to look at is KXO equations where it's going to be important in this case. You can think of like each pair. In this case, you can think of each variable in each equation being sampled independently at random. So I might get equations where I get the same variable repeated twice. It's a lot of mistakes. So I try to go over the same reasoning. I say, well, I can distinguish one with the quantity assignment from one with the random right-hand side. So I get a predictor. What does this predictor do? This predictor tells me now, if I train it on the first so-and-so many equations, it allows me to predict with advantage delta over m what is the value. Over m, what is the value of the next one? Okay? And now we are in trouble, right? Because, well, now I can predict, but I can only predict like, say, 3xO equations. And the difference between this setting and the dense setting is that 3xO is not a good code. So I cannot hope that from just predicting these 3XOR equations, I have enough information to extract the complete assignment. There are too many possibilities there. Right? And sort of there was this very nice idea by Benny Applebaum in this paper in 2012, where he said, well, you know, the way that I'm going to get past this difficulty is by amplifying. So I'm just going to train it on many, many sets of equations. So I'm going to amplify this advantage from delta over m to something which is very close to one. And if I have something that tells me the values of equations with advantage very close to one, then I can certainly figure out what the input was. only figure out what the input was. Okay, so I'm just going to repeat this training process on many independent equations and this will give me, well, I'm hiding something, like there is important property of this problem, the symmetries between the variables that you use to ensure that these predictions will be, that, you know, that the learning will be independent so that you can amplify it. But anyway, hiding some details over the rough, you can make this work. And it's it's a very nice result. It's great. Are we the same words loosened access to? Solution that exists uniquely. That's the render, and it might have so we don't know. It's a reduction, right? If it's not unique, it might give you garbage, right? If it helps. Our goal is to produce any solution from distinguisher. Our goal is to produce one of the solutions? It's to produce one of the solutions, and the reduction, the place where this will be effective is like the regime where the solution is. So let's look at that. So, how does the amplification work? You just train it independently on m over delta squared. M over delta squared independent sets of samples. Like you're sub-sampling the samples? Think about dividing the set of samples you have into batches and then you train it independently on the different batches. You have to use some, like you have to add a bit of randomness, which I don't want to talk about. But in the end, like you can think about if each batch gives me advantage delta over m, if I repeat so many times, the advantage goes close to one. But what did you get? But what do you get, like, if you're dividing by? I mean, you're dividing M, so then your advantage of your sigma gets better? Yeah, no, so okay, so I have a distinguisher that works on a small sample size. To get a solver, I'm going to give it a much larger sample size. And now I'm going to divide this large sample size into small chunks, and I'm going to run the distinguisher independently on all of these small chunks. I mean, it feels to me like you're removing all those things. Feels to me like you're moving your goalpost because once you get it. You are, yes, exactly, exactly. Okay, that's that's a very good point, and that's what I want to address. Is that no, this is not a sample complexity-preserving reduction. Right? It's not going to be a sample complexity. From an arbitrary small advantage, it has to be bigger than. Good! That's the other point. Now, there are like these two great features of the previous reduction, none of them are here anymore. First, you need a lot more samples, and second, it doesn't work when the advantage is small, and it cannot work when the advantage is small. And it cannot work when the advantage is small, because we know that we can distinguish kxor. For example, one thing that can happen is we can get a collision. We can get the same equation up here twice, in which case we distinguish with constant advantage. This is an event of non-negligible probability. So somehow the argument has to account for this. So those are, you were ahead, so you asked the exact two questions. It doesn't work, it doesn't work for the reason that I used pointed out, because you cannot. Because it simply cannot work for small advantage, but it gives us something, right? It tells us that if I have a really good distinguisher which works with a very small amount of samples, then I get a search algorithm which works for a larger amount of samples. So it does something, but it doesn't really explain the way that this community likes to explain this. Where is the gap? Like, why does the gap happen in the same place? So where is the gap for K for for KX1? For Kx1. Like, how many equations do I need to get to be able to distinguish random from planted? Ravesh? He's not saying this comes. And to the career two. And for search? Same. Okay? And this doesn't explain it, right? So it's not, it doesn't tell us, right? If I run this. It doesn't tell us, right? If I run this on slightly fewer than n to the k over 2 equations, it gives me nonsense. It gives me that I need more than n to the k equations. Okay, so this was something that was bugging me. And okay, so I had thought about this for a while. And I thought, well, the reason that this reduction is not good is because the decoding is terrible, right? You have to do this amplification. What if we can decode better? And the code is not a good code. And the code is not a good code, like it's a 3xOR code, so we cannot expect to list decode from it. But what we can expect is to do something called approximate list decoding. We can say, well, I don't want to extract the exact solution, but I want to extract something that has good enough correlation with the solution. And then I can try to fix this correlation. Okay? And that's this paper that I wrote with Manuel Sabin and Prashant Basuderman in 2019, where we do that. We actually figure out how many samples we need. out like how many samples we need. So it goes from something like m cubed to m to the 1 plus 2 over k. But in the end, this paper ended up being not satisfying at all. Because to make everything work, we had to compromise a lot. And the way that we had to compromise is that it doesn't always work. It only works with a tiny probability over the choice of the instance. So I will spare you the detail. I just want to point out that, you know, this was that idea that on the surface looked great, but when we try to implement it, it didn't give us what we did. Implemented, it didn't give us what we wanted. And even if you can get the algorithm to always work, it still doesn't explain the threshold, right? We still get a loss in the number of seven points. So it ended up not being very satisfying this. So what would be satisfying? Let's move on to click. So a couple of months ago, I happened to talk to Shuichi Hirakara and he told me they have this new work about click. So what happens in click? What's the comp stat get for click? In click, what does the comp stat get for click? You know, how, how, how, how, it's like I'm planting a k-click in an n-vertex graph. So, when does the click show up for distinguishing and for search? Square root of n, right? Fold square root of n. And what is the advantage of the best low-degree distinguisher? Well, it's going to be something like k squared over n. Let me cut the suspense here, right? If I'm looking for a clique of size k in an n-vertex graph, click of size k in an invertex graph, like the best I can do is somehow count edges, right? And this would be the standard deviation for the campaign. What do Hirahara and Chibisu show? They show this amazing result. They show if you can do a little bit better, a tiny bit better than k squared over n, so if I can distinguish with advantage less than k squared over n, well, bigger than. Thanks n to the minus gamma, then I can do search. I can completely recover the click. Okay, so this is what we want, right? This explains the gap. It says that if the distinguishing algorithm, if you do tiny better than trivial, then you solve the problem. You solve the city problem. Okay? So I'm not going to tell you how they do it. I think their paper will appear soon, so you should read it. It's really great. I want to now. It's really great. I want to now say: well, can we do something similar for KXORs, right? So, what would be the dream reduction that I get for KXOR? What's the low-degree distinguishing advantage for KXOR? Well, in the clean case, you know, I would count edges. In the KXOR case, at least the way I set it up, the best low-degree distinguisher is the one that looks for collisions. I look for the same equation appearing twice, and I see, you know, that's the advantage that I get. So, what is the probability of the same equation? Is the probability of the same equation appearing twice? It's something like, you know, there's something like m choose two pairs of equations, and the probability that they both match is something like m to the minus k. I need to get the same equation in all positions. Something like about m squared n to the minus k. So what I would want to say is that if I can do better than this, then I can actually recover this. Okay? And I cannot do that. And I cannot do that, but I want to tell you: this is the new part. So there is, well, it cheats in a couple of places, but it's kind of a new reduction, which I feel gets us a little bit closer to the goal. So this is what it says, is that if I can delta distinguish k XOR using m samples, I can get the search algorithm for k plus one XOR, so I need to go one larger, and this is the number of samples I get. It's m times n plus a little bit. Let's ignore this little bit for now. Bit. Let's ignore this little bit for now. What I would really want to have is instead of m times n, I would like to have m times square root of n, because that's how the hardness scales, right? When I add one variable, then the hardness jumps by square root of the n. So this is the set sequence. So just to give you a sense about sort of how far we get from what we want is, you know, we know that we can always look for collisions. We know that this is the distinguisher we expect to be optimal. We expect to be optimal, say I can do a little bit better, right? Well, let me actually say it this way: how much better do I need to do than looking for collisions in order to get a search algorithm? So at this density, like for kx4, when I have n to the k minus 1 over 2 samples, the collision finding algorithm has advantage about 1 over n. And what this says is that if I can improve the advantage from 1 over n to 1 over fourth root of n, n to 1 over fourth root of n, so it's like I need to do quite a lot better, then I can actually get a non-trivial search algorithm. Okay? So yeah, I wanted to show you how this works, but I'm running out of time, so I think I will skip it. Maybe I can talk to some of you later. So yeah, so let me just conclude to say what I think is. You know what, what's what I think is like I think this is a very interesting problem, right? I kind of explored two instances, like one. So these are the two examples that I showed you where we know how to do it. Like there is dense KXOR from the work of Nucentiu and Mo and click from this recent work of Hirakara and Chimizo, where you do get what you want. You get the optimal third-party decision reduction. Right? And so there are settings in which we can kind of do something, but it doesn't give us, like, it doesn't tell us what the threshold is. So as far as X. Threshold is so sparse XOR is one example where you know we can get something but not exactly what we want. So some of these methods that I talked about, they extend to Holdrite function for those of you who know what it is. But maybe under some additional assumptions. So here we can also kind of get some reductions that don't get us all the way to the threshold, but explain something. Like again, like for click, it's known, but again, if you try to extend this. But again, if you try to extend this argument of Hira Hara and Shimisu that I didn't tell you about, say from click to then subgraph or to the problem that Yuval talked about, which is a random planted subgraph, then it fails miserably. So those are like very nice challenges to try to understand. And I mean, there are also totally different problems in the community, which I think in principle one should be able to say something like binary perceptron, right? Where I have like the Gaussian noise matrix and then I take the sine with some planted vector. Can you get a certain Can you get a search-to-decision reduction? Maybe not optimal to begin with, but for any kind of reasonable setting of private risk. So, you know, I think there are questions that want to be addressed. And I also, I mean, I think like maybe these search to decision reductions might have even some positive crypto applications. By the way, like this new result is joint work with my usual collaborator, Alon Rosen, and he had this idea of maybe, you know, get research to decision reduction, somehow we can make them Somehow we can make them, I don't want to go into details, but we can use them positively to amplify the rate of crypto systems. Because one thing we often kind of, we don't even say it when we talk about cryptography is when we don't, initially we don't want to optimize things. We just want to get something that works. And Gulbasser and Michali told us that all we need is a one-bit encryption. We just need to encrypt one bit. But one-bit encryption is not very efficient. We end up using a lot of information. A lot of information to encrypt one bit. So, this kind of search to decision reductions could, in principle, give us a way to pack more bits into the encryption. I'll leave it at that. I don't know how to do it, but I think there might be some potential for questions. I I'm puzzled how this translated there there are models for which the decision is trivially uh solvable just by counting. So the k squared over n value that you get. There it translates into trivial, trivial distinguishing algorithm and other models, despite matrix models as an example. But the search presumably is hard, so you ask my question, David. I'm not being a question. I mean, I don't know. Maybe these reductions will not work. That's the problem. But there could be some phenomena, but usually there's a notion of what is the best advantage you can get. And you could wonder if you get a non-trivial advantage, because that all comes from looking at expected choices of powers. If you beat that, then maybe you have to recover. Exactly. And there's a search to decision here in Kirahara at Jimmy Zoo. No, so let me tell you a bit, like, I really like this here at Karashi. I really like this Hirakarashimizo paper, so let me advertise one more feature of it, which I think is great. So you can actually apply their reduction in the satisfaction in the easy regime. You can say, well, say I have a click of size period of n, what happens when I apply their reduction? So they do give a search algorithm, right? You apply like their stupid distinguisher which just counts edges, feed it into their reduction, and it gives you a search algorithm. It gives you something that fails to click, which is like, you know, it's not a trivial problem. It's been a lot of work. And I think one interesting aspect of his algorithm. I think one interesting aspect of his algorithm is that it's a very low complexity algorithm. So it doesn't do anything like spectral analysis or anything like that. Just like for complexity theorists, it's like TC0 or maybe even TC0. So it's very low complexity. So you know, like, I think one way to approach these questions is to understand from the complexity perspective. Like, can we design low complexity search algorithms? And then maybe this gives us a hint of what the real charge is. Another answer would just be. Another answer would be just because they occurred, the search first decision occurred at different thresholds in that SMR, you could still hope for reductions that just match to the sample conflict. Fair enough, but let's say we are in a region where decision is done really by counting the sum of entries. If the search is hard, maybe it's not a very solid belief, but believe this search is hard. So I'm just, and I think some other. I think that's the problem. You know, the decision problem, like the hypothesis testing, you don't do with probability one minus the whole level one. So there's like some threshold. No, it's logo matters. Yeah, like you say that you can say that you can distinguish with probability like one minus delta, but now if you want to make delta exponentially small, maybe that's the hard part of it. It's exponentially small. By just counting this five matrix model, then you know. Then you know that there is a spike. No, no, but if you want to get really good. This isn't what you want to do anyway. Just discuss the offline. So please turn your mobbed with questions because we're done. Like everyone in the morning session. If you have any questions, join us. Again, a reminder: 7:30 p.m. Galeno's talk is here, so it's a little earlier than the prior. If you think sense, yeah, we'll be back in a second. Yeah, that it was actually uh we talked to you how to use uh for you know which is in the sense for the point of uh Yeah, I mean I got I gotta get my stuff. I gotta drag that. I think the hybrid would be a good idea. No, it it no sense it works, I mean it works for bigger and more people. I don't think so. I don't think so. I don't think so. No, I don't think so, because the take is like for one inside fields, you could stick the oil production, which loses all of it, right? Then you can cheat it. If you try and do it for exponentially large Q, you get something completely mean price. And the whole point was to kind of like make it better somehow. So, for example, right, in this worst-case labor-case subduction. His abductions blew up with Q. Blew up with Q. They said, Look, you know, I can't do it. Then you use CFT and so forth, right? But then, but actually, CFT is like, yeah, it's very clear. And then you have to use properties of the nice distribution system, right? And I'm thinking down, I think. So again, I think that the AK proof certainly applies to any standards. And it's the same as you actually run the right one. It's like a two-line reduction. I know, I know, I need to like it. Yeah, the COVID spin is like the air create. Okay, so one hypothesis is that they need a large field small field. I should have noticed that. No, I don't know. It might be just that they figured out in business. There are many cases, like in UNA, you know, the celebrated sites. So those parts of the theory proving like 145 low p and lower 14. So let me stop doing it with the second thing. And then I think I voted it. I think it should be done. It's good, right? But there was already a huge difference between the oh yeah, you could see this was the reason why I didn't follow the rest of it. Are we are we are we look up yeah before we start to simply