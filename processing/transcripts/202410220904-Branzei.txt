So, what I'll do first is I'll introduce a basic resource allocation model and then I'll discuss the setting where players are kind of fighting over this resource over time. So, the basic model is known as cake cutting. So, for those of you who haven't seen it or familiar refresher, it's defined as follows. The cake is the interval 0, 1. So, you can think of it like it's a plot of land, it's rectangular, you can cut it along the x-axis. Along the x-axis. And there is a set of interested parties or players or agents that are labeled from 1 to n. And each player has a preference, which is expressed through a private value density function. So the player I has density vi. So in the picture here, there are two players with a green and red density. So the red player with uniform density, he just likes as much of the cake as possible, and he's indifferent about which parts he actually gets, he cares about the length. He actually gets, it cares about the length basically. And the green player has a more varied density, so he likes more of the piece in the middle, and there's some cherry there. And if we want to know what's the value of a player for an integral AB, we take the integral of their density. So it's basically the area. So I guess you could also imagine that there are many small resources, like aligned, and then if you you know, if you want to know what's the value for a bundle, a contiguous bundle is the sum. Continuous bundle is the sum. Okay, so this is done. And so a piece is a union of intervals, and the valuation of a player for a piece is the integral of their density over all the pieces. So the goal generally is to find an allocation, which is an assignment of the cake to the players, so that the pieces are destroyed and everyone is kind of happy with what they get. And the notion of happy may differ depending on the criterion of that. So, this model is due to Seinhouse 48. Are there any questions about the model? Is there anything here which is statistical? No, so this is not about deep learning. No, there will be this. So, I have to present the basic results. I have to present the basic resource allocation models. So, okay, my question is: is there any question about this model? There's a collection of potential substances that you said that are unions of intervals. But it could be more general, like I have a collection of sets and I want to. So you can cut it, and initially the cake has not been divided yet. You can cut it however you want. So that's the Want. So that's the task for the algorithm, for instance, to figure out what's a good partition. So maybe there's some land that has forests, it has lakes, it has land for agriculture, and you want to divide it among the set of parties. Yeah, so did I just describe how the valuation is assigned? Is there any assumption that the valuations integrate to the same thing? Yeah, it should be integrable. No, but do they integrate the integral over zero? The like the integral over 0, 1 of both functions of system. Yeah, that's not very important, but actually, for this talk, it almost in generality, we can assume that if a player gets the whole height, their value is 1. Does each piece have to be contiguous or can you have multiple intervals? No, in general, you can get an arbitrary measurable set. Okay. Not the charities only. Yeah, for instance. Okay, so what are these some fairness notions in this? What are these some fairness notions in this space? So, a basic one is proportionality, which says that each player gets their minimum fair share. So, if there are n players, this means that each player I has value at least one over n for their piece that they get. And between us says that everybody likes their piece more than somebody else's piece. So, they don't want to swap with anybody else. There is also a notion called equisibility, which says that everyone is equally happy. Like the value that player I has for their own piece is the same as the value that another player J has for their piece. So, this is generally useful if you think of situations where there is conflict or the players might resent someone else for being too happy, even if they don't actually want that person's piece. And then there is an option like portfolio, which is like each player has value one over n for every piece, so that's a stronger requirement, which can actually boost social welfare. Okay, so. Okay, so for this talk I'm going to assume bounded densities, which means that there is some lower and upper bound which is arbitrary but spaced on the densities. Okay, so just to give an example, like the oldest protocol in existence, probably which is cut and choose, so it's for two players. So here Alice cuts the cake in two pieces that are equal to her, in her opinion. And then Bob is asked to take his favorite, and Alice takes the remainder. So in the example here, Alice has So, in the example here, Alice has uniform density, she cuts at 0.5 basically, and then Bob is mainly interested in the part of the end, and there's some chocolate there, so he takes the right piece. Okay, so for this talk, I need to also introduce the notion of stack of work value. So, here, like, this is showing another instance where MA denotes Alice's midpoint. Like, this is a point where her value for the left and right fish are the same. My right fish is the same, and MB is Bob's midpoint. So, the second work value is the value, it's like the highest value, is the value that Alice gets when she cuts at Bob's midpoint. And Bob is nice and breaks tight in her neighborhood, so he lets her have techniques that she prefers. And it's basically the shaded area. So, this is, in principle, achievable. So, in this work, I'm going to consider a repeated setting. I'm going to consider a repeated setting where in every round a new cake arrives, and it's going to be very simple. So, it will be two players, and the cake will be the same. It's like you know, you can think of some resource that you allocate in every quarter. Maybe it's like some rights over, like you divide areas of the city for who does maintenance services. So, okay, so a new cake arrives, and Alice cuts at a point of her choice, so in round teas for each in order to say. Around teeth for me, let's say A T. And then Bob chooses either the left or the right piece, and then Alice takes the remainder. So, and I'll consider two settings. So, one is a sequential setting where Bob observes Alice's cut point before choosing left or right. And the other one is a simultaneous setting where he only observes the cut point after he makes his choice. And the players start by not knowing each other's valuations, but they can observe each other's behavior over time. They can try to make inferences. Time. I can try to make inferences based on that. So, the simultaneous setting was first considered by Oman and Maschler in this book on repeated gains with incomplete information. They consider a special case where the density is like Alex was a uniform density and Bob was one of two types, but Alice was one strong which Bob shows facing. So, you can think of some, like, basically, you can think of resource allocation settings where, like, every quarter or every period, like, maybe the players have to. The players have to divide something, like I don't know, classrooms or areas of the city among themselves, and maybe different areas have different profits depending on the population of that city. So I'll keep this very high level, but this is the model I'll focus on. Okay, any questions about the model? This is the setting up. the valuation of change between so here it will not change but it could be interesting if it's considered richer settings where it can change okay so so an observation is that if Bob is very myopic and he consistently chooses his favorite piece in every round then Alice can actually exploit that. She can basically do binary search of his midpoint until she identifies as very close within a small error and then cut there for Within a small area, and then cut there for the remainder of time. And this way, Alice can actually achieve her stackable value in all, but like logs. However, Bob could fight back, so he couldn't then try to deceive her by being unpredictable and choosing sometimes a piece he likes less, but that will affect his pay-off. So he has to be careful about how he does that. So, given a history age of play, like this, let's say there have been some number of If they're having some number of year-rounds. The regret, the Stackleberg regret, is the difference between Alice's Stackelberg value, like this is UA believes the number UA star here, like that's her dream, like she would like to get her Stackleberg value in every round, minus what she actually got along this project for the test rate. But this is a non-symmetric version, right? So yeah, I'm going to. So, I'm going to talk about the sequential setting, and I'll mention I'll get to the simultaneous as well. I mean, we could define it for the simultaneous setting as well, actually. Okay, so basically, in other words, like if Bob is myopic in the sequential setting, then he has a strategy that gives her a Stafford regard of the order of log of T. It's actually, it turns out that she can actually get log log T. Actually, get a low block T if she's a bit more careful, and that is optimal, but I won't get into that. So, this is for illustration purposes to show that there is this kind of dynamic of exploitation. So, in general, we can think of Bob trying to be less predictable. He still probably doesn't want to be like maybe very far, like if he's very far from what was expected, peace and every round. Maybe he could do something very smart that would ensure it would fail. But suppose that Bob has like a bounded regret with respect. Like a bounded regret with respect to the stand benchmark of choosing his favorite piece in every bound. So, in other words, like suppose his regret is t to the alpha for some alpha one. Then if Alice knows alpha, she has a strategy that ensures her Steckerberg regret is of the order of t to one plus alpha over two. And this is the exponent here is sharp, so that there is actually like Alice's regret is. Alice's regret is this to one for some bulk strategy. If Alice does not know alpha, then she cannot do as well, but still she can get separate regret of the order of T over log T. And this is also sharp as I'm saying. It sounds related to bandage problems. Yeah, I think there are partners. There are personal information, but different than the random personal information in traditional bandage. Right, so here I guess. Right, so here I guess the randomness comes from the behavior of the other player rather than from, you know, we could also think of settings where maybe the resources have random values or something like that. But it's valid in the sense that there's partial information and you may want to do some exploration exploitation stuff. Right. Yeah, so there are connections to online learning and we actually use that. Is there any other question about that? Yes. Just to make sure I'm not getting that. Just make sure you're getting right. You just need an outer button right and then you can get this. Alice doesn't need to know exactly what its related is going to be, but there's an outer button. So right, so if Alice knows, like, suppose, so this is divided into two parts because Alice can do better if she knows Baul's red bar. And by that I don't mean the question, but like the exponent. Does that answer your question? If she doesn't know the exponent, then she can get still till the Still too remotely with respect to worse epoch value, which is, you know, it's worse. Okay, so I just want to briefly explain how to get this. So suppose that Bob has regretted to the alpha, he has some strategy that ensures him that. So what Alice can do is she initializes the interval, which is initially the colour cake, and she uses the following strategy. So she will discretize this interval by cutting that file. Into by cutting at five points, like here for the first iteration, they're shown. And then what she's going to do is she's going to cut at each of these points some number of times, and then she will take the majority of Bob's answers there. If she cuts enough times, then the majority has to be consistent with Bob's true valuation because his strategy has no regret. So the number of times she cuts at each of these points is some number eta. And so she computes the majority answer. And then she sees a point, like the answer. So if she cuts it zero, of course, like both should choose their right piece. The majority should say that both chooses the right piece. If she cuts it minus the left. And this will be like right, right, right, left, left. So at some point there is a switch, and that helps her identify the interval where Bob's midpoint is. Now, she actually has to be a little careful to take like a larger interval or interval around it. Interval around it to be guaranteed that Boboint is there in case Bob cut very close to the boundary button on the wrong side. So then she kind of zooms in in this interval and then it repeats. And until she identifies the midpoint and from then on she cuts there. Now why this exponent is sharp, just to make a note. So the observation is that if Alice cuts very close to MB, which is Bob's midpoint, but Alice took, sorry, but Bob takes his. To, but sorry, but Bob picks his less favorite piece, then that hurts Alice a lot, but that does not hurt Bob too much. So Bob can do this for a while and make Alice accumulate regret while he is himself fine. And at some point, he reverts to choosing his favorite piece for the rest of time, like the damage to Alice has payoff has already been done. Okay, so this is like this explains it. And in the case for unknown alpha, this is similar, but the number of rounds has to be slightly much larger to cover any possible sublinear regrets that Bob might have. So one observation here is that, so Alice is a bit exploiting Bob, like Bob still gets like a half on average. He can pull that by choosing his favorite piece or something posted. But Bob could in principle envy Alice's happiness. So the question that we So, the question that we looked into is whether we can achieve equitable outcomes, like where the players are equally happy. Maybe the allocation or the interface profile is proportional, like they get value just to have same, but they're also roughly equally happy. And what we could show is that we can achieve very good profiles where they get a bunch of half. So, this type of behavior resembles also other settings where people have studied this phenomenon, like spiceful bidding in auctions, for instance, by buying a sample, where a buyer's utility. Example where a buyer's utility diminishes if other buyers are too satisfied. So, what we showed here is: so, in both the sequential and the simultaneous settings, Alice has a pure strategy, I say, so that for every Bob's strategy, on every trajectory of play, Alice's payoff is of the order of, is at least a half minus the rule of one, while Bob's payoff is at most a half plus the rule of one. So the way to show So the way to show this, or that we showed this, is using the connection to black hole approachability. So I'm going to briefly mention this heading because I don't know if everyone has seen it. So this is from 1956. So this basically considers games with vector-value playoffs. So we have a player and an adversary and two compact convex spaces. They're called X and Y. So the player will choose an action from X. The player will choose an action from x and the adversary will choose an action from y. And we can think of a utility function that is alphine, so it's like takes u is like of x, y, and it takes some values in some space Rd. For p, it's like, you know, x2. And we also have a closed convex set S, that is a subset of Rd. So now is the task here. So the player would like to select a point X in the set capital X that was the The set capital X, the adversary of point Y and capital Y, and the player would like the total payoff u of X Y to be inside the set S, while the adversary would like to keep it outside of S. So given such an instance with the sets X, Y, the pay of vector, the pay of function U, and the set S, we say the set S is approachable if there is an algorithm that selects points in the set X such that for every sequence of actions chosen by the adversary in Actions chosen by the adversary in set Y, the distance between the set of S and the average payoff along this trajectory X T Y T goes to zero as time goes to infinity. So X T here is what, or Yt is what the adversary chooses as an action at time t, and Xt is what the player chooses. And Blackwell's theorem characterizes when a site S is approachable, so it's a simple condition. Basically, it's called whether S is response satisfied. Whether S is response satisfied, but whether for every point Y inside Y there is a point X in the set X such that U of X Y is in the set S. So basically like Minimax theorem doesn't hold in the setting of vector payoffs, but if you look at the repeated version, then you can recover something like Minimax, which is what the theorem captures. So why is this useful? So in our setting, roughly, we'd like the settings to be like Alice would like to keep Bob inside the set. Alice would like to keep Bob inside the set S, which would be the set of, like, she would like to keep him to have his payoff at most a half. So she will define the set S as the space where any Bob, whatever Bob might be, his payoff will be at most a half. Now to do that, so we need to, like, because there are some issues with infinities here, so we define a set of D star that is an approximation for this full variety of balls, so V star would have arbitrarily good approximations for every possible ball out there. For every possible ball out there, it's still infinite, but it's comfortable. And then we basically adapt Blackwell's argument for the setting to work with this comfortably infinite connection set V star. And like Alice's charger will track the average payoff for each type of bottle in V star. And S is like the region where the payoff of every bottle is almost a half. And she will, in each round, she constructs a point that moves this average closer. The point that moves this average closer to S and in the limits she traps on inside S. Analysis fail guarantee follows because there is a bulb similar to current. Okay, any questions about this? No? Okay, so we also have for how can Bob defend himself against Alice, like to make sure that Alice is not too happy compared to him. So in the sequential setting, Bob has a few strategies of. Has a pure strategy so we get for every other strategy when every trajectory Bob gets at most a half minus all of one, a little of one, well Alice gets at most a half plus a little over one and also in the simultaneous setting, well one can do this, but in expectation. So to see the proof case, so the simultaneous setting is trivial. Bob picks left or right to put their heart and the sequential structure is the randomization of the simultaneous one. So Bob kind of partitions the cake mentally into a space. kind of partitions the cake mentally into square root t intervals, like say y1 to y square root t, that are of equal value to her. And then he basically treats each interval as a separate cake and he alternates his strategy in there, like alternating between left and raw, sorry, between left and right in the round, or Alice cuts in that mini cake. So, okay, and finally, like if Alice and Bob place our strategies against each other, then they approach. Against each other, then they approach an equitable, equitable file of one-half, one-half. So, we also analyze a classic learning rule that is known as fictitious play. So, this was defined by Brown in 51. So, under fictitious play, each player, so here we look at, I should say we do the analysis. It makes sense to define a football support as simultaneous, but we do the analysis for this simultaneous setting. So, under a fictitious play, each player looks at the history and interprets things how the other player goes. Things, how is the other player going to play now? They interpret the history as a probability distribution. And they respond, like they best respond to the empirical frequency of the other player. And then if multiple actions perform equally well, you just pick one arbitrarily. So convergence to mesh equilibria for fictitious play has been shown for zero-sum games. This is starting with Robinson, actually, she proved this. And also special classes are general sum games. And also special classes in general, some games, Mathwar, Mongresha, Playberger. And there are a few studies for games with infinite action spaces, like there is quite a similar thing. So this is from the simulation. This shows Alice's average utility over time, like when they both play fictitious play in a randomly generated instance. So Alice is on the left and Bob is on the right. And so what we should And so, what we show is that when both parts are now displayed run futures display, the average payoffs converge to the equitable utility profile one-half, one-half at a rate of one over root D. So just to give a very brief sketch for how we show this, so we basically define some variables that characterize the dynamic and that are easier to analyze. So, alpha t would be a variable for L. alpha t would be a variable for alice which counts like it's a difference of two terms so it's rt minus l t so r t is the number of times involved picked right of the round t and l t is the number of times picked left beta t is a sum so overall rounds from one to t that we look at some round tau and we look at the difference of what is Bob's value for the left piece minus his value for the right piece and then the observation is that this these variables govern the These variables govern the evolution of the fictitious play dynamic. So, Alice and Bob make their decisions in round two plus one based on alpha T embedded T. So, there are a few cases to see that, to check that. And this is like the plot showing the sequences over alpha T embedding T over time. And this is a scatter plot of the sequence alpha T pleta T. So you can see it forms a spiral. So it starts at the center, kind of, and then it goes. The center, kind of, and then it goes out. And so, to formalize the spiral, we define a quantity rho t, which is absolute value of alpha t plus the absolute value of beta t. And we show this is a potential function. So basically, rho t is non-decreasing. And like, you know, so we use it to analyze the change in the sequence alpha t delta t from round to round. So very roughly what happens is that both payoffs can be read off. That both payoff can be read off on the sequence, like his payoff in rho t is of the order t over 2 plus minus rho t. And rho t will be of the order square root t. So bounding the rate at which the spiral expands also bounds ball paleo. So we show that this rho t is of the order square root t. And then Alice cuts in the interior of the cake is actually when alpha t equals zero, but this happens less and less as the spiral expands. Expand. And when she does not cut inside in the interior, then the sum of payoffs is one. So basically, from this, we can deduce that the sum of cumulative payoffs is like t plus minus square root. And combining this with the bound on mobile, we get the bound paralysis payoff. Okay, so what could be future work? So I just want to make a note about the regret benchmark. I think there is something a bit funny about it. Like, so Bob has regret with respect to being. Has regret with respect to being like taking his favorite piece in every wrong. But actually, if he does that, if he did that, he looks back and he's thinking, like, oh, I wish I took my favorite piece in every wrong. But actually, if he had done that, others would have exploited him more aggressively. So maybe a high-level question is like in these studies where there is like a leader and a follower, like what is the right way to think about, you know, what should you regret? We could also have different cakes. Uh we could also have different cakes over time or different I don't know indivisible goods or whatever kinds of resources are that makes sense. Uh and you know the players' valuations could be changing and also multiple players. So thank you also Mahan.