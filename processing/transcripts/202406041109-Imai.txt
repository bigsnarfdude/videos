In the Department of Statistics at Harvard University, and also an affiliate of the Institute of Quantitative Social Science, where his office is located. Okay, so the paper to present today: estimating racial disparities are not certain. Probably enough people are confused about my office location. Well, thanks for having me or inviting me. Yeah, I wanted to give a talk on this joint project with Corey McCartney, my former student, who's actually studying at the Penn State Statistics Department. And Robin Fisher is a researcher at the Treasury Department, and Jacob Gordon, and Jacob Gordon and Dan Ho. I think Dan will show up sometime. He's a good friend of mine from graduate school. He called me up two summers ago and we started to work again at the neighbors. So this is about estimating racial despite even this is not observed, so it's a very concrete problem. Okay. All right, so just to motivate, I guess we don't have to give. I guess we don't have to really talk about the importance of racial despite estimation, but you see this everywhere in different fields. But often individual races, that information is not available. So the reason might be that maybe it's prohibited to correct such information. So there are instances of that type. And also, agencies and companies may not want to correct such information, but then there might be a cues for. Information, because then there might be a cues for direct racial discrimination. So, the question that we really are interested in is: how should we estimate racial disparities when the race is not observed? So, there are standard methods out there. It's something called BISG, Bayesian Improved Serial Energy Recording. I'll just briefly describe it later. But it turns out that this method tends to yield biased estimates, and there's a final Estimates, and there's a prior work that talks about this. And so, we're trying to improve: okay, so given this result, can we do something a little bit better? And I also want to emphasize that the Biden administration, the first day they came in, they issued executive order talking about how to, you know, we should be trying to improve the methods for estimating the police money. So, this project started out when Dan called me. This project started out when Dan called me up. And so the motivating application is racial disparity in the US tax system. There's a very impactful book by Joseph Brown about racial disparity and discrimination in the US tax system. So it's a very important thing for IRS to look at this, but IRS doesn't collect individual risk information. Individual race information. So when you apply a 1040, there's no racial box that you click. Probably a good thing. And you might think, oh, why don't you get that information from Census, but Census Bureau is not allowed to share that individual level data with other agencies. That's the title of the team. So in this paper, and JCOL has a lot of work on this, and in this paper, On this, and in this paper, we sort of focus on one particular aspect. So, it's home mortgage interest deduction for HMID. So, Joshi Brown describes HMID as little more than the 21st century version of and was conclusive must be the guild. HMID does not really find that, she finds, she and others find that it doesn't really encourage homeownership. Doesn't really encourage homeownership. Nobody buy a house because of the tax deduction. But it tends to increase the housing price, which leads to a variety of consequences. I should note that 90% of taxpayers take some reductions, so it doesn't really itemize HMIDs. It's a real complicated, you know, so I'm not going to detail on that. Dan and Jacob can tell you. Dan and Jacob can tell you more about it. But I'm going to try to focus on how to just sort of estimate the ratio of disparity on this particular itemized deduction in tax, IDS tax. So we're going to analyze, once I describe the method, I'll show you the results on a random 10% sample of the individual tax returns from 2019. So it's about 70 million observations. 17 million observations that we analyzed. Okay, so just to go straight into the method, so the data that we have is outcome of interest. We're going to assume it's categorical, just for sake of simplicity. Race is unobserved. We're going to use surname as a proxy for this. And then the residence location. So all this data is available in the plax form. And then maybe some combinatorial. And then there may be some coordinates of interest, like income or something like that. There may be other variables that are also available in census. You may not have that, or you may have those. So these are the data that you actually observe or accept the rates have in your individual level data. So in our case, the IRS. Okay, we're going to combine this with the census data that's publicly available. That's publicly available. So, aggregate census data. So, the aggregate census data will give you the population, joint distribution, the population distribution of the geography and the race and some other covariates. And also the census provides the frequent, for frequently recording surnames, they provide the joint table, the counts, individual counts of race and surname. race and so name. So racial despite the estimate is just average outcome for different racial groups and difference between them. Potentially based conditioning on the covariates that you have also been usually level. You might also want to run regression. So you might want to just regress y on r or maybe y on r and x. Okay, so those are sort of the colours of interest. So, how do people, a lot of different agencies use this estimate the ratio disparity when the race is not available? They basically use this method VISG, or it's variant. There's like lots of variations. So, the assumption of this VISG method is conditional on the observed race. Conditional on the observed race, the surname is an independent geography where you live. So this could be violated if, say, this is very coarse category, so like the Asian category, which may include Chinese, Japanese, Korean, Vietnamese, and so on. And if those racial groups have different surnames and then they tend to cluster, then this assumption would be by rate but this is sort of the standard assumption people make. The standard assumption people make. So, what's nice about this, once you make this assumption, then you can just use the Bayes rule and then drop this G because once you condition R, the S and G are independent. So it becomes this simple form, which all this information is available from the census. So once you make this assumption, This assumption, everything that for theory, distribution of rates can be calculated using the sensor state alone. Some people also condition more covariates, but then assume the joint independence between the surname and covariates as well as Jocker. So once you have these probabilities, then you can estimate the ratio disparities by just By just either doing the weighting or specialing. So this is very, very standard. Like a lot of companies do it, a lot of consultants do it. Okay. So what's the problem? So people use this by saying, well, actually, these probabilities are very accurate. And actually, it's reasonably accurate. People find it very it's reasonably tiberated. It's easily kind of predocular. But it turns out the good race prediction doesn't necessarily mean you get a biased estimate of ratio disparate people. So there's a paper that in 2019, Chen et al., showed that there's a bias if you look at the weight edge. And anyone who knows regression understands this, so it's basically the residual correlation. Residual correlation between the outcome and the race. So, conditional on these three variables that you had when predicting the race, basically the bias is a function of the correlation between this residual correlation between the outcome and the race. But also, the denominator has a ratio. So, the problem of this is that minorities tend to have large bias because proportion of the rate. Because the proportion of the race, a particular racial category, is inversely related to the financial local bias. Moreover, because it's an indicator function, if you're interested in racial disparity between the majority group and the minority group, this tends to underestimate the racial disparity. So it tends to sort of, you know, basically sort of say like even if there's a ratio of disparity, this method This method may tell you there's no relation to this function. So, require assumption from this, you can tell, okay, condition on these variables, y and r is independent. Why is that bad? Well, the whole point of doing the racial disparity estimation is that you believe race has a lot of impact on a lot of folks. So, race may be a So, race maybe affects many aspects of society. So, this assumption seems to be problematic. So, like one DAG that would satisfy this assumption would be the race is affecting outcome only through these two. Your surname, whatever the coordinates the census may have, and where you live. And that seems like a crazy assumption, at least to me. Are rich to me. And also, the whole point of racial disparity estimation is not to believe this assumption. So, can we do something different? So, we just have one simple idea. So, it's a different gap. So, basically, what we're going to assume is that we're going to allow the race to affect the outcome, other than through the geography where you live, and then some other census coordinates. So, it could be education or anything. Be education or any other kinds of potential causal impact that race may have on whatever the outcome is interested in. But we're going to try to say that the surname, once you conditional rates, independent of the outcome. So we just flip the independence. So that's basically the assumption. So in other words, I'm going to use the surname as a proxy. I'm going to use the SRNA as a proxy for race. And what's nice about this is that race can directly or indirectly affect the outcome. And that's sort of, we want for that to happen, because otherwise, what's the point of doing a ratio despite the exchange? So this assumption is guaranteed to hold if you have, say, anonymous application screening, so that screener is not looking at the screen. Screener is not looking at the name of the applicant. But there's some potential violations. So, you know, name-based discrimination, like if the surname directly affects the outcome, conditioning on the race. So you may not have to worry too much, but still, it's possible that the main other direct effect. And then again, if the race category is too coarse, then Category is too coarse, then there might be potential violations there. So it's not a perfect, but it feels like it's compared to the previous one, it's viewable. So you can basically identify once you make this assumption, so you just decompose the observed data into something you don't observe, but you can drop S, because once you condition RGX, S is independent. X as is in frame of y. And it turns out that there are this many equations, and this is the number of surnames, and unknown parameters is at all fewer. There's so many more surnames than the number of ratio categories usually. So you can actually run regression. It's actually you basically regress Y on this VISG probability. VISG probability. This is the linear regression. And then you can aggregate using sort of post-structuration. Now, this will give you a biased estimate of Y given R, but it's a probability we're looking at the categorical outcome. So we sort of have a bit more general estimation method. General estimation method, which is looking at basically like Bayesian estimation and BISG as a probability as a prior. And then you can get this Bayesian model, which we call 40 Bayesian instrumental regression for this filation. And you can do this with a EM algorithm. And what's nice about this is you can get the updated race probably, the conditional one. Update race probability, conditional Y. And I'll show you that it's actually much better race probabilities than the VISU probability that doesn't conditional Y. Did you have a question, Phil? Oh, yeah, I was a little confused about how you're doing Y and R and G, like the red thing you got. You don't have R, right? Yeah, so this is unknown. That's unknown. Yeah, so we want to estimate this. Yeah, yeah. But we have this equation for every combination of G, X, and S. And so there are many equations that we observe, but the unknowns are much fewer. Okay, so you're estimating the red thing. Yeah, yeah, okay. We're estimating. In fact, you can estimate it by running regression. Okay. Okay. Yeah. Yeah, that makes sense. Yeah, but you know, quite good. Yeah, so it's like, you can think of it. Yeah, so it's like you can think of it as like a two-stage spares. Like the VIC probability being the predicted values, and then you run the regression of y on the VIC probability. It's like instrument or okay. So in practice, if you do this, you get like lots of negative numbers and stuff. So you don't want to do like two-stage respares. Even though in theory it's unbiased, so we do more model-based maximum Bayesian posterior estimation. And yeah, so we have some sort of parametric model that we fit in the application. Yeah, so you can do different kinds of modeling. So basically, you model outcome, given race, and your And geography and X, and then fireball in D2. Okay, so there's a potential vibrational key identifying assumption, name-based discrimination, racial categories to quarters. So if you have like a finer ethnic groups, you can improve on the so for example, like Fung FOE Mai is Japanese, my student is Irish. My student is Irish. And so you can assume instead by conditioning some function of S, because S is very, very high dimensional. Maybe you can run data-driven way. We haven't done that. It turns out 1930 census, which was individual level data was produced, made available publicly. So you can find a bunch of smaller groups. Of smaller groups, so we include those as a covalent in the regression. Alright, so let me just quickly give an empirical first validation. So we're going to look at the Nodes Carana modifiers first, which has a self-reported race. So you have a benchmark. You're going to look at the party registration. If you know the southern politics, a lot of white people are African. A lot of white people are African, very few people blacks are African. So there's a lot of racial disparity there. And here's the results. So we're looking at white and black disparity and white and Hispanic disparity. And this dotted line is the true disparity based on self-reported race. And these red circles and triangles are And triangles are the R estimates, and these circle and cross are the usual standard BISG estimates. So you can see they're always attenuated. The bias of the standard methods tend to be attenuated, and our methods tend to do better. So these are different pods and categories. And you can also look at the total variation distance across different. Variation distance across different parties, party IDs, and you can see the party has a lot lower total variation distance. And it's pretty constant across different geographical units that you use to create the BISG problems. And it seems that reasonably, especially like African Americans, we do a lot better. A lot better. Okay, we can also do small area estimation. That's a lot better. And then the race probabilities are much more improved. So this is looking at again the body versus BISG and the ROC curve, which is not something I should be presenting. But how would you build? But how would you build it? Okay. At first instead, okay. And then, so I wanted to show the postness analysis. So, surname used from the 2030 census. We also added some Asian names, surnames, and you can sort of see creation. And you can sort of see correlation between the body residual, so that gives an idea of how much remaining bias there might be. It's very low, so it's like less than 3% correlation across different surnames or group names. So including these doesn't really change, estimates. So I think that surname independence, conditional race and other categories pretty reasonable. Trading piece of it. Okay. So here's the results for HMID. Okay, so on the left we have deduction distribution for different ratio category. It's a little hard to see, but the darker means the higher amount of deduction. And this dotted line is sort of expected, it's basically like using a sensor scale. It's basically like using the census data, we're calculating for each geography, census tells you how many people have mortgage. And we multiply that by the proportion of the race. So if the racial distribution is uniform within each census geography, then the dotted line is what we would expect. What we would expect the proportion, the each racial group, could try to get this deduction. So, actually, weights is exactly, so right along, it's like it's expected, you know, relative to the expected number of proportion of people who get this HIV. There's a lot more Asian who get mortgage deduction, interest deduction. More interested action, interest deduction than what we expect, and then fewer for black and white. So, this is sort of consistent with all these arguments that Josie Brown made and some others also found. On the right, you see the average deduction among the people who actually took the deduction. And again, you see that black fighters have Biners have a lot fewer amount of deductions. So they're benefiting less from this particular deduction scheme compared to the other groups. So this is again also consistent with the arguments that were made in that group and other articles. Okay, so just to conclude, so this new identification assumption, and it's sort of flexible modeling with variable estimation, we fit this. Scarable estimation, we fit this in like 17 million observations or at once within like five minutes. Except it took five months for the IRS to run the code. But once you can run the code, it's only five minutes. It gives you improved BIC race properties, and we also show how to do some strip analysis. We're thinking about doing more empirical validations and understanding bias because it depends on the outcome. So it's really hard to know how much of what we showed can be generalized. How to use the auxiliary information, maybe more robust to small biasing the VISG probably, because the VISG probabilities are not perfect. So, how do we do that? And then there's the You do that, and then there's the paper and the software called border with the different colors of the borders. And that my student created the set. Go ahead. Given that you're still going to do some future work looking at understanding bias in different use cases, Looking at understanding bias in different use cases. Do you think that this is appropriate for use in algorithmic auditing uses or other specific domains where you think this is more or less appropriate given what you have? Oh, I see. Yeah, that's a really good question. Again, if you know, like say renting where the you know, if you know people who are making decisions not looking at the store name, then like you should use the express assumptions automatically. This assumption is automatically satisfied. In other cases, where that may not be the case, yeah, I guess it really depends. But I really think that better than what people are doing now, I mean, that assumption seems just crazy. Yes. So I'm struck by, like, the amount of anchoring on surname because of the bias in surname. So I understand, like, race, but So I understand like race, but I'm curious like gender and cultural norms, like how certain names are passed. And like my wife and I deliberated hyphenating or merging. We decided to merge names. So I'm curious cases where if you looked at gender, female mortgage, people who are the primary lender being women, as well as high. as well as like hyphenated names or other just because like the cultural norms I feel like are pretty dominant. So you're saying like whether you should condition on gender as well or? No I'm just saying like it it's hard for me to imagine that race prediction from surname is as accurate for men as it is for women. Oh I see. So the accuracy may be people adopt surname but they're just about the norm of the way surnames work. Right. Yeah, so unfortunately, the census doesn't produce like raised surname distribution by gender. So only way you can account for the gender is to incorporate that in, like you can use this like, you know, census, geography, like aggregate information. There you can condition that gender to make the prediction better. Because I'm just asking like you did the robustness. It doesn't just ask me if you did the robustness and that robustness analysis of introducing Asian. Oh, with gender. Yeah. No, we should be able to do that. Yeah, no, I think that's a good point. I think we should try that. Yeah, that's an interesting question if surnames are more culturally representative for like women that have sort of miscultural expectations. Yeah. Yeah, 'cause my wife get misclassified as an Asian. Misclassified as an information. I also had a similar question around: are there surnames that have multiple dog increases attached to the American muscle weight? Yeah, that's why it's very difficult to distinguish black and white unless there's in areas where there's assimilation, it's much better. That's why you have to jump. Can you go back to the slides where you were just starting the racial results? This one? The next slide. Oh, next one. Yeah. So I wanted to ask about: do you have any sense of how much there's differences in the rate of taking the itemized deduction versus the standard deduction? And if that could influence this. Yeah, I think we should look at like a yeah, so if you take a standard deduction, you're not. So if you take a standard action, you're not taking HMA. Yeah, I guess we couldn't look at and see. Yeah, so these are not adjusted by the standard reduction. I don't know, people who are taking standard reductions. So yeah. So I need to I guess we need to like look and see like adjust T is expected value by the standard deduction. Value by standard deduction, but yeah, how many people take standard deduction? I had a push on the improved risk probability. So I get that you're using the VISG properly. You want to look at this? From like a methodological level, which posterior update, what is it that you're learning from the why? It's learning from the Y. Yeah, so it's basically additional conditioning on the outcome. So to the extent the Outcome. So, to the extent the outcome is giving more information about race. So, then if there was no racial disparity, would you still expect group racial disparities? Oh, yeah, that's interesting. Yeah, no, I guess you're right. So, if yeah, if there is no racial, yeah, that's right. So, then we wouldn't see that. So, when there is a racial despite, we have more true and here there is a simulation power sudden point. Southern both politics, so yeah. I was just curious, how handle um like surveys are not even sort of frequent with norms. Like, I know the census file censors any cells, like for a survey modern use group, it has like below a certain freshman. But you would imagine for certain groups, like all our ethnic groups in the US, like certain racial essence racial groups that we win through. Yeah, so if you don't have surname information, you know, for the small groups, you basically only rely on the census location information as your prior. Yeah, I mean, how does that affect on the racial disparate destination? I mean, that's a very good question. Like you can see you can see like a lot of bias in the small groups. I mean, that's partly because I think And that's partly because I think the BISG probably de-estimates are very good for Asians and Native Americans. There's different problem there. 30 questions? Yeah, so my intuition is that as the surname distributions for different racial groups, like if they become sort of closer together, everything is still kind of identified under the assumptions that you present. Kind of identify the assumptions that you present, but like the estimation probably gets harder. Maybe it's in this ID interpretation, it's like the strength of the ID or something like that. Oh, right. And so, I mean, this evidently doesn't seem to have been a problem in this application, and you have excessive administrative data sets and things like that. But I'm curious if you think there's settings where this could become an issue. Maybe, especially if you're interested in conditioning on more covariates or more heterogeneity, maybe do you need a strong IP across the entire space or things like that? Yeah. Yeah, yeah. I mean, I guess if there's a, you know, racial polarization is really strong, but you know, Sardinia is very predictive of the, I mean, at least in this country, in, you know, in the US and Canada, I think. Yeah, so I'm not too worried about the strengths of the insurance, but like things like we don't have base distribution. Distribution, sorry, distribution for the less frequent new coronavirus, and that tends to happen for the minority group. So the accuracy gets sacrificed for those groups, which is always a problem. Oh, should we get a little bit? Yeah, we might. Oh, oh, is it not the straight? Okay. Do you want to have a last question? It's not that important. My question kind of. Uh it's not that important. My my question kind of related to that is like do the exact zeros cause a problem, like positivity violations, like the probability of uh uh of being overrays given a particular set name and g RP being exactly zero? Yeah, so we tend to give some prior that fixes some of that issues. Like census has its own, you know, like if you if you use like a census block, you have lots of zeros. Um so we tend not to use census blocks. Um use since it's fraud um like sensitive since it's broken um or when you use it you might want to put some prior to make sure it's not exactly it's not lunch so make your way down after You can't run it code yourself. Yes. Yeah, but you got to be allowed to be a little bit more. But there's different things. Oh yeah, I have all that. Uh really quick announcement. Uh, just a really quick announcement. Everyone okay, sorry, uh, really quick announcement. So, today lunch ends a little bit early at 1 p.m. If you are interested in the working session and the white paper, obviously, if you're uninterested in the white paper completely, please use that time for collaboration otherwise. But the working session will be back in this room or in this foyer starting at 1 p.m. There's going to be a coffee break at 3 p.m. and Coffee break at 3 p.m. and the kind of like formal academic program collaboration notwithstanding, we'll start again with the provocations at 3:30 p.m. Also, in here, so just schedule a schedule because then we'll be able to do it in my mind. Yeah, right. All the stuff.  I want to chat with you because we're not different from the same phone. My question was: I would be afraid if you're if you're using like that. So there's some few people like that. So I would say unfortunately for the more that's coffee, that is there is basically all the better scale. And if you do that, you get a kind of lives for the spelling. Yeah, which is a lot of cycle basic. Yeah, so you use wires and shields, so that is the question first, but also recognize that sometimes even though I would tell that's even fun, you need to share local service, like really local data, but just some expressions on the base and use it as like models. So that's something that I think actually looks like. I think one of the things I get like. 