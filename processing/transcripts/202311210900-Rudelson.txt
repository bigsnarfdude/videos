Beautiful place. I'm joining Park every time. Come. This is a joint work that my students are done. And I apologize, there will be no convexity, but there will be foul analysis. Okay, so we are going to talk about frames. So what is a frame? We take our end. Rn, and we take a system of vectors, which is a kind of an over-complete basis. So, to encode a vector, we take its inner products with the vectors of this basis and transmit it along a channel. And we want to choose the system of vectors so that this encoding. This encoding will be robust and we can deal with two types of corruptions. We can have a small noise added to all entries of the encoded vector, and a few entries can be entirely lost in the process of transmission. And the system of vectors, which does the job is a frame formally. Is a frame formally. This is a system of vectors such that if you evaluate the sum of the squared inner products, you get more or less a squared Euclidean norm. This scaling coefficient K of n, which depends on the size. Which depends on the size of the frame and the ambient space is not important. What's important is the frame coefficient R, which shows how precise this reconstruction is, and R ideally should be of order one. There is one simple way to construct a frame. We can take frame. We can take an orthonormal basis, repeat it many times and concatenate it. Then if we transmit a signal using this encoding we lose a few coordinates but we have many identical copies of the same vector so we see which copies are identical and this reconstruct. Reconstructs the original bit. This is a working system, but it's extremely inefficient. You have to transmit many redundant coordinates. And such basis doesn't have to be orthogonal. But if we recall that we have to deal with the random small noise as well, these bases should be more or less orthogonal, and this leads. Orthogonal and this leads to the concept of a Reese basis. Reese basis is actually a basis which is a frame at the same time. So it is a more or less orthonormal system of vectors. The real frames which are used in signal transmission, for example, in cellular networks, are randomly generated frames. And how to generate And how to generate a frame randomly? Well, consider a vector in Rn centered and isotropic. Then, if I take many independent copies of it, the law of large numbers says that the average of these rank one matrices will be the identity matrix. Surely. And if it is true in the matrix sense, True in the matrix sense, then the limit of the inner products of any vector by the vectors of these random vectors will converge uniformly to the norm of the vector. This is a limit statement, but of course we can derive a non-asymptotic statement as well and. As m goes to infinity, this means that with probability close to 1, we have, if m is sufficiently large, then it will be almost an isometry. How large M is was studied in details and the optimal estimate was obtained. Estimate was obtained around 97, 98, both in terms of n and in terms of epsilon. But now we ask a different question. So this is the most robust and the best way to generate a frame. And the applications people in particularly Particularly, the people studying the frames, like Martin Bonick, asked a question about the presence of risk basis in random frames. So let X form a frame. We can encode it like a matrix with columns X1, Xn, and then the frame property can be translated into the language. Can be translated into the language of matrices as the restriction on condition number. Condition number is the ratio of the smallest, the largest and the smallest singular values. And the frame coefficient is exactly this ratio. So we want the condition number to be about one. And indeed, if m is large enough, if m is Large enough, if m is n log n, this is true with high probability. Okay, now let me pass to the question of Margin Ponik. Suppose that we consider a random frame and let's increase m and we ask for which values of m does this frame contain a risk basis with high probability or With high probability, or even more, for which frames does this for which M's does a random frame contain many risk bases. If it does contain many risk bases, it's not more efficient than this trivial concatenated basis. So at this moment, the random construction becomes inefficient. Construction becomes inefficient. In the language of random matrices, the question boils down to the question of which M the very fat N times M random matrix contains a square submatrix or many square submatrices having a bounded condition number. Number and a beauty of this problem is that it's like an onion. The outer layer, as you see, is a random matrix theory. Actually, we will reduce it to a completely algebraic problem. And then, when we peel this level as well layer as well, we'll find several probabilistic and number theoretic layers beneath. Layers beneath. So let me state the theorem. And first, there is a phase transition. If M is not exponentially large, then the frame will contain no risk basis. And if we pass this threshold, then suddenly it contains many risk bases, and many means exponentially many. And many means exponentially many, although the coefficient C small is, of course, smaller than this C capital, but it's exponentially many responses. And let me formulate this precisely first the existence theorem. Let's suppose that the entries of this fat random matrix are IID symmetric and non-degree. Symmetric and non-degenerate random variables. So I don't need anything more than that. They may have they may be even not L1 entries, so the expectation is not defined. And if the number of columns is exponential in the number of rows, then there are exponential. There are exponentially many disjoint subsets of the set 1, etc., n, such that if you take a submatrix with columns from one of these sets, this is a square submatrix, then its condition number is bounded with probability extremely close to 1. It's not exponential in any... It's not exponential in n, it's double exponential in n. And this alpha is not like a crazy absolute constant. Actually, we didn't chase the optimal alpha, but alpha equals 4 would do. You don't have control of alpha, you can make alpha 1 plus epsilon. This is a question. I don't know how to make it. I don't know how to make it. I own Possum and I wanted to say this question for the end of the talk. But this is a reasonable, so far it's a reasonable constant. For the non-existent statement, we need to assume more about the entries. And we will assume that the entries are sub-Gaussian. I probably don't have. I probably don't have to explain what it is in this audience. And if we have a sub-Gaussian random variable and the entries of the matrix are its ID copies, then when M is smaller than exponential, depending on this parameter t there will be There will be no square submatrices with condition number less than t with probability very close to 1. Again, and this is even not again exponential, it's exponential of m squared. And all the constants here depend on the sub-Gaussian. I'll have time to discuss only one of these theorems: the non-existence action. Existence actually requires a proof, but the tools were already developed in non-asymptotic random matrix theory by Darshinin and his collaborators. And so these are tools known to specialists. The existence theorem in consciousness required new. To it required some new ideas. So, let me pass to this. How do we create a sub-matrix with a bounded condition number? Let's take a deterministic matrix V, which is nice, which has a bounded condition number. And let's look at submatrices which are very close to this V. This V. So, what I am doing, I take, I sample the columns one by one until I find the column very close to the first column of V. Then I continue assembling until I find the column very close to the second column, etc. And then I form a square matrix. And since the columns are very close, perturbation theory will tell you. Will tell me that the condition number will be not much bigger than the condition number of V. And then we repeat this procedure, sampling another time, the first column, the second column, etc., until we find many such submagences. As I described it, it actually doesn't work. It requires a super exponential number of samples. Number of samples to apply perturbation theory because we need the counts to be very close. But there is an easy fix. We can relax this condition and then these sampled matrices will not be close to the original matrix V. Original matrix V, but they will be closed with high probability. More precisely, we take the columns which are relatively close to columns of B and condition on this fact. And it turns out that the condition number of this conditional matrix is small with high probability. And then, And then we use the same algorithm, we sample the columns, we have many such square matrices, we discard very few matrices which have a large condition number, and we are done. This looks like the proof. What is swiped under the rug is only the question where we start from? What is this matrix V? Start from what is this matrix V around which we find as the entries. So, first, the ideal matrix with condition number one is the identity. It doesn't work, of course, because we have ID entries. Each entry is non-zero, so with a certain probability. A certain probability. So the L2 norm of which column should be square root of n. Here we have L2 norm of a column being 1. This is easy to fix. Let's multiply it by square root of n. But this doesn't work either because our entries may be bound. And then how do we generate a diagonal entry of square root of 1? So we need something more. So we need something more. And if we look at this generality, one of the examples of random entries may be random plus minus one entries. And then the values of the coordinates should be two points, a and negative a or random plus minus a entries. Well, and of course we are. And of course, we are reluctant to drop this example because this is the most natural example. So, the only pattern matrix V we can use is the matrix with plus minus one entries. And let's recall what it should satisfy. It works. The norm of each column is square root of n. The only entries are plus minus ones, or we can scale them. Once, or we can scale them. And in addition, if you find such a matrix with a bounded condition number, we are done. Alright. And there are such matrices. Adamar matrices do the trick. They are scaled isometries. Adamar matrices are matrices exactly the matrices with plus minus one entries and condition number one. And condition number one. The first such matrix is a Walsh matrix, which was actually constructed by Walsh. Usually in mathematics, the names of two match. It was constructed by Sylvester. And you can describe it starting from the 2x2 Walsh matrix. And then you tensorize it as many times as you want. As you want, and you get the matrix of order 2 to the n times 2 to the n. Or alternatively, you consider a group minus 1, 1 to the power n and the columns are the characters on this group, which are Walsh functions. Actually, there is a whole theory out of our matrices in algebra. Matrices in algebra, there are even books entirely written on Adam R matrices. And the current world record for the existence of the Adam R matrix is the orders which grow polynomially where k is 2.3 something. So when we have a dimension where we have another R matrix, the problem Where we have another bar matrix, the problem is solved. But we want it to have in all dimensions. And so far, as we saw, there are big gaps between dimensions where RMR matrices exist. And moreover, it's known, and this is an exercise for an undergrad, that if a Adam R matrix exists, then the size of it should be divisible. And the size of it should be divisible by four, just because of the orthogonality of the rooms. No, any Adam or matrix, any plus-minus one matrix, if it's orthogonal, then the size is divisible by four. The fact that it's divisible by two is obvious. The inner product. The inner product should be zero, but actually it's like and so we cannot do it with RMR matrices. So let's define approximately RMR matrices, which are plus-minus one entries, which we are required to have. And the condition number. And the condition number should be about one. So let's summarize what we did. We reduced our original question in random matrix of theory to a purely algebraic question. Does there exist an approximate atom or matrix in any dimension? And at this time, And this time, probability is out of the story. We are with the algebraic problem. So let's try to solve it. Well, if we solve it, if we are doing probability and analysis, the first attempt would be to solve it a problem. To solve it probabilistically, let's pick a matrix with so first the statement is that this is indeed true. For any n you can find such a matrix and again the ratio of C capital and C small is not too large. So, how do we construct such a matrix? The first attempt would be The first attempt would be to try a random matrix. Take a random plus minus one matrix. And it was proved by Vershinin and his collaborator in a greater generality that if we take a matrix with a sub-Gaussian entries, plus minus one is of course sub-Gaussian, then its condition number is. Then its condition number is about n with high probability. This is one of those conjecture. It was proved less than 15 years ago. Okay, so we have this result, which means that we are not expecting the random construction to work. Actually, the closest is even. Actually, the closure is even worse than that. The probability that the random matrix has a bounded condition number is exponentially small with exponential depending on n squared, not on n. Okay, so the second attempt. We have anomal matrix in some dimensions. Let's take a bigger dimension and let's extract a sub. Let's extract a submatrix of it by removing k rows and k columns. Hopefully, it will work, especially if we have restricted invertibility of morgues of free. Doesn't work either and doesn't work in a very strong sense. If we take if k is 1, we have a Adamar matrix in the dimension 1 larger and Larger, and we cut out any n minus 1 by n minus 1 minor of it, that the condition number is about n. So this doesn't work either, and we have to construct this approximate guide of our matrix from the scratch. And we will start with sizes with the simplest possible sizes, the simple possible numbers are. The simple possible numbers are primes, and let's discuss the construction for primes. First, we will search the literature for a matrix for which the singular values can be evaluated easily. And there is one class, the circular matrices. So we have vectors v1, etc. Vn, and then we Then we permute the entries by putting the nth value, sorry, the shifting each time cyclically by one and get this circling matrix. And it is known that the circulant matrix is normal, so the singular values are absolute values of the eigenvalues. And the eigenvalues are the Fourier. Values are the Fourier coefficients of this generating vector v1, vl. So if we can find a vector with plus-minus one coordinates whose Fourier transform has an almost constant value, we are done for prime sizes. And there is a classical elementary number theoretic construction which is Theoretic construction, which is the Legendre simple. So the theorem here is that if Q is an odd prime, then there exists a vector with plus-minus one coordinates whose Fourier transform is almost a constant. It's square root of Q with the error of a much smaller size. Size. And here the main idea belongs to Matolsky, sorry, Jamin and Matolsky, whose paper is based in Turin on a paper of Matolsky and Ruzhan. So we'll use Legendre symbol. What is it? We take ZQ as a group and we consider the And we consider the set of quadratic residues. So the set of numbers which can be represented as squares of some other number. And then the Legendre symbol is 1 when k is in the set of residues, negative 1 when it is in the set of non-residues, and 0 when it is 0. And calculating the Fourier transform is a standard thing in number theory. It was actually done by Gauss. The Gauss sums show that this is square by absolute value it's square root of Q if it's non-zero, so it's constant and zero when it is zero. Actually, Gauss sums. Gauss sums show the exact values of this, but obtaining the absolute values is much easier. So, it's almost what we need. There are two exceptions. The zero value of the Legendre symbol is zero, and the zero Fourier coefficient is the first one is very easy to deal with. I just replaced the zero say by one or negative one. Say by one or negative one, doesn't matter. The impact on the Fourier coefficient is negligible. This one is harder. So, how do we deal with that zero Fourier coefficient? Let's change some of the values of the Legendre symbol. Namely, I take the set of the square residues and And take an independent random variables taking the values plus and minus 1 with a certain probability and replace 1, which is the value of the general symbol at the quadratic residue, by either 1 or negative 1, depending on the value of that state. This probability is chosen to make the value of the zero Fourier coefficient. The zero Fourier coefficient square root of Q in average. So, whatever we mean. And then another Gauss-Sams calculation shows that the average value of other Fourier coefficients will be either one or zero. Actually, I wrote capital of one, it's either one or zero. So it's this is. So it's uh that this is uh this is perfect and uh sorry is a either one or minus one. And if we know this in average then we we can if the concentration of measure works we can say that the same holds with high probability. probability. And this is indeed true, this is a standard argument in concentration of random linear combinations. And this allows us to construct a circular matrix which is approximately a dollar. The problem is solved for prime numbers. Well, let's extend it for primes. Primes. First, there is a generalization of Legendre symbol, which is Jacobi symbol for any natural number. And Jacobi symbols have the same properties that Legendre. They have the Fourier transform the same except you. Except few exceptions, but in these exceptions, the value of the Fourier transform is high. It's not square root of q, it may be q, q to the three halves, q squared, etc. And suppressing these exceptional values doesn't look physical. Another attempt. If we have atom or matrices, then an elementary algebraic lemma states that the tensor product is a domer. And the order of the tensor product is product. Is a number. And the order of the tensor product is the product of the sizes of the matrices. So if I have an n which is the product of the primes, I take these approximate adamers and take a tensor products of them. Hopefully this will be approximate Adam R. But it will fail because we have not exacted a mark. Because we have not exactly the Mars, but approximately the Mars, and there is a cumulative suppose that you have three to the large power, so you have the constant error, and this error can be as large as you wish. So, we need something else. We need a construction of natural number from prime number. From prime numbers, and this construction should involve only a few prime numbers. Products do not work. We look which construction does the job and there is one of the pinnacles of analytic number theory, the Magradov's three primes theorem. I'll formulate only a half of it now. If n is an even If n is an even number, then it can be represented as a sum of four primes, and later it was proved that all these primes can be approximately the same. So, why don't you just take an integer divisible by 4 plus either 2 or 3 or 1? Plus either 2 or 3 or 1. One reason, I don't have. I don't have an approximate LMR for integer divisible by 4. But you haven't. I have primes so far. But you said. I said the gaps are growing like a power. Second, there is no Adam or matrix in dimension 3. But you have approximated. Yes, okay. But again, I have I have large gaps between the exact orthogonality. And plus doesn't work for tensorization. You multiply. I have plus here, right? But here again, these are primes. Primes. And there exist primes. They are not. We cannot claim that these are 3, 2, 1. There exists some primes. I do understand. Let me maybe understand this also to just clarify. So the exact orthography system, orthogonal to dimensions, which are divisible by four, no. They don't not exist if the dimension is not divisible. There are any dimensions, it's not difficult by four. But existent powers, of course. Yes. Okay, so back to the list. Here we have a representation of a natural number as a sum of four primes. Well, the number of primes is fine, it's bounded, it's jolly good. Why it's called three primes. I formulated uh only half of it. The second half is about odd primes. And for those there are three. Sorry, for odd numbers, for those there are three primes. Okay, so now there is a question what to do with this wonderful representation. And we have four primes for. For primes, to primes we have constructions, and the basic we have four different matrices almost the same, but not the same size. What to do with them? We can put them along the diagonal, but that will be it. The off-diagonal space is empty. We don't know how to fill it. How to fill it. Want to fill the whole matrix. So we work like tailors. These are matrices of different size. Let's make them matrices of the same size. We just chop them by the size of the smaller matrix. And then we have one square matrix and three rectangular matrices. Very nice. And now we can put them And now we can put them one next to another, like this. And these, the rows of this matrix will be almost orthogonal. Very good, but this is not a square matrix. Let's make it bigger. Let's copy this times. This matrix, of course, is This matrix, of course, is nothing like almost orthogonal because I repeat the same row and the inner products will be huge. But there is an easy way to make it almost orthogonal. I take a 4x4 Walsh matrix and multiply each block by the entries of this 4x4. And it turns out that this problem. Turns out that this product an almost scaled identity. So we have a nice algebraic construction or semi-algebraic since we used probabilistic perturbation for prime sizes, but this is not a square matrix. These matrices, the size of it, The size of it are four times the size of the smallest of the four matrices. And the number of rows. And the number of columns asks the sum of these sizes, which is n. So we have a gap of 4 epsilon n and we have to fill out the bottom of this matrix. This matrix. And again, we work like tailors. So let's recall that here we cut part of the material off. We have these odds and ends. Let's use them. We take these remaining pieces, stud them on the Them on the diagonal and then declare the bottom part of the matrix. Then we have a matrix of the right size, it's square, n by n. That's what we wish to have. But we ran out of material. Now I cannot repeat the construction I did at the top because these three bottom matrices are of different size. Are of different size. So, what do we have? If we ran out of the material, we don't know what to do. We fill the gaps randomly. And actually, this works. And this is the construction, at least for the even sizes of matrix. Even sizes of matrices. The odd sizes require an additional twist, but since I ran out of time, I'll not mention it. Yeah, about the existence of other market matrices. There's something Yes, I mentioned it. There are books written on that. And the current world record is the matrix is in polynomial size. Yes, actually it's in the part which I had to skip because of the lack of time. So, regarding the question about So regarding the question about alpha being 1 plus epsilon, in those dimensions where you have other matrices. Alpha is 1. Alpha is 1 for these matrices, but for random ones... For random ones, the condition number is n. I know that. I mean, if you... So your theorem is based on approximating your random matrix with matrix similar to your approximate matrix. Similar to your approximate Adamar matrix. So, what if you additionally know that your approximate Dademar matrix is actually Adam R? Then you have control of your alpha? Yes. Yes. But then it means that probably in every dimension you can construct at least a matrix twice the size, right? And then project. Twice the size. I'm not sure. But twice the size because Hanover matrix is existing every fluid. Yes. To them? Yes. So at least what to do with the larger matrix? You cannot, deterministic, you cannot do it at random, maybe you can project. At least you cannot select sub-matrices. This is a deterministic statement. Whatever sub-matrix you select, it's fan. Maybe for applications, it's projection. Projection, if you project, apply. Projection, if you project a plus minus one matrix, how do you get plus minus one entries? Not at the level of deterministic. We get a question of whoever you said, I forgot March and Bonik. Maybe when we phrase it, we get a bit flexible up. But that's not this way. Again, if the entries are the original random entries are plus minus ones, the only thing for the pattern thing for the pattern matrix that you are allowed to choose is a plus minus one matrix. I will square. You misunderstood what I've said and I'll square by some matrix. Okay. Questions? You will thank the speaker again. No, I shouldn't keep up with the time.    This is mostly notes. But this is uh I think I finally got pretty much that I couldn't. But there's a couple of things here and they're not looking at all this. I shouldn't be this computer.  Okay, it's my plan that we introduce our second sketcher on some reasons for you when you speak about volume rate projections of volume tools. This is very much the notation.