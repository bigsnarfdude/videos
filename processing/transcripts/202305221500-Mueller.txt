Very happy to introduce Karl Mueller. He was supposed to be with us, but probably had problems with the volcano. So he will give his talk online. And it will be about the radius of a star polymer. Yeah, well, thanks for the invitation. I would certainly have been a pleasure to attend in person. But I think it was just bungling of the airlines, not any volcano. So, in any case, I'll repeat what I said. This is an online whiteboard. So, if you wanted log in to see past, to see pages that I've scrolled past, this is the web address, and here's the password, the passcode. The pass code. So, radius of a star polymer here. So, this was with E.L. Neumann. And so, I'll start out by just reviewing the situation with random polymers. So, as you know, a polymer is a model like DNA that has segments strung together. So, here are the segments, and the angles may vary in a range. Angles may vary in a random way. And so the segments are attached. And we can model this by a Brownian motion, a random walk on a lattice. Or Brownian motion. So here I'll just draw a typical path on a lattice. Okay, and notice that you could get trapped. So it's not a Markov process because you can't always extend it forward. And so let's see. So actually, now if you haven't got, I hope. Now, if you haven't got, I hope you if you want to log into the whiteboard that you've you've got this address because I have to scroll forward. Okay. And so now these have the prop, you want to avoid self-avoidance. So I already jumped the gun. So here, you know, you could have it get trapped, just like I've shown here. Like I've shown here, you don't want segments to overlap each other because that's not physically realistic. So you want to have self-avoidance and you could have strong self-avoidance and For the length n, you can let Cn be the number of self-avoiding blocks of that length of self-avoided logs. And your probability measure is to give each of the self-avoiding walks probability one over Cn. C capital N. Okay, so that's the strong self-avoidance, and you could have weak self-avoidance where you just penalize the paths. So you give a weight to each path, weight the path as. S n n less than or equal to capital N by what I'll call E sub n, E for exponential. And you have a weighting factor, beta, and you have a sum n not equal to m, and of course, both of them have to be less than or equal to capital N of indicator. That Sn is equal to Sn. So these are the things you want to avoid. So you're penalizing by the number of such overlaps that occur. And so your probability measure Qn of a set A is equal to one over a normalizing factor Zn of expression. Zn of expected value of the original expectation of the indicator function of the set times the weighting, the exponential weighting factors. That's Qn. And so you may ask about the difference between these. Actually, physicists believe in universality. And so they think that all of these models should have roughly All of these models should have roughly the same behavior. So I should say, too, this we're going to consider walks on Z2. That's where we have the most striking results. Well, I'll restrict to that situation. We'll actually do Brownian motion, but let me just write down walks. Okay. And so Okay, and so one thing you can consider is you have this polymer, and what's the radius? How spread out is it? So the radius, R sub N, is often defined to be the expectation of the end-to-end distance of. Of Sn let's see, I have to put absolute value squared to the one half. And so we're going to define it differently later on, but in the spirit of universality, all I'm looking for is some sort of measure about how far apart things are. And just to And just to um uh just to be um you know indicate why you know this isn't the be all and end all. Here's maybe the path of a polymer. So you see the end-to-end distance in this picture is very small, but I would say the radius probably is pretty big. So in any case, there are different definitions, and there's a well-known conjecture. That in some sense, which people don't always make precise, Rn, the radius is approximately n to the new with in two dimensions, nu is supposed to be three-quarters. Three quarters. And actually, this is so Stez Smirnov and Dumino Kapan say this is one of the most challenging problems in probability. And it's according to a survey of Bauerschmid and et al., it's Not known if new is greater than or equal to a half. So one half would be just the simple random walk without any self-avoidance. And it's not known if new is strictly less than one. If you're dealing with the random walk, you know, it has their end. Their end segments, so the end-to-end distance can't be more than one anyway. Can't be more than n in any case, and so nu wouldn't be more than one. But we don't know if nu is less than one. So not much is known. Actually, Lawler and Werner found that if the cell phone If the self-avoiding walk has a scaling limit, it would be SLE eight-thirds. But this is a long-standing problem whether there is such a scaling limit and. If there is such a scaling limit, and probably it's not going to be solved very, very quickly. And if you have this scaling limit, then using properties of SLEA thirds, you would be able to solve this conjecture. Okay. So all of these are very hard problems, and I don't have any solutions to them, but sorry, Carl? Yeah. A car, yeah, yeah. So, is this um last statement a conjecture, too, or is it proof? Yeah, this is um, yeah, uh, it's not known. Um, but if it had a scaling limit, so the if, remember, yeah, so people don't know that. Um, no, no one that's a famous unsolved problem also, but if it did have a scaling limit, it would be SLE eight-thirds, so they know that, okay. Okay, great. Yeah. And so I'd like to broaden the scope of the question. And of course, polymers don't always, they're not always just linear polymers. It could be, you can have branching in polymers. That's a common situation, but that has its own difficulties. So I'll just say, maybe. So, I'll just say many polymers are branched. Okay, but that has some difficulties as well, and so we're going to study what's called star polymers. And as it suggests, this is. And as it suggests, this means you have an origin and you have a bunch of polymers coming out from the origin. So you have capital N polymers length capital T and so our model is Is going to be Brownian motions. BK sub T and T is less than or equal to capital T and K goes from 1 to N. And so this is a more practical model than you might think, because if you do a Google search for star polymers, you'll find a lot of papers in the chemistry. You'll find a lot of papers in the chemistry literature about practical things like how to synthesize star polymers. And so it has quite a bit of interest. And I want to point out again that these B's are taking values in R2. I want to consider the two-dimensional case. Dimensional case, and now let me define what the self-avoidance would amount to under this situation. Okay, so we want to consider occupation measures. So, as L T of X is going to be the operation of the occupation Is going to be the summation k equals one to n the measure of the time from zero to capital T such that the kth one vk t is in a ball of radius one around x. So, as we know, two-dimensional Brownian motion doesn't hit points, so you have to have this ball or something like that to get anything meaningful. And so, this is the occupation measure. And our penalization factor, ET, is going to be defined as e to the minus beta integral. Over Rd, maybe I'll put R2 because we're only discussing two dimensions in this talk: Lt of X squared dx. And so you may wonder why I just don't have L T. Well, if I did that, I would just get the total time. So it'd be n times capital T. So this penalizes situations where all of Situations where a lot of the paths pile up at the same position. And so we define the penalized probability in the same way. Qt of A equals one over a normalizing factor times the expectation, Pt is the original probability. Original probability times indicator A times the penalization. And let's see. So I'll just point out here, this is our method of proof. I haven't said what the theorem is yet, but note. Yeah. Carl? Yeah, I think Devar has a question for you. Oh, okay. Yes. Go ahead. Just as a point of clarity, Carl, is P T the measure P of paths that are determined by time T? That's right. It's just you have n Brownian motions and you just let them go up to time capital T. Thank you. Yeah. So to So, um, to bound QTA above, you need, or at least it would be sufficient to do the following. You need an upper bound. So, you notice here, this. So, you notice here, this is a fraction. So, you need an upper bound on the numerator, upper bound on E P T indicator A E T and a lower. A lower bound on C said T. So I should say, you know, this general strategy, I think it goes back to statistical physics and it's quite probably goes back quite a ways. But in these sorts of problems, Boldhausen was the first to use this method, of course. Then you have to carry out these steps. But in any case, But in any case, so now let me define the radius, which is going to be a little different, but I hope you may believe that based on this idea of universality, it doesn't matter too much exactly how we define the radius. So this should be the median of soup. Being a member of Surota team. T a member of zero to t of absolute value or the length of the vector bk t and and so that that would be the entry of a set and I want to do this for k equals one through n. So for each k I get this supremum. So, for each k, I get this supremum, and then I find the set of numbers for all the different k's, and I take the median of that set. And so now I can discuss the main results. I doubt if I'll have time for very much more. Okay, we also get results for, so I'll remind you: this is for d equals 2, and we all also get results for d equals 1 and 3, but they're not as maybe not as surprising as this. For beta and n greater than or equal to 1, remember n is the number of paths, and beta is the factor that goes into the penalization. Factor that goes into the penalization. There exists a capital C greater than zero such that for this constant less than t less than capital N Q T, so that's the penalized probability measure of measure of, oh, and there exists little C2, big C2 greater than zero such that, okay, little C2 beta to the one-quarter, n to the one-quarter, t to the three-quarters. t to the three-quarters log beta t to the negative a half is less than or equal to the radius is less than or equal to c capital two beta to the one quarter n to the one quarter t to the three quarters To the three-quarters log beta t now to the positive one-half. So the probability of this is supposed to be close to one. And so we say this probability is greater than or equal to one minus. And so we have the error for this as well. Another constant times beta to the one-half. n to the three halves t to the one half log beta t. So I guess there should be a little c here also. So there, let me point out some features of this. There are logarithms, but except for that. For that, whoops, as you notice, we get t to the three-quarters on both sides, upper and lower. We get the same powers for n and also for beta. So we were quite surprised when we got this. You know, remember, it's a different model than if you just have one path. Different model than if you just have one path, but um, you know, this is supposed to be quite a difficult problem to get t to the three-quarters. Um, I should point out another issue as well. So, so I'll draw another picture of this star polymer. So, here it is. So, so you have a lot of paths, and so as you can imagine, if n is large, if you have a lot of paths, you're going to A lot of paths, you're going to get a very dense region in the center, and then after a while, you know, these the paths will go further out and they won't interfere with each other as much. So, in other words, what you should think of in our theorem, we had t less than n. So, here Here it would be where you would spread, you see the t less than n situation. So if we could deal with this part of it, then we would more or less be able to solve the problem for a single path, which is this well-known unsolved problem. So we can deal with it in here, and we actually get the The correct power of t. So, you know, at least it's, you know, I'm not sure if other models have been able to do that. So let's see how we're doing. Okay, good. I have five minutes, but what am I going to do in that time? Maybe another quick question. So your exponential term on the right-hand side is Exponential term on the right-hand side is not small. Is there a side missing? Yeah, thank you. Thank you. Yeah, that there should be, of course. I know that this might sound like a contradiction, but C can be negative, guys. Well, okay, yeah, but except, you know, I said here, it shouldn't be negative. Yeah, but I that's true. I could confound people that that way. Let's see. I think in the four minutes that remain, I might be able to give intuition for one path. If Rt say is less than R, R is the radius, then let's think that the radius is actually the supremum of the path. Oops. Okay. Okay, and ignoring edge effects, you know, R is supposed to be defined in terms of a ball, but if we ignore that by Cauchy-Schwartz integral LT squared Squared is smallest when L T of X is constant in this ball of radius r. Okay, so that means del T would be L t would be t over r squared on the ball and integral l t squared. Let me save time not by writing the l t of x. So that would be t over r squared squared. And we're integrating this over the ball. And we're integrating this over the ball, and that ball has radius r, so it would be about constant times r squared. And so this would be t squared over r squared. Okay, and then also the probability that the soup The soup BT is greater than R. So now we're going the other way is something like X minus X. minus r squared over 2t okay so in other words this is the exponent we get for the less than or equal to this one and this is the exponent for the greater than or equal to and if we set these two equal as we're as we're supposed to commonly do in these sorts of Commonly doing these sorts of optimization problems, we get r squared over t, I'm forgetting the factor of two, is equal to t squared over r squared. And so r to the fourth is equal to t cubed, and we get, and remember, r is our guess for the radius, r is Guess for the radius, R is t to the three quarters. Okay, so now I've done it just at ending at the right time. So there you have the intuition for a single path. Now I'll quit. Thank you. Are there questions for Carl? So, in the end, why is it easier to treat the capital N path than just one path? Yeah, because if you have all the paths on top of each other, then you have quite a bit of, the penalization is stronger because you have a lot of overlap of the paths, and that makes it easier to deal with. And that makes it easier to deal with. So, in fact, we can, it helps if you can figure out what's happening. And if you have a lot of penalization, the paths are just moving out as fast as they can. But here, it's not exactly clear what the paths are doing. You know, they might be moving outward, but they could just as well be moving to the side or doing some sort of un You know, doing some sort of unidentified thing, and I think that's part of the difficulty of why it hasn't been solved so far. But you're not using any kind of averaging or let's see, any kind of averaging. Yeah. Well, let's see, I'm not quite sure what you would mean by that, but no, we're not really using average. No, we're not really using averaging. Oh, you mean because there are so many paths? No, we're just using, it's more that in the center there is a very strong effect from the penalization. Okay. Are there more questions? All right. If not, thanks again. Nice. All right, if not, thanks again, Carl, for this nice talk. Okay, uh, we might still have two minutes for the next law.