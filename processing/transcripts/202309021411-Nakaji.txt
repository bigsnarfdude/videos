I'm Kohen Akhi, a co-sultain group in the University of Toronto. I was kindly asked to review the recent activity in Mata Lab. So today I'm going to introduce about the recent control research in our Mata Lab. Well, the Mata Lab is located in the University of Toronto and it is led by Alan Naspur-Guzik, who is very funny. I feel he's a very funny, exciting, and innovative guy, as you see. And the research area in the Mattel lab is quite diverse, like quantum computing, AI for material discovery, sales driving, love. So there are many things. And quantum computing research in our group is as Alan always say, it's wild. So many different people research different kind of topics. Different kinds of topics. But there is one guiding principle for our research. So the guiding principle is exciting applications, meaning our research must be related to some exciting applications. So this is the guiding principle. So the pursuit of the application naturally relates our research to the recently developed noisy content devices. Devices. As you know, Quantum computer has been rapidly developed recently, but it has limitations. The number of qubits is limited, and also it's noisy and no wrong direction, meaning the operational number of gates is restricted. Variational quantum algorithms proposed in Alan's group a decade ago had attracted significant attention, mainly because Mainly because it can be executable in such noisy devices. And there are many variants of big delays for quantum chemistry, as Arthur mentioned, and like quantum dynamics and quantum mechanics and so on. However, well, after in this decade, there are many researches about BQA, and some of them. VQA and some of them reveal that there are issues in VQA. One of the most severe issues of VQA is barren plateau problem. The cost function landscape becomes flat as we increase the number of units. And also there are other problems, like there are many local minima and many measurements are required. So considering the situation of So considering the situation of V2A, solving the issue of V2A is of course one of the hot topics in our lab. But still, we also try to find new algorithms beyond V2A in different time regimes like near term, early cold tolerant, and cold tolerant. Here I map the recent quantum computing research from our lab. From our lab. So today I want to introduce about some of those researches. So first, let me talk about the measurement optimization. But calculating the expectation value of an observable is an essential subluting, especially for near-term quantum organs. For example, in the QA, we calculate In the QA, we calculate quantum state by using parametrized quantum parameterized quantum circuit and calculate the expectation value of unobservable. So we calculate the expectation value of unobservable by decomposing the Hamiltonian as a summation of tensor power operators. And the naive approach. And the naive approach to estimate the expectation value is evaluating each PJ one by one. But if we apply this naive approach, the number of measurements is, of course, proportional to the number of terms. And it is troublesome when L is large. For example, when we use the electronic structure Hamiltonian, this L tends to be n to four. Tends to be n to 4, proportional to n to 4, where n is a number between. There are actually various ways to reduce the measurement cost. For example, if the Hamiltonian is written as xi plus ix plus xx, then it can be estimated by the measurement only in the xx basis, meaning the basis where xx is diagonalized. Diagonalize. It's the easiest case, but there are more advanced methods newly formulated for the variational quantum eigenzoller. But I want to note that those advanced methods are also applicable to other variational algorithms, such as variational quantum simulation, as we previously showed. But anyway, there are various ways to reduce the measurement cost. One of the examples of such sophisticated method is like locally biased classical shadow, LBCS. In LBCS, measurement generator generates some measurements according to a table of probabilities. The table of probabilities represent for each qubit, which Pauli operator is likely to be generated. Is likely to be generated. In this example, okay with one, x is generated with 10% probability, and y is generated in 70% probability, and so on. And according to this table of probabilities, the set of measurements are generated. So, actually, the tensor product of power is generated, and measurements are performed in the basis where basis where the stensor product of powers are diagonalized. Then we can obtain the measurement results for each basis and we can reproduce the expectation value. The table of probabilities are classically optimized in the pre-processing so that the statistical error for each measurement becomes small. So, this is how LVGS works. Actually, it is submitted three years ago, and there are many more efficient algorithms, efficient measurement generators have been proposed since then. And most of them outperform this LBCS method. And it is still an important direction to create a new intelligent measurement generator. Measurement generator. But we take another approach for this measurement optimization problem. What we did is instead of creating a new intelligent measurement generator, we create some scheme to compose existing measurement generators. So we compose multiple measurement generators. Each measurement generator can be LVCS or other more sophisticated measurement generators. Our scheme works as follows. We set the number of measurements M and the number of measurements is allocated to each measurement generator and each measurement generator And each measurement generator samples the assigned number of measurements, and we finally get n measurements, and we can reproduce the result of any expectation value of the most observable. This distribution ratio and parameters inside measurement generators are classically optimized in the pre-processing. So it's like a machine learning problem. And what we expect is that And what we expect is that by increasing the number of measurement generators, we can improve the quality of the measurements. This is what we expected, and this is what we checked in our numerical experiment. So in our numerical experiment, we compare the number of measurement generators and the required number of measurements for a given statistical error. For our scheme, we compose multiple LDCS generators. And as a reference, we use a shadow grouping method, which is one of the sophisticated measurement generators, without composition. We create a generator for different Hamiltonians. And the horizontal axis is the number of measurement generators, and the vertical axis is actually the number of measurements. Is actually the number of measurements for a given statistical error. So we see that as we increase the number of measurement generators of a scheme, GLB and CS successfully reduce the number of measurements. And when the number of measurement generators is five hundred, shadow gluten is better than GLBCS, but if we increase the number, If we increase the number to twenty five hundred, then our GLBHLBGS outperforms shadow grouping number. So this numerical experiment shows the validity of our scheme. So this is what we did in our measurement optimization. Now let me move on to a different algorithm, Hamiltonian simulation. Well, Hamiltonian simulation is an important sublooting in fault howling regimes, like for quantum phase estimation or for quantum dynamics. The goal is simulating the time evolution and extracting the property. Directly implementing exponential IHT is actually impractical, as Alfred has mentioned in the previous. As Alfred mentioned in the previous talk, it tends to require an exponential number of k. So, what we usually do is decomposing H as the summation. And for example, if this HL is a tensor product of Paris, then exponential IHL decline is incrementable with order n gene of gates and one single qubit flotation gate. One single qubit flotation gate, as we show in this example. So we can approximate the time evolution by the sequence of these exponential operators. There are various methods using these exponential operators, like totalization. But in a regime where the evolution time is small and the number of terms in the Hamiltonian is large, then Q derift is a promising answer for. Is a promising ancillary-free metal. In QDrift, with a certain probability, an exponential operator is generated, and we operate it to the initial state, and we repeat this process, and we extract the outcome. The probability is proportional to the coefficient in the Hamiltonian. Then, systematic error is governed by Error is governed by this delta. The delta is minus lambda t squared over death, where lambda is a value determined by the Hamiltonian, and depth is number of exponential operators, and t is the evolution time. So we see that as we increase the depth, then we can reduce the system. Then we can reduce the systematic error. But in other words, to significantly reduce the systematic error, we also need to dramatically increase the depth. Our proposed Q swap can suppress the systematic error without increasing the depth. The circuit required in our algorithm is not different from the one No different from the one in Tudreft. The only difference is it requires only one answer qubit and it requires one shallow component. That is the only difference. And then, unlike the case in Q drift, the systematic error in Qswift scales as build up to the power of K, where K is the order part. So as far as delta is smaller than 1, then we can reduce the systematic error by increasing the order parameter k without increasing the depth. There is one trade-off. The number of measurements increases with k, but it is less than quadr quadratically, so it's not so problematic. So this is the algorithm of Q squared. This is the algorithm of Q-squared. Here we show the numerical result of the time evolution with hydrogen Hamiltonian. The horizontal axis is depth and the vertical axis is a systematic error. Each line corresponds to each Hamiltonian simulation algorithm, and the black line is Q2F. point is Qswaf. The purple and pink ones are results of Qswift. And this Qswift2 or Qswift3, this number means the older parameters. And the other adopted lines are for the other algorithms, actually the totalization. So as we see, by increasing the depth, the systematic era decreases. And we see that our, as expected, Or, as expected, our qswift always outperforms qsreft as we expected. So, this numerical experiment validates the variability of using this q-swift. Especially, particularly, this q-swift can reduce the depth. So, we expect that q-swift is especially promising in the early fault target. In the early fault tolerant regime, where the depth is restricted due to some residual noise. Anyway, this is the research about the Hamiltonian simulation. Finally, let me briefly discuss our research regarding the differential equation. In fault tolerant regime, solving differential equations is also a promising application. The goal of the problem we target is solving a linear differential equation on a digital computer. As always, we map it to a matrix equation AU equals B by discretization and solve it by numerical algorithms. By numerical algorithms, where A is the n times n matrix, where n is the system size. There are many classical algorithms, and in those classical algorithms, the computational cost is proportional to n, the system size, and proportional to polynomial of the condition number, kappa. The condition number is defined as the ratio of the largest to the smallest. Of the largest to the smallest singular volume of A. There are also quantum algorithms for solving the matrix equation. And the predicate cost for the best method is proportional to the polynomial of cover condition number and independent of n. This is good because we want to increase the value n to be exponentially large. So the argument. So the complexity, the quality cost doesn't depend on n, it's very thin. However, this coupler tends to depend on n more than linearly. So, which means the dependency on copper may spoil the speedup on n. So we want to remove the dependency on copper, but sadly, there is a normal theorem that liquidity cost at least depends on copper linear. At least depends on kappa linearly for general differential equations. However, what we found was that for a certain class of the problem, we can actually remove the dependency on copper. So this is how it works. In classical computing, there is a known technique called Webbrick preconditioning. Vibrate preconditioning works as follows. works as follows. So there is an AU equals B matrix equation and by using wave break preconditioning we can transform it to a different matrix equation and we solve this transformed version of matrix equation to obtain the solution UP and then by using inverse paper preconditioning we obtain the outcome we desire. The computational cost of this whole process is polynomial of kappa B times polynomial of n. And this kappa B is conditional number of AP, not A. The point here is that if the original differential equation is in a certain class, belongs to a certain class, this copy becomes order one. This capital becomes order one. Meaning, the computational cost doesn't depend on the original condition number and only depends on the polynomial of n. So we can, by using this technique, as far as the differential equation belongs to a certain class, we can remove the dependency on the condition number. The definition of class is actually very complicated, so I don't talk about it here, but the class imposes. But the class includes many differential equations, like well plus on Elvenholes, time independence, Schrodinger equation, and so on. What we did is we created a quantum algorithm to perform the wavelet preconditioning in quantum circuit. So we performed the wavelet preconditioning and we solved it the transformed version of matrix equation and we performed the inverse wavelet preconditioning. Then we perform the inverse pre-conditioning and obtain the result. To solve this transformed version of matrix equation, the field equals is kappa b log kappa b over epsilon. But because kappa b is order one, it doesn't depend on depend on the original condition number. The original condition number. And also, it doesn't depend on M. So the creative cost doesn't depend on the condition number and also the system size. So what we did is as far as the differential equations belong to a certain class, we can speed up the process by using this favorite condition. This is what we did in this research. So, finally, let me briefly talk about our research, our ongoing research, beyond the British way. So, there is a variety of quantum algorithms, but as I said before, there are many issues. So, one approach is solving the issues, but another approach is creating the algorithm beyond 38, which works in. Beyond Britain A, which works in near-term content computers. So, what we are thinking, it's promising, the promising area we are thinking about is like combining the knowledge of machine learning, the knowledge of chemistry, and quantum computing. So, where we see the field of machine learning, there are many sophisticated approaches like transformer, GPT. Transformer, GPT, Lajang Boto have been proposed. And they are started to be applied to chemistry problems like for materials and fabrics and so on. So our idea is combining these approaches with quantum computing and create a new algorithm that works in near term. So here is the summary of today's talk. I introduced some of the recent usage in Matala. Some of the recent research in Atala, I talked about measurement optimization, Hamiltonian simulation, differential equation. Because of the limitations of time, I couldn't talk about the circuit decomposition, but it's also nice work, so I strongly recommend you to look at it. And also, I want to ask you to invite the author or the teachers to the seminars in your university or in your town. They are very nice people, so yeah. So, yeah, I will appreciate it if you do it. And for the future, our group is trying to find a new framework for near-term quantum devices by combining the knowledge of machine learning, chemistry, and quantum computing. That's all. Thank you. I don't quite understand the idea behind using the wavelets for the linear element. Wavelets with the linear algebra problem for solving. Typically, when you use wavelets, you component decomposition and you're able to throw away trivial components of the label decomposition. So doesn't that mean that my answer, even just at a very basic bare bones level, will be inaccurate? Your question is like doesn't that just mean that if I perform this procedure, won't my answer have a lot of error? If I start to throw away wavelengths, especially if I use the standard wavelengths on that arbitrary. So it's the recusion is like wavelets in a sense are a unitary transformation. Exactly. And they're spatially local, so they're finite and compact. And the idea is that some of the weights on the wavelet functions are very small, and on the scaling functions they're very large. But even when I throw away the weight of the function, I throw away important fractions of the overall answer. So wouldn't my result be very inaccurate? Result be very inaccurate on the procedure. So you probably are you mentioning the continuous version of the very transform or discrete version? I mean the discrete version. Because you're solving linear algorithm, right? So it's. I'm not sure the relationship. Actually, this is a continuous wavelet here? Yeah, it's discrete version. It's the discrete version. Discrete version and the paper is fitted for this pre-condition. So you create your own wavelet. Your own wavelet, you create your own wavelets and you can solve this problem. Actually, in that paper, not in our paper, but yeah. In that paper, the vibrate, the wave basis for the specific preconditioning is created. So maybe that makes a difference. I'm not sure actually. Maybe we can talk it offline. Sure, because I've seen these proposals for wavelet-based things to use cloud computation. But it seems to me that it's rushing all of the compliance. But it seems to me that it's brushing all of the complexity under the rug, um uh in terms of making it just very, very difficult to set up classically. And then on the quantum computer, that it becomes very easy and that's fine. Yeah. But it seems to me that actually designing the wavelets here in the cases that I've seen can be really difficult. So I was wondering if there was something new that was becoming something else that was being probably the part of the reason the way that partitioning works in Partitioning works in our d search is that you know the class is significant, probably, and the specific network is created as I mentioned before. So, probably that's a reason, but yeah, maybe we can. But also, then it may not scale well to larger systems. You would need to know exactly how the larger system is working, which is as complicated as solving a problem just straight, right? So, making it harder on the classical user before you put it on the block, unless there's some additional trick. Actually, we are working on that. So we don't have the numerical result. But yeah, we have the numerical result, but this technique can reduce the condition number. Because there is a quantum chemistry code called Madness, which uses creative grid points to solve quantum chemistry problems. It works particularly well in play. Chemistry problems and worked particularly well for theories like Cocham systems, but the highest that they can go is a six-dimensional integral. And I mean, it runs into these problems, so I would be very curious if there was some great signal from this, but it seems like this is just making it harder on the classical user. Why be related? Uh might not be related. Uh what is the constant uh in paper number one, this reduction that we lose the poly dependence on kappa subscript? The constant value, you mean the constant value? Yeah. Yeah, actually it's probably one of the token actually this kappa p doesn't depend on kappa, but this constant factor can be large, so it might be the problem yeah, which should Or maybe we should investigate data. Yeah. Thank you. Nice for that.  Thanks. Hi, I'm Olivia. I'm an assistant professor at UBC. And I'm very aware that I'm what's standing between us and the coffee break. And it is almost 3 p.m., so like lowest point in the afternoon. So I decided to do more of an infotainment talk about compilers. I think it's a really interesting topic. It's one of the things that I It's one of the things that I work on in Micron. And hopefully, it will be useful for those of you who are considering running something on actual quantum hardware. Alright, so I like to start these kinds of talks, though, with definitions of what quantum software is, because I say that I work on quantum software and algorithms. So I'm going to define it as any software that enables the expression and execution of quantum algorithms that can run on quantum hardware. So the software that takes us from us to the QP. Us to the QPU. And that includes a lot of things. So you've got your high-level frameworks like CERT, Qiskit, Penny Lane. So how many of you have used one or more of these frameworks, just so I get a sense of... Okay, fair number. Awesome. Yeah, so these are quantum software. That's pretty clear. Compilation tools, though, are also quantum software. So is the software that allows us to actually control the hardware, the things that send the pulses to the real devices. And the reason for that can. And the reason for the can is that quantum simulators too. So just simulating the actual process on your classical device, also quantum software. And this is, of course, a non-exhaustive list. We're really spoiled for choice now in terms of like different options for simulating and also running hardware. But this talk is going to be about compilation tools specifically. Now, to talk about compilation tools, it's useful to Compilation tools, it's useful to go back to classical computing and classical compilers. So, how many of you have used a compiled language before? Awesome. So, most people. How many of you actually know how the compiler works? Okay, a couple people. So, 70 years of programming languages advances in computer engineering has brought us to a point where basically this is what it looks like to run something on a classical computer. You have an algorithm, you write some stuff. You have an algorithm, you write some software in a human-readable language, you push a button, and it runs on the CPU. And you do not have to have any knowledge of what comes in between. But of course, there's a lot of stuff there, and the compiler is doing a lot of work. So I made this example quite a few years ago now just to kind of highlight why compilers are important, because we write in a high-level language like C. But under the hood, what's happening is that there's this whole process that's turning the C code down into the machine code of Down into the machine code of your hardware. And that involves quite a few intermediate steps, one of which is turning that into kind of a machine-independent assembly language, which is, as you can see, much more complex, much nastier than our nice C code. But then there's also assemblers that take that and take you down to machine code. So no one wants to write in machine code. You want to work up here, and we have all of these awesome tools that get us down to there without us having to worry about what's going on. To worry about what's going on. Now, the thing with quantum computers is, well, we still need to know a lot about what's going on under the hood. So, in an ideal world, we would have the same picture. You have a quantum algorithm that you want to implement, you use some quantum software tools to implement that algorithm, push a button, and it runs on the QPU. So, my job as somebody who works on quantum compilers is to make the future look like this. But this talk is going to actually give you a taste at a very high level of the whole process that happens in between your algorithm and the device. Now, of course, I just said my job is to make it look like this. I'm going to show you what's in between. Why bother giving this talk if I want to eventually abstract all this away? So it's still really critical with the quantum computers we have today to have an understanding of this process. So there's lots of reasons, one of which is So there's lots of reasons, one of which is that the processors that we have today have a lot of constraints, and those constraints are going to affect, for example, what hardware you choose and how you write your algorithm. So we have these noisy devices, some of them have constraints on the topology of the qubits, how they're arranged, some of them use different gate sets. So understanding how your algorithm is actually going to be translated to fit that device is going to be important for choosing your device. Other reason is that sometimes the hardware backends apply custom optimizations that might. Apply custom optimizations that might interfere with your workflow. Over beers later, I can tell you about my experience running on quantum hardware and trying to do error mitigation. It was not fun. The compiler thought that it was being very helpful. It's optimizing away all of the gates that I had added to do gate folding and zero noise extrapolation. It took a long time to figure out that's why my stuff wasn't working. So different algorithms also require different compilation and optimization strategies. So whether you're doing Strategies. So, whether you're doing something with a lot of sort of oracle phase, with a lot of reversible functions, you're going to be using different strategies than if you're doing quantum chemistry, where you have a lot of e to the minus some Hamiltonian terms that we've seen today. And there exists specialized optimization techniques that you would use for each one. Other really important reason to understand what's happening is that some hardware providers who shall remain nameless charge per gate. Some hardware providers charge per gate. Some hardware providers charge for gate and have proprietary compiler backends. So you really need to trust that your company's compiler is actually giving you the most optimal circuit and that's what it's running and that's what it's charging you for. So if you don't really know what's happening and you can't essentially do the balancing the equivalent of a checkbook to figure out like what actually ran and what you were charged for, you might be getting ripped off by the hardware provider. And finally, I just also And finally, I just also want to raise awareness about the existence of automated tools. I gave a talk recently where I talked about quantum software more generally, and I showed some tools for circuit optimization. Somebody came up to me after the talk and was like, I had no idea these tools existed. I've been optimizing my circuits by hand this whole time, which is a fun thing to do, but you shouldn't have to want to. Okay, so hopefully motivated enough to convince you that you should understand. Convince you that you should understand what happens in between. So there's a lot of things. So we write our quantum software, that gets turned into a quantum circuit. But then that circuit isn't even the end of the day. We go to lower and lower levels. We turn things into elementary gates. We try and fit that on the hardware. We have other levels that lower down to the pulse level. And I will kind of give the caveat that this is roughly where I'm. Caveat that this is roughly where I stop, so I'm not going to talk much about the pulse level. But then I've also added at the bottom here post-processing and error mitigation because some of what you do up here can affect post-processing as well. And so in an ideal world, though, we only have to worry about this and automated tools take care of the rest. Now, those of you who are aware of classical compilers may or may not have read this lovely purple dragon book. Dragon book, and if you've seen the diagram of a classical compiler stack, you'll notice there's actually a lot of similarities. So you've got all of this stuff at the top that's taking what you've written and turning it into tokens, turning it into a syntax tree, and into some intermediate code. Then you've got machine-independent optimizations on that code, machine-dependent optimizations on that code. All of those things are happening here in the quantum version as well. They just look different. Okay, so in a quantum compiler, what's essentially happening is that your circuit or your algorithm gets put through this sequence of modular passes, and the passes modify a circuit in various ways and change what it looks like, but hopefully not what it does. And these are going to have varying degrees of complexity. So we can loosely divide this like a classical compiler into three stages. We've got this decomposition and synthesis. We've got this decomposition and synthesis, turning something into gates. Then we have our hardware-independent optimization and hardware-dependent optimization. So each of these levels has their own areas of research, essentially. So there's a lot of stuff going on, so I'm just going to try and give a taste of what's happening in each one. Alright, so the very first thing that happens, though, is decomposition and synthesis. So, really awesome. The really awesome results in quantum computing is that you really only need three gates, anything. And that's pretty amazing. So if you have just Rx, Rz, C naught, Ht and C naught, you can implement any quantum computation on any number of qubits up to arbitrary precision, approximated, and some of them you can even do exactly. Now, there's many different choices of gate sets. Sometimes that depends on the Gate sets, sometimes that depends on the hardware that you're using. But we have these gates, but actually finding the decomposition in those gates is not necessarily easy. So for some operations, though, decompositions are well known. There's a paper from 1995 that actually shows quite a few of the decompositions that are still in use today. Probably those of you who have looked at circuit decomposition at all know the one. I see some nodding. So I like to show this picture. So, I like to show this picture when I teach about resource estimation and quantum computing because it really highlights how this decomposition process works. So, if you have an algorithm such as Grover implemented at a high level, we can write at this level of abstraction where we just have some oracle that we'll call part of the Grover iterate. But when you actually feed that into the compiler, it's going to do this whole bunch of steps. It's going to decompose your operations all the way down into one and two communicates. Communicates. As you could imagine, that's a very tedious process to do by hand. Thankfully, it's automated. But it also kind of underscores how many resources are actually required. This looks like a much simpler algorithm than what it would look like if you decomposed everything into that three-qubit toppling operation. So, anyways, when decompositions are known, this process is fairly straightforward. You're essentially just applying an identity and Essentially, just applying an identity and unrolling. Now, when decompositions are not known, you have some arbitrary unitary, that's when things get hard. And I mean like actually computationally hard, not just challenging.