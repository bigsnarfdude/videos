Color to be C if there is something that's called L diluted binary subtree T prime up to T with the root at zero and where all the leaves have color C. What is L diluted tree? So this is a binary tree. Two diluted tree means that I'm allowed to skip a level. So you know maybe I should okay let's see if I can use a different color. So I'll draw a big bound binary. So I'll draw a big binary tree here. Where is my binary tree? Okay, so what is a two-diluted tree? Let me try to paint it with a different color. So a two-diluted tree, maybe it's this node and this node. So it means that I'm allowed to skip a level. And then maybe from this node, I go to this node. And from this node, I go from this node. And from this node, I go from this node, and from this node, I go to this node, and from this node, I go to this node. So it's not a binary tree. I'm allowed to skip every two levels. And what I'm saying is, my procedure is that if this color and this color and this color and this color in my list are all going to be red, I'm going to declare the color of the row to be red. Okay? And I can do it for more than two. So this was two diluted here. L was equal to two. I can do it for three levels, four levels, and so on and so forth. Okay? So I'm going to fix the parameters of that. So, I'm going to fix the parameters of that later. But the basic procedure is going to be, it's again going to be something based on location. But instead of just having two guys, it's going to be more fractal. I'm going to look for a fractal-like structure where all the colors in this fractal are the same color. And if all the colors in this fractal are the same color, I'm going to recover the wood to be that color. Okay, so that's going to be my inference procedure. Okay, so what are the exercises here? Okay, so what are the exercises here? I mean, so now, you know, these are really just exercises in branching processes, so I won't do them. So the two exercises are the following. So there exists, if again, so let's just remember the order of parameter. What I started is what I started is I fixed d and theta such that d theta are greater than one. I didn't say anything about. Greater than one, I didn't say anything about Q yet. So the first exercise is that once I did it, when I fixed and detected such that detector greater than one, there exists an L and an epsilon greater than zero, such that if the color of the root is C, the probability that such a tree exists is at least an epsilon. Okay, so you will see structures like that. And maybe the fact is that L and epsilon do not depend on Q, you know, it's a uniform band, you know, something that sorts for L, you know, you will see the structure. You know, you will see the structure. So, you know, you can do this inference once in a while. Now, this inference, unlike the previous case, it's not necessarily the case that if I see such a fractal, it means that the root is that color. So, I somehow have to make sure that when I infer, I don't make mistakes. And for this, we need the second claim. And the second claim will use the fact that Q is large. So, the claim is that for all epsilon greater than zero and L, if Q is sufficiently large, so Q is sufficient. Q is sufficiently large, so Q is a function of epsilon and L, and if the root is colored by a color different than the C, then the probability that there is an L diluted 2 to the L minus 1 tree with all the leaves of color not equal to C is at least 1 minus epsilon vertical. So this is a much stronger claim. What do I want here? I want here that when I draw my picture, so I draw it in blocks. This is the first A levels. This was not color C. I'm allowing just one. C, I'm allowing just one guy maybe to have the color C. Everything else has to be color not C. And then recursively, I have another L levels. So below this guy that is not colored in C, I allow maybe either none of them are colored C or at most one guy that is colored C. And if the number of colors is large enough, again, it's a branching process argument. You can show that, you know, such a big tree exists when all the colors are not. Now, why are these two claims useful? I mean, I won't show you. Useful. I mean, I won't show you in detail, but it's an easy combinatorial exercise to show that if this tree exists, this tree doesn't exist, and a vice versa, right? So, if you have this fractal, you can of color C, you cannot have this fractal where everything is color different. C. And so, you know, one exclude the other. So, this would say that you would not make mistakes too often. If the color is not C, you will most likely not say C. Okay, so again, the interpretation of exercise one and two. Exercise one says if the color is C, you are going to say C. If the color is C, you are going to say C at least with probability epsilon. And the second exercise says if the color is not C, the probability that you will say C is at most epsilon over T. So for the exercise two, are you looking for a tree where all the leaves have the same colour that is not equal to C? No, the only requirement is that the colour is not C. Now I'm just thinking in a very simple way. Just now, I'm just thinking in a very simple, simple-minded way. It's either C or not C, and I want essentially everything to be not see in this fractal sense. Right, and Ratul was asking to verify that the leaves of the diluted tree are all on the same level. They're all the same level. They're all at level H, which I assume is a multiple of L. And in exercise one, you're not specifying the level? It's again, it's an exercise one. It's L H is again a multiple. It's h is again a multiple of l if you want, but it's true for every h. So again, the epsilon is not dependent on h. Neither on q or on h, right? So it's a property of branching process. So I don't know if you've seen this property before, but for example, if p is large enough, and I just look at the, say, four regular tree, if p is large enough, then we spoke, if p is large enough, I can get a three regular tree where all the branching happens, where there's always branching. Happens, where there's always branching. Okay, so this is a similar phenomenon. So the critical value just for percolation or for branching processes is one quarter. But if my requirement is much stronger, it's to find the three regular tree inside the four regular tree where everything is open, then for large enough p, this will happen. And you said you write similar recursions to the recursion that you usually write in branching process for this condition, and you check, you get some fixed-point equation, and you check there's a root that's less than one. That's how, I mean, it's. I mean, it's something classical in the theory of budgeting processes. Maybe not all of you have seen it, but it's classical and it follows the same proof that people usually do in classical batching processes. Okay, any other questions about this? Okay, good. So there's another exercise. I mean, this is the easiest exercise in this percolation picture. Is that if d theta, I don't know why did I write lambda? I don't know why did I write lambda if dÎ¸ is less or equal than one, then the root and the leaves are asymptotically independent for every q, and that's sort of clear, right? Because again, from this branching process picture, if d theta is two theta in our case is less than one or less or equal than one, then we know that this branching process will die. So we will not reach the leaves, and therefore what we see at the leaves and the root is going to be independent. Okay, so that's all I wanted to say about this simple reasoning. Wanted to say about this simple reasoning. Of course, if you're smarter, you do better. So, Alan Sly and this found a different way to think about the correlation between different parts of the tree by doing a more careful expansion of the magnetization. Okay, we have to say what is magnetization, but the magnetization is sort of the expected value of the root given the leaves, right? So, Mn is in some sense. Mn is in some sense a vector version of the expected value of x not given xh. And what he showed is they showed that you know the if you do the expansion as a function of both d and q, you get the main term, which is the Kirsten's Tigrom term, d theta square. Then you get a second order term, where you see you get this mn squared. And the key thing about it is this comes with a positive or a negative sign, depending if q is bigger than 4 or less than 4. Than four or less than four. So if q is bigger than four, this term is positive, and if q is less than four, then this term is negative. And there's a bunch of things once you do that, and you know, it's not completely trivial, but Alan, so again, this is positive if q is bigger than four. And but once you do that, I mean, what Alan succeeded to prove is the following. First of all, you can prove that if q is greater or equal than five, then the kisten single bound is not tight. Kesten-Stiggle bound is not tight. And the intuition is not too hard. You know, the magnetization will go down and down and down and down, but then at some point it will start going up when you're very close to the custom stigma bound. So that's not too hard to derive once you have this recursion. And also, interestingly, show that if Q is equal to 3 and D is greater than some thousand or a million, then the Kestman-Steven Brown is tight. Steven Baum is tight. Okay, so this requires more work because you have to show that this magnetization quantity is low enough. And I think Alan also says result for Q equals 4. Well, you use the next order term, but I'm not sure these are published. So let me not tell you what they are. Okay, so then it depends if it's ferromagnetic or anti-ferromagnetic. I mean, it's even more like. Okay, and for the few of you who went to the exercise section yesterday, I mean, one of the things that I don't know if Federic had a chance to get to or not. So for general Malkov chains, if you look at not just this coupling process, you can actually have the second eigenvalue be zero, yet the root and the leaves are not independent, right? So you can get, you know, there's sort of no relationship between theta, which is the second eigenvalue. Which is the second eigenvalue of the matrix, and the question if the roots and the leaves are independent, right? So, for country construction, lambda two equals zero means that no matter how wide is the tree, if you just look at the census, then it's independent of the root. But if you are allowed to do whatever you want, you're allowed to apply belief propagation, then there are even examples where the second eigenvalue is zero. So you really forget information very fast, and the root and beliefs are nothing. Okay, and there's an exercise here in the notes that, you know. Here in the notes that does that, but there are other examples even more sophisticated. Okay, so let's see how much time we have. So maybe I'll give the conjecture and then we take it two minutes break. So for those of you who stayed, maybe you're interested enough to actually think about research conjectures in this area. So these are two conjectures that I think I'm formally making. That I think I'm formally making for the first time ever. And you know, so you may be, in particular, it may be pretty easy to refute them. I think that proving them is going to be difficult. I'm going to tell you two conjectures about sort of the fragility, another interpretation of the fragility of the Keston-Stingen bone in the setup. And you are welcome to think about it. It's definitely something that I'm thinking about in some way or another for many years. Okay, so we are going to consider a model where not all the thetas are the same, right? So there's going to be a tree, maybe the tree is going to be binary, but each edge is going to have its own theta. Okay, so that's the model that we're going to consider. And we are going to consider a large Q, it's going to be large enough in particular. There's an interval between what I call theta r, it can be any number that you want, and the crestency. That you want and the Kestenstigum bound, where the root and the leaves are not independent. So the variance of the expected conditional expectation of x not given xh does not go to zero. We know it actually has to go to a limit in this case. So we're looking at the interval where there's some action. So this is in the case where all, so this is in this in this case, theta e is equal to theta for only. Okay, so you know, we are looking at the case where you can do the covered with better than random, but you cannot do. They cover the root better than random, but you cannot do it in this robust ways. So here are two conjectures. The first conjecture is that there is no estimator f such that f of x of h and x0 have non-zero or non-eglogible correlation for all the models where different edges, all of them are in this interval theta r to theta ks, but they are not the same for different edges. So you assume that theta E is known, so the estimator can depend on the no, I exactly. I assume that the estimator does not allow to know what theta is, right? So again, if there's actually two, yeah, excellent question. So there's actually two questions here. I actually don't even know the answer for sure if you know the theta is. This has to do with some monotonicities of this model. But I think this model should be monotone, and that might not be too hard. monotone and that not might not be too hard so maybe a question a preliminary question that you want to show is that if if theta e is bigger than i don't know theta r plus epsilon for all edges then x naught x h and x naught are not are are correlated are significantly correlated if we can I don't know if we can see or asymptotically correlated. Okay, so that's that's definitely you know one thing that you would want to check as a prerequisite for conjecture one. But conjecture one talks about the situation where I don't know what r the theta is. I'm just promised that they're all in an interval. You can choose theta r to be any number that you want to be below theta ks. I'm just telling you that they're in interval between theta r, this theta. interval between theta r, this theta r and theta ks, and you don't know what they are. And I want a universal estimator that does not depend on the values of theta that will give you correlation. Does that make sense, Omar? Yes, and is there any reason to require that the thetas are a lot larger than theta ks? Right, so there is a reason, very good. So Omar is already thinking about the parallel. So what we know, and I mean, so unfortunately, I think I only wrote the proof for q equal to. Wrote the proof for q equal to, we know that at least for q equal to, if theta e is bigger than theta k s plus a little bit plus any epsilon that you want, such f exists. No, I meant for the conjecture: if some of them are smaller than that, then theta ks and some are larger. So, yeah, but I mean it doesn't. It doesn't really yeah, you can you can you can you can you can allow yourself to do that too. I mean it it's the it would it would still make the conjecture interesting. I agree. Yeah, yeah. But I sorry I didn't answer your question, but I do I do want to note that for the case that Q is equal to 2, if all the theta is are bigger than theta ks plus epsilon, then there is a such an f. And it's not completely a trivial f, but such an f exists, right? So there is a function that doesn't need to know the theta is. Function that doesn't need to know the data is and recovers the root in a way that's correlated with the list. Can you not see what I write, Telia? No, I can see it, but I'm wondering when you've changed the, when theta is allowed to be variable, do you always know what the theta r is? No, so this is the question that I let me repeat. So, I mean, let me just try to maybe motivate it a little bit, right? So, in application like phylogenetic reconstruction, it's not reasonable. Reconstruction: It's not reasonable to assume that you know the thetas. But it is reasonable to assume that the theta lie lie in this relatively in an interval where say they're close to one or much bigger than the Kestenstigman because you know how much time evolved between species. So making an assumption like theta Es are all much bigger than theta ks is a reasonable assumption. But making the assumption what that you know what they are, that's a more tricky assumption, right? So in some situation, and you know, this would lead to the second conjecture, in some situations. And you know, this would lead to the second conjecture. In some situations, you want to make this conjecture that theta E is and then you want to a construction that does not depend on knowing that theta E. And in the case that Q is equal to 2, such a procedure exists. What I'm claiming is that for larger Q, it should probably also work. I never checked it. If all the theta are bigger than theta Ks plus epsilon, I mean, in the even Q case, it follows from this general philosophy that whenever you know for Q equal to, you know for even Q. Two, you know, for even Q. So, you know, maybe there's a little bit of working or checking it for odd Q. But I think the fragility comes where you're below the Kestenstigman bound. So your thetas are such that if you are given the thetas, you can apply belief propagation that takes into account in a very crucial way what's the actual value in theta E is in trying to recover the posterior, and you'll do well. But if you don't know the theta E, there isn't like an algorithm that ignores the detail of the process and recovers the posterior. But do we know for sure that theta r is not zero? We know if q is you mean you mean that the interval is non-empty? Yeah. Right, so that's exactly the point of the cast and stingon boundary or the result that we've seen is we've seen that for q equal to q greater or equal than five, this interval is not empty, right? Is not empty, right? Because the Kestenstig compound is lower, is higher than the threshold for actual reconstruction or for actual recovery of the wood. So this interval non-empty when Q is greater or equal than 5. Okay, so this conjecture only makes sense for large Q, definitely not for Q equal to, but definitely for Q greater or equal than 5. Okay, this is a non-empty interval in this case. Index in this case, okay. That's conjecture number one. Maybe I'm happy to talk about this later. And maybe at a high level, conjecture number two, it says it is impossible to recover phylogenetic trees using order of eight samples under these conditions above. And the conditions above means that you don't know what the thetas are. So again, conjecture, you know, the converse of conjecture one in the case q equal two, you know, does not hold. And this helps us in recovering phylogenetic trends. In recovering phylogenetic trees. So I conjecture not only conjecture one, but also that the corollary of conjecture one, you know, the reconstruction of phylogenetic phylogenetic tree breaks down because of that. Okay, I'll just say because I want to take the next source. Strong version of impossible would mean there's just no information theoretical way, and weak version would mean that it's computationally much harder. Computationally much harder. And then the fact that it's computationally much harder, we can never prove. You know, you just say, you know, the only way I know how to do it is via some exponential time algorithm, and it feels like I cannot do it. But maybe even the strong version is correct. I didn't really make progress on this problem. I thought about it on and off for the last few months. I didn't really make progress on this problem. Right now, I have no intuition for if the stronger version or the weaker version, or maybe we can just disprove my conjecture, whatever you do, you'll be. Prove my conjecture, whatever you do, you'll make me happy. Okay, so let's take a two-minute break now, in which you can ask questions, and then I finally talk about some elements of simplicity and complexity of belief propagation. And I'm happy to take questions, as if you are. Yes, and I think participants should also be able to unmute if they want to ask questions or ask on the chat. Or ask on the change. And I guess it's could conceivably there could be some perturbative argument if the thetas are all in some very small interval around some. interval around some theta yeah so my conjecture is pretty strong i didn't i didn't necessarily specify that theta is the first value where you can do something i think that you know my conjecture is that actually for every epsilon that you can think about if you look at the interval between theta ks and theta ks minus epsilon you are doomed so this is a pretty strong conjecture so you might be able to refute it okay and subverta is asking if there is a robust analysis Is asking if there is a robust analog for the block model recovery problem. That actually sounds like a very good open question, too. So, right, but let's just recall that. So, let's actually think about it. Is Robata good question? So, as usual, so let's just think about it aloud for a second. So, for the block model, it's already conjectured when Q is large. When q is large, there's a computational statistical gap. But now Subhavata is asking maybe a different question. Suppose we are looking at the block model where the parameters, thetas, are different for different edges, and you do not know them. You just know that they are in some interval, in this interval, if you want. Do we know that this is information theoretically impossible? So, this is a great conjecture. So, let me call it S-conjecture. Similar information theoretical phenomena in block models. So I think this is a great question. I don't know that anybody looked at this question. I think this is a very pretty natural question to look at. It's a very nice question. Okay, good. So let's move on to the last part of the question, part of the talk. So obviously I have 20 minutes, so I won't prove anything. Maybe I'll state a couple of things. Maybe not. We'll see how we do. Maybe I'll just tell you stupid jokes. I didn't tell enough stupid jokes in the lecture series. So I'm going to talk a little bit about the complexity of MIP. So, you know, what the question is, what is the complexity of MPP? What the question is, what is the complexity of BP? We've seen a lot of BP in this lecture. Is there a way to formally measure what is the complexity? So, in some sense, it's slow, right? We've seen this recursion, it's running the volume of the tree in linear time, right? You start from the leaves, you know, you compute a real number, then you compute another real number, then you compute another real number, so maybe it's very low. But I mean, there's some notions of complexity, one is that it uses. Complexity: one is that it uses real numbers, right? The process that we talked about, say, with q equals 2, is discrete, right? It's just bits. Why is it that when I do the inference, I have to use real numbers? Is it really necessary? And the other question is that it uses depth. It's this recursive procedure. So it's different than just summing the guy, right? You do something iterative. Is it in some sense necessary? And the fractal picture that we've seen just in the proofs. Just in the proofs for the behavior above the casting stick of bound suggests that maybe depth is needed, right? So, okay, so here's my stupid joke. Okay, I'll do one stupid joke. Okay, what is everywhere and understands everything? Okay, so what is everywhere and understand everything? You know, if you Google something like that up, you'll get omnipresence. So, this is not going to turn into a course in theology. So, my answer for what is everywhere and understand everything. For what is everywhere and understand everything is the deep net on your smartphone that understands what you say, right? So, each of us has a smartphone or Alexa or some other software. You talk to this thing and miraculously it understands what you said. And I don't really want to claim that anything we are doing is too relevant to deep nets, but these are some hierarchical processes that run very fast because they understand what you sell, but they're based on multi-layers, right? So, I think. Right, so I think for me, one of the questions that came from the lack of understanding of deepness is the following question. So, you know, one of the reasons I ask these questions about B is the following question. Mathematically, it is natural to ask, given, you know, what we see in BMAX, if we have a process that satisfies three natural properties, one is it's a realistic model of data, so you have a model that generates a realistic model of data. So, you have a model that generates a realistic model of data to reconstruction. You have algorithms that reverse engineer the generative process. So, you have algorithms that actually can, these are maybe very heavy data and very, very heavy computational processes that can put this deep net on your phone. You know, so this is what Google does. It takes them a lot of time and effort. And three depths, you know, what you actually have in your phone requires depth. Okay, so you know, if we take a very, very Okay, so you know, if we take a very, very abstract point of all of these deep net things, you know, you know, as people are doing probability, what do we want? We want a probability model, a probability model that generates data that looks somewhat reasonable. It's a model for which you can come up with the deep nets that will do the inference. So you can recover the net that you're looking at. And finally, it actually requires depth in some form and say. Okay, so what I claim is that we already Okay, so what I claim is that we already looked at this model. So, this broadcast process model or this three Markov chain is realistic, right? People are using it in phylogenetic, in information theory, in a bunch of areas. We saw that you can reconstruct it. This is phylogenetic reconstruction. I'm given a lot of examples. A lot of examples I can build the tree that generated them. And then, maybe the missing piece in this idea of trying to find this trinity is, you know, do you really need the depth? Okay, so that's somehow an abstract point of. Okay, so that's somehow an abstract point of why you are really interested in this question of that. And a related question: you know, why do we really use real numbers? Right? Deep net MBP use real numbers. Why do you need to use real numbers when everything that you're talking about is discrete? Okay, so these are sort of high-level motivations for the question, for the result that I'm briefly going to talk about now. Okay, so I'll actually start from this problem. Why do you have to use real numbers? Do you have to use real numbers? And one way to think about this is that what are the memory requirements for belief propagation? Belief propagation, you propagate these real numbers going up. And already in this paper of Evans, Kenyans, Perseus, and Schumann from 20 years ago, they stated that even for Q equal to n recursive algorithm on the tree, you know, that sends messages back on the tree that uses at most some That uses at most some constant number of b bits of memory per node, can only distinguish the root value better than random for theta that is less than theta b, but this theta b is not the constant stigma bound, it's a bigger number. So if you want to do what belief propagation does, this simple non-linear recursion with real numbers that we've seen, where you just multiply and add and divide, and you want to do it with bounded amount of memory, you know, you can truncate, but you can think about other ways of doing it with bounded amount of memory. Doing it with a bounded amount of memory, then you will not get all the way to this session, you will miss something. So, let me show you the picture. So, we proved that in joint work with Vishesh Jain, Federicolla, who is the TA for Discourse, Jingbo Liu was a postdoc here. We proved that this is correct. And I'll give you the picture from slides that Jingbo made. These are very nice pictures. So, let me try to show them. So, what happened? Let me try to show. So, what happens when we do believe propagation? So, this is not completely consistent with our notation, but there's the root x1. This is a bit, you know, there's a noisy channel that gets to x2, x2, x3, x4, x5, x6, and 7. Then there are the y's. This is what belief propagation calculates. So, these are some surreal numbers. Then we combine these real numbers via some non-linear recursion. And then we get an estimate which is an estimate which gives us some two probabilities. So that it goes down the tree and then from the tree. Right, so that it goes down the tree, and then from the tree we go up the tree. So, the model that we're looking at right now, the broadcast model is exactly the broadcast model that we looked at so far. But in the reconstruction, we are only allowing functions where each of y's contains a log L bits of memory, so it can take at most L values. Okay, we are not allowing it to take infinite precision real number, it can take only at most L possible values, or you can. Most L passable values, or you can describe it with log L bit string. And what the theorem that we proved said, or what the original conjecture was, of if KPS says that you really need the infinite precisions to get all the way to the Kestin stigma. Okay, so any other procedure that you have, said, oh, I'm going to look five level downs and do majority iteratively, this is going to give you something, but it's not going to go to the casting of one. Oh, I'm going to do belief propagation, but I'm going to. Do belief propagation, but I'm going to discretize it and keep just 25 digits. You know, this is not going to get you to the Kistan Sigma. Whatever you are going to do, this is, you know, it doesn't use all the infinite accuracy, it's not going to get you to the Kistan Sigma. So it shows to some extent that you really need to use real numbers. Okay, I think in the last 15 minutes, so Jingbo is something, you know, this proof is pretty beautiful. And Jingbo is. What was the estimate on? On B? Yeah, it was shifted. So theta B is bigger than theta, and it has the right scaling, which is a polynomial in B. So we have both an upper and a lower bound with different constants here, actually. So this should be theta. And one direction is obtained by discretizing belief propagation, but not in the most trivial way, right? So the constructive way is discretizing belief propagation. Ways discretizing the belief propagation, but the main work here is a lower bound. Okay, so I won't talk about the proof because I really want to tell you about a different model where you can, which is more related to this question of depth. And here the game is a little different. Now we still want to recover x0 from xh. Want to recover x naught from xh, but before we had to do it with following this very specific tree strategy. Now we are allowed to do any strategies that we want, right? So we have some deep net in our cell phone, it takes xh, it does some non-non-linear operation, then it goes another layer, another layer, another layer. We want actually, we look at different architectures, but what happens if we allow also non-tree architectures? And the question is, can we do it with slow depths? Low depths. And of course, we cannot expect to prove that we need a huge depth because BP has a depth of order log n, which is order h. Okay, so you know, we cannot expect to get a lower bound that's better than order h. And, you know, here's some parameter from deep nets. Let's not worry about it. The question is, can we prove something like that? So, everything I'm going to talk to tell you about is some results from work with Anku Moita and Colin Sam. Work with Anku Moita and Colin Sandon on this problem. The problem is that I don't know how in 15 minutes I show you three complexity classes in computer science. I'm just going to skip AC naught. And I'm going to tell you what T C naught. Okay, so T C naught is like A C naught, which I skipped. But with majority guides, so that we tell you what it means. These are logical circuits where you're allowed to do logical operations, and the logical operations that you're allowed to do. And the logical operations that you are allowed to do are and you're allowed to do or you're allowed to do not, obviously, and you're allowed to do majority with any fresh that you want and any number of inputs, right? So it's a circuit that you know you have some architecture here, all here is XH. Maybe some inputs are duplicated, then a bunch of them are going to an end, there's an overlap. To an end, there's an overlap, some other ones are going to an O, some other subset is going to a majority, then these guys are going to see you know, and it goes on for multiple layers like that. But the number of layers that I'm allowed to have is constant, it does, it does not allow to grow with age. So, this is T C naught. So, what is the result that we get for this set of architecture T C naught? So, here is a result that already shows something. Shows something, you know, yeah, it's hard to see why it's connected to the Kestensting bond, but maybe I'll try to explain. When the number of colors Q is two, and theta is large enough, it's close enough to one, then in fact, in this class T C naught, you can estimate the root as well as B P. Okay, so even though BP had H level, this class had a constant number of levels. Okay, so maybe I should write this. This equals to thus minus epsilon minus epsilon, and this epsilon depend on the depth. And the conjecture is that this is true for all theta when q is equal to 2. So the conjecture is that for when q is equal to 2, in the linear case, actually circuits of bounded depths with this majority gates can do as well as BP. We know it only when theta is very close to 1. Because when theta is very small, we also know it is theta. Very small, we also know it because BP doesn't do anything, it just returns something independent of the root. So it's very easy to return something independent from the root, right? So that's one result. And I'll just, okay, so that's one result. So maybe I'll draw just the picture in terms of what I said. So here is theta Ks. So here we know that all algorithms fail just because information theoretically you can do nothing. All fail. Nothing. All fail. So there's no complexity question here, right? You cannot recover the wood because there's not enough information in the leaves. So it's really a question about what happened between theta ks and one. And the conjecture is that between theta k and s and one, the kind of algorithm that we use in TC0 actually works in all of this regime. But I mean, right now we don't know how to prove that. Okay, and I'll mention one more class, and this class. And this class is called NC1. So, this is the class of order log n or order h in our notation, depth circuits with and/or and not gates. You can also include majority gates. It doesn't really change much for complexity reasons. And as always in complexity theory, there's something that's not known. So, this class T C zero that we've seen before is known to be contained at C one, but we don't know if they're the same or not. But we don't know if they're the same or not. So it could be that TC0 is actually equal to NC1. That's a big open problem in computational complexity. Maybe not as big as NP versus P, but still a big open problem. Okay, so one thing that's easy is that BP is in NC1. In the circuit where you use all the log and depth, it's pretty easy to do things. But more interestingly, there is a broadcast process for which classifying better than random is. Which classifying better than random is NC1 complete. Okay, so I won't describe this broadcast process to you, but it means that if you want to estimate the root better than random for the broadcast process, you know, it's a three by three Markov chain or 16 by 16 Markov chain, but I'm not going to describe for you. If you want to do better this random for this 3x3 or 16 by 16 Markov chain, then you need the full power of. You need the full power of NC1. You need the power that, you know, assuming that NC1 is not equal to TC0, it means that you need the full power of all the fraction or the same number of layers that you have up to a constant as the broadcast process. So in other words, unless TC0 is equal to NC1, all the H depths or the log n depths is needed. Okay, maybe I'll just And with this conjecture, so we are very far from proving this conjecture, but the conjecture would be that for any broadcast process below the cast and stigma bound and where BP classifies better than random, classification is NC1 complete. So you really need the full power of NC1 in order to classify better than random when you are below the cast and stigma bound. So this conjecture would say that we see the cast and stigma bound. We see the customs tiger again in terms of the computational complexity of learning the goods, not just with three architectures, but with any architectures you actually need it to be. Okay, so maybe I'll conclude here. I have more details about various books that, you know, if you'll ask me, I'll happily apply. But what's the conclusion of this maybe lecture, or this series of lectures, is that BP is simple, you know, it's run in linear time, and above the case bound, it behaves like a linear algorithm. We've seen it both in the analysis of BP for the three process, and we also mentioned, even though we didn't see the details, that Mention, even though we didn't see the details, that in some sense, that's what we behave for problems like the block model. On the other hand, BP is complex. Below the KS bound, our understanding of what BP does for trees is that it does something very factor-ish that's harder to understand. We believe that there are statistical computational gaps, both for phylogenetics and for block models, and maybe even just for the broadcast process. And there's some indication that it. And there's some indication that it requires depth and precision. So I think that's all I wanted to say in a high level. And again, I have more details about various proofs. So if you ask me questions, I'm happy to answer. So I also wanted to thank you guys, the organizers, for organizing and for everybody attending. Whenever you agree to do something like that the week before, you're like, oh, why did I agree to do that? This is so terrible. I'm so stupid. But then, you know, when you actually... But then, you know, when you actually prepare the lectures and talk about them and think about them, you know, you realize new connections and new conjectures and, you know, having intelligent people listening to your rambling and asking intelligent questions, you know, is always good. So thank you guys. Well, thank you, Elchinen. Let's unmute the participants. So we can thank Elginen for his three lectures. I can mute people, but they can unmute themselves for questions. Let's see. There's some thanks in the chat. Still waiting for questions. Okay, so maybe. Okay, so maybe to start things off. So, when you say that you can do things with this bounded depth networks, do you have some explicit construction for networks? Very good. You predicted very well some of what I do. So, here's the TC node construction. I'll tell you how it goes. Essentially, you do the following. I'm going to do this the bad thing. I'm going to do this the bad thing that I shouldn't be doing, which is I'm going to take maybe some number of sub-trees, and for each of them, I'm going to take a majority. I'm going to take a majority of this sub-tree, of this, sub-tree, of this, sub-tree, of this, sub-tree, of this sub-tree, and I'm going to take some estimate of the majority, which gives me an estimate of the roots. These estimates are not optimal. These estimates are correlated with the roots, but they are not optimal. And then I'm going to apply BP on the list for a constant number of levels. For a constant number of levels. So, even though this is big, this is actually small. So, here maybe I have five levels and I apply BP on these values. Okay, so this is very, very explicit. And for a constant number of levers, I can do whatever I want. And why does it work? It works because in the regime that we know that it works, it works because we have a result with in the context of which we developed in the context of block orders with John Neiman and Alan Sly. With John Eaman and Alan Sly, that BP with noise classifies as BP without noise if theta is close enough to one, 4Q equals to 2. So it's not just, right? So we have this notion that, you know, noise doesn't change the threshold, but you can think about the situation. I have the broadcast process, right? So I remind you this picture, I have this broadcast process. And at the end, I add a little bit of noise or a lot of noise for every individual guy. Now, suppose I apply BP to Now, suppose I apply BP to this. So I already know by some robustness results that this will be non-trivial. But in fact, it's not will only be non-trivial, it will asymptotically perform exactly as well as BP. So the probability that you will be correct will be exactly the same. Okay? So BP adds some error correction in this regime. Second? And the noise is not so large that it overwhelms this. So, this noise can be anything. This noise here can be any number that's less than one, any number that's less than half. As long as you, so let me maybe write normally. The noise goes to one. Let me write the limits. Okay, so let's, does the limit h goes to infinity? The limit of the noise, which I'm going to call to eta. That's okay, we have to decide if we call it zero or one, so let me call it zero, okay, of the probability. Of the probability that BP XH is equal to X naught is equal to the limit of H goes to infinity. Oh, sorry, YH is equal to not is equal to the limit of the probability. So X is going to be this vector and Y is going to be the noisy version. YH is going to be going to be equal to the limit as H goes to infinity. h goes to infinity probability that BP of XH is equal to X naught okay so on the right hand side we have the quantity that we like that's the quantity the chance that I'm going to be correct okay on the left hand side I'm telling you that for every value of eta no matter how large the noise is as long as it's a constant amount of man I'm going to get the same level of accuracy as h goes to infinity so the limit eta goes to zero can also So the limit eta goes to zero can also be for every eta greater than zero. Instead of this, I can just write for every it's it's corrected me, but I really wanted to write for every so this is true for every eta greater than zero. So it doesn't matter how much noise you have there when you take the limit of the depth going to infinity, you say just do once, you will do as well as maybe. Once you will do as well as to have eta going to zero inside the no in for this application we don't we know we don't need the eta to go inside again but if you want the in the application the statement is the following if I want to do as well as beep if I want to do To do as well as BP, if I want to do epsilon, if I want to do 0.01 as well as BP, then there's going to be a bounded depth circuit, but the depth will depend on 0.01. Okay, and yeah, so this, so it seems it's hopeless that this would work for large q, I guess. Yeah, yeah, no, this, yeah, something like that does not go. We know that it doesn't work for large Q, right? So this is just for. That it doesn't work for large q, right? So, this is just for q equals to, right? For large q, we know that you know, once you add noise, the y bp of y and x are independent, right? So, this when you're above the custom statement or below the away from the custom statement. Yes. Okay, so Linden was asking if you could go over the AC0 sure. So, IC0 is the start with the definition quickly. So, IC0 is the weakest class. So, IC0 is the weakest class. IC0 is the class of bounded depth circuits with just n and all, no majorities. It's exactly what we had before, but no majority. And the theorem here is that AC naught, if you are given any algorithm of AC naught, and you apply it to this vector xh, you are going to classify x naught just as well as a random bit. So, if I just apply a constant number of levels of end and all, from this, I'm trying to estimate. From this, I'm trying to estimate what the root is. Then it's going, I cannot do better than random. I mean, this is not trivial to prove. I mean, I think, yeah, to act.