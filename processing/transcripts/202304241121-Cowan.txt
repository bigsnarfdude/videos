From Tom Laurie saying yeah, Bob got profile versus marginalisation largely right in the graphics of course largely surely I don't know um okay is it done doing what it's supposed to be done just talking amongst yourselves Talk amongst yourselves. Just an I'm not just split between the buttons. I've got the wrong thing here. Uh yeah, that looks good. Is there a way of dragging it to another screen? Is there an Apple gene removers that? I mean, it has to do with the fact that the output is coming to the monitor, but not to the data projector. Yeah. And it's And it's a new system, so I'm not used to where the thing is. Sorry about that. I can't recall either. Well, I want to see the just so to see why it's going. So what's going to the screen is you also go to the data presentation. Oh, you want to have it like a copied. Here it is. Here, is that the word mirror isn't? But the word mirror is not in feet. No, it doesn't. We're not already recording, are we? Yeah, we are. It's a total message. We'll edit this out working on it. But my incompetence is now. Okay, well, let's go back to the original talk. Sorry, I have a slightly updated. Original tool. So I had a slightly updated, better version of the tool, but it looks like we had to use the original one. Okay, and I don't know how to escape having this. There we go. So now we're back to close half of our and well, I hope this is because it's bad for sure. It's an The shortest number is. Oh, that's what it on the desktop earlier. What is this? If you scoop that window out of the way, it's on the right-hand side. There we go. There we go. Okay. Apologies for that slight delay. Are we now? Now, the embarrassment is going to be there for all to see in the future. Thank you very much for the invitation to the meeting. I'm delighted to be here again. And the weather, it looks like the weather is picking up. Damn. I was hoping it was going to be a nice wet day. So I'm going to give a statistician's view of this problem. That's me. The statistician, of course, is me. Other statisticians in the room will doubtless have different views, and I'm expecting a certain amount of abuse from any. Amount of abuse from any Bayesians who might be here. So, does this go to work? You might have to the arrow key though. Not advancing. It's not. You can just do it on the score with the arrow keys. Okay, so what do we want to do? We want to aim for frequentist inference in this case. Is it large enough? It seems a bit small to me. Is it okay at the back? David, I can read it. At the back. David, you can read it. Okay. So, what we want, we want frequentist inferences, but we want the frequentist inferences to be relevant to the data set we've actually got. In the sense that we want to compare the data we actually observed to some reference, so Quentis inference involves reference sets, and those reference sets we want them to be in some sense like the data we actually observed. We want them to be calibrated. We want them to be calibrated in the sense that the probability statements we subsequently then make with respect to this reference state are accurate. In the sense of R. A. Fisher, who said these wonderful words, we may at once admit that any inference from the particular to the general must be attended with some degree of uncertainty. But this is not the same as to admit that such inference cannot be absolutely rigorous, for the nature and degree of the uncertainty may itself be capable of rigorous. Certainty may itself be capable of rigorous expression. So basically, what he's saying is: you might be unsure, but you'd like to know exactly how unsure you are. And the final comment I want to say is we also want our inference to be secure in the sense that the conclusion is not going to depend too much on secondary details of how we formulate the problem. It's not too strongly firm uh messed up by bad data and so on. Sorry, Anthony. Why the word efficient is not there? Word efficient is not there? Well, that would come out automatically, but we would also, of course, like it to be efficient. That comes in some sense down to the reference set. Because we want the reference set to be the correct reference set, in which case the thing would be efficient. Okay, so this goes in the wrong direction. Here's a trivial example of the relevance. And this is just a textbook example to make the point. We've got some observations from uniform distribution, unknown. Distribution, unknown center at theta, and width half on either side of the unknown centre. The minimal sufficient statistic, which contains all the information about theta in the data, is just the maximum and the minimum, and we can convert those without any loss of information to being the range, the difference between the two, and the average of the two. The range itself is just some number that tells us nothing. Number that tells us nothing itself about theta, which the information about theta is contained in the average. And so, what we're going to want to do is to re what we can do is to rewrite the joint density of the data here in terms of the joint density given the minimal sufficient statistic, which we write like that. There's no theta in the first bit. Then we rewrite this second part: the joint density of the range and the mean, the average. A range and the mean the average in these terms here, and then we have this is the density of the average, sorry, the range, and this is the density of the average given the edge. And it turns out that we can easily work this out in this example, of course, that the density of the average given the range is this thing, and what this does essentially is tells us that the precision of the in subsequent inference is going to depend upon this A. To depend upon this A, the range, even though we know that the A in itself conveys nothing about the precision. And just to give you the picture, here, this is the idea. We observe, in the case with n equals 2, we observe these two points here. This is what the density, the potential support of A and T, in this case, the range and so on. This is the value of the average. Of the average, this value here, and what we observe is that the value of A is this, it's got this value up here, point is 0.7. Okay, so what we want to do is to compare this set of data with other sets of data that are going to have the same value of A. If we do that, the confidence interval we get is this, and it's perfectly calibrated. We could also ignore A and just compute things based on T. And just compute things based on t, and then we would have this interval here, which is wider and is in some sense, I mean here, irrelevant because it doesn't take into account the precision indicator. Okay, and actually, this is not just irrelevant, it's actually logically impossible because this interval contains values of the parameter that would be impossible if you actually observed this particular data set. So the point is here. So, the point is here, as I say, this is a textbook example, just as intended to underline this issue of lightness. On calibration, what we do here, how we get the confidence intervals, is to base our inference, tests, confidence sets, whatever, on what statisticians call, or some statisticians at least call the significance probability, or the p-value function, which is this function here. We look at the probability that the average will be less than or. That the average will be less than or equal to its observed value, given that a equals its observed value, and we look at this as a function of theta. And as we vary theta, this probability here will change. And we choose, if we want, for example, a 95% confidence limit, limits, we will choose the values of theta such that this probability here is equal to 0.025 and 0.975. Okay, so we're basically going to solve these equations. Okay, so we're basically going to solve these equations here for theta, and that gives me two limits. And in the particular example, that just gives some details here, but they don't matter. Okay, the point is, this gives us a unified, simple way to compute confidence intervals in general settings. And perform tests as well, of course. Now, what do I mean by secure? Well, any statistical model, any statistical setup has got primary assumptions and secondary assumptions. Primary assumptions and secondary assumptions. Primary aspects essentially concern the question of interest. Usually that involves a basic model structure. Maybe we assume a Poisson structure. And what we, of course, will do is to summarize the key issues in an interest parameter, which I've labeled as psi here. And then apart from the primary aspects, there'll be secondary aspects, which will often, in some cases, will involve the error structure. In some cases, it will involve the error structure, might involve the nuisance parameters. They might be things like: Do I believe my observations are independent or do I think they're correlated? If they're correlated, I need to pass a joint distribution. But that doesn't change the question of interest, it's just what the answer to that question will be. How will I formulate the answer to the main question? That's one aspect of security. A second Of security. A second aspect is which is kind of a bit more relevant here, maybe, is that we want the inferences not to be too heavily dependent on the nuisance parameterization. And that can be useful for numerical work, but it's also kind of if you mess things up a little bit, you change the parameterization, you're not really changing the primary question. You don't want the answer to your primary question to change. So, what we aim for is invariants. Aim for is invariants to what are called interest-preserving reparametrizations, where we transform psi to some smooth function of psi, called it eta here, and we transform the nuisance parameter to some parameter that I've called zeta, which could also involve psi. And not all methods will achieve in this particular sort of invariance. Of course, we also hope for robustness to bad data, and so on, and so forth. Okay. So now let me talk a little bit about complications in this ideal scenario I've just set up. Of course, nuisance parameters. Nuisance parameters and discreteness of the data will degrade calibration. First, let's talk about nuisance parameters. Ideally, what we would like to have is this factorization here of our original data, density for our original data set. We've got this involves Data set. We've got this involves the interest parameter and the nuisance parameter. We'd like to split that up like this. This is the parameterization, the decomposition we saw before. And then ideally, we would have the psi in this bit of the density, the lambda totally separately, and therefore we would be able to focus only on this bit here of the likelihood. And then we would, if we have a scalar. If we have a scalar psi, we would then just be able to do enunciate the recipe I used in the trivial example I gave, and that would give us our ideal inferences. It just becomes a numerical problem. How do I compute this thing for different values up psi? The issue, of course, is that we don't usually get this happy situation up here. Usually, we have factorizations where this first term splits up. This first term splits up in one of these two different ways, or not at all. If we have this situation here, then there is some psi in the distribution of A. It's just difficult to extract because it's messed up by the presence of the nuisance parameters. So we could still use this, we could still apply the recipe here, but there may be some loss of information. Usually not a lot, actually. And this we call a conditional likelihood. Okay, and this we call a conditional likelihood. We look at this first term here. For a marginal likelihood, we hope that there's some other marginal factorization where the t and the psi appear together, and then in this other conditional bit of the density, there's the lambda as well. And then we base inferences on this term here, and that we call a marginal likelihood. And depending on the type of model you're dealing with, you may. Type of model you're dealing with, you may end up be led to a conditional or to a marginal likelihood. Of course, in the worst case, there's going to be no sufficient reduction. The data themselves are essentially the minimal sufficient statistic. And then we're going to have to base inferences on some more general thing, where Z, this thing that features in this last expression here, is some approximate pivot that's going to involve some. That's going to involve psi, that might be the maximum likelihood estimate, it could be the likelihood ratio statistic, or one of many other things. And then we're in a bit of a fix because this distribution involves the lambda here. So unlike this expression here, the lambda appears, and we're going to have to replace it somehow, either by integrating it out or by replacing it with an estimate or something. Or something. So that's the kind of situation from a statistical point of view. There is a setting in which things turn out very nicely, and that is the case of what we statisticians call exponential families. And I'll put the density up here. This is a linear exponential family in what's we call its natural parameterisation. We've got the t here associated with psi. Associated with the psi. We've got a here associated with the lambda. And then there's something we call the cumulant generator, which is this function here, k of psi and lambda. And this encompasses a lot of models. This encompasses Poisson models, Gaussian models, binomial models, exponential gamma, etc., etc., etc. So it's lots of models come under this general setup. And it turns out that in this case, we can remove dependence on the lambda by continuing. Dependence on the lambda by conditioning on this A here. And we end up back in this happy situation where we have a significance function that only depends upon psi. We have got, there may be some loss of information on psi, but not usually very much. To come to one of the comments that Bob made in his example at the end, the Poisson model is a bit special. It's an exponential family model. It has what's Exponential family model, it has what's known as a cut. This is a language invented by Erlebandof Nielsen, who wrote a book about exponential families, excellent book, in 1978. And basically, what this allows is, in the case of the Poisson distribution, you can write the density in this form here, the distribution of t and a with psi in it, and f of a and some new parameter, zeta. And some new parameter, zeta, which is reparametrization of the nuisance parameter. And what this means is you can retrieve no information on psi from this part of the lighting. And that explains why when you integrate out psi, you end up with the same answers as before, because psi was never there in terms when you split the likelihood up. Split the likelihood up. And as I say, this is well, as I say, this term cut was introduced by Arnold Norsen nearly 45 years ago. Actually, I think it was in his PhD thesis, which was even like 1968. Okay, so that's having said all that, so these are special models, but there is something called a tangent exponential approximation, which was invented by Nancy Reed and Don Fraser, and you can use this as an approach. You can use this to take arbitrary models, arbitrary smooth parametric models, and give approximate, come up with an approximate exponential fact that basically allows you to do this program here, this business, in arbitrary models with a certain level of error. Okay, so this looks like a special setup, and it is a special setup, but you can generalize it so that you can make it work in So that you can make it work in more settings. Okay, let me talk now a little bit about calibration error before I get on to profiling and actually what's the topic of the talk. What we would like to have is for our inferences to be calibrated in the sense of Fischer, is that if we have some alpha level confidence limit for psi, then we would have this expression here. then we would have this expression here exactly for all alpha between zero and one. And in that case, we would get perfectly calibrated inferences. Our 95% confidence intervals would have coverage probability exactly 0.95, our 0.9999 confidence intervals would have coverage 0.9999 and so on. There'd be no big problem about this calibration. Usually what we don't get at, of course, because We don't get that, of course, because if we have nuisance parameters, we have to take account of that. And what then we get is a situation that looks like this, at least in a classical asymptotic setup. And a classical asymptotic setup has p parameters, a fixed number of parameters, the sample size, in some general sense, going to infinity. Sample size does not necessarily mean I put n here, that doesn't necessarily mean the number of observations. The number of observations. In a Poisson setup, that can simply mean that the means are getting bigger because of the infinite divisibility of the Poisson distribution. Because having more observations is the same as having higher mean, essentially, in that case. So you could have n equal 1, but the mean big, and that would correspond to high information in this setup. Okay, so what do we have here? We have a hierarchy of error terms. Hierarchy of error terms, an error n to the minus 1 half, and n to the minus 1, and n to the minus 3, 1, 2, and so on and so forth. And first order error is all of this stuff. It's got the first order error is n to the minus 1 half. If you could make a1 here 0, then you will have second order error. If you can make a2 equal to 0, then we would have third-order error. And in general, you really can't do better than third-order error. Do better than third or better in asymptotic settings. You can try and use methods to go further, but usually the error you introduce just makes the whole things worse numerically. These A, I suppressed it in the notation A1, A2, A3 depend upon the parameters and on alpha problem. Now in a modern asymptotic setup, we have typically We have typically the sample size is still going to infinity, but the number of parameters is not fixed, it's allowed to increase with M. And very broadly, and there are lots of details, but very broadly, the results that would apply in the classical setup, Wilkes's theorem, improvements to Wilkes's theorem, and so on and so forth, apply in the modern setup if the number of parameters is growing more slowly than n to the Slowly than n to the one-third. So you can have more parameters creeping into your model and things will be fine, provided the number of parameters doesn't increase too rapidly. And this is kind of obvious, about as good as you could hope for, actually, because the bias for maximum likelihood estimation is of order p cubed upon n. If you could do some hideous Taylor series. Some hideous Taylor series expansions, and that's what it turns out to be. So, if you want the bias to go away and m and p are going to infinity, obviously n has got to beat p cubed. So, for good inference, this is about as good as you could, this is as good as you could possibly hope here. And so, that's actually helpful because it tells us that improved inferences will improve in the same setting as you could use ordinary. As you could use all the influences if you want. Now, just a couple of comments about this. These are asymptotic results. So this is the result of even more hideous Taylor series, Edgeworth expansions, and so on and so forth. But numerical work suggests rather surprising accuracy for certain situations, even when the asymptotics don't appear to apply. So that's, and we'll see a bit of that later. That's, and we'll see a bit of that later. Okay, let's now talk about profiling, come at last to the topic of the talk. Okay, so under classical asymptotics, and when psi equals its true value, the likelihood ratio, Wilt's theorem tells us that the likelihood ratio statistic, this thing here, the difference between the overall, the maximized log likelihood, the temperature, in Bob's terms, and the temperature at some other point is going to have a point is going to have asymptotically a chi-squared distribution with dimension of psi being the degrees of freedom. When psi is scalar, we can equivalently take the square root of this thing, give it the sine of psi hat minus psi, and that will have a standard normal distribution, as the standard normal is the square root of the chi-square. And both these things are secure in the sense they're invariant. Secure in the sense they're invariant to interest preserving parametrizations. That's good. It turns out for these two statistics that one-sided inference has first-order error, one upon root n. But two-sided inference has second-order error. Miraculously, those A1 terms at the two ends, two sides, cancel. The catch with that is: okay, your confidence interval has 0.95 plus or minus, but in one Minus, but in one tail they might be zero, and then the other tail 5%. So the interval can be in the wrong place, because the likelihoods get shifted across due to the fact that you didn't allow for the profile. So the two-sided inference is not bad, not fantastic, but it's not bad, but the interval is in the wrong place. The modified likelihood root, which R star, which I Which R star, which I can give the formula for if you're desperate for that instead of lunch, will give inferences that are accurate for third order. Basically, it shifts the thing back to the right place. So you have two and a half in one tail and two and a half in the other. Or five in one tail and five in the other. And it's as it's this these are the higher order methods that Bob was talking about and um well I'm just enough sorry. Okay. Enough sound. Okay, so that's that. Now to come talk briefly about this question of simulation. The standard normal approximation to R would use this approximate significance function. It treats the R, the statistic R we're computing as standard normal. But the obvious thing to do is to say, well, why do I have to accept that? I have a big computer, I have a grant, I'll just spend some money and simulate the simulate the this attempt to simulate this distribution instead of instead of using a normal approximation. And what that means essentially is we just simulate lots of data sets from S data set south called the Y dagger from the profile value. And then we replace this normal approximation of the top here with a Monte Carlo approximation. Okay, and apparently this is called throwing toys, which I always thought involved a pram. We call it the parametric bootstrap. Now to come back to this question of its accuracy. Stephen Lee and Alistair Young, and with Tom DeSisio in a variety of papers, but I think this is mostly Alastair's work, proved in a series of papers that this paper Papers, but this paper, this approach has got third-order relative accuracy in linear exponential families, conditionally and unconditionally. And in the final paper here, a 2010 paper, they showed it's very closely linked to some objective beta. So we expect simulation to do very well. It should be, basically, should give us the same results as using modified likelihood room, but by simplifying. Likelihood route, but by simulation route rather than by an analytical approximation. I don't know. I was looking at the abstract and the bits of the paper last night in preparation for this, and there are some things there that puzzle me a bit, because they say that third-order accuracy is achieved in discrete data, and I had thought that was impossible. So probably I'm wrong, but one would need to read the careful. But one would need to read the careful paper rather carefully to be sure about that. Okay? I think it's not known whether in so this is in linear exponential families, right? In curved exponential families, which are much more common, I don't know what is, I don't think the third order accuracy could be preserved, but bootstrap considerations suggest that any way you can get second order accuracy. So we can knock out that first order error. Let's now come to marginalization. So, from a statistical point of view, this involves a prior for lambda, and then we base inference on this marginal density that you see written here. And so, what that will mean from a frequentist point of view is that even first-order inference is going to depend upon that lambda. So, psi, pi. If you get the pi wrong, If you get the pi wrong, then the inference will be neither secure nor calibrated. You won't even get alpha in the minute. You'll get some alpha, something else. In a paper in Biometrica in 2010, David Cox and Emmanuel Wang compared this approach with conditioning to remove lambda from the Poisson model. Again, a simple example. And what they showed in that case is that you don't gain much information by marginalization. By marginalization instead of conditioning. And that misspecifying the pie may give you an appreciably biased inference. And there's ongoing research that I'm aware of that is trying to figure out which aspect of the misspecification is crucial. Is it okay to get the mean right and the variance right and then everything turns out okay? Or do you actually need to get the whole distribution correct so that the inference is. So that the inference doesn't become fragile. Further comment is, and now we come to Bayes, you could regard this as a version of quotes, pauper's Bayes, right? We just say, well, I've got a joint prior on both parameters. It's just that the prior on the psi is independent of that on the lambda and is uniform. In which case, what will Bayesian do? They will compute this and use this to compute. And use this to compute posterior intervals. Okay, this is just the integral over both parameters on the bottom line and the integral, a partial integral over psi on the top, and the integral over lambda on the top. If you apply a Laplace approximation to both integrals then, you will then get a Bayesian version of the modified logic. And that's going to then give you, what this will then provide you with, is a second-order report. Provide you with a second-order approximation to the CDF, the Bayesian CDF for continuous. So you can think of the difference between modified, you can think of modified profile likelihoods as being an approximation to a marginalized answer. So instead of taking the profile likelihood, you take profile likelihood plus something, usually a half. Something, usually a half a log determinant of an information matrix, and that is in some sense numerically very similar to doing this sort of procedure. Here's just, I think this is just some examples here. I've given these, shown these numbers in every talk I've given to Feist and what so it's an anniversary. This is just in a body channel simulation with sneakers. With C equals 10. There are 21 parameters, 30 observations, so 20 nuisance parameters, 1 interest parameter. These are probabilities down the left-hand side here. This is what we would like these other columns to be. And what you see here, here, and here are what we actually get when we use these different methods. So if we use R, we get numbers that are okay down at this end, but then Down at this end, but then the bold means they've gone out of simulation error away from the target by the time we get down here. R star, as you see, there's no bold, so these numbers here are basically within simulation error of these probabilities. And R star B is a Bayesian version. It's not quite what I described before, it's a Bayesian version using a prior invented by Lord Tipshirani, but that is related to objective Bayes priors. Base priors invented by back in the early 1960s. And you can see this also doesn't do quite so well. Essentially, the problem is if you put Bayesian, if you put this with these priors, is they kind of pull the tails of the distribution in a little bit too much. So your distribution, which should be like this, becomes like this, and then your confidence in towards coverage is too complicated. And just here, I think we can skip this one. Here, I think we can skip this one. Yeah, we can skip that, I think. Yep, okay. Just to come to this discreetness, what this picture shows here, so I said there were two problems with the calibration. The first was the nuisance parameters, and I've kind of suggested how you can hope to deal with that. For the discreteness, things are a bit trickier, actually. What you see here on this picture is these are the exact coverages, these lines represented, exact coverage. Represented exact coverage gives the different confidence interval methods for values of pi for the binomial parameter in a sample of size 10. And the exact coverages, so you can see this nice black line up here. This is Klock Opierson, and this is what Bob mentioned. It's conservative. Instead of giving you coverage of the target 0.95, the coverage varies, but it averages out to about 0.98 or something. Averages out to about 0.98 or something. So confidence intervals would be too wide. And therefore test probabilities would be too large. And then here you have various other methods, none of them very satisfactory. The best appears to be this dashed line over here, which is kind of related to a Bayesian version of this problem, where you say instead of having ten observations, I have twelve, and instead of having two successes, I have three, half successes, I have three. Our successes are Africa. And that kind of fixes things up. But there's inevitably going to be some variation in the coverage. So just to, hopefully I'm finished. Okay, so just some comments about this. Of course, the binomial example is going to work better with higher M, but anyway, the exact intervals will give you a lot of power for exact tests. And approximate intervals, especially Bayes 1's, maybe. Intervals, especially Bayes ones, may be preferable. I join with Bob in saying that the mid-p is really preferable in many cases, and it does a kind of averaging. It's like a continuity correction, essentially. Okay, and then finally, I think. Okay, so just to come back to this, the frequentist inferences should be relevant, calibrated, and secure. If they are based on the likelihood, they will also be efficient. Come back to this question currently. Come back to the question hardly. The relevance involves comparing the data with a suitable reference set and offering some form of conditioning, which will either be implicit or explicit. Lots of models of interest, in high-energy physics, I think, are curved exponential families involving the Poisson distribution. So we need to do some conditioning for both for relevance and to eliminate the nuisance parameters. Eliminate the nuisance parameters. For calibration, we can't usually have exact calibration, we aim beyond the first-order approximation. We would ideally like third-order, but actually that's not possible, I think, with discrete data. The likelihood ratio statistic profiling is first-order accurate in general, but the modified versions are conditionally second or third-order accurate. So the conditioning is built in when you do that. You don't have to think about it. When you do that, you don't have to think about it explicitly, it just happens. Simulation reduces the error from profile in special cases. I think it probably does it to second order in general, and that's probably as good as you can expect with the Poisson distribution. But it will improve numerical behavior. And in my view, marginalization seems unwise in general unless you know the pi is the prior as well specified. And then just final point. And then just a final point: discreetness tends to give conservative inquiries. That is that. Thank you very much. So earlier Bob mentioned this idea of taking the nuisances and getting the results by integrating the nuisances in a small interval around them. How do you see that with respect? How do you see that with respect to what you just said about marginalization design-wise if you use priors? The fact that you are close to the maximum likely fit value of the nuisance parameter, does that make it reasonable? Well, it's what a statistician would call a database prior. Right? And so you can, I'm sure that Dave has some views about database priors. Database prior. I mean, they're not gener. Essentially, what you tend to be doing in those situations is using the data twice, and therefore, your inference is too tight would be the usual kind of statistical expression. But if you did, instead of actually marginalizing, you just marginalize. I mean, it's kind of closer to the it's a little confusing. Somehow it does seem closer to the profile, right? Yeah. But from a purely Bayesian point of view, it sounds yeah. Yeah. Don't do it from a purely basing point. Yeah. Yeah, so just on your second and last point, I just wanted to try to understand a little better. Under the situation where you had discrete distributions where you divide these cuts, it seemed like you were saying that marginalization in that case was basically pointless. Yeah. But would lead to what would seemingly be a very good answer. Yeah, I mean basically you're doing an integration you don't need to do. Right. So why would you do it? So, why would you do it? Well, we know that you don't need to do it. I guess that's sort of like sort of in the case that Bob showed. It's not something we didn't know, I guess, but sort of if you follow the procedure and did the integration, you arrived at a nice result. Yeah, so it won't harm you if you know you've got it correct. Right? Well, that will you need to look at the structure of the lighting book and just check that. But yeah. I mean, of course, you could say, well, I can do it. Could say, well, I can do it one dash. As I say, one bulb. Sorry. Put his hand up first. So if you go back to the derivation of the likelihoods or profile likelihood ratio as a good test statistic from Wall, there's some kind of weighting of alternatives implied in proving that it's kind of like the optimal test statistic. Kind of like the optimum test statistic. So there is already some kind of prior. Is there some kind of connection between that prior for which the likelihood ratio test statistic is most powerful and the prior to be used in our generalization? I don't think it's most powerful in total general. The name of Pearson Lemma tells you for simple versus simple, it's most powerful. And if you can condition out your nuisance parameters, then you have this. Use these parameters, then you have this argument for similarity from neighbour. I don't think in a general model, it's necessarily the most powerful. It's just a kind of a statistical hope, but because it's like the neighbour, you hope for Neyman Pearson never kind of applies, although it's doesn't really appear. But what makes this concrete, right? So you imply some kind of prior of, I mean, there is no universally most powerful test statistic, but if you choose a prior on which alternatives you value more, On which alternatives you value more and which you value less. There is kind of a most powerful one for that particular choice of prior, and that happens to be the profile like good test statistic if you construct that prior in a certain way. Is there some kind of connection between these priors and marginalization? Actually, had a prior offer, you probably would be in that decision to do something else. Say, oh well, fine, I can now be a Bayesian, I'm happy with really happy because I forgot to say it's then irrelevance. Of course, because the Bayesian conditions, this is kind of paramount, because a Bayesian will condition on all the data, the inference is perfectly worthless. But on the other hand, there's no comparison to a reference set because the reference set is the data set itself. You have to get randomness from somewhere else. Just to clarify what I was talking, so a lot of our problems in hydrogen physics have the same structure. For example, upper limits on Poisson mean black, rarely lower limits on Poisson. So if someone figures out which prior has good frequencies properties, then we know for that type of problem you can use that. So historically in hydrogen physics, it was learned that a flat prior and a Poisson mean. A flat prior and a Poisson mean, even though statisticians call that their favorite non-informing prior, that gives the perfect frequency answer for an upper limit. If our field did lower limits, we'd want to use a one-over mean prior to get the perfect lower limit frequency answer. So that's what I was advocating: exploring kind of generally, I think, as I mentioned on my slides, generally if you go to a metric where the distribution is kind of Gaussian, then that basically is. And that basically is where you can get away with the flat prior. One other, maybe, one other comment about the prior, because Nick mentioned log normal. So what happened historically there is people were using a normal prior for a positive parameter, and then they would have to truncate the Gaussian at zero. And then Luc Demorti showed that a whole bunch of results from teletron. Results from Tevatron doing a Bayesian analysis with that truncated prior. In fact, you show with paper and principle that the integral didn't exist, like Erdwire was doing numerically. And so people switch to log-normal priors, which go to zero. But then you're introducing this long tail, and so you still got to make sure that it's not causing you trouble. If that nuisance actually matters, then you just can't. Matters, then you just can't kind of arbitrarily use a lot of games. Gamma prior would not have such a long time. Yeah, so the gamma prior prior, except for this case, underneath the hood of what I showed, that flat prior and the sideband gives a gamma prior, gives a gamma prior. I don't know if people constantly turning to gamma priors. Mammoth priors or something like uh T statistics. Sometimes people use this later table statistics. Jump on one or I don't know. I have this note that shows how to get a gamma par, but I don't know. I could jump into that conversation, but that would be abusing my position as a chair. And it would also be me jumping into encouraging you monitoring. Just to comment, you had a slide near the end about discreetness and showing the coverage which was dipping below the nominal value, this one, which from a frequency point of view sounds bad because we don't go undercover for any value of a physics parameter. But like you said, this is specific for n equals n equals test. M equals ten tat. You can choose, if you change that number, the