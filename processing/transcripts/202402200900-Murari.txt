It presents a part of a work I've done in collaboration with Moshe, Ferdia and Carola. And in this project we studied the graph neural networks. In particular the focus was improving the stability, the robustness that graph neural networks had in tasks like node classification. So for today's talk, my plan is to start with a brief introduction to GraphC, and I'm sure that to some extent Sure, that to some extent you are all familiar with them, but it's more for notation purposes. And just a few slides on graphic networks, just to understand really the machinery we need to work with. Then we will move to the problem of adversarial robustness. Adversary robustness, and since I think it's not immediate as a problem to understand on graphs, I will start introducing the problem in the context of E. Introducing the program in the context of uses, which is a lot easier to visualize. And then we will move to the architecture that we worked on and a few numerical experiments here. So just again to set up the notation, a graph is simply a collection of notes or points in some space. Generally, it can be a Euclidean space, but there's no necessity of having it embedded in some linear space at all. And we collaborate. And we collect them in this set B. And then the important thing is that there is some interconnectivity between these nodes. And this is collected in the set of order pairs B, where you have just the pairs of nodes that are intercommunicating one with the other. And the graphs we consider are undirected graphs. So graphs in which if the node I is communicating with J, the same happens vice versa. So J is connected to I. J is connected to I. And this translates into a very simple condition in a way that one uses often to represent graphs, which is this adjacency matrix A, which is a square matrix of n by n, of dimension n by n, where n is the number of nodes of the graph. And this is a binary matrix, so it takes values in 0, 1, generally. It might even be a weighted graph, so this is not what we call. Graph, so this is not what we consider, but in the case of an unweighted graph, one has either zeros or ones, and one is in those positions in which there is an edge connecting the nodes corresponding to the row and the column in that edge. And undirected graphs have the property of having a symmetric adjacent symmetrix, so A transpose equals A. And uh this is just a a structure that one can be interested in, but Can be interested in, but what is really important for graph signal processing and also tasks where graph neural networks are useful is to have some signal on top of each node and possibly also on top of each of the edges. I'm not considering the edge feature maps in this talk, but one can also do that. And this means that to each of the nodes, for example the node di, one can attach a vector or any. A vector or any data information in principle, and generally, we will consider just vectors of a certain fixed dimension C. So we consider a quite uniform structure throughout the graph. Each of the nodes has this kind of length in the signal attached to it. And it's quite convenient to store all these entries, all these signals, into a feature matrix F, which is generally. Which is generally a rectangular matrix, which is tall and thin. So, n is the number of nodes, and C is the number of features. So, what we would like to do is to use these features, this information attached to these nodes to solve a certain problem, let's say. So, there are two main problems that one can be interested in when processing signals on a Processing signals on graphs. There are many others, but these are the simplest or the most difficult ones, as test cases, let's say. And graph neural networks are just a way one can do signal processing on graphs, and they've been shown to be quite effective in many instances. And the first one is the one of graph classification. So the task here is to assign a certain label, a label among a family of labels. Among a family of labels, allowed labels, to a graph. So, the inputs of this model, of this graph neural network, will be graphs. And typically, the training set is made of a bunch of graphs for which we know the label, as usually in machine learning. And then we have other graphs that are not in this family for which we want to infer an accurate prediction. So, for example, here we might have the necessity of classifying molecules representing. A molecules represented by a graph as a protein or not a protein. And this, for example, might be the training cells, let's say. And we have assigned these three labels. But the main task we have considered in this project is the second one, which is the one of node classification. So in this case, you don't have multiple graphs as inputs, or you don't have a data set of graphs, but you have a single graph as input, which you know something about. Something about. And of this graph, you know the labels of a few of the nodes. So let's say 70% of the nodes have an assigned label that can be a color, for example, of that node. And what you would like to do is to make a prediction for the remaining nodes. So this is a relatively simple task. And graph neural networks have been proven in many instances to be a very accurate strategy to solve these kind of problems. Kind of problems. But as also in image cases, in image settings, these networks, after being trained, might be quite sensitive to the perturbations in the inputs. And this is exactly what I want to introduce after this preliminary part on graph structure. So, if you are familiar with convolutional neural networks, for example, that are networks that are very effective. Networks that are very effective to process image structure data. And what happens in those networks is that you would like to exploit the structure of the data sets. So even though images are just matrices and in principle you could just enroll them into a very long vector and just plug them in a field forward network. This is not a viable or a clever way to process the images, but it was shown. But it was shown in many cases that it's better to use the local structure than using convolutional filters, for example. And the same occurs with graphs. So even though we have a rectangular matrix to work with, this F, the feature matrix, and even in this case we could just enroll it in a long vector and use a classical method, which doesn't explore the graph structure. This is not a very convenient way to proceed. Convenient way to proceed, both because it's inefficient, since we are not exploiting at all the knowledge we have about the data set, and also because it provides sub-power performance if you test it, essentially. So the main idea behind many graph neural networks, there are so many graph neural networks coming out every day that it's hard to classify them all. But an important principle is that you would like to replicate a similar principle. A similar principle that convolutional neural networks follow in the graph structure. So, to aggregate information, so to pass information and messages between nodes that are interconnected one with the others. Because this is what the data set tells you, the section. So, this is why a prototype of graph neural network can be of this kind. So, you have a sequence of layers that are relatively similar, one with these. That are relatively similar with each other. Of course, they have the degrees of freedom depending on the parameters, but the structure is more or less the same. And this is TL, where the parameters come into play. In this case, I have big L layers. And each of these layers essentially takes as input the previously updated feature matrix and exploits the locality structure provided by the adjacency matrix A to pass messages between these nodes. Between these nodes. So you can think to these features attached to each node that throughout the propagation, forward propagation in the network, change in size, as usually happening in neural networks. And this change in size and also values is determined by this locality structure in the interconnection. And then after this, what happens is that you have a matrix that is F L, and you can apply. That is FL, and you can apply maybe flatten it. It's in a different setting than at the beginning because generally you would like to shrink it to make it even thinner while you propagate so that you are closer to a vector. And then you can apply a multi-layer perceptron or whatever you think is meaningful in that task. But as I said before, graph neural networks and graph data sets have a lot of structure in them. And we would like to preserve that and explore. And we would like to preserve that and exploit that. And in particular, let's think to these previous examples, for example. What I did here is just assign some numbers to the nodes. But if I were to change the labels, the numbers assigned to the nodes, I wouldn't like my predictions to change, or I would like my predictions to change accordingly, let's say. So, for example, if I want to classify these graphs at the top, and I These graphs at the top, and I relabel the nodes, I would like the prediction to remain the same. And this is what can be represented in terms of the invariance with respect to permutations of the graph neural network. And similarly, if I am interested in a different task, which is node classification, I of course want a change in the prediction, but I would like this change to be accordingly to this relay unit. So this is what this permutation equivariance is. Is permutation equivariance, which is also what we wanted to preserve in our graph neural networks. So, as I said before, what happens generally at the prototypical level, let's say, is that you would like to aggregate local information and pass messages based on a certain set of weights that you learn. But really, what is important is that you don't care about all the notes to update the details or the information. The details or the information on the node one, but just in the neighboring nodes. So you just use this structure, which is very similar to convolutional networks. It's just that here it's more structured data set. So coming to our problem, our problem or our interest in the problem came from a previous project on Atlas Ada robustness for images that Primus yesterday slightly anticipated. And Atra Sara. And alversarial robustness in the image case is quite a relevant problem and easy to understand in the sense that what is very easy to verify is that you can train a big model, a big neural network like a residual network that classifies very accurately a set of images. For example, this is a simple test case, it's C410. So with images of 32 by 32 pixels, or 28 by 28 maybe, and in ten classes. And in ten classes. The resolution is quite small, it's quite poor, but it's quite simple to get a model that classifies accurately most of the images. For example, here on the left, we have an image of a ship which was correctly classified by a model of 92% accuracy when trained, let's say. But then it's even as easy to create a perturbation of the image on the left, essentially insisting on the sensitivity. Essentially, insisting on the sensitivity directions in the parameter space of the network, that for us it's evident that there is some change on the right, maybe there is a bit of blurring, let's say, but it's also evident that they belong to the same class. They are both shapes. But with very high certainty, the network is misclassifying the image on the right as a plane. So we can see maybe why it's misclassified, maybe not, but it's clear that for us, the label. It's clear that for us the label is the same. So we would like models that are reliable in this sense, to perturbations or as much as we can, let's say. And a way to measure the robustness of classification models or classifiers in general is not just to be reduced to the Lipschitz constant as here, but it can be essentially reconduced in an important manner to the sensitivity to input perturbations. Sensitivity to input perturbations of the model of the network. And a very convenient tool to measure this sensitivity is the Lipschitz constant of the map. Of course, it depends on the problem of interest, what are the norms one considers, because it might be better to work in a different metric for different problems. But it's important that we would like to have at least control on the expansivity the network has, so that we can tune it or constrain. Can tune it or constrain it so that the robustness together with other things might be improved. But of course, here we wouldn't like a Lichlis constant which is zero. That would be a very robust model. If we classify all the images in the same way, then we have a super robust model. But we are not dealing with something useful. So what we are looking for is really a trade-off between having an accurate model and a robust model. And a robust mode. So, this is for the image case, but we will use a similar approach or understanding of the problem in the graph setting. However, there is quite an additional complexity in graphs, which is, as I said before, they are not just a bunch of features like images in a structured way or in a structured way, but they also have a particular way of communicating what we want with each other, these notes, and in particular also to pass this. In particular, also to pass these feature informations, information. And this is based on the adjacency matrix, essentially, and hence on the edges of the graph. So, for example, in this circular graph, in which each node is intercommunicated with two other nodes, one can fool or try to fool the network just by dropping an edge, for example, in between node 1 and 2, and adding other edges, for example. Other edges, for example, between 1 and 6 and 1 and 9. This is quite common. It can happen in many settings. A very simple example, but there are many more, is, for example, a hacker that introduces some friendships on a network of friends on Facebook. But there might be more relevant examples depending on what you think of. But really, this is a relevant problem because if we don't know, let's say that the graph we have a big Let's say that the graph we have available is not reliable completely in the sense that it has some attack structure, then we need some different way to process this data set. So generally what one works with is a setting in which the input of your model is not really the clean graph. We don't suppose to have information about the clean graph, but we only support Clean graph, but they only supposed to have available information about the attacked one. And we also can't assume that the attacker has a certain budget that can spend in attacking the graph. And this budget is measured in this case oftentimes in two different metrics, depending on what you attack. So if you attack the features, which is this long and tall and thin matrix F and turn it into F star, turn it into f star what you can generally suppose is to have a bound on the norm of the perturbation delta f in the Frobenius norm and this is epsilon one with budget and then similarly there is a budget on the adjacency matrix perturbation but given that adjacency matrices are binary matrices it's not really so meaningful to consider perturbations in the Euclidean or Strobenius norm and a more common way to measure this is the L. A common way to measure this is the L0 norm. So you account for how many entries are changed. And since it's either zeros or ones, this also is equivalent to a bound on the one norm of the vectorization, as I wrote here. So it's important that attackers are clever generally, so they're not easy to be discovered in the sense that, for example, if the graph is metric is under the graph, The graph is symmetric, it is undirected, such a structure is not broken by the attacker. So you can still suppose to have a symmetric matrix. And the goal is, as you probably have guessed, is to have a prediction that is similar to what you would make as a prediction with the clean information available without knowing it. So, what we decided to work with, because we came from this background and we think it's a nice approach. We think it's a nice approach to design neural networks in general: is starting from dynamical systems and discretizing parametric, suitable dynamical systems, so that your layers of the network come out from a numerical method applied to these parametrized dynamical systems. And this fits quite well even in more general settings, like in convolutional neural networks, where we want to enlarge the dimension of the inputs. Enlarge the dimension of the inputs and process them, project them in the final space, and so on. So, you might not be able to explain everything in the network with dynamical systems, but the purpose would be to have most of the updates coming from dynamical systems ideas and theory. So, this is, for example, a picture in which you have like a lifting layer L1, which is a linear layer. It increases, for example, the size of an image. Increases, for example, the size of an image by channel-wise, for example, and then you have a bunch of layers that are like ResNets, for example, if you choose explicit EURS and numerical methods, but you're not constrained to that. And then you might iterate on this idea. That's it. And then P is just sending into a final space. For example, if you are classifying into 10 classes, it's R10. That's it. That's it. And this means that to define a neural network, in this case, what you really need is a family of parametric dynamical systems, or differential equations, and a strategy to discretize them, so that what you want the network to inherit is inherited really. This is why I'm introducing now the graph neural network that we decided to propose, which is based on updates not only. Based on updates, not only on the features, as I showed before in the prototypical network, but also of the adjacency matrix. So the idea was simply because if we know that the adjacency matrix is not to be trusted completely because it's attacked, then it might not be so meaningful to just process the features and the signals based only on that information. Only on that information. So, what we decided to do is to couple the updates of the features, as typically done in graph neural networks, with updates also of the adjacency matrix. And this is why we have these coupled systems, graph neural networks, because you have a system of ODs, of matrix ODs, one for the features and one for the adjusted C matrix. The first equation is a sec is a gradient flow in the the metric, the trace metric. metric the trace metric in using the Frobenius norm so it's quite a common architecture in the in the field or it's not so common but it's known as PDGCM and G of A I will introduce it more carefully in the next slide but is the graph graders operator and GA transpose is just the transpose of this pushing back into the right dimensionality let's say and instead And instead, the updates of A, which can be uncoupled from the updates in F in our design strategy, let's say, are aiming essentially just not only to update A, but also to preserve the structure, as I will show later. And in this case, we also decided to just proceed with a simple discretization, which is explicit other, having a control on the step size, so that we could play with the LibGuides constant of the network overall. An open actor overall. So A doesn't have to be stay with only zero and one engine. Yeah, that would be very nice, but we couldn't think of a way like that. So it becomes really a weighted graph after this. But then do you make sure that the weights are all non-negative? No, no, it becomes really a weighted. Yeah, what we ensure, as I'm showing now, is that overall the graph neural network will preserve the symmetry. The symmetry properties that we are interested in. So, what I said before, this equivalence property is preserved. But the problem with preserving the binary structure is that it becomes a very structured system again. So, what we do generally in the training is to threshold the entries, so that if it is near enough to zero, then we we drop the entry, let's say, but this is not at the level of of the dynamical system. Of the dynamical system. So that would be a nice solution. And just for completeness, the graph gradient operator is essentially measuring the distance, the difference between the features Fi and Fj. And this difference is weighted by the entry in the adjusted symmetries, Aij, that, as you said, now it's not necessarily EF0 or 1. So it's 0 and 1. So it's a linear map in this case. And then the transpose operator is just defined by usual inner product structure that defines the transpose. And what is really important, I think, is or new in the proposed approach is the second part, which is the update in the adjuster symmetrics. We didn't come up with this solution with this linear layer. It was already Layer, it was already studied before in the paper. And this is the only way to parametrize a linear equivalent map that processes an input matrix. So any linear equivalent map can be written in this way. And the nice thing is that even though it's a lot more expensive to process in a coupled way, not only the features, but also the adjacency metrics, the nice thing is that The nice thing is that it's a good compromise here because we have just nine parameters to learn, essentially. So it's not a dense system that we are learning, but it's just nine degrees of freedom, K1 to K9, that can tune the dynamics. And this is just preserving these two properties I wrote at the bottom. So it's equivariant with respect to permutations, the one on the left. And if the input is symmetric, If the input is symmetric, so will be the output. So you preserve the structure in this case. So the main motivation behind this system comes from a similar procedure we followed in the case of images, which is to design dynamical systems that are contractive in a certain metric and to discretize them accordingly so that even the network afterwards will inherit these stability properties of. The stability properties of contracting systems. And for coupled ODEs, it's a bit more complicated because there is some interaction. So it's not sufficient that the single ODEs, when considered separately, are contractive in order to get a contractivity behavior, contractive behavior overall. But there are probably many other results like this, but in this paper by Sontag, we found that if you have a couple system of food. If you have a couple system of ODEs, and each of the two is contracted in their own metric, then what you can show is that depending on the next Jacobian, so I didn't write it, but let's see. So depending on the Jacobian of the first ODE with respect to A, so the interaction term essentially, you can find a pair of coefficients, M1 and M2, and it's not just a pair, it's a set of pairs that satisfy certain. A set of pairs that satisfy a certain bound that allows you to have a contractive behavior, actually a non-expansive behavior, with respect to a weighted norm in this direct product space. So you have a space where you have this convenience norm F acting on the first component, the features, and the vectorized one norm acting on the adjacency matrix. And if you weight these two contributions properly, accounting for the interactions between the two. Accounting for the interactions between the two ODEs, then you can get a contracted behavior. So, similarly to the picture I showed before, just for completeness, we also design our graph neural network by discretizing the system of ODs, as I said with explicit Euler. And what we do is not to take the initial condition as initial condition the perturbed versions of the graph information. Information, so a star and a star, but also to start with an inner layer, K. So K is just embedding F star in a higher dimensional space generally, and this is taken as the initial condition of our dynamical system. So the graph neural network is simply just the composition of these forward order steps applied to the couple system. So you have this first layer, which is already something. Already something commonly used in the graph neural network literature, and that it's relatable to a diffusion process, while the second equation is instead coming from the adjacency eight of the coupled system. So, what we did is then to consider what happens at the discretization level of our system of differential equations, and based on the And based on the theoretical results I showed before in terms of continuous dynamics. And what we notice is that if we consider, actually this was the main motivation behind this architecture choice, is that if we only update the features, we have a system that in continuous time is actually contracted in its norm, depending on the activation function. And if the restriction is only being If the restriction is only being non-decreasing and one leap sheets, one can just have non-expansivity, as I wrote here. And if you have a small enough step size, it's not something complicated to compute. It's just that there wasn't enough space, or I didn't consider that too important. You can compute a bound on the step size, HI, so that even in the discretization setting, you have this non-expansivity behavior. And the same happens also at the level of the adjacency updates. So, this is not for free in the sense that you need to impose a bit more structure in the linear equivalence update. So, just to recall you, this is the linear update in the second part of the slide. You see that we have nine parameters, k1 to k9. And we notice that if we constrain just the first parameter, k1, in the Parameter K1, depending on the other ones, as I show you here, like here. So we have the degree of freedom given still by a parameter which is alpha, but it has to be non-positive. Then if we constrain K1 in this way, then what we have is that we also get a contractive dynamics at a discrete level two, also for the adjacency updates, but this time in a different method. This is the vectorized L one norm. This is the vectorized L1 norm. So it's just the sum of the absolute values of all the energies of the input. So this result is instead what we could come up, who could work out at the moment. I would like to get something better than this, more related to what I showed before in the continuous setting, this weighted norm. But for the moment, what we could show is that if we don't consider a weighted norm, some of the contributions. Weight ignore some of the contributions, m1 and m2 as before, but just sum them up with a coefficient which is one, let's say, then you can have a bound on the Lipschitz constant, on the expansivity of the neural network, which is not as satisfactory as having a contracted behavior in a weighting norm, but at least it shows that depending on the weights, essentially, in the updates of the feature matrix and the step sizes, these H5. And the step sizes, these HIs, you are in a quantifiable way far from being non-expansive. So you have epsilon 1 plus epsilon 2, which would be great if these were the only contributions. But epsilon 2 is multiplied by 1 plus something else. And this something else is due to the discretization of the ODE, essentially. So if we were to work with the continuous dynamics, that term wouldn't be there. Dynamics that there wouldn't be there, but by discretizing, we have to account also for these contributions. I'm quite sure that we can get better than this, but this is what we could manage to obtain up to now. And now I want to conclude with a few experiments. The first experiment that I think is important to motivate what we did and was the first one we focused on. We focused on is trying to answer the question if we really need to update in a joint way both the features and the adjacency matrix. Because updating also the adjacency matrix is a costly operation. That wouldn't be, if it weren't necessary, it would be better for sure. So we tested essentially the case in which, from the coupled system I was considering before, we don't update the adjacency matrix. Don't update the adjacency matrix, so you could think a dot k0, for example. And this is the case in the first row, no edge, so no update to the adjacency matrix. And in the second case, instead, we proceed with a couple system. So these are all classification tasks in which we have data sets that represent a network of research papers, and these research papers have attached to them. Have attached to them a signal, which is like a word, a long vector, in which each of the entries represents the appearance or non-appearance of a certain word of a dictionary in the paper. And the interconnection between these papers or notes is based on the citations that come and go from one paper to the other. And these attacks are essentially based on different ways to perturb the graph structure. So, for example, meta. Structure. So, for example, NetAttack is a targeted attack in which what we do is to pick the nodes of the graph that are more important based on the degree of the node. So, based on how many edges are incident in that node. And we attack those with at least 10 of these edges. So, in the sense that this is kind of an important metric, and if you have a high degree of And if you have a high degree of anode, it means that it's like a hub for the network, for the graph. And if you attack that, you suppose that you change more drastically the performance of the network. While random, for example, is instead just adding random nodes, so flipping zeros and ones in the adjacency matrix based on how many edges, sorry, adding random edges based on how many edges are there in the Based on how many edges are there in the full graph. So, here, since it's a classification task, the higher the better. And we noticed that it was worth updating not only the features, but also the adjustment symmetrics. And here, just to conclude, there are a few more examples. This is still a similar task, so what we need is simply to compare our models with either other architectures trying to. Either other architectures are trying to solve the same problem. So, ECS GNN is really what I call CSGNN. So, what we have seen up to now, this constraint network with all the constraints on the step sizes. And CSGNN is instead something a bit more with more degrees of freedom. We wanted to see if just having a dynamical system, a data trip and learning of the updates, was still beneficial or could keep benefits in this task. Could keep benefits in this task. So, CSGNN is something with less structure but very similar understanding or very similar theory to what we have seen before. So, we see that here the purpose or the results which are improved will correspond to some some points, some segments that decay less quicker and l less quickly than in the other networks. So Ediotic networks. So, what we have here is the magnitude of the perturbation. So, in this case, as you go to the right, you increase the magnitude of the attack, while here is the accuracy of the classifier. So, what you would like is ideally a flat line, because you see that the network is very robust, but at the same time, you also would like a relatively high value of accuracy, otherwise, there's no point in the class. Otherwise, there's no point in the classifier. So, really, what matters is the area under these segments or these curves. So, we can see that our networks perform quite well in this case compared to the other strategies. And all the other strategies essentially do something similar to what we do, but most of them pre-process the adjacency metrics instead of updating it jointly with the features. So, for example, DGC. So, for example, the GCN SPD, what it does is to approximate the adjacency matrix with a low-rank approximation and then uses this reduced version of the matrix, or like yeah, this low-rank approximation to as a normal SSC matrix. So, this is typically what is done in the other approaches. So, it looks almost like the same. The same methods which are done on low perturbation are also the least true about us respectively perturbation. Correct? Yeah, yeah, that doesn't help if you start lower in this case. But in this case, I don't think this is a general correlation because oftentimes if you drop inaccuracy, it might be that the model is much more constrained, so you get a flatter curve. But yeah, it makes sense. Curve, but yeah, it makes sense in this case, at least in this group here. And then we tested other kind of examples. PubNet is a bit larger data set. So here it was evident that our network, our approach was more computational intensive and expensive. So this is something we still need to work on to make To work on to make the updates more efficient, but still we could at least solve the task and get better or on path performance to the other networks. So, and this is just the other experiment in which we have random attacks, as I said before. So you just account how many edges are there in the clean version of the graph. So you suppose you cheat somehow here, because you're supposed to know how many edges are there. How many edges are there? And you use this as a fraction for how many edges you randomly add. So you see that it's not every time better, but we were quite happy in this phase too. So other than this, there are many other attacks that one can consider and many other data sets that are interesting for us. And a particular problem we are starting to investigate now is other attacks in which the attacker is also allowed to introduce no. The attacker is also allowed to introduce nodes. So, here I was supposing that the graph structure was attacked in terms of adding or removing edges and changing the features, possibly. But if you add or remove nodes, then a lot more complexity can come into play and the attacks can be stronger. So, this is more part of the further work area, let's say. So, yeah, for me, it's everything, and thank you for the attention. So you're phrase A, they have to be positive, I guess, would be. If they're not positive, you're getting into trouble. Trouble in... You get an inverse equation. If they're not positive. You are integrating backwards the equation. Yeah, yeah. Yeah, this is something we thought about. But we are not normalizing. But we are not normalizing it because we are integrating just for a few steps. So we have not encountered this issue. But it might be that well, I totally agree with that, integrating backwards. And we will probably need to do that if we want to add legal models where it's really trying to follow the dynamics of the backward equation. Yeah, I actually have a suggestion that I think would get you over the hum with most of the problems you have because graph theorists don't measure graphs in the opinion or L1 norm. They use what's called the cut norm or the Grothendieck norm. And that allows you, the Grothendieck norm measures structure of the graph as the adjacency matrix, as an operator instead of as a vector, right, which is what you're doing. Which is what's hurting. And what that means is that you can add or delete edges and the graphs still stay close together. Or you can add or delete nodes and the graphs still stay close together. So if you could probably redo a lot of what you did, and if you did it with the cut norm instead of the L1 norm, you would be able to get at the problem that you started with. So one application of that would be: if I feed in an image with a certain resolution, then if I fed in another, And then, if I fed in another image with the same image just with higher resolution, they should still be classified the same. And the cut norm would capture that because the structure is still there. But these vector norms, they don't capture it, right? If you delete a node, it explodes, so you're sort of reduced to looking at noise. So I think it'll, I mean, based on what you just said at the end as well, and based on what you started with, if you What you started with, if you redo the, I mean, if you use the pedal instead, you should be able to do this. Okay, yeah, I'm not familiar with that, so maybe we can. Yeah, yeah, yeah. It's a very, I mean, it's a niche aspect of graph theory, but it's become very important, especially in graph neural networks as well. So people look at what's called transferability. So if you have a kernel based on a graph, you can swap in similar graph structures and the output should be the same. It's sort of a prevention of what you're doing. And you use that on the classified. And it's really a norm. Oh, sorry, is it really a norm? Yeah, it's a true norm. And the other thing is, it accounts for node relabeling as well, so you don't have to build it actually. Okay, no permitting. Okay. Thank you. Any other questions? No, thanks, thank you very much. Sorry, I changed that to people.