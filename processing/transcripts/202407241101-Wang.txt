Okay, well hello everybody. I'm thanks Staphney for an excellent talk. I'm also a representative of the ethical part of the statistical, computational, translational and ethical, but bridging over to the translational as well. Translational as well. So, I'm part of the Genomes People Research Programme at Brigham and Women's Hospital, and the mission of that program is to accelerate the implementation of genomic medicine. So, I'm going to be sharing some ethical issues emerging at the coalface. So, ethical challenges. What are we talking about? In general, we're thinking carefully about the benefits of risks and benefits to the research that we do, and in particular, That we do, and in particular, who do they accrue to. But I think a helpful thing, and sort of one image that I'm hoping that might stick with you, is that we've got an underlying two by two matrix here portraying a former life case and management consolidation here. So, usually in research ethics, we're focused on thinking about risks and benefits, two main categories. We're thinking about the benefits to society that About the benefits to society that are going to accrue from the research that we might do, be that more basic research or more translational research. And then we're focused on the risks that accrue to the research participants. So in this case, the people who are actually going to sign up to be a participant in a bio-bag. And the whole sort of human research ethics apparatus, which in the US is this sort of institutional review board system, in every country basically has something equivalent, is really motivated to. Really was motivated to protect participatory research and knowledge. But it is a two by two, and so thinking about these other two quadrants, which has been where a lot of the focus has been from my world over the last decade or two, first of all, there's the sort of increasing pressure on the likes of yourselves or the people who are setting up the fire banks to think about returning. Biobanks to think about returning value to the participants of the projects. So, in all of us, for example, they're returning results, they're returning me to the Gansa screen results, and they're also returning clinical action findings to all of their participants. And there's sort of an emerging ethical and legal consensus, which is really pushing all people who are setting up new studies to do something up here in this quadrant. And I'm happy to. And I'm happy to talk about that in the discussion. Not going to be the focus of what I'm talking about today. And then what Daphne was talking about is much more in this quadrant, which is the risks that we might and the potential harms that might accrue not to the participants of our research but to society at large, usually specific groups of people that we're worried about. And I'm going to be talking about some experiences that I've had in the eMERGE network, which I looked it up. It's not technically a biobank, it is a network of bio-repositories. But we've heard eMERGE referred to many times throughout this workshop. And we're currently in phase four. We just finished recruitment last week. That's 25,000 individuals to whom we are returning genomic risk assessments. So they're each getting about 10 polygenic. Getting about 10 polygenic risk scores integrated to varying degrees with other information, including clinical information, into these big reports. And we're sort of partway through returning those reports to all of these individuals. There's 10 sites, the coordinating center and various other partners, with a $75 billion NHGRI sponsored project that utilises what gets called embedded LC. So Daphne already introduced LC. LC is ethical, legal, and social implications of genomics. Legal and social implications of genomics. And for those of you who don't know, the NHGRI has a federal mandate to spend 5% of all its funds on this type of research. And that in turn stems from the history of the Human Genome Project, where basically Watson realized that he couldn't get the effort off the ground unless he said, Hey, we are actually going to address some of these concerns that the public at large have with the prospect of doing this participation. So let me tell you a bit more about Elsie. So, let me tell you a bit more about MBED LC. So, there's usually two main parts of it, or here's one way of thinking about it, and there's being two main parts of it. One is people like myself will run research studies, usually going out and engaging with stakeholders, things like patients, providers, scientists, etc., to try and answer specific research questions. So, in the case of eMERGE 4, it really wasn't clear what clinical report designs would be preferred by patients. Report designs would be preferred by patients and providers, or how they might react to that information. So, like, how on earth should we design these reports and sort of processes that go along with returning them in the sort of optimal way? And then, the other sort of stream of work beyond these sort of concrete research projects is to provide guidance on issues as they come up. And it's very key that this requires first identifying. Requires first identifying that there are ethical issues, which is probably the hardest thing. And what does that process involve? Well, first kind of working out what the whole picture is and then collaboratively searching for solutions. So a lot of herding counts. And an example of this that we faced was how to handle the differential performance of polygenic materials by group, which we've also heard a bunch about this much. So just to give two very complex So, just to give two very concrete examples here, I led a study where we interviewed patients and providers on the design of these reports, and we got from that study much more information about how those providers would actually use those reports with their patients. These three different use cases emerged from that. That came up yesterday: wow, what is going to be the clinical impact of these polygenic risk scores? So, that was a Scores. So that was an example of a concrete research project. And the hope is that some of these research projects then inform network decisions, and that's the way that images are set up. And then here's an example of how we thought about this differential performance issue, because there's underlying equity concerns, but also a load of practical challenges that we face. So a series of decision points. Decision points. Again, not in focus, but I hope to talk to that today. Instead, I wanted to talk about something that we're struggling with right now that I suspect that various people either you or that you work with are also faced with, which is how we think about sharing the data more broadly that we collect from our participants. And my colleague Maya Savatello from Columbia and I have. From Pulummia, and I have been corralling cats around this in particular. So, okay, so a network like eMERGE, we have all of these different sites who are collecting a bunch of data. And then we have to smush it all together into the data that the network analyses. And that is going to be non-human subtext data. So there's a process of going from human subjects data, aggregating it, and making it non-human subjects data. It and making it non-human subjects data. And then we have to make some decisions about what of that data we then share with the rest of you. So let's talk through some of the issues that came up. And something that I like about being part of these big projects is the sort of ethical thinking we're doing is not pie in the sky. It's like we need to make a set of decisions right now. And it can help indicate where we need to go. Help indicate where we need to go. But in our case, as is the case for most such data these days, the model is controlled access. So what you're doing is you're going to upload your data somewhere where people who have a certain set of credentials can be able to access it. And that goes through something called a data access committee. Most NHGRI funded research is kind of almost forced to use AMVL, but not quite forced to use AMVL. AMVL, but not quite forced to use AMVL, and so that has its own data access committee that then becomes our data access committee. And as you probably all know, if you've ever gone for NIH funding, before they send you any funds, you have to commit to how you're going to manage and share your data right up front before you've hit any of the problems that are going to come up downstream. So I want to talk a little bit about how we handle sort of Bit about how we handle some of these risks and then a little bit about how we're hoping to handle some of these risks as well. Okay, so minimising risks from re-identification, because really the risks to individual participants can only accrue if people know who they are somehow, right? So, our informed consent form, like most, will say something like, you're Most will say something like, Your privacy is very important to us. We've all probably read a sentence like that every time that we click, Yes, I accept the terms and conditions. Thank you very much. So what we do not say, and what basically nobody says these days, is we will de-identify your data because we fundamentally believe that that's not possible. So huge So, huge, kind of massive technical efforts go into things that protect privacy. Actually, a lot of the talks have included things about federated learning, etc. And massive, massive, massive efforts. But this is the kind of bar that we're trying to reach. So, one question to you is: what is privacy? And this is actually a pretty contested question. A pretty contested concept. Is it something to do with secrecy and like keeping your data secret? Is it something to do with control or access? And then what kind of norms govern that? And one way to think about it is what could you reasonably expect as somebody who is donating your data to a biovaxi? Donating your data to a biomass about how that data is going to be used, who has access to that cypher. We want to minimize these risks. So obviously we're going to remove direct identifiers, but we also have to handle something called quasi-identifiers. Who's heard of quasi-identifiers? Yeah, a little bit? Right. So quasi-identifiers are those things which, in combination with Those things which, in combination with other data points, could lead you to be able to triangulate somebody's identity. So, even without their name and address, you can be like, oh, well, they're thistle, they've got this job, they live in this sit code. There's only a few people who have them. We can kind of work out who that is. And there's some standard approaches to do that. If you've got the resources and you've got the technical. Got the resources and you've got the technical know-how, then one approach that lots of people are keen on is you basically sprinkle a noise and so differential privacy and things like data that were in that category. And a more easier to implement and more standard approach is you quarter the data somehow. So for example, you define age buckets, like this person is between age 65 and 70. And you can do that before you release your data. Data. An alternative is you can say to people who use your data, thou shalt not report on a cell size of smaller than X number of people. But it's very standard to roll up data. So when you're looking at data, you're not seeing the underlying values that the researchers collected, you're seeing these roll-up galleries. Here's an example. So I mentioned this the other day, but the way we The other day, but the way we think about base and ethnicity data, the way we collect that is changing, is in the middle of changing, which poses harmonization concerns. But we, like all of us, are using the new system where you can select multiple of these top-level categories. And as you can see, you get a really See, you get a really big long tail of even in 25,000 people, you know, just two people identify with these three top-level categories. And then there are many other such where you've got this big, long, potentially identifying tail. So, what should our strategy be? A fairly standard thing to do is to bucket everybody who takes. is to bucket everybody who takes more than one category into multiple, just say, you know, multiple race and or ethnicities. But there are issues with that. So for example, you might be able to see from this upset plot that people who picked American Indian or Alaska Native, there's actually 68 people who identified just that way. And there's many more who identified in combination with some of these other categories. So if we were Other categories. So, if we were to take that, bring all the multiples together, we would kind of obscure many of the individuals who identify that group. So, in the end, what we're going to do is we're going to roll up sort of this part of the distribution into other combination of categories. So, point being that there are sort of concerns that come up with this rolling up. Concerns that come up with this rolling up process. So, okay, so that's you're reducing the possibility of re-identification. You can also might want to reduce risks of re-identification if that happens. And by doing that, the main thing that one does there is to adapt sensitive information. So, my question to you is: what counts as sensitive information? And how do we decide what counts as sensitive information? So, in our case, what So, in our case, what we did is we sent surveys to all of the sites, asked them for their thoughts on this, and how they would like to actually put that into practice. And then we had a discussion and we aligned on what we did want to adapt. And during the process of this phase of the e-merger network, there were some pretty large political developments, including the overturn of Roe v. Wade and a lot of legislation targeted at general. Targeted at gender-affirming care, particular departments. And what we ended up deciding to do was to redact anything in our data that could link anybody with something that might now be considered a legal universe. And then we had discussions about adapting various other things, but this is what we decided to adapt. And then finally, you manage access to trusted researchers. So in our case, Access to trusted researchers. So, in our case, that's handled by the ANVL DEGAP Data Access Committee. And then you make people tick the box to say, I have read and understood this safe use agreement, which usually promises that you will not try to be identified on individuals. So, this is a sort of package of things that come with thinking about these risks to individual participants, and I'm going to. And I'm going to return later to sort of the how this could be done better. But now I'm going to switch to this other type of risk, risk to public, and most bioethicists, Daphne is one of them, are more concerned really about this category than they are about this category. And the challenge is it's much harder to know how to deal with it. So, what are we worried about? What types of harms? What types of harms? We might worry that there are research questions that are fundamentally problematic and just shouldn't be asked. So I invite you to reflect on whether you think that there are any research questions that could be asked, so the data that you actually use that fall into that category. Well, another a slightly different category would be research questions that cannot be adequately answered, so it's actually irresponsible to ask them of this type of data. To ask them of the site data. Like, you're just going to get misleading results, so don't go there. Another category that we might worry about, well, so here's an example, like links between genetics, race, and IQ. Right? Probably falls into the scaffolding. That's probably the most obvious one that people are referring to. But we're also worried about. But we're also worried about, just in the process of doing research, that methodological choices that we take, analytical choices, dissemination choices that promote inaccurate ideas that make harmful. And so, and a lot of this isn't like researchers having any kind of malintent. It's just that these series of choices lead to harmful research. And unfortunately, for the genetics field, a lot of the stuff that gets published ends up. That gets published ends up on nasty corners of the web like 4chan, where people are like, oh, you see, there are biological races, and you know, it's no wonder that blah, blah, blah, blah, blah. So a tragic case of that was the buffalo shooting back in 2022. And the gunman, the murderer in this case, had previously posted a creed online. Previously, he posted a creed online where he cited a bunch of genetics research, maybe even some, authored by some of you, in justification of his racist beliefs. So the authors of that research didn't have any kind of man of text. They were just stumbling into these same kind of issues that we've been struggling with. And the challenge for thinking about this type of harm is it's really. Palm is it's really hard to know. It's really, really hard to know how to memorize it. So there's various stages of the research process. As Staphne was highlighting, like you know, some of us are actually recruiting research participants and going through a research ethics committee like an IRB in the US. But in many cases, that's not happening. Instead, like people who will access the e-mash data, you're going through data. E-match data, you're going through data access requests to get at the data. And so maybe there are all these various different leverage points that we could think about to sort of try and tighten the net of some of these potential for group arms. One of them might be these data custodians and data access committees. But unfortunately, they don't want to have any role here. These bodies see themselves as checking: are you a bona fiduity researcher? Is it within the terms of the Is it within the terms of the consent that the participants are? So there are some things if you're a better resource DAC that you could think about doing, including flagging access requests that need closer scrutiny, either by users, research initiators that is, the DAC themselves, or the public, maybe. You could require some sort of training. Anybody who's had to access all of us has probably been through that training. And you can require a hearing. Training, and you can require adherence to publication policies. We're thinking about ways that we could do a lightweight require training, some sort of guidelines for use for our data, but it's not going to have much enforcement. So this ends up being the picture. While we've got various things in place for us as a network, we're not very empowered to do anything once the data is posted for sharing that might help with these commands beyond some kind of. Beyond some kind of guidelines for use. So let me close with saying with what an ideal would look like, and here a lot of synergies of Daphne's talk. So when I pose to you, like, what counts as sensitive data, like, should we be the ones deciding, this is an example, or like, who should be the ones deciding what counts as sensitive data? And I think the answer is that we need that sort of deliberative democracy. That sort of deliberative democracy, community participation to help guide us in that process. So I think this is a really powerful thought that Barbara Koenig and others have kind of articulated, which is that, you know, the likes of myself, a lot of what we end up doing is putting more emphasis, more words into being conformed to consent document, and that's not the way to go. Like, that's just not the way to go for all the obvious reasons. And instead of doing that, we should. And instead of doing that, we should consent to be about giving up control to accept a set of procedures for your data to then be governed. It's consent to be governed. So the biggest need here is for robust data governance strategies guided by the right stakeholders, resourcing the staff with the right issues planned for their attention. So this is the more aspirational piece that I at least have been convinced by through these. I at least have been convinced by through these on-the-ground experiences. So I'll stop there and I'll thank some of these collaborators at eMERGE, including Meyer, Malia Fullerton, Jodie Jackson, and various other co-conspirators, and my primary mentor, Robert Green, and my funding, which is from the K-95 HDL. Thanks very much. For the interest of time, we're going to yourself having.