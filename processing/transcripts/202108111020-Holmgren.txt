Hello, today I will talk about percolation on random split trees. And this is joint work with Gabriel Bersunza, who is now at the University of Liverpool. And to the right, you see a picture of Gabriel and me. And to the left, you see a picture of a split tree where we perform bond procolation by cutting away some of the tree. Cutting away some of the edges with some probability. So we start with an arbitrary random split tree and then we remove each edge with some probability. The probability we choose is small, it's about C divided by log M. So we cut away. We cut away edges with this probability. So then we see that some edges disappear. And then we organize clusters by size. And in the previous study, we analyzed the size of the largest cluster. This happens to be the same as the root cluster after performing bond percolation. We showed that the size of the larger cluster, that is the root cluster. Cluster, that is the root cluster, is guiant of the same order as the whole tree, that is of order M, and that after normalization it converges to the Lauria-Delbruck distribution, which is a weakly one-stable distribution. And the aim of the present study is to find the asymptotic distribution of the next largest cluster. And you see in this figure, this is the largest cluster, and this is the second largest, and then we have the remaining clusters. Then we have the remaining clusters. And split trees, they constitute a large class of random trees of logarithmic height, defined by Luc DeRoy at McGill University in 98. And they often use as models for tree data structures or sorting algorithms, for example, quick sort, which can be depicted as the binary search tree. And the binary search tree is a well-known. The binary serve tree is a well-known and simple example of a split tree. So we start with a set of n-ordered numbers, which we call keys, for example the set 1 to 10, and we place them all in the root. And we draw one number at random, for example, six, and store it at the root. And then we send the remaining numbers, we send the smaller ones to the Numbers, we send the smaller ones to the left child and the larger one to the right child. So one to five are sent to the left, and seven to ten are sent to the right. And then we compared the other keys in each subtree and again divide the keys into two groups in each subtree. So when we draw number three, we send smaller keys to the left, one to two. 1 to 2, and the logic is 4 to 5 to the right. And here we draw number 8, and we send 7 to the left, but 9 to 10 to the right. And then we continue in each sub-tree, sending smaller keys to the left and larger keys to the right, until each sub-tree holds exactly one key. So then we have a tree where each node holds exactly one key. And the class of split trees, they include many. Split trees they include many important random trees of logarithmic height, for example, binary sort trees, M resort trees, quad trees, median of 2K plus 1 trees, simplex trees and tries. And this figure shows a theory and a poor result tree. And the difference to binary search tree is that there are more than one key. So here there are two keys at most in the nodes, and here there are three keys at most in the nodes. And they are part of the class of Emmary. And they are part of the class of M research trees. And there is a more probabilistic way to view the binary search tree. So you remember that the rank of the root P is equally likely to be 1 to M. So it was equally likely that we draw any of the number 1 to 10 before we draw number 6. We can think of this as we have an interval of length m and then we make uniform splits of that interval. Make uniform splits of that interval. So then we send the left part of the interval, that is the size that are sent to the left child of the route, the sub-tree route at the left child of the route. We have about n times u1 keys that are sent to this sub-tree, that is the size of this subtree, where u1 is a uniform. Is a uniform random variable. And also the right subtree is also distributed as n times uniform random variable in the same way. And then we can continue like this. Now we have an interval of length n times u1 and we make a new uniform split. And this means that the leftmost grandchild of the row, that is this node, it has m times u1. M times U1 times U2 about this number of feasts are sent this subtree route at this node. And in general, if we look at nodes at depth k, distance, that is a distance k from the root, then the number of trees in these out trees are distributed as about m About m times a product of a k independent uniform random variables. And flip trees have a quite an advanced definition, but I will give you at least an intuition how they are constructed. So they were introduced by Luc Deroy in 1998, and we start. We start with M balls. These are depicted as the blue circles in this figure. And we want to distribute them to the nodes in an infinite b area tree where each node has exactly B children. And then the split tree is a sub-tree of this. Is a subtree of this infinite B are tree. And B is called a branch factor, and the number of balls in total, that is called the cadenality. And then there is a node capacity, which means that each node can hold at most S volts. So in this figure, S is equal to 4, that is the maximum number of volts a node can hold, and B is equal to 3. And B is equal to three. There are at most three children of each node. And then there's also an internal node capacity. And this is the number of balls all internal nodes can hold. These are the nodes that are not the leaves. So for example, this node. And this feature S0 is equal to 2. So this is the maximum number of balls that all internal nodes can hold. And the most important parameter of the trees is the random split vector V. And this is a random vector where the components are probabilities, sum to one. And in the binary sort tree, there are two components and they are uniformly distributed, as we discussed before. But in general split trees, these components could have arbitrary distributed. These components could have arbitrary distributions, and this is why split trees are very general because we can vary these different parameters and get different types of split trees. And there is an important property of split trees that most walls or nodes are close to a depth which is a constant times log m. And this constant we call mu to Mu to the power of minus one, and this constant occurs a lot when one works on split trees or in the properties of split trees. And for the binary source tree, mu to the power of minus one is equal to two. So most nodes in the binary source are close to a depth to log n from the root. And Luc Froy proved that the last ball converges after normalization to a normal distribution. And I could then prove that also for almost all other balls, depth also contains a central limit law. Law. And then it follows that the depth of a random ball or node in a split tree also obtains a central limit law. And similarly, as in the binary sort tree, an important property of split tree is that we can describe the sub-tree sizes in the nodes. So at least when the sub-tresizes are long. When the sub-tree sizes are large enough. So, when we started in the whole tree, with the whole tree has n balls in total. And then, if we want to look at the number of balls in the sub-tree rooted at, for example, this red node, it's about m times v1, where v1 is distributed as the components. As the components in the split tree. And in general, if we look at the number of walls in a subtree rooted at the node at depth k, that is distance k from the root. Root, it has a subtree size, that is the total number of walls in this subtree, is about n times a product. A product of k independent random variables that are distributed as the components in the split tree. And you remember that this we had this description also for the binary source tree, then the components were uniformly distributed, but for other split trees, we could have other types of distribution. And this holds at least as long. And this holds at least as long as the sub resizes are not too large. That is, we have a distance K that is not too far from the roof. There are two ways of measuring the cluster sizes after performing bond percolation on split tree. Percolation on split tree. So, first, cluster sizes can be measured in terms of the number of balls, these are the blue ones, but they could also be measured in terms of the number of nodes. And in our works, we consider both cases. But in this talk, I will just give results for the number of balls and not for the number of notes. And we remember that when we look at portlation, we considered a regime which is called a supercritical regime. Which is called a supercritical regime, where one minus Pn is small. It's C divided by log n here. And then we look, let T n be an arbitrary split tree with this, with the split vector V. And then one often uses renewable theory when we look at split tree. We also assume here that L and V minus L and V one is non-lax, but we also consider. Lattice, but we also consider the lattice case as well. So we have a theorem for the theorem for that too. But here we in this result, we just consider the non-lattice case, which is the most common case as well. So the lattice case is not as common. And then we let C0m be the number of balls of the root cluster. And then we prove that after normalizing by m, it converges to a constant. So, this shows that the largest cluster is guy, and it occupies the same proportion as the whole tree, as the size of the whole tree. And then we look at the sizes of the number of balls of remaining clusters ranked in decreasing order, and then we normalize the log M divided by M. log m divided by m and then we prove that that this vector of other cluster sizes it converges to x1 xi where these are the actions of a Poisson process with this intensity here so this means that the smaller cluster sizes are of size and divided by log n instead of order n as the the largest size Largest size and the convergence in probability of the root cluster that is the largest one. We proved that in our earlier work. And the main result there was actually to look at the fluctuations of the largest cluster and they were described by weakly one stable distribution. And so this was the root cluster, which is the largest cluster. Largest cluster. And the idea for the convergence of the root cluster is simple. So C0m is the number of balls of the root cluster. And to show that it converges, what we divide by n it converges to a constant, we apply a crucial observation obtained by Batwan for root decrease in general. We let L1M be the depth of a random ball. Random ball. By previous results on depths by Luc DeRoy and me, we have this depth is a constant times log m, the depth of a random ball or node. And then we consider this regime and we note that after Taylor expansion, Pn is about this. And then we can write the probability that the random node belongs to the root cluster in two ways. root cluster in two ways either as the number of balls of the root cluster divided by all balls or the probability that a random node belongs to the root cluster which is this probability pn to the power l1m and this is easy to calculate because of this so we just get this result as you see and by subtree in the split tree we mean a smaller We mean a smaller tree rotated at some node V together with all the senders of V, and we note that such a sub-tree is also a split tree by itself if we just consider that sub-tree. So the grey sub-tree in this figure are smaller split trees in the whole split tree. And look at the convergence of the next largest clusters. We study sizes of procolated sub-trees that are close to the root. And then we show that the largest proclives are substantiated. We show that the largest percolation clusters in the whole tree can be found amongst those early percolated subtrees. We let EIM be the edge with the smallest depth distance to the road that has been remote, and then we let VIM be the node which is furthest away from the road of of the big tree. Root of the big tree. So I will show you in this figure here. So here we remove some nodes with the probability 1 minus pm. So this one is closest, it's the first one we remove, which is closest to the root. And then we look at this subtree. And then this is V1M, this node here. And then we let CIM be the sub-tree rooted at the With the subtree rooted at VIM. So this is subtree here, as you see. And then we look at the other subtree rooted at 2 and 3, like this, for example. And then we let MIM be the number of balls stored in the subtree TIM. So we can define subtree TIM like this. So this was just an example in this figure. And you can see that they can also intersect with each other. So 3N is So P3N is inside P1M here. And we are interested in the subject PIM, such that for P belonging to 0 to infinity, the size fulfills that it's larger or equal to M divided by T log M. We look at these subtrees and then we count these sub-trees like this, N and Subtries like this N and T for a number of subtrees TIM that hold at least n divided by T log N balls. And then the following theorem is the main ingredients to prove the main result that we stated earlier concerning the distribution of the next largest clusters of split tree. And this proves that this N and T converges to a Poisson process with intensity, this intensity with this constant C times mu minus one. C times mu minus one. And you remember that mu minus one was a constant occurring a lot in split trees. And the proof of this theorem uses the following proposition that provides a law of large numbers for the number of sub-trees in a strict tree which hold at least m divided by t log n balls. So we define m and t as the number of subtrees that holds at least m divided by t log n balls. At least m divided by t log n balls. And then the following proposition is rather technical to show, but it shows that m and t divided by log n cover is to this constant. And for the vertex V, that is not the root, we let Ev be the edge that connects V with this parent. And then we define this event. The edge EV has been removed after percolation. And then we write, we look at these. And then we write, we look at this indicator function. So these are just Bernoulli random variables with parameter one minus Pm, the probability of removing an edge. And then it's clear that N and T can be rewritten as the sum which is a product of these two indicators, the indicator that a node holds more than n divided by t log n holds, and this indicator G v. C D. And then we note that these two indicator functions are independent. And we recall that M and T count the number of nodes, the number of subtrees that hold more than n divided by t log n walls. And then conditioning on all subtree sizes in the split tree, we get that n and t is equal to in distribution to a binomial random. To a binomial random variable with parameter m and t and 1 minus pn. And then we also have that n and t minus n and s is equal to this binomial random variable. And then we recall that we consider this regime, the hypocritical regime. And from the position, we have that m and t divided by log m coordinates to mu minus 1 times. mu minus one times t. And then it follows that this converges through this and also that we have this convergence here in probability. The convergence of n and t is then proved by the fact that a binomial random variable converges to proof of some random variable. So this is by D to prove that proposition. Proposition. And then we have this proposition that shows that m and t divided by log m converges to mu minus one times t. The proof consists of two parts. We show that for everything we have that the expected value of m and t divided by log m converges to mu minus one times t. And then we prove the second part. And then we prove the second part that the variance divided by log squared converges to zero. So we recall that M and T was counting all subtrees that hold more than M divided by T log N walls. And we first showed this convergence of the expected value. But then remember that the sub-precises are well approximated by products with Products with n and the components of the split vector, as long as the sub-free sizes are large enough, that is, the depth dv is small enough. And instead of m and t, we consider these nodes instead. We consider all nodes so that the product is larger than m divided by t log n. So it's almost m and t, but now we have changed mv, the real subtle. Mv, the real subtree sizes to these products instead, which is almost a subtree size, at least when we have large subtrees. And then we look at Mm for a small depth, because then the subtree size are small, are big. And then we can show that most nodes that are in this M and actually satisfy that E D is smaller than this depth. And since M n is small enough, And since Mn is small enough, the circle sizes for V with depth smaller than Mm are well approximated by these products. And this means that Mn half V is a good approximation of MNT. And the good thing with MNT, which were just these had these products here, we counted the nodes so that the products are larger than this, is that we can apply an over. Is that we can apply an earlier result that I have shown, which uses renewal theory in relation to decrease. And then we can easily find the expected value of m and hat t. And then by renewal theory, we can prove the result that the expected value of m and hat t divided by log m converges to this mu minus one times t. And the second part of the proof, the The second part of the proof then is then to show that for each t we have as n tends to infinity that the variance of m and t divided by log n squared converges to zero. And the idea of the proof is to consider subtrees Ti that are rooted at a depth that is quite close to the root. And for subtrees Ti rooted at the same depth, given their size, we note that they are all independently trees. So the sub trees rooted at these depths are independent from each other. Depths are independent from each other, and then we look at a rather small depth for some very small constant C. And then we look at the Pi, to be the sub trees rotated at depth, Tm. And given their sizes, the Ti's are independent split trees. And then we will apply the well-known conditional variance formula, which is given here. Which is given here. And then, since we have an independent split tree, we can easily count the conditional variance of this when we condition on all the subtree sizes rooted at. So, this is easily calculated. So, this is the ID of the proof. And then we also have to count this as well. But this is another part of the proof. And we study bond population on arbitrary random. Bond proculation on arbitrary random split trees with M balls. We show that for appropriate proclation regimes that depend on the canonality M, namely the so-called split, the so-called supercritical regimes, the next largest clusters is of order M divided by log M, in contrast to the unique Gaian cluster that is of size comparable to the entire tree, that is of order M. And the main result shows that in the supercritical region, the vector of the first i and x largest clusters, these are the i largest clusters of the unique ion cluster normalized by n, while log n divided by m converges to a vector which describes atoms of a Poisson process. Thank you very much. Thank you, Cecilia. Thank you, Cecilia. Any questions from the audience? I have one perhaps rather trivial question. You should know about the largest cluster size in the subcritical regime as well, right? It's kind of curiosity. What is the right order? So the subcritical regime. There are sub-crypto regime, let's say. Yeah, so there are different types of regime, and we have looked at some examples for some split trees when we have a constant as the probability, for example. And then we have other strange distributions like generalized metaglefta distributions when we look at the binary source tree, for example. So then we have a very different type of result. A very different type of results. So, this kind of interesting behavior that you studied is indeed when Pn is very close to one, one minus Pn is like C over log N. Yes, so this is where we have the supercritical region, where we have this guy and cluster, yes. Yeah, so if P is below that, there are different kind of behaviors depending on what kind of split tree is behind the scene. Tree is behind the scene and the parameters, I see. Yes, exactly. So then we have different types of results, for example, like the Mitagleft distribution when we look at the binary source tree and probably also other types of split trees. Thank you.