Thank you very much for inviting me. This is my first time in a conference of this sort, and I'm joining it quite a bit. I really don't know much about statistics. Statistics, yes, basics. But actually, I'm PhD and science just learner, no? Like, I won't have any statistician. So, okay, yeah. So, I will be talking a little bit about the work I have been doing with Nickel, Larry, and Olaf, and it's going to be trying to do. And it's going to be trying to do uncertainty quantification with something that's called inference functions. So, from a very simplistic point of view, if you don't know much about physics, so you think, well, searching new fundamental particles is a sort of background signal for me. If there is no new fundamental particle, we expect only to see some sort of background. And if there is indeed some new fundamental particle, we want to see some mixture. Like, there has to be some sort of bump that comes from the signal. And from our point, And from our point of view, it's going to be somebody give us a signal region and tell us and ask the question: is whatever is happening inside the signal region sufficiently different from whatever happens outside the signal region? That's the main question that we want to try to answer. And fundamentally, this boils down to you need to estimate the background, and then you need to check if the pump they have in the signal region, or whatever happens here, is sufficiently different from the estimated background. Estimated background. So, mathematically, we think about this as the data is coming from some mixture between a background density and a signal density. And the lambda parameter here is basically what we call the signal strength, right? So, basically, how much signal do we actually have in the data? And we do think that this is between 0 and 1. And I know yesterday there was a huge discussion about this. I can justify this later in the coffee break. Later in the coffee, right? So, in this talk, for the signal strength, what we want to do is, we are not so much going to focus on testing, but on confidence intervals. We're going to try to say, well, we want to have a confidence interval for the signal strength. Let's try to think of it as a function of the true distribution. Here, the true distribution, I mean f, the mixture distribution. And once we have that fixed function of the underlying distribution, since we don't actually have the true distribution, let's approximate it. The true distribution, let's approximate it and compute exactly the same function with this approximation of the true distribution. And then let's try to understand what's the difference between this lambda of f hat and lambda of f hat, and try to quantify how much do they differ. Okay, so there are many approaches for trying to do this. One approach is where you have some data set that potentially has a mixture, and another data set where we know that there is only background. These are actually very easy in the sense. Usually, it's very easy in the sense that when this comes from another experiment or from the simulation, you just need to fit your background there and then you just compare that fitted background to the bank that you have here. And it's very simple, but sometimes we might not trust your simulator or you might actually not have another experiment. So we are focusing on the case where you only actually have access to this sample. Like there is no you you don't have observ you don't have access to observations that come purely from the path. From the background. So, a second approach: if you only have access to one data set that has both like a background and signal, is well, let's assume actually a particular signal. Let's say that for background, we use polynomials, and for the signal, we use the Gaussian. And then we try to estimate the signal strength by doing maximum likelihood estimation. For statisticians, it's basically minimizing the KL divergence between the Convergence between the distribution of the data and the proposed mixture. That's a reasonable approach, but it suffers from some problems. Imagine that you use a very complex background, like very high-order polynomials. Then basically your background, since you are fitting both background and signal together, your background is going to start by fitting part of the signal. So even if you have some signal, the background actually might capture it, and then you will basically get that there is no signal. Degree that there is no signal. So that's basically what makes you restrict the sort of backgrounds that you can use. They cannot be super flexible because if not they capture them. And another problem is that in the case that, actually we care a lot, that there is really no signal that corresponds to lambda zero, it's an over-parameterized model in the sense that when lambda is zero, the parameters of the signal, seeing the Gaussian, that are like the mean and the variance, are unconstrained. So basically you can put the Gaussian anywhere on the space. Anywhere on the space, and the modeling is the same because lambda is zero. So, fundamentally, that translates that when you're computing the maximum likelihood estimation, this minimization or maximization that we do, we actually very badly behaved. And the asymptotics that people typically rely on for the MLE will not follow. So, the third approach, and it's another common approach that we're going to use here, is to say, well, say that if you give me the signal region, outside of the signal region, there is the signal. Signal region, there is the signal values. Well, basically, the signal is like if the signal, if it exists, must be concentrated in the signal region, and outside of it, we're going to assume that this is not there. If that's the case, when you look at the distribution of the data outside of the signal region, it's just going to be a scaled version of the upgrade density. And uh, the scaling is one minus lambda, with lambda being the signal strand. If you play a little bit with that equation, If you play a little bit with that equation, you can basically rewrite your signal strength as a function. Well, here the signal strength is a function of the mixture distribution, f and the background distribution. And what we have here in this function is like this f is just the probability that the probability that an observation that comes from the mixture falls in the signal region. Here in the denominator, we have the probability. We have the probability that an observation that comes from the background falls in the signal picture. So, basically, what we're doing here is this sort of like counting experiment in physics that you usually do, but we are doing it with infinite data, at the population level. We are doing this experiment, this comparison, with the truth distributions, with the mixture distribution and the background distribution. Well, that's very nice. We actually do have observations from the mixture distribution, so we could approximate the F here, but we do not have observations from the background distribution. From the background distribution. So we cannot approximate it. So we need another assumption, in some sense, to rewrite this in a way that doesn't depend on the background distribution. So already with the assumption that the signal vanishes outside of the signal region, what we see is that when we study not directly the distribution outside of the signal region, but the conditional distribution outside of the signal region, then what we have is that the conditional distribution of the data. We have is that the conditional distribution of the data, here is not the distribution of the data, this is the normalization factor, is actually the same as the conditional distribution of the background. So this is something very nice because it tells you that when you're studying like conditional densities outside of the signal region, then they do not depend on lambda. They do not depend on the signal strength. So at that point, you can basically take advantage of that to rewrite the functional of interest as the signal strength in our This is the signal strength in a way that only depends on the mixture distribution. So, what you're doing is: so, here we have now that this depends on the background distribution here, and you're replacing it from some background distribution that it is identified from like minimizing the distance to the conditional distribution of the data outside of the sigma region. So, in order for this to be actually valid, you need to have this assumption that you can identify You can identify the background only using the data that is outside of the state mark region. So that means that the sort of class of models that you can use for the background have to have the property of having unique extensions. If you have two backgrounds that match on the signal region, they must also match. Sorry, I have two backgrounds that do not match on the signal region. They must not match on the signal region. This is not always the case with all classes. Like, if you have very complex neural networks, you might have two neural networks that they match outside of the signal region, but they do not match inside the signal region. And that's a problem. You cannot use that. But say polynomials, that doesn't matter the order, they do have a property. Two polynomials that match on an open interval, they must match on all the real line. So like very complex polynomials are actually allowed. Okay, so it's Okay, so basically we have this. No, we have like a no, we have that the signal strength is just a function of the distribution of the mixture distribution of the data. And in order to identify the background, we need to minimize the distance to the distribution outside of the signal region. And for that, we need to choose a class of backgrounds, which will be this calligraphic pink, and some distance measure. It doesn't even need to be a distance, it can be something weaker, like the KL divergence, which is an F. Which is an F divergence. So just to recap, we have this assumption: like signal vanishes out of the signal region. Then we fit our background only using the data outside of the signal region. We extrapolate it to the signal region and we basically compare the VAMP. We compare the VAMP to the extrapolated background. And this whole process, you basically do it as a fixed function, it's called a fixed algorithm that depends on. A fixed algorithm that depends on f. So now we do not have f, we actually have observations from f. So what we do is with these observations from f, we create what's called the empirical estimate of the distribution. Basically, we take each one of the observations that we get, and we just put a weight of one over n on each one of these observations. And we compute exactly the same fixed function by using f hat. And we need to understand how far are we How far are we from actually using the true distribution? And a very nice tool for trying to understand this is the concept of the reality, right? Like we are doing this sort of perturbation, going from f to f hat. So the idea of a function of the relative is going to aid us in trying to quantify the distance. So how about the reality? It's like the derivatives, you know, but they are like functional in the sense that we want to understand, say that we are evaluating lambda at the F distribution. In lambda at the F distribution. And now we want to see how much would the value vary if we move from F to the H distribution. Like a very small step in that direction. How much does the lambda parameter vary? And one can show under the assumptions that these sort of functional derivatives called the Hama derivative can be thought of as the integral of something called the inference function over the target distribution. Target distribution, the distribution to which we are going, the H distribution. The influence function fundamentally is capturing in which way does the signal strength depend on the underlying distribution of the data. And when you take this guy and you average it over another distribution, say the H distribution, you are basically measuring how much is the target parameter, the system is the signal strength changing when I change F for H. For H. So now basically you can use this sort of results for trying to understand how much is lambda changing when I move from F to F hat. And basically you're going to get is that this is going to behave like the average of the influence function over the empirical distribution, which is just like point masses at each one of the observations. So this is going to basically reach us the empirical average of the inference functions. Of the inference functions, and then by some sort of CLT theorem, central limit theorem, you're going to get that this behaves actually like a normal random variable. So once we have this sort of like asymptotically normal result, we can try to construct this sort of asymptotically valid confidence intervals. And here, I am not telling you how to compute this function, I would use the variance, but the important thing is that this is a function of actually the inference function. function of actually the inference function. So we know exactly how to compute. This is asymptotically, this is basically going to be an asymptotically 95% confidence interval. And therefore we're using these inference functions to construct this confidence interval that quantifies the uncertainty that we have regarding where is lambda of f lambda of the true distribution. Now let's try to see how this works in a simple experiment. So let's say we take the CMS open data and then all have told me Then OLAF told me there is very little chance that something is happening there. So I took that part, I said, Well, that's my background, and I inject some signal there. And now I basically downsample this. I just like create 200 data sets with like 20,000 observations each. This is my signal region. And begin to see when we construct these sort of asymptotically valid confidence intervals, how well are they working actually in finite samples. So let's see here. So let's see here. So, this is all like target parameter, right? And what we have here on the x-axis is what's happening as we increase the background complexity. So, for instance, here I'm using Ernst 10 polynomials. I'm going from first-time polynomials of order 2 to order 45. So, that's actually quite high. Here, we have the normalized empirical bias of the estimates of the signal strength. So, it's normalized because it is the bias divided by the magnitude of the signal strength that we. Of the signal strength that we are measuring. And in this case, the signal strength in this experiment is 0.01. So 1% of the data is coming from the signal. And what we can see is that as we increase the background complexity, well, we see the bias is diminishing. We get better and better approximations, but there is this trade-off. You have less bias, but there's going to be more bias. So when we look at the coverage that the confidence intervals have, at one point, actually, we start not being able to cover it. If we start not being able to cover at the 95% level, but if we reduce ourselves to, say, like polynomials of degree 15, then actually we are getting nice coverage. These are like clock per Pearson confidence intervals of actually the coverage that we have. So you just want basically the middle of this interval to be around the red one. And to choose one of these models, a simple way of doing model selection is something I say in the econometrics literature. Is something I say in the economics literature they call Dana selection, which is that you want to choose the background model that has the best extrapolation power because you need to extrapolate to this region. So what you do is that instead of using all the data outside of the signal region for like fitting your background models, you basically reserve some of the data here, the pinkish region, for evaluating them. So then you use all the data that is outside of the pink region for fitting the background models. The background models, then you extrapolate them to the pink region and you compute basically what is your extrapolation error. And the one that has the smallest extrapolation error is the model that basically you choose for like doing all the analysis. And in some sense, basically, since at the end, when you extrapolate to this region, it makes sense to also choose models by having some sort of good extrapolation properties. So if you follow, say, with this, if we say, if you choose, if you use Like, if we say, like, if we choose, if you use the automatic model selection and we study how these asymptotically varied confidence intervals are working, we can see, say, that's something nice that they have is that as we reduce the signal strength, so say that as we go from 1% of the data is from the signal to the case when like 0.01% of the data is coming from the signal, you are having some, say, statistically like this encourage. This is actually not exactly 0.95, it's 0.97. 0.97. So, we're actually having like on average over coverage, which is not quite ideal. So, we need to reduce a little bit more like the bias. So, that's it. So, the assumptions that we use in this work is like we think that the data is coming from a mixture, that the signal is vanishing outside of the signal region, and that the background basically you can identify is only using data outside of the signal region. If you do that, then you're able to rewrite your. That, then you are able to rewrite your target parameter only as a function of the mixture distribution and use this empirical approximation of f and influence functions to construct this asymptotically valid confidence intervals. Something nice about these influence functions is that we can actually use them for further de-biasing our estimates. I mean, not only doing uncertainty quantification, as I show you, but also if you want, if you split your data with the first half and just do exactly everything. With the first calf, you just do exactly everything that we talked about. You just compute these estimates on the sigma strength. And then you use that inference function, you average it over the second data set. And that basically, under some conditions, we have like a lower bias. We are going to estimate the background a little bit better, basically. And that's it. Thank you very much. So um maybe we can take questions Maybe we can take questions for Luca first. I should check, actually. Are there any on Zoom? There's one question on Zoom for the chat. Okay, so we can move that for discussion. Okay, so any specific questions for this talk? Yeah, Bob. Oh, you mentioned at the beginning taking what we usually do, which is a ratio of profile likelihoods with comparing lambda equals zero to lambda not zero, and you sort of present. So there's an issue there that grammars and model are defined for, I forget which the name of the. The C gram is not constrained. You don't identify everything. But we know how to deal with that now, thanks to Claudia. So what can you say about that? What can you say to argue to switch away from doing that kind of standard way to do it to you to that kind of thing? You mean like by doing profiling? Yeah, so the normal profiling we do, and everything, including the Higgs discovery, wasn't I I didn't understand what advantage this method has over over that, except for this problem that we've solved and put in none. This problem that we saw in none of it. Well, mostly I'm focusing on the case where you only have one data set. So, like, you have this mixture between signal and background. So, it didn't say if you use profiling, right, and you're still doing the drawing fit over the whole data set, right? You will still suffer from having like very complex like very complex backgrounds. So, like, mostly here is how to what are the right assumptions to try to fit very complex backgrounds. To fit very complex backgrounds, we thought this basically fit in the signal. If I'm not mistaken in some of the sites that I read, indeed, when they use profiling, they should use very simple background models. When they use data-driven background discovery. For Higgs-Gamma, Gamma will use fifth-order Princeton polynomials. Exactly. Yeah, and here I'm trying to go up to like 15 or 20. So, but why is that better? I mean, the fifth-order polynomials, there were all these studies that showed that the bias was less than 20% of the statistics. I checked 20% of the statistical too. You're happy when you discover the Higgs was less data than used for the method, for example? No, no, no. Indeed, I mean, like, if for the polyomiums, I mean, they can work quite well. I mean, like, basically, they do not have much bias. But the question is, actually, you might actually be able to have even less bias if you're able to go to higher, like more complex models. Typically, the problem is that the uncertainty of estimating all these parameters kills you, just there is too much parameter. Parameters kills you, just there is too much variance. And with these inference functions, you can try to use it to reduce basically the amount of uncertainty. So I am thinking, obviously, this is just like a first step. You should basically probably continue using the profile method. But it might be interesting to think how to be like for more complex experiments like the backgrounds are actually hard and harder to estimate, what you think about trying to use much more complex background models, et cetera. I think the question is really like before that what Jack discussed is again about the question I mean what's the OBA choice of the background function and I mean we have heard from Chat there like this method of this big profiling then this is another study in this direction I think I think I mean it's nothing at the profiling but this focusing really on the question on the On the question on the optimal choice of like the order of the polynomial. I think that's not an answer by profiling. Maybe didn't understand before. Profiling doesn't give you an answer. That's right. At the time, a number of us old-timers thought five is really high. So I can't remember exactly how chose that it was. We're worried about using polynomials, right? Well, the other thing he was saying, like, what? Okay, we can discuss later. Yeah, so maybe we can take any more specific questions and then we can go into the discussion. So, you're going to get a specific question. I think you can upload your talk with the PDF4, but because not everybody is. That's a very good comment. Okay, so then. So you must have some sort of bound on how wide your signal can be with respect to your search region. Right. Basically, when you're assuming that the signal vanishes outside of the signal region, you're saying it cannot be wider than the signal region. Yes, but how wide can it be with respect to the overall search? Like, here you have the range thirty to sixty. Yeah. So To 6C. So, I mean, it must be a function of the sample size, I guess, and the smoothness of the background function. And you can have a signal that is too white with respect to your signal, with a search to region, I think. Right. Or that your bed is not better. Ah, if you're thinking when you're doing like pub hunting, right, when you're like moving, is you're doing the scanning, that's what you're telling me? Because here we are thinking of the fixed case. Because here we are thinking of the fixed case, like one instance where the signal region is fixed. So then, basically, how why can be the signal is determined by mostly what's the signal region of interest. So it can be, say, I mean experimentally, it cannot be wider than, say, like most of the signal data has to be captured by the signal region. And you can only allow, I say, like 13% of the signal data being outside of the signal. Of the signal data being outside of the sigma region. Yes, but what I'm saying here is the width of your signal is probably three standard deviation shadow rows here, looking at your plot. So if your standard deviation was, I don't know, nine, suppose, for this model to work, you probably need a very smooth background and or a very large sample. I don't know if the physics matters here, but