And um, so uh, yeah, let's get going. Um, this is actually going to be based on two papers with the um same co-authors on the mysteries of max any ESAT and separating max2and, max die-cut, and max cut. And I'm going to just highlight one result that I particularly like out of each of these papers. So, the first one, I'm going to talk about how max any ESAT has no 7/8 approximation algorithm, assuming unique games conjecture. Assuming unique games conjecture. And also, that maximum directed cut is a strictly harder problem to approximate than max cut in terms of approximation ratio. Okay. So just kind of as a quick outline, I'm going to first just go over notation. There's already been a couple talks on the topic, so I'll probably keep that relatively quick. Something that hasn't really been covered in much detail is the basic SDP, which I'll talk about for a bit. Also, Rug Avengers vary. For a bit, also Rogue Avengers theorem. And then I'm going to talk about these two topics: Max NEESAT and Max Die Cut versus Max Cut. And then I'll just conclude with just some interesting questions maybe for the Open PROM session later. So just to start out, today we're only going to talk about Boolean CSPs. So in particular, I'm going to just think of a predicate as some subset of 0, 1, to decay. And our instance is going to consist of a collection of variables. Is going to consist of a collection of variables, which we'll just call x1 for xn, as well as a collection of clauses. I mean, you can figure this as like a hypergraph of some sort, although the exact polyogy of it isn't going to be too important for this talk. And the key thing is we want to find an assignment of these variables to the Boolean domain, which satisfies as many clauses as possible, or yeah, as many clauses as possible. And we're going to just, for a particular instance, v, we're just going to call this. Particular instance phi, we're just going to call this opt-phi. And then the approximation net ratio is just for some class of algorithms. We look at the performance of each algorithm on a particular instance, and we have a maximum quantity, which we seek to optimize. The maximum performance of any particular algorithm. Sorry, we look at the worst instance for any particular algorithm, and you pick the best algorithm out of those. So you can just think. out of those. So you can just think of our class of algorithms today as polynomial time. Of course we're going to just make the standard assumption p does not equal to np. Also just assume unique games for good measure for today's talk. An important question that's been studied a lot is how do you compute these approximation ratios? And I'll soon show that Ragavenra has perhaps the best answer known to date for this, although there are still things to explore. There are still things to explore. Let me first, though, define just a few of the main players for today's talk. So, the set predicate is going to be just taking the OR of K clauses. Any ESAT, you, well, don't allow them to all be equal. And then we're also going to look at AND of K variables, die cut, where I take Take the and of two variables, but one of them is negated. And then you have max cut, which is just basically any E2. I will say something I'm sweeping under the rug is some of these problems, you typically allow negated literals, while others you don't like. For max cut, you don't allow negated literals usually, whereas for max set, you have to. I'm going to sweep most of that under the rug unless it becomes particularly important. So for the purpose. So, for the just a few of these approximation ratios, they've been computed for over the last few decades. For instance, for MaxSAT, there hasn't been much progress on those ratios since the early 2000s. NEESAT looks very similar picture to SAT before our work. And then for the two CSPs, two and die-cut and cut, there was Cut and cut. There was a lot of progress on the hardness side, particularly in light of the unique games conjecture. I guess also the majority of stabilis for many other improvements in the area. But that also stalled out around 2010. So, what we provide is some of the first improvements, both on the upper bound and the lower bound, both in terms of upper bounds, in terms of better. In terms of better, I guess, hardness proofs, as well as new algorithms for some of these problems. I will say in particular, 2AN versus die cut is a little subtle here. For 2AN, you do allow negations. In DieCut, you don't allow negations. And what we've effectively done with our new upper and lower bounds is strictly separated them. Before it wasn't known. Before it wasn't known if the ratios were different. Actually, Perostrin in 2010 had conjectured that they're equal, but it turns out this being able to set negations as a crucial difference. And then also, we also separate max die-cut and max cut, which were conjectured to be equal, but people hadn't made progress on that for a long time. Okay. Any questions on these results? Any questions on these results? Okay. That case, let me then just go into discussing kind of the main technical tool, which is the basic SDP. So, I'm not going to give a full rigorous definition of the basic SDP in this talk. Instead, I'm just going to tell you a few descriptive properties about it, which are enough to Properties about it, which are enough to hopefully follow along with the main arguments. So, if I give you an instance, say on n variables, I'm going to be computing through the basic SDP some vectors, say, V naught and V1 for Vn, such that V naught corresponds to what we're going to call false. So, in particular, if another vector equals that, then it's just deterministically going to be set to zero. And then otherwise, these v1 for v. Otherwise, these v1 for vn represent just vectors for each variable. And the important descriptive properties of each of these vectors we're going to care about are: one, the biases. So this is just the dot product with this false vector, as well as the pairwise biases, where you take the dot products with each other. And then for each clause, you have some linear programming constraints, which make sure that these biases and parallel. Which makes sure that these biases and pairwise biases are consistent with the assignments. Sorry, the biases are consistent with some distribution on the clause. And overall, you seek to find some choices of all these parameters, which maximizes the value, the fraction of clauses which are satisfied, which we'll just call the SDP value of your instance. And then, what we'll just call the integrality ratio is the smallest ratio of the optimal, the true integral value to the SDP value, because SDP value is always at least the integral value, but it can be much higher. Okay, so an important, obviously, this is the most important question you can ask about the basic SDP. And then, as has already been mentioned like twice this morning, the breakthrough feature. This morning, the breakthrough theorem of Rogavendra showed that, assuming you need Games conjecture, this approximation ratio for every max ESP is also equal to its SDP integrality ratio. So in particular, we don't need to think about a general class of algorithms. Rather, we can just think of almost a purely geometric object, which is this SDP integrality problem. There was also a refinement roughly a year later, which Roughly a year later, which showed that you can actually compute these ratios to within epsilon precision, within like roughly double exponential m1 over epsilon time. So you can kind of at this point wonder, like, are approximate max CSPs basically a solved problem? And I'd probably argue that's definitely not the case. And one argument is just purely practicality, which is like, Purely practicality, which is like, oh, I want to compute the ratio of some problem. How do I even compute like the second decimal digit of that ratio in like a reasonable amount of time? But there's also kind of something a bit more subtle, which is it doesn't immediately tell you if you have equality. So, for instance, if MaxSet say has a seven-eighths approximation algorithm, we don't necessarily have an algorithm we can run right now. Don't necessarily have an algorithm we can run right now, which will verify that. Nor, if I just have two CSPs which look maybe a bit different, how can we tell if they have the exact same approximation ratio or not? We need, basically, we need to understand more about how these integrality gaps work in order to have a clearer picture of the landscape. And this is what the works with my co-authors have been seeking to do. Okay. Any questions about like SDPs and kind of related topic? All right. So let's then jump into talking about the NEESAP portion of the talk. So let me just kind of state a little more precisely what our main result is. Main result is, which is assuming you need Games conjecture, the approximation ratio of max, so three, five, and ESAT. So this means I only let you have clauses of size three or of size five. Then your ratio is at most some funny algebraic number is at most, which is a little bit less than seven eighths. And I guess this is a bit more philosophical, but this. I guess this is a bit more philosophical, but this feels like the first hard evidence that MaxSat doesn't have a sub-n's approximation algorithm. It's also the first upper bound on this problem that doesn't arrive from some chain of reductions from, let's say, Hastad's result for 3Lin. But the proof that this doesn't work is there doesn't seem to be any simple way of taking this proof and turning it into any. Way of taking this proof and turning it into anything from accent. Okay, so to get into discussing the proof of this result, I need to just tell you a little bit more about SDP rounding schemes. So Rogavento's theorem says that the SDP integrality ratio equals the approximation ratio. It actually proves something slightly smaller, sorry, slightly stronger, which is that a particular class of roundings. That a particular class of rounding schemes is sufficient to, up to any epsilon, achieve the desired accuracy. But we don't need the full description of that. Instead, we just need the following property, which is when I take an SDP rounding scheme, so in this case, what I just mean is abstractly something that takes a collection of vectors from the basic SDP and then outputs a distribution of assignments. All that. All that matters is the correlation matrices. So, in particular, let me say this: the following. So, if I have my rounding function call it psi, and I look at two variables, say x1 and x2, or I guess the vectors v1, v2 becoming assignments to variables x1 and x2. The distribution of Of those two variables in the rounding only depends on their dot product. So, in particular, if two other vectors also have that same dot product, they're going to have an identical distribution. So, we can capture this by looking at moment functions. So, for instance, here, we can define a second moment function, which just looks at the stop product and outputs the expected value of, say, their XOR. Something I'm sweeping under the rug here. Something I'm sweeping under the rug here specifically for any ESAT is I'm not looking at these individual biases, so like the dot products with the V naught vector. It turns out you can, without loss of generality, always assume those are zero in this case. So I'm just going to keep notation simple and just talk about the pairwise product of two vectors. And we'll also need to look at the moments of four vectors, in which case Of four vectors, in which case it's a little more complicated because you have six possible dot products between the vectors. Although, the good thing is in our case, when we look at this, we'll only really care about when all those six values are the same. So we can just have some short notation, which we'll just call F4 of B. This is probably the most conceptually off the beat and track part of the talk so far. Are there any questions about this notation? Any questions about this notation? All right, then I'll continue. So let's now discuss how you start building the integrality gap. I'm not going to describe the full instance in the talk, but I will talk about the main ingredients of it. Like the main ingredients of it. So, in particular, let's first look at the followings. Let's start with any E3 set. So, if I look at any E3 set, there's the following three satisfying assignments where I just assign one variable zero, and then the other two variables one, and I just, which one I assign zero is picked uniformly at random. You can do this, and what will happen is because of this satisfaction. Is because of this satisfying assignment, you can always find an SDP solution where the vectors have this local configuration. So each pair of vectors will have a dot product of minus one-third, except obviously with itself. Oh, I should have also mentioned the SDP vectors. We're going to assume they're all unit vectors in this convention. For any five set, we're going to do something very Any five set, we're going to do something very similar where we look at again four thing values are set to one, one is set to zero, except this time the one that's set to zero isn't going to be uniform. I'm going to be uniform on four of them, and then I'm going to have twice as much probability on the fifth. And the reason why we do this is then the correlation matrices of this particular distribution look as follows. You get a nice Distribution look as follows. You get a nice all one-thirds, and then you get this last vector, which is orthogonal to everything else. And this will make it very easy to do the analysis, at least relatively speaking. Oh, is there a question? Is it just for ease of analysis? Or you think that if you put all one over fifth probability, it wouldn't work? So, well, you'll see how the nature I can answer to. I can answer that question better in a couple slides. Part of it is you could theoretically, that should be the best integrality gap for this problem. The problem is no one currently knows how to analyze it. Let me say it that way. It should be a better integrality gap, but it's not known how to analyze. So, I guess the main thing that I want to say is that if you look at these correlation matrices and then you try to compute just individually what's the integrality gap. So for a given scheme psi, it's a little bit of, just taking the Fourier expansion of NEE, you can compute that you get for on the left, you get this. Uh oops. You get three minus the three times this F2 of minus one third. And there's a simple just you negate and you'll get the same value or the negated value. So you just get three plus three F2 of one-third over four. And then on the upper one, if you compute it out, and the things of zeros will basically drop out or they'll. Will basically drop out, or they'll expectations work out nice. So you'll just get something in terms of f2 of one-third and f4 of one-third. And then writing this then as like a minimax problem, you get the following expression. And to prove this, you actually don't need to understand much about how these rounding schemes work, except for the following two facts. One is that the F2 of one-third always has to be at least zero. Of one-third always has to be at least zero. In particular, positively correlated vectors must always have a non-negative correlation in their rounding. This is not hard to prove. And then the other is that the fourth moment is always at least the second moment squared. Once you have these two facts, you can then just do a bit of, I guess, calculus and get the upper bound that I claimed earlier. And going back to the question about one-fifth, Back to the question about one-fifth, the reason why is there's no known elementary inequality for abstract rounding schemes about how the one-third correlation compares to the one-fifth. If we knew how to do something like that, we could get almost certainly a much better upper bound. And even this current analysis is not tight because the conditions that would achieve, there's no rounding scheme which actually achieves this ratio. Achieves this ratio. It's just a current limitation of our technique. Are there any questions about the NE portion? I guess I should also ask how many more minutes do I have to discuss or present? Eight minutes. Sorry, eight minutes? Yeah. Okay. So I'll just go. So, I'll just go relatively quickly through the die cut versus max cut part of the talk. So, just to contrast these two problems, so for max cut versus die cut, in directed cut, you care about which side of the cut each arrow lies on, not just that it's cut. So, for instance, on the right with max cut, you can, because it's a tree basically. Because it's a tree, basically, you can cut every edge. But on the left-hand side, because of how the edges are directed, you can only cut at most three of the edges because you say have a convention that the head has to be on the top side of the cut or something. I should also note that it's very elementary to prove that the ratio for die cut is at most the ratio for cut because you just take an. The ratio for cut because you just take an instance of max cut and replace every edge with two directed edges. And then just a little map with the approximation ratio formula shows that you can never do better on directed cut than for max cut. However, this is the first time it seems that there's an integrality gap for die cut being strictly harder than cut. So let me just explain to you the distribution. Explain to you the distribution. It's a little funny. So, here, actually, it does, you can no longer assume that these biases are zero. So, we actually do need to take into account those. So, we basically have some magic parameter, which we'll call b, which is about 0.175. And we're going to look at three configurations of vectors: one where both vectors have. One where both vectors have bias minus b, so they're both, I guess, leaning away from false, so they're more true, one where they're both plus b, so they're leaning towards false, and then one where they're disagree. So one's plus b and one minus b. And then you also need a pairwise dot product between the vectors. For two of those, you just have this value of minus one. You just have this value of minus one plus two b. There's nothing really special about that. That's just there's some triangle inequality constraints on how the vectors can be laid out in space. These are just as anti-correlated as possible. The one that's a little odd is the one where the two biases differ. You have this value C, and this is actually seems to be a very fine-tuned parameter in the proof. It's not exactly clear where that value comes from. Value comes from. In any case, you have a distribution of three different clause types, and now you want to round to analyze how good of a rounding scheme you could do on this distribution. To analyze this, what we do is we there's basically a result of a perostrin, although we had to modify it slightly for die cut because he wrote it for max2and, which is a slightly different. Max 2 and, which is a slightly different problem. But you can prove that for the distribution we're looking at, there's the following rounding procedure is optimal for your problem. Here, what you're going to do is you're going to pick some function, pick a random Gaussian, and then round based on how big your dot product with the random Gaussian is. What the random Gaussian is compared to this function, this threshold function applied to your bias. So, in particular, it's not much of a variant of something like Comens-Williamson. There's a little bit of variant to it. This is, by the way, in the literature called a thresholding. In the literature, called a thresh rounding scheme. It's a fairly simple class of rounding functions. But in this case, because of what our distribution looks like, we only have two biases, minus b and plus b. So we don't need to keep track of every possible value of the function. We only need to keep track of two values, t1 and t2. So in some sense, this is a two-dimensional optimization problem. Dimensional optimization problem, which you can formally analyze using various calculus methods. Although today I'll just give you a proof by picture, if you plot the analysis of these rounding functions on these axes, what you'll find is that there's a maximum value of 0.8746. Oh, I didn't write the value, but it happens when T1. But it happens when T1 is approximately, I guess, minus 0.3 or sorry, minus 0.2. Okay. So this is, and then this is because we found a hard instance of the problem, this is enough to certify the upper bound. Just one minute about lower bounds for this. For this paper, we also found some new algorithms for problems such as max die cut. And to do this, we actually had to design a sort of max min algorithm, which actually generated a bunch of possible rounding functions. And then we solved some giant linear programs to figure out the optimal linear combination of these. And then we had to do some other post-processing and other stuff. But the end result is we ended up getting. But the end result is we ended up getting a nice distribution of like seven rounding functions. The first rounding function, which has 99.7% of the weight, looks very similar to the previous work on this problem due to LLZ back in the early 2000s. But then there's some rather exotic functions of very small weight, which actually are the difference between separating this problem from max two and or it would be interesting to. It would be interesting to have a better theoretical understanding of what's going on. Okay. Let me just conclude with a few open questions. So probably the biggest one is, does Max Setup Sonate's approximation algorithm? I guess you can probably guess my personal guess is no, but it could definitely go either way. Another question you could ask is like, let's say we restrict to a clause size. Let's say we restrict to a clause size of two or three. Can we completely classify all the approximation ratios? Surprisingly, there's still a few cases where this hasn't been done. Like, for instance, this max die cut, we've shrunk the upper and lower bounds, but they still haven't met. And there's other problems like this. Another thing you can ask, and there'll be a talk about this, I think, by Ben Cod on Friday: is like, what about premises? Friday is like what about promise CSPs? This is also an exciting area to think about. Another thing to think about would be just going back to Rogaventer's theorem, is there like a way to simplify the class of rounding schemes that are needed to achieve optimal approximation ratios? If some better understanding of which particular rounding schemes work, perhaps this would allow for more efficient procedures for computing these integrality gaps. These integrality gaps. And that's it. Thanks for coming. So, this math draper dominance schemes, I assume like you went to this complicated thing because the simplest things don't work. Oh, yeah, yeah. Simple things don't work. So, actually, this is the first. The first example I know of of at least a two CSP where you can't just do a single like one-shot rounding function. You have to do a probability distribution of rounding functions. We even also found for 2AN that you need to do a distribution. The previous thing, which did only one shot, you can improve over it slightly, but less so than you can for die cut. Do you have any conjecture on like what, or do you think it's some different combination? Or do you think like it's some infinite combination of crazier and crazier? I personally think there's the optimal, at least under this basis of rounding functions, it's going to be infinite. There might be a different perspective where it becomes a lot clearer what's going on. Maybe related to that, I guess these rounding functions are still one-dimensional, right? So do you think maybe going to higher dimensions, you can buy the You can get by with fewer learning functions? That's a very good question. So it's actually not even known right now if one dimension is sufficient. So I kind of had a little weasel language right here. I said for the distribution I presented, it's not known if the optimal hard distribution for max die cut Austron's theorem applies. There's actually a gap. He assumes something called positivity where the Where the single biases versus the pairwise biases have a certain inequality applied to them, kind of a PSD type inequality. So it's not even clear that one-dimensional is enough, even if you allow infinitely many, much less, yeah, there might be just a two-dimensional or like maybe a, I don't know, a hundred-dimensional rounding function where you take a sample 100 Gaussians and then round. That might, that might be. Might that might be something you can only do one time, but I personally not sure that would give much clarity, but yeah. Last thing, speaker again.  