Oh yeah, on the schedule I appeared as uh the title to be announced or here I'm announcing it multi-scale exploration of single cell data with geometric harmonic analysis. Okay so we already heard a little bit about single cell data from Smita and I believe single cell data also appeared in some other talks. There are many applications that you can think of in That you can think of in biomedical data exploration, in data exploration in general. In the case of single-cell data, we get, well, data that comes from individual cells and contains either gene counts and protein counts. And we're talking about data exploration, and we can put many stories on it. But when you want to distill it to mathematical formulation, Formulation, it all boils down to two types of patterns that we can look for. Either we look for clusters, in which case, what we want is to group together similar cells in this case, let's say to get cell types or phenotypes or archetypes and so on. Or we're interested in transitions between them, in which case maybe we know in advance, or maybe we don't particularly care about the stable cell types. About the stable cell types, what we want to know is how things change from one to another to track trajectories and dynamics over time. And this relates very well to a lot of things which you saw here. Link prediction, for example, can be posed as understanding transitions. Dynamics, of course, and flows can be understood as understanding transitions. Time-evolving data can also be posed as understanding transitions and trajectories, and even long-range interactions. Long-range interactions on graphs. In some sense, we ask how do we propagate information and transition from one region to another region on a graph or in the data. Okay, so I'm going to focus more on this idea of how do you understand transitions and how they assemble together to a global view or global shape of the data that we see. Now, we question Question that was asked several times during this workshop is: What is the dimensionality of the data? And well, for gene counts, you usually see 20, 30,000 dimensions. For protein counts, we see 40, 50 dimensions usually. Maybe it can go up to 100 depending on technology. These are not terribly immensely big dimensions. Immensely big dimensions when we compare to audio, video, and images. And yet, it's already enough to see all the problems of the curse of dimensionality. Everything is far from everything. Normal distributions tend to look like soap bubbles and various other artifacts where you can't really trust anything that has to do with the Euclidean space. With the Euclidean space, with the Euclidean ambient space over these features. But then we can ask if this is the real dimensionality of the data. Or in more accurate terms here, is cellular development really a high-dimensional process? We have intuition on images and audio, on natural language. On natural language, that actually it's not very high-dimensional. If you take a random organization of pixels, you'll see just noise, you will never see a face. If you take a random sequence of fluoroids, there's no chance you'll end up with an intelligible sentence. If you take the random values in a sequence, you will not get music or any kind of intelligible audio. In fact, in the case of cells, we have a more pointed observation. More pointed observation that doesn't come from what random organizations will not give you information, but actually we know how cells develop. We know how they transition from one state to another. They do it by small mutations that are incremental and that aggregate over time. And each of these mutations has very few degrees of freedom. Of freedom, and these two observations relate directly to what we call a data manifold in manifold learning. Now, for proving math, it makes sense to say Riemannian manifolds, compact spaces, and various other fancy words. But what we actually mean in data manifolds is not any of the things because they are completely unrealistic when you work on real data. What we actually mean when we say manifold in manifold learning. We say manifold in manifold learning and in any kind of geometric deep learning, etc., what we mean is that the data can be modeled as a collection of smoothly varying, locally low-dimensional data patches. Okay. It just so happened that Riemannian manifolds, especially compact ones with no boundary conditions, also fit this description. Okay, but what we mean in data is the thing. So this means. Data, if you think. So, this means that single-cell data fit especially well to the assumptions of manifold learning, and therefore, manifold learning methods are especially effective on this data. And we saw several examples of that already. Okay, now this means that if we see data, we can work locally with local neighborhoods. With local neighborhoods that we can roughly trust in the sense that if we take 10 nearest neighbors, even though the Euclidean metric is terrible in high dimensions, if we take our 10 nearest neighbors, we're very likely going to find cells that are similar to the original one. It's unlikely that we will find two cells. That we will find two cells that are very different and they will end up being close together. It's way more likely that cells that should be close together, that are similar, will somehow be pushed to be far away because of the high dimensionality. Okay? Therefore, we can trust very local neighborhoods. The problem is that very local neighborhoods might not go even beyond the level of noise that we see in data, and therefore, we can. See in data. And therefore, we have to aggregate them together to get these non-linear connections, these long-range connections in the data. We saw in several of the talks here things that rely on, or methods that rely on this idea of diffusion to propagate and to get long-range interactions. It's convenient somehow to think of this as geodesic distances or as pathways, okay, if you have reachability between two nodes on a graph. Between two nodes on a graph, or between two points in the data through local jumps, then okay, if you make if you take t steps, you'll have some information propagating from one to another. However, what we have here is actually a little bit more intricate than that. Because you see, geodesic distances are very convenient on uniformly sampled manifolds. They're very inconvenient in real data. Very inconvenient in real data because real data is not uniformly sampled and it is very rarely on a fixed constant dimension manifold. This is part of why topological data analysis is becoming more and more popular, because you don't have one manifold that is a true manifold with a true intrinsic dimension that goes everywhere. Okay? You have different resolutions, you have different dimensionalities, you have clusters, and you have transitions between them. And just the shortest path between And just the shortest path between two points doesn't allow you to distinguish whether this path goes through a cluster that just has a lot of variability because of noise in various dimensions, because we are talking about high-dimensional data, or whether it goes through a certain transition that is sparser and that signifies some change in the data or some dynamics. In order to In order to identify these kinds of things, we have to consider multiple pathways and consider connectivity. So it's not so much the distance or the reachability, the connectivity between data regions that we want to really identify in a quantitative way. Not just are they connected or not, or how many steps. Diffusion geometry allows us to do these kinds of things by things by modeling data as distributions. Each data point now goes from the perspective of having whatever original features to having a distribution of where exactly can you reach from this point of origin and how much connectivity you have there. So a connectivity A convenient analogy or intuition here. If you put a unit of gas on the point, I cannot point here. If you put a unit of gas on this point here and you let it propagate, you will mostly stay in the same cluster. Some of it will, of the gas, will look to the other cluster, but not much. If you take another point from the same cluster, you see similar distributions because you have a lot of connectivity. Distributions because you have a lot of connectivity. You have a big overlap. If you take a unit of gas in the other cluster, it will mostly be concentrated there. A little bit will leak through the transition. You see some connection, some similarity, but less than within the same cluster. And of course, if things are completely disconnected, you will not see any gas leaking between the regions. This idea of the overlap between The overlap between diffusion representations of the points can be quantified as a metric. Okay. And this metric space can be used to build a geometry. Now, because we're talking about points having associated distributions with them, now we're equipped with two items or two elements for each point. We're equipped with the geometry. Geometry that comes from whichever manifold assumption we put in the data, Riemannian manifolds or otherwise, and also a distribution space. This kind of dual setting is exactly the setting where statistical manifolds are defined and where information geometry is used. And in fact, when we look at the two, I'm slightly biased for the second one. I'm slightly biased for the second one, but the two popular ways of quantifying this kind of overlap between diffusion representations and distilling it to a metric space. The first one was presented years and years and years ago by Kohipman and Lafon as part of diffusion maps. It's an L to distance between diffusion distributions. The second one was introduced more recently by certain By certain, I'm slightly biased, but by people from Yale, like Smita, Kevin Moon, who's now at Utah State University. They were gracious enough to put my name on the paper just for contributing a few things. This one is, we called it potential distance because it can be related to Boltzmann potentials. To Boltzmann potentials of the underlying diffusion process, both of these distances can actually be combined together as two extremes of a whole family of information geometry distances based on this notion of diffusion between data points. You can parameterize them by this parameter gamma that you see there. And in fact, at gamma equals zero, you actually get. Zero, you actually get Hellinger distances between distributions. Okay, so this builds a certain understanding and a certain representation of the underlying intrinsic geometry of the data. Now there's a question of, okay, what can we do with this kind of geometry? Okay. And the first thing we can think of doing is visualizing data. We can talk about descriptive methods. Talk about descriptive method and predictive methods and all sorts of representation learning tasks. But at the end of the day, if we want to explore data, we need to see the data. We need to be able to manipulate the data. If you will, we need a certain microscope that allows us to see the patterns and the transitions in the data. And this is what we provide with FAIT, where Where we take our original data in very high dimensions, we build distances from it to identify neighborhoods. For the sake of argument, here we can take Euclidean distances because we just want k-nuous neighbors. But we can use many other things. Smith talked about using Wasserstein distances or by extension MMD and so on between distributions. We can use proximity that comes from random. We can use proximity that comes from random forests in order to guide the representation towards supervised patterns. We can take data that is not at all Euclidean and represent many other similarities or dissimilarities there. In fact, we don't necessarily need to have distance metrics in order to have this second step of building neighborhoods, because what we actually do with the distances is we want to have affinity. Distances is we want to have affinities. All we care about is how to construct a graph, a weighted graph of neighbors. And then from these neighbors, we take diffusion probabilities, we propagate over the data, and from that point to really embed things, informational distances, and let's say multi-dimensional scaling to get coordinates. Now, at this point, it's important to say one thing. Okay, this whole This whole process has some nitty-gritty detail that we diffuse over a certain amount of time steps. This is expressed in graph neural networks as the number of layers that you take over the graph. In the case of geometric scattering and graph wavelets, these are the scales that you look at for the graph. In the case of diffusion, In the case of diffusion geometries, it's the diffusion time. Okay, supposedly, it gives you a scale that you can play around with. Practically speaking, what we find is that if we look at the way the rank of the diffusion probability matrix, the transition matrix, decays, and as a proxy for it, also the spectral entropy, okay, of how many dominant eigenvectors can you actually Vectors can you actually rely on that retain information and don't collapse? Okay, you see that within a very few steps, the spectroentropy drops significantly, which we interpret as this noising region. Okay, we drop a lot of the variability of the data and condense the data towards the manifold. Yeah, I see a question. Well, you know, it's no dimensional because of the rank, the low rank of the matrix. And what happens is that if you just take the row stochastic matrix with one step, you might not see how low rank it is because your steps might still be within the level of noise in the data. Okay, you might be traversing through cells that are Cells that are roughly the same up to noise, like dropout, for example. Smita mentioned 80-90% of transcriptomes are not collected. Okay, so maybe just different dropouts are the difference between two cells. Okay, you have to take several steps in order to get out of this region of noise and actually see real transitions in the data rather than stay within the same cluster. This is similar to the reachability problem that you sometimes see in graph neural networks. You need to get out of a certain region to actually find the boundary between classes. But once you go there in graph neural networks, there are a bunch of other issues like over smoothing, over squashing, and so on. There's a certain balance between underreaching and oversmoothing, as Frederick mentioned in his talk. Mentioned in his talk. But what we find here is that we can actually identify where we move from this region of denoising and collapsing information very rapidly to region where we slowly decay. Now we have to get at some point, this is the diffusion process is an ergodic process. So this means we have a unique stationary distribution by construction, and we have to eventually get to rank one. Okay? Okay, that's just a mathematical fact. Okay, so the decay from the slow decay that we see after the denoising, that's just us losing signal and in some sense cross-graining the data, not necessarily very effectively. Okay, so we can actually find an optimal time using an elbow or knee. An elbow or knee method around this region here, and have a heuristic part to choose to automatically choose the diffusion time. It's like an out of focus, if you will. Now, I think Smith mentioned this data set that we worked on on embryoid bodies developing from the stem cell level to the progenitor cells over the course. Genitor cells over the course of four weeks. If you look at this data with PCA and the colors here, they denote course time labels that you have just by knowing when a certain tube of cells was put in the machine to sequence it. The time is not part of the features that go into PCA or whichever other method I show here. And what we see that initially we don't have a lot of. That initially we don't have a lot of variants because everything is stem cells. At the end, we have a lot of variants because we have a lot of tissues that develop, neural tissues, cardiac tissues, etc. Okay. We don't see any pathways. We just see a global overview of the data. If you run tiseny, we see clusters here. And I'm sure that if you display certain markers here, you can find clusters here that are well segmented. Clusters here that are well segmented and make sense. I'm also sure that if you have the wrong label that just you do by marking by hand various regions, you can also do that. And if you run testing several times, you will have different displays. Okay, but you can't really see transitions here or infer anything about the global structure of the data. If you take fate, get this nice picture, which I believe you already saw. Okay. Okay, and you can delve, you can delve deeper into it, and you can say, okay, which cells we have in each region. And you can see stem cells in the beginning. You can see the germ layers in the middle, the mesoderm, ectoderm, urectoderm, and so on. And you can see the cardiac and neural tissues and so on. You can delve deeper and look at time. And look at time, clusters, branches, various markers that develop over them. And if you're an immunologist, at least, or whichever other experimental biologist that understands what all of these things mean, which I don't, you can even come up with something like this: identifying important regions on this structure. Or a stem cell biologist, I mean. No, a stem cell biologist. I mean, yeah, thank you for the specific profession that would look at this. And you can come up with this kind of roadmap that emanates directly from the data. So you take very high-dimensional data, you display it, you explore it, and you can find this kind of structure in it in an exploratory way. Now, there is a problem though. This is all nice and well for building geometry out of data. The thing is that you need to have all the data in at once. There are, of course, ways to do out-of-sample extension. There's the Neistrom extension, there are geometric harmonics, there are others. It's not trivial, it's difficult, it introduces certain bias, and especially in biology, there are a lot of bad effects that will prevent you from actually having a reliable extension. There's also a problem of scalability. It's not very big. It's not a real, real problem because you can do landmarking techniques and because a lot of the data manifold, they don't really, really require the amount of data that we have in order to have. That we have in order to have a rigid structure. Nevertheless, kernel methods are heavy to run. Okay, you need to know what you're doing in order to get them to not completely take all the memory on your machine. And first bullet point that I have here is actually much, much, much more crucial, I think, in my opinion. You get... You get assign coordinates, whether it's by MDS or by whichever other methods we're using, kernel method. You just assign coordinates to every cell in your original batch of data. You don't actually have a function that embeds the data this way, which means that you can never actually go back. I mean, you don't even know how to go forward for a new data point. You definitely don't know how to go back in any kind of invertible way. In any kind of invertible way, from a position on your embedding or on your data manifold to the features. Now, the features that we collect, they're important, they're informative. We choose them, we choose markers because we know what they mean. Well, not we, I don't know what they mean, but the people who designed the experiments, they know what these features mean, and that's how they characterize cell types and all sorts of things. So it'll be. Things. So it will be really useful to be able to take a flow over the data or take trajectories and be able to go back and monitor what this sequence of cells actually, what do they look like? What does it mean? So as an alternative to this, that comes from anyone who knows about deep learning, and quite a few people do know about deep learning these days, is why you don't be Is why don't we use Adoencoders? Adder encoders immediately give you an approximately invertible function as an embedding. Okay? And there are various legends that art encoders preserve geometry somehow, magically, by chance. They don't. Okay? By the way, there are also various legends that variational outencoders preserve the Preserve geometry. I'm recorded, so I would not say too much, except to say that they don't. And there is no reason from their formulation that they would preserve the geometry. If they do it, it's purely by chance. And as a mathematician, I don't like things that happen by chance and I cannot provide a mathematical explanation for them. Okay? In the case of this tipot, for example, the geometry should look like a circle because we're talking about a tipot that is rotating. We're talking about a tip that is rotating with only one degree of freedom. Okay? The embedding by a simple out encoder does not look like it. So a revised statement to this alternative is use geometrically regularized out encoders. So you can take the fate embedding, or for that matter, any kind of other manifold learning technique that gives you representations. That gives you representations, just I'm biased towards fate, and regularize the bottleneck of your autoencoder to preserve as much as possible the kind of geometry that you saw in your manifold learning technique. So, for example, if you take something very simple, you take non-intersecting toroidal helices. Toridal helices. So you have a torus, you take a helix over it, take another helix over it that doesn't intersect, not very problematic to do. You take them as a cloud of 3D points. Okay. If you run an outencoder, you get, well, you can see what you get. Definitely, you don't get them separated. And there are various attempts. There are various attempts to slap on regularizations on out encoders to preserve things in the data. Okay. I'm sorry, Bastian, your topological outer encoder does not work. We tried. So, topological outer encoders try to basically have the latent space admit the same topological description. Admit the same topological descriptors as the original space. Okay, there are other regularized outencoders here. Diffusion nets, I'm actually not being fair to them. These are outdated results. Since these results were produced, we actually did manage to get them to separate the two helices. But what you can see is that when we do the geometric regularization, We do the geometric regularization with fate and with UMAP. We do get things separated. And you see here three metrics to quantify this. Okay, so the R2 metric looks at a linear regression of basically the arc length of each of the two helices. Okay, that this corresponds to just also the angle if you think of the geometry as a circle. And you can see. And you can see that if you want to get really the kind of continuous structure, the intrinsic parameter of the helices, fate does it, UMAP doesn't. There's a reason why UMAP doesn't. I record and will not say it. But FAIT does it pretty well. Topological out encoders, in fairness, they do allow, even better than Gray with FAIT, to get. Gray with fade to get this kind of intrinsic parametrization. Separation between the two helices captured through linear classification. C gray with UMAP and fate does better than other methods. And I think it's pretty clear from the pictures why. Okay. But interestingly, when you look at the reconstruction. When you look at the reconstruction error, usually you'd expect that an outencoder that is completely geared toward reconstruction would do better than an outencoder that balances reconstruction with any other term, right? I mean, you just think of a trade-off. In fact, that's not the case. Preserving the geometry in this case helped generalization in terms of reconstruction because all those intersections. Because all those intersections that are created between the helices, they create a case that when you introduce the test set and when you try to generalize, we actually make mistakes. Now, having this kind of system of invertible representations means that then we can go back. We can go back from the embedding to the original features, which means that we can. The original features, which means that we can now take trajectories in the embedding space. There are various ways of finding geodesics on a graph constructed on the embedded space. If we take two points, we take a geodesic between them, so now we have a path and we uniformly sample it. Okay? We're not doing it here in a very smart way. I will show you in a few slides a better way of getting things uniform. A better way of getting things uniform and by augmenting the data. But when you do this and you look at the relations, if you have something that is geometrically regularized, you do much better in terms of going back to actual relations that you see between features in the data and in getting a pathway that is similar to the pathway that you can track. Pathway that you can track over the data. Now, a bonus that you get from this thing is that you can also handle scalability because usually you don't really need the whole amount of data. Usually you don't need the whole amount of data to process and capture the geometry. Okay, I think I have the wrong units there, by the way, for the time. I haven't noticed it until now. Okay, but what you can do is now you can create mini batches. You can random sampling of the data, get fade to give you whatever geometry it gets. As long as you have some overlap between things, you can align the you can align with Procrates. Aligning with Procrastis, this representation that you get. And you can train on each of these mini batches, okay, and get one single embedding function that you can then apply to your whole data, even to points that were not sampled in your random sampling. Okay, and you can get much more efficient runtime with that. Now, okay, all of this is for getting the intrinsic. All of this is forget the intrinsic features to get representation to be able to go back. At this point, I want to take one step back and say, okay, we got our diffusion probabilities, we got some understanding of our representation, and everything I said now is how do you get intrinsic, a good mapping to have low-dimensional coordinates, two-dimensional coordinates in particular. If we take a step back. If we take a step back to the actual diffusion process that we have and to the way it works on the data, we eliminate variability. We had this sharp drop where I said you kind of have to take a few steps to go beyond the level of noise. Going beyond the level of noise means you collapse points towards a certain data manifold. Okay? Smith already mentioned magic, for example. Smith already mentioned magic for imputation and denoising. This is what happens with magic, okay? We can think about it as applying a low-pass filter to the data. That's a very convenient spectral perspective. We can also think about it in the spatial domain. You're taking points and you're just pushing them, projecting them on the data manifold. And what happens with that is that if you have something like this relations between two markers, These relations between two markers that should exist but are obfuscated because of noise, because you have all of these red points that have zeros. Okay, as you run the diffusion time, this will work, as you run the diffusion time and they go back towards the manifold, you actually reduce the dimensionality of the data, the intrinsic dimensionality of the data, by creating dependencies between features, features that seem independent. That seems independent originally, but then you see that they are actually related and you create these kinds of dependencies. Well, if this happens when you start from data points, right? Now I can pose another question. What happens if I take new data points? Okay, they're not coming, they're not actual cells, they don't come from the data. I take random points in high. Points in high dimensions. Okay. And I try to project them on my diffusion geometry. Okay. Well, what happens is that over time they will start going towards the data manifold, being project on it. And then on the data manifold, they will move tangentially towards regions where we have a high concentration of points. Of points. Okay? I want to talk about how we use both of these properties: the property of projecting, of going toward the manifold, projecting a data point on it, and the property of moving tangentially. So the first thing that we can do with this observation regarding how points move in the ambient space with respect to the data geometry is Data geometry is a new way of generating new data. So, you see, generative models, beyond all the fancy words regarding them, they work on densities. They try to take data points and indirectly resample the same distribution. Not only that, when you think about what do they sample from the distribution, they sample the main modes of this distribution. Modes of this distribution. The main modes of the distribution are exactly the clusters. Okay, so most generative models work in the cluster regime. You want to avoid mode collapse because you want to find all the clusters in the data. And as long as you capture most of the modes and sample from them, the distribution will look pretty much the same. And then you'll be happy with your generative model. Generative model. What you miss is the connection between the transitions between clusters. And in a lot of things in biology, you don't really care about the stable states and you don't really care about the main modes because they're easy to get. They're easy to collect. What you actually care about is those hard to collect places, those hard to identify transitions. That's where statistical methods fail. That's where you have to invest a lot. That's where you have to invest a lot of effort in designing your clinical trial to actually capture them and make your data as uniform as possible. So, if you're working on observational data where you don't necessarily get to design everything, this becomes the region where you actually want to augment the data. So, if we can avoid the bias towards dense region in the data and separate In the data and separate the geometry from the distribution, then we can say, okay, we want to have a more uniform sampling of the data manifold. And more accurately, we actually want to augment the sparse regions without increasing the dimensionality in some sense. I mean, if you think about it, it's a tough. But it's a tough task to do because you have a lot of sparse regions in your ambient space that are completely useless and you don't want to go there. If you can do this kind of augmentation, you can combine it and now have a better view of the transitions. Now you don't have modes separated by sparse regions. You actually have a geometry that That is smooth and that you can traverse. Now, to do this, what we did in a method called Sugar is we renormalized the kernel to augment transitions that go through sparse regions. Okay, so every path has to go through some region of the data, just that we know that if you go through dense. We know that if you go through dense regions of the data, you'll just have many, you'll have high connectivity. If you go through sparse regions, then in a relatively quantitative way, we need to help you a little bit to increase the connectivity. And if you do it in a careful way, you can actually get rid of those tangential movements on the data manifold and just project in a stable way. And just project in a stable way on the data manifold. There's also the question of where do you start. So I said we start from random points. Well, if you start from completely random points, it would be very, very difficult because everything is far from everything. So you'll just be too far from the data manifold to see anything. We have a way of through the theorem, which I will not get into the details of, but we have a way of understanding from each region near. Each region near the data manifold, how much we need to augment there. Okay, as an example, here we have data that has to do with hematopoiesis, blood generation. And this data has a very sparse transition that's very hard to capture, okay, of B cell maturation. Maturation. By applying this method, you can see that the distribution of different cell types is shifted, it's adjusted to add those missing cells in this trajectory and fill it in. And the impact that this has is that, for example, if we look at mutual information and correlation between markers that should have That should have correlation between them based on our known biology that was gained over many, many experiments and very difficult to infer them, but still the knowledge has been built over time. Those relations are accentuated significantly by augmenting the data based on the geometry. More visually, you get this kind of relations before and Relations before and after the augmentation. So you see that we build this kind of understanding of the relations and enhance them. Okay, now all of this has a hand wave with a lot of things, but all of these things have very good reasons why they happen. The diffusion operator is an approximation of the heat kernel. So if you do Of the heat kernel. So, if you do start with the Riemannian manifolds and you do have an understanding of mathematically of where things come from, you get generalized Fourier harmonics, you get the eigenvalues corresponding to frequencies, you get low-pass filters and impulse responses for these methods. And of course, you can build high-pass and band-pass filters or diffusion wavelets, as we discussed in many talks here. So, we'll not get Here, so we'll not get too much into the details, but all of this, the reason that these things work has to do with all the math that is built in underneath these methods. Now, the one thing that we don't do in all of in everything that I showed so far is to think of whether the original data geometry that we built is Data geometry that we built is a good one. Because we start with very noisy data, we know we do some good things there, but we get stuck on exactly the same one. And even if we think that we can play with the diffusion time and propagate and so on, well, as I said, you can find an optimal time, optimal diffusion time that gives you exactly the best resolution between. Best resolution between denoising and losing signal, right? So it's not really multi-scale. What we really want to do is to rebuild the geometry. Once we have some initial understanding of the patterns, the structures, the relations between data features, we can then go ahead and rebuild the geometry. And to relate this to what happens in graph neural networks, this is exactly what graph attention mechanisms do: they rebuild the graph. Okay, once you have Okay, once you have some node features and some message passing going around, you go and you rebuild the graph by either computing masked affinities. It's basically kernel learning, if you think about graph attention, or you do something better, in a sense, by creating those virtual edges that Ladislav mentioned today with global attention. With global attention, right? And then you don't even mask things necessarily with your original graph. You're looking for those missing links and augment with it. Now, in the case of diffusion processes, when we think of what does it mean to rebuild the geometry, well, rebuilding the geometry means changing the diffusion process, the underlying diffusion process. This means that as you go along time, you don't use That as you go along time, you don't use exactly the same transition probabilities, okay? Which is what we call time-inhomogeneous diffusion processes. Okay, to do this, we build a process that we call diffusion condensation. The idea here is that after every few iterations of diffusion, we stop and we reconstruct. We stop and we reconstruct our neighborhoods, we reconstruct the kernel, we reconstruct the diffusion probabilities, and now we have a new Markov chain. So the Markov chain changes as we go along. Now, what we observe, and the reason we call it condensation, is that things start moving slowly as we denoise, okay? And there are two types of noise. There are two types of motions that we see. One of them is that everything contracts gradually. And there's a good reason why everything would contract gradually. For example, if we look at a geometric perspective, we can show that if you condense things by applying several iterations of the diffusion process, pushing things toward the manifold, then rebuild the manifold, and then do it over and over and over. Over and over, you have a nested set of convex hulls, you have a diameter that decreases monotonically and various other properties. We can also look at it from a spectral or topological perspective. The details are different, the main message is the same. Things contract, things squeeze. But the important part is that at some point, if two regions At some point, if two regions in the data start touching and start having increased connectivity, they collapse together. It's like two raindrops that get close together and merge. And we can identify these kind of mergers. We can build this kind of coarse-grained multi-resolution representation of the data from them. So, for example, we worked on We worked on some sea elegant worms on their nervous system, constructed some diffusion geometry between their neurons, and ran this diffusion condensation process, and then did some very nitty-gritty stuff to build hierarchies of clusters and relate them to various markers and so on. And we found that. And we found that we can identify modules and new modules of neurons that were not revealed before, that were not as evident before as they were with this process. More interestingly, though, we can take this kind of idea of timing homogeneous diffusion and bring it back to the representation learning idea. So now we idea so now we don't now we're building a set of geometries okay and because we build a set of geometries of the data we no longer have one representation okay we no longer have one optimal resolution we have a sequence of them and by looking at how the condensation process eliminates local variability we can identify these transitions between everything contracting slowly Everything contracting slowly at the same pace, and certain regions, certain raindrops touching each other and merging quickly, which means that we can identify which metastable geometries or which metastable regions we have in this process. And because those regions of the data, they collapse on each other, they collapse each other at the same point, we don't even need to cluster the data. Even need to cluster the data, it's automatically clustered for us by just things having the same coordinates. This means that we get this kind of display of multi-resolution embeddings, and we can associate a certain density or displayed here as the size of the point in the embedding. And then we can have an overview of the data. And once we identify through the overview of the data regions of interest. Of the data regions of interest, we can zoom into them and have a finer resolution. So, if I started with the analogy of a microscope, now we can zoom in and zoom out, and we can play with the microscope. We can actually play with the data and see where to focus. There are very often a lot of regions in the data and a lot of processes going on that are just not interesting for what we explore. So we ran this thing on this disease, maybe you heard of it, COVID-19. Okay. And in fact, this is the reason why we wanted to go multi-scale to begin with, because just one display, one resolution, didn't capture the complexity of immune response to. Of immune response to COVID in the data that we were looking at. And just to give you the scale, so this data has a couple hundred patients, okay? Has was something like 56 million cells, if I remember correctly. Okay, 56 million cells, if you try to just have one display through a kernel method, you'd have a bad day. Just imagine a 50. Just imagine a 56 million by 56 million distance matrix. Okay, so it was really necessary also to coarse grain the data and work on relations between regions of the data at a much higher level. Okay, what we see here, if you look here at the top, we see the overview of different cell types identified through certain markers. And we see here mortality. We see here mortality likelihood. This is computed through MEL, through the relative likelihood estimation that Smita talked about yesterday. Okay, so we can see which region of interest we want to focus on here. We want to focus on the high mortality region. We see which cell types are associated with it, and then we can drill down and see which type of T cells we see there. We can see then in a finer resolution how the How the mortality likelihood varies. And we can align directions that go from low likelihood to high likelihood of mortality, of being dead, which I think we'd all agree is an adverse outcome in most cases. And associate this direction through mutual information estimation and so on with certain markers and then. Certain markers, and then say, okay, well, you know, these are gene signatures that are associated with mortality. Okay. We can drill down more and just run the analysis on T cells, okay, specifically filter the data for immune cells and again gain some understanding of the type of T cells and the type of markers that are associated. That are associated with high mortality or high risk of mortality, at least. And then we don't treat cells, right? I mean, I don't know because I'm not a medical doctor, but I would imagine that we treat patients rather than cells. So we can also take this analysis to guide the coarse graining, to guide the multi-resolution perspective, not just to Perspective, not just to arbitrarily go to cell clusters, but go to a patient level and to look at the trajectory of patients over time and identify through that certain features that are important for predicting the outcome. Okay, all of this through exploring the data. And once we get these features, we can say, okay, how predictive are they? And in fact, we can. Predictive are they? And in fact, we can predict the outcome of COVID with something like 80-something percent accuracy, 84%, if I recall correctly. Okay, so you don't have to go and look at all the possible features in your data. You can take the genomic signature, the feature that we identify as being important, being related. And once you focus on them, you can build a predictive model on top of it. This is the kind of This is the kind of conclusion that you can get from this data, from this analysis. Okay. So to conclude, so you'll have time to ask questions, hopefully. No? Okay. That is Lavi saying no. Very good. So we talked about fade and gray for data visualization with diffusion geometries. Geometries reiterated about magic for data imputation denoising. We talked a bit about geometry-based data generation, where we leverage the orthogonal movement from high dimensions towards an intrinsic geometry in the data, and then condensation and multi-scale fate that use the tangential motion on the manifold to rebuild it, to reshape it, and to relearn it as we go. There are various other things we can do, meld. Various other things we can do, MELD that Smith already explained, for density estimation, likelihood estimation, but more accurately or more pointedly for applications like point perturbations, what happen when you change things in the data. RF fate, it's random forest fate. When you want to guide your visualization towards patterns of interest in a supervised way, harmonic alignment allows you to do data fusion based on how things vary over a data manifold. Things vary over a data manifold. And there are many other things that will not, that I just ran out of space here to actually list them. And finally, most importantly, all of this thing is not done just by me. I'm just there for the ride. So I want to thank all of my collaborators and, of course, the funding agencies, because someone has to pay for all of this stuff. And I'm sure I forgot some people, so I'll tell the recording that they. I'll tell the recording that if I forgot someone, I thank you too, and I'm sorry for not having your name there. Okay, thank you.