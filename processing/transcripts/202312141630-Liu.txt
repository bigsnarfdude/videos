This is a joint work with Su Yen Liu, one of my PhD students, and Peng Wei Li from University of Waterloo, and Dr. Jin Chen from NIH. Okay, I will first introduce the background and my motivation. Classification. Classification is a the goal of classification is to assign category labels to unlabel test data based on what you learn from training data. They have many applications including email spine filtering, certain team analysis, and so on. In a classification problem, Suppose we have n observation and id data from training data, and then I will have testing data, which and their labels are missing. So our goal is to make a pre prediction for the missing labels and y n plus m. And y plus m okay. It is a usually usually we make an assumption that the training data and the test data, sorry, we say test, they have the same distribution. If this is the case, many powerful supervised learning algorithms have been invested studied. They used to estimate the common cognition of the The common conditional probability of y equal to smaller y given x. For example, decision trees, random forests, SVM neural networks. And then we can classify the test data using the estimate conditional probability. However, if the training data and the testing have different distributions, Distributions, then the compression methods may face challenges or even underperform. Okay, two special cases of distribution mismatch or distribution shift are covariant shift and label shift. In this talk, we will focus on We will focus on the case where both cover shift and label shift exist. Suppose we have key classes in the training data and we organize the training data as this form. And in the unlabeled test data, feature or convert x may come from one of the distributions or it All it does not come from any of them. If it does not come from any of them, we classify it as an outlier. We use F capital K to denote the CDF of the covariant for the outliers and categ categorize categorize them them as a classi c cl class capital P. class capital P. Let high T denotes the proportion of classic classy class K in the testing data. Then the covariant in the test data follows a finance mixture model. The right side denotes the outliers. So our goal is to make a prediction for Y about Y. For why, about why for each X in the test data. Outlier dissection has been outlined design is found applicable in many areas. For the problem of whether a data point in the test disk is an outline, there are several There are several papers on this problem. However, they may suffer from a non-parametric estimation of the test ratios. In particular, another limitation is that they simply give answer whether a data point is an outlier or not. If it is not an outlier, they do not tell us. They do not tell us which class they belong to. They belong to. Okay. For all the previous methods, they provide a prediction point for the label of each test data point. An alternative method of predicting for label prediction. For labor prediction, to construct a prediction set. For example, the popular density level set example. The density level set here is constructed based on the ideal density functions. For practical use, we need to. Practical use, we need to estimate all the data FK. The prediction side CX here may contain more than one label. If it is empty, then we classify the corresponding X as an outline. Okay. As we can see from the construction of this level. of this level set density level set the the the the set a key is constructed only through the the f key and his his quantile so a key the construction of a key does not use any information from other class of of data this is one of this is his uh weakness so to overcome this problem uh To overcome this problem, Guan and Chip Shrani proposed the BCOPIS method. This method performs better because it combines the information from other classes of data to predict each and construct the CX. However, the the validation of B C O P S is built on an important assumption. An important assumption. The authors can be perfectly separated from the observed classes. This is assumption 6. We found that this assumption is too strong to be satisfied by many commonly used parametrical models, such as normal distributions. To see this, let's c uh consider hereafter we assume uh the capital K is equal to two. capital K is equal to two. And then the conversion from the test data follows this mixture model. Using this notation, the assumption six of Guan Tim Shrani requires that these probabilities should equal to zero. This requirement we believe it is too strong. We calculated some specific probabilities for normal distributions. We can see that each line corresponding to one case, we can see that in each case all the probabilities are non-zero. So their assumption six is Assumption six is, we believe, is too strong. If the assumption is validated, their prediction set may be misleading. So, this motivates us to develop a new label prediction set. Okay, let's introduce our model. Recall that we have direct data from F0 and F1, and the current in the The covariant in the test data follows the mixture model. And we have direct data from F0 and F1. So these two tenses are identifiable non-parametrically. However, we do not have any direct data from F2, but only some indirect data from the test data. If for a mixture model, For a mixture model, for a two-component mixture model, if lambda is, if g is known, but lambda and f are unknown, then from this equality, we can see that lambda and f are identifiable. So by this lemma, we we see that if we do not mix mix any parametric model on on Fr, F two, for him. F2. Sorry. Here, the here pi 0, pi 1, and pi 2 and f are unknown parameters. Even if f0 and f1 are unknown, because pi 1, pi 2, and pi 0, and f2 are unknown, so f2 these parameters are still unidentifiable. Okay. To to Overcome the identifiability and to not to circumvent the model misspecification of parametric model, we assume some parametric density model on the densities. This model is origins from density Anderson 1979. Many Many commonly used parametrical distributions satisfy this usual model. Also, these usual models are closely related to discriminatory analysis and problem subject to covariance shift. Okay. And the the distribution model, we can write the advertise x as the rate form. As this form in this form, F0, the unknown parameters are F0, Pi 0, Pi 1, Pi 2, and Gamma 1 and Gamma 2. So if assumption 1 is trivial. Assumption 2 is a key. We use an notation with a superscript O to denote its true value. So if pi 2 So if pi 2 O is greater than 0 and this matrix is positive definite, then all the parameters are identifiable. Once the parameters are identifiable, next we introduce our semi-parametric estimation procedure. And the distribution model, the likelihood contribution from the training data The training data have this form, and that from the test data have this form. Here we have sufficiently used the structure of the model assumption, this usual model assumption. From this likelihood, we can see that the only gene finality differs from the Uh the dimension uh parameter is the F zero, the capital F zero. Uh the other parameters are all finite dimension. So we use empirical likelihood to handle the F0. Using the principle of empirical likelihood, F0 is modeled by a multinomial distribution. And then we put it into the likelihood and take a log. Into the likelihood and take a logarithm, we have this log likelihood. It is worth mention that the right equation is very important. They hold because F1 and F2 are distributions. So then we maximize all the everything about PI and gamma and pi. And pi, then we will have their maximum likelihood estimate. So I will omit the skip these details. Suppose we have the MLE theta head and F0 head, FP head. They provide the basic element for our construction of the proposed label prediction set. Before introducing Before introducing our label predictor set, here we give some asymptotic properties of our estimators. We show that both the finite and infinite parameters are asymptotically normal. Also, we provide a GM algorithm for the numerical implementation. I think the I think the EM algorithm is standard, so the details are omitted. Now I will introduce some parametric label prediction set. Our label prediction set follows the Guan and Tipistrani's method. We will construct CX. CX like this form. According to Guan and Tim Shrani, a reasonable prediction size CIX can be constructed as the minimizer of this problem. Also, they show that minimizing this problem is equal to minimizing the mission. Minimizing the miss classification loss averaged over the out of sample data. So I directly call the predict all predict the prediction size CX as the minimal of this problem. Okay, see here if test is taken as a weight function to balance the classification accuracy and power. Accuracy and power of outline detection. Here is the problem is in the ideal case because the density of tests are unknown, is unknown. Okay. The problem P, the solution problem P is the oracles prediction sign C star here. We decompose the we try compose the we try transform this problem to this problem this problem is easier to to solve actually the solution a k star to this problem has a closed form okay here is the close form uh here the b k is the density ratio of f key and and and f test um We can see that the construction of AKI use boost use all data use the data from all our classes. This is different from the density level set. So intuitively, this prediction set CX may be better. Okay. Okay, for here we propose a parametric likelihood prediction method because to overcome the problem of one anti-joining, as they require their assumption six, we have shown that assumption six is too strong. As the AK star depends only on the order order ordering of V K. Order ordering of VK. So we can make so any other preserved transformation of VK is permitted when we construct AK star. So we replace the V key by this the new the new V key. Then under our distribution model we plug in all the annual parameters. Plug in all the unknown parameters. Then we will have our label prediction sign. It takes this form. Here we use the FN key. FN key is the empirical distribution of the data of class key, class key in the training data. So FNK is available from the training data. Available from the training data. This is our main result. This is our prediction site. Next, we will give some theoretical results. And these assumptions, assumption five, this assumption is very similar to one of the assumptions. One of the sums in Guan keeps running. It requires that the VKX is neither too stiff or nor too black around a boundary. So we found that as N is big, the NK set can have a symptomically coverage guarantee. Also, our Also, our prediction side have achieved the optimal misclassification limit loss asymptotically. These are the two theoretical results for our prediction set. And now I will introduce some numerical results. We compare three methods. three methods uh two the first two are bc ops uh because uh um in the bcops and algorithm and learning a learning algorithm is a is a required is needed so when you when we you use different learning uh algorithm we will have two different uh pco c method and the third is our method Myself. Okay. Okay. The upper left plot shows the two classes of different points in the test data. The upper right and the lower right and the lower left are the Right and the lower left are the classification results of BCOPS. The lower right plot is the plot of our method. We can see that the boundary of our method is the most clear. So this may show the advantage of our method. We also use another, we use four. another we use four criteria to to compare the the the three three prediction signs uh the the the larger the better for for all the criteria the larger the better we can see that um uh for the the the criteria are and accuracy and the our method is the uh shows uh are are much better than the uh two BCOPS. And two BCOPS. The cover one and cover tree culture is defined as a proportion of points with this form. I think they should be closer to the nominal level, 95% percentage nominal, the closer, the better, I think. So overall, we can see that the SE RPS. That the SE RPS may have the best performance. Okay, I will skip this. Applications we analyze for real data. Of course, we know the labels of all the data, so we sample some of them as training data and take the As training data and take the rest as test data, and then we also calculated their four criteria. Again, we can see that SE RPS has the largest R and accuracy for all the four data for all the four data sets okay for this is the last uh last uh f figure. Thus, the figure shows the plot of accurate type error and various empirical misclassification rate. A method with a lower line will be better. So we can see the red line. The right red line is our method for all the real data set. It is always lowest. It is always the lowest. So it's to some extent it's better than the other two methods. Okay. Here is a summary. The unlabeled data follows a mixture model, and we found that it cannot be identified non-parametrically. We propose. We propose to model the test data by a finite seven-parametrical mixture model and distribution model. And we construct a seven-parametrical empirical likelihood prediction size for the label in the past data. The new method overcome or circumvent the assumption of Guan Tushrani. Also, we established. Also, we established some asymptotic theory, asymptotic theory of our math. All right, and that's all. Thank you.