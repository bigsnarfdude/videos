First, I want to apologize. Basically, I'm like a party crusher. This is more like high-dimensional data network. But my talk, well, you can put it at high dimension, but not quite related to network. So I'm going to talk about this double adaptive spatial quantile regression model for neural image data. This work was done quite early. Actually, this is the only work I did with Hong Tu when I was doing my postdoc, which like 10 years ago, or more than 10 years ago. So first, I would do some introduction, then I would. Introduction, then I will introduce this double adaptive spatial quantile regression framework, and then I will do some simulation studies. Okay, so image data, and usually we need first acquired those image data. We need a scanner, and for example, if we want to get this MRI. MRI data, we need people to do the scanning, then get this neuroimage data. Then we start to analyze those neural image data. And usually, we need quite some process in order for our statisticians to be able to analyze it, like image reconstruction, processing, segmentation, alignment, and so on. Alignment and so on. And also for the image data, the challenge lies on we may have multiple subjects and also we could have multiple images from a single subject. For example, for adding data, you may want to have this sort of longitudinal data to see how this How this Alzheimer's disease gets prognosed. Now, let's now assume everything has been done. The house cleaning work has been done, the registration pre-processing, and so on. Everything has been done. Then we have a clean image data that. image data that mathematician or statisticians we can analyze. So basically we would have a sort of response for example the green matter or white matter intensity and this basically it's a Jacoby matrix after do all the registration and so on. And this is a 3D image and for example here we can have a 64 by 64 Like 64 by 64 by 48 3D volume. But of course, nowadays the resolution can be much higher, could be like 256 by 256 by 198. Now we have some coverage. Okay, let's say age, gender, diagnostics, and we may also have some genetic variation. So what's our task? What's our task? What do we want to do? We want to find these sort of coverage effects. It's always one thing we care about. We want to find how the covariance affect those image. Why we want to do that? The key thing, especially for image data, the final goal is not to Final goal is not to just estimate this covariance effects B D in terms of the location D. The key thing is we want to identify meaningful regions of interest. We want to find those regions that may be related to some particular disease. That's the second goal. We want to find those meaningful regions of interest. Region of interest. But then, as a statistician, find those regions is not the end of our work. We also want to see, okay, the region we find actually is really the region significantly related to this disease. So, we need to, we want to quantify those effects. We want to see, okay, those covariance. want to see okay those cognitive effects are actually significant they are not um they are not just found by checks so that's uh that's our goal our three goals now let's first review um typically how um or traditionally how this would be done the first kind of method would be okay a voxwide method okay let's see Method. Let's ignore the sort of spatial correlation within the brain. We just treat each voxel independently. Then we have this voxel-wise method. So even D can be very large. The total number of D in those regions can be tens of thousands. And we just run the individual regression. And very simple, very fast. But of course, the The drawback is very obvious. We ignore the spatial correlation and we ignore the structure, we ignore the variability of this image data. And that's not what we want. But it's actually a very good step. Now, in order to respect the data, respect The data respect the spatial structure. We can actually consider those kinds of correlation structures. So we have this spatial modeling method. We explicitly model the spatial correlation. For example, we can use conditional autoregressive and mark random field. This is the two quite a popular method. To quite a popular method to model this spatial structure. But for this kind of method, the computation is very intense. The computation was very intense. And this project was done, like I said, 10 years ago. And recently we are doing the revision and we rerun the Previous, so you can see we actually downsize the image to 64 by 64 by 48, previously 256, 256 by 190. And then even in all these computational power, we rerun the real data analysis. It's still quite slow. The computation is still very chunky. still very challenging so another method is this propagation separation method this method can adapt really and spatially small things the image and just small things those image and they might be the key underlying assumption is the image actually is piecewise constant or piecewise smooth because we have certain region of interest and supposedly within that region of interest Within that region of interest, the response should be similar. But for this kind of method, it still ignores the variability of imaging data, imaging data. Now, that's some traditional method. When we start this project, we feel like, okay, we want develop some method. Develop some method which are more powerful. And for image study, if you want to develop those kinds of methods, you have to have a better understanding of the whole process. And so we trying to go over the whole process again, which including typically, like I said, we have the image acquisition and image processing and And then we can do data analysis and interpretation. And the left side is what we want to do. And the right side is what we actually are doing in order to achieve the goal. For example, in order to do the image acquisition, we need to use some mathematical model like signal model and trying to respect the noise exhaust. Respect the noise sources. For the image processing part, we need to do representation and segmentation registration and so on. In order to do this analysis and interpretation, we need to develop the statistical modeling and then do the inference. That's the whole process. So in order to But to make this project successful, we actually go through the whole process. The data we are using is ADHD data. And nowadays online, there are many process ADHD. But for our project, we actually go through the whole process. We did this segmentation registration and the whole thing ourselves. One of the reasons to do that is, you know, for this various image segmentation and registration, when we do the processing, pre-processing, we may result in some non-consistency. A lot of, if you read this. this neuroimaging papers, a lot of time they would have this sort of in-house processing, or sometimes you get it online, people process the data alright. And if you run the same data set, a different pre-processing may have different results. So we may have this non-consistency with error. And also a lot of times during the process, Times during the process, we would assume, okay, the error is Gaussian for community. So we all know Gaussian is easy to do. We have a lot of tools available to deal with Gaussian. But in image data, really is it Gaussian? It's debatable. Another thing is within the grade, the variation of the error actually Of the error actually varying spatially within the brain. So we need to account those spatial correlation, spatial correlation. Consider all these things we want to try to overcome. We actually propose to use quantile regression. And quantile regression, we call it the cousin of the automatic. call it the cousin of the ordinary least square. So why quantile regression? Quantile regression can give a full picture of the data, not only in the center, the mean situation, because we know in Gaussian, and if you just study the mean, then quite enough already. But we don't know whether the underlying error is Gaussian or not. Actually, a lot of times. Or not. Actually, a lot of times it's not. Then the quantum regression is able to give a more complete structure about how the response change with the coverage. Another thing is we actually propose sort of a composite quantile regression. Through this composite quantile regression, we can learn Then this structure can estimate the coherency effect more efficiently. Therefore, we propose this double adaptive spatial quantile regression model. And we update the estimate by borrowing information from nearby voxel. Because nearby voxel, they are sort of correlated. They have this facial. They have this spatial structure. And meanwhile, we borrow information from different quantile levels, different quantile levels. Okay, more specifically, what we have here is for this volunteer quantile regression model, we are trying to model the conditional quantile given. How given some covariates, and we assume it follows a certain structure here. We have this structure mu xi beta d. beta d depends on the location and also depends on the functile level. And this q tau y d given x i satisfy the torque conditional functel. And this d is in the in a three. In a 3D space, 3D space. For example, it could be 32 by 32 by 6, this 3D space. And this mu is a small function in both tau and d. And here, maybe it's more precise to say, okay, it's piecewise. Now, for the quantile. Now, for the quantile loss, we would have this. We want to minimize this zhou tau and the difference between the response and toss conditional quantum. And this rho tau is the check function, is the check function. And in order to do this, there are a lot of algorithms available. We can use linear programming, interior point, simple method, and so on. Point, simplex method, and so on. And there are also a lot of packages available, for example, in SAS, in RML, and you can use those software to do this. But this is work-wise. So, for our DHQRM, the object function will be written in this form. form. Here we actually get two weights, this W D D prime H and this W torque B tau K B and the rest would be the same. And this tau K is in a pre-specified quantile level set and this D prime in a bar and related to this center D, related to this center D. So So basically, this is two-weight sort of you can call it weighted composite quant or weighted quant. And the weight is coming from two parts. One is from location. We want to borrow information from nearby vauxhall. And one is from different quantile levels. We want to borrow information from different quantile levels, different quantile level. Now, how do we borrow information? How do we borrow information? And to borrow information, smoothing is one simple way. So for this spatially, in terms of voxels, to borrow the information that borrow the information they're coming two parts uh coming to coming from two parts uh it's two kernels uh one is related to the location and how close is d to d prime the closer we borrow more information the other one is uh this uh uh sort of uh similarity kernel okay if two voxel are close but if they are estimate quite different then we should borrow Quite different, then we should borrow less information. If the estimates are close, very close, then that means we should borrow more information, right? We should borrow more information. So that's how these two kernels plays in determining the weight, determining the weight, location and estimate, location and estimate. The closer, we borrow more information in terms of location. The closer, the estimate. The closer the estimate, and we borrow more economics. I think I didn't write it here. And basically, we actually use R2D6 things. Uh, we actually use L2 distance of the two estimates like uh estimate at uh uh d and estimate at d prime, bet d and bit d prime it's a vector. Then we look at the R2 distance. Yeah, yeah, and adjusted by, of course, adjusted by its variance. Yeah, I will specify the whole process. Basically, at the beginning, we use a voltage-wise method. Volkswavite method, it's not efficient, but it's still a consistent estimate already, right? So we start that and then we do propagation, then we update. So this kernel is based on the this similarity measure, this similarity measure. I just asked, we actually look at this. There are many ways to do it, but we simply use this sort of R2 distance, adjusted R2 distance, adjusted by this variance. Okay, then the Uh the uh objective function um initially we have this uh um we have this three uh summation and we need weighted by two weights and it's actually can be rewritten um in this format and here we assume uh those weight summation equals one then uh we only need to adjust this to start This torso star, this torso star can be rewritten as this weight. And we can replace this y as a weighted variant and this feature also by its weighted variant. So in this way, when we do coding, it's easier. Now, in this sort of double adaptive estimation, Double adaptive as mentioned, we have three key steps. The first step is this sort of initialization. To do the initialization, we just do it voltage-wise. Volcanoise is not efficient, but it's consistent already. So, and we start with the volcano-wise method. And then from there, we can update. And then from there, we can update our weight. We can calculate the weight. And then we do waged word. Okay, we do waged vert. And for the next step, we will update our kernel, decide how many workflows we are going to include to participate this small participate small. Attestimate smooth and also, um, because we have an initial estimate, we can uh estimate the weight, then we can get an estimate, um, updated estimate. And then from this updated estimate, we can do it again to update or to do the sort of propagation separation again. We can include more voxel, more voxel. More voxel. And of course, we have a sort of a stopping checking. Stopping checking. You don't want to do over-smoothing, over-smoothing. So here is this sort of propagation separation, propagation separation. Initially, we have this voxel-wise, and we estimate at each voxel. Then in the next step, we have a We have a radius, H1. We decide to include nearby vauxhall. And then we have the crease sponging weight. We decide how much information we want to borrow from nearby vauxhall. And then we update our estimate. Then we can increase our radius. We decide to do further smoothing, borrow more information from More information from larger neighborhood. And then we update our weight again and until we stop. So this is the propagation separation. So then we have this sort of a stopping rule. And when we stop, if the updated estimate is too far away from Estimate is too far away from the initial estimate. Because the initial estimate is already consistent estimate. We do this sort of double adaptive smoothing. The reason is we want to respect the spatial structure. We want to consider the correlation. And by considering the correlation, by doing this way. The correlation by doing this weighted small thing, we actually can reduce the variation of our estimate. Therefore, we can increase our detection power to detect this region of interest. So this is the stopping rule. We have this constant C is the distance is. If the distance is the dissimilarity is too far away greater than the C, then we will stop. We will stop. And why we choose this constant C, that actually have a theoretical guarantee so that under certain conditions, and we can choose this C in order to recover this sort of piecewise small thing or piecewise constant structure. Consonant structure. Okay, and then this is the this is the whole procedure. And we have the three key star initial estimate, individual, there are two individual estimates, individual voxel and the individual quantile level. And then once we get to the initial estimate, we want to borrow information from nearby. We want to borrow information from nearby Vauxhall. And also through this, we can also borrow information from nearby Puang Tao level, nearby Puang Tao level. And then we update our estimate through this propagation separation procedure, then we start. Now, what kind of theoretical guarantee we Of theoretical guarantee we would have for this procedure. Of course, the first one is the sort of consistency and asymptotic normality. So the first theorem is under certain conditions, our estimate will converge to this two estimate in probability. And also, we would have this would have this asymptotic normal asymptotic normal property now there's no free lunch right and we we did the smoothie and of course we would we would be introduced by us one way or the other right Sparse one way or the other, right? And in order to study that, we first look at how these dissimilarity measures look so dissimilarity looks like. Under certain conditions, this location dissimilarity can be approximated by this eventually, and this dissimilarity between functile levels will eventually be approximated by this. And based on that, we would have this kernel, how this kernel behave, how this kernel behave, and why it's related to voxel, and why it's related to this functional level. And if you look at those things closely, and you will find essentially this part, the first part is whether they are same or not. And then it's also related to both smoothie. This is a location smoothie. Is a location smoothing smoothing part, and also we have a read log capital N. Now, we also did some small simulations. The model here we consider is kind of spatial Spatial location scale model. And we have a mean structure here. This YID equals XI transpose B D. This is the mean structure. And in order to show the advantage of quantile regression, we actually consider this a skill part, this skill. And this D is in this 3D Um uh space and those features including uh three paths and the first one is just intercept, second one is uh follow binomial and then it's basically uh we are trying to mimic this category variable and this is third one is uniform and this is continuous continuous variable and um the The beta i is in 0, 0.2, 0.6, 0.4, 0.6, 0.8. This is just to mimic the strength of the signal at different level. And we have gamma 1D like this, gamma 2d and gamma 3d. And this FND either follow normal. And this is a regular case. This is the regular case. Usually, in least square, we would assume this area follows this one, and all this t-distribution. This is a situation for your botany. I don't know how to do it. And then under those conditions, the conditional quantile can be written in this format. This is the key structure, and this part is coming from this sort of spatial correlation, this scale part. And essentially, it can be written in this format, in this format. Now, let's see how it looks like. We have two situations. One is the tor equal 0.5, which is a media situation on the left. And the right is tor equal 0.3. And we have three rows. First row is beta 1D, the incept. Second row is bet2D. Third row is base 3D. And for each panel, we have three No, this is beta 1, beta 2, b3, beta 1, beta 2, b3. And this is the initial estimate. This is our estimate. This is just voltawweight mass. And as you can see here, initially, we, especially for this part, we will miss a lot of miss a lot of the pattern miss a lot of pattern well after we use double adaptive smoothing method we can reconstruct the pattern quite quite well quite well the left one is a median median is the easy to deal and this one is taught equal 0.3 and it's a little bit And it's a little bit to the lower part. And that's the information available. As you can see here, we barely see any pattern. However, through our adaptive estimate, we can actually recover those structure quite well. And in our revision, actually, one of the reviewers asked: okay, what if Asked, okay. What if we just smooth this? We also did this simulation. If we just smooth this, of course, we can sort of get better result, but it's still not comparable to this one, comparable to this one. Because in our method, when we do the smoothing, we actually consider different locations and also different quantile levels. And we use more advanced propagation separation methods. So our result is much better. And not only in detect those patterns, but also in further when we do hypothesis tests, right? Because remember, essential goal is we want to claim, okay, this pattern actually, not only we have this kind of pattern, but we can actually claim this pattern is significant. actually claim this pattern is significant. It's really a pattern. It's independent, it's pre-specified, pre-specified. And the key thing here is it's a very good question. We actually also get similar comments, review comments on this ask us to comment on why, you know. comment of why you know you do this pre-specified. We actually did either including, let's say in one direction, five to ten would be good enough. And why is because actually within those regions, the signal is quite strong and some very simple things you do will largely increase. Largely increase the power. You would be able to detect those patterns. And this is, we look at this user mean squared error, this standard deviation, and this RE, which we define this RMS divided by S D. Those patterns are not clear as this one, and also in terms of those kind of measures like good mean square error and standard deviation and I. A standard deviation and I. Basically, the two things. One is in terms of recovering the pattern, not as won't be able to clearly recover those things. That's one thing. The second thing, the variation can be still large, can be still large because we want to have a good variation estimate in order to do the next hypothesis type. The next hypothesis test. Okay, this is another measure to quantify our estimate. And here is the signal region we detected. We detected this is tau equal 0.5 and And this is this side is toy equal 0.3. And we have this initial one and we have this adapted one. We plot this mass log 10p, math log 10p. And as you can see, we actually be able to detect more, correctly detect more regions than. Regions than this initial one, than this initial. Of course, like I said, in this revision, they asked us just to do this simple smooth thing to compare with our method. And it's certainly better than this one. You do a simple smoothing, but not comparable to this, to our mask. Okay, here is some numerical method, a numerical result. I'm not going to show it. So this is the first simulation. And the first simulation, it kind of, okay, we have these two. these two situations the variance one is normal the other one is T but this only represents one issue in this neural images study okay that's under the assumption okay everything is perfect process and those errors only coming from image itself but what if in our processing we have some In our processing, we have some maybe even slightly inconsistent results. For example, when you do the registration, when you do the alignment, and you cannot guarantee they are aligned quite well, right? And we all know, okay, and there are actually no underlying truths at all, right? Your image or your brain and my image and brain, how you are going to learn them quite well. So In our second simulation, the model is the same. However, for the response of the strategy, we have 20% random select subject shift of one, two, two, three volts. They're basically trying to mimic, okay, in the alignment, and there may be some distortion and how it works, how it works. And so, And so here is the result. Here is the result. And everything is a little bit from the initial part is a little bit worse than without this distortion. But still, our method can largely improve those, largely improve those. I'm not going to go to details. To details and the third situation is the model is same as simulation one. However, we have a 20% random select subject. They are the response multiplied by 10. This is mimic. This is mimicked to this to the series distributed underlying distribution. And also, when you do the normalization, then maybe for various reasons, we didn't do well and how it looks like. And here is the result. And again, both initial and The both initial and adaptive estimate be worse than perfect simulation, but still our method can largely recover the response. Okay. I think I will stop here and thank you. Yeah. Yeah, I did, but that's it. Yeah, yeah. Yeah. Yeah. Yeah. Um in my framework. In my framework, the quantum level, we only choose like five, and also they are quite different. So they are at least 0.1 different. And also, because this is doing the smoothing, right? And we actually didn't find the crossing. Yeah. Yeah, you know, simulation. Yeah, you know, simulations are. But in reality, I understand that could be the crossing. But we actually, you know, if we have this crossing situation, I probably would use this Victor's method, just reorder. Yeah. Yeah, um that's a good question. And we didn't think about this, um, but and And the key thing at the of course if we have a better estimate then everything will be better. Um but the key thing at the beginning we only need a consistent estimate and then we iterate to the other one. That's one of the advantages we do quantile, right? So we can still get a sort of consistent one, but may not be as good as no contamination, no this shift. Yeah. Especially you only get 20% down and then you add some median, then yeah, that's also why I only show 0.5, 0.3. If in that situation, if you go to very tail, then it can be a problem. Yeah, that's a good question. So basically, this whole process is very complicated. And if we add penalty, then it can be, you know, very Can be very practically, maybe you can do this. But in terms of to look at the theoretical properties, basically we are quite overwhelmed already. Yeah, but it can be. And also another thing is when we do the propagation separation doing this small thing, it's kind of equivalent to this penalty one way or the other.  you know how so it's able to remember like what be the power square would be the you know when I try to use gender again is genetic predict the the you know the the box so how how easy it's like a how good you can actually predict the box this usually information do you have a do you have a rough idea about the I've already built this area very well just My understanding is in practice, we identify certain regions, right? We are able to the key thing is identify certain regions. Then we talk with And we talk with expertise in this field to see whether the region we identified will make sense or not. And that's how we verify whether we did a good job or not, because it's really the area depend. And if the and sometimes we, even for ourselves, we can, we are able to find the literature, okay, the region of interest we identify actually consistent with the literature. Actually, consistent with the literature, and sometimes if we find something new, we may talk with experts and see whether it makes or not. But I thought, I don't know. That's usually how, at least for me, I do it. It's ADATD data, Alzheimer's disease. Yeah. It's a uh it's it's a no, it they have a health control. Yeah, health control and MCI and AD, Alzheimer's disease. Yeah. We actually didn't explicitly consider the coalitions. Consider the correlations, and it's a bit challenging in quantities. Yeah, yeah, kernel can kernel function can capture some of the dependent structure. But theoretically, we can't say, okay, we this. say okay we we did something to we can just vaguely say okay we we use kernel so we can capture some spatial yeah yeah yeah kernel function uh basically just the regular kernel function would be would be kernel function would be would work um like for example you can use gaussian kernel yeah that's a good question we just uh uh very ad hoc choose some yeah like we can vary a little bit because the whole computation is quite uh computationally intense uh for For the simulation, we actually need to let's we at that project 10 years ago, we rerun the simulation in between like five years ago, and it takes like several, a couple weeks to run the whole simulation. So we actually only choose like for some other parameter, we only choose like three to five candidates then. candidate then we use cross-gradation to choose some good candidate combined and we unfortunately cannot afford to do too much on that yeah yeah thank you thank you 