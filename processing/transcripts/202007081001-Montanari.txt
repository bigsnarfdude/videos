Okay, share the screen. Sure. Or we can start with. Well, I was just going to say we're continuing with the third lecture in a series of lectures by Andrea Montanari on mean field methods in high-dimensional statistics in one college subtitle. There will be another exercise session. This lecture will be recorded, so in case you don't want to be recorded, you should. To be recorded, you should turn off your video and your microphone. The lecture will you are it will be followed by some questions that will be in a non-recorded portion. You're also welcome to ask questions over chat. And after the lecture, about there will be some breakout rooms, and then 30 minutes later, there will be. And then, 30 minutes later, there will be an exercise session again led by Michael Santano. And then tomorrow we will be following with the fourth lecture in this series of lectures. So I'll share the screen now. Okay, so welcome. Thanks for coming. I hope I didn't confuse everybody during the last lecture. So I'll start by giving a summary of what we did in the last lecture. We looked at this generalized regression problem in which we have data that are XIYI and are xi yi and x ti as are vectors so in our model the xi is for a normal zero identity and the yi are responses a real variable that are you know a function a non-linear function applied to theta zero dot xi and the wi are noise variables and my objective is given the data x and y I reconstruct X and Y, and I construct theta naught. And then I describe the standard kind of statistics approach to this, or one standard statistics approach that is minimize a cost function that typically has this structure. There's a sum of a data points of a loss applied to data points plus a regularizer. And then I describe a general class of first-order methods that you can think of either as Either as an algorithm to optimize cost functions of this form, or more generally, algorithm to process the data. And these are any sorts of algorithm that basically proceed by multiplying by x and x transpose. So these are really characterized by the way you query the matrix x. You query it by, you know, storing some vectors and multiplying by x and x transpose. By x and x transpose. And you know, gradient descent and many other algorithms, accelerated gradient, approximate gradient, fit to this framework. And so the question is, can we come up with a general theory of how these algorithms perform and what is the optimal choice? And of course, we are so far from having a general theory, but I describe the theorem. And the theorem assumes that these data are not. Theorem assumes that these data are normal, as in my example here. So the matrix x is the matrix with it rho equal to xi's. I stated this theorem assume that the xi, the you know, this Gaussian model for the data, then for any G theorem, what we get is that the limit of the estimation error, which is what we are interested in, is lower bounded by the limit estimate estimates estimate Lower bounded by the limit of the estimation error of a certain specific GPUM that is base A and P. And this itself can be computed explicitly through something recursive set of equations that is called state evolution. And I described a little bit these two algorithms, you know, two type of algorithms of this type in a special example that is sparse regression. But I didn't prove the theorem. But I didn't prove the theorem. And I will not prove this theorem because I will not have time to do it. But I will try to convey some the basic ideas of this theorem and the proof of this theorem today. Along the way, I introduce a bunch of other interesting concepts, hopefully interesting. But okay, so I decided that actually for explaining the basic idea, it's better to use a simpler. Is better to use a simpler model. And the simpler model that I will use is rank one matrix estimation. And this is related, I think Leo has given a talk, Leo Milano will give a talk in this school. So he will talk about this type of problems. And I think Jean-Christophe Moral also talked about this type of problems. Now, the reason why I don't like particular problems is that they are not particularly interesting from an applied viewpoint. They are not as interesting. Viewpoint. They are not as interesting as regression, but definitely the math is, you know, simplifies. I mean, in the sense that, okay, you'll see in what sense. So this is a model in which the unknown vector theta zero is again a vector with di D entries. You have a noise matrix, which is a GOE matrix. And what I mean by this is a symmetric matrix with entries above the diagonal that are normal 0, 1 over n. 1 over n and on the diagonal, okay, the convention is taken 2 over n. And then you observe, you don't observe theta 0, but you observe theta 0, theta 0 transpose plus noise. And here the scaling 1 over n is such, is chosen so that the two pieces in this here and here, the signal and the noise, have the same operator norm. Okay, so they are comparable. Okay, so this is the problem. You are given an instance, a realization of this matrix, and then you want to estimate theta naught. And this is, of course, it's closely related to things like community detection. Of course, this is your inferencing graph. It's a simpler model, but it's related to those. Okay, so what would be here the classical M-estimation approach, optimization approach to? Approach optimization approach to such an estimation problem would be one example would be okay you try to minimize some loss function of the form let's say x minus one over n theta theta transpose plus perhaps some regularizer of theta okay so this is uh this is this is for This is Frobenius term. This is the log likelihood. So this is minus log pth of x. And this is a regularizer to enforce the desired structure, the desired empirical distribution, perhaps. What would be a GFOM in this context? An example. An example of a GFUM would be, or a way to define a GPUM would be to say, okay, what I'll do is that I'll look at all algorithms that operate by multiplying the previous estimate by x. So you compute some function. And I remind you that here the convention is that F1 is a function that is from Rt to R, and I apply it, you know, row-wise to the n by t matrix formed by theta1, theta t. And perhaps you add some other function. Okay, so basically, this includes, you know, you can write This includes, you know, you can write in this form any sort of algorithm that you can think of as I do entrywise operation on the previous vectors, and then I multiply by x, or I multiply by x transpose. And if you write down the gradient descent with respect to the objective ln, this objective, you can check that actually takes this form. And actually, it's quite difficult to take algorithm, you know, to think of algorithm. To take algorithm, you know, to think of algorithms that don't do this. Well, of course, there are examples. For instance, if you take the matrix X and you do some operation entry-wise to the matrix X, that doesn't fit this structure. Now, what are AMP algorithms? Approximate message passing algorithms are a special subset of this that do basically the same. So I'll call So, I'll call the function lowercase f just to be clear that is another set is not necessarily the same set of functions. And then the key thing is that here the term that you're subtracting is special. So, s minus one, and then there is theta one. Okay, and here this coefficient B's are computed by taking the derivative. Taking the derivative with respect to thetaik so this is this is uh the algorithm you you you do the same thing that is compute an operation you know entrywise on the previous on the previous vectors so here of course So, here, of course, my convention is that, and the idea is that This is just the vector with entries f t of theta 1i theta sorry ti. Okay, Andrea, I think it should be the partial derivative with respect to theta s, right? Right, right, sorry, sorry. Good good point. Likely. Okay, now list it up. Test it up. Otherwise, it would not depend on S, of course. Right. Did I make any other mistakes? Okay, so I will not. I think you lost the T2 when you erased, but yeah. Oh, yeah, sure. Okay. Okay, now perhaps it's approximately correct. So I will try to describe later. I will try to describe later at the end of the lecture where this special structure comes from, this special structure of the term that I'm subtracting. But it should be clear at this point that this is a special case of GFUM. Now, there is a special case of MP algorithm is what I call the base MP. Okay, let me define that as well. In that case, you can I take a special special form of this function. This is just the expectation of the random variable theta given. Xi t theta plus square root of xi t z equal x t okay and the x i t is computed recursively as As okay, where here expectation is respect to theta, the distribution p theta, and z with distribution Gaussian independent. Okay, so and then you know, okay, I once I iterate this, the base MP algorithm estimates any vertex. Any vertex as just the function. Okay, so there are two quantities in the algorithm: one are the theta, and the other the theta hat that are the function of the theta. Okay, now I introduced all of this thing. So this is a very general class of algorithm that. A very general class of algorithm that includes gradient descent, proximal gradient, etc. This is a special subclass. The MP is a special subclass that is a special subclass in which this memory term, this subtracted term has a special structure, and then there is a special AMP algorithm that depends as a special form of the nonlinearity. Non-linearity. Okay. And now the theorem that I stated before, in this case, at least a consequence of this theorem is that basically for any such G F Here, you know, a metric that you can use is the angle between the two vectors, theta naught and theta zero. So, this is the cosine of the angle. You want it as big as possible. And this is what this is the bump. This is the dump and this is you know if I'm not my calculation wrong is square root of certainty. Okay, so there okay now let me let me comment on why I chose this rank one estimation problem instead of the regression problem to explain. Instead of the regression problem, to explain things is because all of these equations become simpler, right? And in particular, if you look above, instead of multiplying by x and x transpose and having two sets of vectors u and b, I have only one set of vector theta. Okay, so the theorem says that bump is the optimal G F and I can predict its value. Okay, so this proceeds in T step, the proof. Proceed in this step, the proof. The first step is a reduction. And by a reduction, what we mean is the following, that it's a lemma is for any GFUM, and here there are some technical assumptions about continuity of the update function that I will not state, but let's say. function that I will not state, but let's say that it produces a sequence of iterate theta bar t. There exists an AMP algorithm that produces, you know, the t. And by this I really mean there exists a sequence of function f t and there exists a sequence of functions phi t from say R T to R such that Such that what theta bar t is equal to phi T so in words this is saying what that NEGFUM is equal to an AMP algorithm plus Algorithm plus some post-processing. The important fact, of course, is that this function phi doesn't depend on x, right? So it's a deterministic function that doesn't depend on the data. I will not go into explaining this lemma, but it's basically a recursive construction. You take basically the lowercase f that are the same as the uppercase F1. As the uppercase F1, except for a change of coordinate, and then you choose the function phi to correct for the difference in the other function. Once you know that the lemma is true, then it's just a matter of matching the two iterations and choosing a function. And you prove that at each step you can find such a function phi. So the nice fact about this is that now I'm reduced to analyzing this. Reduced to analyzing this AMP algorithm. Now, the second point is that the analysis of AMP. And let me write a theorem now. I brought this theorem in another form for regression, but let me write it here because it's more explicit. And so fix any t, and then what you look is the empirical distribution of the true value and the iterates. And the claim that this converges to the law. To the law of theta. Of course, the true vector converges to theta by assumption, and then a sequence of noisy version of theta, where this random variable are defined like this. Find like this Z one Z T and let's say theta are let's say mu P theta called P theta the log theta times normal zero times a certain covariance. This is a T by T covariance that I'll think as the first T by T submatrix of The first t by t sub matrix of an infinite matrix. And now this coefficient, so now this law is completely defined once you give me the coefficient mu1, mu t and the matrix q. And now I'll tell you how to compute this. You compute mu by the following recursion. And you compute Q by another recurve. This is okay, let me write it like this. Perhaps it's easier to read. Okay. So we So, we can precisely say what is the limit behavior of this algorithm in the limit n to infinity for any t fixed. Now, at the end, you know, towards the end of the lecture, I want to, you know, I'll describe where this theorem comes from. And, you know, for this, it's very crucial that you use the right term that is subtracted. But what is interesting here is that this fact that you know. This fact that you know, in general, you wouldn't expect that the iterates of your algorithm are basically Gaussian modulo this theta term, but in this case, there is a Gaussian limit for this special algorithm. Okay, so let me, you know, I want to describe two interpretations. You know, one is that is very simple of this theorem, and another that is a little bit less straightforward. You know, less straightforward, but important. So, the first interpretation is a relatively simple one. Okay, in these interpretations, I will think of theta if you want, equal to zero identically. And the first interpretation is related to the following simple fact. If d1, take two vectors. B1 and V2 that are vectors independent of x that in this case is equal to w because theta is zero. Then if I compute the two vectors z1 and z2 via z i equal to x times vi, suppose that I compute z i in this way. Compute z i in this way, and suppose that I have that the matrix converging to some q. So basically, these are large vectors and they are. Large vectors and their scalar product of a limit, then what you obtain is that the empirical distribution of these two vectors Z1 and Z2, this is very simple exercises, just first class improbability exercise converges, for instance, in W2 to a Gaussian. W two to a Gauss shell. Okay, so if I take a single vector and I hit it with a GOE matrix, I get a Gaussian vector, a vector with approximately Gaussian entries, and I take two Gaussian entries, and I take two vectors and I hit them with the same Gaussian matrix, you get jointly Gaussian vector. Now, in our case, what we are doing is that we are taking, I remind you the algorithm, we are taking a matrix and hitting a function of the previous thing, and then we are subtracting this term, this that we call the Onsager term or the reaction term. Sager term or the reaction term. Now, you wouldn't think that this simple lemma or this simple remark applies to this setting because this because sorry, this and this are dependent. So, this simple remark that I just made about the vector z's and v's seems to be completely. And v's seem to be completely irrelevant because x and f are dependent because you computed theta 1 theta t using x. Okay, so they are completely dependent. There is no way that this simple lemma is going to apply. And in fact, in general, it will not apply. But because of this subtraction term, this Onzager term, because of this special term that I subtracted, because of the Term that I subtracted because of the structure of the term that I subtracted, they act as if independent. So, this is the magic somehow. So, conceptually, the theorem is saying that this update rule, the special AMP update rule, behaves as Behaves as if X and F were independent things at each iteration for any constant number of iterations. So, this is very interesting and still surprising. I have known this for many years now and I find this still surprising. Okay, another interpretation now I want to give. Or perhaps I should stop here since we are halfway and instead of rushing, see if there is. Instead of rushing, see if there is any question. Yeah, maybe I'll read this question to you. So usually for rank one matrix problems like this one, we need to do an iterative method needs something like log n steps, but like yesterday, these results per fixed t. This seems like a disconnect. Is there some spectral initialization or similar? Okay, so. Good point. Now, in reality, it depends. I can cook up examples in which you don't need log n steps, right? Suppose, for instance, that P theta, the prior distribution. So, one standard example is this distribution of the entry is half delta plus one plus half delta minus one. half delta minus one. This is sometimes called z2 sync. So you have a plus minus vector plus noise and you so in this case you will need log n iterations. Now suppose that I do something slightly different. I have one half plus epsilon delta plus one plus one half minus epsilon delta minus one right so the vector as slight So, the vector has slightly more chance to be plus one and minus one. Then, in this case, I need only order one iterations, so a large constant number of iterations, simply by setting as initialization to my AMP algorithm. I can set anything that is as a positive bias plus a plus. Plus C. So you can check that now the algorithm will converge in order one iterations. If you don't have such a bias, so if you really are interested in these problems in which there is no bias in the distribution of theta, this is indeed interesting, you can take as initialization v1 theta 0. theta zero that is v1 of the matrix x times a constant okay so now this kind of initialization for this specific problem has been analyzed and you know basically things behave as you you would imagine right so you know is again you know the state evolution uh recursion hold Recursion holds provided that you initialize it with the correlation that is associated with this initialization. And as I mentioned in the last lecture, it would be interesting to analyze, and probably it's possible, the AMP algorithm in a setting in which you directly don't use this initialization, you use random initialization, but you run log n iteration. But this so far, you know, as far as I know, As far as I know, nobody has done it. But yeah, I think it's possible. Okay, so that was a good question. So perhaps I'll go ahead describing the second interpretation. The second interpretation is in terms of trees. So let me construct a tree Tn with vertex set Bn and edge set En. And I just set en. And this is the infinitary of degree n. Okay. And I'll associate to the vertices. So this is an infinite tree of degree n, and associate to the vertices of this tree, I add the random variables. I add the random variables with distribution p theta and to the edges okay to the edges I'll draw IAD random variable of normal 0, 1 over n. So I'm meaning basically the same problem on the tree and the observation And the observation theta zero i theta zero j divided by n plus w i j okay so let me draw a picture so the picture is that okay this is for n equals three so this is not necessarily the case I'm interested in because I'm interested in large n but you have But you have an infinite tree, and the theta zero i are associated to the vertices, and x i j are observations on the edges. And now I can state a perfectly honest statistical estimation problem that is given the xij estimate all the xij estimate theta zero. Theta zero. So this is kind of the three relative of my statistical estimation problem, low-rank estimation problem. And now here is an algorithm, a class of algorithms for this. So what we'll do, I'll do is that describe a connection between the original problem and this problem. And for doing this, let's And for doing this, let me introduce a class of algorithms for this problem that I'll call message passing. These are algorithms in which you operate with messages u i to j there is one, you know, two of these messages on each edge of this graph. Okay, so let me perhaps draw it here. So, on each edge of these three, I have two. These three, I have two messages going in the two directions. And to indicate directions, I write XAJ. And these are updated in the following way. This is the sum over in the neighborhood of I minus vertex J and then I apply Ft. So this is the same Ft. So, this is the same FT of my A and P algorithm. Okay, so I define this class of updates. Now, the nice fact about it is that this is relatively simple. This is a relatively simple class of algorithms to analyze. And to explain why, let me define a sigma algebra FJ. So this is the sigma algebra generated by all the randomness. Okay, that is in the tree Tij, and I'll describe in a minute what this tree is. Three is okay. So, what is the three TJ I drew before this three. So, let's say that n is equal to three. So, n is equal to 3. So this is eternally 3 infinite. And the 3 tij is this tree here. Okay, so the tree that includes all, you know, that is, includes all the vertices that can be reached from J through a path that first visits I and doesn't come back to I. Doesn't come back to I evermore. So here is the tree on the left, sub-tree on the left. And now the remark is that the one important remark, let me write one, is that ut i to j is in the sigma algebra fij by construction. So it's, you know, I use only information, you know, at each time when I update this. When I update this message, I use information coming from these two messages. This is the way I did it. If you look at the update, when I update i to j when I update i to j I use only information from vertices L that are on this side. Therefore, from this reason, this only From this reason, this only is measurable with respect to that. And in fact, with respect to less than that, only if I call these only those that have up to distance t from vertex i is even measurable with respect to that. Okay? Because of this, it turns out, as a consequence of this, it turns out that you can analyze quite precisely, quite easily, this algorithm. Quite easily, this algorithm, because basically, when I compute ui to j the message here on this edge, I'm putting together, I'm computing an operation on two messages L to I that are independent. In other words, if you look at this iteration, at each iteration, all the summons here, all the terms here, Are independent, okay, because the corresponding trees are disjoint. So this is a sum of independent terms, and because of that, you would expect that for large n, this guy is Gaussian by central limit theorem. The messages incoming are independent because they depend on distinct sum trees, and therefore, you know, perhaps I can apply the central limit theorem to compute this sum. To compute this sum, and therefore, this guy will be approximately Gaussian. And so, this is indeed true that is observed, you know, you can prove an evolution thing for this algorithm on three. So, if I look at these messages for the iterations, For the iterations, this S converges in distribution to okay, so I should write should write and then So, because of your summing independent contributions, many of them, you can apply CLT, and turns out that you get a limit theorem for the distribution of these messages. Now, I called these guys, this limit variable on the right hand side, in the same way as I called it from my AMP algorithm. Called it from my AMP algorithm. And this is not because I want to create confusion, because those are the same random variables. If you choose the same function f, you'll get the same limit for you. Ah, sorry, sorry, sorry. Now I apologize. I've been calling this U on the trade too. I've been calling these guys use. So I hope this didn't create confusion. But the basic reason is what I said before, right? The interpretation of the state evolution is that X as independent of F. F and here is the same. Yeah, here really, okay, the way I define T actually shouldn't include the death edge. Tij actually should be defined this way. It should include this edge. Okay, so now what is the situation? We have an algorithm to do this rank one estimation problem on a finite n and you know on And you know, on where n is the number of variables, and we have an algorithm to do this rank one estimation problem of an infinite tree. Now, these are two different problems. One is an infinite tree with degree n, and the other is a finite graph. And basically, the two algorithms we proved that behave the same way. So, if we want to prove a lower bound for the algorithm on the original problem, On the original problem is sufficient to prove a lower bound for the problem on the tree. Okay, so now this is the third part of the proof, is where you finally prove the optimality of Bayes MP among all AMP algorithms. Among all AMP algorithms. And the idea is what I just said is that it's sufficient to prove lower bound on estimation or for estimation. By the way, these problems of doing a statistical estimation of infinite traces is an interesting area in itself, I think, that is a little bit, yeah, you know, okay, it has been explored in some cases like community detection, but for instance, you know, you can construct a problem on a tree that is associated to Problem on a tree that is associated to linear regression or to phase retrieval, etc. Now, you want to prove a lower bound for estimation on Tn for all message passing algorithm. Now, what we'll use to prove a lower bound, okay, I will not do really a proof, but what one can use is the fact that message passing is a local algorithm. What does this mean? It means that your messages i to j are measurable on the sigma algebra generated by all the xlm with lm in a ball of radius t around i. t around i can be t plus one or t doesn't really matter okay so at each iteration i aggregate information you know from one more step one more distance so after t-iteration i only aggregated information within a finite ball therefore i'm not doing estimation on in this infinite tree in an arbitrary way and actually i don't necessarily even know what that means but i'm doing estimation at each vertex I'm doing estimation at each vertex in a local way. Okay, now there exists an optimal local algorithm. So, you know, message passing algorithms are a special subset of local algorithms, but there exists an optimal local algorithm. But there exists an optimal local algorithm. And this is simply what? It simply estimates that each vertex i by computing the posterior expectation of vertex i given all the guys. So there exists such a local algorithm, right? You just compute the posterior expectation with respect to everything that is in the final ball. Okay, so this is a lower bound and And, you know, turns out that this can be implemented. At least, you know, for large n as a message passing algorithm in the sense that I described before. Okay, so because of that, that's not only a lower bound, but first is a lower bound that you can analyze for which you can write explicit recursion. And in fact, it's tight because it can be implemented as a message passing algorithm in the three and the four can be implemented. Algorithm in the three and the four can be implemented as the master passing algorithm on the graph. Okay, so I will not describe, you know, so all the steps before kind of described at least at high level the reason. I will not describe the reason for this last point, but the reason is related to writing a belief propagation algorithm. And actually, Michael will, in this session, in his lecture, he will explain this connection. Okay, so that's somehow at high level the proof of optionality thing. And of course, there are all sorts of interesting questions that one can try to explore here, but this is what we know so far. So far. Okay, so in the last 10 minutes, there is one kind of mystery that I didn't explain. Is why, so I stated a theorem about state evolution and I didn't explain why I gave two intuition, but I didn't explain any reason why we should expect. So, why should we? Anything as state evolution to hold okay and here for explaining that I'll use the case theta z. I'll use the case theta zero equals zero. I'll use the same problem, but you know, I'll take the case in which you don't have any signal because, okay, let's simplify a little bit the argument. And so, therefore, you know, what we are interested in is really this iteration. W is a GOE matrix, and then I subtract this carefully chosen. Okay, so what is the argument? So, there are a few different arguments. I'll describe one that I think. Argument. I'll describe one that I think it's simple. I mean, it's simple, it shows quite clear, quite easily, that there should be something like this working. And the idea is the following. Let me define gt to be the sigma algebra generated by theta 1, theta t. Okay. Notice that I can write this equation. Let me define xt plus 1 as t plus 1. As theta t plus one plus whatever is on this side, whatever is there, so that is plus sum over s one to t d t s f s minus one theta one theta s minus one. So this x is a vector and this set of equations Of equations, this equation here in terms of this vector, you can write it as xt plus 1 equals w and let me call f t the vector that you apply by, you know, let me call this the vector f t. And now I want to think of conditioning. On the sigma algebra Gt. And the claim is that this is equivalent to condition on the value of these linear equations that is on x one equal wf zero x two equal x2 equals wf1 blah blah blah xt equals wft minus 1, right? So if you give me all the vectors up to time t, it means that all I know that all these equations hold, right? So what do I have? So I can write this in matrix format as xt equals w. equal wft where for instance xt is the matrix that has column x one to x t and f t is the other matrix. Okay, so this is really conditioning a Gaussian matrix on the value, so the Gaussian matrix is W. So condition a Gaussian object on the value of a linear operator, a linear function of this Gaussian object, because w times self is a linear function of w. And you know that if you take a Gaussian and you condition it on the value of a linear function of this Gaussian, this is distributed. function of this Gaussian, this is distributed as the conditional expectation plus the projection of an independent Gaussian on the kernel of that linear operator. Okay, so in this case, this general fact is the same. You can write it as follows. Condition on Gt W is distributed as the projection orthogonal to The projection orthogonal to the columns of the matrix FT of a new matrix plus some conditional expectation the conditional expectation of W given Gt And this conditional expectation is really some function. Let's call it M, some matrix that is some function of, of course, Ft. X T and F T because that is all I know on GT. Okay, so this is okay, so this. Okay, so this is okay, so this is a special, you know, special example of the general fact that if you take a Gaussian, you know, a Gaussian vector, and your condition of being on an affine space is equal to the conditional expectation plus the projection on the kernel of an independent Gaussian. Therefore, if I look at theta t plus one, I can write it as p, you remember the equation here. Remember the equation here. The update equation was this. So now let me replace W by the new expression for W and I'll write and then this is F T. This is really F of the previous. Really, f of the previous thing, and then there is plus this matrix M of X T F T applied to F T minus one minus sum over S one to T of D T S F S minus one. So, this is just an identity. This is twin distribution, if you want. It's just an identity. But now this, it's quite clear the way to go, right? This is, you know, I think it's quite elegant. I can say it because it's not my, you know, first of all, here, what you have, you know, this vector. What you have, you know, this vector is this vector that shows up here is a Gaussian vector because W is independent, Wν is independent from the past. So the little lemma that I stated before, or the little remark, you take a fixed vector, you hit it with a geoli matrix, you get a Gaussian vector. That lemma applies here. Okay, so the solved vector is approximately Gaussian. Approximately Gaussian and is a vector in n dimension. Now I'm projecting a high-dimensional vector orthogonal. This FT, this projector is the projector orthogonal to the column space of F. So it's a projection on a space of rank N minus T. Okay, so this is of rank N minus T. So I'm taking a vector. So I'm taking a vector that is Gaussian in n dimension and I'm projecting out t of them. t is of order one, n is very large, this will not make any change. Okay, so I can drop this projector and I will make an egligible error in L2 sense. And then you get this whole thing. Okay? And you can make You can make, you know, you know, since this is a function of x and f, so this is a function of x and f, you can figure out that this you can write it as a sum of certain coefficients times, you know, whatever theta t and then some other coefficient. Efficient d prime. Okay, so you can express it in this form and then you subtract it. Okay. F okay, so this piece we like because you know, by induction you prove that theta is Gaussian. The previous iteration. Is Gaussian a previous iteration, and so these two pieces together make a Gaussian vector because this the first one is Gaussian. We say this, a matrix, a Gaussian matrix applied to a fixed vector, to an independent vector, and this is Gaussian because by induction up to time t, this should be s. By induction, so this is for s less than t. Less than t. By induction, the previous iteration were good. The pieces that you don't like are this one, but you see that you can choose the coefficient D as to cancel these ones. Okay, now the thing that you are left to prove basically is that, okay, all of these coefficients here, I wrote them. I said, okay, this is this is. Said, okay, this is Gaussian, and therefore, so I am assuming implicitly that this C concentrate, they are deterministic, and so this is something that you have to prove. And you have to prove that these two, you know, the expression, the special expression for D that I wrote before cancel with the special expression that I, you know, with whatever you get from this calculation. So that is a calculation that you can do. And okay, I will not explain that. But this is the basic, one basic idea. But this is the basic one basic idea while you get this Gaussian thing that you choose exactly these coefficients as to cancel this non-Gaussian part from the past. Okay, that's all, I guess. And okay, so tomorrow I'll apply basically the same thing to describe how to get an approximate ground state of Ising spin glass models. This okay, well, I'm going to unmute the participants so we can thank Andrea.