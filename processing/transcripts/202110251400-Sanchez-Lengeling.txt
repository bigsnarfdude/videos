So let me just give you a okay. Let me just give you a small roadmap. And if you have any questions, feel free to just put them in the message or just make a, you know, just call the attention. This is a small roadmap, but what I will be talking about is we're going to talk about on attribution interpretability. Interpretability, and you know, why do we care about that? I'm also going to talk about building attributions for graphs, and so, sort of, what are the techniques that you can build an attribution and kind of get what are the problems of building attributions for graphs? How can we trust attributions? And in part, how do we evaluate them? And some recommendations. And this is some work that is based on published work at NERIPS this last year. And there's a GitHub. And there's the GitHub repo if you want to explore some of these ideas. And so let's get started. On attribution and interpretability, one thing to think about is attribution is just one technique inside of a whole toolkit of interpretability. Where you might have an image of a giant panda, and you might have a prediction algorithm, and the prediction algorithm might actually say it's a giant panda. And you want to understand if your model is actually identifying or making those predictions for the right reasons. So, you can use an attribution technique like integrated gradients, and it might highlight different parts of the images that are more important for prediction. And in this case, it's like highlighting some. Case, it's like highlighting some parts of the fur, and you're like, okay, maybe it makes sense. You can do the same in text. In this case, it's like a sentiment analysis, and you want to figure out which words have stronger positive value than other ones are more negative for the feeling of the text. And in general, there's many reasons why you might want to do this. There's three possible reasons: is one, you want to build more trust for your algorithms. You might want to build Your algorithms. You might want to build credibility and you might want to build debuggability for your models. There's other reasons why you might care about this. And one of the other reasons why you might care about this is for scientific discovery. And so one example that is close to where I work at is molecular nullfaction. And one of the things we care about is identifying mechanisms and patterns that kind of allow us to make new scientific discussions. Allow us to make new scientific discoveries because we're trying to understand so we can create new knowledge. And so, a typical problem in a very basic problem in olfaction might be for a given molecule, can you predict what it smells? Olfaction is a chemical sense. So if you can understand molecules, you might be able to understand olfaction. We have released a data set and we have a preprint about this. And this is, and so in this particular case, And so, in this particular case, we have a binary task. We're trying to predict if it smells like rose or not. And if you look in literature, you might find this historical rule called Boolean's rule. And this is from 1973. And sort of what people would do is they would just grab molecules, they would create hypotheses of why these molecules smell at certain reason. They would identify patterns and they would say, you know what? When you have this group that is yellow, green. That is yellow, green, and purple right here on the slide. Then, so if you have a functional group, a carbon chain, and this group we call F, then it's going to smell. And so he came up with this rule. And, you know, it's a prospective, it's a, it's a, you're kind of dipping into the test set, right? You, you, people will come up with new molecules and you will, maybe they might not actually follow the rule or not, but it's, it's a good guiding principle to see if it works. If it works. And so, one way you can think about it is: how can we actually find rules like these and data? If we have data, can we actually figure out, reverse engineer these rules? And just a small aside and why this relates, you know, one of the reasons why a problem like this might relate to geometry is one of the things we're trying to work on is learn a geometric space or smell. By geometric space, we mean like, you know, if you think about RGB, like. You know, if you think about RGB like colors, you have three dimensions. We're trying to find a high-dimensional space where we can put smell, and we can say, you know, dimension one is for grape and another dimension is for lily. And this is kind of what we've done using graph neural networks. And so if you want to know more about that and how that relates to geometry, you can look into that preprint. So, you know, this is a little bit why we might care about attribution and interpretability. care about attribution and interpretability. Let's look at how we can build attribution for graphs and sort of how do we do it in a linear way. So one way of building attributions in a linear way is let's imagine we have it might be first hard to think about how to make an algorithm that works on graphs. So what we can do is we can just vectorize a graph. And the way that we vectorize it is we look at subgroups. And if it does have that subgroup, we put a one and if it doesn't. Group, we put a one, and if it doesn't, it has a zero. So we have a fixed-length subgroup vector that represents the entire graph that's counting the presence or absence of groups. And we put this to your favorite machine learning model. In this case, it's a graph, a generalized linear model. So logistic regression. And, you know, so if we put this in equations, it's, you know, you put, you have a weight matrix, you're multiplying it by your input, and you pass that to a sigmoid, and it's going to give you your predictions. And you can already. And you can already interpret this with a linear model, right? You can look at the weights of your logistic regression, and the weights are going to tell you a little bit of what is more activated for the prediction. And so you can already kind of interpret it with a linear model. And, you know, if you only care about linear models, then this is the end of the story. You're done. This is a really good model. It's pretty interpretable. You can always look back at the attribution, which in this case is just the weights. Which in this case is just the weights, and go back to the molecule and figure out which are the parts that highlight the molecule or the graph. One of the potential issues that you get with these attribution techniques, and just in general, with like anytime you're trying to learn on data, is that sometimes statistical patterns in your training set can affect your predictions and it will also affect your attribution. And so you can have scenarios. And so you can have scenarios, for example, here we have a subgraph, which is highlighted in red, and we have a bigger subgraph, which is the same subgraph, but an extra element, an extra node. And we could call that a spurious correlation, but you could think about it in other terms. But essentially, every time the red one appears, the blue one appears. And if you have that in your training set, then you might not realize it, but your algorithm might learn that they are. It but your algorithm might learn that they are both correlated. And if you have a test set that doesn't that have a molecule without this blue element, it might actually predict if it's has a rose smell or not. And so this is a spurious correlation, right? There's biases in our data. There can be biases in our label, and it's all because of statistical patterns in the data. And this can be due to how we split the data. It can be due to just like having a sub-sample of the data. Having a sub-sample of the data, there's many reasons why you might have this. And so, even if you have a really nice model with linear regression or logistic regression, you can have this effects in your data that are going to change your predictions and your attributions and explanations. Another possible thing that can happen is your model is not going to be perfect. So, you're going to make your model is not going to have like 100% accuracy. And so, we're also likely to have inaccurate. Likely to have inaccurate attributions, right? You might expect that an imperfect model or a model that predicts as well as random would also have random attributions. And so you can imagine thinking that the higher quality models might give higher quality predictions or attributions or explanations and vice versa, right? And so that's one of the things. You never have a perfect model because we don't have perfect models. Model, because we don't have perfect models, you're going to be like, How can I build a more perfect model? Can I get a model that has higher accuracy? Because this probably might lead to higher degree of explainability or better, more faithful attributions. And so one way you can think about that is like, well, let's add an extra layer. You know, we have logistic regression. Let's add an extra layer. Let's make our model slightly more complicated and we'll have better predictive performance. Performance. And so one way we can do that is, you know, you decide an extra linear transformation. So, you know, in case instead of one weight matrices, we have two, and they're like separated by this non-linear activation. And so here you can't do the same trick that we did with the linear models. But what we can do is we can use the gradient of this model as a proxy for importance. We think that regions that are more important, there's Regions that are more important, the gradient is going to be higher. And regions that are not important, I know, the gradient is going to be very small. So you can think about your attribution as form as a gradient. And this gives another issue. We're trying to make our model have higher predictive performance, but we don't have direct access to the weights. So now we have to have an indirect way of getting these attributes. indirect way of getting these attributes. So this gives you an in, it gives you an imperfect attribution technique. So we keep on adding approximations and you feel like you're slipping, having more issues every time, right? So this is just something that happens and you just have to work around it and figure out what's the best way to ameliorate them. And we were talking another way of thinking about a more perfect model for graphs. More perfect model for graphs is let's actually process the entire graph. And you know, so the trick has been converting a graph into a vector. And so we want to learn transformations that allow us convert graphs to vector. And one way of doing that is a graph neural network. A graph neural network is a neural network that works on graph structure data. If you want to know a little bit more, we wrote a distill post. It's one of the last distill posts that came out. It came out a few months ago. So there's like an interactive. Months ago, so there's like an interactive component where you can like play around with different parts of a graph neural network and where we kind of slowly build up all the different parts of a graph neural network. So, if you want to see it, it's right there, gentle introduction to graph neural networks. But to give you a small walkthrough of what a graph neural network is, you start with a graph. You have graph neural network layers, so just like a normal neural network, deep neural network. And at the end, your output is also going to be a graph. And at the end, your output is also going to be a graph. So, it's input the graph, output a graph. And then we can apply our favorite algorithm on any part of the graph and use this for prediction. In particular, because we're trying to predict a property of a single graph, for example, in rows, then what we're going to grab is we're going to grab what is called the global context of the graph. So it's a vector that represents the entire graph. Let me tell you a little bit how. Let me tell you a little bit how these graph neural network layers look like. So, we have a graph, in this case, there's five nodes. And what we do is for every node, there's a for loop, but for every node, we essentially look at all the nearest neighbors and we pick out its embeddings. So, every node has a vector associated to it, and we pick the collection of these. And because we have a variable number of nodes, Of nodes, we need an operation that is permutation invariant. So, one way of doing that is sum. So, you just sum up the nodes and the embeddings. And this is going to give you a new embedding. You pass it through your favorite transformation. In this case, because we want to optimize it, we can do a multi-layer perceptron, linear layer. It can be whatever differentiable operation you care about. And this is going to give you a new vector, and you update your. Vector and you update your graph with these new vectors. And so, this is going to, in a way, going to give you a new representation of your graph. You're learning the representation of your graph across different layers, and you're getting a more updated graph that you can use for prediction. And this works because when we put all the different layers, the information starts to propagate, you know, from closer information to more farther apart. And we're learning. Farther apart, and we're learning how to do it step by step. And so, you know, this is just showing the pipeline of how you get to your prediction. Because we care about getting a gradient, you know, this is one of the techniques that we had with attribution. We want to get a gradient because we can use a gradient as a measure of importance. You can grab, you know, you could just differentiate it. And what you're going to get is a gradient that looks like a graph. And so whatever part. And so, whatever parts of the graph that you're interested in, you retrieve elements from it. And so, the main element that we might care about for a global property of a graph is going to be what is called the global context. So, it's like it's the vector that represents the entire graph. But if you care about nodes, you just pick out the information on the node level. And so, by having this way of accessing gradients, you get a lot. Accessing gradients, you get a lot of ways of building attributions for graphs. There's many techniques. This is just like a small snapshot of them. There's many ways that you can do it. You don't also have to do it with attribution. There's other ways that you can think about building explanations for graphs. Some of the techniques, you know, some of these involve, because there's different layers in different steps, you just pick out the gradient at different steps and you transform all these different layers. Another way is you can also Other ways you can also integrated gradients is a way where we integrate the gradients across from the beginning and the end, and we have a counterfactual. In such a way, it's kind of like making a contrast between an input and another input. And we're looking at the changes between both of them. There is also something called smooth grad, which you just like essentially make perturbations on all. On all your techniques, and you collect the perturbations and smooth them out. And there's also gradient-less techniques. For example, we can use something called CAM. And with CAM, essentially, usually what we do is we do the same trick with a linear layer. We express all the prediction in terms of nodes, and then we just look at right before prediction, we look at the values inside of these nodes. If you want to know more. If you know, if you want to know more, there's a lot of literature around how to build different techniques that give you different things. But this is just showing you a little bit of the buffet of different things you can do. And so we have an idea of how to build attributions on graphs. We saw that there's many issues that can come out. You can have spurious correlations, you can have imperfect models, and you can also have imperfect techniques. And one of the main techniques for building and figuring out attribution is looking. Distribution is looking at the gradient of your predictive model. So now let's look at, you know, how can we actually trust these models? Because if you're actually trying to discover something, you know, and you know that there's all these issues that you can have, you kind of want to have a sense if you can actually trust these techniques. And if you make a discovery, can you actually, how much can you trust your discovery? And for that, And for that, there's sort of what we worked out as a way to evaluate attribution in different dimensions that you might care about. So in this sense, graphs and graph neural networks can serve as a testbed for attribution. So what we can do is we can generate synthetic tasks that are graph-based. And the good thing is we can actually figure out what is the ground truth label and attribution. So we can just And attribution. So we can generate something like, and we can generate something like: does this graph have a five-ring subgraph? And so that can be the task definition because we can do this computationally. When we have a graph, we can look at does it have it or not. And so this gives us a label. It's a binary label. But we also know which nodes actually activate this task. We know which of these nodes are going to give us a true or a false. So when we have a So, when we have a technique, we can throw our favorite technique, and here we have two techniques, and we can compare, and we're not going to get like, you know, zero and ones, but we're going to get soft values, and we can compare which ones of these match closer to the ground truth attribution. So it becomes another like task, right? Instead of trying to predict a label, we're trying to see if we can actually match the attribution. And so, this allows us to put an And so this allows us to put a number. And one of the numbers we can use is attribution AURC. AURC is area under the receiving operating curve. So you can think about it as a binary task. We're comparing it node to node-wise. And because sometimes you can have multiple ground truths, like sometimes a graph can have multiple rings, you want to consider all possible. Want to consider all possible explanations of that label. So, we actually consider when you calculate this metric, you do it over all possibilities and you get the max. And so, you know, there's many axes and many things that can go wrong when you're thinking about, okay, is this model actually telling us that things are right? So, one of these axes to explore is just the effect of the model. And in this case, we're Effect of the model. And in this case, we're exploring graph neural network architectures. So it can be graph convolution, graph attention, message passing, or graph nets. All of these have different styles of passing information. You can also look at the techniques. So the techniques are the ones that I just showed you, which are different ways of integrating the gradient information or just general ways of pulling information from a model. And we have different ways of different tasks. Of different tasks. So we have these tasks where you try to predict: does it have this element or not? And we have Boolean logic of these elements. So you can have, does it have this subgraph called fluoride or this subgraph called carbonyl? Does it have this subgraph called unbranch alkane or this other subgraph called carbonyl? And you can also do negations and you can do oars, right? And we go up to four subgraphs. graph logic expressions. We also have a regression task that involves subgraph subgraph elements. And this is based on a tool that is generally used in the pharmaceutical industry called Crip and Log P. It's used for predicting solubility. We also have node level tasks. And so this is where you actually try to build explanations based on the nodes. So instead of having So instead of having multiple graphs in a data set, you only have one graph and you have several nodes. And so these are based on work by GNN Explainer, where they introduce a few synthetic tasks with a few like community type of subgraph type of explanations. So you're trying to predict, do all the nodes hit the community you were expecting? And then you have two community types, and there's also sort of like And there's also sort of like a tree structure, and you have you're trying to find out if your attribution method can pick out the grid. So, there's many ways that we're trying this out because each task is going to have like its own intricacies. And so, what this allows us to do is kind of study attribution in a sort of controlled setting because we have access, we have control over the label, we have control over the data generating mechanism. Generating mechanism and also the label generating mechanism. And so we're looking at this through four different types of concepts. These concepts are based on this work of perturbation-based explanation of prediction models. So one very intuitive thing is how accurate is our attribution? So accuracy is just how well doesn't match the ground truth. So this is a very straightforward thing. And so here we have like Cam doing really good, but Grad Cam, uh But GradCam, slightly worse, right? And so this just allows us to rank things. We also have this concept called faithfulness. And so faithfulness is, and this is one of the more complicated concepts, is how faithful are attributions to a model's predictions. So the way we work that around is if a model is perfect, you know, are the attributions faithful to this perfect model? And we can also degrade the. And we can also degrade the power of our model slowly in a controlled way and see how the attribution quality degrades. And so we're trying to see how faithful the attributions are to our predictions. Another one is consistency. And so this is where we have a graph. And if we have very similar models, so you could just think about the same model of just different random seeds. Does it give you the same attribution? Same attribution. And so it's like, how consistent are these explanations? And so we test how much is the variability of these explanations when they're very similar. And the other thing is stability. If I have a transformation over my graph or my input, and I know that it's not going to change the label, how much does the attribution change? How stable is it? If I have a transformation that's like, you know, that doesn't affect anything at all. Like you know, that doesn't affect anything at all, then maybe the attribution should not change that much, or very so. It's testing the smoothness of the attributions and/h how stable they are. So, we evaluate these things and we set up experiments in each of these four axes to sort of be able to evaluate attribution because there's many dimensions you might care about. And it's not just like this number is higher, but there's different failure modes. And so we want to try to account for as many as possible. Account for as many as possible. And so let me just tell you a little bit about accuracy. What we did for accuracy is we train models that are almost perfect. So the AURC is close to 95. And we take all the values for average 10% of the performing models across like 195 hyperparameter combinations and seven random seeds. So all the numbers you're seeing, like we have confidence intervals for them also, but Confidence intervals for them also, but just for the sake of simplicity, like I'm not showing them there, but they're also in the paper. But this is one of the more challenging tasks. But something you can already see is that, for example, a random baseline is 0.5, which for AURC means it's completely random. And something that's surprising is you can already find very, you know, published techniques like GRADINPOT or smooth grad that are close to random. So our model, even though it Them. So, our model, even though it's predicting perfectly, it's like it doesn't know anything, or it can't provide us attributions that are useful at all. The strongest technique that we have is in this case, CAMP. So it's this technique that allows us to express the prediction directly in terms of nodes. The other technique is integrated gradients. And so, both of these tend to give us higher numbers. And something we also see is that the One thing we also see is that a major pattern that we've seen is that more simple models give you better predictive performance, or not predictive performance, better explanatory performance. So if you really care about explanations, you need to make it as simple as possible. And in this case, the hierarchy of how complex a model is, it can go from graph convolution to message passing to graph nets. GraphNets is the more complicated one. The other things we also look at is just like, you know, when we're training these models, how does the attribution quality like degrade? And so here we have here we have like a figure where we're training the model from 0 to 300 epochs. You can see that around 35 epochs, we already achieved almost close to perfect accuracy or perfect AURC. And you can see that even though you're almost perfect, the model predict explanation performance is actually degrading. So you can think, which is a little bit scary because it just means that you can train a model and if you leave it five more epochs, it's explanatory power can degrade. It could also go up, but what it's telling you, it's not tied entirely to the prediction performance. Entirely to the prediction performance. And so we have this like dip in all the attribution techniques where after a while it goes down. And so that's more due to overfitting. When we add consistency, we can see the performance of untrained neural networks. And so you can ask the question, how good can a model, an untrained model, how well can it give us predictions? How well can it give us predictions? You know, it's a good baseline. And we find that you can already find really good models that are like predict that their prediction performance is like already like, you know, pretty random, like close to 0.5, so not that great. But their explanation performance is also can be pretty good. So what that tells us is graph neural networks already have a bias. Depending on how you initialize it, you can also. Depending on how you initialize it, you can already find biases and you can already find some structure that already gives you better predictive performance explanation performances. So that was interesting. You can find randomly initialized graph neural networks that do really good explanation, but really bad at prediction. We also, for example, look at the effect of regularization. So, you know, how much does our attribution Does our attribution vary when we change the regularization? And we find that also for when you regularize your model, you also tend to sometimes have degradations. Sometimes you have these effects where it actually improves it. So, you know, it's a little bit of a mixed bag. For faithfulness, what we did is we have our model, we have the labels, and we can add noise to our labels. And so this allows us to sort of And so, this allows us to sort of allow the algorithm to predict on a task that has noise. And so, we can vary how well our model can predict. And so, on the x-axis, we have the predictive performance of our model, and it's also the cap. It's the most the model can do. So it can predict perfectly to random. At the end, we randomize all the labels. So, it's just a random noise that you're trying to learn. And we look at the attribution performance. At the attribution performance, also on this axis, and something you can see, for example, with a technique called like grad input, the correlation between predictive performance and attribution performance is close to zero. So how good your model is doesn't give you a good indication of how well your attribution technique is. The attribution performance falls down really fast. Other techniques are more robust. So CAM, for example, you have a stronger correlation between these two axes. Correlation between these two axes. And so, for example, with SCAM, you can sort of, you know, if your model is less, has a predictive score of 0.9, you know that it's going to maybe fall in a kind of linear way. Not perfectly linear, but kind of linear. We do this across different tasks. This is just like a snapshot across one of the patterns. Another way that we look at this is introducing artificial. Is introducing artificial spurious correlations. So we have a graph and we have two subgraphs that we care about. In this case, we have the subgraph P and Q. So piperidine and benzene. And we have this extra concept that is higher level, which is P and Q. Does it have piperidine and benzene? And so what we can do is we can create data sets where we can sort of slowly introduce this artificial spurious correlation. Spurious correlation. So we create a data set where we only have P and where the test set doesn't have like the both of them. And we can slowly change how much of P and Q co-occurs on the training set. And we evaluate how well does it predict its attribution performance on the P part and also on the P and Q part. And sort of what we see is that graph neural networks, when you add, especially these type of When you add, especially these types of models, and also when you have some spurious correlation, they will latch onto it really fast. In our case, only having around close to 20% of the spurious correlation, the model would already latch onto this extra correlation, right? And we're considering part of it. And so, what this tells you is when you're building explanations with these models, these spurious correlations. These various correlations are going to affect and can affect if they're present to a certain degree, in this case 20%, they can affect your explanations in a strong way. And something else that you might look at is stability. And so what we do is we perturb our graphs and we consider all the perturbations possible. We have like for each graph, you create 5,000 perturbations and you look at, you know, how much does the attribution change? How much does the attribution change when these perturbations do not change its label? And so you can see that for like random, for a random baseline, that the variance is pretty high. But our most stable model is, for example, CAM and integrated gradients. The explanation does not change that much. And the label is not changing. So you might expect it not to change. It all depends on the context. In some cases, you might actually think, expect it. Think expect them to change. And, you know, so this is just the recap, right? We talked a little bit about attribution and interpretability, we might care about it. We looked at how we could do it for graphs. And, you know, what are some explanation one axes that we might care about? And just some recommendations at the end. And so some very take-home messages that we learned during this process is simpler models give you stronger attribution. Give you stronger attribution performance. So, if you actually care about building, trusting your models being more credible, simpler models are going to be better. CAM and integrated gradients have the strongest attribution performance across tasks and attribution qualities. And GradCam was a runner-up when you think about node level tasks. So, anytime your prediction is focused only on the nodes of the graph. Is focused only on the nodes of the graph. And, you know, if you want to, if you have a new attribution method, you should check out the benchmark code base that we set up so you can like hook in your attribution technique and see how well it does. It's a tool that allows you to like try to, in a quantitative manner, develop it to be stronger. And something that also very important is that baselines, either rather more historical or very important because they kind of allow us to contextualize any results you have. To contextualize any results you have. So, like exactly what I was showing with the smell rules, being able to compare those smell rules when you have a new discovery is very useful. And some of the surprises are untrained random initialized neural networks can have really strong attribution performance. And why we don't know exactly. It might have to do with inductive biases. And attribution performance can vary in the grade. Distribution performance can vary in the grade. And this happens even if the predictive performance is perfect and static. So, you know, the explanatory power of your model can actually change, even though your predictive performance doesn't change in a significant way. One of the reasons why we think integrated gradient in CAM was really good is because you can think about it as being very explicit about the CAM. Explicit about the CAM technique has a very explicit interpretability layer. So, when you express your prediction, it's directly in terms of nodes. So, if you really care about these things, you should really bake this inside of your model. So, this is like a major theme that's going on right now a lot in deep learning is like, if you really care about something, if you care about symmetry groups, if you care about certain operations, you should really put these in your model instead of just like trying to learn it all. Like trying to learn it all. You should try to bake in these biases. And so, if you care about interpretability, making parts of your model interpretable is going to probably give you more interpretable models. And the other insight I think is that. The prediction, the attribution is radians, and in CAM, you can actually approximately to your prediction. So, having this global property in your attribution technique might actually be one of the reasons why. And, you know, this is the work of many people. This is just, you know, a photo of many of the people that were involved in some of this work. Adam is part of the distill post. So, yeah. Yeah, if you have any questions, you know, you can send an email. You know, you can send an email. You know, there's a PDF. I ended up 10 minutes early just in case there's any questions. And, you know, yeah, that's it. Well, thank you very much. That's a fascinating talk. Yeah, so I guess there's an ample time for questions. So anyone who has any sort of question right now is most welcome to go ahead. Go ahead or in the chat as well. And maybe while they're thinking about it, I wanted to ask you what's the part of this work that you've enjoyed the most? Because you clearly like thinking about these problems, but there are a lot of different directions and a lot of relationships to previous work of yours, I guess. So, what is this? Yeah. So, just for me, it's, I think, like, we want to get to scientific discussion. Like, we want to get to scientific discovery. Like, we really care about this mission about building a geometric space for smell. And it's when you're using these deep learning models, it's like sometimes you're not sure of why they're doing it and you're trying to understand it better. And so for us, jumping into this rabbit hole of attribution and evaluation is because we want to actually build these scenarios where we can be very certain about it. So, like, I think the funnest. So, like, I think the funnest part is actually going to come after this work. Like, right now, we're actually, you know, now that we know and have some ideas of how to build new scientific knowledge out of it, we actually want to try it out. So, so the funest part is still to come. Yeah, this was more like work to like, okay, we want to make sure we're doing things for the right reasons. I see. As usual with research, it's like you always want to go to the next step. So maybe. Step. So maybe another opportunity for anyone to ask a question. You can open your mic or just drop it in the chat. And if not, then I would want to ask if you could explain a little bit more about this geometrization of, I guess, olfactory senses or smells or the old space, or I don't know how you want to call it. Yeah. Yeah. Want to call it? Yeah. Yeah. Yeah. I mean, so let me let me see if I can let me just check if I have any slides around that. I think you had one slide that had some. Yeah, let me go to it. Yeah, so yeah, so one of the things we're trying to do is, and this is kind of directly related to graph neural networks, is, you know, one way to think about You know, one way to think about graph neural networks is a way of transforming data that is graph structured and that you're learning a representation of your data. And if you extract, you know, and one way of thinking about neural networks in general is you can think about it as this entire pipeline that will transform your data into a vector. And then you just have your favorite generalized linear model. And if you look at the vector right before prediction, so right before you pass it through your generalized linear model, you can look at the embedding space, right? The embedding space, right? So, this is a penultimate last layer of your network. Because a network is based on a linear model, then when you make predictions, like in the case of classification, you should see islands, right? You should see islands between your different classes because it has to be linearly separable. So, because it's linearly separable, you know, you should, and this is assuming you're pretty. And this is assuming your predictor is good. If your predictor is good, it should be able to linearly separate them. And in the case of smell, we're actually predicting multiple things at the same time. We're predicting musk, lily, cabbage, grape, muguay, lavender, jasmine, sauberry, beef, roasted. So we're predicting close to 138 odor labels at the same time. So you can think about there should be 138 islands somewhere if you're actually able to predict them well, because each of Able to predict them well because each of these are linearly separable. And so, what we're doing here is we grab the graph or graph neural network embedding. The embedding is 63-dimensional. That's what came out of the hyperparameter optimization. We do a PCA to just like visually look at them. And so we have this 2D space that we can look at right now. And we're not doing UMAP or anything. So it's just a principal component. So it's linear transformations. Linear transformations. And you do find these islands, right? So you have the musk island, you have the cabbage island, you have a lily island, you have a grape island. And you can do this for all the odors, and you can actually find these islands. And there is also an odorless island that is like disconnected from all the other odors. And one of the more interesting things is you can actually find a hierarchical structure in these islands. So if you have a macro label like floral. Label like floral, it actually has and has other labels like mugay, javender, and lasbin and jasmine. And you can also have a macro label like MIDI. So the co-occurrence in the structure that we did not give explicitly in the labels gets extracted in these spaces. And so it's a very open question, like, how do we study these spaces in a more quantitative way? Because, you know, how do you study? Because, you know, how do you put a number on these like clustering? How do you put a number on these spaces? Because right now we're only doing it in a hand-wavy way, right? It's 2D, it's a PCA, so you're just like eyeballing it. But you really what you would want is computational tools that allow you to like quantify the quality of your geometric space. And you know, this is for smell, but you could think about it for other prediction tasks. And, you know, there's interesting work around changing these spaces, for example. Changing these spaces, for example, instead of a linear regression, you could put a Gaussian process, and that changes how your model makes predictions and how it creates these surfaces that have to be linearly separable. Yeah, I mean, that's just one idea. But yeah, we're trying to build these vectors that kind of represent smell, and you can use them for other smell-related tasks. So, this reminds me of the flavor network of Barabasi and collaborative. And collaborators that also used chemical ingredients, well, chemical compounds in the ingredients. Like that would be another way of enhancing this sort of smell, because I guess flavor is actually something that's inferred from smell. So I don't know if you'd considered using that other network representation. That one's interesting, maybe, because they've Interesting, maybe, because they've organized it in a hierarchical way by importance so that they can also kind of separate them. I don't know if you're familiar with that work of theirs. Yeah, I've seen the work where, yeah, like flavor and smell are like intimately related. And they deal with molecules in a different way, right? They use this vectorization that I talked about, the sub bag of subgraphs. Yeah. Bag of subgraphs. Yeah, I think, you know, like it'd be interesting to actually have you want to have the graph neural network because it learns a really rich structure of the graph and it allows you to have a really nice, concise representation. But then you want to embed it in this other, into the other context, right? The context of how it's used in recipes and, you know, the proportions and all these other things. Yeah, we haven't gotten into that, but I think it's like it's an interesting. It's like it's an interesting way to combine these different ways of modeling different phenomena. Yeah, I like that work because, in a way, topology for me is like the structural underpinnings of a space, and then you build a geometry on top of that. And then that way, they've kind of already sussed out that topology of the flavor space. So, if you want to relate smell to flavor, maybe that could be something to go for. But anyway, that's kind of going off in a different direction. Uh, kind of going off in a different direction. I don't know if anyone else has another question that they would like to ask Ben. Yes, and small question. So, I'm wondering if you have think about using these techniques to maybe some more graph theoretical thing or maybe some spectral theory on graphs. Maybe, I mean, what I'm thinking is: recently, I've been studying energy of graphs and there's And there doesn't seem to be a pattern that says this kind of structure or subgraph tells you that it has a big energy or something like this. So can these techniques be used to find some patterns about graphs that gives you some spectral properties? Maybe. I think the spectral, like thinking about the spectral properties. Thinking about the spectral properties of graphs is like something that hasn't. Well, it's been studied before, right? Like the before the modern incarnation of graph neural networks, people were doing spectral operations on graphs. And that was like the thing that was coming out. But using actual like statistics of the graphs is like something that for prediction has not been explored. I think often you have to pair it to a very concise application. To like a very concise application because all the graphs can be very different, right? And they have very different properties and patterns, and you know, they have distributions of edges and nodes. And so, so, yeah, so I wouldn't know what to expect. Yeah, because there's some relation between this spectral theory and some chemical properties of the graphs. So there's there's maybe some relation that you can uh use, or I don't know, maybe. Can use, or I don't know, maybe. Yeah. Yeah. Yeah. I mean, I think it's, I think it's something worthwhile to think about how to like formulate and explore. But yeah, not sure exactly. Thank you, La. And thank you for your time. So, all very interesting questions and comments. Anyone else want to ask something at this particular time? Maybe wait for later? Maybe very briefly. Maybe very briefly, just one question. So, thanks, very interesting. So, if I understand correctly the notion of attribution, this is to try to find a sort of relevance in the input that serves to give us an interpretation for the, let's say, reasoning of this machine. Correct? Yes, yes. Like, you know, if you're giving your model three inputs, you know. Models three inputs, you know, a vector of three dimensions. Yeah. You would imagine the attribution will tell you dimension number one is the second most important. Dimension number two is the first most important. Yeah. So it's a relative ranking. Yeah. Yeah. Right, right, right. So, I mean, I know in this direction, well, of course, a couple of works. I mean, the works by Gr√©goire Montavon and Klaus Robert Mulla and the people in Berlin. Then there was the work by Gita Koutiniok that tried to take one step further from there. To take one step further from there, I know that they were also looking at some graph neural networks, and so I, you know, I wonder if you could maybe just comment a bit on what makes your method different. Yeah, yeah, so I would say it's like there, yeah, there's many ways that you can try to build these types of schemes of how is this important or not. We didn't introduce like any new technique in itself. I think what we I think what we introduced more was the idea of evaluating these techniques in a quantitative way in the context of graph neural networks. There's been some previous work about these things. Actually, David Mellis has work on that, on like, you know, some of the failure modes and also how these techniques can fail. But yeah, it's, we, our, our. uh we our work is more like trying to like can i put a number how it's failing because these things are going to fail and and we want to understand how they how they fail so that we can like actually try to patch it up or solve it yeah got it and so yeah so so so like if you add if even if it's not an attribution technique you can still apply the same like techniques to actually evaluate it so if it's like relevance propagation layers It's like relevance propagation layers, or if it's uh, you know, what are these other techniques? I can't remember, but there's non-attribution techniques, and CAM is actually a kind of a non-attribution technique that you can do. Yeah. I see. I mean, maybe I can pick your brain on this.