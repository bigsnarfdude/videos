So, first of all, I would like to thank the organizers for coming, for inviting me to speak here. So, I'm from the National University of Singapore. So, it's a great pleasure to present this joint work with Chang Liu, Xing Chao Liu, and Lu Chi Jiang. So, they are computer scientists based in UT Austin. So, I'll talk about sampling with constraints. Let me change the setting a little bit. A little bit. So, this is the thing we'll try to go over today. So, first, we will explain what this means to have constraint sampling. And then we'll talk about first to review like this method called a gradient flow. It's a different type of sampling approach other than MCMC, right? So, that's, but we just try to review how these things are done when there is no constraint. And then we'll discuss two problems. And then we'll discuss to a problem setting for the sampling constraint problem, assuming that you have a moment constraint or you have a level set constraint. So the first part is already published. The second part is also online. You can find it on open review. So the reason people are interested in the sampling problem, of course, is mostly coming from Bayesian statistics, right? So in Bayesian problem. Statistics, right? So, in Bayesian problems, you want to sample a distribution pi theta, right? So, you want to estimate some priorities theta, and then you assign a prior, and then this is your likelihood, you try to sample this target distribution, right? So, here we want to add in some string on top of this question. That is, we want to the first idea is to say we want to sample a distribution Q that is as close as possible to pi. This as close as possible to pi. But then we will have this moment constraint, meaning we have this test function g, and then we want the expectation of g under q is smaller than some thresholds epsilon. Another possible setting for this constraint sampling problem is to say I want to sample something as close as possible to pi, but we require this test function g to be taking value exactly zero on. Exactly zero on Q almost surely X. So the Q has to be supported on the level set of G equals zero. So it's adding some constraints. Of course, a natural question is why we are interested in constraint sampling problem. There are multiple possible reasons, right? One possible reason is that when you're doing Bayesian sampling, a Bayesian problem, you sometimes don't know. Problem, you sometimes don't know what is your prior, right? And then one possible idea is to say, I want to sample a certain distribution that looks like this, but then I constrain, I require that my loss function or likelihood to be larger than some threshold. I only want to look at the possible state as such that the loss function or likelihood function is higher than some threshold. Function is higher than some threshold. Another important application is machine learning. So these days we use machine learning to do a lot of recommendation system, like for example, whether a student should be admitted to a school or not. And then, of course, you worry about fairness. Let's say you have 20 years of historical data of school admittance, right, who get admitted. Right, who gets admitted, who does not, and then you will maybe you want to let the machine do the work, right? But then somebody will say your data is biased, right? So we know that for his in the history that maybe, for example, women are biased against, right? So then if you train your machine, machine learning model based on these biased data, then at least tend to be biased as well. So then one natural question is that, you know, can I fix that bias? Can I fix that bias? And then, like, a very simple, simple, simple way to fix this is to say I add in this constraint. For example, I have this, this is my model output. So given an input X, this is my auto model output, this is my model parameter. And then I have a protected variable that could be the gender of the student. And then I want the covariance between the model output. Want the covariance between the model output with the protected variable to have zero correlation or zero covariance, or at least something that is less than some threshold. So then at least like formally can restrict the set of parameters that you will be looking at. And you wish that by doing so, you can gain some fairness. And of course, like this is the application. This is the application, and as the mathematician, you are more interested to say, is this a well-posed question, right? That's what would the solution be look like, right? So of course, like after, you know, other than the constraint, we also want to gather information from the existing data, so we cannot just throw away the data. So that's why we want, so this is the distributions coming from your data, and then you want your queue, the target distribution, to be as close as possible to the Close as possible to the informative distribution. On the other hand, you want it to fit some target constraint. So, but the question is that what would the solution be look like? That's not very clear. And then second is that how do we obtain this distribution, assuming that that does exist, right? So, and then finally, for in applications, sometimes we want to know like if I have these constraints, how will that Constraint, how would that affect my distribution? Will it affect my model predictions on existing data? Right, naturally, if you want your unbiased machine learning model to predict existing data, because the existing data is biased, so you'll be losing some kind of test accuracy. But at least we want to know like how these, sometimes we want to know how these two things are, you know, depend on each other. And this is usually called a parado front because you want. Call it parallel front because you want to optimize your closeness to the existing data and then also the you know also maintain on the other direction you know look at how fair the model is and of course like the fairness has been a recent you know interest in the machine learning community and there has been a lot of works but mostly it's focused on the different approach and we're quite new here in the sense that we're doing this Quite new here in the sense that we're doing the sampling approach. So, first of all, we would like to look at the unconstrained case, you know, how to do sampling. And of course, this question has been studied for many years. And one of the major, one, one type of method, the major method to use for sampling is known as the Markup chain Monte Carlo. So, the basic idea is to you want to simulate the marker chain such that this invariant measure. Such that this invariant measure is the target distribution pi. It's fairly well understood, has been studied roughly 50 years, but it often requires the pi to be well specified, or at least like how to say a normalized version of pi. So if you don't know even what's the density of pi, most MCM Smith method will not work. And then also one issue is that the And then also one issue is that the iterates when you generate smartco chains, they tend to be dependent, right? So usually you need to wait several, maybe several hundred iteration to generate something that's roughly independent from the previous one. So also like the convergence of these typical Monte Carlo convergence, right? So we know that's one over epsilon squared if you want the estimation to be of order epsilon accurate. So that tends to be slow if you compare. That tend to be slow if you compare it to like optimization algorithms. So, there is a new initiative in the sampling community that is looking at traditional method. So, that is instead of looking at sampling problem as micro-chain magnet hollow, we try to use ideas from optimization theory. So, one idea investigated is trying to say, let's say we have a density, right? We start somewhere, let's say a resistor. Somewhere, let's say a resistor target pi, let's look like this. And then you start with something else, like Q, and then you want to push this Q towards Pi, right? And then suppose you can gather all these, you know, suppose this Q is, you view it not as a density, but a lot of particles, such that their accumulated distribution is Q, you ask yourself, you know, how do I push each of those particles such that it's moved towards? Such that it's moved towards pi. So, when you implement these methods, it's usually involving like an interacting particle system. So, that's why it's also very interesting in math. It has shown promising results on some of the problems. Sometimes it works better than existing MCMC, sometimes not. One of the real issue here is that the math understanding of these methods is actually still in the, I would like to say, at infant stage. I would like to say at infant stage. And potentially they could be faster because their optimization method, they're optimization phase methods. So maybe they can be faster than the Martin-Carlo convergence. So one of the standard way to formulate this version approach is, of course, you will need to optimize something, right? And then you need to measure how far you are towards the target. The target and your target is pi, right? So that involves like finding a metric to compare to measure distance from your current density towards the target density. So one standard measures the KL divergence. So the idea is that suppose I can gather samples from this density Qt, right? So I'm at time t and I have samples, a lot of samples from this Qt. Then what we can do. Then what we can do is try to estimate the this test function f on of that. Then we want to push each point in this qt such with certain velocity field theta t, and each point is theta t x. And then if why you really do that, then the focal plan, if you like to say, or like Planck, if you like to say, or like the continuity equation of the evolution of Qt will follow this partial differential equation. So once I set up risking up, then the natural question is that how can I choose this vector field phi such that it decays KTKL divergence as fast as possible. So then this turns it into an optimization problem. An optimization problem, and the object to optimize is this vector phi, vector field phi, or in other words, I'm trying to do like a functional optimization. So the answer is actually quite simple. That's the nice thing about these methods. So if you choose the PR diameter, so what you can do is you really can just plug in the formula and Really, you can just plug in the formula and compute how those KR divergence decay, and it is simply given by this explicit formula. And then we try to maximize, of course, the speed of divergence of decay. That's maximizing this. But the problem here is you can see it is linear in phi. So there's no optimal solution unless you are adding some constraint, right? Otherwise, it just go to infinity. Go to infinity. So, one natural thing is to add in the quadratic constraint. So, just quadratic constraint here, we're trying to do a functional optimization. So, we need to choose a norm, a Hilbert space norm for phi. And then depending on your Hilbert space, you have different kind of distribution. And you basically try to maximize this thing, right? So, for simplicity, we'll write down the gradient, the log. The gradient of the log density as sÏ€, right? So, this is called a score function in the statistic literature. So, we want to maximize this and then we penalize by the string of so depending on how you choose this filbert space, you will have different solutions. One of the simplest Huber space is, of course, the L2 space. The L2 space based on the Qt distribution. Then you can immediately see this is just quadratic optimization and then the answer of the fee is simply proportional to risk part. And if you write down this S P minus SQ, then you can actually find out the evolution of risk Q T to be simply the Fokker plot equation of the Langevin dynamics. So this is Long-train dynamics. So, this is nothing really new. It's already discovered by a physicist or coupled in 1998. So, I mean, usually when I learn about Landron dynamics, I was more of a problem and we learn it as like an STE. But then, like, you can also derive. So, here we can see that you can also arrive to the same equation, right? Right, from the optimization kind of view, right? And of course, like when you for Lantern dynamics, you cannot implement it directly, instead, you need to run it, right? So one of the simple ways is to run the Euler Moriyama scheme that's known as the ULA algorithm. So this can be seen as also MCMC as well, because it actually has some randomness involved. So, how does this thing converge? I will just go quickly because it's slow. So, actually, the convergence analysis of this algorithm is also very simple. When you plug in the angstrom, you will easily see that the KL divergence decay like this thing, right? So when you integrate the right-hand side, you will have that is less than KL divergence. So that means that if the minimum overall time will be decaying like the rate of one over T. King like the rate of one over t. Of course, your symbolist is finite. Moreover, if you're familiar with this thing called a log sub-left inequality, then can you further have this lower bound of this L2 difference? So then you can show that this thing, the KR divergence actually decay exponentially. But this is known as a linear convergence of the Langevin dynamics. And this such property can be inherited by the ULA. Property can be inherited by the UNA algorithm. And this is also a fairly new result. So that was what happened if you choose the Qber space to be the L2 space. So in machine learning, another very popular choice of the fiber space is the reproduced in kernel fiber space. So that's actually a literature by itself. Essentially, a literature by itself. But just in a nutshell, like when you have this Kubernetes base, most of the optimization problem can be solved with pretty explicit formula. That's why people, why they like to use this choice of huber space. And in particular, for us, that choice of P will be given by this formula. So you see, it has a very explicit formula. Explicit formula. So we can see this is actually a kernel-Hubert embedding of risk operator. This is also known as the Stein operator, the statistic literature. And the limit points. So when rising goes to zero, you can easily see that you have the Stein equation, which is another way to show whether a distribution follows the target, hits the target distribution. So, the nice thing about this choice of the LKHS is that when you write it down like this, when you do the integration by part, you will have this formulation. So this is actually crucial in the sense that what we want, again, we are trying to find an optimization, we're trying to optimize, and the optimization result is given by this vector view phi. So, numerically speaking, you need to able to find out what's the value of vx. Value of vx, and however, with this formula, you can see on the right-hand side, everything is expressed in terms as the moment of qt, right? And then, when I apply this algorithm, I can pretend I can gather samples from this distribution, right? And then, when I have the sample of this distribution, I can simply plug in, right? I can plug in on, or I can try to evaluate the value of this function on these locations on my sample. Locations on my sample, and then I can actually get a good approximation of the so that's if you write down the idea, it's basically given by aggressive. So it's quite different from the Lantron or the ULA algorithm presented earlier in the sense that this is actually a deterministic process, right? So of course, when you initialize it, you initialize your particles from somewhere that randomly, but after initialization, everything is. But after inertialization, everything is just random, just deterministic. So, this is known as a stein variational gradient descent method, SVGT. So, it was invented in 2016 and has shown very promising results. But I want to say in front of my fellow mathematicians that this algorithm is not so well understood in the sense that we don't really know how. That we don't really know how the accuracy depends on n, right? So, that is actually a huge issue because usually when you have a numerical algorithm, you want to say how does the numerical accuracy depend on some of those parameters. And now it's actually very unclear how does the same depends on that. So, if you're interested, learning this, to research in this, I'll be happy to do. I'll be happy to discuss. But here, when we're doing this analysis, we would just assume we can have infinite many particles. So we can actually just operate on this Qt. So we're in this fluid limit or infinite particle limits, you know, and just discuss things. So it's nice and easy. So in particular, you can show that the character divergence decay like this. So it's a particular The particular norm is called a kernel stein divergence. And then you can show the kernel stein divergence decay like one over t again. Again, like there's very little understanding what's the corresponding log log sub left inequalities. So in a sense, that can I have this lower bound. So this is actually, so some people show it's actually not correct in general, but it's there. Correct in general, but is there it can be fixable if some of the things fixed? So, I'm really running. How much time do I have? Five more minutes, including questions. Five more minutes. Okay, okay. So, I just did a review. I mean, I haven't touched. Maybe we just talk about the first question a little bit. So, early on, we said we want to do risk constraint sampling. Sampling. So, you know, before it was, if you're just doing, considering an MCMC approach, it's actually hard to identify the question in the sense that you don't even know what does it mean because your target distribution is not explicit. But now if you're doing constraint, you try to view the problem as an optimization problem that somehow is resolved, right? Because you just simply say, I want to find a Q that is close to pi such that my Is close to pi such that my constraint is met. And of course, like this is possible that you're even without your pi is already fitting the constraint, then it's very boring. We ignore that. And then if you plug in, if you do the math correctly, it's actually very easy to show the targets you shouldn't have an explicit formulation. It will always look like something like this. So this pi, and then you have this. This pi, and then you have this weight added to that, right? And then you have this constraint that has to be met, right? But this is not the end of the story, right? Because when you're doing sampling, you actually don't know what's your correct choice of lambda star, right? And then when you're running this, you're running into this chicken and egg question. Hello? Is there. Just continue. Just continue. That was just a yeah, okay. So, oh, yeah, so there you have it just checking an egg question, right? So, because if you want to check whether the equality holds, then you need samples from pi lambda, right? But on the other hand, the egg question is that, you know, in order to sample pi lambda, you need to know what's your choice of lambda, right? And of course, you can always try to solve this using a double loop, right? So, I fix a lambda, try to sample pi lambda, I just Lambda, I just repeat, and then I tune my lambda again based on numbers. That will become a so-called double loop, and usually that's quite slow. So, here we're using ideas from optimization algorithms. So I will probably be quick here. It's called a primal dual gradient flow. So, this is actually a quite general way to do constraint optimization. What you do is you add in this like round of multiplayer. You add in this like round of multiplier, and then you do a gradient descent on the multiplier lambda, and then do a gradient descent on the other variable, which is our phi. So what we can show is that, again, it will converge like before. The rate is very similar, so I'll just skip that. Another idea is to do this constraint-controlled gradient flow approach. That is to say, I That is to say, I add when I do the optimization problem, trying to find out the direction of V, I add in this constraint over here. What this has said is basically said when I move these particles, I move the density such that it goes towards the direction such that the constraint doesn't matter. I require that. And what's nice thing about this is that when you're adding this, you can actually again find out what's the choice of your fee. Of your feet, right? So, in the line dynamics case, it just looks like simply like this. SVGD, it will look slightly more complicated. So, again, you can show the convergence is roughly speaking like one over t. So that's the, of course, this is what there's some detailed difference between the solid periods. I think it would be a good time to conclude quickly. Yes, exactly. Yeah. So, any questions? So, any questions? Yeah. Okay, so let's be questions? I have a quick one. In your experience of working with Stein variational gradient descent, sort of does it work for multimodal distributions? Multimodal. So, multimodal, you have this issue, right? So, you have multi-modal, right? right uh your particles have to you you you have to initialize uh like somewhere like on both modes like or so if you initialize just over here it will not work right so the reason is very simple because this dimensional approach is basically computing it's basically trying to match the score right so this is the score function of q and this is the score function of pi right so it's Score function of pi. So if you have using, if your particle is just over here, right, everything is trying to do a single mole distribution. And then everything, and rising is measured over Q. So it's actually computing the difference of risk. If this is very small, it's happy. It says, I find a target. But if your distribution is only over here, right, then this score is very small because it looks like. This score is very small because everything is just constrained in this part, and it doesn't even know the algorithm doesn't know there's another mode elsewhere. And so the score is slow and small and it will just stop. So, I mean, if you're interested in multimodal, I would say like, you know, like this parallel tempering method, right? So that, in my opinion, that's a better method to do. Yeah, thanks a lot. Any more questions? Uh, any more questions? Maybe from the online audience. If not, let's end the speaker again. Thank you.