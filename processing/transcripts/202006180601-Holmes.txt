I greatly enjoy with Wolfgang Coober and Modern Statistics for Modern Biology. And we are happy to have her talking about, I didn't catch the beginning, using all the data, all the data, using the data, all the data, challenges. That was the new part I hadn't seen yet. Thank you. Welcome. Okay. Thanks so much for having me. I really have enjoyed all the sessions and also. Sessions and also looking all at all the data sets, and I'm very excited for the subject, of course. And I think this is a unique opportunity because, as I'll say, I think there's a real communication problem, and this is a very good way to communicate through the data sets themselves, because we all come from different backgrounds. So, I've been working on multi-table data in various guises since 19. Various guises since 1985. So I have a take that comes from my background, but I do think that we have the main problem is the heterogeneity of the data in their format and in various other aspects. So my main area of study right now is the human microbiome. And I do work with David Roman and his lab, and we have a lot of different types of data, and I'll talk about Different types of data, and I'll talk about one example there. And the other area in which I work is immunology and their role in virus response, and especially NK cells right now. I work on HIV with Catherine Blish in her lab, and that's funded by the NIH. So, what are some of the challenges that we see when working with all of these different types of data? So, sometimes we call it multi-domain data. Call it multi-domain data or multi-way data, well, multi-table data, all those different names that different areas have given us. But we have an agreement that the idea here is that we want to keep all the data together. And we're not going to slice and dice the data according to a technology. Very often, the type of perturbation experiments we do with, there's also a structure, a longitudinal structure. Also, a structure, a longitudinal structure where the data are dependent. And the difficulties have to do with the reproducibility of results, both across labs and experimental conditions, but also across statisticians analyzing the data. And one of the things which is left over, and we've seen some nice examples of modern methods, is uncertainty propagation and quantification. And maybe I'll concentrate. Quantification. And maybe I'll concentrate a little bit more on that than I thought I would because I think that it's one of the open questions which we have to address. That's the confirmatory rather than the exploratory problems with the data. So some of the paths that I found useful in analyzing heterogeneous systems have been influenced by the software, the computational frameworks of Computational frameworks available. So I started analyzing microarrays in the 90s, and we used that was a motivation for the structure of special lists with multiple components. And then it became Bioconductor. And we have the array, we had the arrays, but now we have summarized experiment, we have multi-assay arrays. So this has been a wonderful platform for keeping all the components of data. Keeping all the components of data together. On the other hand, especially in the microbiome world, but also in immunology, we often have a lot of prior information in terms of graphs or trees, which is not just matrices. And the gene ontology is an example: phylogenetic trees, we have gene interaction networks, metabolic networks. And we can use these information to modulate or Information to modulate or create special distances so that we have a structure which takes into account all our prior knowledge. Mixtures are everywhere. We never have one parametric population. So deconvolution came up, but we also have mixed membership models, which are useful. But that's a type of heterogeneity which has to do with cell populations, which is important. And sometimes we can decode. And sometimes we can decompose it in a useful way by saying there's a latent variable, which is a clustering of cell types, for instance, or you could have a latent variable which is a developmental pseudo-time or a factor. And that's been an enormous resource, this latent variable idea in explaining things to collaborators, but also in the case where you don't have exact matching or correspondence between the Correspondence between the entities which are being measured across the different domains. So, latent variables, latent factors is very good. One of the things I always tell my group is that you don't have to stress about the choices, which distance to use, how many components, which clustering method, because they are not forever. Because if you use a reproducible research workflow, you can just change these parameters and then rerun your analysis. Then rerun your analysis. One of the things you can do, though, is get back information that you've got rid of. And I've seen a lot of bioinformatic pathways, and I'm going to show you one example of that, where we had to walk back to the raw data because of this difficulty. So the throwing out information is bad. And then I, you know, make a pitch for being responsible and do recycle methods. And do recycle methods because we can't move as fast as the technology moves. But if we have a good expertise with certain technologies, often we can transfer them by using a reuse of the methods. But that's important that we understand the abstract ways of doing that through infrastructure and vocabulary. So let me just talk about the heterogeneity of the data to start with. So we often have a response variable. Often have a response variable in which case we're doing supervised methods. That could be, you know, I work on preterm birth, or that could be gestation periods, or survival in the case of cancer. And that makes the problem slightly easier if you're in a sort of regression or supervised setting. The hard problems, I think, are very much the problems where we don't have a response, it's not supervised, and then you have you often need. You often need to find hidden or latent structure, that is, variables that are not measured in the data. Now, the data types, of course, are heterogeneous. And as I mentioned before, graphs and trees are really important, but also we have maps and spatial information, even if it might be spatial information which is really spatial, or it might be abstract spatial information. And we use a lot of images, and that's one of the Use a lot of images, and that's one of the reasons that at the beginning, since the 90s, when people started talking about, or you know, the 2000 people talked about multi-omics, I said, well, no, you can't just say it's omics because you have all of these images and other types of technology which aren't classified as omics, but it's still multi-way data, multi-domain. And then, as I said, we have longitudinal data and spatial, so you have a lot of different types of dependency. Types of dependency, and we have heterogeneity in the technologies, and of course, that gives us different signal noise ratios. Now, let me come to the communication problems. That's the heterogeneity in methods. And there we just live in, you know, Babel Tower because the different disciplines have come into this and studied it in different ways. There are people. Ways. There are people that did multi-way, multi-array data from chemometrics already in the 80s. There are people actually who do sensorial analysis that studied already in the 70s. Some people call the data multimodal, multi-domain, multi-table. Nowadays in the world of learning and AI, people call multi-view tensor methods, pre-IDIC. Methods, phreatic, you see all kinds of descriptions, and they're all actually the same thing with a different spin, that is, they're coming in from a different type of discipline. The actual methods themselves, you see them called or data fusion, data integration, conjoint analysis. And the different approaches that we see also have sort of Have sort of data science sociology involved. That is, there are areas which are wedded to matrix factorization and dimension reduction in the classical way, but then other in the world of Bayesian methods, they prefer Bayesian hierarchical methods. And then the computer scientists come in more interested in the algorithmic graph-based methods, for instance, and geometry. Based methods, for instance, and geometric graphs and nearest neighbors. There's been a lot of success in embedding methods because they actually combine, in some sense, neighborhood graphs and spectral decompositions. And we did make an effort in the field of the microbiome. Chris Sankaran wrote a, did an enormous amount of work trying to use all the different methods that people were using to do multi-table integration. Do multi-table integration, and the way he did that was he took a data set and he subjected it to many, many of these different methods. And so, that paper appeared last year in Frontiers in Genetics. And that was, I think, you know, one of the things that I think science has to ask itself right now is why can't we communicate to the outside world? And we did a very bad job about COVID, it seems, that you know, policymakers don't understand. Policymakers don't understand us, but it's actually the case that between scientists we don't understand each other. So, we really need to work very hard on communication as much as we do on the development of methods. And in the communication, I think it's useful to think about so if you're looking at different resolutions and from different perspectives. And from different perspectives, multi-view and different windows on the data is a good way of thinking about it. And here are some images which are generated by an origami folkscope microscope, which is a wonderful little apparatus. And it's an example of recycling many ideas to do something quite complicated. And it gives you a wonderful window on the world if you allow yourself the multiple perspectives. Yourself the multiple perspectives. So, we should all not speak one languages which are useful. I want to take an example in the microbiome data of problems that we've come across and the ways that we solved it. And so, why is this multi-domain or multimodal data? Well, we have genomic material where we have a 16S RNA gene, which is the signature of certain types of taxa or strains. Or strains. We have gene expression, transcriptomics, just the same as many of the data that we looked at. We have mass specs, or we have metablites, and we have a lot of clinical and environmental information about the samples as to how they were collected. But there's domain knowledge that we need to integrate. So phylogenetic trees are really important in the study of bacteria, a little bit the same way as gene ontologies are important, although they haven't been developed so much recently. Developed so much recently. There are metabolic networks, for instance, for metabolites which we can use and can help us understand the data. So we want to integrate across many different types. And so when our workflow has been, you know, you take the DNA, we're going to go read, we have short sequence reads, about 250, and then we create the strains by denoising. And I'll explain a little bit about why. And I'll explain a little bit about why data 2 uses more information than all the other methods and how it's more successful because of that. And then we have all the information about the variables. And so this was a place where the bioconductor structures, the S4 structures, were really important and useful. And the incorporation of a phylogenetic tree was what motivated us to make this phytoce package, which is now used by a lot of people. Package, which is now used by a lot of people who do microbiome studies. And here you can see the image of the different structures. So we have matrices, we have data frames for describing the data themselves, of course, but then we have to deal with trees. And we try to reuse and recycle some of the things that have already been usefully done in the eight package, which is a tree, phylogenetic tree package. And for instance, for the strings, we use the bio. For instance, for the strings, we use the bio strings package. So it is a case that we try to connect to the community and make bridges so that we can reuse some of the abstractions. And so this workflow enables us to go quickly from the processed data to using ordinations and making projections. And then we can do inference and testing just by using R. And that made it very easy. Made it very easy. And we made wrappers at the time when we first wrote this, it wasn't at all compatible between ggplot and Bioconductor. So, you know, this was done almost 10 years ago. It was the beginning of ggplot. So we tried to make ways of using ggplot that were easy for these. I wouldn't do that today, but it's been very successful for people in publication. Of course, this gave us our reproducible. Of course, this gave us our reproducibility because now we can use knitter and all the background, so we use phylo-seq and ggplot. And I'll tell you a little bit of why the reproducibility is really, really important. Here's an example of a nature paper that said that there were types or discrete clusters, interotypes of the human gut. And in the media, they sort of made the parallel to what happens in blood types. So that people have a unique type, and you could see that they were. Unique type, and you could see that they were clustered and separate. And in fact, if you go through the study and we reproduced every step of what was done, we found that there were a lot of choices that were made. So there was a choice about the data transformation, but also, so you could take the log, you could take the proportion or relative the abundance, or you could take the original counts, or you could take subsampling, which is a strange thing that some microbiologists like. Microbiologists like to do because it's rarefying the data and getting rid of the differences in variance by saying that every column has to have the same number of reads. You could take a subset of the data, and here they actually declared nine of the subjects as outliers. And you can filter out taxa which have unknown labels or which are rare, or you can threshold. So there are quite a lot of choices here. And then when an ordination Is here. And then, when an ordination is made, I showed you an ordination. There are actually 40 different choices of distance for an ordination. You can use Jacquard or you could use Bray Curtis. And there are many different distances, even some that take into account the phylogenetic tree. There are lots of nice ordination methods, especially in vegan and AD4, so multidimensional scaling, non-metric multidimensional scaling, and extensions, and then you have to choose. Extensions, and then you have to choose how many factors you choose. And they did clustering, so they had to choose a method and a number of clusters. And if you go on like that, you find that your analysis gives you 204 million different ways of analyzing the data. And so as a statistician, there's no method for doing post-selective inference that can control for that. So the only way around this is to have a reproducible workflow. Reproducible workflow. So it's really important computationally to publish the code and not only the summaries of the data, but the way that you went all the way through and all the choices, the filtering choices, all those choices have to be made available to the reviewers so that they could choose one or two choices to see if your result is robust to a different choice. And so that's why the reproducible research and the R Markdown. Research and the R Markdown is definitely the way to go. Now, I want to say, you know, this is the data, these are the problems. This is a way that I found useful to thinking about going forward. So I would say instead of transfer learning for the machine or for the data science, I say transfer learning for humans. That is, we have learned, we have a certain number of methods that we Have a certain number of methods that we master well, and one of them is PCA, there are quite a few other ones. You want to think about the new methods as extensions of the ones that you already understand. And some examples that have come up, which I think are useful, and I'll give a little bit some applications of this, is spatial statistics is a well-developed field with many, many tools. I think we should capitalize on that. Capitalize on that. Probabilistic denoising takes into account frequencies and probabilities, and that's also really important. That's something that gets lost. People tend to round the data without taking into account the frequencies. You can extend covariances from computing covariances between features to computing covariances between tables. And that's a way of extending some of the methods from multi. Some of the methods from multivariate to multi-table. There are methods which allow you to do non-parametric testing, rank-based methods, which extend very nicely to network-based testing. And then we have, of course, the idea of latent variables and latent clusters, which is easy, is extended and usefully to latent networks and latent manifolds. And then again, we And then again, we can extend, you know, by adding to every one of these ideas a sort of hierarchical iterative structure. So transfer learning, but for humans, is a useful paradigm, I think. And then here I'm just going to talk about spatial statistics. This is a study that I was involved in about, well, it's almost 20 years ago. So we were doing immunohistostaining. Immunohistostaining of lymph nodes. And in order to do this, at the time, the postdocs were zooming in and counting cells. And I wrote a system in Java to try and count cells by segmentation. It actually used random forests, and it worked pretty well. And we were able to get, for instance, the spatial representation here. It's just flipped over a little bit, but here you have the T cells representation. Here you have the T cells represented. Every dot, red dot, is a T cell. Here you have the tumor cells. So the T cells, you know, there are hundreds and thousands of them, tens and thousands of tumor cells. And when we do this, what we're trying to do is spatial statistics. So we did this segmentation and then used the data and The data, and this appeared in 2010. I had a terrible time trying to get this published because we use spatial statistics, which you don't do in histology historically. And although this was in situ data, it was pretty interesting. And we read the data into R, and then we can use, as I said, you know, we make the bridge to the existing. So SPATSAT is a very nice spatial statistics package, and you can use it to. And you can use it to analyze the data. And this is, you know, this is what a line of R would look like. And you can look at the slides. And in the book, we have a chapter with Wolfgang on image analysis. And in that analysis, using statistics, you can use a standard spatial statistic. So, a transformation, for instance, of Ripley's case statistic allows you to see whether the point processes. Whether the point processes in the tumor cells and in the T cells are equivalent to a standard homogeneous Poisson process or not. And so we compared the data and you can run all of this. There's a whole example in the book with the code and the data enables you to see how one could use R to do just something like that. And then you can correlate it to the outcome. Can correlate it to the outcome. In the case of this data, we actually found that the clustering of certain types of immune cells was beneficial. And so this was an important sort of spatial finding. Now I want to talk about another example where we had to go back because there was a loss of information. So this is a case, several people brought it up. I know David brought it up in the case of what do you do? In the case of what do you do if the labeling that you have, for instance, for your cells, includes some kind of error. So you're going to be creating labels. That is, this is a case where the features aren't God-given. They're not given to you. You have to generate them from the raw data. And you're going to use your quality scores of the reads in the frequencies in order to do this. So this is a pipeline called Data2, in which we have the microbiome. In which we have the microbiome data, and so this was Illumina Amplicon data, and we want to generate the different tax or the labels of the species. Now, we were motivated because I've been working on this data for quite some time, and I couldn't get any results that reproduced properly. That is, it kept on being that the depth and the different conditions in which the samples were taken was more noise than the Was more noise than the signal underlying it. And many papers, for instance, including ones, you know, one from PNAS, said, you know, there's a huge microbial diversity. And actually, this diversity comes a lot from spelling mistakes. And you could ask, you know, how could you get rid of the spelling mistakes? So I take an example, which is you want to know how many words somebody knows. Somebody knows. Maybe they know 15,000 or 20,000 words, and you start sampling, you know, the words that they write out. You know, these are all handwritten, not somebody who uses the computer or spell check. And you want to know how many words they know. Well, you can use the frequency of these specific spellings to detect that, in fact, some of them are misspelled. And that's exactly how Google does it. And that's exactly how Google does it when it tells you, you know, do you mean so-and-so when you spell in? It has a database. Now, in our case, we actually create the database from the data themselves. That is, we take the reads and we look at the ones which are the most frequent, and that's going to sort of serve as the seed for a core of a specific strain. But the success, and it seems very na√Øve, is that we use this extra information, which is the probability or the frequencies, which was actually dropped from much of the data when people were doing it before. So just to explain what's going on with an image, here I have two sequences. One has been for which I have a thousand reads, and the other one, say, I have 10,000. And the other ones, say I have 10,000 or 20,000 reads, and they have the same variance of error. So if they have the same variance of error, they don't necessarily have the same coverage because you have many more opportunities for error if you have 10,000 or 50,000 reads than if you have 1,000. So it's not the same variance that tells you the size of the clusters. And this is something that sometimes people find confusing, but the difference. Confusing, but it the difference if you have unbalanced samples, this is going to occur. And this occurs in other settings. TC is one example as well. So you have to sort of worry about it. And so the method that people were using, the standard method for the microbiome, was to say, okay, I am going to just create a radius of 97% similarity. And within that radius, I'll call this one taxot, OTU1, and OTU one and this one will be another taxa and all of these will be given you know they have to create their own OTUs they're outside and so that was the OTU method and in some sense what happens is you have the truth on the left these are the true sequences these are sample sequences but then after amplification and reading you you have these amplicon regions you see some have a certain amount of You see, some have a certain amount of error. And what OTUs do is just create a ball of a certain radius. And so it says here that we have two different OTUs, two different taxa. But what you want to do is you want to denoise and make an error model which takes into account the frequencies. And it works along a sort of EM type method, but this is how we denoise the data. And we can keep into account We can keep into account the frequencies and the probabilistic error rates. So we don't have this leakage of information that is occurring if you just take into account the radius and not the frequencies. And the result is actually you get much higher resolution. So here we were able to see these are data from the major microbiome that we study on pregnant women in order to predict preterm birth. And we could see Preterm birth, and we could see that we had six different strains instead of one strain, which was given to us by the standard pipeline. So it's more accurate and you get higher resolution data. So that was sort of a pitch for something that I haven't seen said so much, which is you need to keep in the data all this frequency information. Now, the next thing is, how do you not slice and dice the data? Do you not slice and dice the data? And we've seen many different approaches to this, but I like to do it in a simple way first. So, multi-table approaches, you can start off by saying, okay, we're going to generalize the notion of variance into a notion of inertia, which is the variance is the sum of the squares of a distance multiplied by a weight. And that's what is inertia for physicists. And that helps you because you can generate the notion of variance in this way generalizes to the chi-square in a contingency table, or you have phylogenetic diversity is also a generalization. So that's a useful way of seeing it. Now, when you're studying two variables, the covariance or the the covariance or the co-inertia is just computed as say you know the sum of the it's the inner product so when they covary in the same direction then they they become big together or small together now a simple generalization of this is to say instead of looking at the vector now we can do the same thing but with two tables and it doesn't matter what the tables are they can be covariances they can be Covariances, they can be distances, but you just care about the positions in the arrays and looking at the correlationships in multiple data sets, just doing the same computation. This is what this coefficient called the R V coefficient does. This was, so I have a bias in favor of this method because it was developed by my advisor and I used it a lot in my PhD thesis in the 80s. He had developed it in all. We had developed it in order to do multiple table analysis in the framework of sensorial tables. That is, you want to look at the comparisons. You have different judges and they're tasting different things and you have different chemical components of the different things. So it might be wine or things like that. And you want to use all of that information. So this Rv coefficient is really just a Frobenius inner product between the matrices, and it depends what. The matrices, and it depends whether what you use as your characterizing operators for your tables. In the case of the microbiome, this is an example. We have the read counts for the different patients, and these patients were measured across time, and so we had about 50, more than 50 different samples. We also had mass spec data, and both in positive and negative mass spec. Positive and negative mass spec features, and we had RNA-Sec metagenomic expression data. And we wanted to look at the correlations between the tables. So, this simple coefficient allows you to see that the mass spec on the positive ion channel is the most correlated to the KEG data. And so, this is very, very simple. It's just a correlation matrix, but it is a good first approach to see. First approach to see whether you have links between the different modalities. So, here you make this RV table and you can test, and you know, this is a standard method used a lot in ecology, and there are permutation tests in the ADE4 package for using that. Now, what can we do with that? Well, if you have these multiple tables, and these could come from various situations, one could be all of these tables could be boosted. Of these tables could be bootstraps of the same original matrix, or they could be many different modalities. It's the same thing where you can create your coefficient, your matrix of coefficients between these and then take the eigenvectors of that or get the eigen table. That's called the compromise in this method that was developed in the 70s and gives you a way of projecting all the individual tables on. All the individual tables onto this common table. So, this is what the method status and d-statis does. And the way I used it, I used in my PhD thesis, was the problem that I had was if you do a bootstrap of PCA and then misaligned. So you get multiple projections and they don't seem to correspond. And what's happening in this case, and I only took, say, you know, in this case, I have only five samples, and I did about Samples, and I did about 10 bootstrap to make it clear what's going on. We know that PCA inverts the eigenvectors sometimes, so you get plus and minus arbitrarily. So, you could have the projections on one side or the other. And so, you need to align, take that into account. And you could see how, you know, taking an angle would help you do that. And then, the other thing that can happen in this day is. The other thing that can happen in this data, we had very, you see, the variances explained are very close, 48% and 45%. And so the eigenvectors are quite close. And it's actually what's going on is you have an inversion of which one is the first eigenvector and which one is the second one. And so, but you can align this by, you can solve it by taking the compromise, which is taken by doing a PCA of PCA. Is taken by doing a PCA of PCAs in some sense, doing the correlation matrix of the different bootstraps and then representing, making a representation in that line. So this compromise projection approach is both useful for uncertainty, like the bootstrap, or for multiple table. We used it with Len Huang Niang in a paper for visualizing the uncertainty. When you have data which is actually organized along a great Organized along a gradient. So we talked a little bit in some of the talks, people talk about horseshoes. And so this data has definite horseshoe behavior. Just to say what it is, it's the data that's microbiome of the ocean, and the samples are taken at different depths. And there's a very natural ordering of the samples, even when you don't know the depth. But if you analyze the microbiome, it's very dependent on the depth. Dependent on the depth. And so for us, we wanted to find what was an ordering. And if you look in two dimensions, here the first two factors, you get what we call a horseshoe, which is a nice representation of an underlying simple gradient. And it's very important to detect them. Now, it's not that we necessarily, sometimes when we have a horseshoe, we say, okay, the first two dimensions explain the horseshoe. Dimensions, explain the horseshoe, and then we're going to take it out. But the first thing is to document. If you have a latent variable, you definitely want to document it because it tells you a lot about the data. But we had an added goal for this study, that is, we wanted to incorporate also the uncertainty with which we knew the position of all the points on the ordering. And so we did this by doing a Bayesian model for ordering. For hierarchical Bayesian model. And the problem that we had at the end was you get many pics from the Bayesian posterior and you want to represent the uncertainty. And the way around that is to say you get distances between all the points and you get a distance for every pick from the posterior. Say you have a thousand posterior picks, you get a thousand matrices. And you can use this registration method, this compromise. Registration method, this compromise, in order to find where you should be projecting. And then here we projected the points, and actually, every little cloud here, this is a cloud of contours, which represents the projection of just one point. So you have an image in some sense of how closely we know where an individual can be placed in this ordering. But this was one of the uses of this conjoint analysis. Of the uses of this conjoint analysis in the context of trying to get at how do you visualize uncertainty. So, another furthering of this, and this came up in some of the discussions, so I thought it was worth speaking about the more general factor analysis case. Is when you're doing this uncertainty quantification, for instance, for factors. So, here this is a So, here this is a Bayesian factor analysis in which we had the different taxa and they're in different samples. We've called the samples populations because they're environmental samples which represent populations of bacteria. But here, these extra dimensions, so we have a cube of data, we have a thousand repetitions, that is, we have a thousand pics from the posterior from our model, and here we had a joint prior on the We had a joint prior on the factors, and we used a gram matrix. And we have a pick of the abundances, which is given by this inner product with these unknown parameters, which are defined in some sense as the underlying factors. So this is a paper that appeared in JASA, where we were trying to give uncertainty quantification in a non-parametric Bayes factor analysis. And so the And so the parameters in this case, we were able to, you know, we have a joint prior on the factors and we have this dependency. But what was happening was when we looked at the normalized grain matrix and we projected, this is what we get. And so this was very unsatisfactory. Again, what's going on is exactly this. They're not aligned. That is the They're not aligned. That is, the registration between which factors are coming up first and second, third, are not consistent across every pick from the Bayesian posterior. So you can use again this registration idea that is we have all the different samples and we're going to take here the Rv coefficient. Now in this case our matrices are symmetric and we can compute the inner product and we die The inner product, and we diagonalize the Rv matrix. So, if you have a thousand by a thousand, you get a thousand by a thousand matrix, and then you can project. Now, that's the compromise, so you can project, say, in two dimensions on the samples. And once you do that, this is the picture that you get. And so, in this case, we have two axes of the compromise, and it's satisfactory in some sense because not all Sense because not all the points have the same uncertainty. So, here, here, these samples we're pretty sure of their situation in the multivariate plane. And then we have a couple of samples which we have a lot of uncertainty on. So, what we did was we went back and looked at the samples to see what was wrong with them. So, not only did we have different types, community state types, and that community state type. And that community state type is pretty well respected. That is, in different areas of the space, we have separate community state types. So, you know, one up here, they're all practically here. But in the middle, we have this community state type two, where we had several problematic samples. And what we saw was they are the ones with low read depth. And so it's what we would expect and what we hope for is to see broad uncertainty. Is to see broad uncertainty on things which are being estimated with less data. And so this is very helpful in interpretation, but it's also very helpful in the design of the next step. That is, if you want to do another study, you know, what should we be using and how do you help your collaborators choose? Well, you say, well, you need to do more studies with that type of data. And so it's very useful. Type of data. And so it's very useful to have uncertainty quantification when doing the next stage. Now I want to talk not about spatial data, but about communities and networks. So we have a lot of count data, but also we like to summarize it very often as graphs and networks. And I'm going to focus a little bit more on testing or confirmatory analysis because we've seen a lot of analyses which We've seen a lot of analyses which are exploratory, and testing is still quite a big problem, especially as I said before. If you take into account all your possibilities, you can always get something significant. So, you really want to do a very robust test. And non-parametric tests are good. So, through this idea of transfer learning for humans, here's the extension of a one-dimensional test. So, Friedman and Rafsky devised this. So, Friedman and Rafsky devised an extension of the Runds test. And so, the Runds test is a test in statistics in which you just look at the number of contiguous runs of one type of group and you compare it to the hybrid runs where you have two sample types which are different, which follow each other. So, the way of doing that, if you have a continuous variable, just one A continuous variable, just one that you're measuring, is you pull the observations, you rank them, and then you count the number of runs, sequences of observations from the same sample. And that's for Woffozitz test. And Jerry Friedman and Larry Rapsky had a wonderful extension to this, and this comes from their original paper. And this is a general setup that works for all kinds of different graphs. What they did was they had high-dimensional data, and we know we don't know how to. And we know we don't know how to rank high-dimensional data. And so, what they did, they took a minimum spanning tree, regardless of which sample the data points belonged to. And so, here's my minimum spanning tree. So you have to have chosen a distance between the points in multivariate space. And then you're looking at how many times do I have the edges in the graph whose two nodes Whose two nodes are of the same type. And so here they erased the ones which have hybrid, the hybrid edges, and then you can count the number of edges and you can do a permutation test, keep the graph the same, but change the labeling. So this is a very standard test and it's a nice multivariate extension of ranking, but it works on any kind of skeleton graph. So the one that I just showed you. The one that I just showed you was done on the minimum spanning tree. But you can also use it if you have a geometric graph in which you're connecting two nodes which are within a distance of epsilon. That's what we call a geometric graph. Then you could use that as your graph, or you could have a nearest neighbor graph, you know, four nearest neighbors and use that as your graph. And all of that works the same. Here's an example on my microbiome. On my microbiome data, where I have samples. In this case, they come from different types of samples. And then I want to find out whether there's a relationship between the type and the co-occurrence of certain bacteria. So the way I created this network is actually through a Jacquard distance threshold. That is, if they're sharing more than, say, Sharing more than, say, 50% of the bacteria in common. You compute a Jacquard distance and you find out whether you know it's larger than 0.5 or not. And then for the small distances, you connect. That means they're very similar. And then you can ask, what's the structure of the graph, this co-occurrence network, with regards to this exterior sample type variable? And so here we have the sample type. And so here we have the sample type, which is color, and the edges which are in bold are the edges where they have the same nodes on each side, and the ones which are in dots are the ones which have hybrid edges, mixed edges. So what you do is, this is a little package which was written by Julia Fukuyama, one of my graduate students, who's now an assistant professor in Indiana. And she wrote this phytocycle. And she wrote this phyto graph test, which implements this for this type of mixed data. And here you see the p-value is going to be very significant because the observed value of 69 is way, way out and the permutation values are small. So this is just an example of transfer learning in some sense where you can transfer something that we know in low dimensions and we can do high dimensional tests. I want to finish off just with a little With a little note before I get to questions. One of my heroes and friends is Don Knuth, and his wonderful book on computing is really the Bible. And he says, you know, premature optimization is the root of all evil in coding. And I would say, and if you think about that, you know, you get yourself into trouble if you're trying to go optimize. Go optimize too early. It's actually, I would say, premature optimization is the root of all evil in life as well. Optimization is maybe not something that you want to do too early. Randomness is probably better. But in statistics, I'd say premature summarization is the root of all evil. And what I mean by that is what I see being done a lot in the data, the multi-domain data. The multi-domain data is taking averages of cells in a cell type, for instance, and that erases everything that you can use about the transitions between cell types and the developmental history is lost. So, if you do summaries with averaging too early, premature summarization is really acts against. It acts against you. You're losing information. So, I think that one of the things that I try to fight against is this trade-off between being able to interpret and understand your data without summarizing it. And so that's where the graphics, of course, come in and really important. And as I said before, I believe in transfer learning and in recycling methods in areas where they've developed. Where they've developed a lot of nice tools. And so, for instance, the idea of doing layers, different spatial point processes, different levels, different, that's been very well done in the area of geography and GIS systems where they have spatial relationships. The reason I like this picture is it represents an underlying space which is not Euclidean and Euclidean. And what you have to know about the data, and we see this in the success of methods like UMAP, is the underlying structure of the biological data sits on manifolds. It doesn't sit on a flat space. And so you lose a lot if you only use linear methods. And so think about what we do with the sphere and how we take into account the curvature. Count the curvature. I think that's really something important. So here are some of the material, and I publish everything with a lot of bad, well, not all of it's bad, but a lot of code is not perfect. Let's just put that in the ball. I won't, you know, some of this, of course, was written by my trainees who are much better at coding than me. So then the code's not bad, but some of this is not perfect. But I believe in sharing everything. And so we try to make packages. And so we try to make packages and resources. And here are some, they're on the slides, and then I'm going to make the slides available on Dropbox so you'll have links. But I just want to take one minute to say, you know, what are the solutions? So the first principle for me is you have to respect the data. You can't drop pieces of the data that you don't like to clean it up because you lose too much information. And you need to maintain all the information. All the information and uncertainties so that you can do downstream uncertainty propagation and looking at generalizations. In terms of interpretation, I think that latent variables, gradients, clusters, and networks are really useful. I did talk about reproducibility and heterogeneity using the whole Bioconductor framework. And I do think we need more solutions. So, some people have talked a little bit. Some people have talked a little bit about special versions of PCA, but I would also say that sometimes you can just very cleverly transform your data and still do an ordinary PCA. And that is, I'm a fan of actually doing rank transformations and sometimes threshold them and then do a PCA. And sometimes you don't need the exponential family or you don't need to do PCA, which is. PCA, which is too fancy. One of the areas which I find the most exciting right now is we now have new coefficients for detecting dependencies that are not linear. So Kiman brought up the fact that, you know, maybe biological association is not linear correlation. I agree completely, but you can test whether something is dependent or independent nowadays. So Shorav Chatterjee has a very nice Strategy has a very nice correlation, new type of correlation, which is non-parametric and it depends, it's absolutely non-linear. It detects non-linear relationships of any kind. The other thing is we need not only to take into account local information, but know how to collate patches and align them. That's where a method like UMAP, its principle, is to use differential geometry and to have local maps in know-how. Have local maps and know how they combine. And I think the use of differential geometry is really essential there. And so, as I said, we do a lot of tree and graph integration. We use kernels, so that works pretty well. And so the robustness is something that is enhanced by using sparse methods, but also going over the code. So, thank you to all the statisticians and the bioconductor people, in particular, who Bioconductor people in particular who make all their code available, and to Joey, McMurdy, and Ben, who developed two very good packages. And this is my lab group. And thank you. Any questions? So I'll put these slides up. So, yeah, okay. Do you want to open the chat window and see if there are any? Okay. Right. Okay. Right. There have been quite a few questions and then quite a few answers as well. Okay. Let's see. Starting from the top, I think the last one that really hasn't been answered is from Alana. So she was asking about, when we were talking about visualizing uncertainty across these trajectories. Right. She originally asked how to formulate a statistical test to compare trajectories within conditions. Trajectories within conditions. And then there were a few responses linking to a paper called TradeSeek, a method called TradeSeek that provides a test to compare differences between trajectories across conditions. But then she followed up and said more about thinking about the uncertainty and the trajectories. Like, how would you compare the differences in the trajectories themselves? And maybe Alana maybe will do a better job of explaining. Maybe we'll do a better job of explaining her question than I did, but that was my understanding. So, I didn't know if you had thoughts on that. Yeah, I mean, I mean, I can sort of allow, like, what I was thinking is: you did such a beautiful job. Everybody is focused on trajectory inference. And I thought, you know, you know, you did such a beautiful job of showing variation along those trajectories as well. And then the second you have a variance, right, then the question is: if you have, let's say, two different conditions, right, how do you form a statistical test to compare those trajectories? Test to compare those trajectories, which I guess you could do now that you have the variance. I'm curious about that. Well, it's hugely made difficult by two things. One is, and we haven't talked very much about it, but often if you're looking across different technologies, the variance and the variabilities are very, very different. And so you might have a huge amount of variability in one type of technology, much less in the other. And then Much less than the other. And then, in some sense, you'd want to make the whole envelope of possible trajectories given your error and see whether your trajectory belongs to it. I think another route, which is sort of more promising and easier to do, is to ask topological questions about the trajectory. That is, does it split? Is it a cycle? And so, those kinds of answers. Kinds of answers are sort of easier to do because you don't have all of these problems about scaling. I do think that that's a very interesting, hard problem. And, you know, there are ways of thinking about that. But I'm a big fan of usually using differential geometry, so put embedding it in a higher space, which can allow for the Can allow for the curvatures. And then a method like UMAP and optimal transport also do this. Is you have changes of measure which take into account your local densities. And so this is something which is really important. You know, I insisted on the frequencies and densities at the beginning when we use it for data too. But this is also something that comes up when you're trying to quantify uncertainty. That is, areas which are high, highly. Areas which are highly dense in points have much less uncertainty. And so you can make tests where you choose the areas of high density to do the testing that is targeted testing. And I don't know whether that's a, you know, we could talk about it because I don't know whether I really understood your question. I'd love to talk more offline if you're up for it. Yeah, great. Yeah, right. Thank you. So I think next there was a question from Kim In: How do you explain a non-linear association to a biologist? Or how do we explain non-trivial statistics? Yeah, so how do biologists? So I have this problem all the time with all the different things that I do because I work mostly in interpretation and I work, we move very We move very far away from black boxes. I forbid myself. And I was punished at the beginning of my career. I had all these fancy methods, but they never made it into the papers that I published with the paper. Because if the biologist can't stand up at a conference and explain what was done in the paper, they won't publish it. And so if you can't explain it to a biologist, even if it's the right answer, it's useless. So the best way of explaining is with generative processes. So you make up a process. So you make up a process which is, and they can understand that very well. That is, there's a very famous curve surface that all the biologists know about, which is called Waddington's surface, which has to do with development biology. And it's a manifold in which you have a ball which is rolling down the manifold and it's caught in the right. So, underlying methods, people understand that very well in biology. And they also understand about exponential because they understand, you know, QTPCR and systems which are non-linear or not. Now, of course, the first thing that we always do is we try to make these nice transformation, you know, arc sine hyperbolic or some kind of transformation or non-parametric variant stabilizing so that the data go back to being linear because then we're happy and the biologists understand. The biologist understands perfectly well, you know, I transform the data. So instead of using the log, I did some fancy transformation. That doesn't bother them. And then everything's linear and everybody's happy. But that's one way of doing it. But the other way is by simulation. And I use a lot of simulations in my communications with a biologist. Because, of course, you know, formulas aren't so useful. That makes a lot of sense. Another question from Another question from Lexis. If we use frequencies to clean out the noise, how can we discover rare events or cell subtypes? Okay, so this happens a lot in the microbiome data too. And I have to be honest, I never, as I said early on, I said, I never make choices that I commit to forever. So I don't think there's only one way of analyzing data. And usually with the taxidermy, And usually, with the taxidata, we create two different data sets. We have the rare data, which is just 0, 1, which is very, but it contains everything. And then we have the core data, where we use all the frequencies. So if you just have 0, 1, you have one type of analysis that you do, which is different than the analysis on the core data. So we have two windows. Windows and again, as I said, multi-view doesn't only apply to multi-view, it's not just the data, it's multi-view. That is, it's multi-view on the user side as well. That is, you want many windows into the data, so you're not going to only do one analysis. And so, so, so, so the making it into zero, one and doing a complementary study of another side of the data, certainly the case. Uh, oh no. Is she? Are you still there? Okay, okay. I think you're back. Okay. Did you hear the end of that? Sorry. I missed the last 20 seconds. I don't know about anybody else. I was just saying that you could split the, you do several different analysis. One is binary 0, 1. The other is on the continuous data. Continuous data, all of it together, and then the third one could be ranking data. But I don't exclude that's why I say you it's multiple windows on the data, you're allowed to do all three. They're complementary. We're looking at multi-view technologies, but it's also multi-view methods. You should use many, many methods. A complex data set requires many points of view. Makes sense. Well, I believe we have come to the end of the hour, so let's give another big round. So let's give another big round of applause and say thank you to the speaker.