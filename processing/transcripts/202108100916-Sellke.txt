Tell you about algorithmic pure states for the negative spherical perceptron, and this is joint with Ahmed, who I think is here also. Okay, so let me say what the problem is. So what we're going to be looking at is an intersection of a bunch of randomly oriented half spaces in high dimension. So we're going to have some constant, some positive real number alpha, and we'll look at m, which Alpha, and we'll look at M, which is alpha times N, randomly oriented half spaces in Rn. And each of these half spaces is basically going to be shifted by this other number, kappa. So to generate a half space, we pick a IID standard Gaussian vector GA in Rn. And then its associated half space is Its associated half space is the set of vectors which have inner product at least kappa times root n. So this scaling is going to be chosen. So each of these half spaces is some constant fraction of the sphere of radius root n. So we're going to be just looking at the sphere of radius root n, and then we're going to intersect all of these half spaces. So in So in the negative kappa case, these half spaces are shifted away from, or are shifted so they get bigger. So that's what's shown in this figure here. And so one half space is in blue, one half space is in orange, and their intersection S is this red arc. So we're also constraining to actually be on the boundary of the sphere. Okay, and we'd like to study this intersection. Intersection and say something about it. Let me also say, which I'll kind of use later. You can also formulate this a bit more concisely as just a matrix where the rows are these Gaussian vectors, and then you'd like to find a vector V so that G times V is at least kappa root n entry-wise. Mark, do you have a pointer? Are you meant to have a pointer on the slides? Oh, I don't. I don't know if I can. You can use perhaps mouse if you want to point in mouse. Yeah, no, we'll look at it. Yeah. Can you see my mouse when I do this? Oh, okay. Great. That's good. Yeah. Somehow animations were not working today. So, okay, it's good if you can see my mouse at least. Thanks. Yeah, so this is the spherical version of the problem. There's also an easing analog for any problem like this where we require the vector v to be on the cube instead of the sphere. V to be on the cube instead of the sphere, but this will all be on the sphere. Okay. So let me say in brief what I'll kind of talk about today. We'll consider the negative kappa case, and we're going to make a full replica symmetry breaking assumption on the behavior of a certain variational problem, which is kind of expected to be related to this, but there's not a lot of rigorous results known. And under this Known. And under this full replica symmetry breaking assumption, we will show that this intersection set S is non-empty when alpha is below some threshold, which is sort of expected based on this Preezy-type formalism to be the correct threshold. Moreover, this proof will be constructive, so we'll give an efficient algorithm based on something called approximate message passing, which actually gives you a nice point in the Actually, it gives you a nice point in the set. So we do need some little O of one shifting in either alpha or kappa. I won't emphasize it too much, but there is some kind of little amount of shifting in the parameters that is needed to make things work. Okay, let me give some historical background. So this problem was first studied by Gardner and Derrida in the 80s. This was proposed as a model for memorization. Proposed as a model for memorization of like neural networks. There have also been some more recent connections proposed to things like sphere packing. And the main things you'd look at are first the satisfiability threshold. So when does this intersection become non-empty, become empty? What is the volume if it is non-empty? And maybe more qualitatively, what does its geometry look like? As far as rigorous mathematical work, the spherical case with non-negative kappa is at least somewhat understood. So if kappa is zero, so we're just intersecting non-shifted, nice symmetric half spaces, the critical alpha is two by a symmetry argument. So this means you can intersect up to two n half spaces before things transition from non-empty to empty. Non-empty to empty. Shabina and Tirozi computed the volume also in this positive kappa case, and later Stoinich gave an elegant computation of the threshold just using a Gaussian comparison inequality. In the Easing case, less is known, but at least when kappa is zero, Thing and Sun prove the predicted lower bound. So we're also showing a lower bound, but it's a bit different. There's also some works by Tallagher. There's also some works by Tallagrand which show related results at sort of high temperature using something called the cavity method. So the reason that things become easier when kappa is non-negative is that we can kind of, instead of working on the sphere, we can work on the ball. So we can just kind of fill in the sphere to work with subsets of the ball and things become convex. And this turns out to make things more concerned. This turns out to make things easier in a few ways. This is particularly why this proof of Stoinich works very cleanly. However, when kappa is negative, it becomes a lot more complicated and you can't do any trick like this. So there's kind of a replica symmetry breaking onsats here that the proof is based on. And it says that this set S is, you know, roughly speaking. Is, roughly speaking, going to be something like an ultrametric space, which basically means you can organize it into a branching tree. So the points in S correspond roughly to leaves of a tree with some kind of tree metric. So this kind of onsats is proved for mean field spin glasses, which are a very similar model where you're trying to optimize a polynomial on the sphere on the cube. And while it's not proved here, it's kind of expected that a similar phenomenon. Kind of expected that a similar phenomenon will take place. And kind of correspondingly, the description relies on this increasing function gamma star from 0, 1 to 0, 1. And gamma star is basically the annealed CDF of the normalized inner product of two independent points in the set. And the behavior of this function gamma star tells us what kind of replica symmetry. star tells us what kind of replica symmetry breaking is occurring. So if gamma star is a step function taking finitely many values, then, well, if you have a C D F which is a step function, that means you're a probability measure with just a finite number of atoms. And so this means that the set is organized into a finite tree of clusters and subclusters and so on. And each level is a step of replica symmetry breaking. But if gamma star is continuous and strictly increasing, then we'll have kind of a continuously Then we'll have kind of a continuously branching tree. And this is what full replica symmetry breaking is supposed to mean. So the basic conjecture we're going to be making, which I'll make a bit more precise later, is that this full RSB does hold at least near the critical value of alpha. Let me say a bit about what happens when this doesn't occur. So if the Doesn't occur. So if this full replica symmetry breaking doesn't assumption, assumption does not hold, then the model exhibits overlap gap property. And this type of overlap gap property can be defined in a lot of other types of related models, like these spin glasses and also various inference problems and so on. And the typical message that one can prove is that when an overlap gap holds, finding a solution, so in our case, finding a solution. A solution, so in our case, finding a point in S, maybe more generally, maximizing some function, becomes computationally hard. So these gaps are just the points or just the intervals where our function gamma star is constant. And more precisely, what we can often show is that various classes of algorithms fail to achieve whatever objective. So message passing, Langevin dynamics, Monte Carlo, Markov chains, things like this. Now the perceptron is. Now, the perceptron is not sufficiently well understood for us to make any kind of rigorous statement like this, but it's still sort of what we expect based on these analogies. Okay, on the other hand, when full replica symmetry breaking does hold, it makes sense that we might be able to find an efficient algorithm. And these three works, four works here at the top. For works here at the top achieve this kind of thing in these spin glass models. So, in these spin glass models, the static behavior of the model is kind of well understood. So, it's still interesting to find an algorithm to achieve maxima and so on. But we kind of know a lot about these models mathematically already. And the work today. Clear ready. In the work today, we achieved something similar for the perceptron. And here we don't really know much of anything about the just purely mathematical behavior. These AMP algorithms that I'll be using have a lot of nice properties. So they're very efficient. Their running time is linear in the input size. And they're also very convenient and flexible. You can kind of plug in very general functions to them and get an algorithm that you immediately understand very well. Immediately understand very well. Okay, so there's this gamma star that we are assuming is full replica symmetry breaking. So we're assuming it increases continuously without any gaps. This is defined as the minimizer over increasing functions of this. Of this strange formula here. Okay, so this thing on the right is some strange thing, which, you know, I mean, it's not, it's a strange formula, but it certainly makes sense. And then this function phi here is defined as the solution to this PDE. So there's, it's a function of time and space. So the first coordinate is in zero. Coordinate is in 0, 1. The second coordinate is an arbitrary real number. It has some terminal condition at time 1. And then we solve this PDE to find the value at 0, 0. And that is the first part of this pre-sy functional here. So certainly, it's going to be hard to actually show that the minimizer is for replica symmetry breaking, but we can, you know, we can still. You know, we can still assume that it is and see what that gives us. Let me say a bit about what this kind of means. So, roughly speaking, this phi function at time t in point x is going to represent something like how happy we are with a current inner product value of x, or maybe I should say x root n, if we are searching for a solution. Searching for a solution centered at a point u sub t. So if we're kind of searching a little bit by little bit and we're currently at u sub t and we look at one of our inner product constraints, at the end it should be at least kappa. But right now we still have some more room to search. So we can kind of try to say how happy we are with this, maybe how much extra work will we need to ensure that the constraint is satisfied. So this is kind of why the terminal condition is zero when the constraint is not. Terminal condition is zero when the constraint is satisfied at the end and negative infinity when it's not. Now, the PDE has a time derivative over here, and this last term is just a spatial second derivative. So probably we all know that if you just have a time derivative and a spatial second derivative, then you have the heat equation, which corresponds to Brownian motion. This middle term is the square of the first. Middle term is the square of the first derivative, and this turns out to correspond to a certain type of stochastic optimal control problem. So the idea is you have a Brownian motion with a drift, but you can control the drift, but there's kind of a quadratic cost function. So it's more expensive to have a larger drift. And if you kind of put all this together, you end up with a PDE of this form. And this PDE is associated with a stochastic differential equation, which I'll Stochastic differential equation, which I'll call the Prezi SDE. So, just like the heat equation corresponds to Brownian motion, this PDE corresponds to this SDE here. Okay, so the assumption we're going to make is that we have full replica symmetry breaking, meaning that gamma star is strictly increasing on some sub-interval, and kind of everything is happening in the sub-interval. Kind of everything is happening in the subinterval between q under bar and q over bar. And we're going to assume that the phi at 0, 0 is finite, which essentially means that the SDE shown before actually ends up in the satisfying region always. Okay, and what we do in this case is we give a message passing algorithm that computes a point which satisfies the constraints. So g times sigma is at least. So g times sigma is at least kappa, and it has norm, which is the right endpoint of this support interval of gamma star. And so if, yeah, so the norm is Q over bar. If Q over bar is less than one, we expect this to be a pure state. And we also expect that Q over bar tends to one as we approach criticality. And in this case, you can always just add extra constraints to give a true solution. Add extra constraints to give a true solution, and it will be essentially exactly on the sphere. Okay, let me give a brief overview of the algorithm before I dive into any details. So there are three different steps. The first step is to locate the root of this ultrametric tree. So I mentioned before that the set S should be arranged into an ultrametric tree. The constraints are asymmetric, so we need to find the root. So, we need to find the root. In some cases, for example, if we only cared about the absolute values of these inner products, the root of this tree would just be at zero and we wouldn't need to do this first step. So the idea here is that the root satisfies a type of equation, and we use an iterative procedure to kind of converge to this stationary point. The second step is to descend this tree, and here we take a lot of And here we take a lot of different small orthogonal steps and kind of simulate the stochastic differential equation, the pre-ZSDE in some sense. Finally, there's a third step where we just round an approximate solution. So at the end of this second step, we'll have kind of most of the constraints satisfied, but maybe not all. And we need to do some routing procedure to satisfy literally all the constraints. So the main technical Underpinning of the type of algorithm we use is approximate message passing. So both steps one and two are based on approximate message passing. And this is a class of iterative algorithms that basically just needs us to have an IID disorder matrix. And we have one, which is just given by these Gaussian vectors. And we're going to rescale it by root n, and I'll call this matrix A. So the way AMP works in general is that we fix some determination. Is that we fix some deterministic initial vector in Rn, and we fix for each positive integer L a function fl from L plus one dimensions to one dimension. And we kind of go back and forth, multiplying here by a transpose and here multiplying by A. So we're mapping back between Rn and Rm by multiplying back before by A transpose and A. But the key thing is that we apply this non-linearity function. We apply this nonlinearity function fl at the l time step here on the L plus one previous vectors. So F L is a function of L plus one variables. So if we apply it to L plus one vectors, we can apply it coordinate wise and get out another vector. There are also these on sagger correction terms that kind of make the high-dimensional behavior nice by canceling out certain correlations, but Canceling out certain correlations, but maybe I won't say too much about them. But I'll just say, you know, we have two sequences of iterates here, and roughly one of them is going to be the actual solution path, and the other is going to keep track of the inner products. So there's some state evolution theorem that is the nice thing about AMP. What it basically tells you is that if you look at a fixed coordinate, so like the 10th coordinate, if I set ie here equal to 10, then Set ie here equal to 10, then the distribution of the 10th coordinate of my iterate sequence u's is some Gaussian process with some covariance that comes from these nonlinearities. And similarly, if I look at the 10th coordinate of my vectors v, it'll be a centered Gaussian process with some covariance that comes from the nonlinearities. So I'll remark that in most cases, AMP is using a symmetric matrix. Here we're using a non-symmetric matrix. Matrix. Here we're using a non-symmetric matrix, but it's kind of not that different. And there are also a lot of extensions of this basic formalism that I won't get into. Okay, so the first step is a fixed point iteration. So here we're just going to have our general nonlinearity f depend only on the most recent iterate and just always be the same every time. Okay, and it's going to be some. Some first spatial partial derivative of this function phi. Okay, roughly speaking, the idea for what's happening here is that this function phi is kind of a value function. It's telling us how happy we are. And so what we can try to do is look for a stationary point of this function. And this will give rise to a tap equation that are kind of well. That are kind of well understood in spin glasses. And, you know, this gives us some equation for where we want to be when we're radius square root of q under bar. And if you have an equation for where you want to be, so what this u vector is at that time, then you can, like if you have an equation, you can shift the time into C's to get an iterative algorithm instead. And that turns out to basically be what we're doing. And that turns out to basically be what we're doing with AMP here. So, what this does is it converges to what we treat as the root of the tree. The second step is to descend the tree. So, the idea is to take small steps. So, kind of at any time to go from ut to ut plus delta, we're going to add some linear combination of the Gaussian vectors g. Of the Gaussian vectors GA that we're trying to have good inner products with. And the idea is that we can kind of choose the coefficients. So we distribute different amounts of pushing along different vectors, and we can push harder along a vector when the vector needs more help. So when the current inner product is smaller. And you can kind of see here how this would lead us to the optimal solution of some stochastic control problem. If you think about this kind of naively, it doesn't. Think about this kind of naively, it doesn't actually tell you the correct thing that actually happens because there's this mysterious Onshager term, but this kind of maybe gives some indication of why we would get a stochastic control problem. So we're taking small steps, and each time we're adding some combination of these Gaussian vectors we want, and we just want to boost up the ones that need more help by more. And so the point is that. And so the point is that as we take the step size to zero, we get a stochastic differential equation out of this. Okay, maybe I won't, you know, it's certainly going to be hard to understand this in a couple of minutes, but somehow this idea I just described can be written as an approximate message passing algorithm where all the funny functions are related to this pre-Z stochastic differential equation. And in the limiting dimension, And in the limiting dynamics, as we take a large step size, we recover the differential equation or the stochastic differential equation. Under the hood, there are certain identities that one needs to make all of this stuff work. So these two identities here are needed to make sure that the first step converges to the right radius. This second step is needed to make sure that the diffusion part works. So these are all. So, these are all stationarity conditions for the variational problem. So, these hold because gamma star is optimal. And the reason that we need gamma star to be full RSP is that basically this means that we can take a lot of different first order variations in gamma star while remaining in the space of increasing functions. So, for example, if you have a function that's kind of flat for a while and you want to perturb it in the middle, you can't do that. Want to perturb it in the middle, you can't do that. So you have, you can't take a derivative with respect to like the value at that point. But if you have a function which is kind of continuously strictly increasing, then you can vary it a little bit anywhere. So you get a lot more equations that come from it being an optima. Okay, I think we're just about done. So let me quickly recap. We studied the spherical perceptron, which is an intersection of shifted half spaces. Intersection of shifted half spaces is shown here. And in the negative case, we gave an algorithm to find a point in this intersection, assuming a certain full replica symmetry breaking hypothesis. And there were two main stages, both using approximate message passing. The first locates the root of some ultrametric tree by finding a fixed point, and the second one descends this tree by simulating the corresponding diffusion process. Diffusion process. That's it. Thanks for your attention and let me know if you have any questions. Thank you very much.