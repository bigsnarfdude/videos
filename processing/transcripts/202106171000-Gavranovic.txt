So, take it away. Hi. Yeah, first of all, thank you for the opportunity to speak. It's exciting to talk about the stuff we've done. This is joint work with Jeffrey Cradwell, who's here, Paul Wilson, Neil Ghani, and Fabio Zanasi. So, what I'm going to be talking about today is our goal of really providing a categorical framework for deep learning. This is an ambition. Deep learning. This is an ambitious goal, and here you can see sort of the if there's anything you take away, it's this sort of little animation which describes a neural network. Hopefully, by the end of the talk, this is going to become clear what is happening where, and also where do derivatives come in. So, there's lots of grounds to cover, so I'm going to just start. What I'm going to do is first tell you how supervised learning works in one slide. So, the goal So, the goal is often to find a function, to find the implementation of a function with a known type. So, we know the function takes in an x and outputs a y, but we do not know what is its implementation. And what we do know is we have a data set. We have a list of input-output pairs such that when we input the x, the output we get on y is the thing we have here. And there's many ways to learn this. And there's many ways to learn this, and how it's being done with neural networks is something that I'm going to describe right now. So, we usually start, as I said, with this input-output pair. This is something that's going to be input in our learning process. So, here we see an image of a cat, for instance, if you're doing classification, and a label which tells us, well, this is a cat and not a dog and not a horse. But this is not the only thing we input, we also input something called. We also input something called neural network weights. These are called the parameters of a neural network. And the image, the schematic of the learning process looks like this. So the input comes in here, goes through a neural network, which is a series of what are called layers. In this case, there are two layers. Each layer has something called nodes, where this computation in total, and they can be composed, this computation in total. Composed. This computation in total is parametrized by these weights, which are drawn here. And the value of this node is computed as the sum of all these multiplied by their corresponding weights and put for some activation function, which is usually non-linear. There could be many of these nodes and there could be many of these layers. As this output is computed, we, for instance, compute an erroneous prediction where we're saying, well, this is actually a dog, but maybe a cat. Now, what happens in the Now, what happens in the learning process is that we compare this prediction with the true label y that we had, and we compare it with something called a loss or an error function. So we can think of this as a distance measure, which computes a real value out. Then the learning process sort of happens. Now we just do everything backwards. We ask ourselves, well, this is going to be, for instance, a high number because the distance between the true and the predicted values is different. Value is different. So we want to ask ourselves: how can we make this number smaller? So, since everything inside is differentiable, we can say, well, how can we move this Y to make the output R smaller? And then we can compute that change. And then via the chain rule, we sort of chain this backwards. We ask how can we change each of these weights so that the Y moved in that direction and so on. Direction and so on. This is propagated through all the layers. And then we compute this change in neural network weights. And so here we see on the x-axis, we see the weights, and we see the loss value for each sort of weight. So at the current weight we are at, which is here shown in blue, we are going to get something shown in orange, a vector of change. And then the idea is to compute this new parameter value. This is called gradient descent. This is called gradient descent. There's many ways to compute it, and gradient descent is the simplest one. In machine learning, these ways of computing are called the optimizers. So this was a very quick view, and there are three main components here. That a neural network is a parametrized form of computation, parametrized by weights. Second is that there is this notion of backpropagation of changes. And the third is that we do parameter update. That we do parameter update. So, this simple story permeates all of deep learning. And today, the plan is to take a bird's eye view of neural networks, to trace out this information flow, and to precisely write down these high-level notions in isolation, differentiation, and we're going to be using reverse derivative categories, bi-directionality. So, this is the forwards and backwards computation, which is done with optics or lenses. Done with optics or lenses, and the notion of parametrization, which is done using the para construction. And we're going to see how they, this is the idea behind category theory. We're going to define them separately and just put them together in a nice way. So we're going to see that parametrized optics are a common structure behind neural networks, loss functions, and optimizers. And Paul is going to then give really concrete examples of how these neural networks work and some very interesting layers. So let's get started. The first idea is differentiation. So, this is the tangent categories workshop. So, lots of stuff was mentioned about this. Namely, what were mentioned is Cartesian differential categories. It turns out, as Jonathan mentioned before, they have an analog, which is called Cartesian reverse differential categories. And I'm just going to go through their definition. So, a Cartesian reverse differential category. A Cartesian reverse differential category for every map from F which goes from A to B has something called a reverse differential combinator, which goes from A times B to A, which is subject to seven axioms. These axioms are analog of the Cartesian differential categories, axioms just sort of flipped. And you can compare the standard differential operator, and you can see that the first arguments are the same, but the rest. Arguments are the same, but the rest is sort of, we flip them. So the example, there are many examples, but the two of them that we're going to use in this talk is the category of Euclidean spaces and smooth maps. That is a Cartesian reverse differential category. And so is the category of polynomials over Z2. So really, to see some maybe really concrete example, if we have a function which takes in two values and outputs this. Values and outputs testing. Then its reverse derivative is going to take these two values that were the input of the function. It's going to take a change in the output, so we see there's just one change, and it's going to compute a change in both of the inputs separately. And this is the story that Jonathan said of we just do the transpose of the Jacobi. Now, this category is pretty complex, and we're not going to study. And we're not going to study it on its own. We're actually going to study it through optics and lenses. And we're going to really see how these are intertwined in interesting ways. So what are optics or lenses? So they are a model of bi-directional transformations. So we're going to start with the definition of optics and really unpack what's the intuition behind them. So they're defined for any symmetric monodal category. For any symmetric monodal category. So, the category of optics has objects, pairs of objects in C, and an optic between x, x and y, y is this funky thing given by a coant. So even though this might look daunting, this has a relatively straightforward interpretation. It consists, so an optic consists of three things, a choice of something called the residual M, which we're going to draw here, and an optic from and an optic from x and x prime and y and y prime is something of this shape. So we see there is a forward pass and we see there is some state saved and then there is a backward pass. So this align idea of back prop, we do forward pass and then a backward pass. So what is an optic? We said that an object M and then the forward map really goes, it's really goes from It's really goes from X to Y tensor M, as we see here. So from X to Y tensor M, and then the backward map goes from Y prime tensor M to X prime. We're going to really study what this looks like, but there is one interesting, and there is, of course, this is quotiented out by this co-equivalence relation. But there is the interesting case if our category. The interesting case: if our category is Cartesian, then there is a series of simplifications we can make and get a really concrete description of an optic. So we can see this sort of in two different ways. So we start out with the definition of an optic, which is shown on the right side in this animation. Since the monodal product is Cartesian, we can use the universal property of the product to factor this out into two maps. into two maps. So instead of x to m times y, we can really factor it from into x to y and into x to m. And then the last step is do this thing, do the UNEDA reduction, which basically means like sliding over the part of the map. And that gets us two map, which are usually called the get and put map of a lens. So in this Cartesian case, optics are isomorphic to lenses. And we're going to really be focusing And we're going to really be focusing quite a bit on lenses and optics and understanding their differences. So, as I said, there is this bi-directional information flow. We have computation flowing forwards, and in the lens case, we save this state and use it in the backward pass. When you write out lenses, maybe it's not exactly clear how to compose them, but when you write lenses as optics, it's relatively straightforward. Optics, it's relatively straightforward. You really just plug things together the only way you can. So, two optics composed together save two residuals and collect them back on the backward path. And of course, you can just forget the insides and have the tensor of the residuals as the residual of the composite. Optic is going to be symmetric monoidal, which is going to be important for later. But for now, I really want to give an example, just to have something in mind. To have something in mind while we talk about this. So, an example of an optic error lens is gradient descent itself. So, if you recall the previous example of weights and the loss on this axis, and then we where we have a starting parameter, p, we have a change in that parameter, which I'm just syntactically writing here as this change in orange. And then we compute an updated parameter. And creating the scent works by moving in the works by moving in the alpha scaled direction in the negative alpha scale direction of of the change alpha is something called the learning rate and is usually fixed beforehand so gradient descent is a lens its get part is going to be so it's a lens from pp to pp prime so the forward part is going to be identity on p we're going to save this p that we had and then the backward part is going to be the put The backward part is going to be the put, which has p times p prime into p. And we're going to use it to compute this thing here. What's interesting to note is that many optimizers in machine learning are actually lenses, and moreover, they are parametrized lenses. I'm not going to go into the details because of time constraints, but this is something relatively interesting. And we can ask maybe questions later. Now, going back to Katie. Now, going back to Cartesian reverse differential categories, if you remember, we have for every map F from A to B, we have its reverse combinator. And you can notice that this looks like get and put map of a lens. If you notice this, you would be completely correct because there is a functor from C to lens C for every Cartesian reverse differential category, which on all Which on objects picks out the diagonal, and for a morphism F from A to B, it picks out a lens. And what lens is it? Well, it's the lens whose get map is F and whose put map is R of F. So this is an interesting theorem that is going to be the cornerstone of our framework for backdrop. We're going to start with these maps and then just apply this functor. And that's going to give us in And that's going to give us, in a Cartesian reverse differential category, it's going to give us the going to augment the forward pass with the backward pass. So that's sort of what TensorFlow and all these frameworks are doing. But that's not all, because we also have the notion of parametrization. So as I said, neural networks are parametrized by their weights. Depending on what the weights are, the output wise is going to be different. And the construction we're going to be. And the construction we're going to be using to capture this is called para, and it's defined for any symmetric monodal category C. So its objects are objects of C, but a morphism from A to B is going to be a choice of some parameter space and a map F, which goes from P times A to B. Now, this is a bicategory, so it's going to have two cells, and two cells are going to be reparametrizations. Reparametrizations. So if I have a P parametrized map F from A to B and a Q parametrized map G from A to B, a two cell R is going to be a map between these parameter spaces such that the only possible triangle that makes sense commutes. Namely, if we start from Q times A, we can either apply G or reparametrize Q to P and then apply F. And this is a two cell if this commutes. It should say such that this commutes. It somehow ended up missing. This commutes, it somehow ended up missing. So, examples of para: if we start with set with the Cartesian product, we're going to get the category, para set is going to be the categories of sets and parametrized functions. If we start with the category of Euclidean spaces with the Cartesian product, we're going to get para smooth is going to be Euclidean spaces and parametrized smooth function. And well, you get how it works. We just keep the object the same and morphisms are parametrized. Same and morphisms are parametrized. Now, the interesting part about para is that it comes with a graphical language. So I'm going to show how something works in textual notation, in standard Sphinx diagrams, and in something we're going to be using heavily here is these two-dimensional Sphinx diagrams. So a normal map, P times A to B, would usually just be drawn like this. But we really want to separate out the parameters on a different axis. This is sort of like the private information of the learner. Information of the learner, of the agent that we're going to be discussing. And this also makes the two cells simpler. Instead of drawing the reparametrization on the left, it really is here on the top. And we sort of keep this idea that parameters are separate. I also didn't tell you how composition works. And it works again in the only way possible, which if we wait for a second, so if you have one parameter is map and another one, we plug them together and this. Together, and this two-dimensional notation really makes it obvious what's happening. We thread along the parameters on the vertical axis. So, a quick recap is that a neural network can be shown as a parametrous computation, where, for instance, these weights would be this matrix, and this weights here would be this input here. In this case, it's a matrix. So, I'm going to just continue on because I'm realizing I'm farther along. Realizing I'm farther along than I would like to. Well, I'm lagging behind, that's what I mean. So, para is natural with respect to base change. I told you that if we have a category C, which is monoidal, we can form para C. But if we have a functor from C to D, that induces a functor from para C to para D. And I'm sort of going to maybe skip through this because that's not the most interesting part, but it's sort of done again in the only way possible. Sort of done again in the only way possible, uh, and uh, it's not maybe too important, but the whole idea is that para is also rich in categorical structure, it also is something we're not going to talk about. The star of today is the notion of a parametrized optic. So, I said that if c is symmetric monoidal, we can form optics, which are themselves symmetric monoidal. And that allows us to form para-optic notion of parametrized optics. So, this category is a So, this category is a pretty complex beast, and morphisms are pretty complex. The objects in this category, so objects of para of something, are just objects of that something. So, it's objects of optics, but those in turn are objects in C. Now, morphisms are parametrized optics. A morphism from xx prime to yy prime is a choice of a parameter space, which again is a pair of objects, and a map f, which goes from, well, this complex thing. Now, how do we think? Complex thing. Now, how do we think about this? And it's pretty simple. If you know what an optic is already, the simple idea is you're going to have one extra input coming from top, which is going to be written P, and one extra output on the bottom called P prime. And we see that sort of this, this goes out of the plane, this goes into the third dimension, and we see that it aligns with the idea of para. So, how does the information flow? So, how does the information flow work? We really, this is the thing you've seen from before. We have some inputs, some parameters, save some intermediate state, compute an output, wait for the environment to respond, use that state to compute these two things. Of course, we can compose parameterized optics in sort of the only way possible, which is again shown here. But maybe the interesting thing that I want to get at, because there's lots of ground to cover, is that we get two cells now. We get two cells now, and a two-cell here is going to be an optic itself. Because remember, a two-cell in para of something is a map in something, so it's an optic. And there is this theorem now that you can say that gradient descent is a two-cell in para optic, since it is a lens. Now, I'm going to show how this exactly works a little bit below. And this hopefully is going to become clear. So, there's going to be just some time to talk. Clear, so there's going to be just some time to talk about how this works in a nice graphical way. But one other idea is that really we can apply para, this action of para on morphisms to this to the Cartesian reverse differential categories functor, which takes a category and augments it with a backward pass. So we can apply para to this and get something that really allows us to talk about how we can differentiate neural networks. So we're going to. Neural networks. So we're going to get a functor. Remember, it's going to, instead of C to optic C, it's going to map para C to para optic C. So it's going to take a parametrize map and augment it with the backward pass. That's how we think about it. And we see that the get, the first, the forward pass is the same, but we just add the backward pass. So what does this all mean? I've just really rushed through a lot of stuff. So hopefully now it's going to come together with some examples I'm going to give. Together with some examples, I'm going to give, and then I'm going to hand it over later to Paul. This is a functor, and I'm not going to prove it now, but I'm just going to show you that functorality is important. And an example I'm going to give you is of a neural network plus a loss function. So if you remember from before, we had a neural network and then it produces an answer. And then we use a loss function to say how far away we are. So on the left side, you see two things. You see a map F, which is our parametrase map, which we think of as the forward. Parameterase map, which we think of as the forward path of a neural network, taking in an X and a parameter, say this is a cat, right? Produces a prediction. Then we have the L standing for loss, which takes in that prediction and a true output and produces a real number. Now, the functoriality here tells us that we get the same result if we first compose these things to get a composite parameterized map and then augmented with the backward pass seen here, or if we individually. Seen here, or if we individually augment these components and then compose them, we get the same result. And finally, we can sort of put the pieces together. I told you what a neural network is, what a loss function is, how we can augment them. And I also briefly mentioned that there is gradient descent here. So, really, if I want to substantiate our argument that we sort of can do deep learning in this. Can do deep learning in this highly synthetic setting, I can now put the pieces together and show how supervised learning works in our framework. So this is the complete image that we have here. And if you remember from the beginning, we started with on the left side, we had an image of a cat. This is here labeled by A. On the right side, we had the label of this cat, which said this was a cat, this is called B. And in the middle, we had neural network weights. So, what happens here is that these inputs flow from top to the bottom, and I can play this animation and it sort of goes in a mechanistic way that these things compute here. And the blue thing that computed on the output is going to be a new parameter value. So really, let's unpack what happens here. Now, you remember, many things are coming together. I'm drawing gradient descent here, which is the box where the blue dot is, vertically. Blue dot is vertically, which is a two-cell, as I've said before. So let's study things coming from the left side. A comes in and it shifts in direction and gets inputted into the F, which is our parametrized map. We use this together with the parameter value to compute two things, an output and save this intermediate state, which is sort of tilted. Then this output Then, this output is used as the input to the loss function, together with the value with the label that we had. The loss function computes a distance which we really don't care about in the end, but it also saves this intermediate state, which is used in the backward pass. We compute two things, a change in the label, which we forget, this is the red thing, but also a change in the prediction B. Once we do that, we use this change in the prediction B together with the previously saved. B together with the previously saved green dot on the diagonal line to compute two things: a change in the input A and in the parameter. Can you see me? I see. Oh, okay. The image slowed down. And in the end, we basically subtract the changed parameter, the change in the parameter from the computed parameter value and arrive. The parameter value and arrive the new parameter. So, this was a very fast tour of how we're thinking about this. And I think this is where I'm going to stop and just let Paul take over. Cool. Thanks, Bern√©e. Let me see if I can share my screen. Oh, there we go. Okay, can you see that? Cool. All right, so Bruno's given. Right, so Bruno's given us all this like nice categorical machinery. You know, parallel lens, we've got this kind of two-dimensional syntax for drawing these diagrams. And now we're going to do some machine learning with it in kind of excruciating detail. So basically, I'm going to kind of open up each of these and sort of look at the very, I guess, fine structure of them, I suppose you could call it. And so I'm kind of subtitling this section, How to Build in Your. Subtitling this section: How to Build a Neural Network Out of Lenses. Um, and the my hope is that by the end of this, like, if you're so inclined, you could write this in code. You should have enough information to kind of write a simple neural network in code so that you can put in an example bit of data, you can put in an initial guess of parameters, and you can improve it to get a new parameter out. And you can then apply the functional times, and you have machine learning. Now, since the model Now, since the model part of this diagram that Bruno showed us is essentially a design choice. So, given a problem, for example, the images that Bruno had, you have cats and dogs, and you want to predict, is it a cat or a dog in the image? The design of this morphism is going to depend on the structure of that problem. So we're basically just going to be doing this by example. This is kind of the creative part of the machine learning process. One of the other things I want to do, aside from convincing you or showing you sort of end-to-end how to build a neural network, is convince you that this diagram of a neural network is better thought of as what Bruno showed us earlier. So a composition of two of these dense maps. And I'll come back to this later. I'll explain exactly what's in here, why these are these values. And one thing I want to point out here before I move on is that the Is that there's some terminology in machine learning which is a bit fraught with peril, and namely the word layer. So often people say layer, and sometimes they mean the values of these nodes, which correspond essentially to objects in the string diagram world. And sometimes they mean a morphism, which corresponds to this kind of matrix multiplication thing here. So I'm going to say layer a few times. Usually I'm going to mean morphism, but just to be clear, sometimes it's ambiguous. Be clear, sometimes it's ambiguous. Um, and so there's kind of three levels of detail I want to look at. So, first of all, what Bruno's already showed us is this top-level view of learning where you have these kind of course-grained components, you have these kind of loss components, update component, which is, for example, grain of descent, and the model. The second level of detail I want to look at is what is the model, what's the kind of high-level structure of the model? And that's what neural networks people generally call models architecture. People generally call models architecture. So, for example, this is a picture of an architecture taken from a paper called One Model to Learn All by Google Research. And really, it's like a composition of, you can think of these are the layers, and you can kind of think of a subroutine. So, it's like a self-contained bit of functionality. So, this does a matrix times a vector. This is a convolution. And And this level two detail is like this high-level structure of how you compose together all these sub-regions. One thing I thought was fun to point out here is that this is not a group of category theorists, but they're still kind of using string diagrams. Maybe they knew this, I'm not sure, but it's kind of interesting that they've got this very string diagrammatic location. So finally, we're going to actually look at what goes into one layer in particular. We're going to again do this by example, but Going to again do this by example, but for example, we're going to dig into the matrix multiplication layer and say what are the get input maps specifically. And so, hopefully, by the end, as I said, we should be able to build everything up from the smallest detail. So, I'm going to structure the rest of this talk. So, first, going to quickly recap what Bruno mentioned about supervised learning and talk a bit about reverse derivatives, give some examples of reverse derivatives. Then, we're going to go through this end-to-end example of the neural network. And then I have some other sort of bits and And then I have some other sort of bits and bobs. These are kind of techniques that get used in neural network land, and we're going to sort of translate them into our framework if we get time. Okay, so super for that. We start with a data set of example pairs. So we have, as Brian said, this might be an image, this might be whether it is a character dot. And we want to learn a map which takes an image and gives you the label. Now, Now we have to construct this model, as I mentioned earlier. This is like the creative process. We have to say, like, based on our belief about the structure of that data set, we have to define this morphism up front. And the P serves here to kind of index a set of maps from A to B. So we're going to try and find some P which ends up with the best map from A to B. So that process of searching for that P, for that theta sorry, is the machine learning bit. Sorry, is the machine learning bit. And in gradient-based learning, the way we do that search is to take an initial guess of theta and kind of iteratively update it using each example in our data set. Sometimes we use multiple examples at once, but for now I'm just going to keep it simple. So what we need is some kind of map of this type where we take our current guess, we take an example input from the data set, an example output, and we give a new, hopefully better guess. This isn't guaranteed to be. Hopefully, better guess. This isn't guaranteed to be better, but after showing enough examples, we can hope that he gets a bit better. And although the reverse derivative kind of has what looks like the right type, it's not quite what we want. And Bruno's already mentioned this, but again, I kind of want to emphasize. So recall the definition of the reverse derivative says that from we get this map A times B to A. Now I've added primes to these to emphasize that the kind of role of the value. That the kind of role of the values here. So the B in the R of F is really a change in B, and the A prime is a change in A. So this doesn't actually mean anything mathematically. It's just kind of a useful way to think about it. Paul, we have a question. So Cole asks, is there any particular reason why the objects are all of the form AA? Because the string diagrams for objects. The string diagrams for objects are those look a lot like the CPM construction, which is applied to a compact closed category. A few slides back where you gave this concrete example. This was like I was writing the where am I looking? You missed it. Where you have like R to the power of something? Oh, right. Yeah. Like, see, like, all the things going both directions. See, like all the things going both directions, they're like the same type. Is there a reason for that? Um, a basically that's because a equals a primed in this kind of simplified setting. Like in general, we maybe like the type of changes to be different to the type of points. That's what you're saying, right? So this should be different to that. Yeah, just because the diagrams look kind of similar to like the CPM construction, although it's not doesn't, it's not the same in the Cartesian case, but when the objects are of the But when the objects are of this form, I mean, the reason this is because basically just the type of a restriction says it has to be this way. But if you start to think about if we actually wanted these primes to be different, then they would change. Does that answer the question? Yep, thank you. Okay. Okay, so we're going to try and think about these primes as changes, even though they're not really different. Even though they're not really different. So, if we take the reverse distribution of our model, it has this type, which is almost what we want, because remember, we want something of this type here. And we've almost got that, except that we have two problems. Number one, so we have this true value b from our data set. We have a predicted value, which is what our model gave us when we ran it on the data. But what we need is a b prime. So, we kind of want a way to, in general, like. A way to, in general, like subtract two b's to get a b prime, and that's kind of what the loss gives us. I'm going to come back to these in more detail later, but that's essentially the role of the loss function is to give us a b prime from these two b's. And then similarly, the reverse distributive gave us this p prime, but what we really want is a p. So instead of changing parameters, we want new parameters, and they're different things. And so that's the role of the update map or the gradient descent part that Bruno showed us. That we may show that. And the simplest case of that, it's just subtraction. We'll only addition, probably addition. So now I'm going to show some like, I'm not going to give the actual definition of the reverse derivative because I think the paper does a really good job. I just, and I don't really need it to be honest. I'm just going to give some like simple examples of reverse derivatives. So we have Cartesian structure because it's Cartesian left over to category. So you have copy and discard maps. So you have copy and discard maps, and it has polynomials, or I guess tuples of polynomials. So in the category poly sub par, those would be these functions. So it's a pair of functions each in a single variable that each return that variable. And then discard is the zero tuple of the signal argument. We also have left additive structure. And although that's normally described as you can add maps and you have a zero map, there's an alternative view of it where you have an additional. Alternative view of it where you have an addition map and a zero map of these types. I quite like this view because when you look at the reverse derivatives, you sort of get, it shows that they're sort of flipped versions of each other. And we can also see that the reverse derivatives will say like copy and add, they're all linear because they throw away the point at which they're taken. This is how the RDC's paper defines linearization. So, I'm going to skip past that unless anyone has any questions. We also had the definition on the composition of the reverse derivative, which is the reverse chain rule, it's axiom RD5 in that paper. And the monodal product just looks like this. So you have two changes and two points, and you just rearrange them to get the correct types. Okay, so we're finally going to actually talk about neural networks. So, I promised you earlier that this is what a simple, like, hidden layer in your network actually is. And so, all we need to do to really understand this is just unpack the dense layer and see what's inside. So, there's three things in there, really. First thing is a linear layer, which is a matrix vector product, a bias layer, which is an addition, and an activation, which is a non-linear function. So, I'm going to go through each of these. I'm going to start with a bias layer. I'm going to start with a bias layer just because it's the simplest one and it should be make the others a bit more clear. So because it's a lens, it's two maps. So the get map just adds the parameters to the inputs. So you can see the get map is appearing in this kind of lensy notation down here. And the put map is this guy, as we saw earlier on the definition of reverse derivative. And that's appearing. And that's appearing down here. And although this kind of looks a bit hand-wavy, the reason there's a copy here is basically the definition of composition in lenses and also reverse directives. So if you recall this guy, that copy that was appearing there, that's the same as this one here. So that's this copy. So we can simplify this a bit, and then it becomes quite nice. You can see that. Then it becomes quite nice. You can see that in the forward direction, it's addition, and in the reverse direction, it's copy. And I had to draw arrows here just to make that clear. So this is a bias layer. And again, all it does is add parameters to the data. So the kind of modeling assumption here is that your output is a constant plus your input, which is obviously much too simple an assumption to use in a real world, which is why I put layer in quotes here. But that's kind of the But that's kind of the way to think about this if you were to think of it as a model. So, I guess analogously, a linear layer, the assumption is that the parameters are the coefficients of a matrix and they represent a linear map. So, the modeling assumption is that there is a linear map between your inputs and your outputs. So, you interpret P as this linear map, you interpret your input A as an A-dimensional vector, and then the forward path just multiplies the matrix by the input vector. I'm not going to like derive the reverse pass because it's a bit tedious, but there's kind of an obvious thing that type checks. So, if you take the forward pass, sorry, the get map as a matrix and a vector, then the codomain of the reverse derivative should be this type. And so we need to construct a B times A matrix and a length A vector. So we can get a B times A matrix by doing the outer product of the changes in the output. The changes in the output times the x times the input, and we can get the length a vector by doing the transpose of the matrix as well. So this is everything that's in a linear layer. And then finally, the activation layer, it's a non-parametrized, it's like a trivially parametrized lens. So it's basically just a point-wise non-linear function applied to each dimension of the input. function applied to each dimension of the input. So for example in R2 it's just this some non-linear alpha applied to each dimension. And so the reverse distribution is very simple by the tensor product we get this. And then that gives us our sort of final picture. So then we know how to build up a dense map from each of these components. So we have each of the pairs of maps that facing these and divide a dense map and so we can define the full end-to-end hidden layer and all that. Hidden layer in your network. So, I guess this is just really the same as anything. But this at least explains why these numbers of parameters are what they are. So, we have these are the matrix weights, there's eight of them, and then the bias weights, there's two of them. And that's because eight plus two. Okay, so now, as promised, I'm going to substitute in, I'm going to make all these boxes transparent and sort of show what this end-to-end looks like. Like with kind of in full detail. So here I've substituted some gradient descent for the kind of reparameterization part. I've done this slightly different to Bruno, but it does work out the same thing. So we multiply our change in parameters by some scalar, some small scalar, and then we do I need add? No, I mean minus. Okay, so then we subtract it from our current parameters. Then we subtract it from our current parameters because we want to move in the negative direction, and we do that for both the sets of parameters of each dense layer. I could have drawn the blue box around this whole thing, it's really the same thing because this is kind of a pointwise copy, a pointwise subtraction. And then the last part I've drawn slightly different as well, but ultimately what this part amounts to is a subtraction of our Of our prediction, we subtract, we do our prediction, subtract it, subtract the true label. And Bruno described this part as a pair of maps. So you'd have loss and the reverse derivative of loss. In this case, the loss function would be the mean squared error. So this is the reverse derivative of the mean squared error, where we discard the part of that reverse derivative that corresponds to this input. That's kind of hard to explain without writing it out, which I now regret, but that's what it is. So we have some code implementing this. You can find it here. It basically works as the theory says. So it's a bunch of lenses composed together. And we have a couple of examples where you can actually train models using this. So we have this kind of simple hidden layer neural network that we just described. We also have a more complicated model, which is a convolutional neural network. It's for doing image processing. It's for doing image processing. So, this MNIST data set problem is digit recognition. So, you take a small black and white image of a handwritten digit, and you've got to say, What digit does that represent? And what's the time? What three minutes? So, I might talk about INSF, but we'll see how far we get. So, I'm just going to go through some sort of miscellaneous examples of other techniques now. So, first one is wait time. This is something that's, I think, This is something that's, I think, fairly commonly used in deep learning now. And the general idea is that you have two different parts of your model that use the same parameters. So in the forward pass, you just copy the parameters into each of those parts of the model. And in the reverse pass, you kind of add up their contributions. So what this is saying is like if your F model was off by a certain amount and your G model was off by a certain amount, you sum those to get the new parameter. So like So, like, yeah, hopefully, that makes sense. But the backless parcel, this is, of course, the reverse derivative of the copy. Then another thing we'll do is, as we said, convolutional layers. And you can kind of think of these as doing some wait timing. So a convolutional layer takes, for example, a set of three by three patches in an image. So here, this is Patches in an image. So, here this is supposed to represent an image of a handwritten digit. So, for example, in the endless problem, this would be a zero. And we'd look at all of these three by three patches in the image. And the modeling assumption is like you have spatial locality in your features. So, hopefully, a three by three patch can tell you something about the global image. Anyway, you slide this window across the whole image to compute kind of a smaller, a slightly smaller image. A slightly smaller image. And because you're sliding this window, some of these inputs are being used in the same part of the model more than once. So the P here corresponds to the weights of that 3x3 patch. And so that's kind of where the weight time bit comes in. And then finally, Bruno promised I would talk about a bit about a different setting. So, so far, everything's been talking about like smooth maps. So, we've been dealing with real numbers. But we also have this kind of weird setting of polynomials over ZT. Fabio and I had a paper a while back about essentially can we do learning in this setting? And the other And the answer is kind of yeah. It's a bit weird because obviously your base values are individual bits. So instead of doing kind of a smooth tweak to your parameters, you have to flip a bit. So you can either not change your parameter or change it as much as possible. One kind of consequence of doing learning resetting is that some very weird layers are possible. So namely this one, the lookup table. This one, the lookup table. So you can encode a function which has one parameter for each possible input value, meaning you just look up what the output should be for each input you get. Obviously, that's exponential in the size of the input. But in the case where you have, for example, a convolution, you can just do a lookup for each image patch here. So it's quite interesting. It feels very different to the neural network setting, but because we have all this kind of Casper open machinery, it's quite interesting that. It's quite interesting that we can even ask this question like this. So, yeah, that's it for me. Since it's 45 minutes past, I'll leave it. Great. Let's unmute ourselves and thank both Paul and Bruno. All right, we have time for some questions. Gordon. You're muted, Gordon. I should be unmuted, yeah. Yeah, so okay, so a slightly long question. So, that was a very lovely high-level view of supervised learning. So, the general question is, how can you then help the practitioner? So, the kinds of thoughts that occurred to me, we would have modular specification of supervised learning programs, you could have new programming language features, you could have a special programming language with it based on a lens type type theory, and however you do it. Type theory, and however you do it, the examples should then become beautiful programs in this new way of writing programs for supervised learning. End of question. I guess one is going to say one obvious answer. I think you already kind of answered the question is it at least gives you a way to put together kind of differentiable programs with lenses. One thing that was useful in writing the implementation was that you can. That you can provide both the forward and backward pass and sort of hand optimize the backward pass. Because sometimes, for example, in the convolution layer, it turns out the put map is something like a convolution of the output with the input. I can't remember exactly what it is, but it's like a very, very efficient backward pass. And I assume that that wouldn't be easy to get by, like, say, compiler optimizations. Like, if you relied. Like, if you relied on kind of automatic differentiation of some code without that knowledge, it might not get the same performance. Yeah, I think there were lots of other parts to your question, though, that maybe I've missed there. Yeah, the question is what new programming primitives would be useful for the programmer. Programmer wants to write down your examples very nicely and keep the structure. And since you're focusing on lenses, that would suggest a special type theory. Suggested a special type theory and a special programming language. The idea is that differentiable programming languages are like completely low level. All they do give the programmer is differentiation. They don't give the programmer any way to write down the structure of what they're doing. But you're providing the structures, so it should be made available to the programmer, not just diagrams, which they're never going to use. I mean, at least in terms of our implementation, again, like you can express a lens as a Like you can express a lens as a get and put map, right? So you don't, it didn't feel like there's any extra structure you really need, any extra primitives you really need to compose lenses, right? Right, but you don't give the programmer any way of composing lenses. The lenses are not a primitive you're giving them. I mean, differentiate. Sorry, differentiate. Yeah, sorry, go ahead. I was just going to say, maybe I can try answering. So the programmer, just like in, I mean, The programmer, just like in I mean, TensorFlow is already like secretly using this. It's just that we don't know. The programmer just composes the maps as normal, and the functor that we set from C to lens C augments this functorially. So the programmer doesn't know. Maybe the construct that is useful for them is the fact that the optimizers are lenses as well, and the fact that one can work really internally to lenses. So one can really compose lenses, one can do this stuff, and to maybe build even more, like this is just a part. Build even more, like this is just a part of optics. So, one can do control flow with prisms and all this other stuff. So, maybe this is a different answer to your question. Okay, and one very short phrase, I'm taking up too much time is how can you give this work to the programmer? Question mark? Okay. Well, yeah, I guess I'm maybe if I share my screen here, here's how, again, I don't know if this is going to answer your question, but I hope it does. Here's how in Python you. Here's how in Python you define a dense layer using our framework. It's just a composition. This is diagrammatic order composition of lenses. So this is the parameter part. This is the matrix vector multiplication. This is the bias. This is the activation. So we didn't need any like special primitives to define it this way. And this is like basically just building a string diagram with Python operators. Have I missed the point? Have I missed the point again? I'm feeling frustrated, but I'm taking far too much time, so I think I'll stop at this point. We can take it afterwards. Yeah, so yeah, yeah, yeah. You can take this chat to Gather Town maybe later. Mario? Yes. So first of all, thank you for two brilliant talks. So this is an area that So, this is an area that is very, very dear to me because I ran into derivatives in Boolean algebras a while ago in a very different context, in the forward mode rather than reverse mode. And at the time, we thought a bit about how to do gradient descent on Boolean algebras and something that And something that failed for us is like, for example, if you have a conjunction, so X and Y, and you want to flip the output bit, you have two possibilities. You can flip one of the inputs or you can flip the other. So, how do you pick which one of those you do? And this is a problem that. you do and this is a problem that that that happens more more generally in in the general reverse differential or reverse difference settings right um in in in vector spaces if you're talking about classical gradients you just what you do is what you do for addition right you take the the perturbation on the output and you copy it as the perturbation to either input but in discrete settings does does this approach Do you think this approach still works? Do you non-deterministically pick which of the inputs you flip, or how do you deal with these issues? I don't have a great answer to this, but I think it's really a modeling problem. Because as you say, things don't, the way you construct your model architecture in Boolean land kind of has to be different because of problems like this. So I think there's an example model where Where um I can't remember exactly what it is. It's something like an exclusive or only as your mock exclusive or combined with an AND gate somehow and it will just oscillate on the output when you try and train it. It will never converge to anywhere useful. And there's lots of examples like this where it has some kind of pathological behavior. And we didn't really study like the like convergence behavior of it. We just kind of did an empirical test. So my answer I think is that it's like a model My answer, I think, is that it's like a modeling problem, really. You have to construct your architecture in such a way that this doesn't happen. But I think that sort of applies to deep learning as well, really, because you can build a bad model in deep learning that won't work. It's just that it won't work in different ways. Certainly, well, so you have to buy the bullet then. Well, I mean, maybe there's a better way, but yeah. No, I think that's perfectly reasonable. I just wanted to know if maybe there's some very, very smart thing that. Very, very smart thing that you can come up with. Maybe. Okay, last question from Michelle. Thank you very much. Thank you, you both, for the beautiful talk and the work. It's wonderful. And there is something at a certain point, if I understand well, there is a simplification process. At certain point, I see a diagram, and then you And then you simplify the diagram according to some equation, if I understand well. And I would like to know whether in the implementation this simplification is something reliable or what is the complexity of this simplification process, if you have an idea, if I understand well. Was that this one? Exactly. This is an example, I guess. You have other rules. Okay, exactly. No, no, we didn't have this. Okay, exactly. No, no, we didn't have this. I think to do this properly, you need to have like a symbolic representation of your program. Um, because it would have to be aware of these rules. So, right now, our implementation is just it will just compose maps in this way. So, like, this is this is also Lennon's composition as well as the like reverse derivative chain rule. So, no, we don't, we don't do that simplification. Um, and in theory, do you have an idea of the complexity of this kind of rule? No, I guess it depends like how many of the simplifications. I guess it depends how many of the simplifications you would want to do, because this is just applying the discard map rule, right?