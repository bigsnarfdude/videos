Because the place is gorgeous, and the sessions so far and the talks have been fantastic. We might have a dip in quality just about right now, but hopefully. So I will be talking about MCMC important sampling via Moro Yocida envelopes. We're combining together a couple of different things. So I'm going to try and keep it not so technical. I might make the mistake of going a little technical at some point, but I'll try. Technical at some point, but I'll try not to. So, first of all, this is work led by my PhD student, Apratim Shukla, at IIT Kanpur. And in collaboration with Eric Chi at Rice University, the collaboration particularly came about because of this IIT Kanpur and Rice partnership, and they wanted to fund grants. We sort of knew each other, so we wrote a grant, and turns out it was useful. But I'm going to set context because I'm talking about. Set context, because I'm talking about NCMC and important sampling, so about computational stuff, but I sort of have to speak to Bayesians. So I sort of keep the context in mind, but the context is going to be Bayesian posteriors and the need to either sample from it or obtain estimators of certain useful quantities. I'm pretty sure I don't need to explain what's going on here, but essentially I have a target density that is intractable, only known up. Is intractable, only known up to proportionality, and I want to be able to learn things about this particular posterior distribution, posterior means, credible intervals, et cetera. So it's our usual Bayesian problem we find ourselves in. And a useful solution to the problem has so far been Markov chain Monte Carlo, certainly not the only solution. But in MCMC, what one does is one constructs a Markov chain. We're going to call that XP. We're going to call that xt, such that the stationary distribution is my posterior density. There's a typo here. This should be pi x given y. I thought I got rid of all of them, but at least one has remained. So once I've constructed the Markov chain, then I can use basically sample statistics to estimate population quantities, just like regular statistics, except it's annoying. But I can use samples like sample averages to estimate the mean of the posterior. And I can just use the x bar n in order to do that. Just use the X bar n in order to do that. And Markov chain theory tells us life is going to be okay if we do that. Not great, but okay. Question is: how do I construct such a Markov chain? There's lots of general recipes, and the most common of that is the Metropolis-Hastings algorithm. And so from now on, I'm just going to limit my design to be that the target density takes the form of pi of x proportional to e to the power minus psi of x. It's just the way I'm going to think of it. Think of it. So I've removed the conditioning on data because I'm just thinking of a target density. And given a point small x, I want to see how I obtain xk plus one. And that's done by the whole mh strategy, where I draw a uniform and keep it in my pocket. And then independently, I draw a y, which is my proposal. And that's the proposed value I want to move to. And that's conditional, potentially conditional on my current x, right? And then I choose to go to that point. And then I choose to go to that point y if u is less than or equal to this particular ratio. That means I'm doing this task with probability being this expression. And the fact that I don't know pi completely is fine because the normalizing constants cancel out. And so I can just easily evaluate this. And otherwise, I reject. And so this has been a very sort of simple and almost universally applicable strategy. Strategy, the choice that a user has to make is what kind of queue do you want to use. And that really changes the game in MCNC, the style of proposal that one chooses. A very common one is a random walk metropolis, which is like a Gaussian centered at where you are with the user-chosen variance. Another one can be the very popular metropolis-adjusted Langevin algorithm or MALA, where I use some information about the gradient to go to propose in that direction. To propose in that direction, right? And I'll talk more about these. Okay, I can do this, but then I also want to be certain that I'm actually doing good inferences and I want to understand how well am I estimating posterior quantities, right? Because I have to go to MCMC because I don't know the posterior exactly. I don't know its means, et cetera. So, what is the quality of inference? Well, because XT forms a Markov chain and our samples are correlated, and I'm interested in estimating, let's say, And I'm interested in estimating, let's say, the posterior mean, the correlation in the samples is going to impact the variance of my estimator and hence the quality of my inference. And particularly higher positive correlation is going to drive up the variance, which means I'm not going to do very well in estimation. So, what are typical strategies to reduce or limit this variance, right? If I can consider a setup where I can improve the quality. Improves the quality of inference, then that's really what I want to do with you. So there's one strategy: just choose a better proposal or your MH algorithm in such a way that you have a better moving or better mixing Markov chain that makes large jumps, et cetera. And so the class of gradient-based algorithms are very, very popular. This includes the mala from before. And so a pro is that you basically utilize information about the geometry of your target distribution, and that helps you make informed And that helps you make informed decisions on where you want to go next. And another pro is that there are many, many options available based on geometry. There's MALA, there's Hamiltonian Monte Carlo, there's the more robust Harker's algorithm. There's also like Riemannian manifold versions, which basically take higher order gradient information. Okay, what's another way to reduce variance? And that could be, for example, important sampling, right? Important sampling in the Important sampling in the IID literature has been known to lead to more efficient estimators. So, what I'm going to do for the next, I don't know, most of my talk is I'm going to talk about these two things and see how they interact. Let's look at gradient-based proposals. I'm going to mention pros, but I'm also going to highlight cons because otherwise I won't have a job. I want to contribute in some way. So, I'm going to find cons. So, here's a con. So, if I take the target density, phi of x. Take the target density pi of x being proportional to e to the power minus x to the power 4, which looks something like this. It's wicked thin tails, right? The random walk proposal is the orange curve here for a user chosen H. And the MALA proposal has left the chat, right? And the reason is because the gradient is so large here that it's actually centered at 78. Okay, so if you run a MALA algorithm for this target, it's just not going to move, it's going to keep. It's just not going to move. It's going to keep wanting to move. It's going to keep proposing two big values. And so, typically, gradient-based algorithms require the target to have well-behaved gradients, or it requires the target to be gradient-Lipschitz. An obvious con is that it requires the target to be differentiable. So, if you don't have a differentiable target, then this is going to be a challenge. Okay, so there are cons. Okay, so there are cons. That means there's work to do, right? What about important sampling? So, first, let's take a quick review of important sampling. So, what is important sampling is I'm going to cheat. I'm going to not get samples from pi. I'm going to get samples from g. G is some other density, and I'm going to get samples from that because life is easier with g. But I'm going to define these weights, which is basically telling me unnormalized weights, which is basically the unnormalized target divided by. Basically, the unnormalized target divided by G. But instead of taking usual averages, I'm going to weight the averages according to W. So I construct the self-normalized important sampling estimator, which is basically the averages of these Y's, except they're weighted by the normalized weights of A. And there's a lot of literature that says that this converges consistent. This is consistent. And particularly very well studied for when YT is. When YTs are IID samples from G. So, important sampling in that case is very well studied. And a big pro of important sampling, magically, is that the variance of theta hat n, which is my important sampling estimator, can actually be smaller than doing vanilla Monte Carlo, which is one of the advertisements for important sampling. Okay, so that's a pro, but I'm focusing on cons. So, what are the cons? Well, this is a little This is a little small, I apologize, but the black line curve over here is let's say a target, which is the Gaussian standard normal target. And so that's there in both. And in the dotted line, I have two different proposals, so important sampling proposals. So this is a normal with variance one by three, and this is the normal with the same variance, but mean being one. And in both of these cases, provably, the variance of theta hat n is actually infinite. And the reason really. And the reason really is that in both of these cases, so for example, in this case, the importance proposal G has lighter tails, or not lighter tails, but the target distribution envelopes the proposal distribution G. So it's always going to be higher in the extremes. In this particular example, we're okay on this side, actually, G is higher than pi or envelopes pi on this side. Then pi or envelopes pi on this side, but on this side it doesn't. And so that's what it leads to infinite variance. Okay, so of course, these are toy examples, but we want to keep in mind that we're trying to build this for like a typical Bayesian paradigm in which we don't know where a mode is, we don't know what the tails maybe look like. And so it's very difficult to build effective important sampling proposals for generic distribution, which has been why important sampling has not been used very successfully. Successfully, or at least generally, in MCMC applications. It does find use, though, in some particular cases. Okay, so what are we going to do? We're going to actually build an important sampling proposal, right? So that's what we're going to try and do. We're going to try and do this using what are called Morojo SEDA envelopes and try and have it be as general as possible. Speaking of which, we're going to start making restrictions. We're going to make start making restrictions. Let psi be convex. Okay, so remember, pi is e to the power minus psi of x. And so I'm going to first limit myself to law concave target density. So I'm going to assume psi is convex and lower semi-continuous. So that's just like a half continuity assumption. And here's what the Moro-Yosedo envelope is for psi of x. So for any given lambda positive, psi lambda x is the infinite. Psi lambda x is the infimum of y in my support Rd of psi y plus 1 by 2 lambda, the two norm difference between x and y. So for every any given x, the function over here is basically a quadratic in x with a with a, I don't know what these are called, but with these with these ABC parameters in a quadratic equation. But the psi lambda x is found over minimizing all Over minimizing all possible values this can take over y. So it's a little difficult to see. So I'm going to do what I did when I was trying to learn about this, or Eric was trying to teach me, is we're going to look at this for the Laplace target. And so this is the non-differentiable target, often appearing in models that have L1 penalization. So it's a very good use case. So for the Laplace target, that's my psiopette. The Laplace target that's my psi of x, right? So it's the absolute value function. And so, what I'm going to do now is I'm going to start making parabolas like this. So, this parabola over here is psi of y plus x minus y squared. So basically, I'm going to change the value of y and create different parabolas at different values of y, right? So, as a value of y, sorry, as a function of x, this function, like I said, is nothing but a parabola. This function, like I said, is nothing but a parabola. So I'm just going to create all possible parabolas for all possible y's, which means I'm going to do this at y equals minus 1.5, then minus 1, and so on. Right? Now, once I've done this, in order to find psi lambda x, I'm just going to go to the x I want to find psi lambda at, and I'm going to see what is the smallest value this can take, which is somewhere here. And then at x equals 1, I'm going to go and see where the smallest value. equals one I'm going to go and see where the smallest value that these can take and once I do that I get some psi lambda x which looks something like this okay so this is my psi lambda x in this particular case this is called the Moro-Yosoida envelope right at least for the Laplace now all I need to do is once I've identified psi lambda x I just need to exponentiate it to create a hopefully a density right and that's exactly what we do is we say by pi Exactly, what we do is we say, well, pi lambda x is proportional to e to the power minus psi lambda x. Okay. And when lambda is equal to zero, I get my original target. And as lambda changes, I get different behaviors. So as lambda becomes larger, I get a looser enveloping for psi. And then I get a smoother target density over here. But what are the things? But what are the things we note? So, first of all, I said density. I don't know if it's a density or not, but the class of Moro-Yoseda densities were studied by Marcelo Pereira and Dermis, Alandermus. And they said that pi lambda is a valid density, which means for us that we can sample from it. Psi lambda is convex and shares minimizers with psi of x. I'm going to go one slide forward, for example. Slide forward, for example. So psi lambda x has exactly the same minimizer as psi of x, which means that there's mode matching, right, for pi lambda x, which means that it might make a good important sampling proposal. Pi lambda envelopes pi in the tails. I can see that it's always, it looks like it's always higher in the tails, which means again, it has the promise of being a good, important sampling proposal. And file. And psi lambda x is actually going to be differentiable everywhere and gradient Lipschitz, which means that its size of the gradients of gradient increments are going to be well behaved always. Okay, so now this gives us a recipe on what we want to do. Before I go, this is the using psi lambda x, the Moroi-Yoseda envelope density is nothing new, but so Pereirio first used gradient of psi. First, used gradient of psi lambda x in the Mala proposal for non-differentiable targets, but then use the mh correction for the correct density. So that means it's a pi invariant Markov chain, but the gradient information is coming from psi lambda x, not psi. And this is what we're going to compare ourselves to, which we call this proximal mala. I mean, they call it proximal mala. So we say px mala, and then we can do a similar px HMC version. Similar PX HMC version where we use gradients from psi lambda and then do mh correction for the HMC. Another thing that we don't compare ourselves to is Alain Dermos and co-authors proposal to run basically the unadjusted Langeman version using psi lambda x, right? But this tests two levels of approximation, the approximation between pi and pi lambda without correction for important sampling, and the approximation between pi lambda and the Approximation between pi lambda and the discretized Langevin SDE. So there's two levels of approximation, and we're sticking currently to exact methods. Okay, so there's enough evidence that this has been employed and can be employed in the literature. And there's multiple other papers, but these are our two guiding light papers. So here's our recipe. First, fix lambda greater than zero. So let's assume we've fixed a lambda, and now it's basically what hope. And now it's basically what hopefully you all think it is: I run my choice of gradient-based MCMC invariant for pi lambda. So you decide you want to choose HMC, Mala, Riemannian manifold, whatever you want is appropriate for your problem, you can use that. And I get the samples Xt. Now, in order to do this, you need access to gradient psi lambda x, which you might already be getting nightmares about. It's not so bad, and so I'll talk about this a little bit. since I'll talk about this a little bit, but gradient psi lambda is actually x minus proximal mapping of x by lambda, where prox of x is nothing but the argument at which the minimization happened. So during the course of this, this is something you already have to calculate. Okay, but how do you get it? I'll actually not talk about, but I will tell you that you shouldn't get nightmares about it. Okay, so let's assume we can do this. And now we have samples from pi lambda, and now all we need to do is correct for the mismatch. And now, all we need to do is correct for the mismatch between pi and pi lambda. And that's by using essentially important sampling. So I get theta hat and my be my important sampling estimator of theta. Now, a pro immediately you can see is that can be used for non-differentiable pi. If my given function pi is non-differentiable, I can actually do this. No problem. But I don't know what the variance of my estimator is because anytime somebody uses important. Because anytime somebody uses important sampling, an immediate question is: wait, what about the variances? Are they well-behaved? Are the weights reasonable or not? So that's our next thing, is we say that irrespective of the choice of lambda, your important sampling estimator exhibits a Markov chain CLT as long as the Markov chain allows it. So that's usually common in the literature. So let XT be phi lambda ergot. T be a pi lambda ergodic Markov chain, so targeting phi lambda, that is geometrically ergodic and has two finite moments, then I have asymptotic normality of my important sampling estimator. Notice that the assumption of moments is not on pi lambda, it's only on pi. And the form of cap, unfortunately, I've decided to call this capital lambda, but the expression of capital lambda is actually available at least in Lambda is actually available, at least in the form of the objects in the problem. And I'll get to this later as well. But essentially, you have an element sigma that's coming from the Markov chain, right? And then you have weights that's coming from the weighting strategy, the important sampling strategy. Again, I'll come back to this. And the proof for this basically involves some Markov chain theory, delta method, as you can imagine, and the fact that the weights actually are always going to be bounded and always bounded by one. And this is coming from. Always bounded by one. And this is coming from the Morojo Oseda properties. Okay, very good. So at the very least, I don't have an undeniably useless estimator. It has a finite variance or can have. But what about geometric ergodicity? How does my Markov chain behave? And so essentially, what we're running are MALA or HMC or any other gradient-based method on Py Lambda. On pi lambda. So there doesn't, we don't need to reinvent the wheel on when these estimators work well and when they don't, because we're just running the usual estimators, the usual Markov change, but on a different distribution. But it is useful. And so we have, we go into some specific results, but I'm going to only give you like the loose picture. So let's consider a target density of this form. And this is a very common family that basically moves tail behavior of the Markov chain. So what this says is that So, what this says is that for x, if you go far enough in the tails, your target distribution, let's say, takes this particular form where beta can be anywhere between 1 to include. And so the value of beta will tell you what the tail behavior of the target looks like. And so when beta is between 1 and 2, 1 included, pi mala, which is your usual mala on pi, pi lambda mala, which is your mala on the pi lambda applied to this, okay, and similar HMC are all geometrically. Similar HMC are all geometrically ergodic. Okay, what about the other cases? Well, when beta is equal to 2, which means you have Gaussian tails, all of them are geometrically ergodic as long as your step size is tuned and well chosen. And this is, again, true for all over the literature. And here's where the gains come in: is when you have very thin tails, when beta is greater than 2, phi mala and phi HMC are provably not geometrically. HMC are provably not geometrically ergodic. However, pi lambda mala and pi lambda HMC are geometrically ergodic if tuned properly, because they're eventually, they'll basically have Gaussian tails. Far enough in the tails, they'll have Gaussian tails. So they are geometrically ergodic. So what we get is that when you have non-differentiability, you can actually do this. And when you have fin tails, again, you can actually do this in both those cases. Now, there are many more things in the paper which I'm completely skipping. A, there are general studies of geometric ergodicity for general targets. I've just given you results on one family, so that did not burden you with a lot of notation. In any full Bayesian paradigm, I'm not only interested in expectations, I'm also interested in credible intervals and different quantiles. And so that can easily also be done using the estimators of 10. The estimators of Chen and Xiao from their JCGS paper. Obviously, posterior means may not be the only thing. I could be interested in second moments or some other functions, and that can very easily be extended. And then the capital lambda in the asymptotic normality can actually be estimated using basically a large literature of estimators that have been employed. And so this is also just off-the-shelf usage. Okay, but here's the second burning question. Okay, but here's the second burning question: how do you choose Lambda? I first said let's just fix lambda. And so, this, we have some theory for it, but mostly intuition that seems to work very well so far. So, first of all, as lambda increases, our target distribution gets smoother and smoother. And another intuition is that because you're basically adding a spherical element to the target distribution, it actually has less correlation if your original target. Correlation if your original target is very correlated. So, for example, if phi is Gaussian with covariance matrix omega, then pi lambda is Gaussian with covariance matrix omega plus lambda ip. So now you've basically introduced better conditioning into your target distribution, which is typically going to be useful for Markov chains running on that target distribution. Okay, so then what's the Okay, so then what's the choice of choosing lambda? Well, a good way to choose lambda, small lambda, is so that capital lambda is as small as possible. The variance of your estimators are manageable. Now, the variance of the estimator has two ingredients that are based on your choice. So this, the IP and theta are already part of the problem. And so their dimension and the object you're trying to estimate. So you have no control over those. But we have control over sigma and the weights. But we have control over sigma and the weights, the expected weights. So, what is sigma? So, as lambda increases, mixing should improve because you get smoother and so sigma improves. So, sigma reduces, which is good. But as lambda increases, the expected weights reduce, which is not very good, because that means if this bottom is smaller, the whole lambda is going to increase. So, here's the strategy: is increase lambda from zero. Is increase lambda from zero until weights are too small. And what is too small? Because we can track the weights very easily. And what is too small? Well, again, this is very well studied in the important sampling literature using Kong's idea of effective sample size, which is basically the square root of the first moment of the weights divided by the second moment of the weights. This tells you, we can basically track NE by N, which tells you what the effective sample size per unit sample is for your important sampling strategy. Is for your important sampling strategy. And we find that if you choose basically any by n in this window, don't let it go beyond lower than 0.4, you're going to be pretty safe. Okay, I'm definitely running over time, so I'm going to do an example too quickly. This is a Bayesian L1 trend filtering problem. As is in most of my problems, I'm not really interested so much in the problem, but more in the Markov chain and the estimation. So I'll definitely not. Estimation: so I'll definitely not do this justice. We have a bunch of observations y, and we want to fit a trend to it. There's no other covariates, just observation. And there's particularly this particular paper, and then Eric and co-author tried to build the Bayesian model in order to do trend filtering. And so, basically, psi b is this function over here, where I'm trying to essentially fit Gaussian noise, but trying to do an L. But trying to do an L1 restriction on what the trend must look like. I don't want ziggity-zaggity lines, I want smooth lines basically. Okay, and that psi beta tells me my posterior. And here's what we do. We run phi lambda MCMC and the proximal MCMC and track the difference in the ACFs. So here I have the difference in the ACFs for different lags where the box plot is made over 100 components. Plot is mainly over 100 components in my target distribution. And for Mala, I see that the two methods, actual Markov chains, are not very different. They're different targets, but the mixing is not very different. They're very similar. If I squint, I can see this is slightly in our favor, but I don't want to squint. But for HMC, I see orders of magnitude of difference. The ECFs are much, much better. But remember, MALA is not so good in terms of the mixing. But because we're using an important sampling strategy, if I do the Strategy, if I do the relative variance expressions of what is the relative variance of the estimator, then I see that the MALA is at least 2.6. So this box plot again is over components. So for all the components are at least 2.6 times more efficient using the important sampling strategy. And for HMC, it's at least about 25 times more efficient because the Markov chain is so much better for pi lambda. Okay, definitely. Up. Okay, definitely over. Last note is that we can actually do a quantile fit and get credible intervals because we can do the full sort of quantile estimation as well. There are other examples. I'm not going to bore you with details. Two things I just want to say, definitely only looking at log concave, which is something we want to improve on. And psi lambda, the calculation does require access to proximal algorithms, but this is a very fast developing. So, this is a very fast-developing area of work in optimization. And there is, particularly for L1 penalties, there's a large amount of literature available of algorithms that are easily solvable in almost linear time somehow. Okay, I'm really sorry for going over. Thank you. Thank you. Thank you for a very nice talk. Maybe we can have time for one. Just wondering about the if you could say something about the calculation for the P lambda. And also, I don't know if it's sometimes very expensive to calculate that maximum with something like adaptive importance and in kind of the P lambda, no? The the Pi lambda? The pi lambda. Yes. So, yeah, so pi lambda, rightly so, requires access to gradient psi lambda. Because once I have the gradient, then that's the gradient is exactly what minimizes this. And I just need to take this and put it back in the function because this is where it's minimized. So the work goes into not evaluating pi lambda, but getting gradient psi lambda. So in some sense, so we're sort of trying to see. So, we're sort of trying to see if we can do better than what Pereira was doing. And that literature is already calculating gradient psi lambda. And because it's the minimizer, we can just take the minimizer and put it back into the function. That doesn't answer your question, because pi lambda itself can be, so gradient psi lambda can be expensive to calculate if the problem is such that there's no good optimization algorithms that solve this particular problem. And so, definitely, it's not something that can be employed off the shape. That can be employed off the shelf for all problems, but I'll just give you an example of the three problems we looked at. So, this particular problem over here, there's a very fast ADMM algorithm that's almost linear in time. So, it takes a couple of iterations to find the proximal mapping. The image denoising is actually, there's no optimization, it's an analytical solution to the proximal problem. Solution to the proximal problem. So there's no time really that that takes. And the Bayesian Poisson random effects model has, again, a very fast proximal Newton's algorithm where there's no inversion of a matrix that's required because the Hessian is diagonal. So it's again super fast. Yeah. But in general, as a general recipe, it's a little difficult to apply in all sorts of problems, but we're restricting to problems where the proximal algorithms are employable and usable. Employable and usable. Let's thank again Dotika for the talk.