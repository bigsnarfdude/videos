Thank you very much for the invitation to attend this workshop, which has been really interesting. And I'm really happy to follow Irina right away because I think she introduced this field really nicely. I want to start off by claiming very little credit for this work, actually. This is really the work of For this work, actually. This is really the work of a PhD student of mine, Kevin McGregor, who's very close to finishing, and he's really taken the lead on this work. So as Irina said, we know what the microbiome is. And in terms of presentation here at this workshop, I'm really focusing on improving approaches for finding associations between the set of microbiome and sample features, disease state, covariance. Features, disease state, covariates, etc. So, very much the idea of epidemiology and how that might play into the microbiome. So, you know, microbes are normally sequenced, either the 16S ribosomal subunit or perhaps the whole genome through the metagenomics world. And then they can be clustered and assigned to different orders or classes or Orders or classes or families or genus or species. For the moment, just to facilitate my talk, which again is largely a statistics talk, I'm going to ignore sort of which level things are clustered at and just refer to everything as an OTU, an operational taxonomic unit. And so basically what I'll be talking about here is a method for analysis of data that looks a little bit like this. You know, you have a series of species, or maybe they're a Species, or maybe their families or orders. And then you have a number of samples and you have counts that you've observed. So there will be some data coming from an analysis, a data set of Crohn's disease and controls called the RISC Cohort from 28 centers across Canada and the US with 506 individuals, 300, over 300 Crohn's disease, and 200 controls. And I'll use controls and I'll use those as a bit of an example. And of course Crohn's disease is known to be dysbiosis, known to be related to dysbiosis of the intestinal microbiome. So Irina was saying that there's lots of methods out there for comparing the levels of individual OTUs or individual species, and this is true. And what I'm going to talk about here is really What I'm going to talk about here is really the idea of how do you look at networks in this context. So, here are just a couple of graphs representing simple microbiome networks in controls and in cases. And you could imagine that when it's colored red, it's a negative association in terms of the counts. If it's colored green, it's a positive association. And then the sizes of the actual circles here represent the number of The number of counts for a particular species or a particular OTU. And so we're looking just, it's really great to be placed right after Arena. So we are looking at co-occurrence here. We're looking at pairwise relationships, but we're going beyond, I guess, a pair of two to look at the entire set of relationships in a data set. So I should say that. So I should say that we wanted a network interpretation. And we can look at correlations, we could look at covariances, and that's illustrated here on the left-hand side. You know, you could have positive correlations between A, B, and C. But when you move into working with the inverse of the covariance matrix or the precision matrix, then you end up with a network interpretation because you may have conditional independence of certain elements in your network. Of certain elements in your network. So A and B, for example, could be conditionally independent given C. And that provides you with an opportunity to think about the relationships between the elements in terms of a graph or a network rather than simply in terms of correlations. So, I'm going to talk about two pieces of work from Kevin. The first one here, which is called MDINE, is a model for Is a model for estimating differential co-occurrence networks. So, here we want to figure out how the network changes with respect to a binary covariate. So, certainly what has been done in many different papers and many different situations is to simply look at the network in two different contexts. You know, say look at the network that you have in Crohn's disease individuals, look at the network you get in controls and compare them by eye. Controls and compared by eye. We wanted to figure out a way to actually statistically test to see whether that network was different. And we wanted to do this on the precision matrices so that we could have that network interpretation. So as Irina mentioned, there are some challenges working with the data. First of all, we have all these zeros. We have the counts that depend on the total number of On the total number of sequencing that was done. And if you divide by the total sequencing count to obtain proportions, then you have compositional data. And that creates a problem since there will be negative correlations. There will be a bias towards negative correlations in compositional data induced by dividing by the sum. I've put a little star or a couple of stars here on this third. star or a couple of stars here on this third bullet item. We want to treat small accounts appropriately just because I'm going to come back to that a little bit later. It's not something I'm going to focus on on this slide. So what we have done here is work with the additive log ratio scale. So if the counts for an individual are denoted by y for j plus one different OTUs, then we can work with the log of the ratio between each of the counts and a reference level, yj plus one. yj plus 1. So this gets rid of the negative covariance bias associated with the compositional data. It also sort of normalizes for the total number of sequencing that was done for each sample. A little bit more notation here. So n for sample size, j for the OTUs under consideration, and j plus 1 for a reference category. You can have a number of covariates. Let's say there's K, Y to represent Y to represent the counts, and Z to be the covariate that we're actually interested in. So it's binary. And this is the one where we want to see whether the networks are changing. And the parameters that would be the fixed effect parameters, we can denote those by B. That is going to be a matrix relating each of the covariates to each of the OTUs. And this would be the kind of term that would actually look for different That would actually look for differential abundance in microbiome species between two groups. So, this is the full model here. We're going to assume that y follows a multinomial distribution with parameters pi sub i. This is the total number of sequencing reads here. And the parameters here are going to be called eta. And eta is the additive log ratio transform proportion. log ratio transform proportions. And then we can assume that those have a normal distribution with some fixed effects. And then the covariance matrix that actually depends on Z. So notice that I said I wanted to come back to the point which was indicated with two small stars. So instead of doing a direct additive log ratio transformation of the counts, the additive log ratio transformation is in the parameters. log ratio transformation is in the parameter space, which means that we're not going to have as many problems with really small counts. So those inverses of sigma 0 and sigma 1 then define the networks in the groups z equals 0 and z equals 1. And we're going to use Bayesian estimation methods here. And so we can put some priors on the elements of these matrices that encourage some sparsity. And in fact, we've put a Laplace. And in fact, we've put a Laplace prior on the off-diagonal elements, and that those actual lambda elements might vary across the matrix. It's worth noting that if they were all the same, if they were the same for all of the off-diagonal elements, this would be like the Bayesian version of Gelasso. And of course, the lambda could be included as another parameter with its own prior. So here's the So here's the first, the sort of complete model for this first situation with the binary covariate. We've got the multinomial that I mentioned. We've got the normal distribution for these eta parameters. We've got Laplace priors for the off-diagonals, exponential priors for the diagonals, and an uninformative prior on the actual fixed effect parameters. We estimated it in stan in R. Using Hamiltonian Monte Carlo sampling. And we compared the performance to a couple of other methods. So Speakeasy, which actually runs G Lasso after simply doing a log ratio transformation of the counts themselves, and a method called Mint that's based on a Poisson distribution for counts. I won't go through the details of how the data were simulated, but we sort of started with real data and then worked. With real data, and then worked from there to try and get data that seemed to be fairly reasonable. Here's just one slide of results. And so the method MDyne is in pink. The two other methods are in blue and green. And what's graphed here is the area under the curve for network edge detection. As you vary sample size, which is across the x-axis, and as you vary the number of OTUs, which are these three blue. OTUs, which are these three blocks. And we have two sort of sets of panels here: one for edge detection when z equals zero, and another one for edge detection when z equals one for this binary covariate. And so you can see that in general, the pink ones have better AUCs than the other colors. That's all good. And if you go back to the actual data set on Crohn's disease, and we look at the Crohn's disease status as the variable Z. As the variable z, we end up with the networks that I showed you near the beginning. And so we have estimated the sizes of the dots and the strengths of the edges. But what you can get in addition to this, because it's a Bayesian method, is that we can actually look at the differences between the two Z matrices and for that matter the differences in abundances and sort of get everything for. Sort of get everything from a single model fit. So, what you're seeing here is exactly the same network that was shown on the previous slide, except that the only things that are shown are the ones where the credible intervals do not overlap zero. And so, you know, you would have a node present if its abundance was sort of different from the abundance in the other group. And you would have an edge here if the network edge strength was different between the two groups. Different between the two groups. So that's the end of the first model that I wanted to talk about. And the second one, called Micoray for microbiome covariance regression, was extending this idea to figure out how to look at covariance matrices that vary as a function of a covariate that might be continuous or multiple covariates. And I have to say, we spent And I have to say, we spent quite a bit of time trying to figure out how to do this. I know that I'm coming towards the end of my time, so I'm going to won't give you a whole lot of the gory details. The notation is very similar to last time. The difference is that we no longer have a covariate called Z. All of the covariates we're interested in are now encapsulated in X, which couldn't be a vector. We're starting in the same place, assuming that y follows a multinomial distribution with parameters pi. Inside pi, we have the additive log ratio transformation for these parameters eta. And now we need to bring in the covariates. So the idea here is that these etas are going to be having given a very specific structure. Specific structure dependent on parameters a, b, gamma, and psi. So we have a normal distribution with a mean that is a plus gamma times b times x, and then a variance covariance matrix, which is psi. So this is really not intuitive to look at. So the real question is, why would one wish to do this? And the answer is because this gives us what we want. It gives actually a covariance matrix. It gives actually a covariance matrix that depends on covariance. So, if you work out the variance of the eta parameters after you integrate out the parameter gamma sub i, you get one term which you can interpret as the covariance matrix when x is zero, and a second term where the covariance matrix now depends on x. And so, from this, you can also get an expression for the precision matrix, which is given at the For the precision matrix, which is given at the bottom of the slide. So, here to implement this, it's basically done with Gibbs sampling and Metropolis sampling, also in a Bayesian framework. One of the key points here is that these matrix parameters A and B are actually come from a matrix normal distribution. Which is a sort of three-dimensional structure. And so, of course, there are certain priors and parameters need to be fixed beforehand. We had to choose those carefully in order to get a good convergence. And so, we were looking at real data sets to try and get prior distributions, which made sense. We simulated some data based on a data set called the American Gut, assuming that the COVID Assuming that the covariance matrix depended on a continuous covariate. And we compared results in the simulation to a model from Hoff and New, which assumed a Gaussian distribution, which didn't have the sort of counts built in. So here you can see the simulation results. So the dots, the black dots, would actually be true covariances that were designed, and the red curves would be the estimated ones that are coming out of simulation. And this is a matrix, it's five by five, so there were five to use in the simulation. And you can see that the simulation is capturing the curves really quite nicely as a function of x. If we then go back to the Crohn's data set, we're going to look here at how the network structure varies with age. We've included two covariates, age and age squared. And just for simplicity here, we've analyzed only a subset. Here, we've analyzed only a subset. We've analyzed only the Crohn's cases who were not on antibiotics at the time of sample collection, so we're working with a sample size of 286. And I've got this very exciting animation here to show you. So, here you can see how the network is changing with age. And it's non-linear. I can play it one more time because Kevin did this work, and I'm so proud of him for doing that. Him for doing that. So we can see that we have a bit of a change with age of the network structure. So to summarize, I presented two things. Both of them are R packages. Mdyne allows comparison of precision matrices. We get credible intervals for differences across a binary covariate. And the second method allows for similar concepts, but for multiple covariates, including continuous ones. Including continuous ones. We no longer have the Laplace prior in the second version, which encourages sparsity of the precision matrix. So we don't necessarily have the same kind of sparsity. On the other hand, it's actually computationally much faster. And it has this sort of matrix normal specification built in. I would certainly say, however, that there's room for improvement of this method to make sure that it will work for larger numbers of OTU. For larger numbers of OTUs, that it will cope with very small counts in an adequate way. At the moment, we were dealing with that in a somewhat simplistic way by perhaps grouping a number of OTUs with very small counts into the reference category. And I realize that's not ideal. So I would like to acknowledge funders. And of course, I wish to acknowledge again Kevin. He's actually starting in January 2021 as assistant professor at York University. And assistant professor at York University, and feel free to reach out to him. And his co-supervisor, who is Dr. Orheli Lab, who is at Ashese in Montreal. So, thank you very much. Thank you very much. There are three related questions, I think, in the chat. So let's start with the first one. A naive question. Doesn't the effect of additive log transform depend on which variable you leave out for? Which variable do you leave out for the denominator? Wouldn't spurious negative correlation remain if y plus one is systematically small? So if y, I'm going to answer them in the reverse order, I guess. If yj plus one is small, you will have instability. You certainly could have some instability coming along, but it It won't, the negative correlation is theoretically gone, put it that way. But the first question, actually, no. I mean, in fact, it doesn't depend on which reference category you choose. Your parameter interpretations will change, definitely. You know, how you would actually interpret an element of that matrix of fixed effects will certainly change depending on which category you choose as the reference. The reference, but the fit itself will be equivalent. Okay, thanks. So that probably does also answer the second and the third question, right? Also reference OTU selected. Are the results sensitive to this? Yeah, if the reference category is extremely small, then you could be introducing instability across the entire system. The entire system. But it's sort of conceptually and theoretically, you could choose, there's no particular problem with choosing any particular group as the reference category. And is there a framework for propagation of error of classifying reads to OTUs? That's a very interesting question. No, we haven't thought about that. But certainly, I would imagine, since this is a Bayesian framework, we Framework, we could conceptually build in priors that consider similarities between different OTUs, and that might bring in recognition that one that certain counts may be misclassified. And so we could probably build that in. We haven't done that at this time. Okay. And so how many OTUs did you have in your How many OTUs did you have in your examples? And how did the number of samples you need to construct these networks? Our simulations went up to 25 OTUs. Our data sets would have been in the same kind of range. So we're definitely a lot smaller than the full spectrum. Yes, sample size will be a problem. In the first method, we have the Laplace penalty on the off-diagonal elements. Penalty on the off-diagonal elements, which means that the number of parameters to be estimated isn't as bad as one might think. In the second method, there is also a kind of penalization that certainly keeps things within bounds. But yeah, you know, certainly, if you want to do good inference across samples with respect to covariates X, you will need a reasonable sample size to do so. Okay. Okay. Okay, thanks.