Should have been known for 20 years. But unfortunately, they are quite fresh. Okay, so it's the classical topic. Can you show me? Well, yeah. So it's a classical topic of stability and generalization. And I'll provide you with some recent results. Okay, so you all know what generalization means. So we are asking the following questions. When does the learning algorithm generalize? And when is the test error close to the empirical error? Test error close to the empirical error. So, this is one of the classical questions. And the idea of stability is super old. I think it was introduced actually to the literature even before like uniform conversions. The idea is that stability means low sense, informally low sensitivity of the learning algorithm to changes in just one learning sample. So, for example, we all know this classical picture of support vector machine. Support vector machine. Can you hear me well? Is it fine? We can hear you well, just don't. Ah, now we see you're freaking fixed. Okay, okay. Yeah, so the idea is super old. For example, we know that like large margin, max margin like SVM, binary loss, it's not like sensitive to removing any non-support object. So this is essentially enough combined with the fact that it separates like the points correctly. That it separates like the points correctly in the realizable case, this is enough to prove a nice generalization bound. So, in general, the overall picture is that if I have stability and I guarantee that my empirical error is low, then I imply a low test error. So, another classical example of stability that you know very well is like ridge regression. And of course, if I send this lambda regularizer to infinity, then I'm very, very stable. Why? Because the only reason. Very stable. Why? Because the only reasonable solution is to output zero vector. So I'm very, very stable. However, I do not feed the data. So it's one part fails. The other part is to make no regularization. Then of course I feed the data better, but I'm less stable. So the optimal trade-off in statistics in some sense is the fitting stability trade-off. So you need to fit the data well enough. At the same time, you need to be stable well enough if you want to prove something. Stable well enough if you want to prove something based on these two concepts. Okay, so then the formal definition of uniform stability. I think most of you have heard about it. So if I have a learning algorithm, I'll define it as like A S. So S is my sample and L is my general loss function. I can define the risk. So the risk is the expected loss and the empirical risk is the risk with respect to the learning sample. And now I need to define a sample. And now I need to define a sample, like even deterministically, with one point replaced by something else. For example, I replace xiyi by xi prime yi prime. And this is like there are many notions of stability. And the uniform stability is like, you know, I think the most restrictive in some sense. However, from the perspective of the analysis, you can prove the nicest bound. So from the theoretical perspective, it's. From the theoretical perspective, it's the most interesting. So we are saying that the algorithm is uniformly stable with parameter gamma if the following holds for any sample and for the same sample with one point replaced by something else and any test point xy, the loss of the algorithm trained on the sample and tested on xy minus the loss of the same algorithm trained on s with changed point and tested on the same xy. And testes on the same xy is less or equal than gamma. So, gamma, depending on the problem, it can depend on many parameters like boundedness, leapsness of the loss, sample size, etc. So, this is just a general kind of free parameter. And there are numerous examples of uniformly stable algorithms in GCS, so regularized regression, for example, soft margin SVM, and a lot of recent interest in this direction. Of recent interest in this direction is because of like this paper by Hartresh and Singer. So they were analyzing various gradient descent methods, for example, for convex and Lipschitz functions. And the idea is quite simple. So if you are making your gradient descent, then at the first step, like when you are at the stage of initialization, you're very stable because you more or less output the same function all the time, but you don't fit very well. But you don't fit very well. So, when you do your gradient descent, you lose your stability, but you fit the data better. And in some sense, the intuition is that one of the ways to translate this that we have to stop when you achieve this fitting and stability trade-off. So you haven't lost enough stability, but at the same time, you fit your data quite well. Another interest comes from the connections with differential privacy, and there are some experts in the audience. In the audience. So here is the classical result, and until recently, everyone was absolutely happy with it, with this bound. So this is a Ethereum from the paper by Busquet and Neilisev. And it says the following, that if I have any uniformly stable learning algorithm, and if my loss is bounded by capital L, so you may think for the rest of the topic it's like one, then with hyperbolic. Then, with high probability, at least one minus delta, the following holds. The risk of my algorithm, minus its empirical, so it's like a real generalization bound, has two terms. So the first one is gamma square root n square root log one over delta, and the second one is the kind of a sampling error. So it's like Hufding type inequality bound term. It's L square root log n over delta over n. Okay, so the second term looks fine, but the first term you may But the first term, you may suspect that it's not very useful. Yeah, the problem is the following: when n, the sample size grows to infinity, you would expect that you generalize, but this is like multiplicative. So in some sense, it blows up. And in some examples, like gradient descent with convex functions, we are really interested in the regime where gamma scales is one over square root of n. But unfortunately, this theorem tells us nothing. And the proof of this result is really quite simple. proof of this result is really quite simple it's just an application of the boundary difference inequality so you just do a little bit of algebra and boundary difference inequality okay does it make any sense what i'm presenting i think many of you have heard about it okay but recently uh like you know people decided to uh revise like to check what's going on in these bounds uh because of aforementioned uh papers and there was Papers and there were some very strong papers by Vitaly Feldman and Jan Vondrak. And they were able to first improve this classical bound by Busquet and Yelise and really kind of get something more useful. And this is how we came to this field, or personally, I came to this field because I was interested in what can be done further. And after like building on their progress and making our own improvements, we came. Making our own improvements, we came to the following bound, which is a, I would say, like a state of the art for now, which looks as follows, which reads as follows. If I have any uniformly stable algorithm with parameter gamma and my loss is bounded, then the generalization has the following form. It's linear in gamma. So the square root n term is replaced by log n, which is much better, log 1 over delta, so the sub-exponential term. And again, the sampling error. And again, the sampling error term. So L square root log 1 over delta over n. So if you compare this bound with something you might be very well familiar with, say like, you know, classical uniform convergence bounds based on the notion of Rademacher complexity, then in some sense, the stability plays the role of the complexity. But it doesn't count the complexity of the function space. It's a property of the algorithm. So it doesn't matter how complex my output space. matter how complex my output space is it's all about how my how stable my algorithm is and the second part is quite usual it's a usual concentration uh term now uh how we how we approached it and the idea is that really it's much much nicer and more useful to work with moments and this type of questions and this helps a lot of in the analysis and saves a lot of logarithmic factors so usually if you can prove something about moments of random variables About moments of random variables, then you can always convert it to a nice high probability upper bound and vice versa. For example, if I can say that my LP moment of a random variable scales as say a square root p plus b p, so like sub-exponential, sub-Gaussian and sub-exponential regimes, then I can say that with high probability, it's a combination of sub-Gaussian and sub-exponential fails. And the classical And the classical bounded difference inequality can also be rewritten in this language. So, the idea is that we're saying that the function of n variables has the bounded difference property of parameter beta, if changing like one point changes its magnitude by at most beta. And the classical bounded difference inequality can be written as follows: like if I have, say, independent random variables, and if the function satisfies the bounded difference inequality, this property. Bounded difference inequality, this property, it has bounded differences. Then the L P moment of difference of the function minus its expectation scales as square root number of variables p, so it's a sub-Gaussian tail, times beta. Beta is the parameter of boundary difference. And of course, this is essentially what was used in 2002. Yeah, so it's essentially the core of the proof, the classical proof. Now, however, The classical proof. Now, however, it appears that with all these questions, you need a kind of a more involved version of the bounded difference inequality. And I'll show you that it recovers many results in this area. So, or like kind of, or achieves this state-of-the-art bound. So, it's a bound which is a bit smarter. Instead of just saying something about the concentration of the function satisfying bound and difference property, I'm trying to find or prove some. I'm trying to find or prove some concentration for the functions or for the sum of functions, which are itself functions of loose boundary differences of independent random variables. Okay, let's have a look. Let's fix p greater or equal than zero. And I have a vector of independent random variables, which is like n of them. And I have n functions such that they satisfy the following. So there are three technical assumptions. So the first Technical assumptions. So the first one is saying is that if I take the expectations respect to like everything except the i random variable, which is this ji, so the same index ji for a function, the same for random variable, then it's not very large. So it's almost surely less or recall than m. The second property is that the functions are chosen such that if I take the expectation only with respect to the ith. The ith random variable, so everything else is fixed, then it's almost surely zero. And the third property is that each function has a bounded difference beta with respect to all variables except maybe the th variable. So it looks a bit too abstract, but this is really the core. And the result is saying that the LP moment, the moment of the sum of these functions, I sum them all, has this nice Has this nice behavior. So we just put explicit constants. But the idea is that, like, it's Pn beta. So it's like a sub exponential regime, which is proportional to where we have a proportionality to this bounded difference property. And the second term, which corresponds more to the boundedness, and it has a sub-Gaussian tail. Yeah. And actually, so it looks super abstract, but we can. Looks super abstract, but we can prove that it's optimal up to a log factor. So, if like if you forget about this log 2n and constant factors, then the classical bounds or some modifications of the classical bounds for radomachic chaos of order 2, you can carefully plug in here and you understand that this is like a correct behavior. So, this is sharp up to a log. Okay, so and the proof of the aforementioned bound L scroll. Of the aforementioned bound, I'll scroll up for a second, is actually like a properly chosen functions that are substituted inside this general theorem. Yeah, okay, so it requires a bit of algebra, but once we have this, this is all done. Okay, but this is only a part of the story because we want to kind of continue our kind of a learning theory based on stability and try to mimic some other results because so far we only proved the simplest degeneration amount. Digitalization bound. We have, for example, have accessories bounds, we have these fast rates, and so on. This is not yet covered. And there are some cases where what I'm doing here or what I'm presenting is clearly not very informative. For example, imagine that my stability parameter gamma is scaling as one over the sample size. Then, of course, my stability term will scale as log n over n. So it will be very small. But the sampling error will be huge. Sampling error will be huge, and I always pay it. You may understand that when comparing their true risk with empirical risk, you always pay, almost always pay the sampling error just because of health deep. So you just, even if you're super stable and output the same solution, you will need to pay this second term. And of course, I'm interested in whether it's possible to sharpen the second term. And quite recently, we've done it. So the idea is that like we can prove actually kind of a Is that like we can prove actually kind of a variance type bounds for this kind of generation things? So, suppose that I have bounded loss and my algorithm is gamma uniformly stable, and then I just kind of strongly dominate the previous result because I'm saying that I can prove that risk of my algorithm minus the empirical risk is less or equal than the stability term. Plus, so be careful because this term is like random. So, under the square root, I have the risk of my algorithm, and the risk of my algorithm is. Algorithm and the each of my algorithm is based on the sample. So it's like a random variable in the square root. But in any case, it's log L log one over delta over n, so the variance term, and the Bernstein, the fast term, is one over n. Okay, so of course, if my risk or empirical risk, you may also prove the version with empirical risk, is if either my risk or empirical risk are small, that's what I expect, actually in applications, yeah, then the second, the variance term will be very small. Term will be very small, and then actually having this stability term plus one over n term coming from the second term in the Bernstein. And the proof is quite tricky, so you may expect that it's super simple, but there are some interesting facts that we found. So first, we really need to this huge concentration inequality, this extension of boundary difference inequality, and we need to apply them, but again, to take the variance into account to some different functions. The variance into account to some different functions. Then they will have some residual terms, and we actually need some kind of more or less standard Bernstein type things to get the moments. But then it's still not done because we don't have risk under the square root. We have some kind of useless expectation of some other quantities. And the idea is that if we start applying again kind of boundary difference to equality, nothing works. But then if you apply the But then if you apply the second order concentration, so it's based on these so-called weakly self-bounded functions, so Gabber is a big expert in them, then suddenly everything starts working and you really get this correct tail. So you really need both this inequality, this theorem for bounded differences, and second order concentration to work with residuals. And overall, it provides this bound, which looks quite sharp modula, some log factor. The some log factor in the first term. This is not only the beginning of the story. We also have some kind of excess risk bounds. So far it was the generalization. Now accessorist bounds, they don't usually suffer from the problem of sampling error. So there are not so often we have this one over square different terms. And let's consider another application you might be interested in is like stochastic convex optimization. Convex optimization. So I have a general loss and I have a general bounded convex domain in Hilbert space. So dimension won't appear in my bounds. And I assume that my loss is strongly convex in this strong sense. Yeah, so that if I more or less I have this penalty, which is the norm of the weighting vector squared. And my loss is L Lipschitz. So for any Z, like almost surely in some sense. Like almost surely, in some sense, yeah. The loss, if I change two vectors, then it's controlled by the norm of the difference of two vectors, and again, so for a stochastic convex optimization, I define the risk and empirical risk as usual. And here is a nice result for excess risk. So we're saying that first it was known, of course, that in this setup, empirical risk minimizer is uniformly stable. So this is not a big secret. This is not a big secret. However, this was not, this was not kind of no one knows, it was out of the scope a bit of the methods. So, what we are proving is the following. If I have the loss function, that's gamma, lambda strongly convex in the Lipschitz, then with high probability, any empirical risk, so this should be unique, I think, for strongly convex. So, the empirical risk minimizer, its risk, minus the risk of the best. Of the best vector in the class is controlled by L squared n over gamma log n log one over delta. So it's a very short and nice bound. And up to log, you can prove that up to log n, you can prove that it's sharp. So what is interesting from the theoretical perspective, and I don't know the answer yet, is that kind of usually similar bounds are proved by By applying some kind of uniform convergence type of results. So you have, for example, covering numbers, dimension, fat shattering, or whatever. Now, if you allow me to put the dimension in the bound, like to make it dimension dependent explicitly, then it's an easy exercise. So you can more or less plug in it in the standard techniques and you'll get it. However, here formally, I'm not. However, here formally I'm not allowed. I can play only with the lambda, strong convexity, and the Lipschitz constant and it should hide everything. And formally, it's known that for this set, my underlying set of the lost class, I don't have uniform convergence. Yeah, it might be too complex because I work with the general Hilbert space and it's not even clear that, say, fat shattering is finite or so. And this was first observed by Shalif Schwartz, Shamir Srebre. Observed by Shalif Schwartz, Shamir Strebral, and Switharen back in 2009. So, that, yes, so some in this stochastic convex optimization, there are some cases where at least it's not very clear how to apply a uniform convergence and like this type of result, while not more clever, but some other arguments like stability can give us something interesting. And actually, this bound was explicitly asked back in 2009 by Back in 2009, by these authors. So, finally, we have a technique to solve things like this. And of course, stochastic convex optimization that we are discussing, strongly convex case, is only one of the problems where I have like this convexity and stability, and I have one accessories bound. So overall, we can prove a general kind of more general bounds, which achieve up to log n over n rate. Achieve up to log n over n rate. And so previously, we in the classical literature we knew how to do it with, say, kind of uniform convergence, localization, and Bernstein assumption. Now, the same thing is like the part responsible for uniform convergence is replaced by stability. And we still need the same convexity part, which comes from this so-called Bernstein assumption. So this relation between moments. So the moment. Moment, so the moment of excess risk and its expectation. And let me finally give you a short application. Again, so this is very simple, but was not known. So for example, I want to somehow to see the interaction between optimization and statistics. So I want to run a gradient descent, say, or projected gradient descent, very simple. And I want to get, after a certain number of rounds, I want to get a high probability access risk bound. High probability access risk bound, yeah. So, and what I do because I'm still in this stochastic convex optimization setup, I can do the simplest possible thing. It's just to illustrate the result. I can take the empirical risk and then I can do the like the projected gradient descent using the full batch, the entire sample, step by step, trying to minimize my empirical error. Yes, so this will approximate. Yes, so this will approximate the empirical error, so it will send it slowly to, or not slowly, but like quite fast in this case, uh, to zero. And then I want to see how it affects, how these iterations affect the excess risk. And it's quite, it's an application of this stability, our bounds, and standard results for gradient descent in this case that say after, so without smoothness, you really need quite. Without smoothness, you really need quite a lot of steps. So after N squared steps of gradient descent, if we average everything properly, then the excess risk is really as good as the ERM access risk case, L squared over m gamma and up to log factor. It should be optimal. However, we can also make some smoothness assumptions, also very classical, then the same bound with slightly modified choice. With slightly modified choice of weights, will be achieved much, much faster. And I'll conclude here. So, more or less, the conclusion of the direction is that we can prove the class, more or less mimicking the classical bounds one over square n, one over n from the ring theory that we know how to do. But instead of having Randomacher complexity or something like this, I'm really playing with uniform stability. And like uniform stability of the algorithm is really. The uniform stability of the algorithm is responsible for the algorithmic part. And some extra assumptions like Bernstein assumption or some variance bounds, they're always mimicking what we have in the classical theory. Yeah, and yeah, that's it. Thank you. Thanks, Nevita. Questions? Do you think that log factor can be removed? Do you think the log factor can be removed? Oh, yeah, it's a very good question. So the log factor here, yeah, so for p equals to two, we know how to remove it. If you need a second moment, it shouldn't be there. And I will be very surprised if not having a log factor for p equals two, like, you know, and then for p equals three, it should appear somewhere. It's very unlikely. However, the proof for p equals to two. The proof for p equals to 2 is different for p equals to 3. So this is a general, these are generally orthogonal proofs. We don't know how to like kind of manually, we are not able to even compute this, like, you know, the third moment in some sense. And I think Jan Wondrick and Vitaly Feldman told us the same. So we were trying a lot and up to some tricks, it's not very clear. It's not very clear how to make it work, yeah. Yeah, so log is still there, and most likely it shouldn't be there, but this is the only one log, so it appears only on one step of the analysis. All other logs were completely removed. And to get the final bound, your final generalization bound, you depend on this for every t, this theorem for every t is that how it works? Maybe I misunderstood the bit. There's two debts. Yeah, yeah. Like this, the log factor you have, say, here, for example, it's the same log. If you remove the log in the theorem? Sure, sure. But the reason the log shows up is because it shows up for some P. I don't understand how this follows from your previous thing on the previous slide. Does it use it for all P, like for multiple P's? Sure. Okay. So now I understood your question. So here is a simple thing. Like if you understand how to control the moments properly. To control the moments properly, then there's a very simple conversion, it's like more or less Markov inequality. So, and it appears that in this particular question, it's really very important to work with moments because there are some kind of, you know, you need this Martin-Kevich-Sygmund type like arguments where you control the moments of sums of independent random variables based on the sums on the moments of these independent random variables. Otherwise, the analysis is much. Otherwise, the analysis is much more complicated, and there are log factors appearing from each side. Yeah, so overall, the proof, once you have this theorem, it's like maybe 10 lines, but you really need to always to substitute some carefully chosen functions here inside and convert it into a tailbone. Cool. Yeah, that makes sense. All right, thanks. Yeah. Thanks a lot. Thanks so much. Okay, coffee break. It's until five, right?