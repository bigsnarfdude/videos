Number of persons that know these things now. So, okay. So, the idea is to explain the fundamental idea at the basis of what does it mean probabilistic quantization, in what sense we speak intrinsic. Intrinsic means that we don't put anything by ends, we only proceed deduc deductively. Proceed deductively. And probabilistic means that we start from classical probability. See, for 30 years, quantum probability has been considered a generalization of classical probability. The thesis I want to advocate today is that quantum probability is a deeper level of classical probability. And I will try to explain why. So, So the corollary of this is what I think is the most important consequence. Most important consequence is that for about 100 years, because the discovery of the quantum of action is by Planck in 1900, so 124 years ago. Quantization has been related to physics. My main My main thesis is that quantization now we understand is a universal phenomenon of classical probability theory. And so we understand the usual quantization, both Fermi and Bose, as a very, very special cases of this wider scheme, which is purely classical probabilistic. So you see, what So you see what are the implications? Implications for discipline outside physics are very, very large. I am thinking especially of all applications of probability theory outside physics, which are very many. And you see, physicists took about 100 years to exploit the big power of the quantum probabilistic formalism. But now, But now we understand that this power, and in fact, much enlarged, as you will see in a moment, is extended to all of classical probability. And so all applications in classical probability should understand, they surely have, because mathematically they have a quantum counterpart. And people who is practicing application of probability class. Practicing application of probability, classical probability, should understand how the same thing that the physicists should learn, the same thesis, things that the physicists learned in about 100 years. There are also many feedback for physics, but I will not discuss them today. Of course, there is no time. So, more precisely, in this general scheme, how the standard Bose and Fermi quantization Boson-Fermi quantization entry, a Gaussian quantization is that corresponds to the Boson one. Fermi quantization corresponds to the other fundamental process of classical probability, which is Bernoulli process. Okay, I don't speak of Fermi quantization, but all this story began historically. I will follow. Begin historically, I will follow the chronological development so you will have a feeling of this. They came from a very, very, very special problem that we attacked many years ago with Yoon Gang Liu, which is the quantum electrodynamics, but without dipolar persecution. There is a huge literature on quantum electrodynamics, probably is the widest studied subject in physics, but in quantum physics, but But practically, there were no results without exact results, perturbative results, obviously a lot, but no exact results without assuming dipole approximation. To make a long story short, in solving this problem, we found a very complicated structure, and we felt the need to axiomatize this structure. Axiomatize this structure to understand at least what conceptually it means. And the result of this axiomatization is the notion of interacting Fox piece. We call it interacting because it was born just because of the interaction term in the QED Hamiltonian. So what intuitively is interacting Fox space? Interacting Fox space Interacting folks space is the most general structure in which you have a Hilbert space which has a gradation based on the natural integers. So it's a direct sum of countably many subspaces which are mutually orthogonal. And for which I can define the notion of creator. Moreover, and this is a fundamental concept. And this is the fundamental constraints. It seems very mild, but it's not. Moreover, we require that the creator is an adjoint. Keep in mind this requirement. So I will not discuss analytical aspects, but so let us give a precise definition. What is an interacting Fox space? Based on a real vector space. Vector space, I didn't write real vector space, typically R or R D are the basic example. An interactive space is a sequence of Hilbert space, each with its own scalar product, and a map, which is called the creator map, plus a unit vector, which is called the vacuum vector. In this sequence, In this sequence, we postulate that H0 is one-dimensional, and in fact, the multiple of the vacuum vector. The fundamental axiom is this, that if we define LA of H with the scalar product, the space of adjointable operators defined on a dense invariant subspace. Then, A DAGA is a real linear map. Is a real linear map, real linear because V is a real vector space from V to the adjointable operators. So it must have an adjoint. And the fundamental property is that Hn plus one is the linear span of A dagger V Hn, which means A dagger V is the set of A V and H n, and I apply to arbitrary vector in Hn. So essentially, the n plus one pi. Essentially, the n plus one particle space is the linear span. In infinite dimension, you need the closed linear span, but I will not speak. Essentially, please concentrate. Everything I say is true in infinite dimension, but concentrate in finite dimension because it's easier, no topology important. Okay. So if I put the condition that the direct vector That the direct vector space sum in the gradation becomes a Hilbert space sum, which means or orthogonal sum, then I have uniquely defined a scalar product, you see, on the Hilbert space, which by definition is direct sum. This becomes a Hilbert sum, orthogonal sum. And this is the sum of the summary And this condition means, as I already said, that every A dagger V, V is an element of the real vector space, basis vector. In physics, they call one particle space. And you prove that necessarily the Fock condition is satisfied. This follows from the definition of creation. Okay, so we call in analogy with physics, with the standard no. With physics, with the standard known example in physics, we call this creators and this annihilators, and the H and the N particle spaces. There are many examples. I will maybe very quickly discuss only one. Of course, I imagine that all of you are familiar with the Fox space, but I discuss the full Fox space because, for strange reasons, this is exactly what comes out from the quantum electrodynamics. out from the quantum electrodynamical example. And in fact, full Fox Ray as a vector space, but not as a Hilbert space. As a Hilbert space, in fact, it's a much more complicated structure. It's not a space, it's a Hilbert module. But, okay. So, but the idea of this, you take the tensor algebra over the real vector space V, and V zero is the multiple of the identity of the tensor algebra. Tensor algebra multiplication is tensor multiplication. is tensor multiplication. Okay? And then you define that this is an algebra, so as a left action on itself by multiplication. And in our case, multiplication is tensor multiplication. And you recognize this is the creator map. And L star of the vacuum by definition, you see, on every f, it creates the test function f. Phi zero is considered when phi zero is considered when I consider the identity as a vector I write phi zero when I consider as element of the algebra I write identity so this is the full creator map and the test function f now the scalar product on the full Fox space is very simple simply you take Hn as the tensor product not symmetric tensor product or V O V of the one particle space, and the scalar product is simply the tensor product. So the same Boson-Fog space will be the symmetrized version of this. Okay, so we have the full creator. Once we have the full creator and the scalar product, you compute the anniculator, which is this is a very well-known, and these are very well-known formulas in free probability, very much used. And the crucial point is that I define an interacting Fox space morph, isomorphic. Morphy is more derivative. Isomorphism is something more natural, and I say is a unitary isomorphism between Hilbert space, which intertwines creators. Because we have, so this is the natural notion of isomorphism. In particular, since intertwines, In particular, since intertwines creator and creator generate the gradation, it is gradation-preserving. So, if I have an interacting Fox space, the theorem is that essentially any interacting Fox space is isomorphic to a structure of Hilbert space on the tensor algebra. How you define? The tensor algebra. How you define the structure? You simply start from the scalar product in the interacting FOX space with its own creators, and you transport back to the tensor argument. So you associate to the vector u1, u n, u1, the vector a dagger u n times a dagger u1 applied to phi 0. And then you define this color. Define this color. And you prove that this is a Hilber space isomorphism. So every Pulf space has a realization as a Hilbert space on the tensor argument, but not with the usual tensor product for the n-particle space. There are many other examples that I guess you know. Now let us consider the most trivial case, the simplest. When the vector, real vector space, well, I have For real vector space, well, I put the C, but I take the complexified because later on we take the complexified. But essentially, the vector space is R or R D. But take it's one dimensional. So what I wrote here is not V, it's V complexified. Okay? So complexified means I identify through a basis. A basis is a one-dimensional space in a non-zero vector. Okay. And I, since everything is fixed, I forget this. Fixed, I forget this to simplify notation and I simply denote A DAGA. Okay, so A DAGA, the creator map, is uniquely determined by one vector, non-zero, because it's a linear map. And since the linear Hn plus one is the linear span of the image of Hn and Hn is one dimension, Hn plus one is one dimension. So all So all the vector spaces of the gradient are one dimension. And up to a multiple, you can always take that A dagger phi n is phi n plus one because it must map one dimensional space generated by phi n into one dimensional space generated by phi n plus one. And you have a standard one-dimensional graph. Standard one-dimensional gradation. Now, let us try to understand how the structure of the scalar product of this structure must be compatible with the requirement that the creator is an adjoint. Okay? So, let us compute the norm of phi n plus one. We know that phi n plus one is a dagger phi n, and that a is the adjoint of a dagger. So, you see the emergent of this operator. This operator, A DAGA, which is the language of usual quantization, anti-normally order number operator, but this is not, will not be, will be related in one-dimensional case. Okay. So, so you have this operator which will play a fundamental role in what follows for the notion of Gaussianity that we are going to define. Okay? And so remember that a double. And so remember that A dagger maps phi n into phi n plus one. A maps phi n. So the composition is gravitational preserving. But I'm preserving a one-dimensional space. So this means that there must be the image must be a multiple. But A I DAGA, since A is the adjoint of I Daga, is a positive operator. So this multiple must be positive. Okay? So this multiple. So, this multiple must be possible. You see, I have shown that A, and this multiple I call I call A for a reason that you will understand later, A, I call omega n plus 1. Okay, so omega n plus 1 is a positive. I have omega n plus 1 is a i dagg n. But I know that a dagger phi n is phi n plus 1. So I have the following rule for the aniculator. The anniculator. A of phi n plus one is a multiple of phi n, a positive multiple. Okay? Now remember that we know, already proved that the vacuum is annihilated, and so we put by definition omega zero equal to zero. This sequence, I guess, you begin to understand, has a very, very interesting property. Let us say what property it is. Take this color product. Take the scalar product of the aniculator with n. Then we know that this is equal. We have proved that A phi n plus 1 is omega n, omega n we bring out. Okay, on the other hand, I can have the adjoint. I can take a and bring in the adjoint here. See? And then I use that a dagger phi n in phi n plus 1. So this becomes the norm of phi n plus 1. So I have the One. So I have the following relation. So the people with some familiarity with orthogonal polynomial begin to see some familiar formula. Okay? Now, in fact, iterating this identity just because I have an induction identity here, you see, that. That the norm of phi n plus one is omega n plus one multiplied by phi n. So by induction, I iterated this and I find that the norm of phi n plus one is the product of the omega n and which we denote, since phi zero vacuum has norm one, we denote omega n power one factorial with the analog in analogy with the Gaussian case. Yes, in case. Now suppose how many minutes is there? So I will I will jump. No, this A by C. So because this is a very important property. Now, suppose that, so we have proved that each norm each norm square of V n is the omega n factorial, which means the product. Now suppose that in this product there is one factor which is zero. Okay? Then you can prove that this implies that for any n, and you will see from this, from this formula, it's immediate that if omega k is zero, for every n greater than k, this will be zero. So we have a very special property that very special property that of the sequence omega n omega n equal to zero if omega k is equal to zero omega n is zero for all n greater than k in the theory of orthogonal polynomial such a sequence is called the principal Jacobi sequence okay so they come out from the notion of interacting for space so now we want to go deeper with the analogy at the moment is only With the analogy. At the moment, it's only an analogy, but I anticipate that the end result will be that essentially the category of orthogonal polynomial in one variable is isomorphic to the category of interacting Fox space in one mode. And this is not true starting from two mode on. So dimension, not V equal R is true. V equal, V equal R D for D greater E. V equals R D for D greater or equal than 2 is not 2. But for V equal R, so let us introduce, following the physics tradition, the field operator. Field operator is creator plus an equilateral. In this case, we have only one mode, so we should speak of field random variable. But tradition, we call field operator. It is an observable because it's self-adjoint. Okay? And we have proved that P0 is in the domain. That P0 is in the domain of A dagger of A for all at A dagger power n for all n. So I can speak of the moments automatically the field operator of any interacting, one mode interacting for space as moments of any order because phi zero is the domain of Edaga n and insults of Edaga. You prove. And insults of A DAGA, you prove very easily. Okay? So now, remember the definition of A DAGA. The field operator satisfies this relation. And you recognize in this relation a very special case of the famous Jacobi three-dimensional, three-diagonal relation. Why very special? Because something is missing. It's missing the middle term. them. There is n plus one. Oh, sorry, I made an error here. This is not n, it's n minus one, because the nickel later is acting on n. The a phi n decreases. So this is n minus one, okay? So there is n plus one and n minus one, but there is no n. So we want to recover the full Jacobi relation. So sorry, I remember the definition. You can easily prove given the You can easily prove given the definition. Now, when we call symmetric random variable, we have this is defined algebraically. So, when we seek a random variable, we have to specify the state and we fix the vacuum state. When we fix the vacuum state, x has moments of order, so it makes sense to speak of a random variable, at least in the moment sense, in the polynomial sense, okay? Moment sense in the polynomial sense. Okay. So, and the random variable is called, I don't discuss random variable without moments. So, for me, any random variable will have all moments. So, any random variable will be called moment symmetric if all odd moments vanish. And you prove that. So, the field operator in physics are necessarily symmetric. But this is not so rich. Why? Richard, why let me let me think one of the very old problems in physics was to define the square of a boson field, but let us do the simplest case, the case of a not of a field with infinite many degree of field, of a single random variable. Then a field is a Gaussian distribution. The square of a Gaussian is gamma, which is not symmetric. But it's very natural to think that it's also a Gaussian, a quantum field. So you see, this tradition in physics of This tradition in physics of considering a field operator as a sum, A plus A dagger, is a big recession. And now we can understand what are they losing? Well, they are losing non-symmetric random variables. For example, the gamma is non-symmetric. So now we want to recover. We want to recover. We have recovered from the axioms of the interacting Fox space the Jacobi relation for symmetric random variables. Now we want to recover for all random variables. For this, we need to introduce an additional term that originally, when we introduced the notion of interacting for space, we were following the physics tradition and we did not know almost nothing of orthogonal polynomial. But now we Orthogonal polynomial. But now we understand that the natural thing is to introduce the number operator in this way. The number operator is a standard definition of physics. Okay? But there will be a big difference with physics because in physics, the number operator is A dagger A. And this is not true for general interacting for space. They are different objects. You will see in a moment. Okay? Okay, so for any real, for any functions with values defined on the real now, essentially a sequence, I take a sequence, I can define the function of the number operator by imposing that alpha lambda on the nth orthogonal polynomial will be alpha n multiplied by the n orthogonal n. It's a diagonal in the orthogonal polynomial basis. Okay, so and in fact, Basis. Okay, so, and in fact, since the spectrum is non-degenerate, any gradation-preserving operator in the interacting force space in one mode case is necessarily a function of the number operator. This will not be true starting from dimension two modes on. So, we define the generalized field operator. We add We add the gradation preserving for the moment is arbitrary, completely arbitrary, but still, since, ah, by the way, this is we take alpha to be real valued. Therefore, alpha lambda has real eigenvalue, so is a Hermitian, not self-adjoint, I mean Hermitian operator. Okay, so operator, we have the moments are well defined. This is very, very easy to. This is very, very easy to prove that the moment exists for any n. Maybe it's an unbounded operator, but this is natural. But all moments of this classical random variable, because this is a self-adjoint Hermitian operator, so I identify it in the sense of moments with a classical random variable. And now, if I multiply from the n polynomial, you see, I apply a dagger to Pn, and I obtain n plus 1. And I obtain n plus one. A0 to phi n alpha n by definition of gradient preserving operator. And a applied to phi n omega n phi n minus 1. Because we proved before. And this is the full three-diagonal Jacobi relation. So what we have proved that every iterator space defines with the definition enlarges. With the definition enlarged so to include alpha zero, so defines, but then you define isomorphism of interactive first phase by requiring that a unitary gradient-preserving isomorphism not only intertwines creator, but only also intertwines the preservator, a zero is called preservator, preservation operator. Okay? Okay. Now it comes to the important point. We know a famous A famous result in orthogonal polynomial theory is Favar's lemma that tells you that, conversely, if you start from an arbitrary sequence, omega n, but it must have the properties of principal Jacobi sequence. And you can prove, I didn't mention, I forgot to mention, that this condition that omega n equal to zero implies, omega k equal to zero. implies omega k equal to zero implies omega n equal to zero for each n greater than or equal to k is equivalent to the fact that the the creator is an adjoint so this we have a quantum inter expectation of a very well-known analytical condition on orthogonal polynomial but the favourite lemma tells the contrary the converse if i have two sequences one must omega n One must omega n, principal Jacobi sequence. So positive and with this condition of zero. The other one is completely arbitrary. Alpha n is completely arbitrary. Then there exists a state on the algebra of polynomials of one variable such that the moments of the generalized field operator in this state are exactly. are exactly the um the the moments uh sorry the the the principle it as we have shown that if i define generalized field it has moments of any order so if i have the two sequences omega n and alpha n i can reconstruct a state of the algebra generated by the creators and annihilators and then i define the And then I define the field operator, okay? And this field operator, when I do the orthogonal polynomial, it will have this original omega n and alpha n as Jacobi sequence. So this process, and so we have essentially a to sum up, we have shown that ess essentially there is an isomorphism of category. There is an isomorphism of category between one mode interacting for spaces and orthogonal. Okay, so I think I can do now. Okay, so now we can we can do the converse path due to the Favar lemma. So we start from a classical random variable. We consider the polynomial algebra. We orthogonalize the monomial and we get the orthogonal polynomials. Then we simply reinterpret the Jacobi relation in operator terms, and we obtain the quantum decomposition of the classical random variable. And the three operators are defined in terms of the two Jacobi sequences. Jacobi sequences. Do you see what that is what I was saying before? Every classical random variable with all moments is canonically. Canonically, as you have seen, I have not put anything artificial by hands. I have only produced it, proceeded deductively. Okay? So, okay. And this operator, this is called the quantum decomposition of the classical Reynolds. On the composition of the classical random variable. So now you understand that what I mean when I say that classical random variable, what I speak intrinsic quantization. Intrinsic because every classical random variable is intrinsically associated to creation and equation. And you can compute commutation relation, Gibbs states, normal order. There is a whole theory of which I will not have time to discuss. Of which I will not have time to discuss because I want to discuss only a very special problem. So we have seen that any classical random variable, any classical random variable has a canonical quantum decomposition. Moreover, you can prove that the canonical quantum decomposition is unique up to interacting Fox space isomorphism. Okay? Then, since X is the sum of three operators, X is the sum of three operators, the moments of X are expressed as the moments, powers of this sum. But the powers of the sum, when you expand, you have this product that appear. Well, you see, you have epsilon J can be plus, can be zero, can be minus. Okay, so this is the The what we say. What is this? This shows that the quantum moments can be expressed in terms of the classical moment. No, sorry, the opposite. The classical moments can be expressed as a sum of quantum moments. So we want to study the fact that this is an important fact. Any classical random variable has canonical quantum moments. So we want to study. So we want to study this model and we will see how this study will naturally lead to a generalized notion of Gaussian. Okay, and this is what I want to do in my last minute. So the quantum moments are exactly these expectation values. And now just a comment, a very quick comment to explain what has been developed. What has been developed many years about this theory. So, you see, when we write, remember that we fix the coordinate in the one-dimensional space. So, we fix the basis in the one-dimensional space. So, what we really mean when we write this is that x in a vector v equal to e1 to the basis that we have chosen is equal to this. In fact, since we are in a one-dimensional... since we are in a one-dimensional vector space, and this map is linear, real linear, V1 can be brought again, and the map XV is uniquely determined by XE1, which we called X. Okay? Now, the very interesting fact is that this procedure is functorial. So I can go from one-dimensional space, real vector space, to Real vector space to any real vector space. And from one real-valued random variable to any V-valued random field. And in physics, this is called the second quantization. So not only we have a classical interpretation of quantization, but we have also a canonical classical interpretation of second quantization. Okay. Okay. Okay. Ah, and so we have the quantum decomposition in the general case. You see, here we have A plus A minus. From our experience with quantum stogastic calculus, we know that the A plus A minus are, the diffusion, the analog. So the quantum decomposition can be considered as a kind of generalization. Be considered as a kind of generalization of the Ito Vinerito decomposition of a classical stochastic process in diffusion part and the jump part. The diffusion part is the creation and ignition, and the jump part is the preservation. Okay, we have a lot of examples from class F or quantum sogasic acid. Now No, we, well, I just jumped very, very easily jumped out this. I said, if you remember, dealing with symmetric random variable, we have, what does it mean that the random variable is not symmetric? And the theorem is the following. A random variable is not, and this is true in any dimension, even infinite dimension. A random field is symmetric if and only if the pressure. If the preservation operator, preservation field, is identically zero. Okay? And the idea is that if we take a Gaussian mean zero, you add a constant. It has mean t. The constant is C. So how do the Jacobi coefficient of the Gaussian are known? They are the integers times the variance. How do they change? And if we simply add a mean to a Gaussian random variable, and that And the proposition that we have proved here is just to show you that illustrate if I call Qn the orthogonal polynomial of the shifted random variable, so with the non-zero mean, and Pn, the orthogonal polynomial of the random variable without shift. We have the following relation. The omega n are the same. This is fundamental. This implies, for example, that the This implies, for example, that the commutation relation will be the same for the two. And what changes is only the that the alpha n becomes shifted by the same constant. Okay? For example, in the Gaussian case, omega n is n, alpha n is zero, and so alpha prime n will be constant. Okay, why do I speak of this? Why do I speak of this? Because when you have non-zero, what is the essence of Gaussianity in any dimension is that you know that the Gaussian is uniquely determined, classical Gaussian, is uniquely determined by the covariance, in multi-dimensional by the covariance matrix. Okay? But if I shift the Gaussian, that is why I put this example, then the alpha n appear. Alpha n appear. And then I have only not only the covariance, but I have to take care of the mean. And so you will see that in the general, this is generalizes. When you have arbitrary Jacobi sequences, the omega n will be responsible of the pair partitions. And the alpha n will be responsible of what in classical probability are very famous, they are called singletons. So classical Gaussianity. So classical Gaussianity, for mean zero, the any mixed moment is a sum of products of covariances. But if the mean is non-zero, that probably more complicated because you have to keep into account the singletons. And now we want to have the generalization of this generalization. We will see that there is a generalization to arbitrary random variable. So in some sense, any So, in some sense, any random variable with all moments shares this combinatorial property of Gaussianity. That is, that all mixed moments are uniquely determined by product of covariances and singletons. Okay? That is why, in 1998, in a paper with Bodzeiko, in which we recognized some part of this, the connection. Some part of this, the connection with orthogonal polynomial, we call this process Gaussianization. Because the Gaussian in classical probability is characterized by the fact that the mixed and moments are product of variances. But any classical random variable with all moment is such that its mix and moment are products of expectation of. Expectation of singleton and covariances, pairs. I want to show this. How much time I have? Fire. So let me try to explain the main idea of this idea of generalized Gaussianity. So you see, remember, what we want to compute. We want to compute the quantum moments. The quantum moments are expression like this. Are expression like this. Expectation values of product creation, annihilation, and preservation means the indices are epsilon r 0, 1 or minus 1. So I will show you what is the basic. Okay, there is a lemma. From this lemma, you can, as you see, very simple lemma, but It's a very simple lemma, but from this lemma, everything will be deduced. You need a little bit of work, but everything follows from these two equalities. First equality is this. Take A epsilon applied to phi n. Epsilon is a multi-index. So, by definition, you see, it's a kind of random word. Every time I apply epsilon, I create, oh, sorry, he's. Oh, sorry, he's I put zero, but this is n. Here is n. You see, I shift by one. So essentially, what does it mean? This equality, that this object is a multiple of this vector. And as you see, only the sum of the indices plays a role. This is a very fundamental problem. It has no memory of the configuration, epsilon n, epsilon one. n epsilon one. Only the sum counts. And the other fundamental property is that if I have any gradient preserving operator in one dimension, this means function of lambda, then obviously by definition of gradiation preserving, it will map one dimensional into one dimension. So it maps V n into a multiple of Pn. So the Phi N are eigenvectors of all Gradition preserving operators. You see the proof is completely terrible. You see, the proof is completely trivial. You start with epsilon of m equal 1, m equal to 1. So epsilon is just here. And when you apply A epsilon to phi n, either you obtain n plus 1 or n or a multiple n minus 1. So then you iterate the step and you have the proof. See, and the other is also trivial because just by definition of gradient piece having, this is a multiple and this is exactly f evaluation. is exactly f evaluated in n because lambda is the number operator and the phi n is eigenvector with eigenvalue n. So this is phi n. Now let me just give you the idea very quickly of how it works. So I will apply this lemma for two configurations. Remember that alpha lambda is A zero. The preserve any preservator is the form alpha lambda for some alpha. alpha lambda for some alpha because it's a function of the number operator. And we use, so it is a multiple. Moreover, we you remember this very important operator. Sorry here. This should be I have sorry one moment. This was an error. Okay, the other plus doesn't matter. Okay, so A A dagger is omega lambda. So also preservation. So you see one operator, you see here. You see, here two indices are involved of epsilon. Here, one index, the singleton. See, remember, how do we apply this formula? Let us apply this formula. We want to compute this structure of this vector. So, take this. If epsilon one is an equilater, everything is zero. So, the epsilon one, if non-zero, is universally zero, it must. zero it must it must be a either preservator or a creator okay so in general you will have the first index in which a preservator appear and you have this and here so what happened let me go on with the single preservator so you get the idea so i have a imagine and here are completely arbitrary the epsilon can be Completely arbitrary. The epsilon can be plus, minus one, or zero. Now, what happens? We know from the previous lemma that what is remaining after the preservator belongs to this. And so, when I apply the preservator, I have alpha applied to this, which depends only on the sum. So, this will be very important in the future to have. To have explicit form, which it will be very elegant and very simple, because exactly of this, it depends only on the sum, not on the specific structure. Otherwise, the combinatories would be. In fact, if you look, well, okay, how much time finish? Okay. So, I mean, if you have the other configuration, A dagger A minus A, you apply the same similar identity because the Similar identity because the product of the two, but now you see A minus A depends on two indices, singleton only on one index S. A minus A, two indices. And you remember, A minus A DAGA is the famous omega lambda. So it's gradation preserving. So it produces a scalar, and I bring out, and I have a smaller product. Iterating this idea, I get a, well, you have to make it precise. To make it precise, to have to produce some ideas. But I think the basic idea is very, very, very simple. You see? So, in this sense, we speak of generalized Gaussian. Only pair partitions and singleton uniquely define all the mix and models. Okay, I stop. Thank you. Thank you. Okay, I think we have time for one. Okay, I think we have time for one quick question. Is there any question? Okay, well, maybe I would like any classical random variable, you construct a quantum variable which has the same moment. Not the same. The quantum, to a classical random variable, I associate three. I associate three quantum random variables, A DAGA, creator, A0, preservator, and A minus A, anniculator. The classical random variable is the sum of the three. What are the relations between their moments? This one is between here. Here. You see, since X is the sum of three operators, X to the power n will be the power of the sum. And then you have to expand into a product. That is why this sum appears. So these are the classical moments, and these are the quantum. And these are the quantum moments. And the second part of my talk was devoted to show that when you want to compute this, this kind of generalized Gaussian phenomenon, Gaussianity appears. My question is, is it possible to solve some classical problems? Like you have some problem for classical moments of a classical random variable for this x, and somehow you can solve it in this quantum framework by analyzing the structure of these x. There are many, many, many. There are many, many, many problems that we can do this. The oldest one was a proof in 1998 in this paper with Bodzeko, in which we express the moment as a function as a sum of omega n and alpha n. But this formula, the formula that I have shown you at the end, well, maybe I didn't have the time to explain, show you this formula. To explain, show you this formula, but this is just for your curiosity. This is the explicit formula. See, it involves an index, which I have no time to introduce. But this is the explicit formula. So it is much more than the old formula with Bozeko. And this formula was not trivial. If you look in the book by Obata and Aura on Graph, which uses this. Which uses this approach to orthogonal polynomials. They prove this formula, but very complicated. They use Motzkin diagram and very, very combinatorial. But our formula, as I explained to you, is based on very simple and gives much more, because it gives the quantum moment, not the classical. 