College Business School. So, actually, it's a great pleasure for me to have this opportunity to present our recent work here. And for this workshop, I bought the new completely new suit. And during my flight from London to Calgary, I was worried about that. Oh, the topic is not so related to the matching. Will I get kicked out of the room? And happily, the answer is no for today. So today's talk is about large-land models. Large length models. More specifically, the training process of large length models. We all know that the large length models are reshaping our society now. And training a large length model needs different kinds of data. For example, the pre-training data, the instruction fine-tuning data, and human preference data, and so on. So it brings us a new research problem: that how do we design better systems or mechanics? Better system or mechanism for the annotation process of this data. And today's topic is about a specific kind of data during large language model training. That is called the human preference data. So we design an assessment method called self-consistency monitoring for this type of data. And we also discuss how to incentivize annotators via contract design. So during the So, during the second part, we partly solve an open problem when studying the incentives. So, let's begin. And as a warm-up, let me introduce some background knowledge about large language models. So, what is a large language model? It is actually a billion parameter level, a very huge large language model that's given the previous T-1 tokens, where each token can be interpreted as a Token can be interpreted as a word or a punctuation or anything like that, and predicts the T token based on previous T minus 1 observations. So this ability is called the next token prediction, or NTP. So how does a large language model work? Well, it works in a way that is called autoregressively. Because a large language model generates a teeth token. Model generates the T token and then puts it back to the input space and generates the T plus one token. So how is the Large Language model trained? Well, nowadays the training of large language models usually include three stages. The first stage is called pre-training. And the goal is like you are a parent and you are teaching the kid how to speak one token after another. Speak one token after another. And the data source is like all the data available on the internet, including the archive, the Wikipedia, the Reddit, and so on. And the second step is that now you have a basic ability of NTP, next token prediction. Now you want the large natural model to learn, to perform as an agent, to help you accomplish some tasks. So the goal is to tell. So, the goal is to tell how a large language model can do to accomplish the task as instructed. So, the data source are like handcrafted examples by human experts. For example, OpenAI has its own secret recipe of instructing fine-tuning, and each data platform or data company has its own secret recipe on this. And the third part is the alignment. And the third part is the alignment. That is to teach the large energy model to behave according to human preferences. Because we don't want the large energy model to behave like to be harmful or to be unhelpful, to be impolite, or so on. So the data source is called the human preference data. And that's the topic we are talking about today. So, what does human preference data look like? Well, it consists of three parts. The first part is the prompt X. For example, like the user's instruction: Can you tell me how to launch a bomb attack? And clearly, that's the response one that agrees to help is not desired. So, there are a pair of responses called Y1 and Y2, and the annotator needs to annotate which one is preferred. Annotate which one is preferred over the other. So the indicator variable z represents if y1 is preferred over y2. So that's the human preference data. So the alignment process usually needs millions of human preference data to accomplish this task. And big techs are now directly hiring human annotators to do this job. To do this job. And there are still, there are other data platforms that are hiring even more annotators to do this, like the outlier we'll mention later. And the annotator's task is very simple. It's just to choose one, either Y1 or Y2, as the rejected, and the other one as the chosen response. Can never reject both of them? Can it ever reject both of them? Yeah, there is a new version of the, like if you use ChatGPT, you can have an option to ask the AI to regenerate a response for you. So for the human preference data, that means you rejected both responses and the uh and the AI generates another one. And that's this also works for some some kind of dynamics. Some kind of dynamics. So the current system is more like a fixed payment for a fixed amount of data. So there are not many methods to do the data quality control. And I picked up one comment on Reddit from an annotator who has earned almost $20,000 on Outlier. And he says that, well, do you need some effort? Do you need some effort to do the task well, but people are trying to scam and cheat the system to get the money without paying any efforts. So I'm not to blame this. That's human nature, right? But the situation is that there are so many low-quality jobs on the data platforms, and the quality data overall is not very satisfying. So that's the motivation for our work. Motivation for our work. We want to design a better annotation system or better mechanism for this process. But before we proceed, let me ask some questions about the motivation to dig deeper. The first question is that, well, back to 10 or 15 years ago, when the deep learning starts to boost, there are still the problems of data. There are still the problems of data annotations, like in the image classification problem. And can we directly apply those methods using the image classification? And the second problem, is it valuable to improve the data quality? And how large is the value? What if some of the data is just of pure low value? Go ahead. I'm just curious. So, are you? I'm imagining here that these people who are doing reviews could. Before doing reviews, could they literally pick at random and then just get paid and not do anything? This is kind of what you're saying. That's the situation right now. So people are trying to scan the system, like you said. So the first problem is that what people, let's look at what people did for the metric classification, like back to 20. Okay, go ahead. Normally in crowdsourcing, you Normally in crowdsourcing you don't aggregate the information. Yes, that's that's called cross annotation and you verify if one's annotation is correct by collecting other cross annotation. So that would solve some of the problems, right? Well, not in this case. You can do this for image classification, as I mentioned here, but we'll see later that it's not the case for human preference data. So, for example, Google uses Captcha. We can see this annoying picture here because we've all encountered that. And Google actually is using ReCapture to secretly hire users to annotate pictures for them. So, the images are actually classified into two groups. The first group is the images to be annotated. Is the images to be annotated. So users are just annotating them secretly. But the second group is important. It's a subgroup of images that's the labels, the true labels are not. So it's used to test if the annotator fully commits. So for each capture, well, people just don't know whether if it's a betting image or not. So you don't know which. So, you don't know which group it belongs to. So, if you just randomly pick some parts of the images and say, oh, this is the stopping sign, you will probably get punished by failing the capture if you just draw a big picture from the second group. That's the incentive here. But not for the human preference data. That is because human preference data are very, very Preference data are very, very personalized, so it barely has a true label. It means that it can be determined by various kinds of factors like the styles, serious or humorous. The content is very detailed or very general. But everyone has his own personalized preference. You can't say one's preference is wrong. So it's very hard to define a true label. Define a true label for the human preference data. So, another important reason is that sometimes very subtle, the difference is very, very subtle. We can see from this example. The prompt is to ask the large language model to give some advice on designing an assistance, advice for the elder. And as far as I'm concerned, I cannot tell which one is better. I cannot tell which one is better. They're so similar. They're just of the same meaning, but paraphrased into different forms. So, okay, let's go back. The takeaway message is that, okay, the human preference data is not like previous traditional data type that has a true label. So the next problem is that what is some of the data is purely random. Data is purely random, it's purely noise. So, a recent work investigated the alignment performance against the ratio of the pure random data. So the x-axis is the strength of noises, and the three color curves are three different types of noises applied to the data. And the y-axis is the alignment performance that is measured. Performance that is measured by the win rate, and the win rate is the win rate of the trained model against the untrained model, unaligned model. And the judgment is made by some mature or set as the goal model. For example, GP40 or something like that, because in academia, we cannot afford to hire human annotators to tell the points better. So it's just a method to A method to estimate which one is better, actually better. So we can see from the curve that the higher the data quality is, the better the outcome is, which is true. Just seems like I need not to mention that, but just to verify that incorporated. But the performance is still acceptable, even if some of the data are pure noise. Some of the data are pure noises. Think of the logistic regression. If you just random permute like 10% of the logistic regression data, then it still works. So let's dive into the probability model we are discussing. And the annotation is called ZI here, the large ZI. Means that if Y1 is preferred over Y2, Y1 is preferred over Y2, just recall. And the idealized case is that the marginal distribution of human annotators' preference is a Bernoulli distribution. So we assign a secret parameter P here for each pair of responses to indicate in the population level how large is the chance that. Large is the chance that Y1 is preferred over Y2. So the goal is to learn this P and we use that to train to align Large Length models. But that's the idealized case. So that's the case if the annotator fully commits to just put all his efforts or her efforts to label this sample. But more realistic case is that the annotator may not fully connect because Commit because, as I mentioned before, people just try to scan the system. So, if the annotator doesn't pay any effort, the annotator may just randomly label the outcome, Z, by an equal chance, either Y1 is chosen or Y2 is chosen. So, the full commitment is indicated by this binary random variable v. So as a starting point, As a starting point, we just focus on a very simplified case that the probability of V being 1 is a constant across all the data distribution. And that eta is to be decided by the annotator. So the annotator chooses how much effort he or she puts into the annotation process, and that is indicated by eta here. And it brings up two research problems. Two research problems. The first is that how to estimate this ADA, and the second is that how to incentivize human annotators to reach a certain level, say, ADA star. First problem is actually quite tricky in the human annotators case, because, say, if we have two levels of performance, one is a low commitment probability. is a low commitment probability lower than eta zero and one is a high probability commitment higher than eta one and these two distributions of these are different by a margin proportional to the difference of eta one minus eta zero and a probability and an amount that is the absolute difference between the preference preference probability and Preference probability and a random draw. So here, recall that P is the probability that Y1 is preferred over Y2 in a population level. So if it is actually very hard to choose between Y1 and Y2, then it has no difference from the random draw that is a 0.5 fraction. So if we assess human annotators by estimating just Estimating just DI's distributions, then we will encounter two major obstacles. The first is that the assessment ability depends on the amount here, the absolute gap between P and fraction 1 over 2. And the second obstacle is that we need a benchmark to decide if P is actually larger than fraction one over two or smaller than. 1 over 2 or smaller than 1 over 2. So as an intuition, we estimate some of the P on four top popularity data sets used in the alignment area. And we apply the SOTA open source preference model, the Skywork, as the gold model to estimate the actual preference probability. Preference probability. And we can see that most of these data just lie in the around 0.5 and 0.7. So the difference is actually very subtle. And the information theory tells us that if the difference is very subtle, it means that very little information is contained in the distribution of CIs, right? So if we just use CI to Use CI to separate low-quality data from a high-quality data, then we probably will fail. And the second hardness is that in practical algorithms, we just need to know that which one is actually better. So, for example, if we have 100 samples with 50 of them being one and 50 of them being And 50 of them being zero, it could be that the samples are purely random labeled, right? But it could also be the chance that there are fully committed annotations. It's just that each Y1 and Y2 appear in a randomly commuted order. Right? So this should be part, which one should be better, can be estimated by hiring an additional expert. Like hiring an additional expert, or by cross-assessment or cross-annotation, as you mentioned before, and/or by applying a gold AI model to help us make the adjustment. But whichever way we take, it is actually enforcing a standard answer for human preference data. So, our solution is very simple. The intuition is that if you are registering If you are registering some account on a platform, you probably will need to re-enter your email address and the password you set. That's the re-entering process to confirm that you fully commit to your entered email address. So we just ask the annotators to do the same, to relabel like small n samples to test if they fully commit. To test if they fully commit. So let's say if we have a capital N of unlabeled data sets, and we just randomly pick N samples out of it, and we duplicate them and send it back to the annotator. And we check if those N samples, two versions of labels, are consistent. Right? So go ahead. Go ahead. So, what is your expectation? They will always give the same answer to the same question. Yeah, I will give a break. I can give a quick take over here that we set the consistency probability if if the full commitment happens, it is a large probability, one minus total. Yeah. And this delta could be made very small. Like you can make the reappearance of that label to come very closely after his first appearance. So we keep the anti-ator's memory. So we define a self-consistency variable here, AI, to measure if the two versions of labels are consistent. And the Listen. And the more devoted the annotator is, the higher chance that A1 is equal to 1. So I've talked about this probability model earlier, so I just skipped this slide. And to summary, we just designed a method to employ the human antidotes themselves to act as a standard to help assess the data quality. And this method clearly. And this method clearly has some pros. First, that it clearly respects everyone's personalized preference and is easy to implement and low cost. And it does not depend on the data sets distribution speed, right? So, the more mathematical part or the technical part is the second part called incentivizing human annotators. And as quoted before, And as quoted before, we can see that annotators like human nature are tempted to scan the system to get the money without paying any efforts. And that problem is called a moral hazard. That is because the annotators choose their behaviors only for their own goods, but not for the goods of both parties or the company. So a typical model back. So a typical model back to the 1960s to study this problem is the principal agent model, which still thrives nowadays. So I would give a brief introduction on the principal agent model. The principal is the company, and the company's action is to design a contract, F. F is a map that map is a mapping that maps any outcome to a wage to be To a wage to be paid to the agent. So, for example, if we select N test samples from a test set DN in the previous sections, then we can use F D N as the wage paid to the agent. And the agent is the annotator, and the annotator section is to decide the data quality data. So, the model acts like this: the principle acts first, it decides and It decides a contract F. And later the agent moves, and on observing the contract F, the agent picks a data quality eta as his decision. And the quality eta is not directly observable by the principal. The principal can only assess the action and pay the agent accordingly. So, the question is: how do we design a bank? How do we design a better contract to incentivize to produce high-quality data? So, before we formulate the mathematical form, we just make some basic assumptions on the utility functions of the agent and the principal. The first step, the agent utility is factorized into two parts: the monetary utility here, GA, of wealth W, and minus the disutility, the F. And minus the disutility, the effort paid when making those annotations. So we assume that GA is a strictly concave function and the effort is a strictly convex function, oh, but need not to be strictly convex, just to be convex. It means that the marginal, this utility increases as you increase the data quality. And similarly, for the principal's utility, we factorize it into two parts. That is the wage paid. That is the wage paid to the agent, that is minus W, and the data's utility, which is measured by μ. So we assume that μ is the data's utility function that is a concave function. As I earlier showed, that if you increase the data quality or you decrease the proportion of the noises, then you probably will get a curve like this. So the marginal. This so the marginal utility actually is decreasing. So the model is formalized as this. The principal is trying to maximize its own utility with respect to designing some contract F here, subject to some constraints. The first constraint is the incentive compatibility constraint for the agent. It means that the agent always chooses. It means that the agent always chooses the action that maximizes his or her own utility. That's very reasonable, right? The second constraint is the individual rationality constraint. It means that the contract must compensate the agent enough. Otherwise, the agent can just quit the market by rebuilding the contract and enjoy some laser utility E0. So, this is the model called principal agent model. Agent model and compare different kinds of contracts. We need to set up a benchmark. And the benchmark is the idealized case called the first best. We can see that we are removing the individual compatibility constraint. And that means that the agent no longer chooses his or her action for his or her own benefit, but the principal, the company, purely decides. The company purely decides what the action is. So, the eta here is a decision variable of the principal. So, this is the idealized case that the company can fully control the annotator's behavior. So, this is sometimes called the first best, and the realist case is sometimes called the second best. As we can see from this. Okay, as we can see from this problem formulation, that the second best is worse than the first best by introducing an additional IC constraint here. And what's the source of this gap? If we think back that the problem that we are considering, we can see that the source is twofold. The first is the monitoring method is actually imperfect. It brings up some random It brings up some randomness in the payment made to the agent. And the second is that the agent is risk-averse, strictly risk-averse. So, to compensate the randomness and the loss induced by the randomness, the principal now needs to pay more to the agent to compensate this part. So, we can imagine if a principal could perfectly monitor the agent's action, then the principal can just write that into the contract. Can just write that into the contract. Say, if you don't perform like this, you will totally get not paid. So that means that the principal is how the principal control the agent's action by the perfect information situation. So we are considering like two classical types of contracts. The first is like the binary contract. It's like if you It's like if you're a parent and you are incentivizing your kid to perform well at the exam, you can say that, okay, if you get a grade more than 50% or 85%, then you will get a reward. Otherwise, you will not get a reward. And okay, you can still design a very bizarre contract that if okay, sorry, I'll just wrap up very quickly. Wrap up very quickly. Very bizarre contract. But there is another type of contract called linear contract. That is to pay the to reward the kid by the proportion of the grades he or she gets. So which type of contract is better? So as discussed before, this gap is introduced by the imprecise assessment, and this imprecise assessment can be based. Can be better if the N, the assessment number N increases. So, for different kinds of contracts, how large is this gap with respect to N? So, Frick, Ijima, and Ishii proved that if the action space of the agent is this great, then the binary contract converges to zero exponentially fast. Exponentially fast, and the rate is if you are very familiar with information theory, you might find that similar. That's the rate function in the large divination theory. So, but for linear contract, as we are using the average outcome to pay the annotator, so the variance is, of course, of theta 1 over n. But for our problem, this method no longer applies because we're considering a continuous action space that. Continuous action space, that's eta being any real number between 0 and 1. So the continuous analysis of this problem kind of remains kind of remains an open problem. And our solution to this is that our main result is that under continuous distributions, if the outcome, for example, the total good examples follow a binomial distribution of the quality, that is very natural, right? That's very natural, right? Then the binary contract actually converges to zero at a complete difference rate. It's no longer exponential. It's the rate 1 over squared n log n. And in other words, the linear contract in the continuous case may be superior to binary contracts, which is quite the opposite case of the discrete cases. So here's the proof scale. So uh here here's the proof of sketch and I'll just skip it for limitation and so to summarize uh to summarize that for the human preference data problem that appear in the era of large length models, we propose a novel assessment method called self-consistency monitoring. And we present the first analysis towards continuous case. Towards the continuous cases of the principal agent model. And sometimes it can be interesting that the contract design theory may be different between the continuous case and the discrete case. So for future studies, I listed some of here, and we're actually trying to design a capture for the Large Ranch models version. So hopefully we'll get deeper on this topic. Uh thank you. Thank you for that. Sir, what has been done in practice? And this is very interesting. I guess you mentioned self-consistency, but you can just ask the same question to the annotator model. Do you have any idea what this can tell?