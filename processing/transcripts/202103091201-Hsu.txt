All right. All right. So the last talk for today is Elton, Elton Shu. He was just talking about his advisor, Chung, at Stanford. And now, but now for a long time, he's been at Northwestern. Elton's work is in stochastic analysis, especially in geometric settings. And today he's going to speak on stochastic analysis on Riemannian manifolds. And actually, Elton, I had to check. And actually, Elton, I had to check. This is not exactly the title of your book, right? But Elton has the textbook, The Catholic Analysis on Manifolds, which is astounding. Not on Romanian manifolds, but not on Romanian manifolds. We had an excellent choice of speaker for this talk. All right, so thanks, Elton. Okay, so I'm very happy to be invited to give this talk and see old friends and new friends. And also, I'm instructed that I. I'm instructed that this is a celebration of Bruce Driver's career. So I should make a connection with the part of my work that's mostly related to his work. But unfortunately, this is some time ago. So I'm sound like I'm going to give a talk on survey of the history of this sort of subject. Subject. And I'm very happy to see the people who are the creators of this field and they are present at the creation. And definitely that this subject is a big chunk part of my career here. So it will be a survey talk. This is the third talk. I know people are, you know, Zoom fatigued. Zoom fatigue. So I'm thinking I'm not going to give it a talk with heavy with the formulas and symbols. So you see, my slides will be very sparsely spaced. And all the lines are just hint for me to remind me what to talk about. Okay, so let's start with Gaussian measure. Basically, I'm going to talk about path-space, you know, path-space analysis based on winner measure. Okay, this is the story is mostly started in the late 80s and early 90s. And so you see that I don't really have a lot of years put into it because I don't want to scare young people because this looks so old. So old. Okay. But then I'm going to mention some latest work in this field. Okay. So the simplest case would be a Gaussian measure, Rn. And apparently, I don't know how to write gradient. Okay. This is supposed to be the gradient operator here. Okay. Green operator. So when you do analysis on Analysis on the space, analysis from our point of view, right? So basically, you need a measure and you need a concept of differentiation. So here we do Gaussian measure and then we do the usual gradient operator here. Okay, now the topic I'm going to concentrate on is the Locker and sobler field inequality. Okay, I'm very lucky. Quality. Okay. I'm very lucky to talk after Fabrizi because he basically took one-third of my talk away. Okay, so I'm going to start as a little bit higher ground because he has already paved the way for me. So this is the Lockham Club in Quality here. And there's a long story about this and the first landing in 1975. That's actually almost the very first. actually almost the very first paper I read from about the subject. Okay, so that's the RN case. Okay, now when you have a path space on RN, the Gaussian analysis from my point of view, at least for the questions I'm interested in, doesn't really present any actual problem because all the stuff Problem because all the stuff I'm talking about, you know, if you have done for one dimension, you can go to higher dimensional case, then you can go all the way to infinite dimensional case. Okay, so we're talking about the stochastic analysis on the path space. Okay, the path space from zero to one, and sub-zero means that start from zero, append the path space, and the manifold time being, I'm going to take Rn. I'm going to take Rn. Okay, so the measure is going to be Brownian motion measure. Okay, before it's the Gaussian measure. Okay, now we have a Brownian motion measure. But it's well known that you can translate that into an IID Gaussian by producing Brown motion as a Fourier series with the coefficient that's IID Gaussian. So you're basically Gaussian. So you're basically dealing with R infinity with the Gaussian measure, but with the different gradient. But if you do your calculation carefully, you find that the gradient in R infinity is precisely the same gradient in the path space as I wrote there. This translation is one-to-ones. Translation is one-to-one. So you actually have an isometry there. So everything that's true for R infinity, you know, here's Rn, you can go to R infinity for free. Then you can go to the path space for free. If you just follow the sort of Fourier transform, which goes from IID random variable to the path space. Okay, now, what are the problems with? What are the problems we want to talk about? Okay, now you're talking about a gradient operator in infinite-dimensional space. So you know there's a classical theorem, okay? Because you want to differentiate. If you want to differentiate, you have to make a shift, okay? And that's really the hard point when you have a manifold case and Bruce Driver, you know, really told us how to make that shift. Okay. But before that, you know, shifting our end. You know, shifting our n is very simple. So, this classical result, uh, commercial martin saying that if your h is in the commercial martin space, that means that the derivative is in L2. Okay, then the map is the uh the shift map. The shifted measure and the measure itself are equivalent, and this is the famous car. And this is the famous common marking Mariana form. Okay. This result is actually if and only if. So if you have a shift, shift makes sense in R infinity just for any continuous pass. If you want your shift to be absolutely continuous or to be equivalent with the end shift at all, the shift has to be in camera markings. Has to be in commerce marketing space. I forgot who proved that, but so that's not too terribly hard to prove. So even for the path space, you get the logum solving quality for free, because logum solving equality, if you get one dimension logum solving quality for every possible dimension with bounded fixed constant, then you have logum solved inequality for the product. For the product space, and the coefficient is just the maximum of the coefficient of each piece. Okay, that's called the tensor property of the solving quality. You see that this is going to play a role when I try to go from RN to manifold. My original proof is actually using that kind of tensorial property. Okay, so you have. So, you have a Brownian motion manifold, but a lot of interesting theorems you can prove without actually going to this construction in the frame bundle. All you need is to have a formula. All you need to know is that this Ito's formula in the old days is called Dinkins formula. Okay, so you do not have, you don't need to have explicit. You don't need to have explicit expression for the martingale part because, in geometry, ultimately, you're going to just take the expectation. Okay, so but you see that for real past-based analysis, the EO2L male event construction of brand motion is going to play a very important role is because you do need to know how to explicitly. Explicitly writing down what the martingale is. Okay. If you don't have that, you have to embed your manifold in Rn, and that's not fun because you will get some kind of extrinsic proof of your theorem will be very hard to relate it to the intrinsic geometry. Okay, at the early time, what is important here? Important here is the many geometric problems are related to how fast your brown emotion is going to go. Sometimes you want the brown emotion going fast, sometimes you want the brown emotion to go slow. But the problem is that if you have a compact manifold, or if you have a general manifold here, you're going to have cut locus at cut locus. Locus at cut locus, your radial process, the distance function, if you compose with grounding motion, that's not going to be a smooth function there. Okay. And it was a very long, very old theorem, Yao, S.T. Yao, who proved that the distributional Laplacian always dominates. Always dominates the Laplacian on the manifold, the smooth part of that. Okay. Now, Kendall was the first person who realized that this is precisely what you need to write down some crown Ito's formula for the radial process. Because that precisely will tell you that the non-smooth part. That the non-smooth part creates some kind of local time term. Okay, now the Yao's theorem, you can sort of get this Candle's formula by coding Yao, okay, but that's not fun, okay? And Candle actually finds a proof directly just using the fact that there are some very simple properties of the Laplacian of the radial function shows here that you do have. function shows there that you do have a local time term there. Okay, now the local time term is positive. So RT RXT is actually you subtract local time. That means that if you have if you want a brownie motion to go not too fast, this is precisely you can use. The reason for that is because you can always ignore that term. All the geometric information is contextualized. Information is contained in the Laplace of R. Okay, so that gives you a natural bound for the rate of growth for radial part of Brownian motion in terms of geometry. Okay, so if you have the right input of geometry, you can prove wonderful theorems. And this is precisely what I did in a few of my papers. In a few of my papers, I actually use that fact that proves something. But if you want a lower bound, then you run into a problem. Okay. And then people have to assume there's no cut locus. And then for a time, there's a very intensive work by Malavan Struggle and tried to study the minute properties of this locus local time. Focus local time. Now, I think it's very much related to the asymptotic expansion of the heat kernel, of the gradient of heat kernel. And since this is a survey talk and also it's not directly related to my work, so I basically take a cheap way out. Okay, I either ignore that term. I either ignore that term, I do a lower bound, upper bound, or I just say there's no cut locus. There are geometric conditions would guarantee when you won't have cut locus, or you just do problems local. Okay, so and here are, you know, just give you a sample of the problems which can be studied basically. Studied basically just based on that formula. Okay. Now, this is not directly related to the path space, but this is very good. Shows you that some geometric problem can indeed be solved by considering Brownian motion. And even now, there are still papers on the Dirichlet problem at infinity for Cardano Adam. affinity for Cardan Adama Metapho here, and they try to beat the lower bound of the probabilists. Okay, now the first thing you need to do is to convert your geometric problem into a probabilistic problem. Okay, that conversion, you actually lose something. Now, the Diricher. Okay, now the Dervish problem, we know that when you want to solve Drupal problem for a bounded domain, you need to know the data at the boundary and then you need the Brown emotion to hit that point. Okay. But here it will be the same thing, but the only difference is that the boundary is at infinity. So that means that you need something called limiting angle problem. And this is a very old problem. And yeah, I think the first paper. And yeah, I think the first papers is done by a French problemist, which I, in the end, lost track and probably changed the field. It's JJ Pratt. And one time I claimed that I met Neu V a long time ago in China, the first time where Chinese are usually not that huge and tall. And kind of like Chong is tall by China. Is tall by Chinese standard and noverse, like one head taller, you know. So I claim that he's the tallest probocite I've ever met. And somebody told me JJ Pradesh had even taller than the book, but he's the person who initiated this line of research in the end. And we got to the point that if you put conditions You know, put conditions on the lower bound, upper bound, and then you'll be able to prove the limiting angle problem. And then you'll be able to show that limiting angles now degenerate. And for all continuous functions, you'll be able to solve that. Okay. But you actually lose something because the Diricher problem is weaker than limiting angle problem. But then you can pursue limiting angle problem. Pursue limiting angle problem on its own, you just don't tell your geometry friend because they're not interested in limiting angle. They're interested in whether the problem, the richer problem can be solved. So, definitely you lose something there. Okay. And you see that this thing here is that the lower bound, upper bound is tied down like that. That means that if you have a means that if you have a better lower bound you push it more if you have a better upper bound you can relax your lower bound and uh um uh here the uh the first condition here to beta beta can be at most one and then you you will find that that that uh the quadratic growth is the limit for Growth is the limit for brown emotion to live all the time. Okay, so for one time, for a long time, I can't break that to beta here. And finally, I find some way to do it if you assume a much stronger upper bound. So this is the solvability problem for the Dirichlet problem and infinity. And for two-dimensional case, you can do better. Case, you can do better. Okay. And you do not need the lower bounds because the geometry does not allow you to cross. Okay. So in that case, you can actually get rid of the lower bound here. Alpha can be anything greater than two, which means that this constant has to be greater than two. But geometers predicted that the constant. That the constant here should be arbitrary positive constant. And amazingly, Robert Neal actually did that. For a long time, I just couldn't make that constant close to positive zero. And Robert Neal has a very ingenious argument and very intricate argument at the end showing that in two-dimensional case, you don't need the lower bound. Okay. Lower bound. Okay, so this problem, the problem is by and large, abandoned that problem because the limiting angle problem is stronger than what the geometers want. But then there are still papers you see published recently on how the third problem can be solved in various geometric settings. Okay, now. Magic settings. Okay, now the problem I'm most proud of is a very clean solution, something called the stochastic completeness. Okay, stochastic completeness basically means that the Brownian motion is going to live on your manifold forever. Okay, the Brownian motion is not going to go off the edge in a finite amount of time. Okay, so that problem has a the geometrically means that he kernel means that heat kernel, the minimal heat kernel integrate up to one, because the heat kernel is constructed by doing on the compact set with zero boundary. And then you let the boundary go to infinity, the bounded domain boundary go to infinity. In the end, that you may have the situation that the heat kernel, the total integral of the heat kernel is going to be integral of the E kernel is going to be less than one. Okay. But this problem, if you interpret by probability, nothing is lost. It simply says that the Brownian motion, the lifetime of Brownie motion is going to be equal to infinity. Okay, the Brown emotion is going to live forever. Now, clearly that the way to make sure your Browning particle Your browning particle not goes infinity is to make sure that brown motion does not go too fast. So you can convert that into a weaker problem, say, how are you going to control the speed of brownie motion? Okay. But there are many geometric conditions on the curvature you can control that. Okay. Just based on this formula I have here. Okay, Candles formula. Have here okay, candles formula. Okay, if you just drop that, okay, the geometric condition would tell you that the brown motion is not going to go fast if you have that. Okay, but surprisingly, okay, but I think this is in 1984 or 85. Gregorian had a PDE proof showing that you can actually, you know, there is a necessary There is a necessary, a sufficient condition in the volume growth would guarantee that. Okay, now, this is a very precise surprise at that time because even people like y'all, you know, they prove that, you know, if the curvature is bounded by a negative constant, blah, blah, blah, blah, then you have completeness here. So the fact that this can be done in terms of Can be done in terms of volume, you know, which is very nice because volume is a very weak parameter for manifold. Now, for a long time, I was dreaming that I would find a probabilistic proof of that. But then at one point, I gave up because I have no way of connecting the behavior of brown emotion with volume. Okay. Until Until one day that I read a paper by Takeda, I think, he showed that you can actually, you just consider reflecting Brownie motion. Browning motion, reflecting brown emotion, the invariant measure is volume, right? So that's a way to do it. And then you can get rid of this, get rid of this. Get rid of this, uh, the geometric term you have to use by running Browning motion reverse. And that time, you know, this is since you learn the right tools to solve that kind of problem. Okay, Leon's strong decomposition tells you that if you have reflecting Brownian motion on the bounded domain, you can run backwards, then you can cancel that the The Laplacian distance term. So, based on that paper, and Takeda did this for an explicit bound, okay, like the volume has to be e to the C times R squared or something. But then, so in the end, I basically used this. In the end, I basically use the same trick. And I find out that you can actually estimate the explosion rate of brown emotion without using explicit rate of growth. This integral test is going to be fine. Okay, but now I got a little bit nervous at that point is because I got this condition here. I say, wait a minute, why I have this log, log r? Okay, and Okay. And Gregorian pointed out to me that since the proof doesn't really use the manifold assumption that much, you have to cover much more cases. And there are cases, I think, about Martin Barlow or Bass or something. They have a case where they have that term here. Okay, so that means that this term, if you want to formulate an abstract Want to formulate an abstract theorem, this term is there, okay? Unless you just want to want the stochastic completeness, but if you want to say the escape rate, the escape rate is stronger than stochastic completeness, because once you have a rate, you know the brown emotion is not going to go away. Okay. Go away. Okay, so I am actually solving a slightly stronger problem. My conclusion is a rate, the rate in terms of how fast this brownie motion, this integral here. So to my relief, okay, the integral of this is equal to infinity is actually equivalent to that. I was really surprised. I thought, you know, I must have proven something weaker. You know, this is a really simple calculation. Know, this is a really simple calculus problem. You can give it to your college, you know, like first-year calculus student to do it. Okay, if this integral is equal to infinity, only if this integral is equal to there's no assumption put in, okay. So that means that if you want to convert back to stochastic completeness, this two conditions. These two conditions are the same. Okay. So, in some way, you can say this problem has a sort of perfect ending. Although I'm solving a slightly stronger problem, but this condition is actually quite sharp. There are people who There are people who say that if this condition is violated, if a bee grows regularly in some way, then you have your lifetime is not going to be finite. So in some way, this is the end of this problem. Okay, so that's very nice. And so I did this too late to be included in my book. So I'm doing a not a second edition, I'm doing a new book. I'm doing a new book. And of course, that, you know, this will replace the old, the curvature condition. Okay. So, okay, now we come to has-based analysis. These are finite dimensional analysis where you use brown emotion as a tool to solve geometric problems. Now we're doing probability per se. I'm using probability. Probability, you know, to solve some probability problem. Okay, people can say this is a functional analysis problem. Okay, now my setting is I have a compact Riemannian manifold. Okay, then I have a path space, you know, pinned to a point. And as I said, that I need a space that's a path space. Okay. You can't add to path anymore. Okay. To path anymore, okay. I have a measure, it's called Wiener measure, and called the Brownian motion, okay. And then, um, what's missing here? As I measure, you miss a gradient because once you have a gradient, then you can't just go on and try to prove all the nice theorems about this whole setting here. So, unfortunately, so I don't have a gradient. So, uh, um, at So at that time, this whole thing is a little bit vague because after RN, people move to groups, lead groups, okay? Where you can have, you can define Browning motion, you can do shift, blah, blah, blah all nice theorems are going to be true there. But there is a benefit of not knowing too much. At that time, I just don't know much. So I immediately decided that if... Decided that if there's something interesting to do here, I'm just going to do an arbitrary compact manifold there. At that time, you know, the people who are my age all remember Bismu's little grim book. His little green book has the title called Large Deviations and Malivan Calculus. I was at Cron as a postdoc, and Varda and I and Marla and I, and some postdocs, graduate students, and George Paknikola, and who sleeped all the time because it's just very hard to follow the whole thing. So we spent a whole month on that book, and finally we gave up. The reason for that is that we're not sure whether he proved anything. The formula looks very beautiful and nice, okay? But he uses perturbation. Okay, but he uses perturbation in the past space. And when you start to get into the details, you just get lost. Okay. So now, you know, after so many years, you come back to look at the book again and all start to make sense. So that little grim book is the starting point for me to do this path space analysis here. Analysis here. It's sort of the effort to try to understand this. In retrospect, you sort of know why it's hard is because he has this integration by parts formula for one time point, but you cannot really, you know, with the knowledge at that time, even now, you cannot do it just by finite dimensional analysis here. You have to involve the whole. Analysis here. You have to involve the whole past. Now you start to perturb the past. Okay. And people at that time just are not equipped to understand that kind of technique. And at that time, I have the same problem with the Molly Ren's papers because all the nice theorems are very attractive here. I can never cloud the details there. I just have to find your own way to do it. To find your own way to do it. Okay. So, as I said, that if you want to really seriously do analysis here, you need a gradient. Okay. Not only do you need a gradient, you need to show your measure is somehow invariant because gradient is used to do a shift. And to do shift, you need to take derivative. If your shifted function and your function solve not in the same class. Function solves not in the same class, there's no way you can take derivative, right? So that means that it's a rigorous analysis in the function analysis sense, we need to grade it. And that's where people get stuck until our hero, Bruce Driver, came up. Okay. And yeah, I think this is like in 1992 or three or something like that. Two or three, or something like that. Okay. So basically, for a while, people get stuck there, right? So you just need a right way to replace the common marketing shift. And this is what Bruce Driver did. I remember his paper in the transactions, and I took me a long time to read. Me a long time to read, and still I never completely finished the details there because I just find that his way of thinking, my way of thinking are not completely in agreement. So I have to find my own way to interpret what he got there. Okay, but the confidence-building result is. Building result is that now we know there is a right definition of commercial marketing shift. Okay. In the past space, what he did is that if you have a browning, let me draw something here. So the assume you have a browning pass here. Okay. Now you have your H, okay, is a camera martin, camera martin pass. Okay. Now at time s, what you need to do, okay? What you need to do, okay, is to parallel transport HS. Start from here, parallel transport it. Okay, in my notation, I would do USHS. Okay, now, so each time you got a vector here, vector field, okay, and the path should be shifted according to this vector field. Okay, now. Okay. Now, for the people who came out to look at this, you think this is a very natural definition. But at that time, you remember your purpose is to prove that the wiener measure is quasi-invariant. So how can you get a wiener measure to be quasi-invariant? Well, wiener measure is browning motion. We don't measure is Browning motion, right? So, Browning motion, if you're a causing variant, there are only two motions you can do. You can do a rotation, you can do a shift, okay? You can add a brownie motion plus a drift. Then you have this Karma-Martin Gassano, right? And all you can rotate Browning motion. Brownie motion law is not going to change. Okay, now. Going to change okay. Now you can see that it becomes difficult here. Is that you have to make sure the vector field you define, if you follow the flow, okay, you have to do a translation and rotation in the past space in the Euclidean brown emotion that drives you, drives the. To drive the brown emotion manifold. But that's go through a stochastic differential equation, right? That's the Els Helverty Monavant construction there. You see that this problem cannot be solved without actually using that kind of framework. It's because you just look at the definition of a brown emotion as a diffusion process. As a diffusion process generated by Laplace, and that's not gonna work. So, and in this famous calculation in Bruce Driver's paper, okay, using structure equation, blah, blah, torsion, blah, blah. In the end, he actually shows you that this kind of shift does give you a rotation plus a drift. Adrift. Once you have that, you know that brand emotion has to be, the winner measure has to be quasi-invariant. So here is the vector fields is rolling commercial marketing path per path, P A T H along the Brownian path by the parallel transport. Okay, and then you can have a fluid. Transport. Okay. And then you can have a flow. Okay. Now, once you have a flow, you can once you have a vector field, you can just solve the vector field, you know, solve the flow by solving ODE in the past space, right? And the typical thing is to do the cars then, okay? And since you only want to show there is one, you can just assume your manifold is embedded in our L, for example. In our L for some big L, and then try to iterate and to see that converge. Okay, so this is, I think, the greatest discovery that is a breakthrough is that suddenly you know you can do analysis in the past space. Okay, so this line is a Roots driver's discovery: is that if you define the flow that way, it represents the rotation. Represent the rotation translation. Okay. For some technical reasons here, the quasi-invariance, he assumed the CC1 because the norm he uses is a little bit too big. And beautiful integration by parts formula. Only from that point on, we start to understand what at least part of the Of the content in this little grim book of smoothie. Now you have a full-fledged integrated by parts formula in the past space. Okay, now this formula here, you do not have to assume this is smooth because once you know this formula for smooth age, you can push it all the way to the All the way to the limit, basically to the limit where this formula makes sense. Okay. And that makes sense is in the Kermar Martin space. So this formula is true. Even in that context, it's actually in full generality here. Okay, so then I thought that we should complete that program. Okay, that means that how. And how are you going to show it for H1 for the commer marching case? Okay, so I mean, this is just some details without details here. If you choose a norm more carefully, also you realize that no matter what you know beforehand, You know beforehand the stochastic part is a rotation. So you sort of get that out of picture beforehand because you can have a pass which in the orthogonal group, which is perfectly bounded, but the derivative is not. But that rotation part has to be put into the drift part. So if you know it beforehand, it's in the thumbnail. It's in the orthogonal group. You can sort of pre-solve it first, and then you write your equation slightly differently, and then you get a much better norm to work with. And this is rotation. And then you just take advantage of the fact you know that it has to be in the orthogonal group. And then Bruce River's the iteration would go through. Okay. And you just have a, you know, perfect. A you know, perfect commercial marking theorem quasi-invariance for the whole for the uh pass-base over compact manifold. But I'm pretty sure at that time that the people also tried to do it at different ways. Okay, so in some way, my solution is a derivation of Bruce Driver's way. And Chopstroke can, James Norris, they find an alternative approach to the problem. Alternative approach to the problem, we're probably going to avoid what the first driver running into. And Enchov changes name later. I actually forgot what his current name is. I only remember his current name started with L. But you know, I actually met him once and he told me why he changed his last name. Okay, so that's the The uh, that's the perfect ending for the compact case, okay. Now, how about non-compact case? Okay, well, non-compact case, you actually think it really puzzles people, right? Because how you have a quasi invariance, right? How can you lose mass if you have a flow? Okay, but the question. Okay, but the question is that if it's not compact, you're not sure you have a flow yet. Okay, so you really have to do these two together. You have to construct a flow somehow without knowing you have a quasi invariance property there. So and then you realize that you cannot lose the mass is because the flow do both. Has the flow do both ways, okay? Because the flow usually makes you lose mass, okay? So you will not have absolute continuity there, but you can go backwards. Okay, so that's why, okay, I realized that, and this is OEM, my student at that time, I realized that the only assumption you're going to need is the stochastic completeness. Of course, you need that. Completeness, of course, you need that, okay, because you need the mass one, right? And geometric completeness allows you to construct the flow step by step, because you can have your flow, you can truncate your field and you construct the flow. And once you construct to a certain point, you can continue as long as you will not get to the edge of the manifold. Now, if you have a geometric Have geometric completeness, you'll never get to the edge. That's why you just, you know, these two, these two conditions, plus the observation that the flow can go both ways, in the end, you'll be able to show that, you know, under proper interpretation, that causes invariance is indeed true if you just assume these two conditions. Okay, so that's the perfect. Okay, so that's the perfect ending to the cross-invariance problem. Okay, now I'm come to the most interesting part. I have four minutes left, I should be able to finish. Okay, now after that, we're going to do the log and solve the inequality. Okay, and I remember at that time people talk about whether there's a spectra gap, okay, for the For the gradient, for the orange and Uhlenbach operator in the past space, there's no hope to study it directly. You just have to study through the function quality there. Okay, so this is really, I think, for the Lockham solving quantum path space, this is really a brook through breakthrough here. Okay, it's because, okay, now. Now, you start from something you are not sure, okay? Because there can be, you know, maybe there's no spectral gap, right? So when you don't know the answer, the problem is very hard. If you know the answer, if you know where you're going, it's always easier to aim at a point. Okay, so Shujian Fang, who is also a poet right now, so you should ask him the So you should ask him that to send you beautiful poetry he wrote in Chinese in the classical style. But this is also a very poetic story here. Is that at that time, he was the first person who actually showed there is a spectra gap. Spectra gap is represented by the Pangret inequality here, right? Here, right, you know, that means the mean has to be equal to zero. And this is represent the spectral gap here, okay? Now, there are many, if you on RN on manifold itself, there's many ways to do this. Okay, once you want to pass space, you know, which one that's going to work, okay? You just have to pick one that's going to work. Turn out that. It's going to work. Turn out that the martingale representation theorem is the one that works. That's precisely what he did. Okay. And the great contribution to this line, you know, is that is that he brought in this Martinian representation proof to the Riemannian-Brownian motion case. It's this proof that works. Okay. Now, of course, you just, you know, that gives you the That gives you the confidence that you'll be able to do it, maybe for log and solve inequality. Okay. And that's where I start to think. Now, how are you going to prove log and solve inequality? Okay. Now, I have to say that Shuzanne's paper, the Martingale representation theorem is written in sort of in the way I'm not used to look at. So that gave me some trouble actually. That gave me some trouble actually completely understand the full force of that. And so I find a different approach to do that. Now, a long time ago, I met Lenny, maybe for the first time, he said, you know, there's 10 several open souls in body. If you can chop the path into pieces, you'll be able to do that. I said, wait a minute, brown emotion is driven by you bleeding brown emotion, independent increment that is. Increment that is a product structure there. Okay, so that leads to my first proof. Okay, it's a very complicated proof, but it worked. Okay, I regard the Brownian motion as generated by independent increment there. And then it just worked. Now, there are always different approaches to that problem at the same time. Ida and Elwerthy also proved it. It's also a natural. It's also a natural method because you know login's own quality on the flat space. You try to embed it, blah, blah. But the embedding, the problem is that very hard to get the precise constant here. Okay, now then Ledoux told me that you can have a martingale proof. Okay, now this is only the complicated thing I'm going to show you here. Okay, now my proof is based on this representation theorem. This representation theorem. This is a representation I can recognize, it would be very easy for me to get this bound. So once I saw that, I get this bound, okay, two e to the retreat curvature here. Okay, now, okay, and this is just a cheap shot. Once you see that, okay, you can actually do Beckner because Backner includes Pancray and Logrim Solvel at the same time. Sovel at the same time. You have to admit, this is a very beautiful formula. But Beckner did by using Walkham Solomon inequality here. But you don't want to do that. You can actually do the same trick. Okay. Martingale here, representation theorem, blah, blah, blah. You just get that. And you get the same constant. Okay. Now, the last thing is the best constant. Okay. Now, this is amazing. Okay. This is more reasonable. Okay, this is more recent work by Hashofer and neighbor. They actually find out that Logan solved inequality is nest. If your Logan solve inequality is true, the reach of curvature has to be bounded from both sides. Now, he got a constant one, you know, he got a more precise constant, but if you simplify that, it's just one plus that. Okay, now versus previous bound. Now, versus previous bound I got into. You see that, you know, there's a little improvement, a small improvement here, very important because this is the best constant. Okay, once you have that best constant, they actually show if this bound is true, rich curve here has to be bounded by this constant on the top. Okay, so that means that in some way, locomotive solvent inequality path space also has a perfect ending. Perfect ending here because not only that true, you also find the best counsel. Okay, and then you know, there's more recent work for people showing the equivalence with the boundary and you can separate the lower bound, the upper bound, blah, blah. And there are many recent work here. Okay. And I have to say, this is something I don't quite follow, but interesting is that you do a lot of. Interesting is that you do a lot of circumstances or hyper elliptic diffusions. Okay. And I did something recently on how you're going to go from time dependent metric. And this most recent paper is by Aaron Naber and Householder is doing VL in the past space. So the subject is still alive and people are doing good work here. And by this note, I'm going to end my talk. I'm a little bit Going to end my talk. I'm a little bit over, three minutes over. I'm sorry. I hope my talk is entertaining to you. And I'm ready for questions. Let's thank Elton first for his nice talk. So the