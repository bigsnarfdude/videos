Cool. Okay, very nice. Yeah, so I'll talk about modularity and sampling joint work with Colin. So we'll say what we mean by each of these this time. So what we mean is, so modularity is a measure of how well a graph can be clustered. So let's merely define that. So the idea is that for any graph, G, with some number of edges, for any vertex partition on For any vertex partition on that graph, we get a score of that partition, and then the modularity of the graph is the maximum overall possible vertex partitions of this score. Okay, and this will give us a value between zero and one for any graph. And so it comes from network science, right? And the idea is that higher values of this modularity of graph, this Q star, indicate that the graph G has some community structure and lower values that it doesn't. Doesn't. Okay, and so by sampling, we mean the following. So we've got some underlying graph G, which we can't necessarily see, and some observed graph G. And so what we see is any edge in the underlying graph will appear independently with probability P in graph above, and this sort of observed graph. And so we see all the vertices, we may or may not see the edges. And note that we don't see any. And note that we don't see any edges here unless they were. Yeah, so it's a subset. Great. So the question is: when does this modularity of the observed graph give you a good estimate of the modularity of the underlying graph? Okay, so more technically, it's as we wrote here, right? So for any allowable error, epsilon, what you want is within for with probability greater than one minus epsilon. You know, with probability greater than one minus epsilon, that the two modularity values are within epsilon. Okay, so I'll state the results and then go back and define exactly this modularity and talk a bit about the proof. Cool, so we find the following. So for the modularity value of this observed graph to be a good estimate of the underlying graph, it's got to not overestimate and not underestimate it. And it turns out, so these have different thresholds. So, if we have sort of this quite modest condition here, then this modularity of the observed graph is not going to underestimate the modularity. So, all this is asking for, if you think about it, is the expected number of edges in the observed graph, because this is the number of edges underlying and the probability with which we see each. So that has to be bigger than some constant. And this you need, otherwise, you could end up. Could end up, um, you sort of have a more than epsilon probability of getting, say, zero, one, or two edges in your sampled graph and getting stupid things, right? So, this is a very basic thing to require. The next condition is quite a bit stronger. So, this is asking basically that the average degree has to be bigger than some constant. Okay, and so if we have that, then we do get that the modularity is within. Modularity is within of this observed graph is within epsilon of the underlying graph with probability at least one minus epsilon. Okay, and the other thing we get from this theorem is the following. So say we have some, we can look at this observed graph and we have some sort of high-ish, high-score partition up here, then we can construct what must be a high-ish score partition of the underlying graph. Graph. Good. Okay, and yeah, so these are sufficient conditions. And for some graphs, so of course, if our graph G is a complete graph, then this observed graph is just the Erdogani random graph with edge probability p. And we know quite a bit about the modularity of that. And so that tells you that actually you do need this condition, at least for some graphs, for this to be true. Great. Yeah, so um this is easy to simulate what's going on. So you take some some graphs, for example taken this dolphin network which came out of the thesis of Lusow. So it has 62 vertices, 152 edges. It's quite nice, has some nice properties, some stories associated with it. So people in network science are interested in it. And so the module interested in it. And so the modularity is known to three decimal places. Okay. And so we can sample this, right? So what this following graph is telling you is the following. So for any particular edge probability P, we can sample this observed graph. So we've done that 50 times. There should be 50 red dots above each of these values. And I've called it an estimate of the modularity. So we've used the Levain algorithm. The Levain algorithm, which sort of outputs some cleverly guessed partition. So each of these red dots is an underestimate of the modularity of that, or possibly correct, of the sampled graph. Okay, and so we ran the Levain gives you a random partition because there's some randomness in the starting values. So this Values. So, this we ran sort of 200 times for each of them to get anyway. So, we have a reasonable idea that we're estimating the modularity. Okay, and so and you see here that for lower values of p, we really do seem to be overestimating the modularity of the observed graph. Yeah, so this was noticed in some ecology papers. So, there you have really good ideas of what undersampling is if you have sort of time stamps on. is if you have sort of time stamps on each observation. So you can go through and just sort of look at, you know, what you would have considered the graph to be if you'd only taken observations up to this date, this date, this date, this date. And they do this in sort of hope that the, I mean, you can sort of check maybe heuristically that you've sampled enough if your parameters of this graph are tending towards something as you take these sub-samples. So this is the aside. Now Aside, now the definition of modularity is as follows. So we said that for each vertex partition on a graph, we're going to give it some score. So this is the score that we give it. Okay, so the idea is this is a sum over vertex parts. So the parts and the partitions, so this sets of vertices. So the first bit here, so by E of A, we just mean the number of edges wholly within part. Of edges wholly within part A. And this m is the total number of edges in the graph. So this bit here, after we take the sum, is a proportion of internal edges. And here, we subtract away some sort of sum of squares, which panelizes parts which are too big and also if we have too few parts. Yeah, so you can also think of this as each part being a spin. Being a spin. So, for example, if you write it like this, it looks more like that, right? So, you could replace, instead of having this sum over parts in your partition, you could replace that with, so each part is now a spin state, and you could replace this sum with an indicator, just some over all vertices u v, and then have here an indicator that u and v have the same spin. Cool. Okay, so yeah, the following, I think. So, yeah, the following I think is quite nice, partially just as a way to check that we understand the definition. Okay, so the idea is you've got a graph G and you've got some vertex partition on it. And what modifications on the graph would you expect to increase the modularity score of the graph? Okay, so this first one, what this is saying is: so you've got some graph and some partition on it, and you look at two parts. Partition on it, and you look at two parts and you take two edges which used to be between the parts and you put them inside the parts like this. Okay, so I've drawn an example here. These two edges disappear and they become these two edges here. Okay, and then the modularity does go up. And this is just a matter of checking the definition, right? Because the number of edges didn't change, but the number inside parts did, right? So we've got more inside parts, proportion of edges inside parts went up, so the edge contribution went up. Up so the edge contribution went up, and this degree tax didn't change because I mean, so if you look at each part, the um the volume of each part stays the same, yeah. And these are more just to wet your appetite. And so I think each of these moves is something you would sort of hope would increase the modularity and sort of sometimes does. So, this is another thing. You notice that if you did this twice, you would get the top move, but it's also true that it takes some time. But it's also true that takes some checking that if you so if you've got two parts and some edge between the parts, you can move it within a part, the modularity goes up. But it is important that the part you move it into has to be one of the parts that that edge was going between. If you take an edge between parts over here and you put it into some other part, the modularity doesn't necessarily go up. So I'll put some crosses here. And also these moves do not necessarily. These moves do not necessarily cause the modularity to increase. Yes, here's an example showing that adding edges within parts doesn't always help you, right? So this has modularity half, add an edge, gets worse, add an edge, and this time got better. Good. So this is the parameter that we're playing with. There's a few more properties. So So it's quite robust to small perturbations in the edge set. So this is nice. If you change just 1% with edges, the modularity changed by 0.02 at most. And so we've said this is a value between 0 and 1, where 1 is meant to indicate high levels of community structure. So you can get values, you're guaranteed values near 1 if any of the following happens. So of course, if you have sort of disconnected components. Components in your graph, and they're each small, then this will give it to you. If you have nearly disconnected components in your graph, which are each small, then you have a modularity near one. It's also true for any epsilon, there's a delta, so that if you have no subgraph which is a delta expander, then you're also guaranteed to be within epsilon of modularity one and And sort of conversely, really small modularity values are guaranteed if you have any of these conditions. So, if you have a really small spectral gap, for example, or if this condition holds. So, this is sort of somehow saying the edges are placed as if randomly, but for all vertex parts. So, if it's true for any set X and its complement, the following, then you have small modularity. Following, then you have small modularity. And this is a value that sort of makes sense, right? So if you imagine if you just knew the degrees of x and its complement, then the probability that an edge would land in between, say with the first endpoint here and the second endpoint here, would be the volume of like the proportion of the degrees here times the proportion of the degrees here. And it can go either way. So you get a factor of two. Okay. So So just checking, yeah, maybe 10 minutes, five, five minutes left, I guess. Okay, so just to remind you, this is the theorem that we're trying to prove. Okay, so we've got these two thresholds, and we'll look a little bit at the proof of this, which will say that as long as we have a high average degree, then the modularity of this observed graph is Modularity of this observed graph is within epsilon of the modularity of this underlying graph. The proof is somehow related to this load balancing. So as an aside, if you have, say, some reals, you've got n reals that are each positive and they sum to one. So how well can you balance the sum of? How well can you balance the sum of these? So, we're going to let delta be this value. So, this is the min of sort of the sum and the rest of the values. Okay, so this, so the best you're hoping for here is a half. And so, this value is sort of telling you how close you got to a half. Now, so you can easily see that you can always do at least as well as say, At least as well as say a half minus the maximum value or because you can just keep adding bits and like keep adding x1, x2, x3, x4 until you jump over a half and then go back. So you can always do at least this well. So somehow there's an idea that if you have large xi, that's going to prevent you from That's going to prevent you from getting too close to a half. So, what turns out to be important here is how close you can get in terms of sort of a sum of squares of these xi's. And yeah, so this I think is a nice question. And it turns out you get a half. And so to see that a half is the best you can get, it takes, say, say, one-third. Say, say one-third. So, the best you can do there is one pile of one-third and two-thirds. So, that gives you this alpha value of half. And actually, the greedy algorithm will give it to you. So, that's enough. Cool. And somehow we use this to show that for any partition, we can create an eta-good or eta-fat partition of G so that the modularity value. So, that the modularity value of this ita fat partition is within, is quite close to the sort of partition that we started with. And also now each part in this partition is big in some sense. Great. And then sort of this is somehow the thing you want to play with, right? Because once you've had. Because once you've had this idea, you take you don't want to think about any partition of the ground set or the top set. You just, it's enough to think about these well-behaved partitions, these e-to-fact partitions. And so, for example, you can show me that the following two events are quite unlikely, and then this will lead to the theorem. Lead to the theorem. So, this is sort of saying that it's unlikely that if you, so see, this is a, is not eater on two fat, it's unlikely that you'll have some partition up here, which is eater fat coming from some non-eter onto fat partition. And this one is saying that once you do have a control that both these partitions are this eta fat in some way. Eater fat in some way, then you've got a good control on how the actually sort of partwise, each part down here will be a similar proportion to the parts up the top in the observed graph. Cool, so we're almost out of time. I just want to mention an open question. Yeah, so our first conjecture is really. Is really like it to be true that this tends to a limit. So if we let this here be the expected value of the maximum modularity of an Odyssey graph with p equals c on n, so really nice if we could prove that this tended to a limit as n went to infinity, then we can talk a lot about this function. So these come from results earlier on Erdogeni graphs, but this But this sampling theorem that I talked about would also tell us that, yeah, so we'd sort of have QC at one somewhere and at zero, and also that it's non-increasing in between, which would be nice. And the last thing I want to say is the following. So, this is a conjecture from physics, which means it must be at least morally true, perhaps. Perhaps. And so it's the following. So the idea is: is it true that there's some sort of absolute number of parts, five perhaps, such that with high probability, the maximum modularity of this graph is achieved by a partition with only five parts. Okay, now it can't exactly be five, right? Because if you have disconnected components, then like Than like disconnected edges, each of these should be in their own parts. Okay, so there's definitely more than five. But it could be the case that maybe to first order, this would be true. However, unfortunately, so for any particular C, right, there's going to be some actual sort of non-vanishing proportion of the edges, which are going to be in these small parts. And so you are going to do better if you can break up that into. If you can break up that into arbitrarily many pieces, because otherwise you'll have a piece which is going to contribute too much to the degree tax. Okay, so it's so that's what we're talking about here. And we also have to be careful to make a statement that's not trivial, right? So Din and Ty have a nice result that if you restrict to k parts, you can estimate the modularity within this one minus one on k value. Value. Okay, so the question is: as written here, so is it possible that there's some sort of absolute k, any finite k would be interesting for us, but maybe it's five, such that for any error and for all sort of big enough C, we do get the right modularity by considering paths. Considering parts up to this size k. So, yeah, so with number of parts up to k. Cool. Okay, thanks. Great.