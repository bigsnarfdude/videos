You can talk with her more about all the work that we've been doing and Greg Anji, who's a professor at Marquette University. So, as we've seen in a variety of different contexts, many times when we're trying to train neural networks, scaling up the size of the network has this huge importance on the performance. And we've seen a number of different examples. Seen a number of different examples. This one's from 2019 from Google showing how the number of parameters associated with large-scale neural networks, as it increases the top one accuracy for classification is also increasing. And what we'd really like to understand better is why we see these kinds of effects, why we're getting this huge improvement in performance as we scale up the size of the networks. So here's a little thought experiment that we could engage in. Imagine that we have a set of training examples. Say, XI's are the inputs, YI's are the responses or the outputs. And we train one of these huge neural networks. And we're going to think about a network that's so large that it can interpolate whatever data that we have, for instance. So then when we train the neural network to have zero loss, to interpolate those samples, we're somehow choosing an interpolating function. Choosing an interpolating function. And the question is: well, which interpolating function out of the infinitely many interpolating functions that exist are we actually going to select? And associated with that, we want to understand better the role of the network architecture and in particular the network depth. How does changing the depth change which interpolant we select? And of course, ultimately, we'd like to understand something about how this might affect generalization. So here's the So, here's the notation that we'll be using. I'm sure the concepts here are familiar to all of you. So, we're going to have a network with inputs x, as I said before. And what the network is going to do is it's going to take x, multiply it by first weight matrix W1, send that through activation functions, then another weight matrix, another set of activation functions. And then finally, at the end, we're going to take a weighted sum of all the outputs. Sum of all the outputs, the final hidden nodes, and that's going to be the output of our. So, this network is mapping inputs x to outputs. So, we can even think about this network as represented by a function. So, I'm going to write it as h sub theta, where theta is all of the different parameters or tuning parameters, well, all the different parameters of the network. And L here is going to correspond to the number of layers in the network. So, when we train a network, we can think about So, when we train a network, we can think about this as searching over the collection of network weights and choosing a set that gives us a low value for the loss and also often a low value of a regularization term, where here our weight, we're going to consider weight decay, which is just the sum of the squares of all of the weights in the network. So, weight decay is pretty important to consider. First of all, it's used very widely in practice. It's a very useful tool, but there's also a lot of evidence. But there's also a lot of evidence that even if we're just running stochastic gradient descent without explicitly using weight decay, we see weight decay-like effects implicitly. And so understanding the role of weight decay is a fundamental problem. Okay, so this is how we would select the weights of the network. But we can also think about this from a function space perspective. So like I said, we are, when we're, we can think about a neural network as corresponding to a function. As corresponding to a function. And so when we're searching over the space of neural network weights, we're effectively searching over a set of functions and trying to find the best function to fit our data. So we can think about this training process as searching over a space of functions and choosing one which has a small value of loss plus a small value of a regularization term. By now I'm writing this regularization term explicitly from a function. Explicitly from a function space perspective. Now, this hasn't yet provided us with any insights because I'm going to now define this regularization term in terms of the network weights. So I'm just defining it as simply corresponding to the minimum weight decay that corresponds to the output of my neural network that I can, the minimum weight decay that I can use to represent my function using a neural network. So we can refer to this regular. So we can refer to this regularizer, R sub L, as a representation cost. It's telling me something about how expensive it is for me to represent a function f using the neural network, where the cost is in terms of the sums of the weights in the network. So what I'd like to do is to try to think more about what this representation cost is. What kinds of functions are easy to represent with the network or hard to represent with the network? And that's going to give us some insight into what kind of functions the neural network. kind of functions the neural network will learn. So in particular, if we go back to that interpolating thought experiment that we started with, right, first we said, well, we're going to train a neural network to interpolate training data by searching over the weights. Now we can say from a function space perspective, what we're doing is we are searching over all functions that interpolate our data. And what we're doing is we're choosing the function that has the smallest value of this representation cost. Representation cost. So, by understanding what this representation cost is, or for which functions it's smaller versus bigger, then we're going to hopefully gain some insight into what kinds of functions will ultimately be selected to interpolate our data, and hopefully, some insight into the role of the network architecture. So, a lot of work has been done in this space in the settings of two-layer networks, like I've drawn up at the top. So, when I say two-layer networks, Drawn up at the top. So, when I say two-layer networks, I mean there's one hidden layer. So, for example, several years ago, my co-author Greg Anji and our collaborators Daniel Soudri and Nadi Swagro analyzed the two-layer representation cloth. And what we showed is that it corresponded to the total variation of the Radon transform of the fractional derivative of the function. So, just to illustrate. function. So just to illustrate what this means, imagine that I've got a function that looks like this. In this case, it's piecewise linear. And then if I were to compute those fractional derivatives and display them, what I'm seeing are strings of Bureau delta functions or chains of Birak delta functions. And those chains are aligned with the kinks in this piecewise linear function. So now if I were to take the Radon transform of that, what the Radon transform is doing is it's taking line integrals. Taking line integrals of this fractional derivative through all different angles and at all different offsets from the origin. And so, when we do that, what we end up with are in this particular setting, kind of big spikes for whenever that line integral is aligned with one of these chains of delta functions. And so this makes sense because this now function space representation cost does sort of kind of give us some insight into how many different Rayleigh units. Into how many different Rayleigh units we need to represent the function and something about their magnitudes or their weights. But notably, I'm not doing anything explicit in this norm with regular units or with the weights in a neural network. For any function, I can calculate this norm, and it gives me a sense of what kinds of functions I can easily represent with a two-layer neural network. Now, this is just Now, this is just one example of representation costs that have been derived or proposed for two-layer neural networks with a collection of different works dating back all the way to the 1990s by Andrew Barrow. And one of the ways in which these representation costs have been used is to do things like to derive representer theorems, similar to what we use in Fernal regression, or mini-max rates for functions. Or mini-max rates for function classes where the functions have a bounded value of this representation cost. They also give us tools for handling things like sample complexity depth separation. And this is something that I'm not going to say much about because Sue is going to give a talk about it later in the week, and I don't want to scoop her or offer spoilers. But what I'd really like to talk about today is going beyond the two-layer networks. Now, there's been a Now, there's been a host of papers that have considered multi-layer linear networks. So, when I say linear networks, I mean those activation functions sigma are like an identity mapping. So, there's no non-linearities anywhere in this network. So, for example, Gian Tulgarsky said that gradient descent for these kinds of networks will align the net the layers. And I'm going to talk more about what that means with some examples in a couple of minutes. There's also been Minutes. There's also been some really interesting work by people like Ian Karzland and Nadi Serebro that shows how different architectures within this linear network framework corresponds to representation costs that correspond to different LQNO norms. And also work that shows that when we add depth to this kind of network, it promotes sparsity and low-rank structure. But of course, the name of the game in neural networks is to deal with. Game in neural networks is to deal with nonlinear networks. So, the question now is: what can we say about multi-layer non-linear networks? So, Ergen and Palanchi did some interesting work where they considered some very special cases, such as when the training data has is satisfying some strong constraints, such as the training data all lying on a rank one subspace or being ortho, or all the training feature vectors being orthonormal to one another. Being orthonormal to one another. There's also been some interesting work by Harshur Jakot on settings where networks are outputting vectors of outputs as opposed to a single output, where he can do some really interesting stuff thinking about the structure of those functions. But we're going to present an alternative framework to thinking about these networks today. So, in particular, let's start again with just the two-layer network. I said that we can represent the network. That we can represent the network with a function, and in this case, the function is x times the weight matrix w plus some bias terms b, and then we're going to clip that, send it to a Rayleigh activation function, take the inner product of that with the outer layer with A, and add a constant offset. So that's sort of loosely represented by my graphic, but my graphic doesn't have any of the biases or offsets in the picture. What we're going to do is we're going. Going to do is we are going to take that basic structure and preface these ReLU nodes with a collection of different linear layers. So overall, the network is still non-linear. We've got these output ReLU units, but before that, we have a whole bunch of linear units. So our focus is on networks where we have a total of L layers, and L minus one of those layers have linear activations. And so now we can write this function that's We can write this function that's represented by the network by taking x and multiplying it by a product of different weight matrices before adding biases and the ray views, et cetera. So the distinction is that before we were talking about optimizing over a single weight matrix plus the other parameters here. Now we are optimizing when we're training the network over a collection of weight matrices. Okay, so why care about linear layers? Okay, so why care about linear layers? I mean, first of all, truthfully, we would love to be able to do all non-linear layers, but this is an extremely challenging problem. But we're hoping we can at least get some insights by using linear layers. Nevertheless, I think this is a really interesting setting for a few reasons. First of all, all of the networks that I've described can be written in this simplified form, right? And I'm going to talk more about that before. But the key point is that we're always talking. The key point is that we're always talking, no matter what the depth is, about the same family of functions. It's not like as we add depth, we're suddenly working with a different function space. The function space is remaining constant. And so as we add depth, we're really thinking about the role that depth is playing in the training process and in the selection of one of the functions within this single fixed function space. But there's also been work, for instance, showing that empirically adding linear layers can help with. Directly adding linear layers can help with generalization. It can help improve the speed of training. And in the context of dynamical systems, it can help us recover latent low-dimensional structure in dynamical systems. So there's a number of reasons why people are considering linear layers in real practical neural networks in a variety of contexts. Okay, so just to put a fine point on this, right? With a two-layer network, we've got this function representation, and then our Representation, and then our regularization, our weight decay, is the sum of the squares of all the elements of A and all the elements of W. Whereas, if we had a three-layer network, we're still actually looking at exactly the same type of function, only now instead of representing W directly, we're representing it as a product of W1 times W2. And so, the big distinction now is this weight decay regularization where we're looking at the sum of the squares of W1 and W2 separately. W1 and W2 separately. Okay, so same function class, right, but different regular equation on the weights induced when we do our training. So now I'd like to remind you all of a key fact that I think is familiar to many of you that has been used, for instance, a lot in the low-rank matrix recovery literature, which is the following. Let's say I've got a matrix W, and I want to search over all pairs W. Search over all pairs W1 and W2, whose product is W. And I want to minimize the sum of the squares of all the weights. Well, this is exactly equal to the nuclear norm of W, which is simply the sums of the singular values of W. So now, if we go back to this picture from before and we think about these representation costs in our function space, right, before we're here, we're looking at the sum of the squares of all the weights, a and w. All the weights a and w. Here we're looking at the sum of the squares in a with the nuclear norm of w. And so you can see immediately that we can think about this as looking at the same functions, but inducing a different regularization on undoubtedly. And that's what we'd like to understand the effect of. Now, this idea extends beyond three layers in a very natural way. And in particular, if I have an L-layer network. If I have an L-layer network, then the regularization on the effective W, the implicit regularization on an effective W, is simply a Shatten Q norm, where Q here is 2 over the number of linear layers. So we're just looking at the LQ quasi-norm of the singular values of W. Okay, so now what's our first fast intuition? So recall, at least in the three-layer case, that Three-layer case that we said we're regularizing implicitly the nuclear norm of W. So, this initially suggests that what we're going to do is prefer functions f that can be represented with a low rank w. So what does it mean when w is low rank? It means that in this matrix, all the rows wk, so each wk corresponds to a different Rayleigh unit, they're going to lie in a low-dimensional subspace. And so, as I mentioned before, people As I mentioned before, people like Pulgarsky said this, we can think about this as having the Reylu units aligned. And this is easy to see with a simple picture. So here's a function. I'm just displaying it two different ways, one sort of top down, one a sort of 3D view. And we can see that in this function, there's a lot of sort of nonlinearity. These black dots are different contour lines, and they're not straight and they're not lined up. But here, if I represented a function with a rank one W in this case. Rank one W in this case, then all of the contour lines are parallel with one another. So when people say, oh, the Reyloux units are aligned, it's like saying that these pinks in the Reylou function or these contour lines are all parallel to one another. Okay, so here's a graphical representation of this concept. So in this example, we took a finite collection of points, and we learned an interpolating function for these three numbers. Function for these three networks. So, just a simple two-layer network when we add a single linear layer and when we add two linear layers. And I'm just going to show you what happens when we do this. And so what we can see is that empirically, as we add these linear layers, we are seeing that the contour lines are approaching being closer to parallel to one another, in contrast with the two-layer network, where they're very wavy and go all over the place. All over the place. So, this is, it feels much more structured than the kinds of functions that we're finding when we interpolate using a two-layer network. But this intuition is not really sufficient in and of itself. In particular, if you were to say, hey, I'm going to try to interpolate a finite collection of data points, and all I care about is that the Rayleigh units are aligned, it turns out that there's still an infinite collection of different networks that would satisfy that criteria. So, alignment alone. So, alignment alone is failing to capture what this representation cost is, and we want to get a better sense of this. So, what I'm going to show is that when we try to minimize this R sub L representation cost, what we're doing is we're promoting the learning of functions that have low mixed variation. And I'm going to define what I mean by that in a couple of slides. But an example or examples of functions that have low mixed variation are things like single and multi-index files. And multi-index models. Okay, so a multi-index model is a function of the form that I have up here. So I assume that I've got a collection of vectors v1 through vr that are linearly independent. And I can represent my function f in terms of some other nonlinear function that only depends on the inner product of x with those r different vectors, those vr. Different vectors, those V odds. So, a single index model is an example of this where R is simply one. So, there's only one. So, in this example, I think makes it clear. So, here we're in a two-dimensional space. My function is varying across both x1 and x2, but it's only nominally varying with both of those. But there's really only one direction in this space in which the function is varying. And orthogonal to that, the function is conduct. To that, the function is constant. So, this is an example of a single-index model. And single-index models and multi-index models have been widely studied throughout the statistics and the machine learning literature. One example paper that highlights why people are interested in this space of functions is the following. So this paper by Yu and Lao says we're going to try to take samples and perform regression. We're going to try to estimate a function. Regression, we're going to try to estimate a function using non-parametric tools. And we want to know what's the mini-max rate. How do we expect the regression error to decay as we increase the number of samples? So if the function is s folders smooth, and that's all we can assume, we don't know anything else about the function, then the minimax rate is n to the minus 2s over 2s plus 8. But in contrast, if we were to have a If we were to have a multi-index model of rank R, where with R different directions in which it varies, and this outer link function g is s folder smooth, then the minimax rate is n to the minus 2s over 2s plus r. So here's just an illustration of this. If I were to have a function in a 20-dimensional space, but it was only a multi-index model with two different directions of variation. With two different directions of variation, we can see that the expected squared error drops very rapidly with the multi-index model than without it. So in general, we can infer here that a method that is capable of adapting to this underlying low-dimensional structure, to these R different directions of variation, is going to be able to achieve lower errors or better generalization than a non-adaptive. Oh, yeah. Oh yeah. So I'm just um oh oh sorry. So R here is the I guess ring of the multi-index model. So here I said that there are only r different directions. Right. Yeah. That's just a reference. I like that. That's an older result that I'm just using for contrast. Yeah. But they're saying, yeah, we can do. But they're saying, yeah, we can do much, much better if we can adapt that R-dimensional subspace along which the function is varying. So, this has nothing to do with neural networks. These are just minimax rates, but illustrating the importance of multi-index models. Yeah, more. Yeah, so I guess you're right, the constants are very large indeed. Nevertheless, I think that there's a lot of empirical evidence, including in some of the papers that I cited here, that methods that do try to adapt to that low-dimensional subspace do empirically perform better. But you're right, for these sorts of rates to really kick in, effectively, that constant means that this only really holds when n is very, very large. So it's more like that big constant, and then you start seeing this rapid drop-off. This rapid drop-off. Yeah. So, others have considered how to think about things like multi-index models in the context of neural networks. So, Francis Bach, for instance, said, I'm going to think about training a neural network by performing optimization in a function space. So, unlike what we're doing, he's really thinking about, you know, even training the network within the function space. But within that framework, he showed that even with a two-layer He showed that even with a two-layer represent a two-layer network, using a representation cost as a regularizer, you can actually get fast generalization rates that are adapted to the dimension of this subspace along which the function is varied. Alberco and his collaborators a couple of years ago explored training two-layer networks. So they're explicitly like training real-world networks, not just working in function space. Not just working in function space from the get-go. They constrained W to have rank one. So they're thinking about single index models. And they showed that you can actually achieve generalization guarantees that match the near-optimal sample complexity that I just referenced. So that was a really nice work that really was very thought-provoking for us. Similarly, Damian Lee and Sultana Quttabi said, hey, if we look at two-layer networks and we don't impose explicit constraints on W. Explicit constraints on W, but we use sort of a modified version of gradient descent, then again, we can learn representations of the data that really only depend on this underlying subspace of the multi-index model. But as Morrow hinted at, people are focused here on rates. These rates really only kick in when the number of training samples is quite, quite large. So, a more recent paper by Daniel Su and his collaborator said, Collaborator said: If we actually take a two-layer Rayleigh network and train it with a moderate number of samples, then we can't really learn the parity function, which is an example of a single index model. So it's an illustration, a concrete illustration of a setting where under the same framework as some of these papers, but with a moderate number of samples, we don't really see strong benefits when you have a two-layer network. I'm going to have trouble writing it down. I think you're like counting, you got your X's as being, I think, binary vectors, plus one, minus one elements. You're counting the number of plus ones versus minus ones. I think it's like whether it's more plus ones or more minus ones. So I guess basically the sum is the sum. Yeah. So this is a classical function that people use. A classical function that people use a lot in machine learning theory because it is a quite hard function to learn. But it is an example of a single index model, and that's why it was sort of relevant in the context of some of these other works. I could be wrong, but I think the last thing. The beginning of your talk was more complex or the regularizer. Right, so I'm going to talk about the regularizer and its connections to promoting things that are like multi-index models. And I wanted to provide this as context for other works that have looked at neural networks and multi-index models. You're right that they are not using explicit regularization, but they are imposing constraints either explicitly or implicitly in lieu of doing regularization. Of wing regularization. So it's certainly not a huge amount of overlap, but I thought it was still relevant context. Okay, so what we're going to do is we're going to not constrain our attention to multi-index models, but rather we're going to think about the rank of different functions. So in particular, we can think about the expected gradient of outer products of a function. So we look at the gradient of a function, grad f of x, and its outer product with itself. Its outer product with itself and the expected value of that over our domain. So, I'm going to call that matrix C sub L, covariance matrix, and I'm going to refer to the principal subspace as simply the range of that matrix and the rank of a function as being the rank of that matrix. So we looked at this little picture before. So, this is an example of a function with index rank one, because it's only varying in one direction. And so, this expected gradient of outer products matrix egg opposite. Median of outer products matrix, CGOP is going to be a rank one matrix. So, this is intimately connected with things like multi-index models, right? If I had a multi-index model of this form, and then I were to look at its gradient and compute this EGOP matrix, then what I'm going to find is that the rank of that matrix is going to correspond to the rank of that subspace corresponding to V. Okay, so if V has R linearly independent. V has R linearly independent columns, then the corresponding function here will be ranked R. So it's consistent with what we talked about before, but now we can be a little bit more flexible in our definitions. So in particular, this gives us a mechanism for thinking about functions that are not exactly low rank, not exactly multi-index models, but which are maybe close to it through the lens of mixed variation. So before we go to the definition, So, before we go to the definition, here we just have an illustration of a collection of different functions. All of these functions are defined over R2, and all of them have rank two, right? None of these are multi-index models. But as we move from left to right, the mixed variation is going from high to low. So we can think of the mixed variation as capturing how close our function is to having low rank structure. And in particular, what we're going to do is we're just going to take that. Going to take that EGAP matrix C subeta and take its square root and look at the SchatenQ norm of it. And this is going to be our definition of mixed variation. So this is going to give us a lot of flexibility to think about things like representation cost for general classes of patients. So here's our kind of main result capturing the representation cost of our The representation cost R sub L. So we have upper and lower bounds. Let's look at the upper bound first. It's bounded by the rank of the function times the R2 cost of the function. So how well we can represent that function using a two-layer network and the depth L is showing up in exponents on these two terms. And very similarly, our lower bound depends on the mixed variation of the function. And again, this R2. So what this So, what this is telling us is when we try to minimize this L-layer representation cost, what we're doing is we're favoring functions that vary primarily along a low-dimensional subspace. So these are things like multi-index models, but more precisely, they're functions with low mixed variation, but also which are smooth in the sense of having being easily representable with a two-layer network along that subspace. So let me illustrate this for you graphically. So, let me illustrate this for you graphically. Oh, sure. That's right. That's right. Perfect. Yeah. Thank you, Dustin. Okay. Okay, so here is a numerical example. So I have a bunch of samples that are corresponding to the red and the black dots. And then what I'm going to do is I'm going to train a network to interpolate them exactly. Or I'm going to look at different interpolations of these dots. So this is an example of the function that I would get if I were to train a two-layer network. So this function has variation in both directions. Both directions. There's no low-rank structure here. And then I have in the middle column what would happen if I were to train a network with linear layers. And what we find is that we're still interpolating exactly the same training points, but the function is only varying in one direction v. And what I'm showing with the little pluses here, the projection of each of the training points onto that subspace. And then down here, what I'm showing is a plot. What I'm showing is a plot of the cross-section of the function along that subspace. So the little dots here correspond to the projections of my training samples, and the blue line is the learned interpolating function as it varies along that subspace V. Now, in contrast, here's sort of a made-up function that would correspond to choosing a different 1D subspace V in this direction. In this direction. Now, since these training samples are in kind of random locations, I can project them all onto V and they're not overlapping. So I can still learn an interpolating function. So this is an interpolating function that has good alignment or which is only varying in one direction. But when we look at the interpolating function, we can see it's an absolute mess. And so, again, what our theory is telling us is that when we try to minimize these representation costs, we want to find functions. Clause. We want to find functions that have limited variation orthogonal to this subspace, as we see in both this middle and right columns. But also, we want functions that vary essentially smoothly along that subspace, which we only see in the middle column. Okay, so for instance, if we have two different functions that we're trying to choose between when we are perhaps interpolating samples. Interpolating samples. And if one of those functions is lower ranked than the other, then as long as the number of linear letters is sufficiently large, we're ultimately going the representation cost of the lower rank function is going to be lower. So if both of them were able to interpolate training samples, then a shallow two-layer network could potentially choose the higher ranked function if it's slightly smoother. But when we add these linear layers, we're going. Add these linear layers, we're going to select the lower rank function. So, let me return to some of the prior work that I mentioned earlier, just to make some of the distinctions clear about what we're trying to say. So, in past work, perhaps oversimplifying a tad, we looked at two layer networks and we said, hey, they can actually achieve the near optimal sample complexity for multi-index models. For multi-index models. But all of that theory depended on having a data generating function that does indeed correspond to multi-index models. And for the most part, they considered specialized algorithms in lieu of regularization. And those rates really only kick in when you have a huge number of training samples. So, our work is slightly different in the sense that we're really focused on characterizing the implicit bias of these networks. So, every So, every theory that I've theorem that I've shown you so far does not depend at all on the data generating function. We're really just talking about what kinds of functions are preferred by these networks. And what we see, what I'm going to show you in a few minutes, is that can yield empirical advantages even for moderate values of the number of training samples. This is really exciting. Okay, so let's say that I have I have a training data set, just a finite number of samples. I can use our theory to define something called the interpolation cost. So here with an index R. So this interpolation cost is the smallest R2 costs needed to interpolate the data with a function of rank R. So in this case, I've got a finite number of training samples in both. A finite number of training samples in both settings. I can interpolate it with a rank two function really easily. So the interpolation cost in this example was two. But if I were to force myself to interpolate it with a rank one function, then I have to use something that's much less smooth. So that interpolation cost can be enormous. Okay, so I defined this rank of a function for you before, but the next theorem I'm going to use an approximate. approximate approximation of that sure right so what I'm doing is that I have my function it's only varying in R directions so I can think about that essentially link function g and I can talk about how easily I can represent that with a two-layer network and that's the R2 costs that I'm talking about so I'm saying Costs that I'm talking about. So I'm saying, so here, I'm going to connect this in a second to our representation costs, but here I'm just talking about a definition of what I'm going to call. Yes, that's right. So when I talk about the minimum margin cost, minimizing over both of those things. Right. Okay. Right. Okay, so we talked about the rank of a function a little bit ago. I'm going to now define the effective rank of a function by simply counting the number of singular values that are greater than some threshold epsilon. It's a pretty standard notion. Now what we can do is we can say, what if I were to take my finite number of training samples? And here I'm not assuming anything about where those training samples are coming from. And now I'm going to train. Now, I'm going to train a network to interpolate those samples. And I'm going to minimize this R subl cost. That's how I'm going to choose my interpolant. Then what we can show is that the rank of this learned interpolant or the effective rank of this learned interpolant corresponds to the solution of this optimization problem where we're sort of sweeping over all possible ranks of the function. Of the function. And we're looking at that rank to some power that depends on the number of linear layers, but as well as this interpolation cost that I defined above. And so in particular, so here's the first and I think only time where I'm going to assume something about the data generating function. So this is true no matter what, where no matter where my data comes from. But to illustrate what it implies, if we knew that the data generating function Knew that the data generating function had rank R. And if we knew that the number of linear layers was large enough, then we know that the effective rank of this learned interpolant is going to be bounded by R. Okay, so this tells us that when we use these L-layer networks, we can adapt to latent low-dimensional structure in the networks, which was one of our, in the functions, which was one of our goals from the get-go. From the get-go. Now, one kind of small clarification I want to make is that we are not talking about doing PCA on our features. So, many times in practice, when people have really high-dimensional data, the first thing they do is they run PCA on the feature vectors, and then they try to do regression in that space. And that's not what we're talking about here at all. Okay, so in particular, with the PCA approach, you would apply PCA to your training features so that the subspace that you're working So, that the subspace that you're working on does not depend at all on your responsive fly. And then you train a neural network on those reduced-dimensional features. So, here's an example where I've got a low-rank or low-mixed variation data generating function. Now, if I were to take these samples, which are the little dots, and I were to run PCA on them, well, there is no sort of low-rank subspace that those dots lie on. And so, now if I were to, but if I were to. And so now if I were to, but if I were to force myself to find the best 1D subspace, then it goes in this direction. And so now if I try to interpolate my samples by only varying in this direction, I ended up with a crazy looking function. You can see I wasn't even really able to fully interpolate because it was just so messy and difficult. But in contrast, when we train a neural network with these extra linear layers, then we are homing in on the low dimensional. Homing in on the low-dimensional structure of the function, which of course depends on the function values, not just the locations of the samples. Okay, so I just want to show you a few numerical examples here. So we, well, Sue created a random rank R function multiple times, F star, in 20 dimensions. And then from that, she generated a uniformly distributed. Of uniformly distributed training feature vectors and then responses using that function. And then she trained neural networks at varying depths and with width a thousand. And then what she did is she used just PyTorch's default initialization and the Atom optimizer. And I'm emphasizing this because all of the theory I've been talking about is finding the global minimizer for the representation costs. We are not necessarily finding the global optimum. Finding the global optimum. And yet, we're still going to see a real empirical benefit to having these linear layers. It's consistent with our theory, even when we aren't exactly finding the global optimum. And in addition, we're not relying on specialized optimization algorithms or initializations. This is just really sort of the way in which people typically train these networks. Okay, so first of all, these results show that adding linear layers does promote better generalization, as we had hoped. Generalization as we had hoped by minimizing this lower representation cost. So on the left, we've got rank one functions. On the right, we've got rank two functions. On the top, we're considering the setting where there's no noise whatsoever. And so the solid lines would be just the classical two-layer networks, and the dashed lines show what happens when we include linear layers. And we see for a variety of different training sample sizes, we're consistently. Sample sizes, we're consistently getting much lower mean squared areas. And then in the bottom, when we add noise, the same effects still hold, especially in this rank two setting. And we're very quickly getting to sort of the best mean squared error we could hope for, given the amount of noise that's been added. In addition, we see the same kinds of effects when we consider out-of-distribution generalization. So here I'm showing the exact same. So here I'm showing the exact same kind of thing, only now our test data is on a much bigger domain. We've increased the radius of the domain. And so we're seeing that by finding and adapting to this structure, we're going to do much better out of distribution or outside that original training distribution domain. Finally, what we can show is that when we add these linear layers, we do find lower rank interpolants, which is one of the main theorems that I presented. Which is one of the main theorems that I presented. I said we would be able to adapt the latent low-dimensional structure associated with our training data. And so, in these plots, the solid lines, again, correspond to what you get without any linear layers, and the dashed lines correspond to what you have when you have linear layers. And so, all the solid lines are sort of overlapping each other at the top. You can see every function that they learn has rank one or rank two, regardless of how. Or rank two, regardless of how big the training set is. So, as we were talking about before, even though there's some literature saying that two-layer networks can achieve these mini-max rates under special circumstances, we're not seeing it even in these relatively low-dimensional settings in two to the 11th grading samples. But in contrast, when we add the linear layers, we're able to get lower rank function estimates out of these things. And in fact, not only are our function estimates In fact, not only are our function estimates low rank as they should be, but the subspace along which our function estimates are varying is the same as sort of the true principal subspace of the data generating functions. Okay, so I just want to close by citing an empirical example from Misha Bilkin and his collaborators from last year that is tangentially related to the theorem, but which I Tangentially related to the theorem, but which I thought was pretty interesting. Because if you remember, when we talk about function ranks or mixed variation functions, at the heart of that, we had this expected gradient of outer products. That was sort of the foundational tool that we used to build up all of our theoretical results. So Misha was actually also looking at this expected gradient of outer products and exploring that in a pretty interesting empirical study. Pretty interesting empirical study showing, for instance, how that if you look at the expected gradient of outer products and then you look at the principal eigenvector of it, that a visualization of the principal eigenvector actually shows strong correlations with the kinds of tasks that the network was being asked to solve. And he explores different ways in which you can think about this and ways that it might be leveraged to improve the training of networks. To improve the training of networks moving forward. So, in summary, when we train a neural network with adding all these linear layers, essentially what we're doing is we're trying to find a low-dimensional subspace along which our function is varying in a smooth way. That's the bias that's effectively induced by adding in these extra linear layers. But this still leaves a huge number of open questions that I don't yet know how to answer. We'd love to know the role of depth more generally. Know the role of depth more generally, not just linear layers. And in particular, if we had more general neural networks, we expect that there is going to be some kind of latent low-dimensional geometry that's significant, but it's not going to be subspaces. And so what kinds of latent low-dimensional geometry are going to appear in these more complex settings? The nonlinear layers are nice in the sense that they give us networks that are inherently compressible without kind of searching over possible compressible. Kind of searching over possible compressions of the network. So I think that's a nice sort of side effect of this. And in addition, there's been some additional work on adding linear layers that are not arbitrary, but have some kind of structured sparsity. And that work is really focused on using linear layers to improve computational and memory requirements of training large-scale networks. And an open question is: how does that structure in these linear layers? In these linear layers, affect the implicit bias and the representation costs that we've been talking about. So, thank you very much. I really appreciate your attention and all the questions so far. And I wanted to thank my sponsors and co-authors again. Thank you very much. We have examples that show that these upper and bound, lower, that our bounds are tight, that you cannot, that the upper bound is indeed controlled by the rank and not the mixed variation. I think if I think if we were to make additional assumptions, we might be able to get matching upper and lower bounds. But without additional assumptions, I think this is the best that we can do. My second question is related to something that you noticed towards the end of the speech, which I'll get to. Does this suggest that, like, suppose I want to find this principal subspace? Could I just train a big net and then compute this E-G-O-P thing to find the principal subspace and then project onto that subspace and retrain a smaller network? Yeah, I think so. And in fact, I think he's got some papers that are more algorithmic in nature that take this onsatz, as he calls it, and uses it to inspire the development of new training modes that are, I think, more or less doing what you're describing. Doing what you're describing, yeah. I think they need the microphone for the Zoom or for the recording. Is this better? The implementation. So you point out that effectively it collapsed all of these linear layers into this shuttle positor. So now you have two alternatives, right? You could work with your layers, then you can. It could work with your layers and you could analyze these squares, or you can compute this honey quasi-mark. Which do you think is more efficient? And I mean, this type of optimization problem that you're solving, is it scary? You know, does it have lots of local input? Or is it not a lot scary? The optimizations are all scary. They've got complex. complex optimization landscapes. So here, maybe I'll go to this slide. So I think what you're saying is, hey, now that we know all this stuff, why don't I just explicitly use this regularization as opposed to implicitly having it by adding linear layers? So I would say a couple of things. First of all, people have explored explicit nuclear norm regularization. And my understanding. And my understanding is that it is fine, but it adds an additional computational burden, and the performance gains are not worth that extra computation. So people are not using it in practice. Now, but in addition, our goal here was not to come up with new regularizers to use in practice. It was more to try to understand the role of architectural choices in affecting sort of implicit biases of. Sort of implicit biases of these networks. And so we're saying, you know, not that we kind of are advocating for this form of regularization, but rather, if you were to add depth in this particular way by adding linear layers, then what's happening is you are trying to find functions that as much as possible have this low-rank structure. And so, yeah, it's a slightly different sort of objective. I maybe this is a dumb question. So you need to how does this depend on the number of neurons that you have per day? And in particular, does that give me a knob uh to modify the value of the RNL that you will obtain? Supposedly uh the value it is maybe all of my While there it is, maybe all of my colours will go down together. How does that behave with the width? Right. So I've kind of shoved that under the rug in this talk, but really, what we're talking about here is the infinite width limit. So as we were to take the width and extend it to infinity. So if you start constraining the width, then the function spaces do become much more complex. And we don't really have the right tool set for analyzing the representation costs in that setting. Setting. Yeah. So I won't say my name because I already quickly, Alberto. You mentioned a work that says that. But you mentioned the work that says that two layers are not enough, but it's hard to be available. Yeah. When you add more layers, or you add the same sparsity, and then fix that, or is it still? That's a great question. We haven't explored the parity example empirically. I think that, as I said before, these are complex optimization landscapes, and it's not clear that we're finding the global minimizer. So that case might also be challenging. So, that case might also be challenging in our setup. I'm not sure. But my point there was simply that, you know, as Morrow suggested for the minimax rates, these papers looking at rates for the two-layer networks also have this effect where the rates seem to kick in only when you have large numbers of samples. So, you know, what we're in the numerical results that I showed towards the end, right, we were seeing that we were consistently. That we were consistently outperforming the two-layer networks when we added the linear layers, even though, you know, for instance, Nancis showed that the two-layer networks were sufficiently rich to be able to achieve the mini-max rates. And so my interpretation here is that what we're doing with linear layers is obviously not improving on the mini-max rates, but really affecting these constants so that we can do much better in the moderate sample regime. We did fix the dimension, yeah. This is, I've seen a phenomenon where as you increase heat, the number of iterations during the training phase goes down. Because I mean, we're, I guess, sort of one single indicator, but we do the class certain extent index models with the constants and largest functions. And that estimator would minimize the additional functionality to rate. So I wonder if that would motivate competition with Book of Right. So we have not looked at the computational complexity. Everything is focused on sort of the minimum representation cost functions. So, you know, in contrast, I mentioned the paper by Damian Lee and Sultana Katabi. And there they're really thinking very carefully about the gradient distance dynamics. So they're really looking explicitly at the computational aspects. You know, the computational aspects. So, our work in contrast is just sort of trying to characterize this cost. But empirically, I wouldn't be surprised if what you're saying were to happen, that you have to have longer training times for higher dimensional data. Sounds plausible to me, but it's not something we've investigated systematically.