For hosting this event. So I think we couldn't find a better place on ours to host workshops. So please continue to host this every year. Yeah, so I'm Body Joe, so I'm an assistant professor at UCLA. So I work on computer vision and also machine autonomy. So I'm thinking more about how we can incorporate the 3D generation or generative models to enhance the To enhance the interactive environment where we can train using bodied AI. So, today I would like to share with you some of our work on the large-scale scene generation and simulation from intermediate representation. So, I would like to start my talk with this condition scene generation. So, let's say we want to generate some things, right? So, we want to generate that. One way we can generate the thing is we can have a The thing is, we can have a semantic mask as inputs that use a pit-to-pig to generate a street view. We also can generate street view from the large models, right? We can give the prompt, like a busy street with a lot of pedestrian vehicles, then generate those amazing images. But there are still a lot of issues. Let's say we want to try to add more vehicles or try to move these vehicles. Or try to move these vehicles inside the things. So it's very difficult to do this kind of editing using the semantic mask, right? So because we don't know the distance between those vehicles and with the cameras. So here I would like to propose a new representation on this using the bird eyes view representation to do this kind of additive. So we can have this kind of top-down. So, we can have this kind of top-down view to configure different vehicles. Then, we can easily know their distance and the relations. Then, we can edit the location of different vehicles and their posts very easily. Based on this vertical view representation, we also can do a lot of scene interaction and animations. So, you can see here, it's very easy to see that. See here, it's very easy to see the dynamics inside the scenes. Then we can use this top-down view or bird eyes view for scene simulation. So, in this talk, I would like to talk about using this bird eyes view representation for both scene generation. We want to generate street view images from this bird eye view representation. We also want to use the bird eye view representations for scene simulation. Scene simulation. The first work I would like to talk about is this BEV gen work. We have recently put on archive. So it's a conditional scene generation. So we want to generate the street view images condition on the bird eye view layout. So DV is birth eye view as input. Then we want to generate multiple views of the things. So here you can see that we have three views. See that we have three views captured by three different cameras. So, there are many challenges here. So, the first challenge: we want to make sure the condition is correct, can generally correspond to the input verde view. We also want to ensure the cross-view and camera consistency. If you have three views, so there's some overlap between the cameras, so we won't have the consistency. Cameras, so we won't have the consistency across the camera. So, in order to achieve that, so we develop a framework based on this condition generation. So, we use a VQVE architecture. So, we have encoder and decoder for the street view images. Now, we also use VQVE representation for the BQV representation for the VA layout. Then we can then have an automated resource transformer to link to do this kind of translation. We want to translate the street view images from the encoded representation or vertex view. Then we can also incorporate some consistency to ensure the consistency across different views. So based on this design, then we can generate the street view imaging. The street view images giving the simple. So, here is one result, so giving this bird eyes view representation. Then we can generate a sixth view, different images. So, we also can keep the consistency across different things. So, create another image and see and generate the buildings, background, and vehicles. This is also a demo video showing you the generation result with the ground truth. Result with the ground truth images. So the this bottom line is the ground truth, the real street images, and the top row is generated things. So here we don't have video constancy, so each frame just generated individually. Then you can see they share some similarity with ground truth because here we use birth size view as a reference. Bird's eyes view as a representation. So don't have the similarity of all the visual appearance, but the semantics are all correct, like the location of vehicles. So the previous work is more about image generation. So right now we want to go from this image generation to 3D aware generation. We want better control in this vehicle and more accurate. Vehicle and the more accurate geometric constraints on the generated six. So, this way moving to this to combine the generated model with a NERF. So, this is my student went to 30's lab at this lab and did this wonderful internship. Then try to incorporate the 3D layout at input and generate things. So, here we have the 3D layout. We have the 3D layout inputs that configure the location of each viewpoint. Then we leverage the new renderings to further render this search 3D layout into the realistic scenes. So then we develop this work called discussing that try to learn to disentangle different objects using this generative radiance field. Then we can achieve control ball. Then we can achieve control of 3D aware scene synthesis. So, for this work, published at the CBPR, so we have the 3D layout as a prior for the inputs. Then we can generate the object. So into the op generator, then we can generate the object inside things. Then we also sample from the background generation. Then we have background foregrounds that will mingle with each other. Mingle with each other, then we have volume renderings to render the whole things. Then we further incorporate some discriminators using the GAMOS to further improve the realism of general things. Because we have a 3D layout, we can further use the ground truth layout to crop the general scenes and put some loss for this foreground object. So we can further improve the So, we can further improve the quality of the foregrounds. Through that, we can, after the training, then we can do our editing very easily. The whole generation process is conditioned on the 3D layout as inputs. Then, we can easily manipulate this foreground in the 6. Because this is a new rendering model, then we can very easily change the camera. The camera pose of the things. So, here is the camera control for the generated things. We can further use layout inputs than added the different objects inside the sinks. So, this demo video showing you the same manipulation results. You can easily control the 3D pose of the object inside the sinks. So, we further compare with previous. Here with previous 3D on your image generation results, we draw off and each 3D and show our D-Sub scene can achieve better generation quality. So that is the first part. We leverage the layout of Bird's SVU to generate the scenes. The second part, I would like to talk about how can we use a B-vay representation to do scene simulation and then crash. Simulation and the interaction. So that is very important to meaning the interaction and the robotics application. So after we have the generative scenes, how can we use it to enhance robotics and the embody AI? So the purpleized view is actually a very common structure to simulate things. A lot of the atomic driving companies actually collect their data and represent the trajectory in this kind of top-down. In this kind of top-down view. So, here is some of the traffic scenes that we imported from the WIMO data set. But those data are still passive data, right? It's just collected, then it's hard to directly interact with the trajectory. So, to make those offline data more useful, along with my students, we are building an open source driving simulator called the Metal Drive. Drive. So this meta drive is very lightweight, can run more than 500 FPS. So we did a trade-off between the visual appearance and the flexibility. Based on MetaDrive, we built a range of embodied AI tasks from safe reinforcement learning, multi-agent reinforcement learning, and more. It's already open source through this link. We also, this simulator can provide multiple observations. We also built the interface. We also built the interface that we can have human in the loop to interact with the simulator. So, one first generation capability we explore for this simulator is using the procedure generation. So, we found out rather than some neurons and models, so the procedure generation actually can give very nice traffic scenarios. So, we first defy different traffic blocks, then we can sample from those traffic blocks with different configurations. Traveler brawls with different configurations, then we can just glue them together, then we can create things. So, this demo video is showing you this position generation process. So, after we glue these different traffic blocks together, then we can further turn them into this interactive environment, then where we can train and evaluate this embodied AI agent. So, here is a demo video showing you this learning process. Video showing you this learning process. So, here we can train some agent to drive inside the environment. So, this is the starting training process. You can see this agent just doing all kinds of crazy stuff in order to learn this correct driving behaviors. So, we can use this environment to develop some new or better safe reinforcement learning algorithms to train this agent. So, after the training, so this agent actually performed very well in testing. Perform very well in testing case. This is a testing case. After all, this accident and clinicians and agents can drive in this environment very well. So you even know how to bypass these two travel cones by the shortcut between the two traveling cones. So we further want to enhance this simulator by improving the realism of Improve the realism of the environment. Then we have the initiative on this I call real toolsing, right? To try to borrow more real-world assets and data inside the simulation. So we have this perception module that can input the real-world driving logs in the simulation. Then this is the real world driving videos, then we can recreate their digital twins and provide a different perception. Perception modules for that. Then people can use this simulator to develop better perception and autonomy algorithm. To further enhance the scenario we have in the simulator, then we have this recent work on scenario net and try to input more driving logs inside the simulator. So here we input more than 500,000 different traffic scenarios. Different traffic scenarios collected in the real world inside the simulator, then we can replace all those different complex situations in the simulation. Based on the imported data, we also can learn some models. So the first thing we did is extract their encoding representation, and we built some TCME visualization for this traffic scenario. So basically, we treat, we use some. We treat where we use some auto-encoder structure, then giving a new traffic scenario, then we can project into feature space, then we can build some t-shirt visualization of all those different scenarios that imported from different data sets. To the side is the procedure-generated data. So you can clearly see it's very different to the real-world scenarios. Even for the real-world scenarios, there are some differences across different data sets. Some difference across different data sets, such as a WIMO data set and the new plan data set. The things is a new plan data set is usually collected in, I think, most of the data, more than half of the data are collected in Singapore, so they have a very narrow street. So compared to the women collect the scenario in the US, so they have more complex intersections from more number of lens. So based on this, So, based on this improved scenario, we also want to evaluate this autonomous driving stack. So, one thing we tried is we built a bridge to some open source driving stacks. So, this is called OpenPilot, ADSTACT. Basically, it's an open source autumn driving software. So, you can actually install in your own cars. Then, you have a dashboard camera, then you can wear this into your Wear this into your control, then this system can control your vehicle. So you can have basic driving capabilities. Then we can rewire this open pilot into the simulator. Then we can run this AD stack inside this simulating environment and evaluate their performance. So that is a pretty cool function. Then we can see whether we make some mistakes into the simulation. Then we can further. And we can further train this model to improve their capabilities. Using this simulator, we also have this have human interface. So it's more like we can have human, my graduate students are pretty happy playing the simulator every day. So we can have human demonstration and then collect human driving data or evaluate human performance in this complex. Performers in this complex situations. So, this is a demo video showing you our recent work we've done. We use human interactive learning to teach the agent driving environments. So, this video is showing you the whole training process. So, at the beginning, so it's more like doing imitation learning, like each step. Then, this agent will imitate human behaviors, then gradually. Then, gradually, this agent will become more independent and then will pick up the correct driving behaviors. After 20 minutes of training, then you can see this agent can drive safely inside this simulator. Okay, so since second last slide in my talk, from this talk, I show you that the user. From this talk, I show you that using this intermediate representation, this bird IDU representation, then we can very easily have a controllable scene generation. And we also can use a bird's eyes view to do a scene simulation. So then we can explore the synergy between the scene generation and the scene simulation. This also opens many opportunities to explore the synergy of the 3D generation. We can generate the different. Generation, we can generate the different assays and generate the different things. Then, how can we input those generated environments inside the simulation? Then we can simulate the environment and create a digital twin for that. Okay, so I would like to end my talk here. So, this meta drive already released, so you are welcome to check it out and let me know how you use simulators to facilitate your research. Thank you. Thank you. I like the layout PEB properly for the senior. But the question is that what do you think the challenge is of extending this? Currently, you only have the things with objects, with the cars, but the thing is more complex than just cars. There's like buildings, trees, besides drains in the road. So what do you think the challenge or the challenge of it? And your current pipeline. Of expanding your current Python into this kind of all-complexing scenarios? Yeah, that's a great question. So, most of the trajectory data people collected only content vehicle trajectories. So, we even don't have much less pedestrians, which are very important role for the traffic scenes. I think we can better leverage the video, the driving video, really synchronize the driving trajectories with the driving videos, then we can run some semantics notation or run some neural rendering on. Or run some new rendering on the driving videos, then we can use that modality to recreate the background and other objects and synchronize with the traffic blocks. Then we can recreate more realistic traffic scenarios. Do you think there are enough real-world data to run out like a generative models on say making decisions and trajectories for our situations? Or do you feel like you we still need to make Do you feel like you still need to make simulators to get more data on that? Yeah, that's a great question. So I think compared to this indoor case, like Nova talked about collecting this indoor household environment, for the outdoor street view things, I think there are much, much more data there because automatic driving companies have the data collection vehicles driving around and they collect all kinds of RGB, LiDAR. RGB, LIDAR, and even running slams all over the cities. So there are much more data so you can use to train and evaluate them. But one issue is those data after the collection become offline data. But if you want to train some embodied AI, so you want to evaluate some counterfactual evaluation or some cause-effect analysis, then you have to turn the software data into the simulation and then they can become. And the term, then they can become more reactive or interactive than where you can trimuate. Thanks for the really super cool talk. As many people in A-B space also have more of the bird's eye view now, in the last years, Baymo in particular started advocating for these vector net style approaches where you use vectors as view, these vectorized representations. What are your thoughts on this component? What are your thoughts on this compared to bird's eye view? Because particularly for prediction, this was a big deviation from what we used to. Yeah, I think this is an open question. So like Tesla also proposes the open occupancy map. So occupancy map is something beyond this bird at view. They have some the height, so they have some 3D representation on that. So in that case, it can capture more structure about the environment. But I would say it depends on the top. But I would say it depends on the task. Basically, right now we want to simulate the environment in simulation in a more large scale and more efficient way. So that's why we take the first light view. But if you're doing some other tasks, like forecasting or some 3D scene reconstruction, probably another OpenSIMAP representation would be a better choice. I have maybe a a dumb question, but so it seems that uh these simulators they focus on These simulators they focus on like detection and planning. But how about interaction? For example, if a car hits a tree or if a car hits another car, so this should be accounted in the planning, right? Maybe it's better to hit the tree than to hit the car. And like, does the car change? So do this open world simulator support this kind of compact? Yeah, I think first of all, so how to simulate this obnormal or accidental Abnormal or accident situation is an unsolved problem. So, because all the data collected in the real world, most of them are normal things. There are no less collisions here. So, we have some work that try to use some adversary training to intentionally generate such safety-critical scenarios. So, if we can have such a scenario, then we can replace in the simulator and study these interactions. Actions. But one advantage of having this simulator is that you can control everything. You can control the path of the players, the agents, then to simulate all kinds of scenarios you want. It just depends on whether you want a rule-based simulation or you want a learning-based simulation for that. So what is the difference between outlay and indoor layout? Indo-mail people hierarchical structure. Yeah, I'm not aware of any people modeling with a syngraph because here after the collection they have to stop them build and then just do their operation and representations for that. I think GJPAC tomorrow will probably talk about this indoor layout. Though on the out. So I look forward to that. Thank you.