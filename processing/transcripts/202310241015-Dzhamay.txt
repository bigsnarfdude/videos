I have to sort of explain that this talk is about some work that I've done quite a while ago. As you can see, the archive paper on that is about 10 years old. And that's something that I really want to understand better. And that links to the theme of the workshop, although tangentially. So it's somewhat of a curiosity, and I'm wondering that maybe people in the workshop will notice some relations to what they're doing and help me understand this curiosity. I think there is some content there, but I'm not quite sure exactly what it is. Okay, so after this strange introduction, let me explain what this is all about. So So, this work started when I was trying to understand the Lagrangian description of discrete integrable systems in a sort of classical, not multi-form way, similar to the one mentioned in the previous talk. And I was interested in understanding the relationship between discrete Pan-Ave equations and isomonodromic transformations. And in doing that, I noticed some very interesting relationship between eigenvectors of Lex matrices. And the behavior of this eigenvectors is somehow at the background of integrable, interesting integrable dynamics. So that's the combinatorics that I want to explain. So for discrete Panavay equations, For discrete Panav√© equations, what you're interested in is studying isomonodromic transformations of the following four. You have some Lax matrix L of Z, and I will specify exactly what kind of a form this matrix has. And you break it into some factors, and then you try to permute the factors. So it's a very classical construction. And after you permute. And after you permute the vectors, you what is called refactorize. There's some definition of what the first matrix is and what the second matrix is. And that gives you a dynamics on the space of matrices. This is isomonodromic dynamics, but you can also look at the isospectral dynamics that doesn't have this shift in the parameter z. The isomonodromic dynamic was interesting for me because. Dynamic was interesting for me because it was related to differential Panovere-5 equation. But for the purpose of the stock, I spectral dynamic is also just as well, just as good. And the objective was to try to see if these equations of motion on the space of matrices can be interpreted in Lagrangian form, or specifically, if it's possible to write. Specifically, if it's possible to write a Lagrangian function for these discrete dynamics, what do I mean by that? So if my configuration space is denoted by Q, there is a classical continuous formulation of the least action principle. And in the discrete, instead of the integral, you go into summation and or the Lagrange equation take the following form, where, as usual, form where as usual q under tilde and q upper tilde are previous and next point in the configuration space i think that the same equations were written in the slides of the previous talk and so that's the framework and such a dynamics often can be realized in what is called the Mozer-Vesselov approach. Moser-Vesselov approach, where the dynamics on the configuration space is in some way induced by this refactorization transformations on appropriately defined space of Lax matrices and appropriately defined blocks. Okay? So that's the isospectral dynamics where you permute the blocks. Just some kind of a conjugation. So So that's the dynamics that I'm interested in. But classically, people work with matrix polynomials. And that question was posed by Igor Krishever, who said that you really should look at rational functions because matrix polynomials will have irregular singularities at infinity, typically. And if you break, if you instead Break if you instead rewrite it as rational functions, you can break it into simple zeros, simple poles. So that's what we want to do. So the space of Lex matrices here is just the space of matrix functions on the Riemann sphere that satisfy the following two conditions. The first condition is normalization at some point z naught that we take to be infinity, so that the matrix there is Is diagonalizable, and we assume that we choose the gauge in which it's actually diagonal. And then the singularity structure. So, what we do is we go from the matrix function to its determinant, which is just a rational function on the Riemann sphere, and we assume that this determinant is generic in the sense that we have only simple zeros and simple poles. So, this is an important condition of the determinant. The first factor comes from this diagonal matrix at infinity, and I will denote poles by the letter z, and I will denote zeros by the letter z. Okay, so of course, the numbers of zeros and poles is the same, but I'm deliberately using different indices for zeros and poles. Pulse. So this condition on the determinant generically is equivalent to the condition that this matrix function has simple poles. And moreover, the residues at simple poles have rank one. Otherwise, you start getting higher order contribution to the determinant. So then we define this residues and denote them by LK, and being of rank. And being of rank one means that you can write it as a product of a column vector into the row vector. So, this is the first pair of eigenvectors that we consider, this AK and BK dagg, where this dagger means transposition that it's a row vector. It turns out that in addition to looking at the matrix L, we also want to look at the inverse matrix that I will denote. matrix that I will denote by M and it satisfies exactly the same condition only with zeros being replaced by poles and here we have another pair of vectors C alpha and D alpha hat okay so this collection of four vectors two of them indexed by the indices of By the indices of poles of L, and the other two types indexed by the zeros of the determinant of L or poles of the inverse matrix, we call the eigenvectors of L of Z. And these eigenvectors are actually null vectors of the matrices evaluated at appropriate points. Okay? But Okay, but here notice that A and B are associated with residues of L, and they have to vanish for M at this point. Okay, so this is this collection of vectors. And then the representation that we looked at is what is called an additive representation of matrix L and its inverse. But you can also try to construct a multiplication. Can also try to construct a multiplicative representation, but to construct a multiplicative representation, you need to define what are your elementary matrices, elementary products. And I will look at the matrices that are called the Blashkipa type of matrices. So these are matrices that have the structure that a determinant of a matrix has a simple zero and a simple pole. Then, Then you can write this matrix in this form, where in the numerator that's just the corresponding eigenvectors of the matrix B. And I assume that the trace of this matrix is non-zero, then you can normalize by it. And if you do it, then indeed, the rational factor here, this would be the zero and this would be the pole. In doing that, Would be the pole. In doing that, I am having making some pairing of the zeros zk and oh, sorry, the pole zk and the corresponding zeros. So I need to choose some kind of association there. The nice thing about matrices of that type is that the inverse matrix is exactly the same. You just need to interchange zeros and poles. And for For a matrix that has multiple zeros and multiple poles, what you can do, you can define left divisors, which is this matrix B of the type that I showed you on the previous slide, and the right divisor that sort of pick up the corresponding pair of a zero and a pole. And the remaining matrices are regular at either zero. At either Z's or Zetas, depending on which matrix you look at. So we call this matrices left and right divisors. Then it's easy to see that the vectors that appear in these divisors are again pairs only sort of reorganized like before we had A and We had A and B for L and C and D for M. And here we have A and D and C and B. Okay, so for the left divisor, you have column eigenvectors of L and row eigenvector of L inverse, and similar for the right divisors. So if we somehow match some zeros with some poles, and there is a variety of ways of doing that. And there is a variety of ways of doing that, but if we choose some matching, then it can be shown that this matrices or the corresponding eigenvectors, they parametrized the space of the Lax matrices. And then on this space, we look at the dynamics that is generated by elementary transformations, where you just do the conjugation by a particular Do the conjugation by a particular matrix R, again of that form, and you choose this matrix based on the spectral curve of the Lex matrix L of Z. Okay, so the spectral curve is the usual one, and then you choose two points on the spectral curve, and then you have the eigenvector. The eigenvectors corresponding to these two points, and you just build your matrix like this from these eigenvectors. I'll show you on the picture. So here's a spectral curve for L. It's so at every point here, on one hand, you have the value of k, which is your eigenvalue, and then you have some vector psi, which is your eigenvector. Which is your eigenvector. And there are two of them: one is a right or column eigenvector, and the other one is a left or row eigenvector. And then you can choose two points. And at this point, at the point P plus, you choose the row eigenvector. At the point P minus, you choose the column eigenvector. And you build a matrix of your transformation. And here, we will. And here, mu plus and mu minus are projections of these points on the spectral parameter coordinate z. So that's a generic transformation, and you can always build it based on this pair of points on the spectral curve. But if you choose the points that lie over zeros and poles, that's when you get the block matrices. So here, if you choose a Here, if you choose a column vector at some point lying above zero and a row vector, some point lying above the pole, then that transformation corresponds to picking up the right Blush-Kipotapov factor and moving it to the left, or just refactorizing or really just moving it. And in a similar way, if you switch the roles of If you switch the roles of zero and pole, that will correspond to choosing the left factor and moving it to the right. So that's the refactorization dynamics. So my interest was stemming from Panlev√© equations, where if you look at two by two matrices and only two zeros and poles, like this, you define the spectral type of You define the spectral type of L as being the pair of zeros, pair of poles, this matrix in the diagonal, and three more parameters related to residue at infinity. So, and is just some gauging. So, you have a collection of spectral data, and then you want to completely parametrize. Completely parametrize the set of matrices. There's also something called spectral coordinates, where, as usual, one of the coordinates is a location of zero of a corner element, maybe this one, L21. And the other one is the value at that point of a diagonal element with some normalization. So, what happens? So, what happens is if you choose L of Z and break it into this left and right divisors, you have this data, and now you have your gamma and pi here. Inverse matrix, you see zeros and poles flip. The diagonal matrix at infinity flips, this case just change sign, and you have this type, and then if you This type, and then if you do the refactorization transformation, because it's non-autonomous dynamics, you have a shift, let's say, on a chosen zero pole. That's the choice of this factor that I'm using. And these parameters will change. And it turns out that you can explicitly parametrize your column and row vectors in the residue matrices. And the way this parameter The way these parameters change is what is called the discrete Panlever equation. Back then, I didn't know much about discrete Panley, so it was a very brute force computation, but that's something that I really want to revisit now that I understand how the geometry of Panlever equations work. But that was the motivation. And essentially, to prove that, you compare eigenvectors for Eigenvectors for original matrix and the normalized matrix, and establish this correspondence. And then there was this question, okay, so if I want to do an arbitrary refactorization dynamics, I can factorize like this, or I can look at the right factor, left factor. I can look at the right factor, left, right factor with some pairing or switch the pairing. And then I can do this transformation by taking the right factor over here and moving it to the left. And that exchanges, and after that, refactorizing, changing the order back from one, two to two. Changing the order back from 1, 2, 2, 2, 1. And isomonodromy is the same, only with the shift. So that's the dynamic that I'm interested in. And the question was: can it be written in the Lagrangian form? Is it possible to write the Lagrangian? And turns out that it is. And here is the explicit formula for the Lagrangian that can be adjusted to the isomonodromic or isospectral case. The formula is the same. The formula is the same. And the combinatorics that I want to talk about is really the combinatorics of this formula. Okay? So let me try to explain how it works. So this is a very simple case where I have only two building blocks. And even though I assume that they can have an arbitrary length r for core. Length R for columns and rows, in reality, it really can be two-dimensional. Then, if you look at the correspondence between multiplicative and additive representations, you see that the vectors in the multiplicative representation can be connected with vectors in the additive representation. This is simple because essentially what you have when you compute the rate. What you have when you compute the residues, this residue L1 at the point Z1, that's just that. But what you have here is on one hand, you have a column vector, and on the other hand, you have a row vector that you get by acting on this multiplicative eigenvector Q1 by the second matrix at the point Z1. And similarly for the other. So there's The end. So there are some relationships here. And if you do it for the inverse matrix, you get similar relationships between factors corresponding to inverse matrices and the other pair. So then you get this mapping and you can create a Lagrangian. And where does this Lagrangian come from? And where does this Lagrangian come from? There is this general property that if you take this Blashkip-Patapov matrix of this form with coefficients with P and Q eigenvectors and hit some vector W by it, you get a vector V and then three columns, vector column vectors that appear. The input vector W, the output vector Input vector W, the output vector V, and the vector in the Blashkip-Potapov matrix P are linearly dependent. And if you introduce a very special normalization, normalization given by this row vector Q, then they satisfy the following relation. Okay? And you can visualize this relation by the following elementary Elementary graph. So the three vectors that are linearly dependent are the sort of nodes of this graph, and the normalizer QI hat is the center. And the parameters that appear on the edges, they're related to zeros and poles of this matrix BI and also the point ZK. Also, the point zk that we have in here. Okay? And then there is a generating function that, for example, if you have the knowledge of this part of this graph, it extracts this vector as a derivative on the components of the. The components of the raw normalizer QI hat, you can get this V. But of course, it's all completely symmetric, so you can use this to get W and so on. Then we can use this to generalize a little bit and define a notion of a triple where you have a row vector in the center and three column vectors that satisfy this linear dependence. Your dependence. And conversely, sorry, not conversely, but more generally, you can just think about vectors in general vector n-dimensional vector space and its dual and introduce corresponding projective elements and And you can write down the same linear equivalence. So you can disconnect from a coordinate-based approach. And you can define dual triples where now you will have three row vectors normalized by a column vector sitting in the middle. And again, you can do it generally, or you can just work with coordinates. Worked with coordinates. So it was just a bit of a curiosity, but then turns out that there's the following fact. If you choose two column vectors, oh, sorry, two row vectors and two column vectors and parameters on the edges, then you can sort of build a cube. And I have to admit, this cube was inspired by multi-dimensional consistency, but I'm not sure it's actually. But I'm not sure it's actually about multi-dimensional consistency. So, and if you put this condition that occurs in the Lagrangian of refactorization transformation for Pandava equations, this condition propagates over all faces. And for each face, you can write down this function. And that function is the generating function for the Generating function for the opposite face. Like here, L. So you have this expressions built off the column and row vectors living on the top face with the parameters associated with them. And then to find this vector in our normalization, it can be done by this derivative. Sorry, I guess I was talking. I was talking about this one, right? So, this would be a generating function. You take the partial derivative with respect to this node with normalization coming from this parameter. And that's how it works. And also, if you have two triples that share an edge, you can glue that together. Again, you create. Again, you create this cube. So then we get this very strange relationship between the eigenvectors of refactorization transformations. So remember, we had this sets of four vectors, A, B, C, and D. So you put them in the vertices of a cube with a little bit of a twisting given by this normalization at infinity matrix L naught. And then And then you put this expressions that involve zeros and poles, and then that completely encodes the other four vectors. So the factorization transformation for one of the dynamics is given by Lagrangian that corresponds to this phase, and then it goes from here to here. And you can write down the formula for the Lagrangian. Write down the formula for the Lagrangian explicitly. And if you want to go in the different direction, it's just a Lagrangian sitting on a different face. I find it very neat and satisfying, but I also don't understand exactly what it means. And this very few words, this is a little bit of a speculation. If you want to go from two to three factors, Factors that becomes really difficult, and in a way, if you connect it with a cross-ratio system that is what's called the Q1 system, you can think about this four points on projective line as sort of four points corresponding to A and C vectors with a normalization given by. Normalization given by row vectors. And dually, there is another four points that correspond to B and D vectors. And if you look at all the relationship, you sort of get four points on one line and four points on the other in some subspace of your. Subspace of your vector space that are coupled. For example, here, this B2 hat is this point that couples these three. So you have a bunch of couplings. So in the three factors, this is connected to a Young-Baxter map. So if you look at the classical Young-Baxter map, there's one way to kind of There's one way to construct Young-Baxter maps from refactorizations. So, if we look at the same kind of dynamics, and suppose you have a matrix that you factorize in a particular way. So, here, each building block is associated again with some zeros and poles. If this factorization is unique, Um, you have uniquely defined matrices B2 prime and B1 prime, for example, when you interchange the first two factors, and then you can flip here again and flip here, and different ways of doing that results in the same answer, and then give you the solution of Young Baxter. The solution of Young Baxter. So if you visualize it using the eigenvectors, you get some strange picture like this. You look at your additive vectors A1, A2, A3, and these are for L. And for the inverse matrix, you have C1, C2, C3. And similarly for the Similarly for the row vectors, then A1 and A3, they spun some subspace. So you have these subspaces. And turns out that at the intersection of different subspaces, like at this point here, that's exactly where you get the various blocks of your factorizations. Factorizations and changing factorization, you sort of start at one triangle here and sort of flip it in this way. Okay? Almost if we wrap up in a few minutes, that would be rushing towards the end. So you can do it this way. You can do it this way. And I'm curious how to explain, how to describe. How to describe this combinatorial, if there is any way to introduce combinatorics in there and what this all means. Recently outside, I heard a talk by Nick Apple. I don't want to mess up his last name, but he was. He was giving a talk on triples, TSU triples, I think. And that had exactly the same kind of connections on vectors with normalizations, and it was related to cluster algebra. So maybe there's some relationship there. Okay, let me stop at this point.