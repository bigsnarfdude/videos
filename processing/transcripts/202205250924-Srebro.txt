I'm going to start without slides, but then switch to slides. So that is great for some of you to actually do methodology. Doing math without notes in the board. I wanted to, what I wanted to talk a bit about, which I thought this was in a family form, is a bit of a sort of review of analysis program, I would say, to learning using undetermined models. And reflect a bit and look back at which part. Reflect a bit and look back at which parts of it are plausible and which ones we know are not going to work or will work or might work. So the basic, so it's probably if you've heard me speak in the past half decade or more, then you probably heard me talk about my view towards understanding learning in deep learning or in big underturned models. Or in big underturned models, which really my view is, and I think now probably many of you share this view, I think, is that really all inductive biased learning is controlled by the optimization algorithm. So really what we're doing is we're learning essentially in the space of all functions. We're training these huge models that are very rich, that are maybe even infinite width, and so really can capture. Infinite width, and so really can capture in some sense all functions with some notion of all, some notion of capture, but they're originally the space of all functions, or they're just like so over-parametrized, they're effectively kind of all functions. Definitely the model test itself doesn't provide any effective inductive bias or realization, and everything is just coming from the dynamics of the optimization algorithm. So, optimization algorithm is leading us not to just any zero-error solution, you know, it's optimizing, it's very easy to minimize the loss of its basis of all functions, just memorize. Space of all functions, just memorize, put zero everything else, but that's not the function that our dynamics will be based on. And so we really want to understand the dynamics, where will the dynamics lead us and why would that be good for generalization? So that's kind of the basic view that the only inductive bias is coming from this optimization. Is coming from this optimization. Now we want to understand what is an inductive bias and how can it lead to generalization. In one particular, so this is, I would say, maybe if we want to give it name, so this is maybe the thesis, right? And now we want to try to analyze it. So this particular analysis program. And the analysis program rests on identifying an implicit regular outer and saying that by step one, so Are we supposed to add an E at N because we're in Canada? How does this work? Well, we're going to do that, sorry, like this. Veroon, where's Varun? Yes. Like this, right? So we'll go with the program step one is identify Some implicit regularizer R and H, where H is that we're searching over the space of functions, H is the functions we're searching over, so that gradient descent or whatever optimization algorithm converges, essentially converges to the minimum. The minimizer of this regularizer is subject to the training error. I think I'm going to use Light. I'm going to use later. I'm going to use L hat or L S L Hat. The L hat of age equals zero. Okay? And we can. We can frequently do that. So, for example, the easiest case to do that is we're looking at the underdetermined linear least square problem. We know that we're going to converge to an MUPU node solution. And we've had a bunch of work in this. And in particular, one particular approach for this is to break this up into two substips. And really think about what's going on in weight space versus in function. Going on in weight space versus in function space. So we have optimizing, we're running ready to be sent over the weights. But really, what we care about is how these weights represent functions. And it might be easier to identify the employees by S here in particular. And so in particular, in here. And so, in particular, in here, if we're doing gradient descent, gradient descent kind of corresponds to the Euclidean norm. Maybe what we're doing is just minimizing the Euclidean norm in weight space. So, we have a very simple duct device here. So, again, we can break this up. To first of all say that gradient descent converges to the minimum Egypt in our inweight space, subject to LFW is equal to zero. W is equal to zero. So HW is this mapping from W to HW. And step two is then to say, okay, what does this inductive bias in wage space correspond to in function space? And this is studying the representation cost, which I think Becca and Surya talked about, and Adev also mentioned maybe a bit. And so understand RH, which is Rh, which is the minimum cheapest way in terms of w to such that h is equal to hw. Okay, and indeed something that I was, okay, so let's actually just continue, and then we'll get back and see whether this approach works or not, and whether it works or not fails. Either way, we want to identify R of H. And then we're going to say, well, once we identify R of H, our effective hypothesis graphs are really sub-level sets of R of H. Really, sub-level sets of RFH. Okay, so we want to understand the learning of the hypothesis class that is subject of sets. So, what I mean by learning in here, first of all, is this class learnable, right? Can we learn it using? Well, can we learn that using essentially inside this class, we're essentially doing here or regularize? So understand kind of this is identify this as an inductive class. And in particular, if we're saying that we're minimizing it, we're always saying inside a sub-level set, we can establish learning guarantees if we can establish uniform convergence inside the set. So one particular approach here is to establish, let's call this set HRB, okay? Uniform convergence inside HRB. So in other words, I'll just say that with the high probability over, or actually let's say forget high probability, you should try that an expectation over In expectation over our sample, you know, found the deviation between the expected and empirical losses. I can cite this, I just write it as H. So if we can control this quantity, we can establish learning. Right, okay, so now, and so And so, I think starting from about seven years ago, we started filling in a menu of these, and there was also a lot of success of just for you to not give the impression everything was failing. So, in many cases, in generic cases, we can say in fact that grading descent generally does converge to minimum completion or regularizer. So, it's definitely true in linear models, but it's true also for minimizing the logistic loss or. Logistic loss or X plus, and it's true for general homogenous models that we get because we minimize, in the sense that we minimize the Euclidean arm subject to a marginal constraint. We just minimize that, right? So this is work by Matos and from my group and others, and really, I think, mostly based on Matus's work even before we started talking about this, when they're looking at boosting, and in general, also grading descent, or Steve's descent in respect, in general. Or Stevens's spectrum, the general norms. And so we do have some general results here, but we also know that that's not always the case. So sometimes it's not always the case that we have, like, some, what we get is approximately that. So for example, when we looked, starting for looking at metrics factorization, we realized very early on that it did seem that we're getting this union nuclear norm, which is the representation cost corresponding to the Ferminius norm in the weight space. In the weight space, although now we know that's not exactly the case. But from the beginning, we knew that we're not actually minimizing the, in that case, we're not actually minimizing the Frobenius norm or the Euclidean norm in wage space in particular. It's definitely not the case when you're initializing away from zero. It wasn't the case generically. And you can see it also in my new favorite model, which is the linear diagonal networks, which we heard multiple talks about. I also mentioned yesterday. And also there, we Mentioned yesterday, and also there we can analyze exactly the implicit bias, and we know that we can also analyze exactly what would happen if we minimize the Euclidean distance to initialization. Because really here, maybe we should say, this is assuming zero initialization, maybe more accurately, say W minus W0. And this is what happens for these square bodies. We can analyze what happens there, and you get a different effective rigorizer if you do that. Effective regularizer if you do that. So we know that many models you've done with that, but still the phenomena that you get are similar to that. So you can say, okay, maybe we're not exactly minimizing this, but we can kind of things behave similarly. And I think the big surprise for me, where you completely deviate from this, was Nadab's work on deep metrics factorization. I hardly thought he called it something different, maybe. Oh, I did. Deep metrics factorization, okay? Where when Nadab, okay, so Nadab showed it for matrices, but I'll just talk about. For matrices, but I'll just talk about it in the diagonal model where we actually analyze it. That for any linear diagonal network, if you go beyond the model, and your model is just, we talked about it yesterday, but I'll write it again. So it's just your h is just, h of w of x is just linear in x, but this is given by w plus squared minus w minus squared. Squared minus W minus squared. So this is the depth 2 version. And this, when the initialization goes to 0, and this is an L1 regularizer on, let's call this beta. This is equivalent to an L1 regularization on beta, which completely agrees with L2 minimization of W plus and W minus. But when you go to higher depth, so this is depth bigger than 2, then now if I actually minimized the The bleeding norm of W, this would correspond to minimizing this bridge penalty of Q over D bridge penalty on beta. But what Nadab showed is that if you do graining descent, this actually still corresponds to minimizing the L1. And so we see a very qualitative difference here. This is not something that we could. Here, this is not something that we could say, oh, it's approximately Euclidean, and then it's approximately this is really just a different type of phenomenon. And this is still a completely homogenous model. Okay, so this is, and I should say this is for squared parameters for squared loss. And then there was also a deviation from squared loss and logistic loss, because for logistic loss, we have the general results of friendly homogenous model, including this. We are effectively minimizing, at the limit, minimizing a pleeding normal parameter space, which would correspond to this bridge penalty. So, which means that if you To this bridge penalty. So, which means that if you use the same model with logistic loss, you're effectively doing bridge penalty regularization. And with squared loss, you're effectively doing LY regularization, which is also kind of strange because that means that we really need to really think of the loss as a fundamental part of this. Then we looked at it a bit deeper, and it seems that it's much more tricky than that. And then it's true that for minimize the logistical cost, you will. When you minimize the logistical cost, you will actually converge to this bridge penalty. But the convergence would be ridiculously slow. In the sense that if you board, I can't show you the plots, but maybe you've seen it before. If now you can direct them, what you can do, what we did is we tracked in low dimensions, like three dimensions, where you can actually completely track dynamics and see what happens. We tracked how we run gradient descent in a logistic loss and tracked where, you know, how better. And tracked where, you know, how beta behaves over the course of optimization. And so we have something that looks fun. So this is a three-dimensional problem, but in three dimensions, really, what we care about is only the direction, so it's really just a two-dimensional space solution, just the direction on the sphere, and we're looking at storing a slice of that. So there's the so if this point is, say, the minimizer of the Minimizer of this bridge penalty, and this point is the minimizer of the L1 norm. Then the way that the optimization trajectory would look like, it would start somewhere here. It'd actually start exactly the mean of the data. And then it'll go here, and then go here. And actually, the path that go here would be all of these are minimizers of the The we can characterize exactly how Google, that's not so important. The point is that if you ask, you know, over here, you know, what is this, what's the, actually over here even, what's the sub-optimality? The sub-optimality here can be, you know, even in three-dimensional problems with like 10 data points, can be like something like 10 to the negative 100. And so, and over here, you know, when you really want to get close to this, Here, you know, when you really want to get close to this, it's like, I don't remember the numbers we got, they're just ridiculous numbers. And so, just a second, I'm going to take a question. And so, it's true that we have this asymptotic result, and we know asymptotically gradient 7 will in fact converge to here, but for any practical purposes, it actually behaves like L1, right? Because we're never going to optimize to 10 to the negative 100 accuracy. And so, that also, for me, I was a bit surprising that we have this contrast between what we, symptomatic results, we think, oh, symptomatic results. Symptotic results, we think, oh, symptotic results are fine, okay, fine, maybe we get rates later, rates are also important, but you know, it kind of gives us the behavior. And here, the symptotic result really does not give us the behavior. Yeah. Do you want to say that the last line you have already classified? Oh, you're already classified already from like... Oh, here. Okay, so the training error is zero from like this one or so. So in the beginning, you start the mean of the data, and then you very quickly. Of the data, and then you like very quickly, for these problems, it's very easy to get the training error at the zero because they're very underdetermined in height. And then the question is just which of these we get to. So the sub-optimality is of the logistical. Or actually the X plus on these experiments. So the X plus subtitle had to be ridiculously small. In the matrix vectorization case, we know that even asymptotically it's L1. No, it's L1 only. No, it's L1 only the metric the issue is metrics versus vector. The issue here is squared loss versus logistics. Yeah, so for squared loss, for squared loss, even asymptotically it's L1. Whereas for logistics, asymptotically it's the bridge penalty. But in practice, okay, so at least there are cases where it's practicing this. I should say it depends on initialization. So if initialized is very small, then lower self-penality. But it seemed that in realistic settings, you would be much closer to. Settings, you would be much closer to L1 than to you. So that means that this approach is in general, I think still has valid study in some cases, but you really maybe can just say, okay, it's, is there a red marker there? Right, okay. So we really can take this for granted. Okay, so the algorithm approach we had is how many. How much time do I have actually? Do you want to give it up? It was 40 on the program. From starting from one now. Ah, we started five minutes late. So 20 minutes from now? Okay, 25. Okay, 25. Okay, okay, because I do want to give actually another talk. Some actual content, I don't know. I mean, I guess it's up to you. If people told me they really liked Amatosh's talk, you know, just like maybe about choose your own adventure, so if you uh I'll leave it up to you to uh okay. I'll leave it out to you to uh okay. So this approach maybe is not good. The other approach is to go back, you know, just think about what happens in function space. And it turns out that all our analysis that I'm aware of, or almost all of them, in function space, really rests on, maybe that's what I should have talked about, really rests on, so in all the cases we can actually identify device in function space corresponds to the fact that what we're doing in function space is mirror descent respective subpotential. Actually, maybe you know something. Actually, maybe you know something forget about maybe that's what I'm just gonna change the time it's kind of crazy. Okay actually I'll ask you what do people prefer hearing an impromptu unprepared that might be correct? I mean I think I'll be correct about how about studying what happens about how our approaches for identifying biblioster go to asking if some issues. Asking if some Hessian is, if some, sorry, if some metric tensor is a Hessian map or not, or hear about uniform convergence and interpolation learning uniform convergence and interpolation learning. Okay, maybe I'll finish this and then we'll talk about this thing. Okay, so the thing is that you can identify RX directly, but as I might tell you, all our approaches really are in a very special case that makes it Case that makes it easy to identify this device, which is not the case in general. And in general, we might not, you know, as I think Nadal also alluded to yesterday, not only might it not be easy to identify this, but it might be that gradient descent does not simply converge to a minimizer of some RH. In particular, you can think of in this one particular thing to note is if we say that gradient descent converges to some minimizer of some regularizer subject to zero error, then Subject to zero error, then think of linear regression kind of this means that you only, okay. So this means that what grading the SM converges to only depends on the solution space, only depends on the set of zero error solutions, but not on the data itself. So you can have different data sets that define the same space of zero solutions, thinking of linear regression, right? If you multiply equations or take a linear combination of equations, right? Take linear combination of the coefficients, right? So, and that's not necessarily, you know, that would not necessarily the case because, in particular, if you think of gradient descent on, so something, actually, think of it this way, there's something magical about gradient descent, the simplest thing, gradient descent on a least square problem. So, gradient descent on the least square problem, the trajectory definitely depends on the scaling of the equations. So, if I take a least square problem and I multiply one of the equations by 100, right, multiply Equations by 100, right? Multiply, or think of x and y, multiply both x by 100 and y by 100. It doesn't change the space of solution, but it's going to change the trajectory of gradient descent. But the point that gradient descent converges to does not depend on the scale. So it's kind of like strange. Why would this happen? Now you have, you know, I have one data set, and Liniac has a different data set, which I probably mispronounce it, sorry. Oh, the screen looks familiar. A different data set that has, you know. Data set that has some linear transformation of my data set, both of our optimization directories will be completely different, but some will both converge to the same place. So it's kind of a magical property. In general, this would not be. In fact, in general, it doesn't happen. If you're optimized using some other optimization algorithm, it might not happen. And this type of characterization suggests it does happen. So this type of, you know, having such an implicit regularizer is a very kind of strange thing and not something we should generally kind of. Not something we should generally kind of expect. Exactly this, right? Playing with the data set without changing solution space and showing that it could converge anywhere in the space. Is that like what I talked about yesterday? No, what you did and didn't talk about. One of the recent papers. Do you have a paper on that? Oh, about like how you can play with the data set. You can play with the data set without changing it. No, I think we almost know that it's not the case, I think, for certain models with a single. Yeah, yeah, no, so there's okay. So now let's be careful. Yeah, I'm saying this is, in fact, I'm saying this is not the case. I'm saying there are definitely explicit examples where you can prove that there is no regularizer where we can characterize grading descent as minimizing that regularizer. I should say, though, that here I still feel. But here I still feel, okay. So now, so I tell you this is a programme, now I'm telling you what doesn't work, now I'm going to tell you what maybe I still think maybe works, right? So even though it's true that there's no regularizer that exactly captures it, in many of these cases, you can still capture, there's still an implicit regularizer that captures the behavior. In the sense that you don't need it to, so in order to, for this program to work, so we can relax this program. Okay, so let's actually. Relax this program. Okay, so that's actually, it's a good maybe point to talk about that. In order for this program to work, you don't need gradient descent to exactly minimize the regularizer. It's enough for gradient descent to approximately minimize the regularizer. In fact, even if you approximate the regularizer and you're up to a factor of two, and I'm not talking about like even approximately one plus epsilon, approximately up to a factor of two, and you're relying on standard kind of human convergence analysis, having a regularizer which is a factor of two off is perfectly fine. Right? So if you think of like a So, if you think of like your SVM or something like that, instead of minimizing the norm, you only find a two approximation to the norm. Then, in a hard margin SPM problem, all our theory basically is almost unchecked. You get some constant factors that get swallowed up in even bigger constant factors that you have. And so, you can talk about a relaxation of this program where instead you talk about approximately minimizing some regulator, or you don't even need to approximately minimize. Or you don't even need to approximately mit them and take it and and that I think still we're still not sure of how far you can push that. Like how far, I think all the negative results say there's no exact regular, there's no regular as an exact redestruction. But in all those cases that there are negative results, it's pretty easy to have an approximate one because they're, you know, I'm not saying that's generally the case, but it's just because they were designed not to have an exact one, right? So they didn't weren't designed to preclude an approximate one. But it does seem a bit harder to preclude to Harder to procruce to please in terms of analysis to prove the approximate ones. And I'm still, from my perspective, still wondering in realistic systems how far you can get by still talking about approximately some regularizer. And in fact, I'll go one step further, and this is something I wanted to, an example, I'm not trying to go through details, but this is recent work with Louis Ligny and Okay, so this is recent work with Idan Emil and Oiliffny. What? If I'm a devil's advocate for a second, once it's not exact and there's no unique minimizer for R, then I can just say, okay, R is constant. No, no, no. It's a constant. Okay, you're wrong. No, no, no, okay. So, of course, it's always, in fact, then it'll be the minimizer, right? So the relax program. The relaxed program is to say that if you tell me that gradient descent approximately minimizes some R, you identify some of Rh, and then suppose you could prove that gradient descent always converges to a factor of two approximation of Rh. So in fact, I can give you such an example. So this is something that Surya is writing right now. It's already written, but finally it's written two years ago. Finally, it's written two years ago, but finally going to be finished writing. If you do gradient descent on a least square problem, and you look at the, let's do gradient flow, right? It might be easier to talk about. So you do gradient flow in the least square problem, so you have W t. And you can compare this to this is to W lambda, which is the argument, the realization factor, the argument of The argument of the loss. The loss here is just the squared loss in the linear model plus lambda times WT. So Wt and W lambda are different. So Wt is not actually a minimizer of this subjective. However, it is up to a factor of 2 or less than 2. What's the number? 1.68, right? Okay. And in this case, State, right? Okay. And in this case, that's perfectly fine to get generalization guarantees. So this is an example of which this is happening. You can actually establish that the optimization graph does give you a factor of approximation to an explicit regulation. Going back to here, if you're, you know, this is without early stopping, we should talk about early stopping in a second, but if you minimize the regularizer up to a factor of two, then now you have to look at this set with, you know, maybe with a factor of two over r, which usually does. With a factor of 2 over r, which usually doesn't make any difference, and you maybe get some slight increase in the central flux. But the key is that you still need to have an identifier regularizer so that it's minimizing it approximately would be sufficient for the infinite convergence to work. And that wouldn't work with a constant, right? Because the constant, this set, you know, would be the entire space and wouldn't work. Does that make sense? Okay. And so this is the kind of relaxed program. We still don't know how much that is sufficient. Okay. Sufficiently, I think. Okay, where was I? Okay, so this is about this relaxation. In fact, you can take it one step further, and this is this work with Idana Mil and Wa Livni. And what we analyzed is gradient, what we looked at is again gradient descent on the generalized linear model. Okay, so going, this is a very, this is a disparate problem, and let's look at this. these four problems. Let let's look at the um uh L W being expectation over C of some convex scalar function of W phi of Z power s. Okay, so this captures in particular supervised learning with it with any convex loss and I'm taking let's take G to be sorry the concrete column to be convex and what. So, you have a complete problem to be convex in one dimension. And so, in this case, gradient, now we want to, so in this case, just to give you some background, so this problem is taking phi of z, so let's say L is one Lipschitz, and I want to compete with, just as a learning problem, I want to compete with W's, which are bound in norm, and so I need to put. Bounded norm, and so I need to bound everything else here, and could work. So, this is a relatively easy problem. I can solve it in a variety of ways. I can solve it by regular, by this W lambda with a specific trace of lambda. I can solve it with SPD. In all these, I get semi-complexity of one over epsilon squared. We know that very well. Can we solve it now? If we take, go a step beyond generalized linear models and talk about a general convex objective, then we get all kinds of gaps. Then we get all kinds of gaps because we know that in that case, empirical risk minimization doesn't work, uniform convergence doesn't work. And there is recent work also showing that for gradient descent, so if we take gradient descent, batch gradient descent, so stochastic gradient descent still gets you always the optimal rate of one of epsilon squared, but if instead we do gradient descent on the objective and we rely on early stopping, we can get consistency here, but with a sub-optimal for the general problem. For the general problem, for general index, we can get a suboptimal, we get a suboptimal rate of one over representative force. And the question is, can we, if this, but that case is kind of, there's a big difference here between the generalized linear and the general convex, because for general convex, we do not have uniform convergence over these balls, so this last part of the program doesn't hold. Whereas for general linear, we do have uniform convergence. From convergence. And ERM does work. In this case, in those cases, can they just rely on gradient descent from physicalization? So again, my algorithm here is gradient descent without projections. So grading descent with projections would definitely work, because gradient descent with projecting to this bone would give me ERM inside that bone, and I could do uniform conversions there. But the whole question is to look at the implicit bias of gradients. So my algorithm is just grading sent on L hat, gradient scent on the batch training error without projections. And you get to choose the stopping time and the Stopping time and the steps. So, in that case, we do not track the realization path, not even up to any constant factor. So, we cannot kind of do it by saying that we actually are implicitly minimizing it. But we can still prove, get statistical guarantees to it using uniform convergence. The way we do this, so this is I'm not going to go into the details, I encourage you to look at the paper. And in order to do that, we have to think a bit more creatively about uniform convergence. Think a bit more creatively about uniform convergence. And this is part of the theme of one of my talk today, and hopefully, we get to the other example. If not, is if we do uniform convergence over this ball that I, so we have, so this is our origin, and I want to compete with this set, right? So I can try to do uniform convergence over balls around the origin. And then I have to compare two things. So grading descent on one hand, I have to let gradient descent work with for enough durations. Ratings and work with for enough iterations and large enough step size to actually be able to optimize the training. On the other hand, if it does that, the more I let it bigger step size and more training iterations, its norm might be bigger. So I have to choose kind of the step size and the number of iterations, and then look at what ball can I ensure grading sign will be in, and then I can do grading U for convergence by that ball. So I can do that. And if I do that and balance those things, I can do it, but then I can. I can do it, but then I can only get the sub-optimal rate of 1 over epsilon to the fourth. Instead, in order to get the optimal rate of 1 over epsilon squared, what we have to do is not uniform convergence around the origin, but we do uniform convergence around the result of gradient descent on the population. So this point is gradient descent on the population objective. I don't know this point. The algorithm doesn't know this point. Don't know this point. But what we can analyze then is after T iterations of gradient descent with step size eta, okay, so this is a gradient descent of population with t iteration step size eta, and gradient descent on the training objective after t-iteration step size eta, we can bound how big a ball around here it will be. And now again, we have to play this game of we have to make sure that we actually optimize the training error, because we are going to re-line convergence, so we want to get Line info convergence. So we want to get L hat small. You have to assess how well we can optimize the training error and how big a ball will be around this point. And again, we can balance these things and get the optimal rate of 1 over epsilon squared. And so what we're doing here is what Zeeko culture and you don't realize they're really bad with names, so you're going to have to help me here. That's about having slides. Zico student work on deliberating attendant unit projects. What? Yeah, exactly. Yeah, exactly in Agralian. So algorithm-dependent uniform convergence. So the set, and really it should be called the distribution-dependent uniform convergence. The set over which I'm taking uniform convergence is not fixed a priori. It depends on the population distribution, which seems strange because this is not information available to the algorithm. I mean, if I knew the source distribution, the algorithm would be trivial, just output optimum. So the algorithm doesn't know this. But in terms of analysis, it's completely kosher. In terms of analysis, it's completely kosher to define, to look at uniform convergence around the set because it's deterministic. It's just determined by the distribution, not by the actual set. And that gives you a lot more power. So in this case, in particular, we can prove that any non-algorithm infinite unit from convergence, and again, I'm using this term algorithm infinite uniform convergence, really, you should think distribution-dependent unit from convergence, we cannot give you anything better than 1 over epsilon to the fourth. And if you allow this distribution dependence, you can This distribution dependence, you can get the opponent rate of one of epsilon squared. And so, this is, I want to mention this example both because I think it's a cool paper, but also it shows that really with looking at this program, you have to be much more flexible. When we talk about uniform convergence, we can actually get quite a bit of uniform convergence if you're willing to be a bit more flexible. And also, if you talk about the regularizer, what we saw here is we can prove that we're really not minimizing, not approximately minimizing this object. We're not doing, we cannot see. We're not doing, we cannot say that we're approximating a constant factor, but we can still identify an effective regularizer that controls the statistical properties. Our statistical properties are still given. We can still understand the statistical properties in terms of some specific regularizer. I get the same, like, in the sense that if I'm relying solely on implicit regularization, I'm picking on Surya this time, is relying on explicit regularization with this regularizer, we would both have the same statistical properties. And that's something that I, you know, Properties. And that's something that I, you know, again, I think is satisfying, and it's not going to be always the case, right? It might be that really there are situations, and I don't know right now of any such, but there could be that really you can do that there is no such a result. I asked a high-level question, which is like, what do you think because it seems like the problem here is that there's it's hard to identify the if there's a regular razor, and sometimes there isn't, but for practical purposes, is it possible to This is, is it possible to, like, even if we cannot identify the implicit regular master, just use an explicit one and you get the same result? Like, is it important for optimization algorithm to kind of really identify what this is? Or you can't really override it. So, this is kind of what we're asking here. Is it the case that there's an explicit? So, another way of asking this question about saying that there's an implicit regularizer that determines statistical properties is basically asking your question: Is there an explicit regularizer that if I use That if I use that one instead of realizing an empty syrupization, I'd essentially get the same results. Right, like I'm looking at the subgradient of the L1, like a change the gradient of the I. Right, right, exactly. So this is exactly what we're asking here, but we're asking it from a theoretical perspective rather than empirical, rather than looking at a specific problem, a specific source distribution, what happens there, which is all I can ask empirically, basically, trying to understand it. Trying to understand it from an analytic perspective. Oh, I see sort of like forming equivalence between the third component, the last component I wanted to really dwell on a bit is this uniform convergence. So we already started talking about, in this example, both about how things here break down, but also about uniform convergence. The other, what I have, five minutes? Another question about negative equivalent. Okay, but that's fine out. That doesn't talk to you. So when you say making it equivalent, whether you do implicit regularization or do the explicit regularization to this free, it's in terms of like what you want to do, right? So it's not like... So one thing I understand trouble with is like when you say that minimizing that is simpler, like minimizing without that. No, said that. Wait, but oh no, under. Okay, no, so it is true that under. Okay, no, so it is true that under certain assumptions. Okay, so something else that we didn't talk about here, which is super important, is that the nudge of this analysis, things are dependent on the marginal distribution over x, I would say. Or maybe we do need to make what? Writing an x. No, just an x. The marginal over x. So we do have to depend in the distribution for many cases. And in particular, for metrics vectorization, you can say that under some assumption over x, Can say that under some assumption over x, indeed minimizing the open norm and losing rank are similar. But it's definitely not the case. I wouldn't say that nuclear norm and randomization are statistically equivalent. They have different statistical properties. Okay, but even in this case, you think like if there is an actual problem where we run like the two versions, we would get like the same kind of results because you are essentially proving upper bounds. And then you're not. Upper bounds, and they do not essentially translate to qualitative. I mean, there might be some qualitative differences when you actually apply them in the middle. I see. So, we are doing a bit more than just upper bounds. Because we actually, in this, you know, this is a relatively simple example, and you can actually work out the minimum complexity in terms of lower bounds. And you can still argue they're not instance-dependent, so there's still. They're not instance dependent, so there's still some subtleties there, but like it's a bit more than just uppercuts. But you're right. Okay, the last example that I want to just give okay, I'll just advertise this. I'm not going to have time to go through this, is for, again, for looking at uniform convergence. And this was sparked, so this is looking at interpolation learning. Thankfully, Ohad gave a really nice talk yesterday about interpolation learning, so I don't have to. So, I don't have to justify it, but just say that I think this is really a big mystery that doesn't fit with our preceding knowledge. That we can learn by just, we can actually frequently get good guarantees, even in noisy cases, by minimizing, by insisting on zero-error solution. And this is not something we now do. And this is inherently tied to this program, because note that the first step of this program is driving the error to zero. In noisy settings, that seems wrong, but maybe it's not. I'm going to try to understand it. Wrong, but maybe it's not. I want to try to understand it. And then I think one of Misha's most convincing plots to me was this plot. And this plot, Misha added label noise to, I don't remember what data set he used here, it doesn't really matter, added label noise and then used different training methods. And what did the so the green line is like the ideal thing you can get, and of course you get a bit above the green line because you do have some estimation error. But the crazy thing is you're always getting better than you're getting at least weekly. Better than you're getting at least weak learning. You're always getting better than null, even if the label price is really high. And this is kind of strange. So your generalization gap is still kind of magically small enough to guarantee that you're not going over the slide. You're never like crazily overfitting. And going back to the discussion I think I have with some of you, this is the belly I was mentioning, right? So this gap does become larger here, but somehow it's magically always beneath this side. And this was. This slide. And this was one of the reasons that prompted Misha, or is it, to say that it's impossible to explain this with universe from convergence. Because if we try to explain this with unium from convergence, you'd get the uniform convergence like I'm getting here. This is the bound between the training error and the test error. It'll have to magically be, this is not going to be this gap, right? The training error is zero. So you'd have to say that this gap is magically exactly between the green line and this line, between all points. Green line than this line, between 0.7 and 0.9 in this case, which seems like super tight. Like, it just, I think, Misha's quote is: there are no balance like this, and there's no reason they should exist. So even if you're off by a factor of two, that invalidates the duck code. And so when somebody tells you something is not possible, I think somebody already mentioned this yesterday or one of the days, right? We tried to do this, and we did manage showing this with uniform convergence. And I'm not going to time. And I'm not going to have time to go through this. I should say this is work with Freddie Collar, Lija Joux, and Danica Sutherland. And you have to be creative here in two ways. One way is we have to think about what we talk about uniform convergence. So we have to, let's skip this. When we talk about uniform convergence, we're going to talk instead of a uniform convergence, what we've been expressing around here. What we expression wrote here, the supreme over all predictors in the class on the sublevel sets of the deviation between the interpretive expectation, we really only care about interpolators, of the predictor with zero training error. And we know that if the training error is small, then the gap is also going to be much smaller. I mean, this just boils down to the difference between Brinstein and Hofning, if you want to think of it that way, or the fact that the variance of a coin depends on the probability of the coin. And this is something we understood ages ago. We know that for a realizable, we get rates of florality. For realizable, we get rates of, or think of it as a realizable, we get rates of one over m and then agnostic one over squared of m. It's all like different hashes of the same phenomena. And so this is really still definitely a different convergence guarantee. And in particular, the type of one way to get this is with these optimistic rates that depend on the right-hand side, also in the empirical rate. And so using some of such rates, and this is a rate that we had a We had a bound like this for smooth losses from work with Karthik and Ambush from several years ago. And in principle, it could explain inform convergence, actually. It could explain interpolation learning. The only problem is that in order to explain interpolation learning, we need to get this O tilde to be just one. And what we actually could prove at the time, I'm sorry I'm not going over all the details in this slide, was a constant of this two hundred thousand times log to the third n. Thousand times log to the third n. And as we said, like as Misha said, the bound is to be tight, exactly, one. I need to get one, not like two, not seven, and definitely not 200,000, and definitely not log to the third n. And so together with Ferdinand, Lisa, and Danika, we actually managed proving it. So this is a generic bound in terms of the Rademacher complexity, so generally bounded deviation in terms of Rademacher complexity. And here, Complexity. And here, so this is now uniform over all w. So this is for all w, not only interpolators, it's just sensitive to the law. So you can, I think, definitely, I would definitely prefer this as uniform convergence guarantee. But we're bounding the empirical, the expected error in terms of the square root of the empirical error plus the radio x-square. Another way maybe to writing it will even look more like a uniform convergence guarantee is that we're looking at. Is that we're looking at the squared mean squared. This is specifically to the squared loss, we're bounding the root mean squared error. We get a uniform guarantee on the root mean squared error. So this is grounded by the random complexity of the class. Okay, but this makes a huge difference because now, of course, for an interpolation setting, Now, of course, for an interpolation setting, this thing is zero, right? The empirical error is zero, and so we do get this red marker complexity squared, which is what we wanted, with a constant of one, or one plus a little of one, but that's fine. The leading term is still one. And using this, we can actually analyze, this is sufficient for analyzing, you know, for convergence in particular, sorry, for analyzing interpolation learning in particular. And I'm not going over all the details, I think Alhadar also covered many of these in his talk. Many of these in his talk, the benign overfitting conditions that Bartlett et al. recovered, and even their strengthening, but Sigmund Bartlett, that give conditions that basically these conditions say that the signal part is low enough capacity, and then there's a high enough effective dimensionality from the junk part. This is kind of what these conditions say. We can recover the same conditions using uniform convergence. And so, you know, I'm trying to kind of defend a bit like last point here. I still think you can get a lot of biological uniform. Still, I think you can get a lot of mileage from uniform convergence, but again, you have to be flexible. Maybe do it distribution-dependent, maybe do it in a way that's attuned to the empirical error, maybe do it in a... So instead, the main caveat here is that this analysis is only valid for Gaussian data. So it may be depend on the data. And what we're in particular... Let's say I have it here. Yeah, so this does assume that I think it's stated here also in the signal, but we can now extend. So in the signal, but we can now extend it to, I just need the x's to be Gaussian, and the labels can be anything. The labels don't have to, you know, can be arbitrary. I just need the marginal distribution over x to be Gaussian, an arbitrary covariance metrics. And we're now trying to extend this beyond Gaussian to, we believe we can extend it to also generic sub-Gaussians with like some another like little O-term here that would account for the effective. For kind of the effective gallon extra parameter of sub-gaussianity. But we do have to be a bit more flexible in terms of getting, at least relying a bit on this source distribution to be able to get these tight concepts that we need. Okay, yeah. So the thing is that you don't want to rely, though, in the random upper complexity. So if I replace the random upper complexity, the dependent expected square norm. If I replace the dependence of expected square norm or the dependence of The dependence of expected squared or independence of the parameter of sub-Gaussianity, I could do this. But that's not going to be good enough. Because that's going to be too loose. For a Gaussian, the parameter of sub-Gaussianity and the expected square norm are the same. But for the general sub-Gaussian, it's not going to be the same. So I do want to keep the expected square norm inside here and depend on the parameter of sub-causality only in this little. Does that kind of make sense? Maybe you do have a better way of doing it. I don't know. I mean, this is at least. Okay, yeah, you should look at the paper. I completely believe that it's possible to do it. It's just that we so far haven't been able to do this. So, I mean, it's perfectly possible that you see how to do this. No, I'm just confused because it seems like all the things that I'm refining the game automatically are parts of the function.