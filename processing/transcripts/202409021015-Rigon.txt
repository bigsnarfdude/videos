I'm there with you, with my part, so sorry for this, but let's get started with the presentation. So, today I'm going to talk about work that I'm done with two of my collaborators here in Bicoca, who are Lorenzo Gilotti, who is a PhD student of the University of Milano Bicoca, and Federico Camerlenghi, who is an associate professor here. An associate professor here. So, the talk is about feature allocation models, and it's about a general class of feature allocation models that is closely related to Gibbs type species sampling models. In a sense, it's the direct equivalent of for feature models of Gibbs-type species sampling models. So, instead of rushing into the literature of feature models, I decided that since I have some time, it Since I have some time, it would be a good idea to refresh the species sampling framework first and Gibbs type species sampling framework. And the emphasis of the talk will be on ecological and biodiversity problems. So in a sense, I'm not going to use those priors and methods as a latent tool, at least in this talk, but as essentially, I'm going to assume that I actually observe, for example, Observed, for example, x1, xn, which are which is a collection of species or taxa. And well, as you probably all know, a typical way of modeling this kind of data is through species sampling models. And the species sampling models is nothing but a discrete random probability distribution. So you have possibly infinitely many locations that Zh, those Zh are the labels of the different species, and then you have a The different species, and then you have a set of random probabilities, and those random probabilities represent the prior uncertainty about the species proportions. So, what you typically want to do with this kind of model is to quantify the uncertainty in some way. And the fact that this distribution phi tilde is discrete means that you will re-observe the same species multiple times and getting frequencies for those. Getting frequencies for those n1 and k, so those frequencies here, that are called abundances in ecology. Okay, and you observe k of them in a sample of n observations. Okay, so that's a very simple, simple setup. In this setup, what you want to do then is to measure biodiversity, and there are several ways of doing so. There is no unique way of measuring biodiversity. The sample coverage, for example, is one of the first. Sample coverage, for example, is one of the first proposals, which is the sum of the proportion of the species that has been observed. So this is good 1953. Another way of measuring that, which is like a graphical way of measuring biodiversity, is through the usage of extrapolation curves and the ratifaction curves. So this goes under the name of accumulation curves. So essentially, for each sample, you count how many species are distinct. Species are distinct. So those are increasing and concave, sorry, convex functions. I will show you a couple of examples later on. And the extrapolation of an accumulation curves is essentially a prediction. It's the prediction of the number of unseen species that have not been observed in the sample. And even for that, there are some superhistorically nice references back to wooden tulmin. And then there are other indices. And then there are other indices like the Simpson index, the Shannon entropy, and so on and so forth. So, one key point that I want to make that holds both for feature allocation models and species sampling models is that this notion of alpha diversity developed by Pittman is actually a very natural notion of biodiversity and, in a sense, captures and encompasses all the previous notions of biodiversity. In a sense, once you Biodiversities. In a sense, once you go into the vision of parametric modeling, everything is interconnected and everything boils down to the estimation and the quantification of a single number, the alpha diversity. Let us see in what sense this is actually true. So, but wow, first of all, in order to get inference, you will need to specify a prior for this pi tilde. This pi tilde is discrete. This pi tilde is discrete. And as you probably know, this fact that it's discrete induces a random partition among the observations. And the distribution of this random partition is this function pi n, which is a function of the n1 and nk. This is not always the case, but it is always the case. It's the function describing the partition, which is only a function of the frequencies, of the abundances, and goes under the name of exchangeable partition probability function. Of exchangeable partition probability function. We're not going to consider all the possible species sampling models. We're going to focus on a specific subclass of species sampling models, which are Gibbs type priors. And Gibbs type priors that have been developed by Gnendin and Pittman. And then there is this nice review paper by the Blasi and co-authors that review all the results available up to 2015 about Gibson priors. Essentially, the characterization of Gibson prior is that they have. Of deep side prior is that you have an exchangeable partition probability function that can be expressed as a product form. Okay, so you have this VNK, which is a coefficient characterizing many properties of the specific choice of the Gibbs type prior. And then you have this product form that depends on a single parameter, alpha, the alpha parameter, which is going to control a lot of properties of this class. Once you have the exchangeable partition probability function, you also. Partition probability function, you also have the predictive distribution. So, for example, which is the probability, the behavior of the next species given that you observed all the previous one. And so, for example, in particular, you have that this probability here controls the, is the probability of discovering a new species. Okay, very simple. So, this is something which is, of course, a special case of this is the Dirichlet process, the Piedman Yar. Lab process, the peat manual. I'm sure that you know all this stuff, so I'm not going to enter too much into the details of this. What it's probably worth remembering is that the Dirichlet multinomial, the Dirichlet process are special cases of Gibbs priors. And in particular, you just need to choose the appropriate coefficients. So for the Dirichlet multinomial, it's just a finite-dimensional Dirichlet. A finite-dimensional Dirichlet distribution. That's another way of seeing that. So, if you choose this specific coefficient, you get the Dirichlet multinomial species sampling model. And it's important to see that alpha in this case is negative. And you have this H number here that represents the total number of species that you get in a population. So, the total number of species is upper bounded in this model. Then you have alpha is equal to zero, which is the derivative. Have alpha is equal to zero, which is the an example being the Dirichlet process, and which is the coefficient is this one. So it's the usual V and K that you get for the Dirichlet process. And then you have the case, probably the more most theoretically, the most interesting, not necessarily from an applied sense, but from a theoretical point of view, this is an interesting quantity. So you have the case in which alpha is in between zero and one. And I'm showing you here an instance of An instance of Gibbs type prior, which sounds super complicated, but stay with me, it's not going to be super complicated once you do some extra step. But essentially, this is the VNK associated with an alpha-stable Poisson Kingma process. Okay, so you have this complicated integral form, and the integral is with respect to a difficult density, which is this density that you have here. Density that we have here. So, why am I showing you these three densities? Well, the reason is that all Gibbs side trials are actually a mixture of these free building blocks. So, everything can be obtained by mixing the main parameters of those free distributions. So, when alpha is smaller than zero, all the coefficients are mixture with respect to the Dirichlet multinomial weights. Dirichlet multinomial weights. When alpha is equal to zero, all the Gibbside priors are mixture with respect to the beta parameter of a Dirichlet process. And when alpha is between zero and one, everything is a mixture of the alpha stable post-sunking mammal. Okay? So the Dirichlet multinomial and the Dirichlet process and the alpha stable are essentially the foundation of Gibbside prior. And in a sense, that means that any Gibbs prior can be represented in a hierarchical manner. So you essentially have those. So, you essentially have those three models, and then you put a prior on the key parameter, which are H. Sorry, this is a typo, it's beta, and then gamma. Okay, so you put a prior on those three key parameters, and as a result, marginally, you get a Gibbs prior. Okay, so that sounds like a nice theoretical story. Then, what's the biodiversity implication of this? Well, the biodiversity implication is that those three parameters are. Is that those three parameters are actually coincide with the so-called Pittman alpha diversity? I'm going to show you that in a second. Stay with me at the moment. So what can we do with this class of model? So before getting into the alpha diversity, let us try to measure and quantify the simplest form of biodiversity, which is the taxon richness. So the richness is how many species do we actually get? How many species do we actually get in the sample? A priori, the distribution of kn of any Gibbs site prior follows this distribution here. So you have essentially the PDF is a combination of the weights V and K, this generalized factorial coefficient, and the alpha K. And if you let alpha go into zero, you get as a special case, for example, the Steeling number of the signless Steeling number of the The signless Tealing number of the third kind, which are the ones that you also get in the Dirichlet process as known by Since Antoniac, let's say. The expectation of those distributions are, I think that's interesting from an applied perspective. So ecologists call those expectations ratifaction curve. Essentially, it's the average of all the possible of the number of distinct values that you get. Okay? But perhaps more interestingly is that. Is that the fact that we can actually get the posterior distribution of the species? So we can predict the number of new species in a future sample. And the distribution is available. It's not something that we have done. So it's something that has been done by Lioi, Egon Klunster, and Ramses Mena in a biometrica paper. And the posterior distribution is this one. So it's available in closed form. And in many cases, you can actually get the posterior expectation for Expectation for this extra extrapolation of the accumulation curve. So you actually get an estimator. Okay, so it's very well known that if you keep extrapolating those values when alpha is greater than zero or greater or equal than zero, you actually diverge. So think about the Dirichlet process. When alpha is equal to zero, the extrapolation curve diverges logarithmically, but it diverges. So it might be tempting to use It might be tempting to use this extrapolation as a measure of biodiversity, but when the model is divergent, it's not a good measure, let's say. You want something which is, you know, a finite number. So the idea here is to rescale the accumulation curve by a suitable constant. So in the Dirichlet multinomial, you do not rescale. A multinomial, you do not rescale anything. So you consider the accumulation curve as is, and as n goes to infinity, it converges to h, to the total number of species that you have in the population. When alpha is equal to zero, you rescale kn by log n, and this converges almost surely to the beta parameter of the Dirichlet process. When alpha is in between zero and one, When alpha is in between 0 and 1, the weight and V and K are the weights of the alpha stable Poisson Kilman, then by rescaling with a polynomial factor, you converges to the gamma parameter of the alpha stable Poisson Kilman. So in those three cases, the Kn rescaled, possibly rescaled, converges to a deterministic quantity. And this deterministic quantity is indeed the alpha degree. Quantity is indeed the alpha diversity. So H is the alpha diversity of the negative regime, beta is the alpha diversity of the logarithmic regime, and so on and so forth. So in a sense, by considering the entire class of Gibbs by prior, we are putting a prior on the alpha diversity, and then it makes sense to consider the posterior of the alpha diversity. And that can be done in many cases that this case. Many cases that this can actually be done in closed form. And for example, people have done that. So, well, so this is a recap. So the alpha diversity can be seen as a rescaled richness measure. The alpha diversity is deterministic in those three cases, but once you consider the entire class of Gibbs priors, you're learning it. You're learning the biodiversity. And the posterior blow of the alpha. And the posterior blow of the alpha diversity is indeed what I think it is at least one of the main quantities of interest if the goal is to quantify the biodiversity in a sample. There is also this nice connection from another paper with Chim Lung and David Danson. So essentially, it says that the posterior distribution of the alpha diversity coincides with the extrapolation. With the extrapolation of the accumulation curve. So, if instead of compare that to the prior result by Pittman, the prior result by Pittman says that Kn divided by this constant here converges to the prior that they're choosing. So, this F of alpha, which is the alpha diversity. Instead, this is just a slight change. So, in essentially, the posterior, so the extrapolation. Extrapolation of the accumulation curve converges to a random variable, and that random variable is indeed the posterior distribution of the key parameter of your Gibbs type prior. Okay, so this is the setup for species sampling models, and hopefully, this is somewhat clear. Most of this is already known, so I think that most of those results are well explained, well. Are well explained, well, and actually, they're explained much better than what they can do in the paper by Leo, Igor Prunster, and Francis Mena in 2007. But the reason why I decided to present you all this intro is because now, okay, so, okay, sorry, detour, final slide on speech assembly models. I forgot about this slide. So, there are actually choices for the prior distributions for the alpha diversity when H. Alpha diversity when h in the alpha smaller than zero case, there are choices in this paper by Meddin. So there is a nice prior that for H that leads to closed form expressions. When in the alpha equal to zero case, so the Dirichlet process case, a classical choice is the gamma prior from Escobar West. And we recently proposed by with Alexito, this still in gamma prior, which is the conjugate one. Which is the conjugate one. And it turns out that the initial process is an exponential family, so you can actually choose the diaconistically Zachary conjugate prior for it. So, and here we are. In the alpha, in between zero and one, you can choose a polynomial twiled stable distribution, which sounds a fancy choice, but it's actually the choice behind the pit-major. So, the pit-major is implicitly choosing a prior for the alpha diversity, and in the paper, in another paper, And in the paper, in another paper by Lioi and Cautors on JRSSP, they actually have the posterior of it. It's kind of the Pitmaior is not usually defined through this hierarchical construction, but you can actually see the Pitmaior as the marginal distribution of hierarchical construction in which you have an alpha-stable post-sonking model, and then you have this fancy prior for the alpha diversity, which is this polymer. Alpha diversity, which is this polynomially tilted stable distribution. And if you do all the calculations, marginally you get the Pittman UR. Another choice is the exponentially tilted stable density, which leads to the normalized generalized gamma process. And if you want to convince you that everything can actually be done analytically, when alpha is equal to one half, the alpha stable Poisson Kingman model has a nice set of coefficients. Of coefficients, which are much nicer than the one that I showed you before, which are those. So, when alpha is equal to one alpha, this crazy formula simplifies a lot. And some magic happened because this is the density of a stable distribution. Then, when alph is equal to one alpha, it magically simplifies. And so, this is what you get. And this is something that I call Aldo Smittman process because it has been this. Process because it has been described for the first time in a paper by Aldus and Pittman in the Annals of Probability, I think, 1997. So, quite a long time ago. But for some reason, it's not something that you see a lot around. So, I like to advertise it because it's a nice prior that is not well known, I guess. And that's the end of the story for species entry models. So, now let's move on with feature allocation models. Models and most of the what I'm going to show you is going to be an extension, or if you want, there will be strong parallelism with what I've just told you about. So what's the setup? So the setup is slightly different. So instead of having species that we observe one at a time, here we observe, suppose we are again in the ecological setting, so one observation is a vector of binary. Vector of binary features, and a feature in my context at least is a species. So, in a each observation is a sampling size, so a geographical location, for example. And in a specific geographical location, you observe, you record the presence or the absence of a given species. So, this is a graphical description of the data that we have. In ecology, those are called incidence data. In ecology, those are called incidence data, as opposed to the traditional sampling mechanism of species sampling models, in which observe x1, xn one at a time. And a feature allocation is fn describes the pattern of features in a sample of size n. A random feature allocation model is just a random distribution of it. And so a nice, I'm going to focus on a nice subclass. Focus on a nice subclass of feature random feature allocation models, which are those admitting an exchangeable feature probability function, which is something that has been defined as far as I know by Tamara in a Bayesian analysis paper. So essentially, we're going to restrict our attention in the case in which the distribution of the feature allocation model can be expressed through a function that only depends on the frequencies of the features. Okay, so we're not considering all of them, just a subclass. But this subclass is highly tractable and it's going to be quite interesting. So this is a way of defining that. We also need to keep it to account the fact that we need to ensure some Kolmogorov consistency conditions. So it's not enough to come up with a symmetric function of the frequencies to have a distribution, a valid distribution. A valid distribution, so we are going to need some constraints, but let us keep that aside for the moment. Another way of defining feature models is in a sequential manner. So essentially, we sample the number of features for the first individual, then we, at the genetic step, we either re-sample or re-observe the previous features. Sample re-observe the previous features, the previous species, and also we sample a certain amount of new unseen features. This is a graphical depiction. So, for example, in the first case, the first observation we get cheese, carrot, and something. At the second observation, we re-observe the cheese with a Observe the cheese with a certain probability, we do not observe some previously observed dishes, but we do observe something new and so on and so forth. And the reason why I'm showing you dishes is because, well, there is this Indian buffet metaphor that kind of serves to as a metaphor to describe the sampling process. But the Indian Buffett metaphor is just another way of saying that where the Way of saying that we're dealing with binary matrices. Okay. Okay, so popular examples. So there are two popular examples. One, the very, the super, the simplest example is the beta-Bernoulli model. So we assume that there is a fixed amount of species and all or features, if you want, and capital N. Alpha is a parameter and it's negative, and theta is another parameter which must be greater than minus alpha. Which must be greater than minus alpha, it's easier to describe this model in a sequential manner. So, at the generic step, we observe one of the previous features with a probability that is proportional to the frequency, or we observe a new feature, y n plus one, an amount of new features, according to a binomial distribution. Okay. Okay, and the probability of observing that species is the one that is depicted here. So, this is a toy example, it's a toy model, which is kind of obvious. So, essentially, you choose H, N capital N features, and to each feature, you assign a beta distribution to the probability of observing it. The other famous example is the three-parameter in. Example is the three-parameter Indian buffet process. The original process was the one by Griffith and Garmani. And then later, Thibault and Jordan discovered that there is the beta process underlying that. But this is the three parameter generalization by Te and Gaurur. And essentially, you have three parameters: gamma, which is a positive parameter, alpha, which is in between zero and one. So I'm 0 and 1. So I'm here is positive, here is negative. So keep this in mind. The probability of observing all features is essentially the same as the beta-Bernoulli model, with the only distinction that alpha here is a negative parameter, so we are positively reinforcing the frequencies. Here is a positive parameter, so we are negatively reinforcing the frequencies. The frequencies. And the number of new features is a Poisson distribution. So now we are in an entirely radically different scenario. So the number of features is not bounded a priori. So as n grows to infinity, the number of features grows as well. Instead, in the binomial case, eventually we Eventually, we will reach the upper bound, which is capital N. So, those are different regimes. So, now you may be wondering, what's the AFPF of those sequential mechanisms? Well, it turns out that they both belong to a general class of product-form feature allocation models that can be written in this way. There is a nice paper in the Annals of Appropriate Probability by Batty Stone and Co. Probability by Batistone and Quarters that proved that if an FPF can be written in this form, then it must be written in this special form. It's kind of the equivalent of a result for species sampling models by Gneddin and Pittman. So, in a sense, you have once again those Foucault factorial symbols and this V and K. And this V and K coefficient on top of it. And on top of that, and that's a super strong characterization result. So Mattiston and quotors prove that for a fixed set of parameters alpha and theta, all product form feature allocation model must be mixed with respect to gamma with respect to the gamma parameter of an IDP. of an IDP. When alpha is negative, all Gibbs type feature allocation models are mixed with respect, sorry, too much, mixture with respect to n of the beta-Bernoulli model. So this is the equivalent of the result for species sampling models in the feature allocation setting. So essentially, you have two building blocks, the beta-Bernoulli model. The beta-Bernoulli model and the three parameter India Buffet process. And then, if you choose a prior for gamma or a prior for n, you generate all the possible Gibbs type feature allocation models. So, this was a known result. So, this is a known result. But the paper by Battistone kind of stops here. So, what about creating, studying those models? What can we say about the general class? Can we study this general class? study this general class can we find tractable tractable examples so and the and the answer luckily is yes so what i what i i started well lorenzo started doing actually was well let's put a gamma prior for this gamma parameter let us consider a poisson prior for this n parameter and let's see what happens and it's not surprising once you see the formulas that you can See the formulas that you can actually get closed form expression for those. So, when alpha is negative, we are in the bounded case, so we're considering the beta-Bernoulli feature allocation model. The associated VNK is the one that you see here. And so, in order to generate a new Gibbs type prior, for example, you can choose a Poisson prior for this n parameter. And if you choose this Poisson prior, you end up with a new model, essentially, which With a new model, essentially, which is a mixture with respect to the beta Bernoulli, and the associated VNK is this one. And it's written in closed form, so everything will be nice in a sense. And if you choose a negative binomial distribution for this capital N parameter, then again, you end up with a tractable VMK. So here we are in the regime in which Kn converges to a finite limit. In the other regime, when alpha is between 0 and 1, well, you can play the same trick. So in the free parameter IBP, the V and K has this expression here. But as you can see, choosing a gamma prior for the gamma parameter is kind of a natural thing. You can tell that the integral will be manageable. Will be manageable. And indeed, if you choose a gamma prior or gamma, the marginal VMK will have this formula here. So it will be again tractable. So the availability of those VNK essentially means that you have the probability distribution in closed form. So those VNKs are the normalizing constant of this quantity here. Okay? So. So, well, not just the normalizing constant. They are an important factor of the distribution. So, first of all, we have three new models, the Poisson mixture of beta-Bernoulli, the negative binomial mixture of beta-Bernoulli, and the gamma mixture of the IBP. So, those are three new, as far as we know, IBPs. But on top of that, you can study the entire class of distributions. Class of distributions. So, first of all, this is a nice result. What's the total number of features that you have in the sample? So, the total number of features has this distribution that you can see here. And it's highly tractable. As long as you know the VNK, everything is available in closed form. And on top of that, if the VNK is the one of the binomials, so after the beta-Bernoulli, essentially. Bernoulli, essentially, or if this is the one associated to the Poisson mixture of the beta-Bernoulli or the negative binomial, then it turns out that the total number of features follows very well-known distributions. So, in the beta-Bernoulli case, the distribution for the total number of features is a binomial distribution. This is a known result. This is not something we discovered. This was known in the literature. So, it's a binomial distribution. So, it's a binomial distribution, and this Pn probability is actually super nice. It's a fraction. So, essentially, the expected value of Kn can be seen as a fraction of the total number of features n. Okay, so you observe just a fraction of them. In the Poisson mixture of beta-Bernoullis, the nice thing is that you get a Poisson. You get a Poisson, and the interpretation is the same. So you have a fraction of the lambda number of features, but now it's unbounded. It's important to see that Pn converges to one when n goes to infinity. So when n goes to infinity, a priori, you converge to a finite number of features. And for the negative binomial, you have a similar interpretation. In the alpha-positive regime, which is Positive regime, which is the one of the Indian classical canonical Indian Buffett process, you have similar results, although the function g n here has an entirely different asymptotic behavior compared to Pn. Gn here diverges when n goes to infinity. So the number of features grows unboundedly when the sample size grows. And the fact that this is a Poisson distribution is a very well-known result. So it's not something new. Result, so it's not something new. But if you randomize gamma, well, it's kind of unsurprising in this respect, you get a negative binomial. This negative binomial is associated with this set of the NK. Okay? So this is a priori. The super nice thing is that, okay, well, this is a technical result. We can actually characterize also the The number of features appearing exactly are times in a samples. So, not just the total number of features, but also, for example, the number of singletons. This is the general formulation, and if you specialize this result, you can actually have binomial, Poisson, and negative binomial distributions for those quantities. But that's probably less interesting. It's more interesting what happens to the posterior structure. So, what happens to the predictive structure? So, what happens to the predictive structure for the general class? So, the first customer enters the restaurant. So, the first customer selects the dishes according to this distribution. At the generic step for the entire class of distribution, the probability of re-observing an old dish. Yes, fantastic. Um, the old issues are um Dishes are observed with this probability here, which is the same for the entire class. The new dishes are observed with this distribution here, but once again, once you specialize this VNK, you get very well-known distributions for that. And the previously selected dishes are selected independently, which is kind of Independently, which is kind of a consequence of the product form structure of this class of modules. More generally, the posterior distribution of the number of unseen features is available in closed form. Once again, this looks complicated, but once you specialize this result by plugging in the appropriate coefficient, you get very well-known distributions. So when m is equal to one, So, when m is equal to 1, this corresponds to the predictive step. So, how many new features I observe in the predictive step. But in m step, this is what you get. And I think it's easier if I focus on some examples. The posterior distribution for the beta-Bernoulli is again a binomial. So, in a sense, those models are conjugate. I start with a binomial prior, the predictive structure for the number of new features that. Structure for the number of new features that I still need to observe is again binomial. In the Poisson case, I have a Poisson posterior. So those are the interpretation here is also very nice. So in the binomial case, I have the number of remaining features multiplied by the probability by a certain probability, which is a tilted version of the one that you get a priori. In the Poisson case, you have a combination of the fraction of. Of the fraction of feature that you haven't observed before times the fraction that you still need to observe in the next sample. And then in the negative binomial, you have a similar quantity. And again, and even in the infinite case, you have Poisson and negative binomial distributions. So once again, in a sense, those models are all conjugate. So you start with a Poisson, you get a Poisson. You start with a negative binomial, you get a negative binomial. Even though, as before, Minomia, even though, as before, the asymptotic regimes of those two classes are entirely different. And so, finally, we are back to the alpha diversity. So, I spent a lot of time discussing about the alpha diversity for the species sample models. So, it holds that even for feature allocation models, you get a very similar result. So, the alpha parameter is a structural parameter, it controls the asymptotic regime where you are. Symptotic regime where you are. And when alpha is negative, Kn, the number of features, converges to capital N, which is deterministic in the Bertabe Bernoulli, but once you randomize it, becomes a random variable. Once you choose a prior for it, it becomes a random variable. If alpha is equal to zero, then you have a logarithmic growth. And when alpha is between zero and one, you have a polynomial growth. Okay? And summarizing, you get the very same result that you get for species sampling models. Result that you get for species sampling models. So, S of alpha is the alpha diversity for feature allocation models, which is the analogous of the alpha diversity introduced by Pittman. And the Gibbs type prior for Gibbs type feature allocation models are indeed as in species sampling models, they can be represented in a hierarchical fashion. So, basically, you have building blocks, which are the beta-Bernoulli and the free parameter IDP, and in those And in those models, the alpha diversity is deterministic. And by making it random, you end up being within the class of Gibbs type feature allocation models. And a posteriori, sorry, I go a bit faster here. A posteriori, the extrapolation curve converges to the posterior distribution of the alpha diversity. Alpha diversity, and actually, you can characterize it in a closet form in many examples. We have even more theoretical results, but I don't want to bother you too much with completely random measure at the moment. So, you have an hierarchical representation, and all these Gibbs-type priors, Gibbs-type feature allocation models have this hierarchical representation in terms of beta-Bernoulli processes. And we actually have the posterior for the whole process. Actually, you have the posterior for the whole process. So you can actually characterize even that in case you were interested. I think it's easier, it's more interesting to get to go to the application directly. So those are actual data. It's a simple data set that can be used as an example. So the dots are the actual data. And they record the distinct feature that you get in a sample. And on the x-axis, you have the number. On the x-axis, you have the number of observations. So, this is the dots are the empirical ratifaction curve. And here I'm depicting two models, the Poisson beta-Bernoulli and the gamma IVP. So, in this specific example, it's kind of clear that the Poisson Beta Bernoulli is doing much better than the Gamma IVP. It's not something that happens all the time. It's just this specific example. And the reason why this model works better than the Gamma EVP. This model works better than the gamma EVP is probably because the data suggested there is a finite, even though it's random, but there is a finite number of features in the data. And so by the idea, so this is the rally function. So in a sense, it's the training model prediction, so in-sample prediction. What we want to do is to extrapolate this ratifaction. And if you, something that you can do in this way. Something that you can do in this way. So you extrapolate the rarefaction using the formulas that we have seen. And though this extrapolation is just a combination of Poisson random variables, it's not something that you have, you don't need to do MCMC or anything. You just sample from Poisson random variables. You don't even need to sample them, actually. And if you continue extrapolating these error faction curves, then what you get Curve, then what you get is the posterior distribution of the richness, which is the total number of species that you have in the sample. And the posterior distribution is once again available in closed form. So in the Poisson case, it's once again a Poisson distribution. And in the negative binomial, it's once again a negative binomial. And in this plot, you have the posterior distribution for the total number of species that you get in a population. And here I'm comparing. And here I'm comparing two models: the Poisson model and the negative binomial model. The Poisson model is more concentrated. Negative binomial models allow for over-dispersion, which is exactly what's happening, even though the mean is the same, more or less. And so, if you remember, the observed number of features was about 225. And if you keep And if you keep extrapolating, our model says that there are about something between 290 and 300 species in the total population. Okay, hopefully I'm more or less in time. Those are some references, and thanks a lot. Thanks. Maybe we have a time for one quick question. For one quick question. Thanks, Tomaso, for a wonderful talk and to the force of like a huge class of models. I learned a lot. One stupid question, like one of the attractions of working with the independent buffer process is computational fairness, like posterior simulation. I assume all of that remains true under your mixture model, right? Your mixture more, right? Okay, so I hope I heard the question correctly, but if I understood correctly, it's about computations, right? Okay. So, well, this is an instance in which computations are not an issue at all, because you're not using those models as a latent level, but you actually assume that you observe the species. Assume that you observe the species and you observe the yeah, you observe the species. And so you do not even need Markov check Monte Carlo. The posterior distribution is a Poisson. That's it, which is nice. There are some subtleties. I mean, there are some hyperparameters that you want to set. And so those are just a couple of parameters that you can choose by using Markov Che Monte Carlo or empirical base, as you prefer. Or you can even elicitate them in a subjective manner. Felicitate them in a subjective manner. You are free to do that, but there are just a couple of parameters, so it's not problematic at all. But conditionally, on those extra parameters, the posterior is either a Poisson or a binomial or a negative binomial distribution. Wait a moment. I mean, of course I could use those models for latent structures. I mean, you didn't tell us that, but I could. Uh, should I? Uh, I'm interested in the answer is maybe not. Your answer is maybe not. No, no, no, it's possible. You can. People have done that a lot in a lot of models. They use the Indian Buffett models for generalization of stochastic block models, for example, for matrix vectorizations. Those are generalizations of this approach that a way to be written. I think we will explore that in a second stage. In a second stage, but not in this paper at least. But of course, if you generalize and if you this, I mean, if you use those priors as a building block of a more complicated model, then of course you need Michael Che, Monte Carlo, the user machinery, and you end up with the same complications that you would have in mixer models. Yes. Yeah, maybe in the interest of time, we can again Tommaso? Again, Tommaso. And if Isabella, if you can turn on your camera.