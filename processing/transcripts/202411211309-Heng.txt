Sorry? No, yeah, no, no, yeah. Whatever long it should be, yeah. Oh, please. Sorry. Yeah, but Julia go. All right. Hi, everybody. So I'm going to tell you about a formative idea that we have to try to improve the performance of G-Spinetry. This is the algorithm that's currently in the data quality report that identifies glitches and can also identify signals. And can also identify signals in the Pelicans in image or image series. This is going to be a project led by Nicola Kosh, who couldn't be with us. He's a PhD student at UBC in collaboration with Mervyn, myself, and the rest of the team. Alright, so we didn't get the chance to really explain what G-SpyNet Tree does exactly. So it uses the same feature sets as Gravity Spy. So this is a series of spectrums of different durations, as we'll see in a second. Spectrograms in different durations, as we'll see in a second. But what's really unique about it and why it works better than gravity spy proper as a signal versus switch identifier is not only the multi-label element that Mervyn mentioned, where it can identify both in the same image, but also that it's trained to focus on different regions of the time frequency parameter space. So, for example, it splits up the decision-making process into three different classifiers that all have features. Classifiers that all have features constrained within different durations. So low mass below 50 total solar masses, high mass below 250 solar masses, and then extremely high mass required a separate feature set. We actually applied the Mercator transform to an omega scan to try and differentiate between real signals in that mass range and these low frequency blips. And this is the feature set. So who's used Gravity Spy before? Said, so who's used Gravity Spy before? Seen Gravity Spy? Okay, so most of us. So you're very familiar with this. Then we've got for the same event, the glitch signal, maybe both, these four different durations that are flexible enough to capture glitches and signals with different time frequency morphologies, from the shortest to the longest, at least for our purposes. So we can then assign this set of four images, this multi-label. Set of four images, this multi-label class score, which can again, as Mervyn said, identify more than one image. So this is a study done by Sofia Alvarez-Lopez, who is the main developer of GSPINE3 before she went off to grad school at MIT, that's showing that it does pretty well when it's tested on the same kind of data that it was trained on, which is maybe not surprising. So this is a test done on Lego data showing that it's got about 90% accuracy on gravitational X. 90% accuracy on gravitational signals. But here's where things get interesting. So, this is a study done by Irene over the summer in collaboration with Francesco, like Lost Director Francesco, there's maybe not with us yet. So, what Irene showed is that the G-SpyNet tree results trained on LIGO did it, even for the same kinds of glitches that have identical signal morphology by definition like blips. G-SPI-NET tree is tricked when you feed it the same example glitch, but in The same example glitch, but in Virgo data. So, the suspicion is: this is our hypothesis: is that the model is looking at the background. And we really don't want it to be doing that. This is not helpful for classification, and then it can also introduce this kind of performance issue. So, this is our idea. This is called the human-in-the-loop, or if you've seen this in the literature, they call it the HITL, the H-I-T-L, human-in-the-loop. We're just starting now to think about leveraging both the About leveraging both interpretability mechanisms like GrantCam to verify this hypothesis and look at where in the image the classifier is looking, and then also incorporate a human as part of the training set, where the human can come in and during training, the algorithm can say, Okay, here's a visualization of where I'm looking. If it's looking around where you're hoping that it's looking to make the decision, you do nothing. However, if it's looking somewhere off However, if it's looking somewhere off in the distance, which doesn't have relevant information, you can penalize it. And you can work this into the loss function and help it to learn what kinds of regions of the image it will get. So our summary, we haven't made much progress on this yet, but we've made enough to understand that it could be really powerful. It could really help us to train this linear classifier to look where we want it to look. However, it's really, really hard, it seems like, to avoid introducing. Really hard, it seems like, to avoid introducing bias as part of this process. So, where the human is in the loop and saying, don't look there, that could potentially introduce, especially if you've got multiple humans doing this in different ways, that could potentially introduce some bias in the model. So, this is where we are. We're trying to understand this. We think it could work really well, especially for feature-proofing algorithms so that we don't have to keep retraining them for every observing run. We'd love to hear your thoughts or maybe collaborate, especially if you've tried something similar. Similar. And I'll leave it there. Thanks, Jess. Questions for Jess? Melissa? Yes. Super interesting. I think I have the question that, because you guys are using real data. I know how you do that. Um, do you think that it will be like interesting to understand that better like to create like a malf data tool? Yes, absolutely. And I think that's something that other people for other purposes are thinking about in data technological responses. I saw this somewhere in one of the notes, this idea of a benchmarking data set. I think that's a fantastic idea. We did it back in 2012 to try and downselect what we call event trigger generators. Omicron was the number one algorithm that came out of that. But I think it's a really nice way if we can all. I think it's a really nice way if we can all agree on what should be in the Mod Data Challenge to have that baseline. Yeah. And I think then I can't remember some anything that you're trying to mention on the model or any other pipeline. So Tom. Yeah, I have a question and then also perhaps a suggestion. You know the way with gravity spice spectrum. You know, the way with gravity spice electrograms, the Q value, even with two glitches of the same class, they can have two different Q values depending on what the algorithm finds as optimal. And how much could that affect outside the background? A lot. Well, I've glazed over some of the details. We've done a lot of studies to try and figure out how much variations in the feature set could change the outcome. We've shifted the centering of the image. It's the centering of the image. We've tried, every single image that's used by GSpyNet tree has the same Q value. To your point, that was extremely important in getting the good performance. So, yes, absolutely. And I suppose a suggestion would be maybe to condition this model on the Q value, that you don't just feed it the the Q scan, you feed it the Q value and then with regards to the background, uh feed it the P S D at the time of the glitch. At the time of the glitch. And in that way, you might the model might learn to be robust against different environments. Could be one suggestion. There was one researcher at LSU that basically combined, this might be a bit expensive depending on the application, but she basically combined all Q values, all Q scans, and the Q, like changing the Q value of the same Q scan between whatever it is, 1 and 65. Between like whatever it is, one and 64, I forget the range of center tip. And then, what she was doing was trying to do like an unsupervised approach to cluster the glitches, not just using the optimal Q value, but all of them to get like all possible views. It sounds expensive even saying it. It sounds a bad standard Q scan. You have to do that in order to get the best one. So, just we'll use all of that. So, just take all of them and get the model to learn all of them, and that way would perhaps be robust against Q values. To be robust against Q values and stuff like that. Yeah, Q value we don't have an issue with anymore. Ever since we required that they're all the training set, test set, everything is the same Q value. The same Q value. Okay, okay. But that's the PSD is a really interesting idea. I haven't thought about it much, but I really like that angle. Yeah, feed it in as extra input at the beginning and let the model figure it out. Yeah. Sorry. I take Travis provided and ask you one more question. Please. In that example that you have. Please. In that example that you had, you know, clearly the glitch was there. The machine should have focused on the glitch, but the machine went somewhere else. Well, this is a hypothetical. This is. I understand. Yes. But in such a hypothetical, I know the hypothetical examples there to demonstrate. But these kinds of examples here are the thought that you don't need a human to tell it not to look there. Do you see what I mean? Are the problems that have arisen more subtle that you actually need the human? Or is it more subtle that you actually need to, you know, for example, the square is a rubber shape that's actually focusing on some weird thing or something? I mean, do you know, or are you still investigating? We're still investigating, so the next step is going to be to use GregCam to visualize what it's actually doing. And then you're exactly great. Depending on what the answer is, we'll tune whatever intervention we can to do with that. Sounds good. Thanks. Thanks, Jess. Next one is Chris. Oh, okay. So, what is the problem we're trying to solve here? So, the problem, which again, as Siong mentioned, is still a hypothesis. It may not actually be what the issue is. For some reason, this algorithm seems to struggle when you change just the background fluctuations, which indicates to us that it's probably looking at the background fluctuation. So, the human holy is trying to penalize it for looking at Penalize it for looking at places in the image that don't affect what they see through the image. Is it going to be like between the input and output load? So how much is it going to change the latency of infrareds, for example? For the training, it will change the training time from the order of minutes to weeks. So very time-intensive. But what's the most trained? So we'll still be just as fast. Great. Okay. So. Great, okay. So now it's up well down to Chris to tell us about dinosaurs. I've forgotten this is called dinosaurs until I looked up playing the code called dinosaur. Yeah, sure. So this is one of the postdocs in Glasgow, Joe Bailey. And this has just got one slide. How do I. There we go. It's gone animation. And it's going to loop and it's going to get hypnotic. And I'll try and explain what this is doing. So the question we were trying to ask. But the question we were trying to ask was: if you detect an arbitrary H of T signal, so specifically a transient one, specifically a burst signal, so we've all been looking for, not all of us, but many people have been looking for bursts for 10 years or so or more. If we find one, one of the questions might be, what made that burst? What mass distribution specifically made that burst? And so here is an example. And so here is an example. I'll explain it in a minute, but we have the strain on the right-hand side, the black curves, in three separate detectors. And for that particular waveform, we've got the truth because this is all simulated. The black dots, if you see the black dots, they were the mass motions that were simulated to generate that waveform. And then we've got a normalizing flow to predict what the possible mass motion could have been. Possible mass motion could have been, and that's what the orange and blue points are representing. So they're representative of the posterior distribution of what mass was doing to generate that waveform. And so I just thought that was quite a cool little project. I wouldn't argue it's top of the collaboration's priorities to do this, although it would become higher if we detected a transient signal that we didn't really understand. Some of the caveats here are: we have Here are we have trained this, or Joe has trained this on completely random motions. So, to do the forward process here to generate the training data, we make random motions of masses and then we work out what H of T is and we do that millions of times. So, we have mass motion and H of t and then we train the normalizing flow on that so that we can basically go backwards. If somebody gives us H of T, we can get the posterior distribution on the mass motion. This is, as I said, The mass motion. This is, as I say, trained on arbitrary mass motion right now that is not physical. But what the next stage of the work is, is to try and train it only on mass motions that are more physically realistic. Because if you look at these black points, gravity alone would not really make them do that, that wiggle. But if you want to be completely agnostic about any for what's going on and you find a waveform that looks like that, it's generated like like that. Generated like that. There's a lot of the weird things where it breaks into four and stuff: the symmetries and the intrinsic symmetries in the HFT, how you get from mass motion through quadrupole to HFT. I guess I could have a question. So, how do you train this? Like, you go from the motion to the strain, right? When you're training? That's the forward model, is the easy one. The forward model is the easy one. Yeah, we can make the motion and compute the strain. And then we do that loads and loads of times. And that forwards the training data set for the flat. So basically, like, these are just the total mode, right, which we're showing. The total mode of the super complicated things. And this motion, you can just vary this motion and then find the strain, the quadrupole strain it produces. Do you use some kind of package like what are you using to calculate the strain? Just using a very basic quadrupole being the time average of the squared derivative of the strain, of the mass motion. No, it's not complicated, it's not numerical relativity, it's just back-of-the-envelope type things. So, this computation is Newtonian order, or is it? Yes. That Maco first. That Maco and the airport just go. I will try to reverse the process. Have the trained model, give waveforms, and see what you would see. No, that is. The T is arrayed. That's it, so sorry if it wasn't clear, but this is the input. This is the input to the model, and we get that the answer. But we can only go easily compute this way. We can easily compute this way. That's easy, right? So we train the model with all the data having gone that way, and then this becomes the input, and we get that. I should also say one other thing just about this I forgot to say. You can't quite see it, but at the end as a validation check, we take every one of the masses that it generates and work out what the waveform was. And that's plotted on here as well. So every single one of the blue-orange pairs, you can't identify which relax, but they generate exactly the same waveform. Generate exactly the same waveform within tiny areas of what we've got here. So it's a nice validation of the flight. No, I was wondering, but can you invert the problem? Is it one-to-one? I thought that... It's not one-to-one. Yeah, so how can you train a machine to do that? It's one-to-one that way. It's not one-to-one the other way. It's many, many mass motions, as you can see here, produce the same. As you can see here, produce the same waveform. Okay, but there's one mass motion produces one wave. Because, for example, you had a dipole in your dipole component in your motion, you will get the same wave for the same motion. But there's lots of degeneracy going by. And this is even without noise. And when we put noise on, this just gets puffier. I think Francesco. I think franchise. What happens to the dots that are shooting away or coming in? That's machine learning. That's machine learning. That's the tails of your. Okay. And the ones shooting in, where they come. Do you have more masses at infinity and they're coming in from there? No, I think this is. I think if we train for another day, those go. You you can see these things. That's a that's an effect from from it just not being a perfect model. No, no, but I kind of understand the ones leading those. I kind of understand the ones leaving the square and the one's coming in. Um I think one came in from the right. I don't know. I mean um doesn't affect the waveform, I'm sure I mean they may well be waveforms that are. I don't know exactly how Joe did this. This might just be the 90% error. So there might be the ones that are flying in might be given mad waveforms, but it's one. Give them mad waveforms, but it's one in thousands. Okay, last two questions and no more. I'll move it first and we'll have it. Thanks. So, this is very interesting. Have we tried keeping it and it's viral and see what happens? That was the original goal to do this. Now, it just turns out technically the way that we are parameterizing the masses, this is this looks like it's all time series, but we we don't actually put time series into the model to parameterize. Series into the model to parameterize the motions. I think in this particular case, it's a limited set of Fourier components in both bases that describe the motions. And it just turns out that it was quite hard in that particular choice of bases to get a waveform that looks like an inspiral. We can, of course, just after training, we could throw in an inspiral waveform, but that would be out of distribution and we wouldn't really trust the results. But that really was one thing we wanted to do. Was one thing we wanted to do because I was really hopeful that that would be the cherry on the cake for me. Because if it showed Mass is doing this, we know it's all working. That's something we still absolutely need to do. Okay, last question. Yeah, so you said going backwards from the signal to the mass motion is not one-to-one. I understand that. So, what do you need to make it such that you can go from the signal to the mass motion? What extra information do you need? To the mass motion. What extra information do you need apart from the stream? For example, here. Oh, you to go back perfectly. You can't do it. There is intrinsic degeneracy in the way that we receive the two polarizations of the gravitational wave through three detectors. We don't have enough information to break the degeneracy. There's both intrinsic degeneracies here with multiple modes, but there's also going to be no. But there's also going to be noise, and at that point, we'll never get the perfect thing. Maybe more detectors will make, will break potentially at degeneracy, but I don't think so. So the eventual use case is that you have some strain and then you are able to go back and map to the motion classes, right? My ultimate use case is we detect the first burst thing that we're statistically confident in in multiple detectors, but we don't know what it is, and then We don't know what it is, and then we tell you it was three events coming in and doing this. That would be my ultimate. Yeah. Oh, sorry now. It's been like ten minutes for you, we've got five minutes. Are you going to fill up two hours anyway? We can come back to this later if we want. You should leave Joe's face on for the rest of the hour. Yeah, music with that. I thought you didn't do my. No, no, you music left because I tried to bring up the slide. Because I tried to bring up the slides and make it funny by so maybe lost it was a feature. Okay, hello everyone, I'm Yin Yang one on my research topic is searchable gravitational wave from Parket Supernova. So let's think about one scenario. What if a galactic supernova exploded when J is only one detector is in operating? Detector is operating. It's possible. Because based on the duty cycle from O3 and O4A, you can say in O3, there are around 30% of data are in the single reflect mode. And in O4B, this percentage is reduced to 18.5%, but still there are a lot of data. So if a galactic supernova explodes is only one detector in operating, probably we can use machine learning. Probably we can use machine learning, powerful, to reduce the noise and to do the searches. For example, for supernova 2023, each supernova exploded on last year. So in last mid-19th, in the hot galaxy, it is on Modern 1. The distance is 6.7 mparsec. Each one of the closest signals exploded in the last 13 years. In the last 10 years. So it's one of the lowest in one we got so far. And at that time, both Canford and Leiviston were in observing mode in the engineering mode. So we have some data to do the searches, OCAS each to detect the search. The search result is available in archive at this moment. And OCAS we found no signal in this area. In this area. So, but one thing to notice is the search window, which is called Ansource window. We estimated as a file data. In this file day, the data between Humphrey and Levians and the coincidence data between this detector is around 0.8 data, 15% of the answers window. So, just a small amount of data we can use to do the search, to perform a coherent search. But meanwhile, Search. But meanwhile, if we look at the data for a single detector, for Leibniz, we have more than a day, so 27% of the outsource window we can use. And for Hanford, it's 30% of the outsource window. So for a single detector, we have more data to do research than the coincidence data between two detectors. Then how can we use this data? And how does data look like? Data look like? Okay, let's look at the search background. So, this is the search background produced by the search pipeline, which is called CWB. So, the left-hand side is the search background for the two-detect, and the left-side is for single-detect, which is livid. So, for the x-axis is for celebrities in uh per year, and uh the x-axis is the ranking statistic in CWBT jobs. In CWB Joe. So compare these two plots, we can clearly see in a single detector, the data is super synchronizing. If we look at the rate, and for your two detector pairs, the rate goes up to 10 to minus 1 per year. But look at the lowest in your single detector, it's more than 10 to 4 trillion per year. So the single detector. So, the single detector data is supernova. And then I'm thinking if we can use machine learning to classify noise and supernova signals so that we can exclude some noise tribals. Yeah, here I show the workflow about how I do this project. So, first I get a data set in O3. So, I inject a lot of supernova signals into the O3 data so that I can Into the O3 data so that I can get the signal triggers. And meanwhile, I can also do a background search with CWB, then I get the noise trigger. Then this is a band problem. Then I use the machine learning algorithm to do the classification. Classify the noise and supernova signals. I train a model, and of course, I want to test the accuracy of the model. And after that, I can apply this model to the supernova, and I can select FCF triggers. triggers so it will get a list of triggers that the model identifies them as noise and that I can remove all these triggers the model identifies them as noise so I can remove them so that I can reduce the noise triggers then I generate the final result report this is the idea about this project at this moment I only have one preliminary result with However, preliminary results to the running forest. Although machine learning algorithms is not limited to running forests, other machine learning algorithms can be classification, can also apply into this project. So I trained a very simple model which accuracy of 92% and I use some features. And here is the computer metrics and also the ROC curve. This is a a very simple mod. A very simple random forest model with a fat tuning from the O3 data. And next, I apply this model into the supernova single attribute. So the same plot for the lambda was short. The black curve is the one I showed you before before we apply any machine learning models. So this is the original force lambda curve, and the red one is. Is getting from after we vetoed all the noise triggers identified by the model. So we can clearly see the first line width is lower than the original one. This is a small improvement for the background, which means the background is less noisy. This is what I have so far. I'm still working on this project. So, of course, the next step I want to tune in the models because I really haven't Tuning your models because I really haven't spent my time to tune models. I can also try different machine learning algorithms. I can have an undergrad student working with me using HDP. He really got similar variable model, but I haven't had time to apply it. And I also want to apply the model in the simulation to see if I can improve the detection efficiency for different supernova models. And the most important thing is I also try to find a way. thing is I also try to find a way to combine a single detector result with two detected results so that we can use more data in our search. So that's what I want to share today. Thanks again. Yes. I think this is very nice question. Or does normal mean like that? So over here you're you're taking the aim. Are you checking the the input? Or are you checking sorry the input is the immediate data set? The data set is O3. But first for the noise triggers, so it's background, so CWV, the search pipeline, can do the time shift on the data so that I can get a search background. So which means all the triggers I get is background search noise. This is a This is my noise from O3. And then I also injected a lot of supernova wheel forms. And CWB can recover them. So then this triggers the simulated supernova wheel forms. So then I identify the initial signal triggers. So to kind of trigger. So this triggers and what features? They have a lot of features. They have a lot of. Yes, yes, yes, are the features from current reports, for example, the network SNR, the nexus, some specific parameters defined by CWB and also the time, the stream, and other features. For the model I showed here, I only use 10 features, but I think overall, there are probably 40 features. Yes, I'm at more feature level when I train the model. Yeah, I think utility also should understand. The utility also to understand what are the computer requests relevant features to machine learning can be more can be smarter because when the random forest are used, it actually gives me the output or the significance of each feature. Yes, so I can essentially I can put all I can give our features to an already. Give our features to the forest and let it tell me which feature is more important. Yeah, yeah, yeah. OCAS uh to be your machine learning address to reduce the knowledge in a single detector case. It's not limited to main forest. it's not limited to running forest. FC boost and also genetic programming, you know, see uh I feel like uh probably most of our machining algorithms as long as I can do classification it should be applied to us. Yeah, I think my last thing we just let's see if anybody else has a comment and then we can we can we can we can have a list of uh sorry Yeah, so you know, we've been to inform in Australia by using the glitch classification algorithms to produce the single detector background. Yeah, I know we had a chat about it, but we haven't chatted since, so yeah. We're still working on it if you wanna work together or I forget so. Yeah, of course, oh cast, okay. So we can we can add a more glitches. I mean, classify the I mean, classified, yes, we can work together. Oh, class. We changed our email, and now we can really start working on it. Yeah. Yeah, I had a question about what kind of signals, what kind of supernova signals are you injecting? A lot. A lot. So if I remember correctly, I your 15 supernova will from family. Different waveform families. So from neutrino-driven explosion to the faster repeating models. So yes, there are certain 15 move form families and each family also have different models. So a lot. I don't remember a clear number if you ask me, but I can tell you a lot. Anyone else? Other several things? Are there some adjustments? Um yeah, so I think my previous point was that in my little understanding of some cuts that they're talking about. Yes. I think also the machine learning itself, they choose some parameters to be able to discrete better in the background. So. Yes, so I think that's what I meant for learning from maybe you suggest that we can also try XCBoost. Well, I mean, I think XB boost need to be X. Yes, there can be any machine learning algorithms. But I think the point is to understand how you're how like to interpret how the random forest interoperates. That's kind of a sounding comment. Thank you. I have a question. Yeah, yeah. Did you use any of the information about Of the information about the supernova in your injections? Because you are effectively performing the targeted search. Yes, yes, I use a supernova SCAD location. Yes, when I get the training data set, which means I inject all signals in EO3, at that time I use... Oh, actually, no. Yeah, actually, no, no, no. When I get the training data set, no, I don't inject it at all. I don't want to inject it as a specific sky location. I think that I'm not sure that's the right thing to do. Perfect. I mean, if you want to make it a generic single detector all sky search, yes, that's the right thing to do. But I suspect that if you want to do it 23, that I would use the distance and the sky location because that's why I was wondering why that didn't go down much further. Because if you use the supernova information, I think it falls a lot with that. Yes, yes. I saw her if I did a sign. Use the data set just get from just use the information from the signal XF. I think my model probably the model can classify more noise. But my problem, honestly, I want to get a geometric model which not only used the for XF but also can use for other supermodels. So I will tell you that that's a very difficult problem and it would I can tell you that that's a very difficult problem, and it would be better to start off with the target research problem. That's what the suggestion is. Yeah. Okay, we can't talk about that. Anyone else? No, then thanks again, Annabel. Thank you. So I'll give you a look just in the model. No, no, no. Well, I was really weird. Why is it so starved? Okay, I'm here to present some work that a grad student at UWM has been working on, Pratt Yusabla. In just case it's not clear, that is Prox Yusabla. We have been working on trying to solve the age-old question of what event we should send. So, for the background, I'm talking about. For the background, I'm talking about the preferred event. So, in low latency, we typically have a whole bunch of uploads to your ACB, and we have to ask the question: you know, which one of these do we want to send out? Like, all of these could have their own sky map produced by VSTAR, all of them are going to have their own false alarm rates, et cetera. What's the one that we actually send? Currently, we pick the max SNR. The argument is in Gaussian noise. Max SNR is the one that's going to be. Max SNR is the one that's going to be have the smallest sky map and should be the most accurate and that it matches the template, like the template is the closest to the signal, et cetera, et cetera. This is somewhat contentious because we don't have Gaussian noise, we have Gaussian noise with a whole bunch of other stuff, and there have been some results, one of which I'll even show you in here, that other metrics are possibly better, but SNR is what we have. This is a great problem, we thought, for machine learning, because then we Machine learning because then we can take the pipeline bias out of it. And I say this as a pipeline person, so hopefully that carries some more weight. But you know, us pipeline people, it's hard for us to like remove our desire for our event to be the one that gets sent out from the equation. So maybe machine learning can help us remove that, you know, kind of human, I want mine to go out effect. And here, just as an example, people not in the cloud. People not in the collaboration can't normally see this, so I've blacked out the sensitive stuff. But here's just from one of our recent public events: not even the entire table that shows all of the events that we upload. So we've got to take this, you know, 10 plus events and make a decision which one of these is the one that we send out. So what we decided to do, we did, in the collaboration, we did an MDC, low-latency MDC, before 04 started. It's still ongoing, just as a testbed. A test bed, but we put a really, really high, like non-astrophysically, non-astrophysical, high rate of loud injections and stream them in low latency. And the NASDA pipeline is like, hey, run in low latency as if you were running in production and upload the GraceDB. And we'll use this as an end-to-end test of our entire low latency system. And so that's really nice for us. It's not astrophysical, but there's tons of events that we can play with and tons of super events that each have all of the G events that have been uploaded to it. Have been uploaded to it. We decided to use a convolutional neural network, even though those are typically used for images. So we did do some checks to make sure this made sense. But we basically just give it some lists of inputs, the SNRs from each of the detectors, because this is a three detector. So usually you have H, L, and D, the false alarm rates from each E event, and then the chirp mass associated with the template that found that E event. And at the end of the day, what we try, you know, the motivation for choosing. You know, the motivation for choosing one event, you could argue, comes down to, you know, what's the best sky map going to be, right? Because really, we want to make sure we're giving astronomers the most accurate sky map we can possibly give them. And in reality, you don't know what that is. So we decided to use this pretty common metric called the searched area, just in case anyone hasn't seen it before or needs a reminder, like I did shortly before this talk. You do an injection campaign, like we did, and then you run Baystar. And then you run BayStar, the rapid localization software that we have, over all of the events that you've uploaded. And for each SkyMap, again, it's an injection campaign, so you know where the signal came from. You start scanning through the sky map as if you're an astronomer. I think what we do is we start at like the, you know, the highest peak of probability and then just like lower the water, go down. And you keep track of how much area you scanned through until you hit the point where you find the injection. And then you call that the source. You find the injection, and then you call that the searched area. So, sorry, I got to go to the cloak wing. So, lower searched area hopefully means that astronomers won't have to towel the sky as much or look through as much of the sky map to find the signal source, which is what we want. And skipping right to the results without any details, but there's a lot going on in here. It's this laser pointer, laser pointers. Yeah, okay. So, all right, a lot going on in here, so let me walk you through it. So, y-axis, just a typical cumulative, normalized cumulative distribution. X-axis is the searched area. The green is the actual searched area from all of our events. So, this is kind of the best that you could hope to do, like if you had the searched area at the upload. Searched area at the upload time when you're going to decide what you want to send out. This is the best you could possibly hope to do. SR is orange, that's what we currently do. And you see, you know, there's a lot of room for improvement. And this is what I mentioned earlier of, there's some evidence that it's maybe not the best even for what we could already do. Combined far, so that's just the false alarm rate. There's historical reasons why in our tables it's called combined far, don't worry about it. The combined far arguably does better than the SNR. Does better than the SNR. So that's one thing I just like to advertise occasionally. Becca Ewing from Penn State, I think, was the first one to actually make a plot that showed this and present it elsewhere. So this isn't a novel find on our part, but it has been shown before, and we showed it again. And then each of these black lines are different iterations of the CNN that we tried. And it's just what we did was we took some fraction. Some fraction of the total MDC results and trained on that fraction. And then we tested out our newly trained model on the other result. And these are on the rest of the set of data. And so these different iterations are just, you know, maybe in the first one, we use the first 50% as the training and the second 50% to test. And then the second one, we used, you know, 25 to 75% for training, and the first quarter and last quarter for test, et cetera. So it's just to see, you know, like. So, it's just to see, you know, like how much spread is there. And for most of them, there's a couple that kind of bump down here, but most of them are at least on par, if not slightly better in terms of being closer to this optimal line than the combined far and SNR lines. So, the first interesting thing is: you know, with a fairly naive model, we can at least do as well as we're currently doing, and we can probably do slightly better. Probably do slightly better. So that's kind of the first punchline result of this. And then we found one other really interesting thing that I wanted to highlight here, which is just the exact same plot again, a little bit bigger. So you can see again, the closer to the green line, the better. And most of these black lines are closer to the green line than the blue and the orange. Not all of them. So if we were to ever do this in production, there's still some work to do. This is mostly a proof of concept. But you might notice we said ignoring none Python. You might notice we said ignoring none pipeline up here. Something we did just to see is we said, you know, does this change at all if we ignore any single pipeline? And most of them look very similar to this, but for reasons that we don't understand, and there are many reasons before I show this, there are many reasons to believe that this is because something's wrong with the pipeline. So please, nobody take away that I'm trying to attack any pipeline. But it is interesting that if you ignore MBTA, That if you ignore MBTA, that everything gets closer to the green line. Now, again, let me tell you a couple reasons why that could be, that has nothing to do with MBTA doing a bad job. One, their false alarm rates are based on their p-astros. Everyone else does it the other way around. So they have already kind of an inherent difference in that way and how they compute false alarm rate. And two, like I said, these MDCs are not astrophysical. Astrophysical. They have an extremely high injection rate, much, much higher than you'd see in reality anytime soon. And both PiCDC and MBTA have said that that biases their former calculation. And so you have to keep that in mind when looking at these results. This could be just an artifact of the MPC we used. It could be just pointing out that, like, hey, because they compute their false LUM rate differently, it has a slightly different, it follows a different population, and the algorithm has recognized that. Algorithm has recognized that and doesn't know what to do with that. So, just as a few final notes in a punchline, like I said, we can get comparable or possibly even slightly better results using a CNN as opposed to SNR or FAR. So, mission accomplished. Something I didn't talk about is there are cases where, like, it just the algorithm just obviously picks the wrong thing. So, there are cases where it picks something where it's like. I know, I'm flustered so I can't think of something off the top of my head. But there are things where, if you look at individual results, there's a percentage, a small percentage, where it's like, this is clearly the wrong thing to pick. So possible that what we could do, you know, one of the things that I've asked Prop to look at is, is there anything that's obvious to you when you go and look at these where it's picked the obviously wrong one? So maybe we could do something where we say, hey, most of the time we're going to use the CNN, but, you know, say, You know, say, like, let's say it's NSBHs, it's not, but let's just say, oh, we see for NSBHs that the algorithm just can't figure it out. So, for those, we'll default to MaxSNR or Minfar or something like that. So, some sort of like common case bailout for your corner cases where you know your algorithm doesn't work. But overall, I think it's pretty promising. Shows that like we could maybe just have a machine make these decisions for us, which is always nice. And Pratt's done a lot of work on this over the last few months. Over the last few months, he's learned so much more about CNNs than I have, and I've picked up a little bit from meeting with him and talking about it. But I'll try to answer any questions you have. But if you really want to dive into the technical details, I should put you in touch with Pratt. Anyway, thank you. I thought Chris first login on a little. Chris? Can I ask what's the dimensionality of your input data? You said some fars and some SNRs. How many numbers? How many numbers? Five numbers per G event. And how many events do you have to train in? Like, in the total, oh gosh, I don't know off the top of my head, thousands. Just because that sounds like not a problem for CNN. Yeah, I agree, which is why I was a little surprised that we got something that works as nice as it does. And we did a few things to sanity check because, you know, these aren't images, right? So we did some things where we scrambled the inputs before we did it. And we saw, like, oh, even if you scramble. Before we did it, and we saw, like, oh, even if you scramble it all up, you still see very consistent results. Yeah, because CNN will take advantage of structure in an image or a time series, and you don't have that. Right. As long as you don't scramble it every time you iterate on the training. I wouldn't argue that it works. I would just say there might be simpler neural network designs that would be better. Even a decision tree might be better, a more optimal thing. Yeah, absolutely, absolutely. This is just kind of. Yeah, absolutely, absolutely. This is just kind of a proof of concept, like vaguely speaking, can we look at this and take our biases out and do at least as good as we can, and we can. So it's nice to say, like, okay, now maybe we invest more time into doing something smarter. Marvin was next, but did Jess want to say something? I was going to suggest drawing for it. So I'm going to select that or matter most of that to you, Katie. Cool. Thank you. Thank you. Go ahead, Marvin. Thanks. So yeah, this is very interesting, Cody. So I'm glad that you saw your plot where you get all IPT and you have these different plots. And you have these different plots because it is actually consistent with what we found with our study with JupySky. So we need a JupyterSky team because JupySky uses SkyMaps as inputs. So we look into a lot of SkyMaps and we found that again, we're not trying to attack pipelines. I'm not saying this is a mostly thing or anything, but one thing that we found is that the IPTA system magically has smaller localization files compared to other pipelines. I was compared to other pipelines. So, well, if this is true, well, I have the plots here, but if this is true, then this might actually explain the disparate you see there. So, yeah, I'm assuming we should just cut the case. Yeah, that's super exciting. I'd love to look at this. Have you tried comparing this to Max General B's kind of score? No, we have. I would be really interested to see that. I actually have a message to prop from earlier during. Have a message to Prot from earlier during one of your presentations saying, Hey, remind me to talk to you about GW Skynet. I'll get back next week. So, perfect. Okay, Tom, Jay? Yeah, I mean, the question was already asked, I guess, but I was just curious. So, did you try, when you took out the other pipelines, it didn't have, it didn't improve, it was really MBTA. You took that one out and saw this improvement. Okay, so when you took out another one, then it stayed. Yeah, yeah, I have another one. Yeah, yeah. I have, I don't want to try to do this live, but I'll show you later. I've got plots for each, like none, remove GSTLL, remove HyCDC, remove sphere. Sure, yeah. No, no, I was just curious about that. But yeah, yeah, I was wondering if, yeah, maybe that was some result of using a CNN or something. It doesn't sound like it, because then you would have seen the same thing, I suppose. But it would be interesting to just do a comparison of how these things change depending on the model. The bottom is pretty much it. Yeah, for sure. No, I agree. Okay, thank you. Amarta? Okay, so Emmett, then Shai, and then I was curious to see the thing which we saw that orange curve and the blue curve were, I think you showed that false alarm rate, combined false alarm rate is more important or gets you closer results than the SNR. I was wondering is that something which might be affected by Something which might be affected by the model which we're using? Oh, these are. Sorry, I should clarify. For the green, the blue, and the yellow, there's no machine learning at all. This is just like we looked at the G events, and for SNR, we said, okay, we're picking the highest SNR events, and then this is going to be the search area that we'd send out. Does that make sense? So, this is just the injection campaign results. Yeah. Yeah. Yeah, yeah, yeah. Yeah, and this has been shown. It might have also been on the MDC, but like I mentioned, somebody not doing any of these, but somebody had, Becca from Penn State, had plotted these two lines before and had shown this and presented it low-latency calls of the past. So that wasn't a new result, but it was nice for us to independently verify that. This is really cool, I never heard of this search here yet. Had never heard of this searched area statistic before. Do you think it would make sense to also include distance when you're doing this sort of comparison or just rather just going through the area? Basecart also gives you. It does, yeah, yeah. I'm not good at thinking on my feet, so let me like chew in that for a bit. Let's talk about it over here tonight. I tend to think that I would give you an answer to that. So you could have been using NVCs as a The injection, we know the injected distance when we have seen that the week is every 40 days. And we try to match this sky area by shifting it by how many sigil days that we have. The original injection was done in the data sometime before 3. Everything with that incurred GPS time, we need to move the distance. Remove the distance. But the interesting thing is, SNR remains text. That means the distance information is productive. So I would suggest not to do that in any sort of distance on this. I think this is super interesting. I was wondering if I really told you that the combined farm is the combined. No, sorry. It's just called this because we've got like table schema that call it that, but it's really just the false alarm rate for one geode. Sorry, hey. Sorry, for one pipeline? Yeah, yeah. So what this is, is you take a super event, right, which is a whole bunch of GEvents that all found the same thing, and you take. Thing and you take, you say, okay, the event that we're going to send out in this case is the one event that has the lowest false alarm rate. Then I think my question is more like: if you had information of the SML, would you like to make an algorithm that kind of connects the information? Because every time we see something into the textures, you'll have kind of like Maybe not super similar, but maybe some other similar systems detectors and effects. Well, so the false alarm rate already folds in, depending on the pipeline, it can already fold that sort of thing in. So the only ranking statistic I know well is the GST-L ranking statistic. But for example, in that one, and actually Example in that one, and I'm almost 100% certain all the pipelines have something like this, but there's a turn in the ranking statistic that checks: you know, is the SNR difference between Hanford and Livingston, you know, what you expect it would be given the relative sensitivities, and is the difference at end times what you would expect it to be, you know, given that it's a signal which has some structure as opposed to noise, which would just be like a unique distribution. So, a lot of the signal consists. A lot of the signal consistency across detectors is built right into the false alarm rates that are reported here. Do you think that's this consistency between that if they are consistently too far light, then the search area becomes like not precise? That's a good question. It's hard to say because pipelines have. It's hard to say because pipelines have, you know, because false alarm rates are functions of the noise models, and every pipeline uses just a totally different noise model. So if you're looking at signals, like there's no, it's not clear if pipeline A has far A, that you can guess at all what far B and pipeline B will be. So I'm not sure that you could look for correlations there. Look for correlations there. Obviously, the louder the signal, but usually the way it happens: louder the signal, the more similar all the pipeline uploads look. But it's not clear to me how we could fold that in. But also, like I said, I'm not good at thinking in my feed. So if I just totally misunderstood, feel free to come up to me later and pick my brain more. No, no, it's essentially that. I was wondering if there's some information that is common among different platforms of different detectors. Right, right. I mean, that's what I was hoping. I think it's inconclusive to me right now. Michael, did I see your hand up at some point? Okay, okay, I mean something parallel each other seems kind of in that vision what what is the future here if that uh if all of that kind of infrastructure goes. Do we still have you could argue no? Like if we can get rapid PE SkyMaps that are ready to go at the time we want to send the preliminary notice, I think a lot of this disappears. Now what this might help solve is the sociological issue of people are still going to want to, you know, pipeline A is still going to want it to be their false alarm rate, and pipeline B is still going to want it to be their false alarm rate. And this is a nice way to say, like, hey, the machine made. A nice way to say, like, hey, the machine made the decision, we didn't. So you kind of like take your hands off and say, hopefully, no one gets upset. Sorry. It better be. A small question. Last question. Do you try changing how these results change when you are taking SNR and fossil fund rates only from one or two detectors at a time? So this should be here. So, like Sushant has mentioned earlier. Like Sushant had mentioned earlier, this is take over 40 days. So there should be one detector time, two detector time, and three detector time in here. So you're kind of marginalizing overt isn't the right word, but it should have everything built into this, just like you would have in real-done latency where you never know how many detectors are going to be on in any given day. It would be interesting to see, though, it is a fair point that maybe we should check to see if you only look at Maybe we should check to see if you only look at three detector times, is this more informative as opposed to only one detector times or something? That's not a bad idea. Marco, I've been forgetting to ask if there are any questions online. Have you been wondering? Is there anyone online? But I'm not sure. Yes, there are a couple of people online. I'm looking at it because I'm going to suggest, because somebody suggested this too earlier, we're going to take a break now, five minutes. I'll set up. Five minutes, I'll set up the Mentimeter stuff. It's gonna be a poll kind of thing. And you need your phone. Probably your phone is better. But if people want to go and go and have a drink, or go to make themselves comfortable, I'll start in quarter past. Yeah, I saw it. I'll look at it carefully and then we've got a lot of different things. Not only that, we've done we did that. We tried this one and I'll just pipe by the other photos that's guaranteed.  Okay, okay, okay. So we do the whole webinar after once we're like we know there's issues with this. Yeah, we have the best truck because it's pretty much working on the sixth one. That's why it's trying to do that. That's the user. Austeria sample has to parse out each sample. Yeah, just tested the solution. So we get one. So I've added another set of channels to see if we can. Sounds like it's time for you to take that.