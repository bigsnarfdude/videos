David, for the introduction. And also, I would like to thank for the organization committee, David, Xu, Grace, Peng, and Chen Ying for organizing this and for inviting me to give this presentation. I'm very grateful to have this opportunity to share part of my Share part of my research. So, the title is integrating summary information from external studies with heterogeneous populations. This is joint work with one of my PhD students, Yu Chi Chai. So here is a very brief outline of my talk today. So first we're going to give a short introduction and go over some motivating examples. Go over some motivating examples of this study and this product. And also, then I'm going to present the proposed method, then followed by some simulation studies and a data application. And finally, I'm going to give a brief discussion. So during the presentation, please feel free to interrupt me if you have any questions. Either, I mean, for those of you who are remote, I won't be able to see, I can't see. Be able to see. I can't see you because I minimized the other window. So just feel free to interrupt me if to start me if you have any questions. So now we are actually living in the error of big data. So it's very common to have data collected or have data available from different sources. So data integration, I view this. So data integration, I view this as a process where data from different sources are combined in order to reach a final or unified view in the end. Now, the issue of having data analysis focusing on a single data source is that you might get a biased estimation or inaccurate conclusion in the end. But through data integration, if we look at multiple data sources, hopefully in the end, we'll get a more insightful decision. The end, we'll get a more insightful decision. We can make a more insightful decision. This is a motivating example of this study. So this is the study of the risk of having high-grade breast cancer. Now, for high-grade breast cancer, there have been many, many studies available in the literature. Most of these studies, they make use of the conventional covariates. Here, I listed some of these conventional covariates. They include age, race, PSA level, the DRE findings, and prior biopsy results, and so forth. And then people started to think it might be better to also consider biomarkers that are related to the molecular mechanisms of the progression of pressed cancer. So, here are two such biomarkers: the PC3. Two such biomarkers, the PC3 and also the T2ERG. People found that these two biomarkers they have better specificity in detecting early stage prostate cancer. So our question, the question that we ask is, how does the risk of having high-grade prostate cancer depends on both sets of covariates, not only the conventional covariates, but also these two biomarkers. So we So we do have, or our collaborator, we do have access to their individual data. So we refer to this as the internal data and this study, we refer to this as the internal study. So we do have individual level data available to fit a model to study the dependence or association or dependence of the risk of high-grade breast cancer on these two sets of covariates. But the problem is that the sample But the problem is that the sample size is not that large. So, for high-grade breast cancer, there have been tons of research available in the literature. And these previous studies, they provide very rich auxiliary information regarding the risk of breast cancer. And in particular, because some of these studies, they were in very large scales. They were carried out based on very large sample sizes. So, here is one such example: the process cancer prevention trial risk calculator. This is the first online processed cancer risk assessment tool available. So the first version was released in 2006, and it was built based on data collected from 5,519 men. 5,519 men. And this risk calculator used the conventional covariance only. So it'll use age, race, PSA level, DRE funding, and prior biopsy results. And later in the data application, we're going to see a little bit more about this. So this risk calculator, it was built based on the distribution regression model using this five, one, two, three, four, five covariates. Five covariates. Now, our goal is actually to try to make use of the information provided by this risk calculator, because this calculator, it does provide auxiliary information about how the risk depends on this five covariates. So our goal is to make use of this information when we fit the model to our internal data. But the problem is that here, well, this is a direct. That here, I well, this is a direct quote from the PCPT risk calculator webpage. The results of this PCPT risk calculator may not apply to different groups of individuals. The reason is that there are some special characteristics for the samples, for the sample used to build this calculator, and these may or may not be generalizable to other samples or to other populations. So, this slide here, I So, this slide here, I listed some commonly seen challenges when we do data integration due to data heterogeneity. One challenge is that in the different studies, they may collect different sets of variables. Like in our motivating example, so the external study, the risk calculator, only use the demographic or conventional covariance. And by the our internal study, we do have some biological. We do have some biological biomarkers available, biological variables available. And different studies may collect variables in different scales. So some studies may class, for example, for age, some studies may collect age as a continuous variable, some other studies may collect age as a categorical variable. And even if the studies, for those studies who collected age as continuous, they might still break age into different categories when they fail the model. Model. In different studies, they may release their data in different forms. Sometimes we have access to some individual level data, but some other times we only have access to aggregate or summary data. And also different studies, they may be based on different designs. Some studies, they may be based on random sampling. And while here, I mean, well, this. And while here, I mean, well, this is we're making things a little bit sometimes. It may be unrealistic to assume it's random sampling because many studies, well, although ideally they would like to have random sampling, but if you look at how they collect data, then data are collected based on convenient sampling. But anyway, so here are different studies that may have different designs, like random sampling versus case control studies, for example. Example. And also, different studies they may target different populations. This different, so for example, the demographics and the biomarkers, they could have different distributions, and also the disease prevalence and disease mechanism. And some of these differences, we are able to check them based on the data we collect. And some of them we're not. And now let's take a look at some notation. Notation and a setup that will be used in this presentation. So, for the internal data, now we do have individual level data available. So, we have the response Y or outcome Y, and we have the covariate X, this is the conventional covariates, and we have Z. We use Z to denote a newly discovered covariance of Covaries of biomarkers, for example. And the model we like to fit that is this conditional distribution, model for this conditional distribution, or you can think of this as a regression. That's the model actually we have in mind, a regression model. And beta here is the parameter of interest. And beta zero, this is the true value of beta. Without any external information, we are already able to. Are already able to estimate this beta zero. So, based on the internal individual data, that is to maximize this MLE, sorry, maximize the likelihood, and we get the MLE. And we all know that the MLE is consistent and the osmotic variance is the inverse of the feature information matrix. Now, for the external study, now to make things Now, to make things general, so we consider, you know, there are K external studies available. So there are multiple external studies available. And these studies, they provide information. Well, each study provide two pieces of information. One piece is this H and another piece is the theta star. We can take a look at this one by one. So the external study, let's say the kth external study. Say the case external study, they fit their model, they build their model based on some individual level data. But we are not assuming, well, we are assuming that we do not have access. So those data are not available to us. And the covariates, now the external study, they study the same outcome, but they are using different, they use different covariates. So the XK here, this is a possibly coarsent version of Is a possibly coercive version of x, for example, subset of x or categorized version of x. And h, k, this is the estimating function they used by the case external study. So, for example, if the case external study build a parametric model for regression y on xk, theta k is the parameter, then the h is simply the corresponding score function. In the case where only some disease prevalence information is available, then the HK could simply be in y minus theta k. In this case, theta k simply denotes the mean of y. And this could be stratified based on some covariance. And a theta k star here, this is the estimated value of theta, the parameter theta, from the kth external study. And we are assuming there is no And we are assuming there is no uncertainty associated with the theta k star. So, this is the case when the sample size used by the kth external study is much larger than the internal study, which is the case we are considering in this product. And also, this is a commonly made assumption in this literature. So, many oftentimes people just assume external study samples. External study sample size is much larger than the internal sample size. So, in this case, then the information, external information, is summarized by this moment condition or estimating equation. And here, the expedition EK, this is under the kth study population. So, the external study models, if you look at the external study models, they use less detailed covariates. Covariates and while we do not have access to their individual data, and also they didn't use the covariate Z. So we assume that Z is only available in the internal study. Now, without population heterogeneity, so in other words, if we for now, if we assume that all the populations are the same, then we only have one data distribution and we only have one expedition. So we use zero. Have one expedition, so we use Z to denote that expedition. So then the external information is summarized by this. And if we do a simple transformation, that is to first condition on X and Z, condition on X and Z. Now, because we have specified this model, why given X and Z? So conditional X and Z, we can calculate the expedition of H under the conditional distribution. So if we call this the U function. if we call this the u function and this u function this is a function of x and z and of course also a parameter of our interest a function of a function of beta and also evaluated at theta k and this is equal to zero now if you look at this this equation here it provides some constraints on the marginal distribution of the covariant marginal covariant distribution so realizing this people proposed the so-called constraint Proposed the so-called constraint maximum likelihood estimator. So if we simply impose a discrete distribution on the covariates, and then if we maximize, still maximize the likelihood, but now the likelihood has two components. One is the parametric part, and one is the non-parametric part, the marginal for the Parametric part of the marginal for the marginal covariate distribution. Now, the covariant distribution, the discrete distribution on the covariance, it satisfies this constraint, which comes from this constraint here. So this is the so-called CML method, constraint, maximum likelihood. And people have, well, this has been well studied in the literature. So people found that this estimator, because of the use of the external information, Of the use of the external information, it is more efficient than the MLE without using the external information. So, in the presence of population heterogeneity, now different studies now, they have they target different population. In this case, we want to make it clear that the target population, our target population is the internal study population. So, with this target clear, then if we introduce this notation, if we define gamma. If we define gamma k0 to be this expalation, then for those studies whose target populations are different from the internal population, this gamma k0 is no longer zero anymore. So previously, I mean, this expectation is always equal to zero when the populations are all the same. But now this may not be zero anymore. So then the constraints for some constraints for CML are no longer valid, no longer correct. Longer valid, no longer correct. So we need to select then the external studies that corresponds to the gamma k0 equal to zero and discard the rest of the studies, rest of the external studies. Now, very straightforward solution would be to first to test whether this value is equal to zero or not. This can be very easily carried out. And then based on the result of this test, then we decide to whether to make use of Then we decide whether to make use of that information or not. So, this is a two-step procedure. Now, the drawback is that the error that is introduced by the first step of testing, that is very hard to be accounted for. So, we reformulate the problem. We formulate the problem into an estimation problem. So, if we simply subtract gamma zero from this. from this um from this uh from the cu and then the expatiation becomes zero so if we introduce some additional parameter gamma k um and the gamma k has true value gamma k0 then the gamma k0 well as as we previously just pointed out so gamma k0 equal to zero means that the study k information is useful and should be uh accounted for now we need to estimate gamma Now we need to estimate gamma k exactly as zero for those studies with gamma k0 equal to zero. So this becomes a problem of shrinkage because we want to shrink some of these estimates of gamma, gamma k exactly to zero. So this can be realized by the shrinkage technique. This is commonly seen problem in statistics. There are many different shrinking techniques available that can Techniques available that can achieve this goal. Now, so Lascell is definitely one of the most widely used techniques to shrink some parameter estimates to zero. Now, but Lascell shrink each individual parameter. But here, for different external studies, we are treating the moment constraints coming from the same study as a group. So it's very natural to shrink the whole group to zero. So it's so. So, in other words, we might want to have a group-wise rankage. So, this can be achieved by a group group law. And then, of course, we want to have consistency of estimation. So in the end, we want to select only those external studies who will have the same targeted population as the internal study. So, in order to achieve that, we want to follow the Follow the adaptive Lascelle technique. And then, if we want to achieve both consistency and the group-wise shrinkage, then there is the so-called adaptive group Lascelle technique available. So, that is the idea behind our selection of which shrinkage technique to apply. And so, this is the estimator we develop or we propose. So compared to the CML, we added one penalty here. It's a penalization on the parameter gamma k. Now the gamma k is the parameter, you can think of this gamma k as some parameter quantifying the bias of this expedition of this u case. So we are trying to then we want to estimate some of this gamma case, the bias parameter exactly. case the bias parameter exactly to be zero for those for those parameter values indeed are equal to zero so so compared to cml then the difference is that now we have this additional penalty on the bias parameter um so the for the penalization for the penalty term we uh we use the the adaptive group of soul penalty so here lambda n this is the some new tuning parameter Some new tuning parameter. And a gamma tutor here, this is some first step, a consistent estimator of the gamma k0, the true but unknown bias of the moment constraints. And in our case, we can simply take it to be the sample average evaluated as the internal MLE. And this W here, this weight, in the literature, people simply take this to be either one or two. Those are two commonly used values. Two commonly used values. And this procedure then allows us to select, to simultaneously select the external studies and estimate the parameter of interest beta. Now, the theoretical investigation of this is actually a little bit, it's not trivial. The reason is that we do have a constraint of maximization. So the objective function is not at least squares. So that did cause some trouble. Some trouble. So, but we were able to, through some investigation, we were able to establish some properties of this estimator. So under some conditions, so in particular, under conditions about this tuning parameter. So the tuning parameter has to convert to zero, but it has to convert to zero at a certain rate. It cannot convert to zero too fast or too slow. Too fast or too slow. So these are the requirements on how fast this lambda tuning parameter converts to zero. So under such condition and of course some other regularity conditions, now we are able to show that the estimator for both beta and also for the bias, newly introduced bias parameter, they are both routine consistent. And also we are able to consistently select the external studies. Left the external studies who, well, that corresponds to gamma zero equal to zero, or in other words, the studies that target the same population as the internal study. And for the osmotic distribution, so we are able to show that osmotically the estimator for the parameter of interest follows normal distribution with such osmotic variance. Now, here I didn't list. Now, here I didn't list the exact expression for the different matrices, but the osmotic variant has such a structure. So, compare to the osmotic variance of the MLE, which is S inverse, so the inverse of S. So, this sigma is smaller than the inverse of S. So, in other words, it does provide efficiency DM compared to the MLU without using the external information. Without using the external information. And also, this ostematic variance is actually the same as the asthmatic variance of the CML based only on the external study with gamma k0 equal to zero. So in other words, we are able to achieve efficiency as if from the very beginning we only, well, we knew which external study target the same population as the internal study, and we only made use of those external studies. Of those external studies and completely discard the rest. So, for numerical implementation, the implementation is not, well, it's of course more complex than having a least square objective without a constraint, but it's not too bad. So, we are able to implement this based on Lagrange multiplier method. So, we have two loops, the inner loop max. Loops, the inner loop maximize this function for any fixed beta and gamma. This rule is the Lagrange multiplier. And then afterwards, we do an outer loop, while the outer loop minimize this sort of negative objective function. And this g function here, this is based on the constraints provided by all the external studies. This is how we implement the method. The method. Now, for tuning parameter selection, this is very crucial for our procedure because it's actually, in some sense, it dictates the performance of the procedure. So, a commonly used or widely used procedure or strategy for tuning parameter selection that is cross-validation. But we didn't follow that. The reason is that, well, there are several reasons. One is that One is that the internal sample size is usually small in order to apply this method. Because if the internal sample size is very large, then there's not much point to make use of the external information for the purpose of experience again. So the internal sample size is usually small. With a small or moderate sample size, it's probably not ideal to further divide that into training and validation. That into the training and validation data sets. So, another reason is the computational complexity because we have a constraint optimization. So there is certain level of computational complexity. So we don't want to do a very time-consuming cross-validation. And also, here, our goal is not necessarily in prediction. In prediction. So, oftentimes, we are also interested in estimating the effects of covariate effects. So, that also makes the cross-validation less ideal. So, for tuning parameter selection, we actually this is what we did for tuning parameter selection. So, we take lambda to be such firstly fixed order of the tuning parameter lambda. So, recall that we have two requirements on the convergence. Two requirements on the convergence rate of lambda. So to balance these two requirements, we just select the rate right in the middle to balance these two rates. And then for the loading term C, we follow some existing literature to select this C. And it turns out this procedure worked pretty well numerically. Okay, so another important consideration is, well, I call it the non-group-wise. I call it the non-group-wise selection. So, so far, well, because we adopt the adaptive group LASO, this is based on the intuition that because different external studies, they provide information in a group-wise fashion. So, they provide a set of moment constraints. And so, it's very natural to select the information in a group-wise fashion, treat different. Fashion, treat different studies at different groups. However, so the parameter gamma k0 for the kth external study not equal to zero, it doesn't mean that every component of this vector is not equal to zero. So it may still have zero components. And actually, this is not uncommon at all. It's very easy to construct examples where, you know, Where, if we factorize the joint distribution of y, x, and z into the three pieces, then under the difference in any combination of the three pieces, we are able to very easily construct an example where the gamma k0 is not equal to zero, but it does have zero components. So, in terms of reality, in terms of practice, what this means is that external studies, although they may External studies, although they may target different populations, but because the similarity between the populations, although they are different, but they may still be very similar. So because of the similarities of the populations and because of the similarity of the models used, now the external studies, they may still provide partially useful information. Although if you look at the study as a whole, the information may not be the target population, the population may. The target population, though the population may not be exactly the same as the internal study. So, this is particularly relevant when all external study populations are somewhat different from the internal study. Because in that case, if we treat them as groups, then we wouldn't use any of them. But because of the similarity between different study populations, well, we might still provide useful, partially useful information. Still provide useful, partially useful information. So, because of this consideration, we actually considered a component-wise selection instead of group-wise selection. By component-wise selection, we mean that within each study, because each study provides a set of momentum constraints. Now, within each study, then we look at each moment constraint separately and see if And see if there is partially useful information. And this can be easily done. So it doesn't really add any extra effort. Well, this mathematically is equivalent to just treating each moment constraint as a single individual external study. So with the samples, sorry, with the group size equal to one. Is equal to one. So this can be easily implemented. Now let's take a look at some simulation results. So for the internal study, we considered four covariates. So X1, X2, they are generated as bivarium normal. But after we generate bivariate normal, X2 is dichotomized. Is dichotomized, is a dichotomized covariate. Now, N3 follows exponential. N of Z, which we use Z to denote the biomarker. So it follows normal distribution. And then the disease mechanism is simply a distribution model. And the internal sample size is 800. And we summarize the results based on a thousand replications. The results based on a thousand replications. Now, we consider two external studies. So, the first external study is based on exactly the same distribution as the internal study, but it collected data only on y and x2 and x3. And it fitted such a regression model, a logistic regression model. Now, because the external study one has exactly the same population as the internal study, so the corresponding gamma. internal study so the corresponding gamma gamma one zero that that gamma parameter is just a zero vector now for external study two the covariance distribution is different from the internal study but given the covariance the disease mechanism is still the same as the internal study now study two collects data on only on these three variables so only on covariance x1 x2 and fit a little X1, X2, and fit a logistic regression model. Now, if we calculate the vector gamma 2, 0, so it turns out that there is one component that is very close to zero. Well, there's another component, of course, is not close to zero, but there is one component very close to zero. This is numerically an illustration that indeed there are cases where the populations are different, but Are different, but uh, but uh, you know, the gamma still has zero component. And the external study models we were fitted based on a very large sample size, 50,000. We probably didn't need such a large sample size, but this is what we used anyway. So here, this slide shows the results by considering only the first external study, external study one. Now we compare the MLE, the CML. The CML, the Oracle CML estimator, by Oracle, we mean that we only make use of the cracked external study. Now, in this case, external study one is indeed target the same population. So the CML, Oracle CML indeed is just the CML always use the external study one information. And then we have the group-wise shrinkage and component-wise shrinkage. So here we have the bias, we have the Here we have the bias, we have the empirical standard error. And for the article CML, because it makes use of the external study one information, so for corresponding to the covariates that are used by the external study one, that is the intercept and these two covariates, we can see that they're in paragraph standard errors, they are much smaller than the MLE without using external information. Now, for the purpose. Now, for the proposed method, the group-wise, it actually always selected the external study one. So, that's the reason why, if you look at these empirical standard errors highlighted in green, they're very close to the Oracle CML estimator. And the component-wise as well. So, they correctly select all the components, all the correct components. In this case, all the components are correct, so they are always selected. Always selected. And these are also very close to the Oracle CML. Now, for external study two, if we only consider external study two, then recall that study two has a different population from the internal study. But there is one component that is very close to zero. So indeed, we can see that the oracle, well, this is the CMI estimator that always used that. Estimator that always used that particular component. And we see that while there is a substantial reduction in terms of empirical standard error. Now, the group-wise selection, it never selects external study 2 because it is treated as a group and it never selects that group. So it has almost exactly the same performance or actually exactly the same performance as the MLE. But for the component-wise selection, now it does select the Does select the correct component from the external studies. So the performance of this estimator is very close to the CML, the Oracle CML estimator. And when we consider both external studies simultaneously, then the message is actually the same. So the group-wise, it never selects study two. So it only improves components on corresponding to the components. Corresponding to the components of a study one, but the component-wise, so it correctly selects all the components, the zero components, so it has almost the same or very similar performance to the Oracle CML estimator and is substantially better than the MLE. And this slide shows the for inference, we actually Turn to bootstrap for calculating the standard error. This slide just shows how bootstrap performs on calculating the standard error. So, this is for group-wise selection. We can see that, well, there is a slight overestimation of the standard error. So, the bootstrap standard errors overall, they are slightly larger than the impiral standard error. So, this means that the conclusions are a little bit conservative. So, the coverage probabilities of the 95%. Probabilities of the 95% confidence intervals, they are overall, they are slightly higher than 95%. And this is also true for component-wise selection. So we see a slight overestimation of the empirical standard error. But I think the overall performance seems to be totally acceptable when we use Bootstrap to calculate the standard error. Okay, so now some data application. Uh, data application. So, back to the study of the risk of having high-grade breast cancer. So, the internal study now we have a sample of 1,244 men for diagnostic biopsy at seven U.S. community clinics. And the model we want to build is a distribution model. And here, X contains the conventional covariates, like here, Lag transform PSL. Like here, Lag transform PSA, age, and binary indicator of VRE, binary indicator of biopsy result, and binary indicator of African ancestry. And for the newly discovered biomarker C, while we consider true biomarkers, PC3, large transformed, and also dichromized T2ERG. And for the external data we consider, that's the PCBT calculator. Calculator, we are able to find exactly the Ledus, about the exact Ledus Grange model, and also the exact parameter estimates from this paper. And here I do want to point out that there is a heterogeneity between the spec population and our internal population, like the age distribution is quite different, and also the PSA distribution that is quite different. And also the Also, the prevalence of the disease is also quite different. Now, if we calculate the gamma tilde, the parameter, we see that indeed there is a component that is very different from zero. This is not surprising because the two populations are different. However, we do see that there are some components that are very close to zero. This also confirmed our observation that Observation that it's quite common to have to, well, for different studies, they target different populations, but some components of gamma vector are still very close to zero. So in other words, the external study may still provide partially useful information. So now we are trying to integrate that information into internal model feeding. For group-wide selection, actually we never select an external study because of the very Because of the element that is very different from zero. So we never select the external study when we look at a group-wide selection. But for component-wise selection, we actually estimate four components to exactly equal to zero. So that is the last three components, and also, well, this one here. And then the table shows the results. The results, model fitting results for MLE without using external information and for our proposed method. Because of the time limitation, so I'll only point out one major observation. That is, for risk variable, you can see that without using external information, it's not significant, but after using external information, it becomes significant, which is in full agreement with existing literature. Agreement with existing literature. This is because in the internal study, there were, I think, probably just 80 or about 80 African American subjects in the individuals in the internal study, but for the external study, there were considerably more. So, this actually helps us to improve the model fitting. Okay, so now sure. Okay, so now some discussion. There are, of course, many unsolved problems, unanswered questions for this matter. One of them is that how to account for uncertainty associated with the external studies. So as I mentioned in the literature, it's very common. It's a common practice for people to assume that the external study sample sizes are much larger than the internal study. But still, there are cases where we might want to integrate information from might want to integrate information from studies that have a comparable sample size as the internal study. In that case, the uncertainty cannot be ignored. So how to account for the uncertainty? That is a very interesting question that needs to be addressed. And also, what if there are multiple or many external studies? Actually, this is the case where that I originally planned to talk in this workshop, but I guess Workshop, but I guess I was a little bit too ambitious. So the results were not ready yet. But when we have many external studies, now the different studies, because their populations may be similar and because their models may be similar, so there may be multi-collinearity among the momentum constraints they provide. So this multi-collinearity will jeopardize the numerical performance. The numerical performance. So, currently, we are investigating that problem. And also, while by many external studies, we actually mean that the number of external studies can increase with the internal sample size. So this product I'm presenting here, it actually is the number of external study is fixed. And also the internal study sample may be biased. So here, this product focuses on the random sample. This project focuses on random sampling for internal study. Even in that case, when we really collect the data, the data may not be collected as a random sample. So how to address that, that is selection bias, that is an interesting problem. And also the internal study, when we consider biomarkers, then the biomarkers may be of high dimension, especially genetic biomarkers. So how do we deal with that? And that is also a very interesting question. So finally, Question. So finally, some take-home message. Now, the existing studies usually contain rich, very rich auxiliary information. But if we integrate the external information blindly, then we may introduce bias for internal study when there is a publication huge need. Now, we propose a method that can simultaneously select the studies that target the same population as the internal study and simultaneously fit a model. And simultaneously fit the model using that information. In the end, we are able to achieve the so-called Oracle property in the sense that we are able to achieve efficiency as if we knew from the very beginning which external studies should be incorporated and which one should be discarded. So while this actually, this whole presentation is based on a recent publication in the Journal of Computational Graphical Journal of computational graphical statistics. So, if you are interested, you can definitely take a look at this paper and see more details there. Okay, so with that, I think this is the end of my presentation. Thank you so much for your attention. I'm happy to address any questions. I know it seems that I'm a little bit over time, so thank you very much.