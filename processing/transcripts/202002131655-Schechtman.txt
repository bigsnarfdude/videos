So embedding means for me lipships embedding. So you know it's very good with the definition. You have a metric space and a normal space and we say that M embeds in X with distortion C. If there is a map going from M to X, which you can always scale it so that the lower bound is a. I'm sorry, I re. I'm sorry, I already read a mistake here. This should be f of x and f of y. Okay, the distance between f of x and f of y is larger equals than the distance between x and y, and smaller equals than c times the distance between x and y. And the best possible c here I denote by c sub x of m. This is the distortion of embedding m into x. And what I'd be interested in this talk is in fact. Be interested in this talk is in the following quantity. So you fix N and C and you look at the smallest k such that for any subset of size n in x, x is a norm space, embeds in a subspace y of dimension k. Okay? So it's a K, okay? So it's a with distortion C. So you have endpoints in some Banov space, you want to embed them in some space of the smallest possible dimension. And the smallest dimension with distortion C, and the smallest dimension I denote by Kn of C. And for most of the talk, you can think about C as being 2 or something like that. I'll say a little bit about general. I'll say a little bit about general constructs. So there are very it turns out that there are very few results before this result that I want to speak about. There were really four results. Two in the positive direction. Positive means for me that you can that K and C is very small, something of logarithmic order, and two in the negative direction. Direction. So, in the positive direction, the most known one is the Johnson-Minister's framework, which says that in the Hilbert space, you can embed endpoints in a subspace of dimension log n, with distortion, approximately log n, with distortion two. And this is very well studied, and everything is known about it now. Exactly the behavior of the constant, and so on. It's not so important for me, but it's known exactly what is the dependence on epsilon if you want. What is the dependence on epsilon if you want the distortion to be one plus epsilon? There is another space, not classical space, it's actually what's called the two-convexified Serverson space, in which something, a similar phenomena works. This is due to Johnson and Orr. So these are the two positive results. The two negative results. The first one is due to Matushek, which is very Which is work for infinity or C0 if you want, or any space which contains that infinity n 3. And it says that in this case, you don't have good dimension reductions. So there are endpoints in L infinity, which does mean some metric space of endpoints, because every metric space embeds isometrically in L infinity, which you cannot embed into any subspace of L infinity, which means any subspace. Infinity, which means any known space with dimension smaller than n to the power of some constant over C. Okay, so there is no good dimensional reduction. You can not do better than polynomial. And this is also, there is also an upper bound, a matching upper bound due to Johnson Angelus and Scherkman. And let me say Johnson Angelus and Leslie. Okay, this is one result. Result and the other negative result, which is more connected to my talk here, is due to Brigham and Sherry Carr. This is in L1. I'll explain in a minute why people, Brigham and Sherry Carl are computer scientists, why they're interested in such things. And in A1, again, there is no good lower bound, there is only polynomial lower bound. Of course, there is always an upper bound of N, there is no problem. You just look at the spin. There's no problem, you just look at the spin of the point. So this means that you cannot do much better. And also, here something is known about the constant, not very well. Okay, why is this, why are computer scientists interested in this? Because you can do, well, for instance, this Johnson-Linger's lemma means that in L2 you can do computations. There is this Quadratic programming, which is supposed to be efficient. You can do computation. So it appears that you can do computations if the embedding is effective in some way. You can do computation on any metric space. Maybe. In L1, there is something similar. Linear programming works nicely in L1. So if you could do something like that in L1, it will also be good. It will be actually better than L1. Be actually better than L2. Unfortunately, so this is a negative result. Unfortunately, you can not do it very well in L1. Okay, now the... Kidding on this example by Matushek. I mean. Matushek? Yes, this example. How did it look like? It's used algebraic geometry. Okay. There is a new proof of that by a staff which is also quite complex. A staff which is also quite complicated with different methods, but it's not easy. Okay. There was some weaker result before that of Johnson Minches and myself, but not as strong as his. Okay. So the purpose of this talk and our result is to add one more example, unfortunately, again in the negative direction. Negative direction. And this is a trace plus node. So let me remind you what is a trace plus node. So we are talking about operators from D2. You define, I need not just the, this is also called the Shatten one norm or whatever, or nuclear norm, but I need also the Shatten P norm for later, so I define it for all P's. And the Shatten P norm, you know, by S P. denoted by S P is you just look at T star T. This is a positive operator. You can raise it to any power. You raise it to the power P over 2 takes the trace of it. And 1 over P, it's the same thing as looking at the singular values raised to the power, the sum of the little P norm of the singular values. Okay, this turns out to be a norm. And so just to remind you, for infinity, you take, of course, the supremum. Infinity, you take, of course, the supremum, this is just the operator norm. S2 is the most known one, this is the introduction. And I'm interested really in S1, which is called the trace-class norm or the nuclear norm, or Schattenorman. Okay, now this is also something which is the Schatten norm, is also something, or all actually all the S P norms, is also something which is computable. I mean, one can compute eigenvalues. So for computer scientists, this is almost as good as Is almost as good as L1. Unfortunately, again, the result that we proved is negative. So here is what we proved. It just appeared actually in this K-Don computational geometry. We proved the same result as this frequent Sherry curve thing, exactly the same thing. So, okay, so it's the same lower bound as both there. And let me emphasize again what does it mean? This means that there are both. What does it mean? This means that there are, that for every n, you have endpoints in this Chattan1 class, such that if y is any subspace of S1 of dimension K into which this endpoints embed with distortion C, then the dimension of this subspace must be large. This means large. Okay? Now, so I'm going to describe the example and so on. But notice first that little L1, of course, embeds in this one. Little L1, of course, embeds in S1 on the diagonals. Diagonal of S1 just isometric to little L1. And actually, so you need to produce here some bed set. And the bed set that we produce is exactly the same as the one of Brinkman and Sherry Cart. So it actually sits on the diagonal. These are endpoints, n matrices, n-diagonal matrices, which do not embed well into any subspace of S1, not just diagonal subspace, but any subspace. Subspace, but any subspace. Okay? And these are the diamond graphs, which I will define in a minute. So our theorem is really a strengthening of frequencies or colors. Okay. So what are the examples? So these are the diamond graphs. It's a sequence of graphs. The first one is just what did I do? The first one is just uh uh uh two vertices with a Two vertices with an edge joining them. And then what you do to go from D zero to D one is you erase this edge and you replace it with four edges like that. And to go from D1 to D two, what you do, you erase each of the edges and replace it with such a diamond, such a D1 thing. And the next one will be you erase each of these things and replace it with a diamond and so on. Is it clear? This is a sequence of. This is a sequence of things, of examples. Turns out that it's quite easy to show that they embed in little L1 with distortion two. So this can be viewed as subset. Of course, I'm looking at them as metric spaces with a graph metric. And okay. So now I want to prove that such things, that they do not well embed into Bit into. By the way, I call them DN, but they don't have end points, they have exponential points. Okay, the N heroes. I will not, I will not do, try not to be very technical, so I will not. Okay, let's see in a minute. Now, the proof imitates one of the proofs of this Brink-Man-Scheller theorem. The original proof, the original proof is, for me, is very complicated using duality of linear. Duality of linear programming. I don't really understand, completely understand it. But there were several later proofs. One particularly nice of Lee and Orr. Yeah, I wrote it here. So we essentially go along the line of this proof. And it consists of two steps. It goes through this shuttle and peak classes. So the first step is that you. So the first step is that you show that this DNS does not well embed into shutten P for P fix some P larger than one, and you get some lower bound on the distortion of embedding DN into S P, lower bound in terms of N and P. Okay? We'll see later on what is a lower bound. And so this is, and there is no This is, and there is no restriction on the dimension here. This could be infinite-dimensional S1. Okay, and the next and the second step is to show that a subspace of S1, a k-dimensional subspace of S1 does embed well into Sp. Okay, if you have some upper bound on the distortion. Of course, this is r really linear to for a subspace, a Lipschitz embedding or a linear embedding are the same, for say finite dimensional subspace. Fourth, say, finite-dimensional subspace. So this is kind of a linear thing, okay? To show that a subspace of S1 of dimension K is close to a subspace of SP of dimension K. The two things together, I mean, of course you need them with exact estimates and things like that, but the two things together give you the proof. Now I have some dilemma, and this is that I want to show That I want to show some proof. The first part is basically the same as the thing for L. So they say there are, well I said, talked about two proofs, there are actually three proofs. There is another proof using, well, never mind. So the proof for the first thing is basically the same for SP and little LP. And little LP. And there is a very nice proof, well, proof that I like very much, which is a simplification of the proof of Lyon Or. The proof is due to Johnson and myself. So, but as I said, it's not very new. And the other one, so the innovation is really in the second part. Here the treatment of S1 and L1 are very different. However, it's not very nice to present, especially not in the 14th. Especially not in the 14 minutes that are left for me. So I think I'll do, since I think that almost nobody here, maybe even completely nobody here, saw this proof of the first part. I think I'll concentrate on that. This is the last talk of the conference and kind of I think it will be something nice and simple. And I hope to have some time to say what enters into the second part, but I will not speak so long. But I'm not speak so I'm not really speaking about the this paper, I'm speaking about something older. Okay, from now on. So I think that what I said is included here. The proof of the first bullet is very similar to the literary P, and this is what I'll show. And it depends on, it has to do something with the modules of uniform convexity of sp or lucler p. Okay, so I'll concentrate on that. Concentrate on that. So let me remind you what is the model of uniform context of the known space. It's defined here, but it's better to look at the picture. So you look at the unit ball, you fix an epsilon and look at two points on the sphere, which are at least epsilon apart. And you measure this part here, the ratio between this part and that thing. Okay? So. Okay, so this is 1 minus, this is the midpoint, the midpoint of x and y. 1 minus z. So if the bone is uniformly convex, there is some lower bound on. This thing is positive. Okay, the infimum or such pairs. And this is the modulus of uniform convex. Little F P and S P for P larger than one are uniformly convex and we know we know even how the how this function behaves there. How this function behaves there. It will be important for us. I'll say it later. Okay, this is a uniform convexity. And this is a very simple demo, but this is actually the main part of the proof. So what it says is I'm looking at D1. Remember, D1 is just this diamond with four points. This is a very simple thing. And if you embed D1 into X, embed d1 into x with distortion m, this is what is written here. Then, okay, so this is a diamond. I'll denote this point I call top and this point I call bottom. Okay, the distances here are one, all these four distances, and of course, the of course Of course, f of top minus f of bottom is larger or equal than 2 because of what is written here. Because the distance between the top and the bottom. Okay, so of course I'm not talking about z, but I'm wrapping this with some f into some known space. Okay, so it's maybe something like that. Some four points which don't have to look like a diamond. And so of course this is trivial, and it's also This is trivial, and it's also trivial that this is smaller than 2m because of what is written here. But you gain something from the uniform convexity. You gain some factor smaller than 1, depending on the uniform convexity models. This is what this label says. And let me show you the proof of that. This is very simple. So I'll show it in the black box, even though I hold it there. So I look at this thing, but let me do it not with F, but with F over M. Okay? Okay? And also, so this is F of bottom divided by M. But I can always shift things in the range, so I can assume that this is the point zero in the node space. Only let me call also this x and this y, so this is f of x over there, and this is f of y over here. And this is f of y over m. And let me look also at the midpoint. Okay, so first of all, f of x over m, the distance between this and this is at most 1, divided by m. Okay? So if this is 0, this is in the unit ball. Also, this thing is in the unit ball. Unit ball is really something like that. And this is a midpoint between x and y. So B x and y. So from that and the definition of the of the of the uniform convexity I get, I'm sorry, and also the distance between this and this is larger equals than 2 over m, 2 over m. Yeah, the distance between this and this is 2, so the distance between this and this is larger or equals than between f of x and f of y is larger equals n2, and divided by x, so it's larger equals n. Divided by n, so it's larger equals than 2 over m. This is written here. So from the uniform convexity modulus, I get that the distance between this and that, this and 0, is smaller than equal to 1 minus delta of 2 over n. This is this 2 over n. Okay? So this distance is smaller than equals than 1 minus delta 2 over n. Now the same. Now the same argument works here, works with this point of this two. So also d thing is a also d thing is also smaller equal to 1 minus delta 2 over n. And you get what you wanted. So the distance between this and this by the triangle inequalities. And so also the this is what I just said. This is also smaller than zero. This is also smaller equal than that, and you just add the two things. Use a triangle inequality, and so there f of bottom over m minus f of top over m is smaller equal than twice the thing. Just multiply by m dispose 11. Okay, very simple. Now, what is the relevance of that? Um I th there is a simple call out there which says the following. So now I'm looking at the whole sequence of the diamonds of these diamonds and denote by these diamonds and denote by mn I denote the list m such that the distortion of embedding of embedding dn into x. This means, so it's a list m for which this happens. And I claim that you have this inequality. There is an inequality between mn minus 1 and m n. So let me quickly say why this is really a common number if this is not. So let's look at Dn. So Dn consists of, remember how we got DN? We had some, I don't know, let me do it like that. We had some, we had dn minus 1. This is, I'm drawing here dn minus 1. Let me let me take another colour. So this is dn minus 1, and now I'm looking at dn. One and now I'm looking at dn. So dn is in red, I hope you can see it. So this is dn, okay, built out of dn minus one. And now look at just one of these small diamonds. And look here called this top and this bottom. And by what I know is that the distance between this and that, well, let me first say something. Well, let let me first say something before that. To compute m n minus one, remember m n minus one is a Lipschitz constant of f. It's enough to this is a graph, it's enough to look just at edges to show that the so it's enough to look just at things like that. And the lemma says that this thing minus that thing is smaller equals than something involving mn. Okay? So you immediately get some upper estimate of m. Estimate on mn minus 1, on things like that. The estimate is exactly this one. I will not say any more because I have five minutes and I want to say this is all proof. Okay, now why is this a proof? Once you have such a thing, then it's immediately clear that you get some lower bound on m n. I mean, they go up this m n, and you get some estimate if you know. And you get some estimate if you know if you have a good estimate on the model of uniform convexity. For a general thing, it's quite complicated to do that. But in L P so you get a bound on, a lower bound on M n in terms of delta of x. In L P, it's known exactly what is the behavior of the modulus. It's the same in little L P and in the shuttle and peak. And in the Chart and P class. And it behaves like for P between 1 and 2, it behaves like epsilon squared with a constant in front. The constant is P minus 1. It's very important for these things that you know exactly. It's some absolute constant times P minus 1. And from this, once you plug it in, you compute, you get some lower bound on M n, and I tell you just what you get. You get this thing. You get from this that M n is larger or equal than P minus 1 times. larger equal than p minus 1 times n to the 1 over p. And now you and now you given n, you find the best p, whatever. You get some lower bound. This is what I meant by dn does not collect grade into s. Okay, this is a quadruple. Okay, this is the proof of this first bullet. Okay, I've let Okay, I have less than five minutes, so let me say something about the second part. Before that, let me tell you some side issues. It follows from the discussion that this sequence of diamonds do not embed in any, with uniform distortion, in any uniformly convex space. It turns out that this actually characterizes, and the same thing is true for spaces isomorphic to spaces, which Isomorphic to spaces which are uniformly convex. These are what's called superreflexive spaces. It turns out that this is a characterization of superreflexivity. This was proved by Johnson and myself. These characterized spaces isomorphic to red spaces. And the proof that I showed you actually is taken from this is not. This proof does not appear in the paper of Gilles and Nassau and myself. Also because we are actually proving something stronger, I'll say it in a minute. Something stronger, I'll say it in a minute. But I thought it would be nice to present it here. Okay, let me very quickly say something about the second bullet. Remember what it was? A k-dimensional substance of S1 well embeds in Sp if P is close to 1. And here there is a difference between the little P and the S P behavior. So maybe I'll not okay, let me say something about the little the Let me say something about the LP situation, the L1 situation. How do you show that a subspace of L1 is close to a subspace of Lp? There are really two ways to do it that I think can think about. One is if you have a k-dimensional subspace of L1, then it's known that it embeds in little L1 of dimension basically K, actually K log K. This is the theorem of Talagan. For the purpose of what we need here, it's it would be enough. What we need here is would be enough that it embedded in a subspace of dimension k to the ten or something like that, which is much easier. This is something I proved a long time ago. Okay? This is one way to do it. Another way to do it is you have a subspace of L, you have a subspace of L1. You do what you want to use the identity map. Think about capital L1, L1, of a probability space. You want to use the identity map between the L. To use the identity map between L1 and Lp to compute the distance. Sometimes it doesn't work, but you have the freedom of changing the position of your subspace. You first apply some, actually, isometry of the LP space, which changes the position of the subspace, and then you look at the identity map. This is what we call change of density. And it works in LP to get these things. Both these things do not work in S1. Okay, the first one, maybe. The first one, maybe I have one minute, maybe, so let me talk about the first. This is just a description of the first one. This does not work in S1 or in C3. It's not known. So this is a, I want to just present a problem. Given K, what is the order of the smallest M, such that every K-dimensional substance of S1 embeds in S1M? This is a relevant thing. If this was small, if M was just a polynomial. Was just a polynomial in K, then we could then use just the identity between S1M and SPM and get the right thing. But this is not known. It's not known if M is polynomial in K, and I actually conjecture that it is not, but I don't know. This is a problem. The reason I conjecture that it is not is because there is some related result of Regev and Vidic, which again are computer scientists. But some related one, but not really solving this. So this I wanted to So this this I wanted to more or less I wanted to state some problems. So here is a problem. Let me tell you how we do this. I don't have time at all, I see, but how do we do that? So we don't use this method, we use the other method. This is change of density. Now, as I said, it does not exactly work, but something similar to it works. You need to find some special basis in your subspace to the Subspace to the few of you would know what I'm talking about. It's similar to the Lewis spaces in a subspace of LP. So you find some version of it in Schaten classes. It was first done by Nicole a long time ago, but we are not we are using not i this was not good enough for us, we use some other version of it. So it it but it it falls in the linear theory. It falls in the linear theory and I will not this part does appear, this part does appear in the paper with a lot of details, so anybody who wants can read it there. Thank you. Questions?