match the same level per se but I'm I'm actually gonna speak about a topic that's in some sense very much related but in another sense very different. So we're gonna use topological features essentially to process data but what we're not gonna do is we're not gonna start with plot clouds but we're typically gonna think of the complex as a given, right? So but okay let's But, okay, let me dive into this. But before I do this, I want to just acknowledge a few of my companions and co-workers on this. This is Austin Benson from Cornell, Mitch and Santiago from Rice, then there's Alvin and Gerd from TU Delft, and Mustafa, who has just moved to San Francisco from Santa Clara. So, since I was not sure about whether any of these words here, apart from Are there any of these words here apart from the last three? I mean, so I was not sure about the signal processing kind of background that people have. So I thought, okay, what I'm going to do is I'm going to do a very quick introduction into single processing that consists of, well, basically one slide, right? Basically, I'm not going to do the full duty here, but I want to basically put a few Yeah, put a few pins in your hand. I think that was a very good expression. I just want to emphasize essentially one point, and that is about linear filters. And so, okay, if you do signal processing in the simplest way, what you have basically is the following abstraction. And so basically, you think of data as signal or plus. Of data as signal plus noise. And that doesn't mean much because, of course, like, well, okay, it's an additive model, let's say, here, right? But what I really need to tell you, if I want to do signal processing, I need to model the signal and the noise, or at least one of them, and then the rest will be, let's say, the noise, right? And so I'm just going to think of my signal here as some vector that lives in a potentially very high-dimensional vector space, right? Dimensional vector space, right? And my signal is going to be some subset or some whatever subspace in there. And then the rest is going to be noise. And so now what I want to emphasize is the following view, and this is a linear filtering. So let's say I have this, right? And now I want to construct a linear filter. What is a linear filter? A linear filter is nothing but a linear map. Filter is nothing but a linear map, or even more applied in a way, is just a matrix. We want to take Y, apply some matrix to it, and we get something out. And this is going to be our filtered signal. And ideally, this filtered signal is something useful to us. For instance, maybe we want to recover S. That would be the simplest kind of thing you could do if you want to denoise this, right? And if you want to do linear filtering, And if you want to do linear filtering, well, I think one very useful way actually to think about linear filtering is rather than as a one-map that takes the signal and gives you some kind of interesting output, some filtered signal. I want to think about this here as a three-step operation. And the three-step operation is useful. You can also think about this in machine learning, lingo if you want, but let's say how this works from a Say how this works from a signal processing perspective. So, the idea is that, you know, maybe you know that your signal lives in a certain subspace of interest, and you have put this subspace or whatever, right? I mean, you put this into some matrix, and then you fill it up with the rest, and this is going to be nice. But, okay. So, what I want you to think of is this filtering operation is basically three steps. Is basically three steps. First, you take your signal, you project it into some new kind of signal basis. A machine learner would say we do feature extraction, and then he throws away a lot of things as well. But let's say we have a new signal basis here, right? Now we want to modulate certain frequencies. In the machine learning language, actually, you would probably just keep some of those features and throw away all the others. They would be the most extreme you could do. The most extreme you could do. You can also do this a bit more, you know, well, not as extreme, right? A bit more relaxed. We just modulate certifies. And then, of course, we transform back. So, and here's the classical example. Think of y as representing a discrete time series. So, it's a vector, the first entry, right, is a t0, and so on. We go on and so forth. What is the most What is the most classical feature? What is the most classical basis, set of basis functions that we want to use here? Well, this would be just the Fourier basis, right? I mean, this is classical time series processing, and we put in discrete Fourier basis. Okay, so now this is the mental picture. I'm now gonna declare Gonna declare as, you know, this is this is linear single processing, and it's hard somehow. But of course, the name of the game really is not just doing Fourier analysis, but the quest is really what are good signal representations, what are good features, what are useful things. And Fourier is the oldest and maybe the most canonical example of this, but the name of the Example of this, but the name of the game is really how to do this better. And you could sort of look back and you try to describe the history as finding more and more interesting representations, maybe, right? So over-complete dictionaries rather than instead of bases. You can do wavelets, you can do sparse dictionaries, you get to compress sensing, right? And you can do other things. The newest thing, of course, to do is to use adaptive features somehow. Adaptive features that are learned from the data. We don't do static things now more nowadays, right? So, we, like lots of people, do basically adaptive and compositional features that are learned by deep nets. I'm not going to talk about deep nets here because I think in some sense this dilutes the message that I want to send. But of course, one has to say, in practice, deep nets have taken a large chunk of A large chunk of that space because they're incredibly successful. And nobody understands why. Well, at least I don't understand why. Some people understand better, but I would say nobody really knows why they are so powerful. So what this talk then is about is basically very simple. I'm just going to tell you what is a useful basis function set for signal complexes, and I'm just going to give you the answer straight away. Use the spectrum. Use the spectral representation of the Haunchdel Placian. There you go, now you can forget everything that comes up with. But I want to sort of motivate why this is the case. And if you have Roussay's talk at the back of your mind, this is a perfect starting point, actually, because I'll just reiterate some of these things and maybe add my personal slant on these things, right? So now I've talked about signal, you know, signal. You know, single processing on the normal world, let's say, for one slide. Now I want to talk about single processing on browse. I'm going to do slightly more than one slide, but I'm also not going to do fuckedy of these things here. My goal is basically to flash some of the main ideas and trigger some discussion. And if I succeed with this, I'll be very happy. So, the question, if you want to first of all think of signal processing on graphs, right, the question is, of course, again, what is signal and what is noise? If we want to generalize all these operations, I mean, and it's hard, graph signal processing, the motivation is that we can do all this cool stuff with, let's say, images and time series. We can do smoothing, interpolation, imputation, what have you, right? We want to do this with signals. Want to do this with signals that live on graphs. Why won't we do it? Because there's lots of signals that are naturally supported on some structure that is not really regular. I mean, maybe an image is always a square and that's nice, but I don't know, maybe you have geographical sense-placed sensors, alright? And you have data that's associated with atoms that correspond to Atoms that correspond to a molecule, or you have whatever is your favorite example. Lots of cool things. You know, I haven't come up with all these things as well, of course, but I'm just going to give you my perspective. And so the question, again, the fundamental question is, what is signal and what is noise? So here are a few more examples. So maybe you're in computer graphics and you have some sort of discretized version of a manifold and you have, I don't know, a temperature. Temperature on the body, right? You measure it in a few places, and you want to interpolate now the body temperature. I don't know whether people do this or not, to be honest, but I think it's a nice picture in any case. And of course, you could do this with graphs as well. Maybe you have a highway network. I think actually this is from the US, some part of the US somewhere. Maybe you can recognize ties and forgot. Yeah, it could be Wisconsin, it could be one of the things. So, but, okay, now. So, but okay, now I want to say a bit why and how you could do this. And in some sense, the way I like to tell the story about graph single processing is basically retelling history, in some sense, because from a graph single processing point of view, you could say, well, actually, all we did up till here was always graph single processing, we just didn't realize it, right? And that's sort of the motivation. Because time series, well, what is time series? So, what is time series? At some abstraction level, you could say, well, time series is nothing but looking at a path graph or a line graph or something and associating to each node one data item. And what is an image, but a grid? You could ask, well, you know, why don't you put the the diagonal links? Yeah, you can do that as well. I'm not, but you know, but the point is these things are actually very regular. Very regular. We have basically this path, we have the product of two graphs, we have maybe the product of three graphs. If you go to video, this is all fairly regular. I mean, what is the video but lots of images, right? And now we want to go to graphs, which is basically a generalization of this in some way. And so we're going to say, look, this is all. We're going to say, look, this is all encompassed in this, in some sense. In what sense, actually? In what sense, actually? So now, this is the narrative, right? Keep in mind, I'm simplifying the story here, but I trust you can fill in the blanks. So here's my example. It's discrete time signal processing. And I said, okay, I want to think about this as some data that's attached to the notes on a graph. On a graph, and a mathematician in me wants to make this a bit nicer, so I make it wrap around, I make it circular. Okay, so this is my graph. Now comes the graph Laplacian. So I look at my favorite book of graph theory and I find that, oh, you know, there's this thing, the graph Laplacian, it's a pretty cool thing to represent my graph because it has lots of My graph because it has lots of cool algebraic properties and spectral properties that I can use to describe this graph. And I just, you know, write this out. And it's D stands for the degree matrix. The degree of every of these notes is one. Talking about out degrees here. And A is just the adjacency matrix. You know what this is. So I'll put a one if there's a link between any two nodes, right? And so now it turns out that in this case, the Laplacian is a. In this case, the Laplacian is actually a circulant matrix. What does a circulant matrix mean? Well, basically, it looks like the adjacency matrix of a cycle graph, roughly speaking. And the crucial thing in some sense here is that the eigenvalue decomposition or the spectral decomposition, what are the eigenvectors of such a circular graph, any circular graph, or any circular matrix, sorry, are Fourier modes, right? Are Fouribots. And now here's the thing that I said, like retelling history back, right? I said, like, okay, actually, and you can do the same thing for these other grid-like things if you make them wrap around, if you have periodic boundary conditions. Actually, what we always were doing in this Fourier analysis without noticing, what we actually were doing was looking at the eigenvectors of O plus. That, you know, this is not really true, but you could pretend that this was. But you could pretend that this was true. And then you have a recipe to go further. You have a recipe to now go to Christ, right? And there's also other reasons for doing this. But let me give you the other reasons now. And I'll do it by means of an example. So, what is the key idea here? We'll look at the spectral decomposition of the Laplacian. These eigenvectors. These eigenvectors are now our atoms, our basis functions in which we represent our signal, right? And now we want to get a notion of frequency. What is the frequency, right? And basically, I'm going to say the frequency corresponds to the eigenvalues that we attach to these eigenvectors. And so a smooth signal is something, smooth, you know, smoothly varying in that sense. Smoothly varying in that sense, signals something that has a dominant contribution from the slow eigenmodes, i.e., the small eigenvalues. And you can already see why this is perhaps a nice idea. If you know a bit of spectrograph theory, you know that you have a connected graph, then it has a single zero eigenvalue, and the associate eigenvector is a constant vector. What can be more smooth than a constant C, right? But let's dive into this example a bit. Dive into this example a bit further. And so, we're going to look at interpolation of graph signals, i.e., smoothing. We consider that we have measured a signal just on a few nodes. And now we want to do this thing. We want to do interpolation between those. So, or single, well, interpolation, but we actually we want to do fill that whole graph with not just Not just between those, right? So here's one way in which you could do it. It's basically kind of a least square thing. So what does it say? It says that, well, y is the measured signal. It says, okay, pick an estimate y-hat that's close to what's measured. That seems relatively natural. So we want to do that, but what we also want to do is we add this gradient. Want to do is we add this quadratic form with the Laplace. And you can already see where this leads to in terms of the spectral properties, but if not, I'm going to explain it. So, first of all, what's the solution to this? You can write it down. It's a linear filter. It's a linear filter with the graphs here. And clearly, this matrix, right, we just added an entity and so on, has the eigenvectors of the Laplacian as its basis functions that it operates with. Of basis functions that it operates with. So it's here, there is sort of hidden here, right? Is the transform into the Laplacians? What is the modulation? It's 1 over 1 plus alpha times lambda, where lambda is the eigenvalue of the Laplacian. In other words, the zero eigenvalue goes through without any modulation, just one. Everything else gets stamped out, right? And then you go back. And so this is a linear filter. And here's the result. Looks kind of neat, right? Looks kind of neat, right? And why this is a good idea to add this particular penalty here. So if you write out this quadratic form, well, I think there's a factor one half missing, or well, it depends on how you do the indices, you get this quadratic, right? You get the sum of squares. And so when is this penalty high? When do we pay a high cost? Well, we pay a high cost if two connected nodes, so if this Two connected nodes, so if this is non-zero, have a large difference. And we measure this in a squared sense, right? So I said this high costs if nodes that are connected have very different values. And the minimal cost is zero. Just pick a constant signal, right? So here's the visual. I think it's probably very difficult to see. This graph looks like there's a triangle here, there's an edge here. There's an edge here, and then there's like a square here, right? I think it's difficult to see even in this room. Maybe it's easier online. And I'm plotting here now just the eigenvectors of the Laplacian. And what I want to emphasize is here that this kind of makes sense, right? So, I mean, you can think about what this is doing, really, like you decompose this, think about it as some sort of it's not a Rayleigh quotient, but you know, it sort of You know, it it sort of uh does that in the sense that you want to express it into into those coordinates that have small eigenvalues, right? And the smallest is of course this, this has zero cost, this has a sort of cost of 0.6, so this is a slow frequency. We're going to high frequencies from the, from, well, we're going to from high to low, from left to right. Right? Interrupt me at any time if all these things don't make sense. Yeah. Yeah. Yeah. Looking at the quadratic form there. So as a quadratic form you're thinking of it really as a symmetric matrix, right? Yes. So you got me. So just from a directed graph to an undirected graph. So here I tacitly changed from my directed time series example to undirected graphs, and it's true, I'm going to assume everything is undirected here at the moment for graphs. Here at the moment for graphs. This is a topic that is ongoing. I think how to generalize this to directed graphs and so on. It's nice to have an undirected graph because of the issue if it's a directed graph, the spectral decomposition might not even be defined. And there are other things that, but yeah, complex eigenvalues, all sorts of things, but that's a cool question. We can talk about it. Cool question. We can talk about this. I'm interested in these kinds of things as well. But so, yeah, you know, I guess Cosette said there are always small inaccuracies in every talk. Here's the first one, right? So undirected, undirected. I'm assuming undirected. More questions for this example? No? Okay, so now we have a lot of hand waves. With a lot of hand waving, I might have convinced you that this might be a good idea. So, what I'm gonna do is I can now construct general filters. And basically, well, there are many ways of writing this. I mean, you can always write this in this sort of three-step spectral way. And that is basically like this, right? So, then if you have this basis, so I'm declaring my Laplacian eigenvectors are a great basis. Now, the name of the game comes is. Now, the name of the game comes is basically designing these filtered coefficients, this modulation function. The frequency response is what the signal processing people would say, engineers and something. And now you could say, well, that's nice, Michael, but actually, if I give you a million node graph doing that spectral decomposition, it's going to be hard. And maybe not as hard as some other things, but you know, Some other things, but you know, it's not necessarily easy, and so we don't really want to compute the spectral decomposition first. What we want to do instead is actually we want to use sparse matrix vector multiplications. And so another way to think about this is we can, of course, try to approximate this function by a polynomial. And then we are changing our, if you want, now our approximation. Our, if you want, our representation of the filter is a matrix polynomial of this but mat sense. And so you can try to pick your coefficients here to approximate this function well, and so on. And why would you want to do that? Well, that's basically, let's say, machine learning. We want to do this more efficiently. We want to basically not do the spectral decomposition first, but we want to do what people machine learning would say message passing. If I multiply If I multiply with the Laplacian, all I have to do is basically talk to my neighbors. The Laplacian just encodes my neighbors. This is a very sparse thing typically. And so you should see that everything that I need to do is iteratively apply a sparse matrix vector multiplication and put the right coefficient in there. And that thing tends to be easier to train than whatever training means, right? Whatever training means, right? So again, here's my story is basically. What is really cool about this is really because it's aware of the underlying thing, of the underlying graph in this case. That's the magic. Now I'm going to add another layer here. This is a picture that I took from A picture that I took from the graph learning course, I think, from Alejandro's Ribievo's group. So it's a nice course, you can watch it online for free. Much more comprehensive than anything I'll tell you here. So what you do is you can actually combine these linear filters with a non-linearity. It's a point-wise non-linearity, so you apply to every element of your vector element-wise. Element-wise, and then you compose this, and you call one such thing a layer, and you know, then you take the output of a layer and put it into the next layer, and you make these coefficients all learnable. And then, you know, you can do lots of machine learning thing. You can basically try to learn these coefficients to basically approximate some kind of complicated input-output relation for signals that are defined on the nodes of a graph. Defined on the nodes of a graph. And people do this, and they can do it very successfully for certain things. They can, I don't know, predict certain properties of proteins, let's say, that are represented as grass, right? But I want to emphasize not all that technical stuff here, I still want to emphasize this thing. The basic features, even in this thing, the basic features are still inherited from what's called the shift operator, and that's basically that thing here. And that's basically that thing here, which I actually told you this is going to be the Laplacian. That's also not quite right, because you can argue that maybe you should have a normalized Laplacian, or maybe you should actually have the adjacency matrix. The adjacency matrix is not so nice, because it's not a positive semi-definite matrix. But you could say, well, I want to normalize. So there's the normalized o plus in some people, you know, you can use other things there. You can use other things there. I'm not claiming that the Laplacian is the coolest, but actually it is getting close. So now here we go. Single processing and complexes. Okay, so you could sort of see where this leads up to, right? Regular domains, graphs. What could be even cooler than graphs? Yes. Complexes. Complexes. Alright, it sounds also more complex already. So now I'm just gonna dive into one motivational example that should convince you somehow that graphs are not enough. We need to go beyond graphs, right? And it's a very innocent example, but I think it's a nice way to explain it. And so, what if the signals are defined on the edges instead of the notes? It's fairly natural. You have flows of mass, energy, information, goods, what have you, anything. Goods, what have you, anything, discretized vector field, if you wish, they said. And can we now use the graph signal processing toolkit? That's a natural starting question, I would say. And of course, now the avid machine learner in you tells you, yes, of course, come on. Michael. Trivial. I give you five different tricks in which you can do this. And here's one, right? Just do a line graph transformation. Just do a line graph transformation or dual graph transformation. So, what's that? Just to get everybody on the same page, when I say line graph now, I don't mean this path thing. I mean the line graph transformation in which every node of the original graph becomes an edge in the line graph. Or I should actually explain it the other way around. I should say every edge in the old graph becomes a node in the new graph, in the line graph, and two nodes in the new graph are connected. In the new graph, are connected if they are incident on a common node in the old graph. So let's look at D. D as an edge, right? Since here, it is connected, of course, to edge C, right? It is connected to edge E and it's connected to edge A. So clearly, this is one trick. Now I've constructed a graph in which the edges are the nodes. The edges are the nodes, and I can just associate the old edge signals with the nodes of this new line graph thingy, and bam, we're done, right? It's just a single little script that does that. And yes, maybe the line graph gets larger, but you know what? We have powerful computers and all that stuff. That could be your thing. Of course, you're more clever, you've already spotted the mistake, but here is one thing. Here is one toy example that explains why sometimes this is not a smart idea. And so it goes like this: let's say we have a conservative flow on a graph. What I mean with this is that everything that goes into a node, every flow that goes into a node, comes out. For car traffic, it seemed like a natural assumption that every car that goes through the crossing comes out in the end. So I think for many things, this being close to a conservative way, maybe you park. Conservative way, you know, maybe you park somewhere. It's not exact, but it's kind of natural. Natural is always a tricky word, right? So we're going to play the same game now. This is the original. I add noise to it, and I want to recover. I want to recover the original signal with a linear filter. And so here I go. I do my line mark transformation at the back, and so on. I do the filtering, the outcomes, a thing that has an error that's even worse. A thing that has an error that's even worse than before. And that's bad. Now you can say, well, did you pick the wrong filter form and whatnot? That's on. And that's not... You can do slightly better than this. Yes. But my claim is that the reason is not the functional form of this filter. The reason is the features, the basis functions. They were somehow fundamentally wrong. Why are they wrong? Now I need to explain. Now, I need to explain a bit more, right? So, what I told you before is that if you look at the Laplacian, what will it penalize? It will penalize if two nodes that are connected have a very dissimilar signal. So, what it pushes you towards is basically having adjacent nodes getting similar, right? So, you want to do that. So, if you wish, the gradient on the original The gradient on the original graph, the gradient along an edge, the discrete gradient, this is just the difference between the two endpoints, this should be small. Now, let's look at this example here. Is this actually a good thing to do here? Well, you can see that this is actually what's happening here, right? So we're doing some kind of averaging. Everything is kind of getting close to each other. 5.2, 5.0, 5.2. But this is not a good idea. This is not a good idea because, you know, just because I have, like, I'm, there's nothing that leads to that this should be similar to this, right? Actually, what I wanted to capture is this notion of a harmonic flow, right? I mean, sort of, I wanted, well, conservative flow. I wanted to have that everything that goes in comes out. This was my fundamental model, anyway. It's not that what comes in here should be in the city. It's not that what comes in here should be somehow similar to everything that comes out. It's not natural, right? Okay. So the third one is no longer anywhere close to being conservative, right? Yes. Yeah, yeah, yeah. It pushes you in the wrong direction. So basically, the notion of smoothness or the notion of feature, of smooth feature, of relevant feature, is not adapted to this flow like theta. And of course, Like theta. And of course, what we now want to say is we need a new operator. The graph of plastic is not enough. And what is this operator? It should capture the cyclic structure of the graph. And we're going to extend this, of course, then to voids and so on. But what is this operator? Well, it's going to be the Vochloplausim. And in the simplified setting that I showed you here, this is just the Etsy Law Plusin. There's no, you know, there are no two simply C's. You know, there are no two simple C's or something like this. Yet, we're going to have them in later. But then, if we do this, we end up with the Harsh Levels. And if I focus on the edge picture, right, then why this thing is the right operator, the point is that the null space of B, so the kernel of B, or the null space of L1 in this case, is actually the cycle space. So it encodes the cycles. And these are. Encodes the cycles. And these are all my conservative flows, right? I can make up all the conservative flows if I have a basis for this. And the image is the gradient space. And so that's now the relevant set of features in some sense. And now, if I modulate those features, I'll be able to filter much better. So here's the high-level picture. And my claim is basically, even though I showed you this in this simple example, Same simple example. My claim is that the Hodge decomposition is a really cool tool to represent signals on cellular complexes, essentially. And smooth signals are basically those that are associated with small eigenvalues of the associated modulo plusins, and so on. And why is this cool? Well, first of all, it's cool mathematically, right? Let's face it. But it's also cool because But it's also cool because it provides this interpretation, right? And of course, this interpretation is much more difficult if you go to higher and higher or objects, right? But here, I think at least in this sort of discrete vector space domain, I think there are lots of problems where this could be really applied much more. And basically, what you're using behind the scenes is essentially discrete exterior cutters. And yeah, you know all that stuff, so actually. Yeah, you know all that stuff, so actually I can keep that short. But here's my picture now again. Now I'm showing this is without faces, right? So now I'm showing the same, the eigenvectors of the Edgeloclausian. And as you can see now, so okay, what does it mean? I picked an arbitrary reference orientation here, and the errors are basically sort of indicating these. So here's a second. So here's a cycle space, right? And of course you can pick your basis here in a slightly different manner, but basically for every hole I have one eigenvector, right? And they have zero eigenvalues, so they are perfectly conservative flows, and it's nice because now we can say, look, let's filter according to a low pass filtering and it's going to achieve what we want to do. And as you can sort of see here, right, this is becoming more and more grain. More and more gradient-like. And actually, I think I have a slide that explains what I mean with this as well. But, you know, let's first see that actually what I claim works. And so, the only thing that I did here now is I changed the operator that I put in. And you see, now it works. And the reason is I put, well, essentially I put in a priori knowledge. Essentially, I put an a priori knowledge in the choice of my operator. Okay, so you can apply this in different ways. I just want to show you a few examples. This was smoothing, so we measured everything, measured all the edges and there was noise on every edge. We can also do interpolation or semi-supervised learning, as it would be called. So, this was from the work of Austin Benson. He is what he was. Is what he was. He's moved out of academia, at least temporarily, I think. And so now we're going to do the same kind of thing. So you should, okay. Okay, interjection. We might actually know if we sometimes we might deviate from this, we might sometimes know that there are certain sources and things and we can put this in as well. There's no need to make everything harmonic sometimes. Everything you know harmonic sometimes, but harmonic is not good. Back in the day, we basically had the same kind of assumption, but it's you should see that it's very simple as before. We want to be staying close to what we've measured, to the observed flows, right? Then we have this, which now looks slightly different, but trust me, it's basically this is a quadratic form at the back with some additional source terms here. This is penalizing flows that are not conserved. Flows that are not conservative, essentially. And now, well, because our null space is potentially high-dimensional, we want to add this regularizer just to make the solution unique. You can pick whatever favorite regularizer you have, or if you happen to know that you've measured sufficiently many edges, so you've covered sort of all the all the all the cycles, right, then you can. Right, then you can drop off that thing as well. That's fine. So, here are some results, right? Computer science basically goes like this: if you go to a machine learning conference, your curve has to be above the other curves. And if it is, you win. And so here's the demonstration. We win, right? And so this is the ratio of labeled edges that we give to the algorithm. And this is basically an inner product or correlation. Basically, in a product or correlation with the truth, and you can see we can actually beat line graph-based approach significantly. So, what I told you so far with not fully precise, but basically the notions were that, okay, single processing on graphs and complexes, I have presented to you this just as a slight generalization of Slight generalization of single processing properly. And the main claim that I want to keep in mind, you to keep in mind, is basically that this Haussing composition is really the enabler of a lot of these things. So you get a unitary basis for the signals, you get a notion of signal and noise, and in particular for flows you can say that smooth signal is something that conserves the flows. You may want to actually have something that does not conserve the flows, right? But you can construct a linear filter that does the opposite filter and so on. Does the opposite filtering and so on. And of course, we are here at the complexes, and nothing was a complex so far, it was all graphs, right? So now with faces included. And it's the same story. And here this is, you know, if you go to the review of Federico Battiston, High Our Networks and so on, you find all these cool pictures. They're great for explaining this stuff. Great for explaining this stuff. I don't think, actually, I need to go through all of this, but basically, we're now considering abstract simple complexes, right? So, one simple X will be an edge, and two simple X is this little triangle, three simplex tetrahedron, and so on. And of course, the difference representation wise is we can now represent something like this with an actual object, and whereas if you had a graph, you could not distinguish between three pairwise things and the. Things and the triplet problem. Okay, so where do you get those things from? Well, sometimes you actually collect data about interactions. You need to construct this influence complex. I'm going to sweep all of this under the rug, right? As I promised you at the beginning I would do. This is a complicated thing in general because it really means you need to understand something about your problem. I think that's the complication you need. You make sure that your abstraction kept Your abstraction captures something that you want to capture, that it actually does this well. And so, you know, some people say, ah, maybe a simple two complex is not good. You should go for a cellular complex. I have made this argument as well. And indeed, sometimes, I mean, why have triangles? There's nothing fundamental about a triangle. But we're going to stick to them because they simplify exposition and similar math, right? So you You've seen some of this already as well. So, just a reminder, so the Hodge Laplacian is basically this composition of the boundary maps in the right way, exactly how you would think it should be done. And the Graf Laplacian is but one entity in this hierarchy of Hodgeville plausibs. Every Hodgel Placian basically covers, connects. So, this is like zero simplices to zero simplices. This is the graphical plausion. Simplices, this is the graphical plus. Then you go from one simplices to one simplices, this is the one podula plus, and so on and so forth. And you can go all the way up. And so it consists basically of two type of boundary maps, right? And this is the issue of picking a reference orientation. You have to, I always describe this as, if I talk to students, I say, look, these are the B matrix and the B stands for book key. It stands for boundary operator footprint. But the B is essentially. Operator flow. But the V is essentially bookkeeping, and why? If I want to measure flows, I need to account for one direction. And this direction I will call the positive direction, right? And I need to do this, and it's an arbitrary choice. It does not mean that there's no car that can go the other way around, which could also be an objective of your modeling, then you would go to direct the graphs or something else, right? But this is not what this captures. This captures just the bookkeeping to to to enable computations. To enable computations, right? Of course, we're talking about real coefficients, essentially. And the claim is that this captures already a lot of things that we want to measure. So now, what does this buy me in comparison to what I already had? I have the agile plus, and why do I actually need to go higher? This could be your question. And well, Well, what does this buy you is essentially you can now dissect your features in a more granular way. Before I was essentially able to distinguish radial flows and harmonic flows or conservative flows. But maybe some circular flows are different than others. Are different than others. And I want to be able to capture that. So, this is a modeling perspective of why I would want to do that. There are other reasons that you can come up with. But by the way, I'm indicating, I'm just saying everything, every triangle that you see is filled. And I'll just, to make it visually clear what the holes are, I put this little hexagon in there. I simplify it, draw it. Simplifies drawing. So, okay, so now this is the same space, and now notice that I've basically here this is a hole, this isn't, it's filled, right? So now this becomes a curl flow. Okay, let me just, because I have maybe a few more minutes to say this. So this here, I want to just say what this means. Of course, this means this is basically isomorphism to the harmonics. This is a curl, and this is a gradient. And since José brought up this. And since José brought up this vector calculus notions, let me just say: okay, it's the image of B1 transpose. B1 transpose is basically the discrete version of a gradient. So how do I create any gradient? Well, I assign numbers to the nodes. These are my node potentials, if you want, potentials. And then I compute the voltages or the potential difference. And these are gradient flows. Anything that you can see here is basically an assignment of It's basically an assignment of nodes, you get values, and then you can create all those fluxes. And in fact, the right numbers to pick here correspond to the eigenvectors of the graph rows. And now, how do you get curls? Well, I need to assign numbers to those two simplicities. I assign one number, and let's say this number is one here, and then what this induces is basically in the reference orientation. Is basically in the reference orientation that number of law, right? So if you're an electrical engineer, this is sort of like in Maxwell's equations, time-varying magnetic fields. Forget about the time-varying and so on, but this is sort of like a magnetic potential and this is like an electric potential. Yeah, right-hand side. Exactly. You know, you finally understand why this right-hand side will come so. Okay, so here is an example why this is Why is this a useful thing, perhaps? So, we just made a mock, a choice example. We have a simplical complex, we have some flows that basically go from here roughly to there. There are two holes in there, so our homology is non-trivial. What are the eigenvectors, or what are some sets of eigenvectors that correspond to this? This is the first one, and this is the second one. And notice what this basically gives you, it's kind of like a, again, a feature extractor, right? It's a feature. A feature extractor, right? It's a feature map, and this is the harmonic flows that go around this hole, and basically they go around that hole, right? Okay, now you could say, well, I want to analyze flows. What's the simplest thing you could do? You could try to do something, and that's akin to diffusion maps. If you don't know what this is, you don't need to know. It's just, but if you know, this is basically the idea. So let's say I have some flows, right? And I could now sort of try to project how the Could now sort of try to project how does each edge actually contribute, each flow that I measured here relative to those harmonics. Like, how does it contribute to that thing? And you would see that, well, you know, you can basically distinguish attributes. And then you say, oh, but this doesn't really look like extracting much. And I would agree. And this is how the embedding looks like. Notice, however, it has this peculiar shape. We can talk about what this is as well. I'm not going to do it here now, but I want to. I'm not going to do it here now, but I want to get to a different thing. Because I think where this actually is much more useful is if, rather than thinking about individual edges, think about trajectories. You're now given trajectories. What are trajectories in our world here? We think of them as basically sequences of edges that we just put into one vector. So this is one vector. I put a one if my trajectory traverses the edge in the right reference orientation. Reference orientation and it's minus one if it's opposite to it, and a zero as well. And now I have these trajectories, and let's look at how do they project into my harmonic space and look what this does. Actually, it dissects these nine trajectories, right, according to their homotopy type, if you want. I mean, basically, you have these guys, right? They go straight through Right? They go straight through and uh they get a negative projection onto those two eigenvectors. Um this basically is mostly associated to this single harmonic flow here. It's a nice thing to see this and so on. So we have extracted sort of a very very low dimensional representation of an a priori relatively high dimensional thing which is trajectories in this complicated space and you can use this to Space. And you can use this to do all sorts of stuff, right? You could do this to detect certain outliers. Maybe there's a trajectory that goes this way or pick this up. But you could do lots of other things. And typically what we do then, of course, is just to say, look, this makes sense. And we haven't gotten much for it all, but we're pushing this more. But now let's look at some real data. We have trajectories of buoys that drift in the ocean. This is basically a big piece of planet. Drift in the ocean is basically a big piece of plastic you throw into the ocean, but it's a useful piece of plastic. It has sensors in it, and you can try to understand the ocean currents there. It's very noisy and it's incomplete. These lose track of GPS often, then you see nothing, you have missing data and so on. So we thought, okay, we want to test this data because this is a useful application. And we want to derive a very low-dimensional representation. So what do we do? So, what we do, some artificial choices, admittedly, but of course, you know, tied to our thing. We discretize space and we use the hexagonal grid. Why hexagonal? Because we're going to get syntax complexes out of it. And how do we do this? Basically, every hexagon becomes a node. If there's sufficient flow between two hexagons, we put an edge. And then, of course, we also fill this if there's sufficient flow in this triangle and so on, and then we have a simpler circumference. And then you have a simplistic complex. So pick a cellular complex, and you can do whatever polygons. So do you define by faces? That's what I just told you, right? And we encode now the trajectories as these edge sequences, as I told you before, and we do the production. And now what we can see, this is Madagascar, by the way, in case you haven't noticed already. So yeah, this is Madagascar, and in Madagascar, there's a flow of current coming from the, I mean for least. Coming from the east, going to Madagascar, and you can basically go south or north. And so, this is the first harmonic that picks this up. How do you encircle Madagascar essentially? And then there's a second one, and this is more like an outlier. There's a little vorticity here, and we pick up sort of vortex-like feature, and this corresponds to this kind of current. And depending on whether you want to describe this as an outline or whether you. Want to discard this as an outlier, or whether you think this is actually a useful thing? Maybe you want to pick up on Melinio or whatever. So we discussed a lot of back then. It's like, oh, you know, you could try to figure this out or that and so on. You can do actually cool things, I think, at least. So other applications. You can do finite impulse response filters. This sounds scary, but this is, I mean, I've basically talked about smoothing and harmonics and so on. It's not necessary that the harmonics. It's not necessary that the harmonics are really the important features, but you can, you know, you basically put your coefficients such that you pick any other features. I think it's much more difficult to argue why other features are useful. You need more domain knowledge to know that, basically. But gradients could be a useful thing as well. And yeah, you can then have other filters. You can also do, you know, I totally can go beyond Fourier, you can do wavelets, fodgelets. Wavelets, hodgelets. This is what we did with Midge. And you can see that the hodgelets you can get away with fewer atoms for certain things. Why? Because they are not sort of globally supported anyway, right? So you get a localization in space, in geometrical. You can, of course, extend this to cell complexes and And that's useful in some sense because triangles are not really why triangles, right? I mean, they're not a fundamental thing. And of course, this makes basically the computations, you need to be more clever with bookkeeping and so on. You can also look at doing these kind of things over time, product spaces of a graph with time, let's say. We've done something a lot of We've done something in a lot of slides. But now I'm done. So again, let me hammer home this message. Signal processing on graphs and complexes, really, I think it's a nice picture in mind. It's wrong, but you know, Newton physics is also wrong in some sense. So I'm not claiming that we're anywhere close to the utility of this, but the main point here, I think, is, and that's the workshop here, right? I think is, and that's the workshop here, right? I think the Hodge decomposition of the one non-plastic, but in general, is really Jenna Hitler here. And yeah, happy to talk about other ideas and so on. And you can look up publications. I'm happy also to point to other works. We're by far not the only group working in this area. Sergio Barbarossa's group in Italy, for instance, was signal processing on this. There was a great book by. This. There was a great book by Grady and Poulini, I think. It's called Discrete Calculus that actually has a lot of those ideas in there as well. And I think it's a great source too. And with that, I want to say thank you for your time. And I don't know whether I'm over time or not, but you're fine. Okay, great. Yeah, I'm happy to take questions. Great. Thank you. It's like user. Questions? I guess I have a rather specific and somewhat maybe personally motivated question. So you talked in the beginning about sort of building these filters where you take powers and matrices and you're applying them to essentially a function acting on the like a foundation. And the motivation for doing that is you want to be doing things that. For doing that, is you want to be doing things with respect to the spectrum, but the spectrum can be hard to compute, so you'd rather use sparse matrix operations. Then you later showed an example where you're essentially looking for a projection. So this example we're trying to project on the harmonic space, which if you're looking for projection, really you want your function to be discontinuous, right? You want to jump at 0, 0, and 1 everywhere else, which is not an easy function to represent polynomials. It's not at all. And so the question I have is: if you say you wanted to only use a couple terms in this series, or Use a couple terms in this series, or maybe you didn't know your graph exactly, but you knew what family was coming from. Are there methods for doing this sort of thing where, if you want to do something like projection, you could pick your coefficients in an optimal way in the scale series that's not? Yeah, that's a good question, of course. And some of the things are a bit incompatible in that sense, right? So, if you want to design filter coefficients of this polynomial to approximate a certain Polynomial to approximate a certain desired function. There are ways to do this. You can do this polynomials and do proper transformations and so on. More clever people than me have thought about this for a long, long time, essentially in standard signal processing. Basically, even this kind of single cost. But what you said is basically this perfect low-cost filtering is sort of cut off, right? Sort of cut off, right? This is difficult to approximate, and that's exactly right. In fact, in sort of classical signal processing, this is also an a-causal filter. It means there's no future to actually compute it. So there are even more complications with this in some sense. What you can do is basically you can fix your polynomial order, let's say, and then you can try to get or let's say you can pick the phenomenon of the Pachaviche polynomial. You pick the family of the Pachevichev polynomials to approximate this. Pachevichev is standard choice, as far as I know, because it's numerically stable and robust. But there are other options and yeah, that's general. Like you can also, of course, I mean, I think where this especially comes important is if you don't know what the frequency response The frequency response that you want to have a priori, you want to learn from it. This is even more, then this becomes even more important. Yeah, I was going to ask what do you do if you want your H in some ways to respond to the I values, like actually if you want it to be optimal with it. So if you know, I mean somehow, if you know the frequency response already that you want to have, right, it means you know something about the problem. You know that maybe you have conservative faults. You know that certain gradients or certain features. Certain gradients or certain features are for. More like graph neural networks and so on, they assume you don't know that function, but what you know is certain examples, you know, like this should map to that, this should map to that, right? And then you don't want to just learn this linear filter, of course, but you actually want to learn this whole stack of linear plus non-linearities and so on. And you can do that with this. And you can do that with this approach, arguably, although in some sense it's the same sort of than this, right? But this one is far more dominant. These are called message passing neural networks. And some people think of this as two sort of fundamentally two different things because they say, well, you know, in this case, here's the coefficients that you need to learn somehow, right? And you first need to do this work. Somehow, right? And you first need to do this representation. And so it's, but well, I think really it's you shouldn't sort of make this distinction at a fundamental level. Of course, at the computational level, it matters. Whether you do it like this, and you need to know this first, and that's it. Other questions? I wonder if you can learn low pass filters. Learn low pass filters. Yeah. You mean like this? Yeah, yeah. How you learn those things, like the coefficients that go in there? Right. Yeah, I mean, so actually you just learn them like you learn deep networks. Fundamentally, what you get here is a is a matrix vector. A matrix vector multiplication with some coefficients that you can assign. This nonlinear, it is fixed. This is typically something like a rectified linear unit or other things. So that means just, you know, if it's negative, throw it away or set it to zero, and otherwise it just keeps on. And then you have to define your loss function that you want. And then it turns out that in a certain way, this is basically just like a Basically, just like a specific deep neural network. So, all those deep learning pipelines that can deal with all sorts of general layers, they can also deal with that. And then there is a package called PyTorch Geometric, for instance, that will do it for you. PyTorch is the center. Okay, well, thank you very much. So, that bell is our time. So, we'll be back in the afternoon for some discussion. This is lunch break for everybody. For some discussion. This is lunch break for everybody. Let's thank Michael. She can't hit the record button.