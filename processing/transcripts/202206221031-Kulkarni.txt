So, I always imagine myself talking in. So, as you might guess from the title, this was a little bit of an experimental and proof of concept project. And this was sort of the brainchild of myself, Catherine Heal, and Ember Sertoz when we were all at the MPI in Leipzig. Catherine Heal is a visiting machine learning specialist. And what we really wanted to do was sort of like, you know, there was this sort of frontier of results coming out in deep learning about like, oh, Coming out in deep learning about, like, oh, maybe we can try to apply this to mathematical problems, and people try to learn, you know, train some sort of machine learning neural network function to learn things like, you know, how can we predict some sort of algebraic invariant or something along these lines? And so, you know, they would have the machine guess like, oh, maybe the value of this function is this thing, or maybe the value of this invariant is this. And then there would be like zero certification afterward. And so what we really wanted to do was to try to use deep learning in a way which is sort of Deep learning in a way which is sort of giving us rigorous mathematical results. And we were wondering, well, how could we possibly use a neural network for these purposes? And the problem that we decided to set ourselves to is the computation of the periods of a hypersurface in projective space. So we're always going to think about a hypersurface as being a smooth hypersurface of dimension n living in a projective space of dimension n plus 1. And our goal is to compute... And our goal is to compute the periods of this hypersurface. Now, when I say compute, I really mean give numerical approximations with certified error estimates. Oh, whoops. Oh no. This has gone horribly wrong. There we go. Okay. I don't know what button that was, but I don't want to press it again. Okay. So we've been Okay, so we basically want to compute these periods up to some sort of certified error estimate. So if you give me a precision, our method for computing these hypersurfaces should be able to tell you what the periods are, sorry, for computing the periods of these hypersurfaces. It should be able to tell you what those periods are with some sort of certified complex fall telling you that the period is wrong up to this amount. And so even though these results are numerical, we can still do things. We can still do things and predict some interesting invariants about these hypersurfaces. So, for example, if you give me the period vectors for, say, two quartic smooth quartic surfaces in P3, i.e. two quartic K3 surfaces, then the Torelli theorem for K3 surfaces will be able to tell you whether or not those two things are isomorphic. Now, of course, we have periods with error estimates, so if the periods are the same, they're very likely to be isomorphic. Likely to be isomorphic. And of course, you could definitely tell when they're not isomorphic if one period lands in this aeroplane and the other period lands in the other aero ball. Now, with these sort of complex numbers, we can use the Hodge conjecture, which is true for smooth quartic surfaces in P3, and then we can detect which integer linear combinations there are and estimate the Picard rank for these quartic K3 surfaces. Three surfaces. And finally, we not only get the structure of the card number, but we really do learn the entire lattice structure of these periods. And so by checking which transformations map the lattice back into itself, we can do things like estimate the endomorphism ring of the transcendental lattice or the transcendental part of the K3 surface. Okay. So we had to choose some sort of feasible sub-problem to look at. Feasible sub-problem to look at. Right now, it is really beyond the computational range to take some sort of arbitrary dense quartic surface in P3. And when I say dense, I just mean that of the 35 possible monomials that are there, maybe they all show up. They have some complicated rational numbers as coefficients. It's really outside the range of computation to be able to compute the periods for such a dense quartic surface. So we really wanted to do some sort of test problem. Do some sort of test problem which was large enough to be interesting but feasible enough for current technology. And so the set that we focused on were the set of few nomial smooth quartic surfaces in P3. And when I say pionomial, I just mean that it's a sum of either four monomials or five monomials, where all of the coefficients are equal to either 0 or 1. So the number of such surfaces, well, there's 108 four nomials, there's 3,408 five nomials. 408 5 nomials. And this gives us a reasonably large data set. And the results of our computation were actually kind of interesting. So we sort of computed the periods of almost all of these things. And then we were able to estimate the Picard numbers. And you get this funny looking distribution here. Of course, this is the distribution only for the five nomials. The fournomial Picard numbers you can get by other techniques. But in any case, this is not the typical distribution that you would expect for Picard numbers. Expect for Picard numbers of your average K3 surface, because we're already specifying some sort of very special structure by saying it's a sum of four, like a few number of monomials. So you would naturally expect there to be things like extra lines or extra algebraic cycles, et cetera. Sorry? Yeah, so the the numbers we're really just taking coefficients in in zero and one, so I mean these are did you measure. Did you measure the Picard numbers? The Picard numbers of V over Q on V over C? Oh, no, we're looking at the Picard numbers over the complex numbers. Yeah, we're not insisting that the algebraic cycles themselves are defined over Q. We just want to say, like, are they actually present in the K3 surface? Yes. That's also a four nomial, so it doesn't show up in this. Oh. So, it doesn't show up in this. Oh, yeah. Right. So, in fact, the very high Picard, like the Picard number 20 things, are definitively not inside in this list. Okay. So, first I'm going to talk about the sort of mathematical algorithm that we use to compute these things, and I'll get into the deep learning stuff a lot later. So, the idea is to use a homotopy continuation-type algorithm. So, you start with something where you happen to know the periods to whatever precision that you want. Periods to whatever precision that you want. And then you try to track those periods as you deform along a one-parameter family. You see how the periods are changing, and then you end up with the periods of your target result. So when I say period, I'm going to think about a slightly more extended notion of period. So instead of thinking about differentials on my surface and integrating those differentials against the homology cycles of that surface, I'm instead going to think about meromorphic differentials in the ambient projective space where their poles are. Projective space where their poles are constrained to lie along the defining equation of my hypercircles. So we'll say that the Meromorphic periods are the integrals of these Meromorphic differentials, which I can always write as being some polynomial divided by the defining equation of my hypersurface to the power of m. And of course, in that case, we say the pole order is equal to m times this omega thing, which is just a sort of fixed generator chosen in advance, which is a global segment. Which is a global section of the twisted canonical bundle on P3. And then the integrals we take, well, we need to take an integral of this differential over something of full dimension. So what we do is given an algebraic cycle on our coordinate surface, we draw the tube around it, and now we have something full-dimensional over which we can integrate our differential, and then we get some sort of numbers. And then the period matrix is the thing that you normally expect, but now with narrow. But now with Meromorphic periods, where the rows are indexed by your meromorphic differentials and the columns are indexed by your tubes around, or integrals around the tubes of the homologous eigenvalues. Now, what do these have to do with the classical thing that we think of as a period? Well, it turns out that there are a couple results that let us relate these back to the numbers that we all know and love. So, if we think about the space of meromorphic differentials, or rather the differentials holomorphic outside the local Holomorphic outside the locus of our hypersurface, we can map those or associate those to differentials on our hypersurface. So, what's a co-homology class? It's just something which is the dual of a homology class. And so, if I have some sort of meromorphic differential on my ambient space, I can think about it defining a functional on the homology classes on my surface by just taking that homology class and then integrating over the tube around that homology class. Homology class with respect to that differential. And so, well, that's a functional, so now we can associate that to some sort of well-defined co-homology class on my quartic surface. But we don't get all of them this way. We only happen to surject onto the ones by this theorem of, can be found in a number of places, but for example, this is in Foisin's book. We don't get all of them, we only get the differentials where the integral of the Lefschetz cycle, i.e. the algebraic cycle corresponding to a hyperlink. Algebraic cycle corresponding to a hyperplane section is equal to zero. And so, well, this actually has a lot to do with the normal things we think of as periods. If you look at the meromorphic differentials, which are holomorphic outside of x, and you look at the regular differentials on your quartic K3 surfaces, both of these emit some kind of filtration. So, for the Meromorphic differentials, you can grade these things by pole order. So, there's one Meromorphic differential with exactly. One meromorphic differential with exactly simple poles along your cortic hypersurface. Then there's a whole bunch which have poles of order two. And then finally, there's one extra class you get up to linear, well, up to sort of associating the right, subtracting off the right components from the lower dimensional spaces, you get one extra dimension from the poles of order, differentials with pole order three. And so, well, if you look at that. And so, well, if you look at that filtration, that looks an awful lot like the Hodge structure on your quartic K3 surface, which has a single holomorphic differential. And then, ah, it's not collaborating. There we go. Sorry? Oh, it might actually work a lot better, yes. There's only ports on this side, sideway. All right, thank you. Okay, hopefully the computer should stay awake. We'll see. I guess it depends how interesting I am. So anyway, so if you were looking at the filtration based on pole order and the natural Hodge filtration on the homology of your quartic K3 surface, these agree exactly. Well, only up to the differentials of pole order n plus 1. Pole order n plus 1. Of course, you can sort of define arbitrary pole order differentials on your ambient projective space. These are not so important for your Hodge structure. Okay. And so, well, now let's get back to this, how do we compute the periods of quartic hypersurfaces? Well, the first thing that we want to do is compute some sort of initial values. And in general, Deline showed that given some sort of precision, he didn't state the results exactly in this way. What he did was he showed that the periods of firms. Was he showed that the periods of Fermat hypersurfaces are given by exact formula involving various gamma factors and complicated terms, but nevertheless, they are exact. And you could turn this into a machine for giving you periods with arbitrary amounts of precision. If you give me some number, you can choose to evaluate these gamma factors and complicated terms and so on to get certified periods for the Fermat coordinate with whatever your desired precision is. And using complex ball arithmetic libraries, you can even. All arithmetic libraries, you can even certify that these are correct to some precision. Okay. And well, now how do we track the deformation of these periods? And this is where we use Picard-Fouch's ordinary differential equations. So we can think about a one-parameter family of quartic K3 surfaces as just being a quartic K3 surface defined over the function field where we adjoin a single variable. Okay? And so now we're going to be able to do that. Okay, and so now we have a bunch of meromorphic differential forms on our ambient space. These are not necessarily constant in t, so we have the operator, which is just differentiate with respect to t. So, okay, if I start with a differential and I differentiate it, you might see from something like the quotient rule that my poll order might increase after differentiating. But that's okay. So if we land in sort of pole order m, n plus one, then we'll n plus 1, then we're kind of happy because this was part of our original Hodge filtration that we cared about. But we might step outside this zone, but there's a way to get back. So if I happen to have something of pole order bigger than n plus 1, then there's a method called drift-dork reduction, which will take this meromorphic differential form, and then by adding some very particularly chosen exact differential, I can reduce the pole order to get back to the space of differentials of pole order. The space of differentials of pole order at most n plus 1. So if I start with my favorite meromorphic differential form, I can differentiate it and I get something else. And then I differentiate it again and I get something else. And I keep landing back in a finite dimensional vector space. So eventually there's going to be a relationship among the homology classes represented by these differentials. And so, well, okay, if I have a linear relationship among these things, that's just an ordinary differential equation. That's just an ordinary differential equation with coefficients in my function field. And now we can, you know, we have a bunch of differential equations. We have initial values for an initial value problem. So we can solve these initial value problems and sort of numerically take the periods from our Ferma hypersurface and then carry them over to our target hypersurface. Now, you might complain a little bit. These ODE solvers kind of carry these periods along analytic arcs. Periods along analytic arcs inside of the complex plane. So, you know, depending on the choice of path that you could have picked, maybe you circle around the poles one way and you get one set of numbers, or you circle around the poles the other way, you get some completely different set of numbers. But of course, we understand the monodromy of K3 surfaces, so we can always correct for this error. All right, now I've painted a rosy picture so far of, oh, look, we can compute the periods of some arbitrary Portic K3 surface. Arbitrary ported K3 surface just by starting with Fermoff, drawing the one-parameter family to that target space, and then just doing this homotopy continuation thing. Unfortunately, there's a number of problems with this that kind of prevent this algorithm from working practically. The first is really that these computations are extremely expensive, right? So when you're computing these Picard-Fuchs differential equations, the Griffith-Dorp reduction procedure that I described earlier. Procedure that I described earlier actually involves these iterated Grubner basis computations. And it's kind of worse. You have to express certain elements as being particular like polynomial combinations of a fixed basis for your ideal. So the coefficients involved in this are enormous, right? And not only are they enormous, they're defined over a function field. So it's very often the case that you just run out of memory when you try to do this computation, even if you let it run for a long time. Even if you let it run for an arbitrary amount of time. And the second thing is: not only do you have to do this once, you have to do this for all 21 periods that you're trying to track. I say 21 instead of 22 because we're always ignoring the period associated to the Lefschitz class. We can always recover that at the moment. But nevertheless, we have to do this 21 different times, and each and every one of these computations can fail in. Of these computations can fail independently. So, okay, well, even if we manage to get these differential equations and write them down, and okay, the initial values aren't so hard to compute, but then actually trying to solve that initial value problem with a numerical ODE solver, it can crash. It can be even more time consuming. And like things like in your one-parameter family, the singular locus of this family can form like a palisade around one of the Can form like a palisade around one of the target points. And so, trying to sort of use this homotopy continuation algorithm to transition over the palisade, you might need so much precision that the ODE sulfur just gives up and, well, crashes in, you know, spectacular fashion. So, okay, maybe there's a way to deal with this. Right? And, you know, suppose all roads lead to Rome. Some roads are faster to take or more convenient to take than others. To take or more convenient to take than otherwise. So we can just think about the graph that we care about, the set of quartic surfaces that we want to compute the periods of. We may throw in a few extra junction points, but nevertheless, we have some sort of set of vertices that we're interested in. And then the edges in this graph are going to be the connections, in the sense of an edge, where the homotopy continuation algorithm I just described is feasible. Now, you might complain that this graph is a little bit different. Complain that this graph is a little bit different depending on whether I'm using my household laptop or some fancy server with like thread ripper cores and 48, like, you know, huge amounts of memory and so on. But this is really a talk about sort of solving the practical problems. How do you get the periods for particular quartic K3 surfaces? So this graph is good enough for our purposes. In any case, well, you can look at this particular subgraph with three quartic surfaces. Subgraph with three quartic surfaces, and starting from Fermat, if you just look at the direct deformation, that takes about 152 seconds. But it was much faster to throw in this junction point, somehow compute, solve this sort of homotopy continuation problem going this way. And then, you know, now that we have a bunch of meromorphic periods here, we have yet again a different set of initial values, and then we can compute the card points. The Picard-Books, Picard-Books differential equations for this deformation, and then we can just use the ODE solver to solve that initial value problem. And going around this way was much faster. So in this case, it just turned out to be, oh, we computed the periods of this particular quartic hypersurface faster than the alternative. But I'm not showing you vertices where it was literally impossible to do the direct connection between Fermat and that vertex, but it was possible to take some sort of route. Was possible to take some sort of route by throwing in intermediate vertices and going from point A to point B to point C to D, and so on. In fact, the diameter of the graph that we ended up getting was something like 21. So there were paths between Fermat and something else that took something like 11 different steps to get there. Which was really quite surprising. But yet again, oh, yes. Sorry, when is it possible? Oh, which is the bottleneck? Is it the Groger bases or is it the Is it the Groger bases, or is it the Palisades, or what? Oh, yeah, well, some of them it's just impossible to compute any one of these ordinary, like, Picard-Fuchs differential equations. And so, you know, if you can't even compute the ODE, then you're already sunk. Yeah, so that's like the first and most prominent obstruction. And then, because we didn't even attempt the problem afterward if that step failed. So, yeah, I mean. So, yeah, I mean, okay, so it sounds like by throwing in extra junction points, we can really check for easy connections and then keep going. The problem is, there are, you know, as your set of vertices grows, your set of edges that you have to check is sort of growing combinatorially. And you also have, you know, you would think that maybe it's nice, you know, I have a graph, I have a bunch of weights on it, so I can just use Dijkstra to figure out how to go from point A to point B as fast as possible. Except you can't, because you have no idea what the weights are. Because you have no idea what the weights are until you actually try the computation. Right? And so, well, if you have two of these equations that have many common monomials, then maybe it's easier also. I mean, it looks a little bit like this in your example. Right. So that is an excellent question. So not only did we sort of try to apply these heuristics, we tried to automatically learn these heuristics using deep learning. Using deep learning. So, if the machine could pick up on heuristics like this, where they could sort of look at features of our problem, learn something about them, and say, oh, maybe we could try those edges first. Then we'd be able to sort of win at this period computation, provided it's possible. If it's impossible, then you're just going to waste, well, the heuristics aren't going to help very much. So, that is, in fact, precisely the idea that we decided to. Precisely the idea that we decided to try. And the nice thing about using neural networks in this way, where we try to use the neural network to predict which instances of this problem are possible, versus actually trying to use the neural network to solve the problem, well, now they don't have to be even remotely correct. They don't have to be reliable. The results don't have to be certified because the network predictions are not actually part of the output. As soon as you have a route between FERMA and Between FERMA and the cortic surface that you care about. A posteriori, you just hand someone that route and you'll be able to get the periods just fine. So the neural networks are only helping you get to the final answer. Okay, and furthermore, by using this network as an antidote to combinatorial, like essentially by sorting the edges in the order that you would optimally like to try things in, you can use neural networks as an antidote to combinatorial complexity. Okay. Okay, now if you've never seen a neural network before, this is a bad introduction, but I will go through it anyway. So, really, a neural network is just a function, right? And it's a particular type of function that's specified by some space of functions determined by your neural architecture. So, one way to think of the layers of a neural network, for instance, is just a sequence of vector spaces. These are linked together by affine transforms, which are parametrized by, well, Which are parametrized by, well, parameters. So you have some weights here, you have some biases here, which define your space of available functions. And in order for this composition to be nonlinear, you need to introduce some nonlinearity somewhere. And those are these endo functions, basically, which these are really just the activation functions that we normally think of as associated to the nodes. And there are many different architectures depending on the types of activation functions that you could choose. Activation functions that you could choose, but you could pretty much choose these activation functions to be whatever you want. And as a very, very concrete example, we can think about polynomial interpolation as being some sort of instance where you could parametrize polynomials of degree up to D by a certain architecture of neural networks. So in this example, we have an input layer, which is just x. There are no parameters or biases. Parameters or biases associated to these, so the links between the input layer and the hidden layer. In each of the nodes of the hidden layer, we're just going to assign a different activation function to each node. The first one just being map your input to one, the second one being do nothing, and so on. You just, you know, each node is associated to a power of x. You now have this second layer or second level of functions which take your hidden layer to the output layer. Which take your hidden layer to the output layer. Here there's no biases, but we associate a single parameter w. And the resulting output layer is just the sum over all of the incoming inputs from these nodes. But that's, of course, just some sort of polynomial with unspecified coefficients. And so now, if I give you a bunch of x and y pairs, this is just a fancy way to do interpolation, where if those x and y pairs actually came from a polynomial, Pairs actually came from a polynomial. If you run these learning algorithms, you should be able to recover the coefficients of that polynomial. All right. Now, of course, if you want to actually feed this thing into TensorFlow, you might instead extract a number of features of the value x, those features being just the powers. You then run it through a slightly non-standard activation function, but at least you can get it into the library, where you just take the identity for each of these hidden nodes, and then you do the same thing. The same thing. Okay. Now, neural networks have a large amount of expressibility. They're really quite remarkable in that way. So this is one among many universal approximation theorems. But the point that I'm trying to get across with this thing is that, well, if you allow some nonlinearity within your nodes and you allow your network to have enough expressiveness, so you allow it to be of width W, but arbitrary depth, then But arbitrary depth, then these architectures define a whole bunch of spaces of functions. And these functions are dense in LP spaces. Okay, so the learning techniques that we use for our problem about computing the periods of hypersurfaces in projective space, we were using supervised learning. There are other learning models which I'm not going to talk about. So supervised learning just means you have a data set labeled by an expert. You have a data set labeled by an expert, or in this case, labeled by some computation the computer did. And our goal now is: given some sort of particular architecture, which we choose by some more artistic means rather than scientific ones, we try to learn a function where f of x is equal to y for the future inputs of x and y. And so there's a lot of practical issues in dealing with this, right? You don't want to over-memorize your training set and then have a function that's just completely useless in the same way that, like, you know, if you specify. Same way that, like, you know, if you specify a small number of x, y pairs for polynomial interpolation and allow 100 degrees of freedom, then you're going to learn something that accurately approximates your training set and is completely useless otherwise. Same thing with neural networks, just at a much larger scale. So we divide into training and testing sets. We choose some sort of loss functional, which you hope represents the reliability of the thing that you learned. So that's also more of an art than a science. That's also more of an art than a science. And then you minimize the loss function over the parameters by using something like gradient descent. And when I say minimize, you're not getting an absolute minimum most of the time, but you're just trying to get after something that's good enough. And then finally, you've checked to make sure that the learning procedure was mildly successful. Okay, so going back to this polynomial interpolation, everything I've described has really been implemented in TensorFlow, or really a larger Or, really, a larger number of neural network libraries. So, there's a little bit of stuff above this MLP classifier function in the file, but those are all like importing TensorFlow and telling it to be quiet about depreciation warnings and so on. So, this function just says, well, if you give me a bunch of hidden layer sizes and some activation function, it just tells me, okay, well, I want some sort of sequential multi-layer perceptron. I'm now going to add layers into my network with this. Layers into my network with this model.add method. And then I'm going to choose some sort of way of minimizing or optimizing these parameters to minimize the loss function. And then I'm going to tell it to put the whole thing together. And then now I read in my data. I look at my x and y pairs and I sort of extract features from my x and y pairs. Features in this case just being powers of x. I then divide into training and testing sets. And finally, I run the MLP classifier. Oh, I also choose the activation function. Oh, I also choose the activation function to be the identity. So somebody says 150 values. Exactly. My training set is only 150 values, but I'm not actually trying to learn a very complicated function. But indeed, it's only 150 values. How we extend that whole data set again? Oh, um let me find out. Let's see. Uh and then I think it was called data, right? And then I think it was called data, right? So it looks like there's a thousand data points. Okay, so we're going to run this particular training program to see if we can learn our mysterious function. So Python iDemo. There we go. And there it goes. So it's training. And you could see from this lost estimate that it really does seem to be learning something about our data set. So now, you know, we have. So now we've got some sort of neural networking. We're going to try and predict some future values of this function. So I can do nn.predict. I'll throw in the powers of some number that isn't in the training set, say 5. All right. Oh, right. Sorry. Throw in a degree bound of. Okay, good. So we predict the value of 61 on input. Value of 61 on input 5. And if I actually look at the function I was trying to learn, this is an instance of the polynomial interpolation function. So I was trying to learn x squared plus 7x plus 1, and if you evaluate that polynomial at 5, you indeed get 61. So this was an extremely convoluted way of doing a polynomial interpolation exercise. But what I wanted to show you is that neural networks can really, like, it's not too much overhead to actually do this kind of machine learning. Had to actually do this kind of machine learning thing yourself. Could you create a computer quick with a secret? Oh, probably. I don't know the syntax for that off the top of my head, so we'll have to get back to you. Okay. There we go. All right. So as I mentioned, there's a number of practical considerations to look at. So how do you allow for enough expressiveness, but avoid this problem of overfitting your data set? Fitting your data set. And more relevant to us, how do you take really complicated, mathematically interesting structures like computing a one-parameter family and computing Picard-Coupes differential equations and trying to guess how long that takes, how you extract the mathematical features from your problem in a way that the neural network can eat, right? How you turn them into a tensor that the neural network can then intake. And finally, there's this issue of data creation, which seems to get swept under the rug, but it's actually a lot harder. Swept under the rug, but it's actually a lot harder than you might first think. Like, generating this data set, or even generating the training set to begin with, took an enormous amount of time. It took something like 10 CPU years just to generate the training set. And then, you know, it sort of shows you that computing periods of hypersurfaces is really, really difficult in general. But in general, generating data is not an easy thing. And the LMFDB, for instance, exists for a reason. Exists for a reason. Okay, now there is a type of feature that you can extract very easily. And that's, well, just some sort of numerical invariant of something. So I just wanted to mention the DeepMind paper that came out. And, you know, it got a lot of media buzz. And some media titles were a lot more eye-rolling than other ones. But just to talk about this for a little bit, what they did is they took knots and they computed a bunch of numbers that you associate to knots, these being things like These being things like meridional translation or like injectivity radius or symmetry group, you can just give that a label or volume. So you can compute a bunch of numbers and numbers are things that neural networks can eat. And then they tried to predict algebraic invariants like the signature of the knot or the Jones polynomial or something along these lines. And you can actually use neural networks now to try and extract relationships between these features. Between these features. So, in this particular diagram is showing you how sensitive each feature was in the input layer to perturbation. So, what they wanted to do was figure out a relationship between geometric invariants and algebraic ones. So, they had a neural network, learn the data set, and then they started poking at the neural network to try and extract associations between these quantities. So, they poked it with something like, say, the adjoint torsion degree, and it turns out that perturbation. Degree, and it turns out that perturbing the input didn't do very much to the output at all. So you might guess that it's not that important to the output that you're trying to predict. On the other hand, these invariants at the top, when they perturb them a little bit, you actually got a lot of variation in the output. So you might guess that the parameters associated to those inputs are the more relevant ones, and that there's actually a relationship between these quantities. So in fact, once they learned this, they gave this pile of data to the mathematician and said, hey, we think there's a relationship between Hey, we think there's a relationship between these three guys and some other invariants. And they were actually able to then go and prove a theorem based on those predictions. Okay, so how can we apply this to our problem of computing periods of hypersurfaces? What we needed to extract features. The first feature that is available to you is, you know, you take a vector, you take these quartic polynomials, there's 35 different monomials of degree 4, so you can just turn this thing into a 70 by 1 vector. This thing into a 70 by 1 vector. Okay, so we trained a multi-layer perceptron on the x-y pairs for this guy, and it turned out to be okay, but it didn't do great. So we tried to extract more relevant features, and the next thing we tried is, well, we have these meromorphic periods and a differentiate by t operator. So we're going to differentiate our meromorphic differentials by t and then evaluate those different, like express them in terms of some homology basis, and evaluate those matrices at either 0 or 1. Either 0 or 1. Right? And then we get a bunch of pictures. Well, we get a matrix of rational numbers, which we then turned into pictures. These are complicated rational numbers, so we just assigned some sort of height score to them in the hopes that this would be relevant. And then we ran this thing through a convolutional neural network that just tries to read the thing as an image and then use that to predict how long will this computation take. And you could see kind of what it's picking up on, right? So if the matrix of this... You know, matrix of this first-order differential matrix is simple looking, or it looks like it's upper triangular or something like that. It looks like, oh, yeah, it's going to succeed in this ODE computation. And, you know, if it's complicated, it looks like it's going to fail, but it's not precisely that. There are things that the human would never pick, or at least me, the human, would never pick up on. Maybe there are some humans who are better at this than I am. But basically, like this complicated-looking input. Like this complicated-looking input actually managed to terminate within 30 seconds, whereas these simple-looking inputs didn't. And it's a bit mysterious as to why this was the case. So somehow there's something the neural network knows that I don't. Okay, the best network that we actually used was some combination between the two of these guys. So we, through the sort of vector of coefficients through our multi-layer perceptron, we threw our first-order cohomology matrix. Our first-order cohomology matrices through this convolutional neural network, we ran them through a soft end, and then we got some sort of prediction that actually turned out to be pretty good. So now we can go back to our version of Dijkstra, but now we have an oracle that can estimate, well, maybe this computation is going to succeed, and maybe it's going to fail. Of course, this oracle isn't reliable, and you shouldn't trust it, but it does help us sort things by plausibility. Things by plausibility and thereby get from point A to point B. And in fact, it lets us get to certain destinations we never would have been able to get to by just doing classical techniques. Of course, when I described this algorithm, it looks really simple, like it's four steps and it fits on the slide. But when you're dealing with this much data, like our total data set ended up being something like 80 gigabytes. And then stuff starts to break. So the engine driving this thing is about 3,000. Engine driving this thing is about 3,200 lines of code. The engine driving this thing to make sure that, oh, what if the server crashes in the middle of your computation? How do you avoid trying the same computation twice and wasting time, et cetera, et cetera? That kind of builds up in the complexity of what you need to get this going. Okay. There are further compute details I could talk about, but I think I will refrain from doing so. How much time do we have left? How much time do I have left? Okay, great. I will refrain from doing so. You can ask me about the details later. But, okay, there's obvious equivalence between K3 surfaces, which is like, you know, two are equivalent and you can permute the variables around and get, you know, send one polynomial to the other. And out of all of these S4 equivalence classes, we ended up getting 96% of the total number of Number of targets within our data set. There were like five classes that are still holdouts to this day. So, you know, it could be that these are just fundamentally unreachable with the current implementation of the algorithm or the current hardware that we have at the moment. So finally, I just want to point out that there's some other things, some other ways that people are thinking about using deep learning and mathematics to obtain rigorous results. So, for instance, So, for instance, Pfeiffer, Stillman, and Helper and Weisner tried to do this with Grubnobases algorithms. So, this is an example where an agent, so they use unsupervised learning to try and figure out, well, at any given instance in Buchbrook's algorithm, you have to choose two pairs of polynomials to run some sort of S-pair elimination step to generate new things inside your ideal. And if you do this enough times, you'll end up with a Grobner basis for your ideal. For your ideal. And of course, at posteriori, you could certify whether something is or is not a global cases. So, what they did was they just tried to have an agent make those decisions in the midst of that algorithm to choose, you know, maybe this is the pair that you should pick to get a growing basis faster than this other pair. There's also, of course, the deep mind paper that I mentioned where they try to use neural networks for conjecture and discovery rather than actually obtaining the. Rather than actually obtaining the rigorous results, there's new work coming out on, you know, again using a sort of reinforcement learning model to take a knot and perform a sequence of like radomice the moves to see whether or not it's the unknot. And there's of course various work about say trying to feed the neural network a whole bunch of data and seeing how good is it at even predicting mathematical invariance in general. So due to the chime, I think I will end. The chime, I think I will end. But thank you all for your attention. I will point out that the code is available on a GitHub repository. The data set is 80 gigabytes. We did manage to send this to someone, but you might need to talk to me if you want it. All right, thank you. Oh, there was a question. Okay, so you said do you have certificates for like for accuracy? Yeah, so all of the complex arithmetic we were doing is using this ARB library. So whenever you do some sort of computation with these numbers, it adds the complex numbers and also adds together the error balls around them. So hopefully by starting with small enough error balls and hoping the error doesn't blow up too much in your computation, all of the arithmetic will then give you a little bit of a line. All of the arithmetic will then give you complex numbers with error balls around the estimate. So the periods we get actually do have error balls certified around them. Okay, so that's great. So can you turn some of these computations into proofs of 10 sentence? Oh, I don't really know. Somehow I think the answer is no because we only have like the sort of approximate representation of these numbers and I don't know the theory well enough to And I don't know the theory well enough to sort of say whether or not that's enough information to tell you whether these numbers are algebraic or not. You might guess at algebraic relationships of a complex number, but I don't know if there's a way to certify them with our data. Do you have to worry that the matrix of periods doesn't determine it doesn't vanish as you move along the path? Well that's not As you move along the path, or that's not important. Oh, well, there's a little bit of an art of like, you know, what if my path crosses the singular locus at some point in time? Then all of a sudden my period, like, well, the IPP solver is going to start complaining as I get closer to the singular locus. So, what we do is this Voronoi diagram trick where we compute where the singular locus is, which in a one-parameter family is just going to be some finite set of points, and then we try our best to avoid. And then we try our best to avoid the singular locus. Of course, if you get near a singularity because you're kind of forced to by like going through some sort of barrier, the numerical computations sort of get more unstable. And sometimes you can cross that mountain range, sometimes you can't. Perhaps one comment, which is a little bit of an issue with R, and that is if you read the documentation, you If you read the documentation, you see that their computation of the error bound itself was not actually certified. So it's assuming that you're using enough working precision, that is still actually an accurate representation. Ah, okay. Good to know. Thank you. I'll see if Emre has thought about this. He's a little bit more familiar with the sort of numerical side of things. And the next talk was in the afternoon.