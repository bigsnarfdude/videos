I have a disclaimer. I'm going to be very informal in this talk. I won't have twenty minutes. But I hope you understand the motivation behind this line of work. Okay, so everything is going to be about unrestricted circuits. And I guess like the status of lower bounds against that circuit is very primitive. For instance, we don't. For instance, we don't know how to separate NP from circles of size n square, and I'm not even aware of a promising approach like how to start addressing a question like this. So this seems to be very far from what we're doing right now. However, there have been a lot of progress in understanding these unconditional lower bounds for larger classes. So for instance, if you have a probabilistic polytime computation with an NP. Time computation with an NP oracle, then we know how to unconditionally separate the class from like circles of size n square or n cube or any fixed phenomenon. And we also have lower bounds for the randomized version of MA if we have like one bit of advice on each input length. So we do have some significantly strong lower bounds for larger classes. And one frontier here that has attracted attention is how do we get lower bounds for smaller classes? How do we get more bounds for smaller classes or incomparable, but in particular for classes that are deterministic, like P to the NP? Okay, so we still don't know how to do it, and this is one of the frontiers in this area of lower bounds against unrestricted surveys. And there's a second frontier, which is although we do have lower bounds for larger classes, like the exponential time version of MA, for instance, we can separate from P slash poly. Separate from P slash poly. All these lower bounds, they don't show that the problem is hard on like every large enough input length. They only work infinitely often. And this is something very intriguing, and we currently don't have techniques to understand this situation well. Now, is it reasonable to expect that for natural problems, on some input lamps, computations could be easier than in other input lamps? So it's not so clear. So, it's not so clear, but let me mention like a hypothetical scenario here. So, in many computations or algorithms, we make use of some kind of mathematical objects. For instance, finite fields. And we know that finite fields, they don't come in every size, right? Only prime powers. However, like these structures, they are significantly dense in the set of natural numbers. So, if in the input length you care about you don't have a field, we can find one that is slightly larger. One that is slightly larger. But it's a possibility that there could be some very exotic structures and they are very sparse, so very far away from each other, and that they could help us doing some computations around some input lengths. This is something that seems unlikely, but we cannot rule out at this point. And this issue of understanding hardness on our input lengths also translates to some Translates to some interesting phenomena in the realm of algorithms. So, for instance, the fastest known algorithm that we have to generate canonical prime numbers, it is only known to succeed infinitely often. So, it's a very natural question and it is affected by this sort of gap in our understanding of hardness on how input lands. So, in this talk, we look at these two frontiers, understanding like These two frontiers, understanding like lower bounds on these classes, and understanding this lower bound zone, like all input lengths, from the perspective of logic. So what do I mean by this? So we'll be using like theories in the standard sense, so first order logic. And we are interested in what these theories can prove about algorithms and complexity. For instance, does theory T prove that SAT can be solving all the time? This would be like an interesting question. Will be like an interesting question in this area. And my personal take on this line of work is that we're trying to understand a complexity theory that takes into account not just the running time, but also how difficult it is to establish the correctness of computations. Okay, so it's a way of refining complexity theory. So let's be more specific. We'll be discussing fragments of piano arithmetic. And so this theory is they're supposed to talk about natural numbers. Supposed to talk about natural numbers, but of course, numbers can represent like other finite objects like binary strings. So we can discuss computations in a natural way, although the theories they talk about numbers. So I won't describe all theories using the work, but I want to briefly explain an influential theory from the 70s. It's called I delta 0. So it uses the language of arithmetic. So you have the constants and addition, multiplication, and the order relation. And then you have like finitely. And then you have like finitely many axioms that relate the symbols. For instance, for every x, x plus 0 equals x, and so on. But most importantly, these theories they have some form of induction. And why is induction extremely important from the perspective of algorithms and complexity? So we use it everywhere, right? So if you're like teaching textbook algorithms, like divide and conquer and dynamic programming, we prove that these algorithms are complex. Let me prove that these algorithms are practiced by induction. So, you want to have some kind of induction axioms in your theory if you want a reasonable computation. So, in the case of i delta zero, the induction scheme is only included for what is called the bounded formulas. And this is a very important notion in bounded arithmetic. So, a bounded formula is one that only contains quantifiers where we place an upper bound on the sides of the objects. Okay, so this upper bound is given. Okay, so this upper bound is given by a term in the language. In the case of i delta 0, it's going to be a polynomial. And this term cannot refer to the variable x. So in particular, this is what distinguishes these bounded theories from the arithmetic. Okay, so the fact that we have like upper bounds on these quantifiers, they sort of move the theory from like computability to complexity theory. Compatibility to complexity theory. So, in particular, if I give you a bounded formula and you want to evaluate it on some three parameters that you fix to be some natural numbers, then you can do it in exponential time. But if you didn't have these upper bounds, then this could be even like undecidable. Okay, so this is bounded arithmetic. And however, this theory is not particularly useful when we talk about polytime computations because this kind of Computations because this kind of upper bounds here, because they're like polynomials, if you look at the bit length of the objects, it only grows linearly. So, you would like to have like a better language to manipulate polytime computations. And this is done in some different theories introduced by Steve Cook and Sambas. So, these theories they have a more appropriate language and they're more closely related to levels of the polynomial hierarchy. So, the idea is that you're gonna restrict the induction axiom. Going to restrict the induction axiom just to certain kinds of bounded formulas. So, for instance, the FBT12 has induction scheme only for formulas that correspond to NP predicates when you interpret the formula in the natural numbers. Okay, so sort of like restricted to which kind of induction hypothesis you can make to prove correctness of your algorithms. And in this talk, we actually use a more expressive language. We're going to have simple. We're going to have symbols in the language for every polytime algorithm. And I want to stress that this is not introducing extra power to the theories. Here you're really just like the theories in the weaker language, they can already talk about like finite programs and fixed polynomial bounds and how different programs can call each other. So the only thing you're doing here is really just giving a name. Here is really just giving a name to each polytime algorithm. Okay, but you're not introducing like an extra power. So this can be done in a meaningful way. But crucially, it doesn't mean that the theory, although it has names for our polytime algorithms, can prove correctness of the algorithms you care about. So one example is that although T one two has a function symbol for the primality testing algorithm, it doesn't mean that it proves that the function evaluates if and only if the inputs are prime. Only if the inputs are prime. And there's like a hierarchy of theories where you have induction for more expressive formulas. And the base case is Pv, has the weakest induction. It goes all the way to this i delta 0 I mentioned before. So it depends on how many quantifier alternations I have in the induction. Yeah, so this is something technical, so let me not technically. In Spanish. Yeah, so if you're a student, you can read more about bounded derivative in some textbooks, this recent mode by Kerala tech. But for this talk, let me just tell you what things we can formalize. You already know how to formalize in boundary derivative. So for instance, in PV, we know that the PCP theorem can be proved, meaning that this reduction that creates a gap for SAT, you can prove its correctness in PV. Okay? And if you go to like to a stronger theory, If you go to stronger theories, like T22, then we know how to formalize the standard lower bounds, like for AC0 and for monotone circuits. Okay, so a lot of interesting facts can be done. But often, to formalize these proofs, you need some kind of like it can be very ingenious, okay? Because it's far from clear how you're going to manipulate things like probability space or real-valued functions. So you need to try to make sense of these arguments inside this restricted space. Sense of these arguments inside this restricted setting. And for the rest of the talk, we're going to focus not on what we can prove, but on independence results. So let's see how we can say things about circuit complexity. So we have names for all polytime algorithms. So here's a way to say that parity function has circuits of size 5n. So there exists an object, and this object, you can check its description. Object, you can check that its description is a circuit, so this is a polytime computation of this encoding. And it has like n variables, it has five n gates, and on every object whose bit length is n, if we evaluate the circuit on the object, it's one if and only if palette is one. Okay, so all these are like functional symbols in the language. These are polytime algorithms. And let me skip this technical detail here. So this sentence talks about parity, but you could replace this to a more complicated But you could replace this to a more complicated predicate. So we are also able to talk about the circuit complex of like predicates defining other problems, other functions. And this is like done in a very natural way. So you just use these quantifiers and you can claim that something can be done by circuits of size n to the k, or maybe that it doesn't have circuits of size n to the k. So it's very natural, you can write down the sentence. The first sentence you can prove you can have in PP? You'll be able to prove this in PvC. And two directions have been considered in this area: on probability of lower bounds and on probability of upper bounds. Let me briefly mention on probability of lower bounds, because I think this might be more familiar to most of you. It was initiated by Rasborov in the 90s. And I guess the main motivation here is really trying to understand why is it so hard to prove this super complex lower bound. Hard to prove this with complexity lower bounds, right? So now let's look at the axioms, let's see what is missing. Can we formally understand this hardness? And of course, there are also other motivations for this, but I think this was perhaps the initial one. So here's a concrete question. We know that this theory established the monotone overbounds. Can we show that, well, you need new techniques to handle non-monotone circuits. So it would be like extremely interesting to establish something like this. To establish something like this, and hopefully, like unconditionally. But unfortunately, for probability of lower bounds, the unconditional results that we have, they are still very weak. Okay? So, our paper is about on probability of circuit with upper bounds. Let me give you the motivation for why we look at this. So, right now we cannot rule out that SAT has very small circuits. But, can we at least show But can we at least show that like using these restricted axioms, it doesn't follow that SAT is easy? Okay, so that's the motivation. So we're trying to look at specific techniques, and although we believe the problem is hard, let's just say, well, at least we cannot come up with a good circuit or algorithm using these techniques. Okay, and I claim that this is at least as interesting as the previous direction for the following reasons. First, if you believe that SAT is hard, it's Believe that SAT is hard, it is necessary that you show that an upper bound cannot be proved in a given theory. So, you believe that the upper bound doesn't exist at all. So, you're looking at a specific theory now and trying to get evidence for that. And if the theory is natural, you know that this theory can formalize many algorithms, like dynamic programming. So, if you're showing like an unprovability result in the sense you formally uh explain that some techniques won't help you to Some techniques won't help you to prove these upper bounds. It also gives formal evidence that the problem is hard. Because from the unprovability of an upper bound, we know by basic logic that there exists a model of the theory where the problem is hard. And if this theory formalizes many results in algorithms and complexity, it means that this model is not too far away from the standard mathematical universe. And finally, connection. And finally, connected to the title of the talk: if you can't prove a sentence, then you can add the negation of the sentence to the theory, and you know that this is consistent. Okay, so if you can prove upper bounds, then you can develop a theory of computational complexity where you can add computational height as an assumption, and you know it will never lead to contradiction. Okay, so these are different perspectives on robots doesn't seem to make you evidence that's Doesn't seem to me to give evidence that the problem is easy. So, why is it the case that I'm proof here or upper bounds? Well, we can also view it as evidence that it's easy, depending on your beliefs. Like, if you're really not biased towards things are easy or hard, then it works in both directions. Yes. Sorry, can you go back? I just have a cover. So the second uh two like V M satisfies many normal science and algorithms and complexity. Yes. Do you do you go in and like check those theorems in the model? Check those theorems in the model? Or do they have to do that? So if you have already proved these theorems in the theory, then you know that any model is going to satisfy those theorems. Oh, okay, so it just inherits all the existing proofs that you've already formalized. And with three, say something like, if we promise to never use algorithms whose correctness we can't prove in whatever theory you've targeted with this, then it might as well be the case that SAT is hard. For that class of algorithms, is that? Yeah, from the perspective of this. Yeah, with the from the perspective of this theory, uh there exists hardness and you can use it and you know it will never be contradictory. Okay, and you can like further develop this in possibly interesting directions. So a few works connected to this line. Kupink Raichek studied the consequences of the probability of upper bounds and they established some conditional results. A few years ago with uh yeah, we proved unconditionally that PV doesn't prove upper bounds. PV doesn't prove upper bounds for problems in P. And more recently, there's a model-theoretic version of this proof that gives you something slightly stronger. But I guess the weaknesses of this previous result are that they only apply to PV, where we would like to get improvability in stronger theories. And also, they are affected by this almost everywhere, whereas it's infinitely often an issue. PV could still show that problems in P are easy. Could still show that problems in P are easy infinitely often. And here we would like to use logic to maybe get some new insights into this. Maybe we can say something stronger. So in this work, we get the probability of upper bounds in T12 and weaker theories. And they are strong in the sense that the theories can't even prove infinitely often upper bounds. And our results are unconditional. So let me start with an informal statement. Start with an informal statement. So for each constant, T12 cannot prove that P to the NP is infinitely often in size n to the k and so on. So for the weaker theories, you get like a smaller class. Okay, so in particular, we now know that P V can show P is infinitely of n size n to the k, which was unknown for P V. And let me mention an extension. Mention an extension. So you can even add several new axioms to this theories, and the improvability result still holds. So, for instance, you could add to 212 for our sigma b1 sentence that are true in n, and still the upper bound is not provable. So, for instance, here's an example of such a sentence. So, for every uh x, for every number x, that's how you should think of it. For every number x, that's how you should think of it. So x is composite if and only if aks are if an output is 0. So this is like a for ru and a bounded quantifier. And this is true in the natural numbers, so it's part of true y. And even with like such kind of axioms, we still cannot establish the upper bound for the entire P to the N P. Although now because you have this correctness, You have this correctness, you know, for instance, that the theory proves that there are small circuits to the side primes. Okay? So does tell us something that is sort of like connected to what we do in algorithms and complexity. So quite signal one, right? Because it's it's an equivalent. Say again? It's an equivalent and therefore it's not signal one. Yeah, but you can take the universal quantifier and move it to the outside universal quantifier. Okay, let's take it off line. Uh okay, so here's a more precise statement. So these theories can talk about the entire uh classes, but they can talk about uh sentences expressing that some particular predicates have small circles. And uh formally what we are proving is that there are some predicates defining uh over the natural numbers languages in this classes. Languages in these classes, P to the MP, and for some of these predicates, the corresponding theories cannot establish the upper bound, not even the infinitely often sense. Okay, so that's how you formalize this. I don't have much time left, so let me just mention that right now we know two approaches to unconditionally establish such results. One of them is by exploiting a connection to One of them is by exploring a connection to Karp-Lipton theorems. Some of you might have might know that Karp-Lipton theorems uh is a way of proving this fixed polynomial lower bounds, but we don't have them for p to the NP or below. The collapses are not strong enough. So what we use here is the fact that if the upper bound is not just an assumption, but it's also provable in a theory, you know, you can get stronger consequences, stronger collapses. That's sort of like the initial idea to prove such theorems. And so this approach. And so this approach doesn't quite work for PV. And here we need a different technique, which is that our sentences are talking about non-uniform circuits. But again, because we have these proofs and we have some understanding of the theories, then we can extract more information about how such circuits are constructed. And we connect this to existing techniques in complexity here. Alright, so let me, before I finish, just tell you about. Before I finish, just uh tell you about um this infinitely often in almost everywhere issue and how this is connected to bounded arithmetic. So I I have a simple example. So looking again at i delta zero, so if you take any bounded formula, then it's a result of Parik that if this figure i delta zero shows for every x there exists y such that a xy, then the figure can also place. Then the theory can also place an upper bound on the size of this object y. It's also possible to prove this inside the theory. And now let's try to see how this can be useful for algorithms and complexity. Suppose now that one of these bounded theories, which also satisfy similar results, they could prove that SAT is easy infinitely often. Okay, so on infinitely many input layers, Okay, so on infinitely many input lengths, that is easy. But now, if you apply a theorem like this, the theory will also be able to place an upper bound on the next easy input length. Okay, and this upper bound is going to be polynomial. So now you know that SAT is easy on many input lengths, they are at most polynomially gapped. And because SAT is a paddable language, I can make just look at a slightly larger input encoding and I can solve my uh instance at in ha at hand. Instance at hand. Okay, so in this case, an infinity of an upper bound will translate to upper bound that is everywhere. But this only happens because sat is paddable, and not every language is going to have this property. So in our proofs, we need to do something more. Any questions so far about this proof techniques? Okay, so uh let me uh just finish uh with a significant conjecture in this area. Uh it is a way of investigating the P versus N P problem from the perspective of bounded arithmetic. So given a function symbol in the language of P V, a polytime algorithm, we can write a sentence that expresses that this algorithm correctly solves SAT. In the following sense, whenever a formalist Sense, whenever a formula is satisfiable, if you given this formula to the algorithm, it's going to print some satisfying assignment. So you formalize this as follows. So for every formula x, for every y, if y satisfies x, then f of x also satisfies x. So that's a universal way of capturing p equals np, being witnessed by this function symbol f. And a conjecture here about And the conjecture here about unprovability of upper bounds is that for no function symbol in the language of PV, this theory proves this P equals NP sentence for this function symbol F. If we could establish this, this would be very significant because PV is able to formalize some very nice algorithms, like textbook algorithms. And this would be a very general and principled way of saying, you know, if. Saying, you know, if P equals N P, you're going to need some completely different argument. And you can view it as like an argument that giving more evidence that P is different from NP. So we show in the paper that if you want to investigate this conjecture, it is enough to study improvability of lower bounds, like something I mentioned earlier in the talk. Okay, so in some sense, these two research lines. These two research lines are connected, improve a bit of lower bounds, and the probability of lower bounds. And I think it will be extremely interesting to get a better understanding of this. Right, thank you. Question for you? About the uh so so you proved that uh it is unprovable uh Some upper bounds are improvable. So I think there is the result that shows that if you have extended frag and lower bounds, then you get unprovability of upper bound of NP equal to P. So if you show extended frag is not super, then in particular you establish this conjecture. That there's a way of showing that. So do you have another connection to your you showed this for the N to the K, right? The n to the k, right? So maybe there is a connection to propositional proofs that you get this from lower bounds. Yeah, I haven't tried to explain this. Possibly. What's the connection? So the 02, I mean, what kind of connection? So if let's say P V cannot show enter the K lower bounds for languages in P, then this is true. So I guess Uh what about like uh something like canon, like a symbol canon hierarchy here and like circuits of size n to the 2k superstar circuits of size n to the k. Can these people marked? Yeah, that's an extremely interesting question. So this is a form of the pigeonhole principle. Right, before you know that there are circuits there are functions that don't have circuits of size n to the k, you need to prove the existence of such functions. You need to prove the existence of such functions. And one way of doing it is that you look at the map, taking like small circles with two tables, and then you want to claim that there exists some string that is not in the image of this map. So it's not known if this can be done in PV. That's an extremely interesting question. Alright, so uh let me uh finish uh with uh advertisement. So we are organizing uh September We are organizing in September Czech Republic workshop to celebrate Jankrajek's 60th anniversary. And there are many great speakers coming. And following this workshop, we're also going to have like, it's part of a larger event, Complexity Theory for a Human Face. And we're going to have also tutorials and contributed talks. So if you're interested, go to my website and you can find more information. Website, and you can find more information, or talk to one of the organizers, Pavel Rugesh and Jan Pika here. In particular, you might be wondering what is this phase here. Me too. Just give me a slide to this important phase. All right, thanks again. Okay, so