We'll be given by Andrew Claudellins. Yeah. On catalytic embeddings. Okay, so I think you had a cool talk scheduled for now, but unfortunately it fell through. So you had me instead with a last second board talk. So, you know, buckle up because I didn't exactly prep for this all that much. So, you know, let's keep it loose here, basically. Yeah, so basically, I'm going to be talking. Yeah, so basically, I'm going to be talking to you today about these things called catalytic embeddings. This is work by myself, a couple of members of the audience, Matt and Julian, Sam Mendelsohn was online, I think, at least on Monday. What else? Lisa McKessia and Matthew Crawford, that's right. A lot of names. And we have one paper out on archive right now. There's a QIP type. Right now, there's a QIP talk if you want something a little bit more formal than this that you can go watch on YouTube from like two years ago. And there's upcoming work that's been sitting on an overly for a little bit too long that we need to get up basically. But I'm just going to introduce you to the concept if you're not familiar. And yeah, we'll see what we can kind of do with it. So the basic idea of what we're Basic idea of what we're trying to do is basically take circuits that we think are going to be expensive. So, imagine you have this, you know, just general circuit, right, where you have an input and some unitary that you want to apply. And let's assume that we have some, say, fault-tolerant gate set where naively this gate is not going to be exactly expressible over that gate set and is going to require some amount of approximation. Going to require some amount of approximation. Well, there's a bunch of different algorithms and cool new work, it looks like from earlier in the week that basically says this is going to cost you, you know, oh, log one over outslog, roughly speaking, essentially, no matter no matter what you do. Great. So we'd like to maybe not pay that much if we can avoid it. So, you know, all these algorithms say like basically that's as good as you. Algorithms say, like, basically, that's as good as you could do. So, why do we think we could maybe do better? Well, the basic thing, the basic premise of maybe what we might think about trying is instead of applying U directly, why not try and search for some other unitary that is actually exactly expressible over a gate set? And using this unitary in maybe some state that you have to prepare. Actually, get the action of the unitair you want out on the other side. And in particular, we're going to hope that we can find a way to essentially keep that resource state along with us. And so what would that buy us, right? Well, if we didn't have this, right, if it was just we had to consume this state every time, then we're not really helping ourselves because this state being a state that's probably State being a state that's probably not expressible, not achievable from like a standard computational basis state, and our gate set is going to require approximation. So our hope is we can produce this state precisely once to be able to apply as many of these as we want. So that's the basic structure here, right? You imagine you have a circuit with many of these unitaries and then some gate sets that are expressible over our gate set. Are expressible over our gate set. Suppose this is on many qubits, so you can't just sandwich these together in like a, you know, using linear algebra on a computer. Then when you take this structure and apply it to the circuit, what you get is some sequence of these bigger unitaries and acting on this catalyst space where you can kind of picture each of these just applying. Each of these just applying you know a single UGIP, right? And then you get out the action of the circuit that you want. Okay, so that's that's the basic premise of what we're at. So you hope chi to be the same for different five of us, right? To be able to implement different unitaries? Not necessarily. Like suppose suppose we just had some, like, you know, we'll get to an example where we show that like a root T gate, for example. That, like a root T gate, for example. You know, you can, if you're just going to add a root T gate, then they can all use the same cactus. And you can certainly come up with catalysts and sets of these gates where you can all inject all of those gates with the same catalyst. And there's some like broader questions about like, what if I want to parallelize circuits or keep parallel structure? But that's a whole can of work. So we're not going to go there quite yet. But yeah. So can. But, yeah, so can we ever do this in practice? And it turns out that we actually can do it. And so, the sort of motivating example for all of this is essentially just the state injection protocol that Nadish had talked about earlier for the particularly efficient diagonal case of something in the clipboard hierarchy. So, when you want to apply a T-gate, you, you know, in a fault-tolerant setting, you can't apply these. Fault tolerance setting, you can't apply these directly. You use this like magic state distillation and injection protocol. So you produce this T state, and then you run some Clifford circuit that is adaptive, right? So you perform a Clifford operation, perform a measurement, and then given the value of that measurement, perform another Clifford. Okay, and that gives us the action of the gate we want. Now, we could certainly, instead of applying this measurement in This measurement and adaptive Clifford directly, you could think about doing something like a delayed measurement, right? Like if you basically pull this measurement back, this becomes a controlled escape. And so that essentially becomes this circuit if I had like a measurement here. But if you just kind of like take off that measurement and ask, like, well, what's the state I have out on that other side? It actually turns out it is precisely the T state that you started with. So this is already kind of. So, this is already kind of pretty cool, right? But what's really, really interesting to me personally, as someone who's looked a lot at like characterizing gate sets, is when you look at the circuit in here directly, I've got a CNOT gate and a CS gate. So if you look at the types of circuits that you can implement using Clifford and CS gates, they actually don't contain the T gate whatsoever. Okay, so that the entries in the matrices just don't work out to where you're going to be able to get a T gate. So, in some sense, this structure is allowing us to introduce gates that were never in our original gate set to begin with just by using this kind of reusable cannabis state. So, this is this kind of very interesting notion that even though, like, Notion that even though, like, say, the Clifford and T gate set aren't equivalent, they kind of are equivalent if I have access to exactly one T state that I can reuse forever. So this is already pretty interesting, right? But the CS gate here, you know, for those of you who know your Clifford hierarchy, you're like, well, this is in the third level, the Clifford hierarchy. Like, I'm just paying a third level thing to get a third level thing. Level thing to get a third level thing, like why do I care? And so, to answer that, we can go to kind of like the next obvious level of this thing. So, it's like, what if we try this with like a root T gate? And if we try this with the root T gate, you can go through the same procedure and find that you can actually basically generalize this entire construction. And this works for like arbitrarily fine. Like arbitrarily fine generalizations of the T gate. And again, this is not, this is a fourth level gate in the hierarchy. This is a fourth level gate, but you can take this circuit and synthesize it exactly over Clifford and T. So you can use only third level gates to get a fourth level gate. And those third level gates you're using have some very fixed cost, right? So this normally when we want to apply root T, Normally, when we want to apply root t, we're like, okay, we're going to have to apply a log one over epsilon thing, whereas this has some O1 fixed box. Okay, so it feels like this is somehow buying us more accessibility for what we can get with cheap circuits than we would normally be able to get. Okay. So I want to take this example and maybe zoom in. And maybe zoom in a little bit to understand maybe what's going on. And so I'm just going to take this circuit here and write it as a linear operator. So if I do that, I have the following circuit. So I'm going to use omega as a th root of unity here. As an eighth root of unity here. So I'm going to put some maybe suggestive dotted lines in this matrix. So if I take those suggestive dotted lines and try and picture kind of the tensor product structure of this matrix, then this is like kind of what happens when I start in the zero state and end in the zero state. This is like start in the one state. zero state this is like start in the one state end in the zero state uh reverse that and then the the one one to one so if i just write this uh kind of using raw ket notation i get something like zero zero tensor the two by two identity matrix plus one one tensor zero one 0, 1, omega, 8, 0. So, you know, what's interesting about this? Well, if we go back to kind of the gate that we were interested in, right, that gate would have the form of 0, 0 times 1, basically, plus 1, 1 times a 16th root of unit. But if I take this matrix, But if I take this matrix and just check what happens when I square it, I find that this gives me the matrix omega 8, 0, 0, omega 8, which is like omega 8 times the identity, right? So somehow this matrix is behaving exactly like the thing that we're essentially trying to. The thing that we're essentially trying to get in that entry, right? Like we wanted a 16th root of unity, and we found it instead of finding a number that behaves like a 16th root of unity, we found a matrix that acts like a 16th root of unity. And this shouldn't be surprising for anyone who's like a big number theory person, because there's this strong notion that you can take complicated number fields and represent entries in those number fields as linear operators over simple. Operators over simpler number fields that live inside the bigger number field. So, this matrix, to make this a little bit more concrete, this matrix here satisfies the following equation. Right? So basically, if I take this matrix and kind of interpret omega eight here as omega eight times identity, and zero here as like the two by two zero matrix, it satisfies. zero matrix, it satisfies this equation. This equation is exactly the minimal polynomial for the 16th root of unity over the number field generated by the number field q bejoin omega 8. Right, so we've constructed a matrix that satisfies the minimal polynomial for that particular algebraic number. And it turns out that you can basically just take this structure and just absolutely run with it. Absolutely run with it and it just essentially works. Right, so this is going to give us an entire prescription for how to construct these things. Now, it should be, I should revisit this for a second here. So what's going on with this state here? What's this root T state? So that root T state is basically root T acting on plus, which is one over root two, one omega. Omega 16. I didn't screw anything up. And if I check what kind of relationship this has with this matrix over here, you find that it's actually just the eigenvector of this matrix that has the eigenvalue of omega 16, which we know must be one of the solutions to this minimal polynomial here. Right, so now we've got a way to both get a matrix that To both get a matrix that's doing kind of what we want in this context, and we've got a way to kind of extract out the algebraic number that we really wanted just by picking the right eigenvector of that vector. Okay, so we just go and generalize this really nicely, where you say, suppose I have some field F and I want to take a unitary. A unitary described over f adjoint alpha, where alpha is some algebraic number over f, and I want to map that into some matrix just over f. Then all I'm going to do is I'm going to take my unitary u, break it up into pieces of the form u i alpha to d i, where this is, you know. Is you know over your base field f and alpha is this this thing you're adjoining and I'm going to replace this with essentially a tensor product with some now matrix A where this is still the same thing over F, but now this matrix A is also over F. Where A solves or A is annihilated by the minimal polynomial of L. Okay, and it turns out. Okay, and it turns out that when you try this and you just plug in some matrix A that does this, if you start with a unitary, you're generally speaking not going to end up with a unitary. So you're like, oh, like maybe it doesn't work. So why did it work in this particular case? Well, it turns out it works in this particular case because the precise matrix that we get here is normal. So if we put on this extra condition that Extra condition that where a normal matrix A so enforcing that A is normal along with being annihilated by this minimal polynomial will ensure that the resulting thing is actually a unitary matrix. Okay? Sorry, let me just clarify what you're putting. So imagine you. So yeah, imagine you with your square root of t gates. What are the ui's? Should I take a bowl? Yeah, so if you just write the square root of t gates, each of these is an element over alpha, so like q adjoin omega 16. And so you can decompose those. So writing elements of extension fields over as like in this form of like something in the base. Like, in this form of something in the base field times powers of things in the thing you're extending by is essentially just like a linear algebra construction. There's a basis for your thing. And so you can write this as basically the matrix 1000 plus you suggested to me that it was a unitary work. Yeah, sorry, sorry. So these these are not unitary themselves. They're just kind of like pieces of the unitary that you're breaking off depending on. That you're breaking off depending on what power of alpha is appearing in the given entry point. Okay, so I've talked a bunch about fields. For weird reasons, in quantum computing, we don't really deal with fields all that much for like fault-tolerant quantum computing. We tend to deal with number rings. So you can ask the question, like, well, what happens if I just like say like, well, that's not a Just like, say, well, that's not a field anymore. That's just some ring R that I happen to care about, say, like one of these Z adjoin one-half rings or something like that. And the answer is it basically works. There's a few asterisks in that number rings can have a bunch of like complicated properties that mean like they're not, you know, instead of a basis, like number rings don't always always. Basis, like numberings don't always have a basis in terms of alpha. But like for basically any numbering that we would actually practically want to do this stuff with, it basically just works the same, is what I'm trying to say. Okay, so how much time do I have? Five minutes? Okay, so maybe I'll go to just talking about Just talking about this quick, which is the QFT. And so, this on the left here is the QFT. Yep. Before you cover that, I was going to ask a question about the circuits. Can you make it, please? Okay, so this square of T state is constructing that cost about the same as constructing of T state, as if not, there's a trade-off. As if not, there's a trade-off between which user stage you use and how expensive that original decomposition might be. Yeah, so this has some fixed costs in terms of Clifford and T-Gates, this circuit that you would actually implement. I'm going to look at Matt and ask what is the cost for a controlled T-gate? It's probably like five or something. Nine T-gates, but I think. Fadim said five. I'm going with Fadim. Yeah, but that's a key relation. It's not quite the question, though. I guess, like, if you're using distillation, like to get that T state, that's right. It's like 15 to 1 protocol or whatever, and that has some cost. That's the cost of the square root T or what other resource states you'd be using, are those comparable? So I honestly don't know the answer to that question. Don't know the answer to that question. Someone who's looked more closely at distillation would have to be able to weigh in there. It could be that really this is just kind of building, if there are benefits to like, hey, it's not that much more expensive than getting tea. My guess is this structure is essentially building that cost benefit into it. Vanilla? I can quick comment, right? For example, you do control T. For example, like you do control T using one CCB actually and one T use CCZ cost of four T states, but in some architectures it actually can be much cheaper to distill CCZs than for T states, right? And this construction actually turned out to be like, say root Ts will have like relatively nice cost. And for example, like one example that like, say, say quantum architecture. Say, say quantum architecture, it's better to use approximate symbols using root keys than keys for this reason. And actually, catalysis is used to get shit with keys when you do a posting method using a field of post-boot keys. Okay, and so I do want to run through a slightly more complicated example, but before I do that, I want to say these things are actually useful. Useful. So, why are they actually useful? Well, on the left here, we have the standard implementation of the QFT. On the right here, we have, if you go to like Craig Gidney's blog or like read the Maslow paper on like improving QFT costs, you'll see a circuit that looks basically like this. These things might be written in a slightly different form depending on how the individual likes writing their classical circuits. And you can ask, like, well, how. And you can ask, like, well, how did I get from here to here? And the answer in both of those cases is some very smart people sat down and were like, I know how to do like, you know, adders and whatnot and thought about this and were like, oh, if I just add these extra states, then I can use kind of like the power of classical computing to do a lot of the rotations that I want. And the catalytic embedding work that we did basically gives a new lens on this. Did basically gives a new lens on this. Essentially, if you just take this circuit and then catalytically embed it into the say Topply-Hadamard gate set, which you can do because we've kind of seen how you can take these like power of two rotations and put them in that gate set, then the resulting circuit is exactly this. And the only thing that's not exactly this is basically you get. Exactly, this is basically you get out some classical reversible circuit. So you just throw that into your favorite optimizer, and then it like does the smart QFT implementation for you. So it's like, I don't have to be Craig to know that like I can do this. I can just use this tool, which is actually rather simple when you boil it down, and then throw this into my favorite circuit optimizer, and I should get the good implementation of the QFT. implementation of the QFT. So that's his motivation for why this might be a powerful construct, essentially. Okay, so I'm going to try. There's so much I want to talk about, but don't have enough time. So I'm going to try and go through one example very quickly to show that, hey, you can do things besides just like power of two rotations. So I'm going to give us a Rotation by an angle of 2π over 5 using a third-level clipper hierarchy gauge. So fifth root of unity is 2 pi over 5, right? E to the i 2 pi over 5. Minimal polynomial is this. And I can. And I can see if I can find a Flifford and T matrix that has this as that is annihilated by this polynomial. So I can, and I can write it down here. Writing that out. Is it hard to find these matrixes? Yeah, so it turns out in the field case, it is not hard. It's essentially a solved problem now. And in the ring case, I'd say it's an open question as to when you can find these matrices with this form. Yep. Just for my purposes, this is essentially the Kaylee-Hamilton theorem. Yes, exactly. This is exactly Kaylee Hamilton. So. So, this is a 4x4 matrix. You can go plug it into Mathematica or something and find that really it does have this characteristic polynomial for the matrix. So it's annihilated by this polynomial. And actually, the cool thing is, this is a Clifford circuit, too. So now we've got a way to apply like just a phase of a fifth root of unity. And so you can stick it. And so you can stick it in like a, you know, kind of a Z-like construct now, where you've got something like this. And so you can map this to basically, you know, that tensor I4 plus that tensor lambda, and bam. You can use this along with. Can use this along with the catalyst state, which is just going to be an eigenvector of this matrix here, to inject fifth roots of unity into your circuit. So that's super cool. Like there's no reason why you should be able to do that in general. You're naively. Yeah. So in this case, we're going to need, I believe, two qubit catalysts. That's right. That's right. Yeah, yeah. Yeah. Because it's all my fault. Yep. And I've been restricting to the case where basically these matrices. Case where basically these matrices have like power of two sides. You can absolutely do things where these matrices don't have power of two sides. It's just like now you use kind of like some subspace of that to basically have your catalyst live on. Yeah, Peter? Do you even need the catalyst state? I mean, if the matrix already is a fifth group of unity, like can't you just start? You don't know which fifth group of unity and just start with a random state or no state. To start with the random state or low state, so I don't think you can start with the random state because then you get this scenario where essentially you've got like uh you get a different root. Yeah, yeah, it's like you've got a you've got like a linear combination of the roots of unity. So it's like you've got some I almost think it should be useful for from an algorithmic perspective, like to do some algorithm in number theory. Like you're basically, when you look at the structure of these things, they're really related to like relative field traction. To like relative field traces and relative field norms and things like that, various properties of these things. So it feels like it should be useful, maybe, to do some interesting like algebraic number theory algorithms, or maybe not. One of the things I wanted to do with this talk is just put this in front of people and say, hey, do some cool stuff with it, please. Yeah. And I think that's a good time to ask the question being target time. We keep trying to climb and put the hierarchy here. The climate fluctuation hierarchy here. If you inject from the bottom, you're essentially doing an ancillary case. So, do you think this could give a new way of looking at the ancillas? Um, yeah, like basically injected from the bottom, it says like it's um polyacro. I think that could also be like doing plastic or something. So, these things say something that could be said about the symbolist before perspective. Yeah, so I don't know if I'm going to answer your question exactly, but I'll give you an example of. But I'll give you an example of how catalysts can give you new perspective related to ancillary statements. So, Julian, Matt, myself, and a few others put out a paper pretty recently on giving number theoretic characterizations of like Clifford plus increasingly fine power of two roots of unity. That problem prior to that work was like monstrously hard. I think Vadim and a bunch of people like John Yard worked on it for quite a long time. Yard worked on it for quite a long time and showed precisely what you could do in like the single qubit case, uses like some pretty advanced number theory stuff. That paper that Julian, Matt, and I put out is like, there's honestly like a page of map that probably a high school student could do. And it just like proves the result that like these gate sets are equivalent to matrices over unitary matrices over these number rings. So it's like, if nothing else, it's a really useful theoretical tool to really simplify how. To really simplify how hard a problem is. And it kind of points to this viewpoint that, like, hey, Ancilla, like, you know, these Ancilla, like, having like ancillary systems really makes life easier sometimes rather than making things more complicated. And I think the ZX community is definitely on that in that camp. Yeah. If I have a rotation angle, that is not a nice group of use. Yeah. So Yeah, so what can you do with this? So we have shown, it's not in a paper yet, but it's going to be in a paper whenever any of us have time. We have shown that you can basically do any rational root of unity using the diadetic case. So, like Topoly Hadamard, you can get like rational roots of unity. You can do totally crazy things. Like, if you just handed me any me any any like n by n matrix and filled it out with uh dyadic entries you know uh and made sure it was like say symmetric or normal or whatever and as long as this is um uh is such that it is a matrix that essentially represents a a thing with minimal polynomial of a degree row Minimal polynomial of a degree related to the size of this matrix, then that will inject some crazy algebraic number into your circuit. So it's like almost the amount of flexibility you have is very, very high. Now, we don't know the converse question of like, given a specific algebraic number, is that explicitly representable over your underlying gate set, like number ring, you know, especially these gate sets with like number rings? I think in general, that's. Number rings. I think in general, that's probably going to be a hard problem to satisfactorily answer. But we do have a no-go theorem that says just because you have a characteristic polynomial over a certain number ring doesn't mean that you can implement it. So I think the denominators that appear here have to be the denominators that you're going to get in this matrix have to at least go up to the degree. The degree essentially of the thing. Yeah. You know, I don't know if you still doesn't the equation or mapping equations very, doesn't that need five terms? I just, if I'm understanding what's correct. No, no, no, because I took each of these elements and I expressed them as a sum of powers of omega to the fifth, and I don't have any higher order, like I don't have omega to the fifth squared. Order, like I don't have omega to the fifth squared or omega to the fifth cubed or any of those terms appearing in this linear decomposition. So really I should think of this matrix as you know 1000 times omega to the fifth to the zero power plus zero zero zero one plus sorry, the number read you're going to be using is it isn't like uh you know you're not going to be having like omega like a finger. about the omega like the finger of unity squared in that number range uh no so so that that absolutely lives in that number and you could you could give me another uh unitary doesn't matter how big that has crazy linear combinations of these fifth roots of unity with with all sorts of other stuff and with this prescription i just take that matrix decompose it in this sum form with in terms of higher and higher powers replace it with these Cars, replace it with these tensor products with this lambda matrix, and then I'm going to get up something unitary that I can work with. It just via the power is zero. Yeah, so if that's zero times omega squared, one minus over here, yeah. Sorry? So we just mentioned that. Yep, sir. So you just mentioned that the cost of catalytic embedding comes from preparing this catalyst state. I wonder if you do something other way around. So we asked people working in hardware, what is the cost of doing different catalyst state? And then we, because we know how they're the item state of certain operators, and then we find out the circuit and implement these operators. And we see that if we input this magic, like this calculus state, then what gated can we get? Calvinist state and what gated can we get? So it's another way of exploring what are the convenient universal case set for thought tolerant quantum computing. Instead of saying we want to start with like a specific case set and we find the catalyst state, we set to the bottom-up approach. I think that's a phenomenal idea of figuring out what the hell can hardware actually do easily and then kind of reverse engineering what gate set I should be working with. So like just to follow up with Lee's remark about like Regina's remark about like SAR quantum. So, like when they're using the CCZ like state, like what I was told is that this is just something very inherent to the photonic system. But I'm wondering, like, for other systems or other things, is this always the case? Or different systems will have different convenient categories? I think every system is going to have its own states that those systems kind of natively prefer. Natively prefer that are just going to be easier for physics reasons to produce. Then, based on this logic, it means that different systems should have different, convenient, native, universal PSAC. Possibly, right? At the same time, a lot of these things embed really nicely into Toffley and Hadamart, which I think feels nice to work with because it's so easy and intuitive. So maybe it's just like, really, we should all just work with Toffley and Hamart. Like, really, we should all just work with Toffley and Hadamard. Like, throw Clifford and T out the window, like, Topley and Hadamard's the gate set. And then, uh, yeah, just catalytically embed everything there. There's probably good reasons to not do that, like, just from a, you know, yeah, I don't know. That's a can of worms. I know we're way over time, probably. I will be available to chat about this stuff. And I didn't get to a bunch of stuff I did want to talk about, but yeah. Yeah, close.