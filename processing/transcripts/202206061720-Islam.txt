Hi, so my name is Jesse. Today I'll be talking to you about case-based neural networks. This is a method that, well, more of a framework. So we're going to be talking about how exactly it works and what makes it different from normal survival analysis so that we can sort of appreciate the subtleties into why this makes it a very interesting method, at least to me, and hopefully to everyone by the end of it. So, first off, what is survival analysis? First off, what is survival analysis? The very basics of it. It's any data set concerning a time to an event of interest. This can be time to death, graduation, or even a disease. Maybe it's the time it takes for you to complete your assignment even. These data sets usually consist of individuals who are followed over a period of time. It's worth noting that these are usually a study where the study may be fixed, so it ends like in five years, or it could be open-ended as well. And it's worth noting that some individuals may not actually experience. That some individuals may not actually experience the event that you're interested in, right? Someone may not complete their assignment by the assignment completion period, or they may not get breast cancer by the length of which the study is following these individuals. Then someone may actually drop out earlier on because of some unknown reason. Maybe they moved and no longer a part of the study. So all these individuals are labeled as censored. It's essentially telling us that we have partial information for those individuals. And we'd like to For those individuals. And we'd like to incorporate them in our analyses, but we don't know when they will have the event, just that it doesn't, just that it has not happened yet. So first survival time. This is more of a graphical, just like a visual for it. Let's say survival time till assignment completion. That is what we're interested in for this little picture on the side here. So, individual A, for example, they started their assignment right away and then finished it a weekend. Right away and then finish it a weekend. So their survival time would be one week. Individual C started at week one, and then by the fourth week, they finished their assignment. So their survival time would be three weeks. Individual D would be starting their assignment on week one. And by the fifth week, they still haven't completed their assignment. So in other words, their survival time will be greater than four weeks. We have this partial information. And so when we have our data, or rather, the outcome that we're interested in in this case is actually going to be. In this case, it is actually going to be these two variables: the survival time of the individual and whether or not the event occurred, whether we have full information or partial information. And this is very important to differentiate between like a binary outcome where we're saying something happened and something did not happen. This is something happened and something is expected to happen later on. Okay, now another thing is these, what the output that we're interested in. There's two things we're interested in: these hazard functions. There's two things we're interested in: these hazard functions and also these absolute risk curves. So, the hazard function itself is a rate at which the event is expected to occur, given you have survived up to a certain period of time. So, for the hazard function, there's two main parts to it. Here, the H naught of T, which would be the baseline hazard. This is usually assumed to follow a specific distribution, say Gonfertz or exponential. And then we also have the exponentiate linear predictor. So, this baseline hazard, you can think of it as sort of this average. Sort of this average for the entire study population. And then the linear predictor would change your prediction given each person's covariate profile. Now, I'm a proponent for the hazard function, the full hazard function, but the survival analysis field has a preference for this hazard ratio. Now, there is a reason for this. Now, Cox regression is most likely a method that you've heard of and have come across. And in this method, it assumes proportional hazards, where the effects of covariance. Proportional hazards where the effects of covariates are not going to vary with time, whereas we would like to make that different or focus on those variables for our method. And how this works is essentially you take one of your hazard function divided by a hazard function with no covariates, essentially. You're setting the covariate values to zero, and then you end up with just a linear combination, linear predictor, sorry. And what this means is that your baseline hazard gets canceled out, and you no longer have to make any assumption about what the baseline distribution is. But this comes at a caveat. Distribution is. But this comes at a caveat. If we're interested in something like the three-year risk, yeah. So this would be three weeks from now, what is the risk of completing your assignment? Maybe the risk of getting breast cancer. That might be a better one here. What we do is then we look at this curve, we go, okay, up to three weeks, and then we'd see it's either 40% or 60%, right? It's what we actually get when we use a Cox model. When we use the Cox model, when we're doing any type of a hazard ratio, we have to use another method called, in this specific example, the Brezel estimator, which gives you a point estimate. Well, not really a point estimate, but the idea here is that you can only estimate at points that you've seen, and you can't actually get a value in between. So you could, of course, smooth this, but these are extra steps that we have to go through, which we could avoid if we just assume something for the baseline hazard. But in general, that is an assumption we have to make. I'd like to. Assumption we have to make. I'd like to not make that assumption, but still get that smooth cure. So this comes in now, the half of the method that we're going to be talking about comes in as case-based sampling. So this is going to be changing how we look at survival data. We're no longer going to have to look at that one or zero, meaning the event hasn't occurred yet. We will be switching to a binary outcome, which makes a lot of different methods easily applicable in a survival context. And we can still get the hazard function that we're interested in. Can still get the hazard function that we're interested in. It'll be the full hazard with the baseline, the baseline hazard part of it. So, for case-based sampling, how this works is that for each individual survival time, what we're going to be doing is discretizing into these moments that they are experiencing. We no longer care about who experienced what. We're just looking at the moments that they experience. So, this creates the base of our study, all the person moments that have been experienced. Then, what we're going to be doing is splitting them into two groups. Moments where an event occurs, this will create this. Moments where an event occurs, this will create this case series, and then we're going to be taking a sample of all the moments that have occurred within the study. This is going to create the base series. The reason we make a sample is because you can essentially have an infinite number of discrete moments, depending how far down you go. And the sample is going to be able to represent all the moments that have occurred. So, with that said, here's a bunch of equations. But basically, what we're doing here is that we Well, we can get the exponent basically, this is the equation for the odds, and then we can make some replacements. And I won't go too much into the algebra here, but what we end up getting is that when we formulate our survival question in terms of the odds using case-based sampling, we can now get the log of the hazard equal to our linear combination plus this bias term. This bias term is introduced because of our case-based sampling approach. So, when we do that ratio, when we're getting We do that ratio when we're getting the blue points rather than having all of them, this will cause a slight underestimation of what the actual hazard is. And all we have to do is adjust for it when we're fitting our model with an offset term. So one of the benefits of this is that now to have a flexible baseline hazard, different ways that we incorporate time are going to be able to represent different distributions, right? We can have splines of time to have a flexible model if you're familiar with the. A flexible model if you're familiar with the Royston-Palmer model. We can do no time at all, and then we're simulating an exponential, not simulating, but approximating an exponential. Yeah, so the how do I set? Yeah, so the big B is the way that we do it is essentially just a sample, like you can take a random sample of any number. So you can decide conceptually if it's years or minutes or. Uh, years or minutes or seconds, but that really is just a random sample of a number that you decide. Uh, the small b, we usually make that to be one to a hundred ratio with the k series. So, what we ended up finding out is that when you have that one to a hundred ratio, this makes it so that you're only introducing one percent variance that wasn't originally in the model. You could go even further, so it's even smaller, but that one percent tends to be fine. So, that's what we end up recommending. Yeah. So, yeah, that means because these are values that we essentially chose, this means it's fixed. And that's something that we can adjust for without having to worry too much when we do our model thinning process. So when we do case-based sampling with logistic regression, we now have this flexibility in terms of depending on how we incorporate time in the model, we can actually approximate different baseline hazard distributions. So here I just did splines of time. Here, I just did splines of time to represent this model. And what we get is a smooth curve. And when we were, if we were to interpret our three-week risk now, well, now we have an exact measurement. It's 50%. Maybe I wanted to know my 3.3 week risk, and now it's like, I don't know, 53%. Yeah. So this is one of the benefits of going with this approach. And of course, this benefit is to any model that assumes a baseline hazard distribution. But now that we have this flexibility, what if we were to do something else instead, right? What if we are were to do something else instead right what if we were to use different types of modeling neural networks of course is where i'm going with this uh so case-based sampling permits the flexibility for this baseline hazard and then what about the flexibility in the covariates so how many contribute are there interaction terms is there non-linearity ideally what we're aiming for is that the model would learn all these things from the data itself that we wouldn't have to specify it and that's where we're going to be combining this case-based framework with neural networks so uh now the So, now these are three methods that we'll be comparing to. So, of course, neural network methods do exist for survival analysis, but each of them have these small caveats to them that we want to address. So, deep serve was originally the, well, there are different implementations, but deep serve is the most popular one for how you can use a Cox neural network. They essentially made a loss function, the loss function for Cox regression, just put it into their neural network, and then they are essentially able to make a proportional hazards model. So, here are all the downsides to COX. So, here all the downsides to Cox regression, or we have to use a Brezel estimator, and so on are still present. Deep hit is a first-hitting time neural network, but essentially it assumes an inverse Gaussian distribution for its baseline hazard. And here, it's not really an issue, but the downside is that the baseline hazard is still assumed to be a specific thing. We want to make it data-driven. Then we have deep survival machines. Now, this one is interesting. It allows you, it uses a mixture model for the basic. You it uses a mixture model for the baseline hazard, and what this means is that you essentially just select a set of base distributions and then it creates a mixture model of those to represent your baseline hazard. The only downside to this method is it does not permit time-varying interactions. So this is where our method comes in. Providing a solution to this need for a parametric method that permits non-proportional hazards and a flexible and baseline hazard. So, yeah, that's what we're proposing to do. And so the steps are relevant. And so the steps are relatively simple. So, first, we're going to be doing our case-based sampling. What this creates is essentially we get a longer data set because of the sampling process for creating the base series and case series. And then we also have this offset term that I mentioned before. This is essentially a value that we know of because we have explicitly decided to case-based sample. Then, what we do is we just pass this data through a specific model structure. Now, there's only a few things that are required to make it work. To make it work for survival analysis, where all your covariance, well, in this case, features, plus time, are going to enter any feed-forward neural networks. So it's up to you what type of neural network structure you want. In our analyses, we only did the just basic deep artificial neural network. Then the output of that model will be added to our offset term and passed through a sigmoid, which will give us a probability. Next, what we'll do is pass that probability and take that probability and convert it to a hazard. Convert take that probability and convert it to a hazard. So, how do we get to that? To do that, it turns out the algebra works out very nicely for us, though it is going to be a lot of equations for a second. So, the log of the hazard is equal to log of the event occurring over the event not occurring, plus this offset plus the bias that is introduced from case-based sampling. So, all we did for our model is the sigmoid in the sigmoid activation function, we have the feed-forward neural network. Have the feed forward neural network, any neural network that can be defined as a feedforward neural network, plus this offset term. And then what the reason we add this offset term is that it's going to cancel out the bias later on. So when we expand, simplify, and get to the end, what we end up seeing is a log of our hazard will end up equaling to the output of our feed-forward neural network, which is great, because now all we have to do is fit our model and be able to get the hazard, even though we are fitting a binary outcome now. Are fitting a binary outcome now, yeah. So, here, um, I guess I'll go through this kind of quickly. The uh, we decided to use the IPA score, but this is based on another metric known as the right-censored briar score. Uh, ignoring the equations on the slide, there's essentially three categories of individuals. So let's say we wanted to know the three-year risk for an individual. Uh, we have information that you have individuals who have experienced the event already, who at the three at the point of three years, let's say someone that's uh At the point of three years, let's say someone that's experienced event at year two. Then we have someone that might be censored at year two. And then we have people that are going to experience the event or be censored later on. So for those individuals who have already been censored, their information is partial. So the goal of this red sensor brief score is to incorporate that partial information into our score, which is what the inverse probability sensory ring weighting at the bottom here is doing. At the bottom, here is doing. And then on the left-hand side, we have the event occurring, and then the right-hand side, we have the event not occurring. Well, so essentially, the censored individuals that are happening past the survival time of interest. Anyway, so with that said, the interpretation of this metric for index of prediction accuracy ends up being a lot easier. So here it's essentially the prediction of our model of interest for the Breyer score divided by a null model. Here, it's usually used as. Model. Here it's usually used as the Kaplan-Meyer score, the Kaplan-Meyer curve. And how we interpret this is that if the model is bigger than zero, it performs better than null. If it's less than zero, it performs worse than null. So here I just list out the hyperparameters. So we decided to have this fixed for all the studies. This just made it easier to compare somewhat fairly between each of the methods that we're comparing to. Yeah, this listed here, but they don't. Yeah, this listed here, but they don't necessarily matter too much. We can go into details if you have questions about them. So, yeah, so we have two simulation studies. Time-wise, I think I'm okay. Okay, yeah, so I'll just keep going. So, simulation studies. For the simulated covariates, they're going to be the same for our two simulation studies here. There's just three of them, and they're super simple. It's just, what is it, Bernoulli for a Z1, then a couple of normal distributions with very little variance. Little variance split between the Bernoulli variable that we have. And the goal here is really to have a very clean data set so that we see in ideal conditions how all the models are going to perform. So in the simple simulation, it's just an exponential model that we're simulating with very small effects. And what we end up seeing is that in this simple simulation, what you should, and given that we have a very large network, you should be using. Work. You should be using, what's the word? Regression models for the simple simulation. It turns out that a model, the fixed parameters that we decided on, ends up causing a bit of overfitting. Or at least that's the explanation that we went with. But it turns out that CBNN, so our model, actually doesn't perform too badly, whereas DSM and DeepServe end up having a major drop in performance, while Deep Hit performs relatively similar to CBNN model. CBNN, a model of interest. So then now the very important simulation that we have. So that was just a baseline to see, okay, what's happening with our models. And from there, we get to do our complex simulation. Here, what we have to simplify things is a what's the word? We have, so for the baseline hazard, what we did is we simulated off of this breast cancer data set from the FlexServe package. This breast cancer data set was originally used to demonstrate the benefit of splash. The benefit of SPLINE's models for the baseline hazard. So, the Royston-Palmar, the original Royston-Palmar study. Using this data set, without covariance, we essentially fit a model for the average of individual, and then we use those basis lines to create our baseline hazard in this simulation. Then we also have a bunch of interaction terms. So, the time-varying interaction is the one that we're most interested in, plus, we also have two normal. In plus, we also have two normal interactions. For this time-varying interaction, we made a very large effect. The reason was that it's to demonstrate the benefit of our method: that if we have a very clean data set with a very strong effect, we expect that if you can model a time-varying interaction, you will have improved performance. And it turns out that the other models don't have that. So, here, the optimal model is we essentially fed a regression model using K-Space with logistic regression. Uh, case-based with uh logistic regression, but with the exact like splines and the interaction terms inside of it, so that represents sort of like the best we can do in this specific scenario. That's what this black line is. Whereas then it's followed by case-based neural networks, uh, followed by the regression models, and then the neural network models. So it turns out that in this case, we see that CVNN outperforms everyone else by, I'd argue, a drastic amount. So then we had two real data studies. Now, the data sets Real data studies. Now, the data sets that we decided to use were the ones that were used in all the other neural network model papers. So we have the support study, which is the study to understand prognoses, references, and outcomes and risk treatments TIP. And this is essentially following 8,873 hospitalized adults followed for 5.56 years. 68% incident since they are all very much at-risk individuals with 14 covariates. It's also worth noting that this. Variants. It's also worth noting that this requires imputation, but we use the exact same data set that they use in the deep surf paper so that our comparison could be somewhat more fair. Yeah, and then on the right-hand side, this is called the population time plot. It just shows you the sort of the incidence density over time. So it's very, most of the patients are dying at the beginning, and then it tapers up. So in this study, what we ended up seeing is that performance is actually very similar across the board. And again, note that we didn't try to tune the parameters for our hyper for any of the neural For our hyper network models, beyond for the shared hyperparameters. If there are unique ones, for example, like DPID has an alpha parameter, we might have tried a few things to make sure that we're giving it a fair shot. But yeah, what we ended up seeing here is that actually all the models perform very similar to each other. I'd argue that the regression models, so this brown and yellow line for Cox and case-based logistic regression, perform better, followed by CBNN and then the neural network competitors. Neural network competitors. Then we have the Metabrick study. So, this is the Molecular Taxonomy of Breast Cancer International Consortium data set where we follow about 2,000 individuals or about 60% of them died due to breast cancer, 30 years of follow-up, nine covariates. And again, we use a pre-processed data set from DeepServe. So in this study, what we ended up seeing is something interesting. So this is potentially an issue with the hyperparameters that we fixed. However, what we ended up seeing was that the regression However, what we ended up seeing was that the regression models perform better than null, while all the neural network models ended up performing slightly worse. The CBNN ends up being relatively close to DPIT, while then we see a decent drop in performance for deep serve and then DSM, the deep survival machines. So with all that said, what we end up seeing is that if time-bearing interactions and a flexible baseline hazard without user specification are of interest, CBNN should be strongly considered. It provides a parametric. Considered. It provides a parametric, flexible baseline hazard. What I mean by this is that we have a smooth estimate now. So if you want a specific 3.25-year risk, you can estimate that exactly. You don't have to smooth it. Then we have, it permits time-varying effects of covariates, and it may be applicable to high-dimensional data sets, which is where we're going to next. I'm currently making the, oh, there's an R package for it, but I'm making it more user-friendly, but it'll be posted. Friendly, but it'll be posted at that GitHub link. And then, if you wanted to see any of the code for the analyses or the paper itself, it's in the CBNN manuscript. Yeah.