A very different scale on cognitive decision making. And it's been very illuminating being at this workshop because I normally don't think about the direct implications of information theory when it comes to my models. I think about optimizing with respect to different objective functions and then seeing what those models do with information. So, if I squid it hard enough, I've been able to recognize a lot of interesting things and what people have been talking about, and it's given me a lot of cool ideas going forward. Ideas going forward. So, hopefully, we can meet somewhere in the middle, and maybe we can learn a bit from each other today. So, if I can. All right, we'll be clicking. So, what are the sorts of decisions I'm talking about? So, there's kind of broadly speaking two different types. There's these perceptual-based decisions. That's kind of you stare at maybe a dot moving on a screen and you're trying to tell if they're going left or right. What I'm going to be focusing on. Right. What I'm going to be focusing mainly on today are value-based decisions. So these are things that we as humans encounter very regularly throughout our lives: where you're going to go on your commute, how you're going to vote, if we're in Banff, where you're going to go for your hike tomorrow. And all of these decision models have kind of two different components to them. So the first is the information gathering stage. You observe the environment that you're in, and you try and gather information. You try and gather information to update your belief as to what the state of that environment is. And that is all very well understood, but the second component of these models is a bit less well understood. And that is converting that accumulated information into some sort of rule that decides, am I going to go to Sulphur Mountain or Rundle? Or should I wait and gather more evidence so I can increase my overall accuracy? So the So, the really key nuance of thrown here is that in realistic environments, you can have possibly very rapid fluctuations. So, for example, if the weather forecast is changing and you can see that cloud fronts are moving in, you need to take those fluctuations into account when coming up with this rule for commitment. And so, what a lot of people normally do, because coming up with optimal models for those things is difficult, is you use some sort of heuristic or phenomenological Sort of heuristic or phenomenological basis to try and model what people are doing when you actually look at cognitive experiments. But a really key thing is that a lot of these models rely on strategies that are fixed. Before you see any evidence, you decide what your rule is going to be, and then you go into the experiment and you apply that rule. And that doesn't, to us, make too much sense. You should be able to come up with a rule that is flexible and adaptive based on what the environment is giving you in real time. You in real time. And so, what we're basically going to try and do is account for those different environmental fluctuations by extending normative theory or Bayes optimal theory to dynamic environments. So that's a typo. Basically, there's going to be two different parts of this talk. The first is kind of more completed work that's currently up on bio archive, which is looking at normative behavior for decisions that have some within. Decisions that have some within trial fluctuations to their environment. So, I'm going to focus for purposes of this talk on maybe that the payoff for these decisions is going to be fluctuating, but we also did some more work on the quality of that evidence changing as well. Then I'm going to talk about some test work or preliminary work, which is extending that a bit further to not focus on fluctuations in the environment within the course of a single decision, but over the course of a sequence of decisions. But over the course of a sequence of decisions, and that there may be some correlation structures to the environment. And we're going to also try and add in some naturalistic constraints and kind of leverage two different sources of information when we're developing this model. And then talk about where we're planning on going next. So just to give a little bit of background, because again, this is pretty different from what other people have been talking about. The way I'm thinking about decision-making processes is as basically a Processes is as basically a Bayesian inference process. So, if I want to have an optimal model, my goal is essentially: I'm getting information from one of two Gaussian generating functions. Let's say I have a state minus and I have a state plus. I don't know what state is generating the evidence. And so, my goal is from getting samples from one of these two distributions to infer what distribution I'm looking at. So, this is all pretty standard. The way to do this sequentially goes back. The way to do this sequentially goes back to Wald from the 40s. And the way to incorporate your evidence into updating your belief about what distribution you're drawing from in order to maximize the information of your sample is to track the sequential log likelihood ratio that I'm going to call y. And this is all kind of in discrete time evidence. You can extend this to continuous time evidence by taking a continuum limit of this using functional. limit of this using functional central limit theorem and what pops out of this is a stochastic differential equation where essentially what this has two parts you have some sort of drift where the sign of this drift is conditioned on what source you're drawing from so it'll be positive if you're drawing from the plus distribution negative if you're drawing from the minus distribution and then you have some sort of white noise that is the observation noise essentially and so what I've shown here at the bottom is just some sample trajectories Bottom is just some sample trajectories of this process. This parameter m is essentially looking at the quality of the evidence. If you back it out from these two Gaussian functions, it's a scaled signal-to-noise ratio. So essentially, the larger this M, the larger the drift, which means your decision is easier in a sense. These two Gaussians are well separated. If M is small, these two Gaussians overlap quite a bit, so it's hard to distinguish which you're drawing from. Distinguish which you're drawing from. Like I said, this part is fairly straightforward just by turning the Bayesian crank to get out the model. The more complicated portion is to create this commitment rule, and this is really the part that we focused on. And so what you need to start with is figuring out what your objective function is. What are you trying to maximize using your rule? And so we're going to assume that as kind of a, I guess, as a human or as some other As a human or as some other organism, that you're going to try and maximize something called trial average reward rate, which is this row. You can think about this as it's essentially the average reward you get for a decision. So that has to deal with both accuracy, which is this average R, and a cost and any cost that's actually incurred by gathering evidence. And then it's that per unit time, how long do you spend making units? Unit time, how long do you spend making the decision? So, I think a couple of days ago, somebody mentioned in a talk that there's this speed and accuracy trade-off associated with their process. And we see a very similar thing here. Essentially, you can come up with a rule that is very accurate, but then you're going to spend a long time making each decision. Or you can come up with a rule that makes very rapid decisions, but each one is going to have a lot of uncertainty associated with it. So, given the subjective function, So, given this objective function, what we want to do is come up with thresholds on our stochastic process that represent this rule. And so, then, whenever our process hits one of these thresholds, that triggers commitment to an associated choice. So, here, what we're looking at is an example where you hit this positive threshold. You would say, okay, given that point, I'm going to guess that I am drawing from the plus distribution, or I'm going to commit to the choice that is associated with whatever that evidence is coming from. It's coming from. And so, what I've shown here are just some simple constant thresholds because that's what people used for quite a long time, and it's relatively simple to back out all the statistics of that. But that doesn't assume anything about optimality here. So what we want to do is actually use a subjective function and compute without any assumptions what the optimal form of these decision thresholds are and see if they widen, that that means we're favoring accuracy. If they narrow, that means we're favoring scope. If they narrow, that means we're favoring speed in that rule. And do we get any sort of interesting behaviors and motifs that come out of those versions? And so, to do this, we're going to do something called dynamic programming, which because it's a bit more of a mathematically inclined audience, I will go ahead and go ahead on the previous slide. What are the two t terms in your denominator? Okay, so this t, this first t, t sub t, is the total length of a decision, and this t sub i is what's called. And this T sub I is what's called an inner trial interval. So basically, in between decisions you make, there may be some delay period. We are not allowed to accumulate evidence. And so that's what both of those represent. Yes? Is it obvious that the movement for when to commit is kind of like a constant special? It is. This is just something that people assume. And so in general, it is, as I will show you, it is very much a non-concept of this. Let me follow up uh Mike's question. So can you change the two T's so that this can take into account a demand reward or something? Yes, yes. And so that that is exactly what this kind of T sub I is representing, is the kind of delay. The idea being that there's then a period of time in which you're not allowed to gather reward because there's a delay in between your decisions that you're making. So, how we do this is it's most We do this is it's most natural to think about in discrete time. So let's say at some time n, you obtain a piece of evidence cn and you update your state likelihood from pn minus 1 to pn, where this state likelihood is just the conditional probability of being in the plus state given all your previous evidence. At that point, you, as the observer, have two different actions you can take, broadly speaking. You can say, am I Speaking, you can say, Am I sufficiently confident where I'm going to commit to a choice, or am I not sufficiently confident and I want more evidence? And so, we can take kind of a more economic utility theory point of view to actually assign value or utility to each of these actions. So, the value for choosing the plus state and committing to that choice is equal to the average reward of that choice, which is the reward you get from being correct times the probability you're correct plus the reward. You're correct plus the reward you would get for being incorrect times the probability you're incorrect. And similarly, the value for choosing S minus is just is given by permutation. The value associated with waiting is a little bit more complicated. So essentially, I would want to wait if I expect the utility of my belief to increase by weighting. So the value associated with waiting should be the average value I get by drawing a new piece of evidence. By drawing a new piece of evidence, less any sort of cost I incur by accumulating that evidence. That cost can be explicit, it could be essentially having to pay for a piece of information. It could be more subjective, it could be essentially the opportunity cost of spending too long making this decision when you could be going and doing other things. It could also be a metabolic cost associated with doing these computations cognitively and storing everything in your short-term working memory. And so, given these three different utilities, you should pick whichever action has maximal utility. That gives you this equation here, which is called Beldman's equation. You may note that all three of these utilities depend on your current state likelihood, so it depends on your belief. So, as you move between different belief values, what the optimal action is, changes. And so we'll divide the space of belief. So we'll divide the space of belief into three different regions. One where you would always choose plus, one where you would always wait, and one where you would always choose S minus, kind of as I've demonstrated here in the Soul schematic. And so the boundaries between those three regions are exactly these thresholds. And so those are what we're really interested in at the end of the day. So that's how we find them. And what we're going to do is basically see in fluctuating environments, are there any In fluctuating environments, are there any interesting patterns that we see in the dynamics of these thresholds? Do we see observers actually reacting to the information that they're given? So, the way that right now I'm going to be introducing environmental fluctuations is by varying the reward over a single decision. So, maybe if we want to think about this as a behavioral experiment where you're getting some sort of payout, that payout could be changing over the course of your deliberative process. And to And to keep things as simple as possible, we're actually going to make this change in payout known to the observer in advance. They can fully anticipate that if I'm correct, I'm going to get $4 and then $10, then maybe $8, for example. Give them as much information as possible to see how much are they able to leverage that information. Do we really see the model taking advantage of that? Yes? So in that first term, we're very inward evidence. Varying lowered evidence quality. Yes. That would be a place where you could introduce misinformation. Yes. So, well, what you could do is essentially if you wanted to have maybe a high evidence quality regime and then it switches to something very low evidence quality. And we can extend, although I'm not going to show it here, we extend this to environments where the observer doesn't know the dynamics of advance. So maybe they have to infer that, oh, I need to notice. Infer that, oh, I need to notice that the quality of my information has really gone down. And so that is something that we can account for. So if we add, just to show some sample reward time series like this, and we go through and we do our dynamic programming and we look at the threshold model we get, we get something that is very much non-constant, where the thresholds aren't plotted, in fact they're infinite, which means that an ideal observer would never respond in that period. Never respond in that period. But what I really want to highlight here, and what we noticed was quite interesting, is that, so here in these kind of blue box regions, you have these similar patterns of reward change. You have some higher reward, and then the reward decreases by some amount. And if you look at the threshold dynamics, you actually get similar behaviors that are kind of independent of time here, that you have thresholds that slowly collapse in and then expand outwards here because we're getting towards the end of the trial. Here, because we're getting towards the end of the trial, they still have this slow collapse, although at the end it's a bit different. But what this made us realize is that there's probably a set of different motifs, if you will, of these threshold dynamics that really depend on the change in reward pre-change and post-change. And so rather than, or to make this more clear, rather than look at this kind of complicated time series of reward, if we focus on just single If we focus on just single reward changes, then we can expand out and see all the different motifs that these models can exhibit. And so that's what I'm doing here. So at the top, I'm plotting on the x-axis is the value of reward before a change, and on the y-axis is the value of reward after a change. And as we move through this landscape, we see some quite different threshold dynamics. Different threshold dynamics. So, for example, to explain, we have at the top is actually plotting these thresholds over time. For all of these environments, the switch happens at 0.5 here. And on the bottom, what I'm plotting is what's the response time distribution. So as a continuous distribution, the probability that an observer responds at any point in time. And so what we can see is that, just to kind of walk through some of these, these purple threshold here. A purple threshold here initially is very is infinite until the change and then decreases. That corresponds to initially having very low reward and then reward increasing. So that's something that heuristically we can understand. Why would I decide when I'm only going to get a dollar if I can wait a few seconds and get $10? I wouldn't want to respond when I'm getting paid out low. And that's also what we see in the response time distributions. On the other extreme, On the other extreme, so this is when you have high reward and then low reward, these thresholds collapse right before the reward decreases, which means you would never respond when the reward is low. You always want to respond when you're going to be getting a higher payout. And then as you kind of titrate between these two extremes, you can see some intermediate behavior. This case is the one that I find most interesting: that if you have only a slight increase in your reward, that In your reward, that there is some benefit to responding early because you get to then go on to make another decision, get another shot at making more reward. So, if you see very informative evidence up front, you should respond. However, at some point, the allure of only gaining a little bit of benefit in your time versus gaining benefit and reward trades off, and then you have this kind of peeling back of thresholds. And again, you have this period where you don't want to respond and you would. Period where you don't want to respond and you would rather wait until the change occurs. Yes? So those kind of cases that we actually don't know how much before you try to get? So we have not looked at an actual uncertainty over the value. Well, I guess, so what we've done that's most closely related to that is the observer knows maybe I will get one of these two values, but I don't know which. Which. And in that, maybe they have to infer what reward they're going to get from some other evidence. And in that case, you see actually behavior that is much less interesting, where essentially you track some constant threshold for high reward, some constant threshold for low reward, and as you notice that the environment has shifted, you kind of noisily piecewise track those two thresholds. So that's what's kind of interesting is that in that sense, Is that in that sense, you know, less information means monotonic changes in the environment produce monotonic responses. Whereas here, when you have more information, that same monotonic environmental shift can produce highly non-monotonic responses of the model, which is, for us, quite interesting. And so these, at this point, we may be thinking, okay, it's kind of a clever parlor trick that looks kind of interesting from a mathematical point of view. Go from a mathematical point of view, but do these actually give any sort of advantage over using a much simpler heuristic model? So, the way I want to convince you of that now is by looking at the robustness of these thresholds with respect to noisy perturbations. So, we're going to do that. We're doing a little bit of model comparison. We're choosing two different models that we're going to present here. This kind of constant threshold, again, the kind of simplest heuristic that people have been using for a long time before this, and then this other. This and then this other model called an urgency gating model, which essentially has monotonically collapsed thresholds. That, for historical reasons, is one we are very keen on comparing our results to. And so, what we want to do is then add in noise to these models in a way that would be plausible to see how subjects actually exhibit noise. So, the way we'll do that is in two components. The first is that we're going to actually add on some external noise to the below. Some external noise to the evidence accumulation process. This could represent basically that nobody is going to be perfectly doing this log likelihood calculation in their head. There's going to be some sort of noise in that computation. And then we're also going to add on noise to the actual time that they respond. In a laboratory setting, this could be that you've formed this decision in your mind that I'm going to respond, but there's actually some motor processing delay with committing to the response by clicking a button or a bio. Response by clicking a button or by, I don't know, vocally describing your choice. You also could have early responses because you just so now and so you lapse and you respond earlier than you should. There are definitely more sophisticated ways of implementing this noise, but kind of as a first pass, this is what we've looked at. And so what we see, to walk you through this figure, this top panel here is actually looking at the performance of these three models in different rewards. In different reward regimes. So, flipping back to here, what we've done is we've restricted all of our reward schemes to lying along this anti-diagonal. That way we know we're cutting through all the possible behaviors of our model. So if we restrict ourselves to lying along that diagonal, it makes sense that the optimal model always does the best. But interestingly, we kind of get this trade-off in Kind of get this trade-off in which of the heuristic models does best relative. For low to high reward switches, we have that there's quite a good separation between the performance of all three models, but for high to low switches, they're all fairly comparable. And the way we can explain this is actually basically the power of having these discontinuous thresholds. So, here, what we've done is in the Norgan model, Done is in the Norman model, because this threshold is initially infinite for a low-to-high reward switch, the observer is not going to respond unless they're perturbed that way by this kind of Gaussian response time filtering. And so what that does is symmetrically kind of skews the responses in both directions. Whereas for the constant thresholds and for the Ernstigating model, which both have finite thresholds at all points in time, they're also affected by this belief noise, which is going to induce Belief noise, which is going to, in general, cause responses to happen much earlier, which is in the regime where there's very low report. And so, what you see is that as we increase noise going from darker colors to lighter colors here, both of these two heuristic models suffer from responses that are pushed very far early when reward is low and then get smoothed out, whereas normative model is essentially immune to this other type of noise. And so people really. Noise. And so people really haven't been looking at these thresholds that can have these kind of discontinuous dynamics. And so this is really leading us to want to investigate this more, particularly running some behavioral experiments to see are there regimes where people just don't respond as often because of the change in the reward dynamics. So there's a lot more to that story that I don't really want to go into right now for the sake of time, but I'd like to show just a little bit of preliminary. Like to show just a little bit of preliminary work on kind of extending this to fluctuations not within a single decision but across decisions. And so the way we're thinking about this is kind of we want to understand some interplay between these environmental structures and in particular naturalistic constraints. So what we're thinking of is maybe a foraging animal. There may be some correlation if they're going out into their environment and trying to look for food. We're trying to look for food, sites that yielded a lot of resources yesterday will probably also yield high resources today. And so we imagine that they would take advantage of that correlation structure. However, there is probably some time frame after which those resources, let's say these are bees looking for nectar in a flower field, those flowers only have a certain blooming season. At some point they are going to fade away and the bees have to go look for another source of For another source of resources. And so, if we impose kind of these naturalistic constraints of maybe a finite energy or finite action or budget, do we see that there's kind of a trade-off in leveraging information that you have previously gained about the environment versus information that you're actively gaining from the environment. And so, the way that we're going to do this, I've kind of schematized up here where the environment. Here, where the environment, so kind of this correct choice, the generating distribution of this evidence is changing each decision we make. And that has a probability of switching with some probability epsilon. So here, at first, the blue choice was correct, then we flip a coin, and in this case, the correct choice stayed blue. But then on this next trial, maybe, okay, the choice switched. And so you would have to notice that as an observer now. Now. And then, in order to implement the sort of finite energy constraint, we want to think about the observer only being able to make a total number of actions. So they can, as these bees, you can spend the day either surveying the environment or you can spend the day actually going out and gathering resources. And so what's kind of interesting is that what we're seeing is as we change this parameter epsilon, so kind of change the coupling between decisions. The coupling between decisions. We get this very interesting sort of rippling that occurs in thresholds that seems to exhibit some sort of interesting time scale dynamics, even though this environment doesn't really naturally have such dynamics. And so we want to try and understand more what's going on there. But one thing that we can say at this point is: so we can also plot the probability of responding at any point in time. And what we basically Time. And what we basically have been noticing is that in a strongly coupled environment, what really matters is the first decision. You accumulate evidence, you gain confidence until you make that first decision, because then it's very likely the environment isn't going to change. So you can ride that evidence forward and just commit to subsequent decisions and gather a lot of resources without having to worry about switches. Whereas in a weekly coupled environment, you'll often make You'll often make some observations, you'll make a decision. Now there's a high probability that the environment has changed, so you'll need to go back to making more observations again. And there's this kind of back and forth pattern that happens. And so kind of the first thing that we want to then look at is sort of a time series of this. That what I'm showing here on the left is essentially a spike plot of decision times with up. With up, with excuse me, with spikes on the top mean that you committed towards this plus alternative, spikes on the bottom mean you committed towards this minus alternative. And what this reminded us was, thinking more in terms of neuroscience perspective, is these spike trains that you get where you have sort of rapid bursts that you can characterize with some sort of inner spike interval, and then quiescence in between where you don't have any fire in the finger on. And so, we wanted to see: can we kind of get those two? We kind of get those two different lengths to change off? Can we get those two time scales to play off of one another by changing the coupling of the environment? And it seems like that's what I'm plotting here. So blue is essentially the length of respond, respond, respond chains, and orange is the length of this average quiescent space where you take many actions to go ahead and sample the environment. And it seems that we can get some sort of trade-off between these two. These two kind of quiescent and bursting phases of activity by changing the coupling strength of the environment, which is an interesting kind of first finding. And that's about as far as we've gotten with this because I started working on this project a couple weeks ago. So there's a lot of interesting directions we can go. If we're thinking about sort of foraging tasks, it would make sense that you should be getting some sort of feedback as to whether your decision was correct because you would actually go and gather some resources. Because you would actually go and gather some resources and you'd have that in your hand. There's a lot of interesting time series analysis we've talked about with these. And again, going back and looking at heuristic models and seeing can we approximate some of these strategies with maybe better informed heuristics. So I hope that kind of what the main things I wanted to talk about here were that you can go through and do this normative analysis and you get very interesting dynamics of models that actually turn out Mix of models that actually turn out to have kind of nice intuitive explanations and endow some nice performance advantages compared to heuristics. And I think adding in these constraints is kind of an interesting way of going forward to, you know, not compare optimal models to heuristics that we expect to fail, but compare constrained optimal to heuristics and maybe give a more reasonable explanation of coming up with even better heuristic models. And there's a lot of work to be done both on the theory. Of work to be done both on the theory and on the experimental side. So I'll thank my advisors and thank the organizers, and I guess I'll take questions with the time I have left. Questions?