This work that I'm talking about today is joint work with a former postdoc, Daniel Arthock. And we thought this result was fairly surprising. And I suppose with a lot of results that are surprising, when you think about why they do occur, they're not really that surprising. It's not really that paradoxical. But anyway, let me move on. Anyway, let me move on. So, well, the focus of this talk is on a recent result with Daniel Alfock that a classifier based on a partially classified sample, or I'm using the word classified the same to mean it's labeled. So I'm using that interchangeably here. So a classifier based on a partially classified label sample can actually have a smaller area. Can actually have a smaller error rate than if the sample were completely classified. Well, obviously, this is not going to happen all the time, so it needs some assumptions in order for it to hold. And firstly, we adopt a generative model approach where the joint distribution of the observations and their labels is assumed known up to some vector of unknown parameters. Of unknown parameters. So we're adopting, in a statistical framework, we're adopting a fully parametric model. I suppose people in computer science call this a generative model. And secondly, we're postulating that the probability that the feature data have missing labels depends solely on their entropy. And this second assumption, it applies to situations where. It applies to situations where the unlabeled entities, the unclassified entities, tend to fall in regions of the feature space in which classification is difficult to perform, such as regions near the class boundary which have high entropy. My screen seems to block up. Locked up. Okay, well, a fundamental issue in semi-supervised learning is identifying the mechanism responsible for the unlabeled feature data. Indeed, one could say that semi-supervised learning is not really intended to be used without any prior assumption about the distribution of the missing labels. Of the missing labels. And there's also the question of whether the use of unlabeled data can hurt the learner. Now, there are many different ways of approaching this semi-supervised learning. And as I say, we're adopting a generative approach where we're working with the joint distribution of the feature vector and its label. Now, for generative classifiers, construct Of classifiers constructed by modeling the joint distribution of the features in the labels, we can, it's possible to have a go at working out the Fisher information in the unclassified observations. And so the benefits of using unlabeled data can be assessed on a theoretical basis. So to get some notation out of the way, this is a classification problem. So we're considering the formation of a classifier or decision rule. Classifier or decision rule, which I'm calling R of Y. Y is the feature vector associated with the entity to be classified to one of G predefined classes. So these classes are known to exist and we have say little G of them. And we make this decision to assign an unclassified entity on the basis of this feature vector y. vector y and so if r y is i then we assign the entity to class i now so far as the so-called learning of the classifier we've got these um we've got observed feature vectors so we've got n independent feature vectors y sub one to right y sub n and we've got these class labels well these class last class labels may be available but Class labels may be available, but maybe not all of them are available. So Z sub 1 is the class label of Y sub 1 and so on, and Z sub n is the class label of Y sub n. And we could just take each of these to be a scalar, taking the values 1, 2, 3, depending on whether they belong to the first, second, third class, etc. But in here, I'm taking this class label to be a G-dimensional. Able to be a G-dimensional vector. And it's got zeros everywhere, except in the position where it's got a one in the ith position if the feature vector comes from the ith class. Now, you will see that we're going to introduce these so-called missing label indicators, M sub 1 to M sub N. So M sub 1 is 1 if this label is not available, it's missing, and it's 0 if it's disavowable. And it's zero if it is available, and so on. So M sub n is zero if we know Z n it's available, but if it's missing, then M sub n is one. Now with supervised learning, as we all know, we've got a completely classified sample. So all the MJs are zero. And with unsupervised learning clustering in a statistical context, all the, we don't have labels. Don't have labels, so all the MJs are one. And in the present context of semi-supervised learning, MJ is one for some of the J's, or some of the entities. So we're now going to take the conditional density of our feature vector to given that it comes from the ith class, that is, its label has got a one in the i position, then it's In the i position, then it's denoted by f sub i. So we know this density, and it's known up to a vector omega sub i of unknown parameters. And little pi sub i denotes the prior probability that the entity comes from the ith class. That is, the z sub i is one. So the vector of unknown parameters theta consists of the mixing proportions. There's g. Mixing proportions, there's g of them, but because they add up to one, we've only got g minus one, and then we've got the parameter vectors associated with each of the g classes. And the unconditional density of y is just this g component mixture density. And so the optimal, or the Bayes rule, assigns an entity with feature vector y to the kth class if k is the k is the is k is our mag r max i of this quantity here. This tau sub i y theta is the posterior probability that the entity comes from the ith class. So before we observe y, we've got a prior probability pi sub i, and then having observed y on the entity, then we've got this posterior probability by using Bayes' theorem. And we have sine And we assign the entity to the class to which this posterior probability is greatest. Now, to simplify the analytical calculations, we're taking y to have a multivariate normal distribution with a different mean in each of the classes. We're only taking two classes, and they've got a common covariance matrix. So, this is the homoscedastic model. The assumption. Scodastic model. The assumption of, of course, as you would know, of a common covariance matrix makes the classifier linear and drastically reduces the computation. So in this case of a common covariance matrix, the classifier, which we'll take to be the logit of the posterior probability of tau sub one, we've only got two classes, is this linear function of y. So we've got beta naught, the cutoff point. Got beta naught, the cutoff point, and then this vector beta sub one contains the coefficients of this elements of the vector y. So this is what beta naught is equal to, and this is what this vector beta one is equal to. So we can do a reparameterization here. Theta can be decomposed into a vector theta sub one and this vector. One and this vector beta, which is the thing that we want. We're not really interested in theta sub one in forming the Bayes rule. So theta sub one here contains the elements of the mean vector mu of the two component mixture with means mu1 and mu2 in proportions pi 1 and pi 2 and a common covariance matrix sigma. And it also consists of theta 1 also consists of the Theta one also consists of the distinct elements of this quantity here, which is the covariance matrix of the mixture model. And then finally, we've got this convenient canonical form where we can take the covariance matrix to be the p by p identity matrix, and we can take mu1 to be the vector that's got zeros everywhere, except, say, in the first element, which we'll call delta, and then mu2's got zeros everywhere. Got zeros everywhere. And this delta is the Mahalanova's distance between, well, this is the squared Mahalanova's distance between the populations. So delta is just the square of that. So, okay, we've got this partially classified situation. We've got these independent feature vectors, and they're all observed, but some of their labels are missing. Tables are missing. So, in order to try and see if we can get an estimator of beta with smaller variance, we're going to follow the, well, we're going to adopt the approach of Don Rubin to write a paper in 1976 and many other papers on it later for missing data. And so he wasn't considering the estimation of the classification rule, he just put forward. Classification rule, he just put forward a general theory for putting forward a framework for missing data. And so we introduce these missing indicator variables n sub j, and they're treated as random variables. And in the particular context here, mj is one. If the label z sub j is missing, and it's zero otherwise. And so we need a missing data mechanism, and here it's defined. And here it's defined in terms of the conditional distribution of these capital M sub J's given all the data. So capital M sub J is the random variable corresponding to the observed value, well the value little mj. Now, so this is our framework and we propose to treat the labels of the unclassified features as missing data and then to introduce Data and then to introduce this framework put forward by Rubin. And on introducing these random variables, we've got to specify their distribution conditional on all the data. So in the terminology of Rubin, missing the MCA missing completely at random, it's the probability here, it's the probability that the probability that the this missing label indicator is one given the data y sub j and z sub j and it doesn't depend on the data it could depend on some parameter um xi but xi would have to be distinct from theta it doesn't depend on theta we need theta to to well we need part of theta to construct the base rule so it doesn't depend on that and then we've got missing at random And then we've got missing at random, where the probability of mj being one given y sub j and z sub j depends on just yj, not the class of origin, not the origin of the class that it comes from. But it depends on theta. Oops, it depends on theta. And of course, it could also depend on the exile. It depends on why subjune. And of course, if this probability. J and of course if this probability were to depend on Z sub J, it wouldn't, we would be missing not at random. So this is the case that we're going to be considering. Now, when we've got missing at random, we could have ignorable missing at random. So that's when this probability doesn't depend on theta. It depends on y sub j and some other parameter, perhaps, which is distinct from theta. Distinct from theta. And in this particular case, we can just go ahead and form the likelihood ignoring the missing data mechanism. So we can just write down the likelihood function as just ignoring the fact that we've got the missing data mechanism. And we can take the Hess n of the likelihood function, log likelihood function, and take the negative of it. And that we can use as an estimate of the... Use as an estimate of the covariance matrix. Of course, if we're trying to work out the Fisher information matrix directly, then we'd have to take into account the missing. And this is an example where a label is missing at random, but we can ignore it. So in this particular paper, this Richard Gordon here was a professor of medicine and part And part of this, we had these observations where if the observation was less than this constant k, then we did not know the label. But if it was greater than k, it was ethical enough to perform an operation to actually get to work out what the label Z sub J was. So, okay, we now let L sub C of theta. L sub C of theta be the likelihood formed from the classified observations. So MJ is zero for the classified observations. And so we get the log likelihood is just the log of the mixing proportion times the class conditional density. And then the log likelihood forms from the unclassified data is given by this: MJ is one for the ones that are unclassified. Unclassified. And so we can go ahead and so this is our log likelihood now based on the partially classified sample ignoring the missing data mechanism. So it's given by that. Now we're interested in this case where we've got the model is non-ignorable missing at random labels. So in this case we're trying to get extra information about beta. Extra information about beta from modeling the distribution of these missing label indicators. So we let q of y, q sub y of j, theta xi be the probability that n sub j is 1 given y sub j and z sub j. It won't depend on z sub j because it's moving at random. And now we're going to take this to be the logistic function of the log n. function of the log entropy of y sub j. So here's the entropy of the observation or the feature vector y sub j. And we put a minus here because in the case of two classes with a common covariance matrix, two normal classes, minus the log entropy can be written as a linear function of minus the square of the discriminant function. So it makes a calculation. Discriminant function. So it makes the calculations much easier if we replace the minus log entropy here with the square of the discriminant function or the classifier. So, but in the case of more than two classes, then we would have to use this entropy measurement here. So, and gamma, it's the expected proportion or the unconditional probability that a feature vector will. That a feature vector will have a missing label. Now, for example, here are some data sets where, in this particular example, in flow cytometry, a technician using their so-called manual gating approach classified the data into four clusters, but was unsure, four groups, but was unsure about these square, these observations where we've got a black. These observations, where we've got a black square. And you can see they're falling in regions where it's difficult to make a decision. And these points here have high entropy. And here we have some other examples, two classes where the blue and the red, and then quite a few missing observations, observations with labels and black. And here's another example there. So we've got the space. So we've got the space classifier now, which is beta transpose y. And so the information about beta all from the partially classified sample, where we're going to use the full likelihood. I haven't given the definition of the full likelihood, but what we do is we just add on the likelihood based on the observations M sub J. So this So, this full likelihood is, well, first of all, it can be decomposed into the Fisher information about beta based on the completely classified sample. Well, we don't actually observe all the labels, so we lose the information. An unclassified observation has the same information as a completely classified one minus. Classified one minus the classification, the information about beta in a logistic regression setup where the labels are regressed on the feature effect. Now, gamma happens to be the proportion of them that are missing a label. So we take away minus gamma, the information in the logistic regression model, except that. Except that the only ones that are missing labels have the mj have the m equals one. So this has got a conditional expectation given that the feature vector has a missing label. So this little c here stands for conditional logistic regression. And then we get the additional information about beta by the introducing the to the likelihood the I should have somehow took out a slide and I haven't got the so this comes from just writing down log the probability of the based on these m sub j's. Now so here's our theorem. The idea is of course when we derive this result we weren't thinking that this could be large enough to overcome this. Overcome this. So we're losing information because we haven't got a completely classified sample. And so if this was one and this wasn't a conditional, then that would be the information as if the sample were just working with an unclassified sample. But because it's partially classified, we've got a gamma there, a conditional expectation there, and now we're adding on this additional information about. Additional information about beta. And so if this term happens to be greater than that, then there's more, the Fisher information about beta is going to be greater for the partially classified estimate. Now, so to work out the The ARE, we take actually. I could just pull this down. This quantity here is a conditional error rate when we use the rule based on the estimate of beta based on, well, you can't see it, but in the denominator, it's the completely classified. It's the completely. So it's the completely classified. This is the estimate of beta based on. Of this is the estimate of beta based on the completely classified sample, and it's the conditional error rate, so it's based on beta hat and beta, the true value, and this is the optimal error. And this is the error rate when beta is estimated by the estimate of beta based on the partially classified sample. And they both got the same first order, they both got the same term of order O1, and so the One and so this is this here. We expand this up to terms of the first order. So this ARE is just the ratio of the coefficients of the terms of order one on n in each. So that's what it comes out to be. And we can do that because we know what the information matrix is. Now. So So here's the ARE. And to our surprise, we expected all these values to be less than one because we thought that the error rate using the completely classified sample would be less than what it would be based on the partially classified sample, even though we have tried to introduce extra information there about beta. And so if we look at, say, this case. And so, if we look at, say, this case here, we've got this as four. So, the first order term in the expansion is 40 times greater, in the partially classified expansion, is 40 times greater than what it is, for the coefficient of one on n in the completely classified expansion. And as this And as this exi one decreases in magnitude, then there's less, we're saying the probability of taking it to be it depends less on the amount of information. And so the number of the ARE falls away to see that. Way to see that. Oh, first of all, we thought there must be a mistake. So we did some simulations. And I know you probably can't see the thing here, but for example, this 40.4, when we did a simulation, we got 38.1 and the simulated values are in parentheses here, in blue, in brackets, but they're all pretty close to what we expected. So we were worried that we must be making a mistake. But you can see what's happening. But you can see what's happening when we take it to be a logistic function. When exone, which is given by this red line, is minus 10, it's only going to select an observation. It's going to have a high dependence on the entropy. But the chances then, and then it falls away very drastically. So this on the y-axis, on the x-axis here is tall sub one. It's, and of course, we're And of course, when tor sub one is 0.5, that is, the discriminator function is zero. Um, there's very it's highly likely to be, um, well, it's got high entropy, the highest entropy you can have. And then as exi one becomes smaller in magnitude, more and more observations can be, have a chance of being selected as and so, um, and as this exhibitor. And as this exy north gets smaller, you can get more, you get less, but this here extends out. So if you I should have a so if we come back, look at this table here now, this is the table where xi naught is just taking the value of five, and then xi1 is going from minus 10 to minus 5. From minus 10 to minus 5. So you see, in this particular case, let's look at the case where it's very hard to make a correct decision because delta is one. So the two classes are close together. And when the mixing proportions are equal, the probability, the optimal error rate is 0.3085. I should have said we're taking the prior probabilities to be equal here, but the results hold also if they're unequal. But if we look at here now, this is. Here now, this is the ARE, it's 40. In parentheses below it, it is the value of gamma. So, in this particular case, about half the sample will have missing valuables because we're saying that it's too difficult to make a decision. And then, as this value magnitude of Exalia 1 falls away, the number of unclassified observations in Provide observations and increase. So here we've got 99% of the sample is, it's almost completely unclassified. If it were completely unclassified, this value here would be 0.005. So this is about 0.005. So it's gained a bit. But so, of course, we. Of course, we don't have control over this exile one and Xi0 unless we're designing an experiment and going to make a rule how we're going to whether we're going to take an observation as being classified or not. Or you might have say a group of medicos looking at say videos from a colonoscopy. If the say in an example we've got Say in an example, we've got if seven of them agree, they take the observation to be um to have what they agreed upon, whether it's benign or malignant. But if they don't all agree, they take it to be unclassified. So, oh well, I think I'm running out of time here. So, we have to go through these calculations to work out the information matrix for the For the Fisher information about the estimate of beta based on the partial classified sample. So we can get a formula for the ARE like this. And as I say, the values we plotted were for when pi one equals pi two, but similar value, we can get similar results when they're not. These matrices, a lot of these matrices involved become. A lot of these matrices involved become diagonal when pi one equals pi two, which makes the job easier. And so there's a fair bit of, even in this simple case of two normals of a common covariance matrix, there's a fair bit of algebra involved. Now, this was an example, as I mentioned, where there were 76 observations on four features, and there were two classes: whether a person's tumor was benign or malignant, and Malignant, and there were 35 cases classified or had labels, 40 we took, and 41 unclassified. When we worked out the error rate using the full rule, we got 0.158. When we use the rule based on the classified sample, completely classified, we got an error rate of 0.171. This is applied to, this is using leave one out, so we applied it to. Oh no, in this example, we actually. In this example, we actually did have the ground truth labels. So, saying it's classified, this is classified according to the seven medicos that looked at the colonoscopies, the videos of the from the colonoscopies. So when we we could actually work out the error rate because we had the true ground truth labels. So in this particular example, it's a fairly encouraging result. And well, I think that's, I'll stop there because. Well, I think that's I'll stop there because I'm not too sure when I started, but anyway, I'll stop there. Oh, let me say that these are the main result here is in this paper in statistics and computing. And we've written a sort of a brief review of semi-supervised learning in this paper here. And we've got a package out for vetting mixtures of normals, multivariate normals, with Covariant normals with not necessarily common covariance matrices.