present this work in um on behalf of uh elena and the rest of my team elena kwok and the rest of my team bar patricelli and philip morowski and um next slide the outline of the presentation will be the following i will introduce as some of you have done before As some of you have done before, multi-messenger astrophysics, show some machine learning applications within our group on gravitational wave data, and then introduce the concept of multimodal machine learning and how this can be applied in the future for astrophysics. I will show as an example a brief study we did on binary neutron star data. Binary neutron star data gravitational waves and gamma-ray burst emission, specifically gamma-ray burst afterglow. And finally, I will describe the project of Wavefire, which points to a real-time classification of astrophysical events. So the starting point, as I said, is multi-messenger astrophysics. The GW170817 has been. W170817 has been already mentioned as the first multi-messenger event observed in gravitational waves and electromagnetic channel. And specifically, there was the coincident observation of the gamma-ray burst and gravitational waves, which led to the first direct evidence that some binary neutron star mergers at least are progenitors. At least they are progenitors of short gamma-ray bursts. There was then the identification of the host galaxy, and the first two observations were useful to also then observe the optical counterpart, the delayed emission, both in optical infrared and UV and X-ray and radio. All of these results are described. All of these results are described in various papers, among which the Abbott et al. 2017 reference. What is multi-messenger astronomy? It's simply the merging of the information from a large number of observational channels. And I showed here two possible sources of two possible astrophysical sources which emit. Which emit a different number of messengers, and specifically the binary neutron star merger, which can be divided into two parts, the early triggers, which are especially useful for fast alert and sky localization for partner observatories, and the broadband follow-up. Broadband follow-up. Both these kinds of events will lead to a better physical understanding of the underlying physical processes involved. And this was very useful in the 2017 observation of the kilonove, specifically with results on the nucleosynthesis of L V elements. You see, the structure involves a large number of partner observatories, and so Partner observatories, and so an overall collective effort of the astrophysical community and astronomical community. Another possible source of multi-messenger different emission sources which can be observed are core collapse supernovae. And we expect some of them to have a gravitational wave emission, and this is. This is coupled with the known neutrino emission, and both of these will provide the initial trigger since they are prompt emissions, while later on we might observe the electromagnetic delayed emission. This could be useful to shed light on the explosion mechanisms of supernova of core collapse supernovae and give important information on the characteristics. Important information on the characteristics of the progenitor stars. So, on to a shift of topic to machine learning applications. Here I will describe a couple of projects in our group. The first one will deal with core collapse supernovae, while the second with compact binary coalescence and anomaly detection. So, core collapse supernovae emit a very particular Emit a very particular type of gravitational wave signal, which has not yet been observed, but is expected to depend on a number of different characteristics, namely the progenitor star and its mass and rotation, then on the physical mechanism involved in emission of gravitational waves, and we have a number. And we have a number of possible physical processes here, which are proton-Newton star oscillation mode, the standing accretion shock instability, just to name a couple. The main difference with compact binary collapse signals is that in the case of core collapse supernovae, we had a large degree of stochasticity, and it is virtually impossible to build a template bank in which we can follow. Template bank in which we can follow the phase of the signal correctly. Therefore, we have to search for alternative methods based on excess power and possibly machine learning. At the moment, we have only simulations, as I said, of core-collapsed supernova gravitational wave signals, and they are produced through computationally expensive 3D signals. Computationally expensive 3D simulations by a number of groups in the science community. Another aspect is that they are very rare, meaning that the event is rare in our galaxy, about one every 100 years, optimistically one every 50 years. And the amplitude is lower, therefore we cannot observe the events at the same distance as. Events at the same distance as which we observed compact binary collapses. Here on the right, I show a number of possible explosion mechanisms and the associated emissions from a reference from Ott et al. in 2017. Bear in mind that the neutrino mechanism is the most probable according to According to scientific literature, and it's also the one with the weakest emission. So, as a test, what we try to do is try to extract information on core collab supernovae and do a classification basically to distinguish between supernovae and noise, or even between different. Or even between different supernova signals. So I show here an example of these signals produced in the simulations. You see, they have different lengths, different amplitudes, and different characteristics in the time domain. This is because of the things I said before. Basically, some of these models are exploding, some are not, some have some peculiar low frequency. Peculiar low-frequency emissions due to the standard accreting shock instability. Some have a large degree of convection, and they can have different peak frequency of the emission. And this can be observed if we focus on two of them. You can see here, for instance, one signal, the one on the top, has a higher peak frequency emission and a delayed explosion compared to the first one. Compared to the first one, to the bottom one, sorry. So the idea was to test classification using machine learning method and convolutional neural networks by adding simulated signals added onto a background noise. The background noise is produced starting from design sensitivity curves of advanced Sensitivity curves of advanced Virgo and the Einstein telescope sampled at 4 kilohertz. We chose a different distance range for the two interferometers due to their different sensitivity. Then we added the signals to the data, choosing a random sky localization in order to observe also the different antenna patterns waiting on the two polarizations of the signals. Of the signals. In this way, we obtain a large SNR range, and we also add synthetically produced noise transients, namely Sangaushen and scattered light glitches, which are also added to the noise. And we try to distinguish the astrophysical model from the noise model. Okay, so this sums up the whole procedure. So, this sums up the whole procedure. We add the waveforms after resampling and filtering to the strain, gravitational wave background, sorry, interferometer background. Then we run a whitening procedure on the data using the wavelet detection filter and produce triggers through a wavelet decomposition algorithm. The triggers are mainly timestamps which are fed to Are fed to a machine learning classifier. They are used to build the samples to classify after a training procedure. So the wavelet detection filter is an event trigger generator which was developed by Elena Cuoco. And it carries out the whitening procedure in time domain and then runs. In time domain, and then runs a wavelet decomposition. After fixing a threshold to apply wavelet denoising, signals are defined with an internal definition of signal-to-noise ratio, which is basically the squared sum of the wavelet coefficient expansion. There is a tunable window length and overlap. And the overlap can be chosen for consecutive segments. And in this way, we can observe transients of different lengths. And we can refine our search on smaller intervals. On the right here, you can see an example of the efficiency of detection of signals by the wavelet detection filter on a number of supernova injections and glitch, so, noise transient. Lich, so noise transients with increasing SNR. And you see that at some point, all signals are efficiently produced a trigger for all models. So the idea was to run a classification on all these samples using a neural network and more specifically using one-dimensional convolutional neural network. Convolutional neural network for time series analysis and the two-dimensional convolutional neural network for time frequency images. So we divided the data into train, test, and validation sets, used a varying number of convolutional layers depending on which network we were using, and the standard activation function rectified linear unit optimizer. Optimizer with the cane learning rate. We also applied early stopping to avoid overfitting and chose a batch size of 64 or 128. We then use, since it's a classification, we use the categorical cross entropy as our loss function. This was run on a GPU, all the analysis. And you can see here the first part of the project. First part of the project was to train the network to distinguish between a core collapse supernova signal and a glitch. In this case, all the different signals were used to all the different models were used to train the machine learning model. And you see that in this way, the accuracy is quite high, meaning that you can generally distinguish between the noise and astrophysics. The noise and astrophysical transient. So, the next step was to add a layer of complexity and try to distinguish among the individual classes. So, namely, distinguish not only between a signal of astrophysical origin and a noise transient, but also among the different core collapse supernova model. Core collapse supernova model and glitch models. And you see that in this larger confusion matrix, where the diagonal is the percentage of correctly classified signals in each class, what can be observed is that the lower amplitude models are generally classified with a reduced accuracy compared to the modern models. Models since this was a more complex task, the training time also was longer. So adding another layer of complexity, we continued the project on real noise and we chose a number of segments from the O2 science run for all three different interferometers. So the Virgo. So, the Virgo and the two lag interferometers. And we also added three new models: one with the rapid rotation, one with the Wolf Raj star progenitor, and one very similar to a previous model, IC18, but which did not include perturbations in the convective oxygen shell, resulting in Resulting in the presence of the standing accreting shock instability emission at low frequency. In this case, we simplified on the other side by fixing the distance to one kiloparsec, so a small distance within our galaxy. And we also added to the convolution neural network classification also another type of neural network, which is the long-short-term memory recurrence. Short-term memory recurrent network architecture. The noise changes in the following way: the noise background since it's real noise. The power spectral density is non-stationary. The different interferometers obviously have different noise curves. And there are a larger number of noise transients involved in the data. In the data, so multiple glitch families, and we don't try to, in this case, to individually classify each glitch since we don't know the numbers of each glitch in the data. Still, the SNR distribution of the core-collapse supernova signals will be affected by the interferometer antenna pattern. And in this case, we chose a data set relatively. We chose a data set relatively small over 15,000 samples. The final data set was then imbalanced due to the fact that higher amplitude models were more present in the data. And you will see this later on when I present the results. The idea to add long short-term memory network was to test the fact that they are generally used to track dependencies in time series. In time series, and they avoid a number of problems in simple recurrent neural networks, such as that of the vanishing gradient problem. On the other side, we observe that they are generally difficult to train and they don't provide a better performance compared to 1D and 2D convolutional neural networks. So, this is an example of the results of Obtained for classification of the different types of signals, and in particular, these are for one individual interferometer. You see on the diagonal still the low emission models, which are also generally less represented in the data set, have a smaller classification accuracy. And you can even see that there is this Anderson S11 model, which is basically. Which is basically never observed, never triggers the wavelet detection filter pipeline. And therefore, it's generally classified as noise or randomly. Another thing I spoke about is that the long-short-term memory networks require longer training time and a larger number of epochs to converge respect to the convolutional neural network. The convolutional neural network architectures. Here you see all the results for all interferometers and all the models, and the true positive rate here in the total column. But finally, I want to show also the results on the merged model on the three interferometer data. So, in this case, what has been done is What has been done is take not only the individual images for one interferometer, but for all coincident signals in all three interferometers. This means that the data set is reduced because, for instance, Virgo doesn't observe many signals, and ideally, we should use the triggers maybe only from the more sensitive interferometers. Anyways, we still. Anyways, we still have a good accuracy at least for some of the models involved. Okay, so the next project I didn't work directly on it, but I can describe it. It's anomaly detection in gravitational waves using convolutional autoencoders. The idea was to create a pipeline which Create a pipeline which could detect anomalies, meaning transient signals in the data, both of noise origin or astrophysical origin, and then try to reconstruct the signal for such anomalies. The framework is similar to the preceding one, but in this case we have a convolutional autoencoder. So this is the workflow. Flow. The idea is that you want to model the noise and predict the noise, and what you subtract is therefore the actual signal in this case. Also, in this project, initially, they were using simulated data as the background of the background noise. The background noise and a mass range, varying masses in this mass range of 26 to 40 solar masses, I believe total, and a varying distance. The sampling rate was lower. And here you can see an example of the reconstruction obtained. And you see that with this method, basically, only the very latter part of the merger is. Of the merger is reconstructed well, where you see the actual peak over here on the right. As a means of defining an anomaly, they use the main squared error, and you can see that specifically in the case of advanced LIGO, the same signals have a larger mean squared error, and therefore an And therefore, anomaly is more is observed more frequently compared to the Virgo data. The same was done on real noise, and the results are kind of similar, still only the merger part is observed and reconstructed, and similarly provide the same mean squared arrow distributions plot for all three interferometers. For all three interferometers. This is an example of the reconstructed signals for, I don't know which exact event, but one of the okay, GV15 or 914, sorry, and this is GW seventeen oh eight oh seven oh six Okay, and so on. Then the main topic is the multimodal machine learning. So, here we saw just some application of machine learning to some types of signals. And the idea is to go on with a different approach in the future and introduce the concept of multimodality. Multimodality is based on the fact that data in the world. That data in the world is provided in different modalities. For instance, we can have images and videos, we have texts, and we can have speech and therefore audio signal. And all of this different data contains different types of information that can be extracted and fed to an analysis pipeline. The information can then be merged. The information can then be merged and concatenated. And as an example, you can imagine an algorithm which describes a video where also noise background is provided. For instance, say the video of a train running on the rails with the background noise, for instance. And you can have thousands of different examples. Have thousands of different examples of applications, okay? So, yes, this can be implemented into a pipeline, especially for classification, but also for regression. And there are a number of applications which have already been done in different fields. In different fields, stemming from medical application to robotics and sentiments analysis, for instance. And to our knowledge, this has not been done in astrophysics, or at least is a new topic. And the idea is to the perfect, let's say, lab. Let's say, laboratory for this is the one of multi-messenger events, where we already have a large number of different observational channels. And of this channel can provide data in different modalities. For instance, gravitational waves can provide time series, but they can also provide time frequency images, and so on for all the different types. We can have neutrinos, we can have Neutrinos, we can have GRB light curves, supernova light curves, and all of these different modalities can be analyzed either separately but also in a single framework to provide predictions. We did a case study using gravitational wave and GRB signal. Wave and GRB signals. In this case, what we wanted to estimate was the redshift of the gamma-ray bursts associated with binary Newton star mergers. So we simulated some gamma-ray bursts and we assumed a knowledge of the redshift only for a part of them. We then trained a pipeline on gamma-ray bursts with a known redshift. The known redshift and predicted the redshift using both gravitational wave and GRB data. So this procedure followed three steps. There was the generation of the binary neutron star systems, and we simulated the gravitational waves signals for the Einstein telescope that I. The Einstein telescope detector, which will have a larger binary neutron star horizon distance. And then we simulated the associated gamma-ray burst light curve afterglow as observed by a Fermi lack detector. So all the data here is simulated also due to the low number of gamma ray bursts available, short gamma ray bursts available. Short gamma is very available in literature. So we used a varying distribution for the mass, but in the range where neutron stars are believed to be. And we used a uniform distance distribution between 1 and 500 megaparsecs. And we assumed that the neutron star spins were aligned and Aligned and focused on sources with an on-axis GRB, so a small inclination of the GRB emission. As I said, the background noise in the gravitational wave data was provided by the Einstein telescope sensitivity curve, so the design sensitivity curve, and we modeled the binary neutron star. Modeled the binary neutron star and added this to the Einstein telescope simulated strain. On the GRB side, what we simulated was the afterglow of the gamma-ray light curves following the approach in Patricia Lietal 2016, which uses GRB090510 as a prototype. And the light curve is corrected to take into account the distance of the source. The distance of the source that we choose and a possible range of GRB isotropic energies. The sampling frequency of the gravitational wave data was set at 2048 Hz, and we simulated a relatively low number of events, so 3000, and divided the data in the usual train, validation, and test set with the following percentage. Set with the following percentages. You see here an example of the two modalities involved. So on the right, you see the GRB afterglow light curve, the decaying with a power law. And on the left, you see the gravitational wave time frequency map. So these two modalities are. So, these two modalities are fed separately to two networks: the 2D convolutional neural network to analyze time frequency image between five convolutional layers and with three and three kernel and max pooling after each convolutional layer. And on the other hand, the 1D CNN was used to analyze the GRB light curve. And similarly, I give the details here and the output of And the output of these two networks is then flattened and concatenated and fed to a fully connected layer with a linear activation function for the regression problem at stake. We used the usual rectified linear unit activation function for the convolution neural network, same ADAM optimizer as in previous project. In previous projects and a batch size of 16. Here you can see the entire workflow where you have the two images, the image and the time and the light curve, which are fed to the two different architecture, concatenated, and give the overall prediction. So the redshift. So here you see a summary of the results. Specifically on the right, you see On the right, you see the predicted redshift and the ground truth. And you can see that the prediction is okay at lower redshift, but then is generally spread overall when going to higher redshift. And you can also see that there is like a wall above 0.1 where the redshift are always can are. Are always not well predicted. So to end the talk, I will speak about Wavefire, which is a project of a prototype for real-time analysis of astrophysical events, and which has already been already implemented for gravitational wave data. It's part of the escape framework, EOS future framework, and the idea was to set up a prototype for real-time detection of transient signals and their classification. It follows the best practice for software management and it tests different software architecture, modern solutions to provide a scalable pipeline. To provide a scalable pipeline for big data analysis. And here you can see the architecture of Wayfire. And the idea was to prepare this framework for detection and automatic classification using some of the newest technology from computer science and using cloud cluster and Cluster and here you can see the different parts stemming from data management. So, the importer of the data, the supplier which feeds the data to the actual parts which process the data and do things such as whitening, trigger generation. And in the end, what we have is a dashboard which reports in real time. Reports in real time all the results of the pipeline. And you see that there are a number of technologies involved and software libraries and tools such as TempSourflow for the machine learning classification part, Grafana for the dashboard, Kafka for to manage the data stream, and obviously an underlying Python structure. This has been already, this prototype WAFIR has been already tested on a cloud cluster at GAR using Kubernetes and Docker containers. And now it's also being tested at the European Gravitational Observatory using singularity. So, since dockers cannot be used at EGO due to fire. At EGO due to firewall issues. And the idea is to extend this not only to gravitational wave data, but to be able to analyze data from different messengers and be able then to run a multimodal machine learning approach and to extract all the different types of information from all the partner observations. All the partner observatories at once in a single framework. And here you can see the idea. So you will have a flux of data from all the different observatories, which will be then fed to Wayfire to extract relevant information and provide an analysis and a classification of the event. As I said, As I said, this is part of the large escape project. And I think I'll leave you with the end slide, which basically sums up the entire WAVIR framework of a real-time alert system. Okay, I think I'm done. Yes. Thank you very much. So, do we have questions? Hi, Alberto. I'm not sure I totally understood the The multimodal machine learning work. So you're using a GW detection and then you assume that this is coincident with the GRB and you use that information to estimate the redshift of the GRB in your simulations. Is that what you're? Yeah, that's what you're doing. Yeah, it's a simulation. It's not a real event, it's simulations. So you simulate the two types of signal, the light curve and the gravitational wave. Right, but what would be the goal in a real case scenario? In a real case scenario, it would be just to provide an estimate of the red. To provide an estimate of the redshift. Not a precise one, though. In this initial study, it's just a test case. Okay, I see. There's a question on the question about this is online. This is online. Hi. Hi. Yeah. So I actually had a similar question on the shall I go ahead. Okay, I'll just go ahead. I'll be quick. So I had a question on the GRBs again. So the plot that you showed where you had the injected versus the recovered redshift, I noticed that your redshifts only go out to, I don't know, 0.1. That is not quite. One, that is not quite the GRB population. So I'm wondering when you did your simulation, what kind of prior that you assume? Because the general, I think, short GRB population, I think, peaks at around redshift of about 0.5. And we don't see binary neutron stars out to that distance. So I'm wondering, I mean, was it just a prototyping case? Yeah. Yeah. It's, I agree with your. It's uh I I agree with your uh with your with your point on the on the redshift distribution, actual distribution. Okay. Do we have any other question I have a question? May you go ahead with your question? Go ahead with your question. Yeah, thanks. Nice talk, Alberto. I was thinking about the last part of your talk. And you said that all this information will go and produce a classification. Is the idea just to have a classifier, or is it also to predict all sorts of parameters ultimately? Yeah, going by step, a classifier. Step, a classifier is obviously the first step, but what you actually want is more information. Say you can start maybe from the sky localization and then see the feasibility of also providing some sort of first parameter estimation, which can be then refined. So, yeah, I said classification as a first goal, but As a first goal, but yeah, the idea is probably to set the bar higher. Yeah, okay, that's interesting because, I mean, ultimately, the big diagram that you showed is like the science machine, right? We put all the data in and then all the science pops out for any kind of the science that we want, it seems. I mean, tomorrow I'm talking about some of the science we can do with just GRBs and gravitational waves. And gravitational waves. Do you envisage this kind of analysis? Maybe you should see the talk first, but do you envisage all this just going inside into a machine and then something coming out of it? It seems very ambitious, but interesting at the same time. Yeah, yeah, I agree with you. It seems ambitious. Seems ambitious, but I think it's a path we can try to undertake in the future. Okay, thank you. Thank you. Do we have any other question? If not, we thank Alberto again and 