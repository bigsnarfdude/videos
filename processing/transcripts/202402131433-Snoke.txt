Speaker is Joshua Asu. He's going to talk to us about de-biasing the bias, methods for improving disparity assessments with noisy group measuring. Thank you. Awesome. It's great to be here. It's also great to go after an hour-long tutorial about fairness stuff because I feel like you all are primed for what I'm going to talk about. And that was a phenomenal tutorial. I also just want to echo, really strongly agree with the last point you made about the specific technical elements. It's like, I don't know. It's like, I don't know. I also work in privacy, and I think both of these spaces, we kind of want to imagine that we can just create really good methods that will solve all the problems. But at the end of the day, we have to work with stakeholders, and we can't just magic it away with statistics. So, anyways, I'm going to be talking about devicing the bias. It's a bit of a cheeky term for the title because sometimes people talk about algorithmic disparities as bias, but also their statistical bias. But also, there's statistical bias, so I'm going to be talking about situations where you might have statistical bias in your estimates of unfairness or disparities in your algorithmic performance across groups. If I can advance, we've got a a wheel. Oh no, it's spinning. No, it's spinning. Go back to the colour. I'm actually really amazed that we're almost at the end of the second day, and this is the first time this is happening. Oh, is that the door? All right, awesome. Real quick plug, and I'll make it even quicker now because we're short on time, but I also, as I mentioned, work on privacy. I'm on a planning committee for a privacy and public policy conference in September in DC. We're actually, this is our initial one. We're hoping to make this biannual. And it's intended to bring together people who are expert researchers in privacy as well as policymakers. And privacy as well as policymakers. Again, I work at a think tank, so I'm really focused on turning all this stuff into a real application with private policy. So, anyways, if you're interested, the link is there. Abstracts are due at the end of the month. I'd love to have people who are interested in privacy come. Okay, also some quick thanks to my collaborators. In particular, Solvey Vostfed, who is actually the lead author on the paper I'm going to be talking about today. She's finishing up at University of Minnesota Biostat. Finishing up at the University of Minnesota Biostat, which I think some of the people in the room are from. So shout out to that department. She's really great. Did really great work with us. She came and worked with my company last year. And unfortunately, she's already got a job, so you can't hire her. As well as my great collaborators at Pad Rand and our funders. Okay, so kind of what we just heard for the last hour was a lot of different stuff about fairness. Now we're going to zoom into one particular type of problem. One particular type of problem and one type of solution that we're offering. So, the space that we're thinking about is predictive classification algorithms, and we're thinking about the healthcare space specifically. So, my collaborators at RAND, some of them are health researchers, and they think about something that is becoming more and more common, which is clinical decision support algorithms, where basically an algorithm is developed to help practitioners or medical professionals. Practitioners or medical professionals make decisions about people's testing. In this case, we're trying to predict osteoporosis. So there's an algorithm that's been around for 20, 30 years looking at trying to predict lone bone density. And this is just one example of algorithms that are used to try to improve healthcare delivery. So, the reason that we are interested in assessing it, as we've just heard, is that there might be differences in. Heard is that there might be differences in performances across different groups. So, here we're thinking about group level fairness, and in particular, for this context, we're thinking about racial and ethnic groups. So, the goal is to analyze differential performance across these groups. So, the way we might do this, and this is pretty standard stuff for fairness, is assuming we're generating binary predictions, there's a Predictions. There's a class of metrics for which you can evaluate group level performance, and then you might look to see if there are disparities and then think about potential mitigations if your algorithm does have disparities. So for example, you might think about the selection rate, the rate of assigning treatment to individuals of different groups, or whether there are differences in the false negative rates, false positive rates, so on and so forth. Okay, so the issue in this particular So, the issue in this particular context, which was raised in the previous talk, is that in order to do an assessment of performance across groups, you have to actually have information about people's race and ethnicity in the data. And in the data that we're working with, these are commonly missing or unreliable in healthcare settings. So, the gold standard for race and ethnicity is considered to be self-reported data where people are self-reporting their own race. Are self-reporting their own race and ethnicity. But in a lot of data sets like Medicare or other data sets, again, here I'm kind of really focusing on the US. That's the context I know. A lot of the data is third-party report, or there's a constraint set of response options. So people might be grouped into other or something like that. And even if there are self-reported data, these are often have high non-random missingness. High non-random missingness. So, for example, in the American ACA marketplace data, about 30% of it is missing. In commercial health plan data, this is often higher. So, even if you have some self-reported data, you're still not going to be able to do a full evaluation because a lot of this. So, one option that is fairly standard, kind of, in other types of health disparity analysis. Health disparity analysis, kind of stepping outside of the fairness world for a minute, is to use measures that are imputed or proxy information on race and ethnicity. And in particular, there's an algorithm called the BISG, which imputes probabilities of people's race and ethnicity based on names and residential information that comes from the US Census Bureau. So this gives you a vector of six probabilities for these six different racial and ethnic groups. And ethnic groups. And importantly, it's really just used for measuring difference between groups. Sometimes people make the mistake of thinking that it's to be used for predicting individuals' race synthesis. It's definitely not supposed to be used for that. And so there are other methods that exist out there. We're not the first ones to kind of think about using proxy information. People, I don't know, use geographic or other information. But in the healthcare context, this one is a pretty well-validated. Care context, this one is a pretty well-validated one. It's been shown to be very accurate in terms of the group level accuracy for assessing people's race and ethnicity. And it's commonly, you can commonly do this imputation in the healthcare setting because the information on people's names and residential addresses are in those data sets. So then the question is how do we use this probabilistic group information to do our algorithmic disparities? So you just mentioned that you're not using this this imputed data in building the model, but you have all this missingness. So I'm guessing race definitely is not included at all in when you build the model. Correct. Okay. Yeah. I mean, you might, I mean, I guess someone could use also imputed information of arrays in their model. In our particular case, we're not doing that. Okay, so how do we move towards estimating these algorithmic disparity metrics, performance metrics, using Performance metrics using proxy information, probabilistic information about people's race and ethnicity. So, real quick, this is pretty fundamental. If we wanted to estimate the false negative rate for predictions from our model, this is what we would do, which is estimate the number of false negatives over the number of positive cases. If we wanted to think about fairness and think about how false negative rates maybe vary by groups, we're going. By groups, we're going to estimate this over different subsets. So, here A represents different groups, different race, ethnicity groups in the data. And so, we're going to estimate separate rates for each group. And then we can think about whether or not there are disparities between those. And then, simply, if we wanted to use a probabilistic input, it's basically the same thing. You would just use that, and you would have a weighted estimate where these pies. Estimate, where these pi's are the probability of being in a specific group conditional on some information Z, which in the case of the BISG, again, is names and kind of location information. Right, so this is not very complicated. We're really just swapping out the indicator of someone's group for the probability that they belong to a particular racial or ethnic group. And you could do this with any proxy method. There's nothing unique about the BISG. There's nothing unique about the BISG. You could do this with even something that predicts a categorical racial variable rather than a probabilistic one. So everything I'm going to show you methods-wise, you could apply this in other contexts with other types of proximate information. So what is the problem then? The problem is that if you use this information, right, if you don't have the true group labels, the true information about people. The true information about people's race and ethnicity, you're going to add bias to your estimates of the group level false negative rates, which is going to then give you bias in your disparity assessment. So this picture here is just notional. Don't worry about what conditional dependence means right now. But it's basically just showing you kind of to help understand where the bias is coming from. So we have these dotted lines, which would be the false. Which would be the false negative rate if we had the true group labels for two different groups. And then the solid lines are the weighted estimate, right? So you can kind of think of the x-axis as the amount of noise in your group probabilities. And so at zero, the weighted estimate and the true estimate are equivalent to each other. And so you would have the correct, you have no bias in your disparity estimate. Into your disparity estimate, but as you move to the left or the right, you're either going to overestimate or underestimate the disparity. So conceptually, what's driving the bias? So in our paper, you know, if you're interested, you can look at the whole theorem and the derivation to get a more technical understanding, but I'll give you some intuitive understanding of what's driving the bias. And it's really the covariance between the groups, the racial groups A, and some. Groups A and some function of the true labels Y and the predicted labels Y hat conditional on Z. So basically, if there's some additional covariance between your groups and your model performance, that's not explained by Z, which is what's producing your group probabilities. So, a couple intuitive ways you can think about this is: there will be bias if there's additional information that A predicts in Y that's not captured by Z. That's not captured by Z, or there will be bias if there's correlation of errors in the predictive model with errors in the BISG model. So, for example, if there's something that predicts osteoporosis, which is, or sorry, if there's something in your predictive model that you're not capturing that predicts osteoporosis, that is correlated with being a racial minority. With being a racial minority in a particular census block, then you're going to have bias in your estimate. So, helping, you know, thinking through this intuitive understanding is going to help us kind of reason about how much bias we would expect in this model. Okay, so to before we kind of derive the formula for the bias, what we did was we can write all of the We can write all of these different performance metrics. So these are all performance metrics based on the confusion matrix. We can write this in a general form, right? So this new here is a group level performance metric. So this could be the false negative rate for a particular group. And we have these H1, H2 functions. And basically, what the table at the bottom shows you is: if H1 is y and H2 is 1 minus Y hat. And H2 is 1 minus y hat, then this general form would be the males negative rate, and so on and so forth. And the reason this is nice is the bias formula that we derive works for any of these group performance metrics. Okay, so again, as I said, there's lots more details in the paper. I'm just kind of giving you the highlights, but here's the estimator for the bias that we derive. I'm not going to go through all of the notation because the important takeaway here is that. The important takeaway here is that everything that's not red in the right side here can be estimated directly from the training data set, or it can be learned from population information. For example, the prevalence of osteoporosis among different racial groups is something that's published in medical journals. And that's the kind of thing that we would need to estimate the bias. The exception is these two parameters. The exception is these two parameters, epsilon and epsilon prime. And the way we propose to handle this is to basically conduct a sensitivity analysis on these two parameters to get an idea of how much bias you might have in your disparity assessments. So what do these two parameters mean? Like when I say you're going to do a sensitivity analysis, you kind of have to have an interpretation of these parameters. So specifically, this is what they are. This is what they are. At the top, there, there's the mathematical formulation. Intuitively, what they are is the average error in your group probabilities among specific cells of the confusion matrix, depending on which performance metric you're computing. So when I say average error in the group probabilities, what I'm talking about is if you have, say, 60% of the people in your data are white, and your average probability of people being white is 0.7. Being white is 0.7, then the error is 0.1. It's just the difference between the mean probability and the proportion for that racial group. And then among the cells basically means for people in, you know, for how the model is performing, basically. So for the false negative rate, for example, the epsilon parameter would be the average error for people whom the model fails to predict a positive class correctly. Positive class correctly, and the epsilon prime would be people where the model does correctly correct the positive class. So you can see here how these sensitivity parameters relate to what I was saying before about what's driving the bias. It's the correlation between the accuracy of your proxy method, your probabilities, and the accuracy of your outcome model. So if you're not doing a good job predicting. Doing a good job predicting the probability of being of a certain race for people for whom the model is, say, accurately predicting that they have osteoporosis, then you're going to have more bias. So it's kind of the combination of those two things that's really driving the bias. But the nice thing about this is now we can start to reason about how much, you know, what magnitude these values should be. So we kind of use a standard way. So, we kind of use a standard way of, there's a couple different ways you can look at this, but we use a standard way of looking at how large the bias might be depending on the value of these parameters. So here we're plotting the two sensitivity parameters, epsilon and epsilon prime, against each other. And the gradient lines are the level of estimated bias in the false negative rate for each group. The false negative rate for each group. And as you can see, if you either have a large value of epsilon and a small value of epsilon prime, or vice versa, the bias is more extreme. So that gets back to, again, to what I was saying about the correlation and the errors between your outcome model and your proxy model. And actually, if they are similar to each other, that red line is zero bias, and that's where. Is zero bias, and that's where epsilon and epsilon prime are not exactly the same, but they're close to each other, at least in this simulation. And basically, what that's saying is you could have really bad estimates of your probabilities of people's race and ethnicity, but if it's unrelated to how the model is performing in predicting the outcome, you're actually not going to have bias or as much bias in your disparity assessments. But if it's related, if those errors are correlated, If it's related, if those errors are correlated, you're going to have a lot of bias stuff. I'm not going to spend a lot of time on this, but we also derived a bound for the bias under some other mild assumptions, which does provide a simpler approach if you don't want to have to kind of do the mental reasoning of how large or how small those epsilon values should be. And so, for some people, this might be preferable. The drawback, and why I'm not really going to talk about it that much, is it's a Talk about it that much is it's not a very good bound unless you have really accurate methods for estimating the probability of race and ethnicity or your group labels. And so it's not a very useful bound basically in a lot of real estate cases. It's very loose. Okay, so coming back to our case study, I'm going to kind of walk you through how someone might use this in practice. So we're going to come back to the score algorithm. Score algorithm. So hopefully that visual is legible, but I'll try to walk you through it. What we did was, again, we didn't have access to the actual algorithm, but we recreated this score algorithm with a data set where we didn't have true group labels. We only had, we were able to create estimates of race and ethnicity using the BISG and an extension of the BISG called the BIFSG. And we And we basically saw, you know, there are, we observed differences in the false negative rates for these different racial and ethnic groups. And we wanted to estimate, you know, what was the potential bias in what we were observing for different levels of kind of this epsilon, these epsilon parameters. And we looked at both uncorrelated and correlated errors. Correlated error is going to lead to more bias. A couple notes, so there's two. So, there's two intervals that you're looking at here. The thin bars on the outside are simply sampling uncertainty intervals, right? So, there's uncertainty in the false negative rates because this is the test that's a sample. And especially for the smaller groups, like the green, which is the American Indian Alaska Native group, that's, you know, they have very wide uncertainty intervals because there's very few of them in the test set. And then those thicker inner bars is the Inner bars is the range, the estimated range of the potential bias. So basically, the way you can read this is rather than just thinking about kind of the point estimates in terms of the group level false negative rates, which might lead you to believe there's a certain amount of disparity, you might actually need to look at kind of a full range to get a better sense of how large the disparities or how small the disparities might be. Call smaller disparities might be. Conversely, right, someone might, it might give you more backing if someone says, like, well, you don't really know that there's disparities because you don't really have true group labels, and you say, okay, but under extreme levels of bias, we still see differences, then maybe you have a stronger argument that there actually are disparities. So the first two, the VIFSG difference and the VISG difference, if you can read those as the first two lines, that's where we assume that the difference is the difference in kind of the particular cells. The difference in kind of the particular cells of the confusion matrix are equivalent to the difference that we observe in the whole population. So, this is kind of an assumption that there's no correlation between the way the VISG is working in those particular cells. And then we kind of do 5%, 10%, 20% just to look at increasing levels of error in the probabilities as we go down. As we go down. So basically, someone could use this to say, you know, even at very high levels, for example, on the bottom left there, we still see some disparities. And in practice, in a real context, someone might have a good sense for how large those differences might be. Real quick, last plug is we also have a tool that we created to let people estimate these. So you can just drop your data set in it with the prediction. With the predictions and the group probabilities, you can play around with the sensitivity parameters, and it will give you very similar outputs to what I just showed you. So if you're interested, you can check that out. All right, and then just a few final thoughts, kind of wrapping up and summing up everything I just talked about. You know, it's important to assess the performance across groups for these CDSAs, right, to get a sense for whether or not they are performing. For whether or not they are performing fairly for different groups of people. In this context, though, there's often missing or unreliable information about race and ethnicity, which is going to hinder the ability to do those assessments. You can use weighted metrics that utilize imputed or approximate information about race and ethnicity, but you need to correct for the bias. And if you don't correct for the bias, then your assessment of the disparity is going to be biased. There are some assumptions that are required, but There are some assumptions that are required, but the methods that we provide give you a way of doing this in a sensitivity analysis type approach that allows you to kind of bound and scope out how big those disparities might be. All right, thank you very much and happy to take any questions. Time. All right, and then we'll take a couple minutes to bleed into our coffee break so we can have some questions. Yeah, okay. I have a general question. So I understand a lot of people nowadays are using like false like accuracy as a measure of fairness in terms of the two groups should have like equal about equal false positive rate or those accuracy measure of the model performance. But this is a little bit different than the original Different than the original goal of achieving fairness. So the original definition that I read many years ago was like, for example, if we're using the Amazon hiring example, male or female should have equal probability to have a job. They should now, their hiring algorithm should not discriminate against male or female. So the accuracy measure now is like the Now is like the new 20 thing to look at in terms of fairness. I'm just wondering, what's your opinion in terms of that? So, I'm not sure I'm completely understanding your question, but I think what you're talking about is the difference between group-level fairness and individual level fairness. So, but you're equalized us, right? So, in terms of accuracy, so many people have looked at the prediction. The prediction rate over the false positive rate for male should be equal to the false positive rate for male. Female should be similar, but it doesn't mean that the system itself is not discriminating against male or female. So I just feel like now this fairness criteria is slightly different than the original fairness criteria, seeing that male or female should have equal power a hired. Yeah, so I mean, so to go back So, to go back, I'm not going to go all the way back, but right, basically, you have a whole set of metrics that you can get from a confusion matrix. And I mean, I would argue that it's a question of what you're trying to be fair on. So in some contexts, like you said, the probability of being hiring, maybe you want an equal selection rate of men and women, or maybe you want an equal positive value. You know, positive predictive value. I'm not sure exactly which one it's going to be for what you're talking about. But say in this context, the reason we focused on the false negative rate is we don't want to miss people, basically, who do need this treatment. And so I would say it's more of a question of what is the important metric that you're concerned about a disparity existing for. And there are some ways to do multiple of them. Ways to do multiple of them, but it's also not possible necessarily to get all of them all at the same time. Is that your question? If I understood correctly, in one of your slides you mentioned that even though the proxy sensitive attribute prediction may not be perfect, it still improves. So how should we understand that? Still improves what? The damage. So I think maybe the point I was making is just because your proxy method is inaccurate doesn't necessarily mean there's bias. And I would take that with a grain of salt because there probably will be bias. But I don't want to mislead you, but my point was simply that the bias is not just coming from inaccuracy of your proxy method. Of your proxy method. It's specifically coming from inaccuracy that's correlated with errors in your outcome model. Does that make sense? Gotcha. Thank you. Thank you. Yeah. Do you have a question? What extent does this end up looking like errors and variables type models? To what extent does this end up looking like errors and variables type methods out of statistics? And I'm imagining if I had some continuous protected attribute. Was protected attribute that I'm only measuring or guessing at noisily. There is a literature on, hey, my co-variety is a mature noisily. What does that do to me? And I don't know how strongly related. I would love to know. I haven't looked specifically at the relationship between the two things, but maybe we can talk more about it. Yeah. Well, let's thank our speaker again and take our coffee break.