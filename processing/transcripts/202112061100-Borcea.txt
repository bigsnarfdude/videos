I want to tell you about some work that I have been involved with over quite a few years now. And we just sort of keep at it and we add new results to it. So because this work has evolved over so many years, there are many collaborators. So I started this work with Vladimir Druskin. Vladimir Druskin with Alexei Mamonov and Mike Zaslavsky. And the first half of the talk is work done with them. But then the later part, so in terms of applications of this work to solving inverse countering problems, then I'm happy to say that my longtime collaborator Jocelyn Garnier has joined the team, and also Jon Zimmerling, who's an excellent postdoc here at the University of Michigan, who unfortunately is going to Of Michigan, who unfortunately is going to leave me this year as he has a very good job at Uppsala. Okay, so what are we trying to do here? So we are trying to solve an inverse scattering problem, which is not so different than the problem that Sue mentioned. So we have a wave equation. By the way, everything I say, in fact, we have currently a project on inversion with electromagnetic. On inversion with electromagnetic waves. But what I'm going to present today is all for scalar waves. Okay, so I'm talking about acoustics. So I have the acoustic wave equation here. And our goal in inverse scattering is to estimate the unknown coefficient in this equation, which throughout the talk will be called C. So this is the wave speed. That's the inverse of the slowness that SU had. And this is a coefficient that appears in this operation. A coefficient that appears in this operator A. I will give the operator in just a second, but okay, this is the acoustic wave equation. Okay, so in order to solve this inverse problem, we are going to use data which are gathered by an array of sensors as shown in this picture. So these sensors are probing the medium sequentially, so one at a time, by emitting some pulse F, which By emitting some pulse F, which appears here in the right-hand side, as the force in the excitation. And then we measure the resulting waves at all the sensors in the array, and then we repeat by exciting from the next source and so on. So this way we gather the data. Now, so this is a very well studied problem, but it's a very hard problem as Sue already told. As Sue already told you. And so we want to bring ideas from computational reduced order modeling in order to sort of, you know, improve the inverse scattering problem. Okay, so what is the reduced order model, first of all, and why use it? So the reduced order models that we pursue are matrix proxies, so they are small size. Proxies, so they are small size algebraic, so they are matrices to use instead of the operator A, or in fact, operator related to A. So, in this particular talk, we will talk about reduced order models for an operator related to A, even though we are now working on reduced order models for A itself. Okay, so now the idea here is that the construction of this proxy, this matrix. Proxy, this matrix proxy, which we call the registrar the model of ROM, in short, is done via a non-linear mapping. So it's quite a complicated nonlinear mapping that takes us from the data to the ROM. But the interesting thing is that this nonlinear map can be computed with linear algebra tools. Okay, so even though we are doing nonlinear calculus, we are using only linear algebra tools, as you will see. Will see. Now, the idea is that if you do this well, then this ROM captures information about the wave propagation. And therefore, you can use it in order to solve the inverse scattering problem. And what I will talk about today is an application of using the RhoM for estimating the Green's function for this wave equation. This wave equation corresponding to sources that are at points inside the imaging image in the imaging domain. So we will estimate the Green's function from points in the domain that we don't have access to to points at the array. Okay, so now I have to say, so this idea of using ROMs for solving inverse problems has been tried before. In fact, there are publications published. So I think Shari Moscow must be around. Shari Moscow must be around. I don't know if she's here, but so Shari Moscow has done work like this for a related problem. So she has worked in the frequency domain, in particular for Schrodinger's equation, and she has tried these kinds of ideas for estimating the Schrodinger potential. Okay, so in that respect, you know, but so the formulation here is different, but the idea is somewhat related to what she has tried. Okay, so. Okay, so let me begin with the setup. Okay, so I repeat here the equation. And for various reasons, we want to work in a bounded domain. Okay, so we look at the wave equation in a domain omega, which could be an actual domain or it could be a truncation of an infinite domain, which in the time domain it's easy to justify simply. To justify simply by using causality, finite speed of propagation, and the finite duration of the measurements. Okay. Now, so computationally, it's easy to understand why we want a bounded domain, but also from the analysis point of view, we are going to do quite a lot of operator calculus. And in particular, we will look at the spectral decomposition of this operator. And then that spectral decomposition is much easier if we work in a bounded domain. If we work in a bounded domain. Okay. Okay, so we will have a bounded domain. There is nothing interesting about that, except that we want the array, the sensors in the array to be very near one boundary. The reason for that is that we want to restrict the wave propagation on only one side of the array. Okay, now if that's not the setup that you have, then that means that one has to do some data processing. So if you Data processing. So, if you have a medium beyond this array, which is shown here, then you have to do some data processing to subtract the waves that move beyond the array. And you can do that if you know the medium there. Okay, so that is in terms of geometry. So, we have this boundary domain, we have the sensors near one boundary which excite and measure waves. Okay, now the operating. Okay, now the operator that we have in this equation is this operator, where C, I remind you, is the unknown wave speed. Okay, so this looks a little bit unusual, this equation. So usually when you write the wave equation, you write the operator as minus C squared Laplace. So we don't want to work with that operator because we want to have a self-adjoint operator with respect to the usual inner product. And so therefore, we have this. Product. And so, therefore, we have this new formulation where instead of working with the acoustic pressure, we work with this wave, which is the acoustic pressure divided by the wave speed. Okay, so in general, you know the medium near the sensor. So, if you measure the pressure, then since you know the wave speed, you also measure this way. Okay, so that's not a big deal. Okay, so this is our operator A, and C is the unknown coefficient. Now, in terms of the excitation, we also have The excitation, we also have a technical requirement, which is that the pulse that is emitted by the source is an even function with Fourier transform, which is non-negative. Okay, so this may look very strange. You will see in a moment why we need this. So we need this for the operator calculus that we do. But let me just say that this is not a big deal. So if you have sources which emit different pulses, then you can do some data processing, which is Some data processing, which is very well known actually in radar science, which essentially takes the measurements that you have and you convolve them with the time reverse pulse that you emit. And then that way, mathematically, you make it look like you work with a pulse which is even with non-negative Fourier transform. Okay, so none of these assumptions are deal breakers. And our array data, what we call array data. Our array data, what we call array data, are measurements of the wave, okay, which is so corresponding to m sources. So we have little m will be the number of sensors. The sensors act as both sources and receivers. So we measure for each time instance, essentially a matrix which is M by M corresponding to M sources and M receivers. And also we measure only at two. We measure only at two n-distinct time instances, which we call JÏ„, where tau is the time sampling rate and that has to be chosen well. Okay, so this is the setup. So now I would like to begin with the description of the reconstruction of the ROM. So there are basically three steps that I want to go to. The first one is to One is to put the problem in a framework where we can do reduced order modeling. Okay, so computational reduced order modeling is really looking at problems that look at dynamical systems. Okay, so first we want to write the wave propagation problem as a dynamical system which evolves according to this operator A, which captures the medium from an initial state, which is generated by the excitation. Which is generated by the excitation. Okay, so there are many ways of writing such a dynamical system. And in the literature, you can see many of them. Most of them are approximate. So one typically takes the second derivative in time and uses some finite difference discretization. Okay, so we are going to use a discrete time formulation as well, except that our formulation will be exact. Okay, so as you will see. Okay, so as you will see. Okay, so we will have we will for the first thing is to write the wave equation now as a dynamical system for the evolution of the wave. The second step is to compute the reduced order model. So the computation of the reduced order model, as is typically done in this field, is by a projection of the dynamical system that we will write on an appropriate space. Okay, so the tricky part here. So, the tricky part here is that this construction of the ROM must be data-driven. So, we only have information about the wave at the sensors in the array. And from that, we have to construct the ROM. And of course, then the third step is once we have constructed the ROM is to understand its properties so that we can use it for solving the inverse problem. Okay, so the first part is to write the wave propagation. The first part is to write the wave propagation as a dynamical system. So, we want to write an exact, discrete in-time dynamical system. So, the first thing that we want to do is to transform the problem, which we had here with a source. So, here we had the source as the excitation. We want to map that to an initial state. Okay, we want the problem with zero force and an initial state. And an initial state. Okay, so a good way to do that is to work not with the wave itself, but with the even extension in time of the wave, which is defined here. And now I just have to say that this is, of course, a mathematical construction, but this is really not a big deal. And you can deal with that in practice for the following reason. Okay, so because of, if you look at Because of, if you look at this equation, okay, so the wave is zero before the pulse is emitted from the source. Okay, so the pulse is of very short duration. So if you look at this wave, this wave lives at negative times that are only within the support of the pulse. Okay, so the support, so the pulse, let's say it's centered around zero, at time t zero, but it has some width. Zero, but it has some width. And so this wave is non-zero at negative times only for times within this pulse width, which is very short. Okay. And so for times bigger, for t bigger than the width of the pulse width, this guy here, the wave at negative time is zero. So really, what we call the even wave is really the wave. And now for time within the pulse width, Within the pulse width, then this one will make a contribution. But the point is that since we know the medium near the sensors, and since the pulse is short and the wave propagation is causal at finite speed, we can calculate the wave in the medium with known speed around the sensors, and therefore we can calculate this. Okay. So by adding this here, it's not a big deal. So this is something that. Is not a big deal, so this is something that can be calculated for negative times. Okay, now if we do this, okay, so there is some mathematical analysis here to be done, and so there's some calculus with operators, you can show that this even extension in time of the wave is given by the cosine of time square root of the operator A. So remember, my operator A is self-adjoint, positive definite, and so we. Definite. And so we can define functions of operators as usual, you know, using the spectral decomposition of A. And then here we have the Fourier transform of the signal, which is evaluated at the square root of the operator A. And all this, so this is an operator that depends on A, which acts on the initial state on the Dirac delta, which is located at. Delta, which is located at the source location. Okay, so this is, I mean, you can find this in our paper, so it's a little bit of, you know, tedious calculation, but one can prove this. Okay, now what are the data matrices? So we define the data matrices, again, for M sources, M receivers, okay? And the data are collected at two n time instance, and we define them to be We define them to be the even wave. So, remember, we measure only this, but for negative time, we can calculate this. So, we define our data matrices to be the even wave evaluated at the discrete time instance j tau. And here I just wrote what this wave is. So, I recall the expression of the wave here. And then the evaluation is done via the Dirac delta, which is Dirac delta, which is located at xr. Okay, now using the fact that functions of the same operator, these are actually analytic functions of the same operator, commute, and using the fact that f hat is non-negative, I'm going to take the square root of this f hat, and I'm going to move it on the left, and I'm going to write these data matrices in this symmetric form. Okay, so you see that these data matrices are You see that these data matrices are now symmetric and they are given essentially by the inner product between this wave, which is written here in red, and this function. And these functions, we call them the sensor functions. And they have this expression. Okay. And what are they? Well, first of all, they are supported near Xs. Okay, so you can prove that they are supported near the source. Okay, so a function like that is. Source. Okay, so a function like that is essentially a Dirac delta, which is smoothed a little bit by this operator f hat, the square root of f hat acting on A. Okay, so okay, so we did some manipulation, some analysis manipulation to obtain these data matrices in this form. Okay, now I introduce here this new wave, which I call U. Okay, so this wave now So, this wave now solves a problem, which is this homogeneous wave equation with an initial state, which is given by this sensor function. Okay, so this initially, this wave lives near the source, okay, it's highly localized near the source, and it has a support which is determined by the frequency content of the signal. And okay, so this is the new way. And okay, so this is the new wave. Okay, now I take this new wave. Okay, so look at this problem. So I write U is equal to cos T square root of A acting on the initial condition. And okay, so in order, it's just cleaner in order to have all these, you know, indices XS and so on, what we are going to do is we are going to group all the initial states in this. State in this vector-valued function, which I call delta f. Okay, so this is the initial state of the waves for all the sources. Okay, so this is a vector-valued function. And we define what we are called as snapshots, which are this wave view, which is obtained from the initial state as it evolves through the wave equation. Okay, so. Okay, so the solution of this equation is given by the cosine of t square root of a operator. Okay, so this is the solution, formally the solution given by this way. Now, once we write the wave this way, then we can write the exact discrete time stepping, which is our dynamical system. Okay, so this is simply using the trigonometric identity of the cosine. Trigonometric identity of the cosine, okay, which says what is cosine of J plus one tau square root of a plus cosine of J minus one tau square root of A. Okay, so you just use the law of the cosines and you obtain this evolution equation for the wave from one time instant to the next. Okay, this wave starts with the initial state which I wrote. With the initial state which I wrote here, and this is just because I have an even in time wave, and the evolution is dictated by this operator of A, which we call the wave propagator operator. Okay, so now we have an exact, so this is exact, there is no approximation, so we have an exact discrete in time dynamical system which describes the evolution of the wave, and what we want. And what we want is, we want to learn the wave propagation as is described by this system using a reduced order model. And to do so, we can only use the following data. So we have the initial state, U0, which are these initial states which can be calculated. And we have the data matrices, which I wrote here. So these are n. Wrote here. So these are m by m matrices, okay, which are given by this integral. And I'm going to denote henceforth this integral using this notation, even though it's probably deceiving, because this is typically inner product notation, but this is not an inner product because this is a matrix. Okay, so I'm going to use this notation to denote this integral of one, okay, this. One. Okay, this integral. Okay, so our job now is from this guy and these data matrices to learn to define a ROM. Now, what is the ROM? So this idea of constructing a ROM as a projection of a dynamical system is, of course, the bread and butter of the registrar modeling community. And so the first thing, you know, so we will construct a ROM. So, we will construct a ROM which is a Galerkian projection of the dynamical system that I just draw. So, first we have to decide on the approximation space. And the approximation space is the space spanned by the first n snapshot. So, remember, we measure two n time instances, and we are going to need, in order to construct the ROM, the first end snapshots. What are these snapshots? They are again this wave. Okay, so this. This wave, okay. So, this is a vector-valued function because I grouped together all the excitations from all the sources. So, this is the wave at time instance j tau, and we line them up up to time n minus one, and we form this vector-valued function capital U. And using linear algebra notation, we are going to call the approximation space the range of u. So, that is the space spanned by this. Is the space spanned by these snapshots? Okay, so the Galerkin approximation is, as usual, written. So you seek a solution, an approximation of the solution in this approximation space. And then in this approximation, this GJ are matrices of the Galerkin coefficients. Okay. And so as is typical, what you do in a Galerkin approximation. Typical, what you do in a Galerkin approximation, you take this approximation, you stick it into your equation, which is our discrete time dynamical system, and then you ask that the residual, which is here, be orthogonal to the approximation space, which is what I wrote here. Okay, so this is the Galerkin approximation. Okay, so now let me just say a few words. So, Rome was a Galerkin. So, Rome as a Galerkin projection is not new. Okay, so obviously, you can find lots of references in the reduced order modeling community. So, however, how they do it is very different than how we do it. Okay, so usually when you look in the literature, one usually projects the PDE, so that is the wave equation with the second derivative in time, or Time or it projects an approximation of this PD where one takes some approximate finite difference version of the second derivative with that. Okay, so from the beginning, the dynamical system that one has, if one works in a discrete setting, is approximate. In our case, the dynamical system is exact. Okay, now the second difference is that one knows the approximate. Knows the approximation space, okay? And so this U is known. And in fact, in most of the applications, particularly people who do climate science and stuff like that, this space, the number of snapshots is quite large. And one, so what I mean by that is that the approximation, you know, the set of snapshots is usually redundant. And then the idea is to construct a registrar model which To construct a registrar model which compresses all this information. And that is usually done using principal orthogonal decomposition idea. So it's kind of like an SVD idea. Okay. So we don't want to compress anything. We just want to learn how the wave propagates. And so first of all, our U, our set of snapshots, is supposed to be linearly independent. In fact, it's rather important that it is so. So this U is full rank. Okay. And moreover, Okay, and moreover, because we project an exact discrete dynamical system, we actually have an exact approximation. So this thing, which is usually an approximation, and if you look in the literature, there are many estimate, error estimates, and you know, all kinds of stuff, you know, kind of like numerical analysis type of work with that respect. In our case, because of the way we construct this, this approach. Way we construct this, this approximation is actually exact at the first time steps, at the first end time steps. Okay, so in particular, what that means is that if you look at these Galerkin coefficients, the first one is this, what we call E1, and the last one is, I mean, the n minus 1, 1 is En. And this, what are these? They are just block columns of the n times m identity matrix. Okay, so this is sort of how this differs. Sort of how this differs. And of course, now the big if is like, but okay, but you know, so in the literature, they assume they know all these snapshots. We don't know them. How do we do this? So, okay, so let me rewrite this Galerkin equation. So what I took this equation, okay, and I just rewrite it in a compact form. So this is the Galerkin equation, how it looks. And I introduced now this typical Galerkin. Now, this typical Galerkin notation, where the matrix M, which is given by the products of the snapshots, this M is known in the Galerkin lingo as the mass matrix, and this S, which is given by this product of the snapshots against a propagator acting on mass-down snapshots, this is called the stiffness method. Okay, so remember that we So remember that we know the gj, the Ganelkin coefficients, actually for the first n minus one time, for the first n-time instances. Okay. And now what I'm going to show you is that even though we don't know the approximation space, in fact, this mass and stiffness matrix can be computed just from the data that we gather at the array. Okay, so even though we don't know the approximation space, we can actually calculate everything. Can actually calculate everything in this Galerian equation. Okay, and so here goes. Okay, so let's look at the L J block of the mass matrix, okay, which is given by this product of the L snapshot with the J s one. Okay, now what I did here is I just wrote what the snapshots are. So this is the initial state, and this is the operator which depends on A, which tells me. Which depends on A, which tells me how the wave is propagating from the initial state. And then similarly for the other synapses. Now I use the fact that A is self-adjoint. Okay, so I can move this thing on the right. And then using the law of the cosines, I can write this like that. Okay? But then when I stare at this, I recognize that this is nothing else but data matrix. is nothing else but data matrices at time instant l plus j and a time instant l minus j okay so this shows you how you can calculate the mass matrix without knowing the approximation space just from the data and a similar calculation applies to the stiffness okay so we have now we started with the dynamical system uh and i showed And I showed you now how we can get this Galerkin equation. Now, I want to write, that's not yet the dynamical system in the registrar model space, so I'm going to write that now. So, in order to write this registrar model, so I mean, already we have a registered model, right? But we now want, so this you can think of it as a registered model, but we now want to bring it in a form with which we can work. With which we can work. And so to do that, we are going to take the data-driven mass matrix, which by construction is a symmetric positive definite matrix. And we are going to take the square root of this matrix. And we want a specific square root, which is the Cholesky factorization. So this is the block-Colesky factorization, where this matrix R is blocked up. Is block upper trend. Okay, so we can certainly do that. And then if I take the Galerkin equation, which we saw how we can compute from the data, and we multiply this equation on the left by this Cholesky factor inverse and then transpose, we obtain this equation. So this is the reduced order model dynamical system. And in this equation, I introduce what we call the reduced order. What we call the reduced order model snapshots, which are given by this R times J. And recalling in particular the special way we constructed this, we made this construction that the GJ are block columns of the identity matrix. We observe in particular that the first n ROM snapshots form exactly this upper triangular matrix. This upper triangular matrix R. Okay, so this, the way we want, so you know, this structure of this upper triangular matrix R is very good for what we want to do because in particular, it respects the causal wave propagation. Okay, so let me explain that. Okay, so each block column, each block column corresponds to a time instant, okay? And the row of blocks corresponds. Correspond to sort of like, I would say, the range location in the domain. Okay, so if you look here at the first time instant, which is the first block column, okay, we know that the waves in the physical space are located at the sensors. Okay, so they are right there at range which is very small near the array. Well, that is reflected by the fact that the That is reflected by the fact that the first ROM snapshot is localized only in the first row of blocks. Okay, support it only in the first row of blocks. It's zero everywhere else. Now, as time progresses, of course, the wave propagates down in the median, and that is reflected by the feeling in the row of blocks. Okay, so the second column block, okay, so that is the wave at time instant tau. Okay, so the wave propagated a little bit, we have a feeling in the second row, and so on. A filling in the second row, and so on. Okay, okay, so these are the ROM snapshots, and then the ROM propagator is given here, so you can see that it's data driven. Okay, so now let me sort of summarize what we have done so far. So now we have the reduced order model dynamical system, which is this. And this is the algebraic version of the true dynamical system, which is this. True dynamical system, which is this. Okay, so they look very similar. Okay, now I already said that. So the initial state is supported in the first row of blocks, okay, which is reflecting, so it's the algebraic version of this delta F being supported near the array. And moreover, we can prove that this the propagator, this P rho. The propagator, this P ROM for the ROM, is a matrix which has very special algebraic structure. It has block tridiagonal structure. And therefore, so you start with the initial state, which is supported just in the first row. Because of this block upper triangular structure, I mean, because of this block tridiagonal structure, then you will see that the next snapshot is supported in the first two rows of blocks. Supported in the first two rows of blocks and so on. Okay, so this block tridiagonal structure of Prom is again a reflection of the causal wave propagation. Okay, and finally, this is something which is actually not quite obvious. So it may be from the respect, from the point of view that we use the data to construct the ROM. But in general, in reduced order modeling, it's not so easy to show that you have interpolation by the You have interpolation by the registrar model of what you try to match. So, in this case, it's true. So, in fact, you know, I don't want to go through the proof, you can find it in our papers. So, we can prove that the ROM not only has a dynamical system that looks like the true one, but also interpolates data. So, that means that whatever we call data here, which is given here, this is the mathematical model of the data, is also. The data is also predicted by the ROM. So the ROM gives the data by taking the ROM snapshots and evaluating them essentially at the array using this delta ROM. Okay, and this is true for all the measurement steps. Now, another way to look at this is to view, to make clear. To make clear, you know, so what we have done here is really a Galerkin projection, and I want to make look a little bit more at this projection. Okay, so to do so, we are going to take the space of the snapshots, which again we don't know, okay, but mathematically we can look at it, and we are going to construct an orthonormal basis of the space of the snapshots, and to do so, we are going to And to do so, we are going to use the Gram-Schmidt orthogonalization procedure, which is a procedure that calculates this V, which has orthonormal columns, if you want to think of it. I mean, these are functions, really, okay? But they are, so this is a vector, it is a vector-valued function with entries which are orthonormal functions and they form an orthonormal basis of the approximation space. And the construction of this. Approximation space and the construction of this spaces is causal. Okay, and that is because we have this R here, which is has this block upper triangular structure. Okay, so the first block of the V's is in the span of the first block of the U's, which is the initial snapshot. Then the next block is in the span of the first two snapshots, and so on. Okay, now this R is exactly the same R that appears in. Same R that appears in the Cholesky factorization of the mass matrix. Okay. And if we recall what the stiffness matrix was in terms of the propagator and this U, okay, and we recall the expression of the propagator of the ROM, which is here. So this is how we can compute it. Okay. So the data construction of the ROM propagator is from the data we get M from Data, we get M, from M, we get R. From the data, we also get S. And so then that's how we construct this PROM. But then when we look at this, we see that in fact, using this Gram-Schmidt factorization, we can also write this PROM this way. And so now you see that what this PROM is nothing else but the projection of the true propagator, okay, in the physical space on the On the approximation space using this orthonormal basis. And similarly, we have an expression for the Rom snapshots. Okay, so now let me say a few things about the properties of this orthonormal basis. And, you know, we understand some of it, we don't understand all of it. So there is still work to be done here. So the first thing to say, I already said that, is the basis is causal. That is the basis is causal. Okay, so this is rather important for the construction. Okay, so now the second thing is that if you look at the snapshots in this view, so the wave, how the wave, you know, the snapshots of the wave, they depend strongly on everything. Okay, so they depend on both the rough part of C, which in imaging we call the reflectivity. The reflectivity and they also depend on the smooth part of C, which is what defines the kinematics. Okay, so Sue worried about the kinematics a lot. Okay, so this U depend on all of this. Okay, now if we work on problems in imaging, which is the application I'm going to focus on today, then the kinematics is assumed known. Okay, so the smooth part of the wave speed is assumed known. And what you want Assume known. And what you want to find in imaging is the reflectivity. So that is the rough part of the wave speed. Okay, so interfaces, places where waves scatter. And in fact, in imaging, we only want the support of the reflectivity. Okay. Now, okay. So now the idea is the following. So if the reflected and transmitted waves can be approximated well in the space, It well in the span of the snapshots in the reference medium. Okay, so we are not saying so. If you look at u and u naught, so u naught is the matrix of the snapshots in the reference medium, they will look very different. Okay, however, if the spaces spanned by these two snapshots are not very different, then the orthonormal basis, because it's constructed in a causal way, is not going. Way is not going, it's going to be approximately the same. Okay, now, where on earth is that true? Okay, so this is the tough part. So, we have now managed to prove that this is the case with very explicit analysis in two particular situations. So the first one is in the case of layered media, and the second one is in waveguides which contain inclusions that are not too slanty. That are not too slanty. Okay. And how have we done this is because, in these two settings, we have available very explicit tools in the analysis of the wave. So we can decompose the waves in a very precise algebraic way, and we can analyze in great detail the spaces that are spanned by u0 and u. Okay, and what we found in these two cases is that as long Is that as long as you have a good enough time sampling of your data, this tau, and also if the sensor separation is not too large. So the rule of thumb essentially, so there is actually a trade-off between sampling too much and not sampling enough. But the rule of thumb is that a good tau is chosen according to the Nyquist sampling criterion. The Nyquist sampling criterion for the wavelength that you emit to excite the waves. And the sensor separation should be roughly at about half a wavelength where wavelength means the central wavelength in the signal. Okay, so then if that is the case, then you obtain that the spaces are not very different and therefore V and V naught are approximately the same. Now for the other settings, we don't yet have an analysis, but Yet, have a numeric analysis, but we have numerics, which indicates that this extends to more general settings. So, I show here now some plots. Some of you already have seen this. So, this is a simulation in a layered medium for which now, of course, we have analysis. So, this is a medium where we have the acoustic impedance that has four jump discontinuities, one, two, three, four. They create reflections. And these are the snapshots in a two. In at two different so here on the these two plots, there are the snapshots at two different time instances. With green, I show the snapshots in the medium with these scattering features. And with blue, I show the ones in a homogeneous medium. Okay, so in a homogeneous medium, you just have a pulse that keeps going, nothing happens to it. In the scattering medium, you see reflections. Okay, so you see lots of reflections. So, you see lots of reflections. And in the bottom, I show you the orthogonal snapshots, which are components of this orthonormal basis. And you see that even though these guys are very different, the orthonormal snapshots are kind of similar. Okay. And a similar situation here is in a waveguide where you have actually this is more like an open space because the boundaries of the waveguide are really far off. Of the waveguide are really far off, so they don't really affect the simulations. So, here you see two reflective structures. Here are the waves in the reference medium with no reflective structures. Here are the waves in the medium with reflective structures. You see very different these ones than these ones. Yet, when you look at your tonormal snapshots, that is the components of these bases, they look pretty the same. Look pretty the same. Okay, so now let me go to the application. So we want to use this registrar model to estimate the Green's function. Okay, so what we want, so what can we estimate? Okay, so the best possible that we can hope for is the following. So is to take the wave that originates from Y, okay, we can't quite catch. We can't quite catch a Dirac delta with a ROM, but we have sort of a projection of this Dirac delta on the approximation space. So this projection operator is V V transpose. And then we also have the smoothing due to the signal F one hat. Okay. And then this is propagated through the true medium from the point Y in the imaging region to the receiver. The receiver. And if you do the calculations, you find that this expression of this g, this is the ideal estimate of the Green's function, is this. We can calculate here u j ROM, but we cannot calculate V because we don't know the approximation space. What we can calculate is this stuff, which we show over here. So the difference between this and this is that instead of having the projection of the Dirac. Having the projection of the Dirac delta at y, we have this thing, which is v0 transpose. Okay, so if v and v zero are similar, then this guy is going to be peaked at y. And if this is peaked at y, then this is a good estimate of the Green's function. When is that true? Okay, well, so here is the. Sorry, you have five minutes. Yeah, I think I'll be fine. Thank you, Christmas. So, um. So, here is a simulation in a layered medium. So, this is what we call a randomly layered medium. Okay, and we have some target here. And we want to see how this approximation of the Dirac delta projected on the true space, or this approximation that we can, this is the only thing that we can deal with, how do they? Deal with how do they differ? Okay, so the left two plots are what the projection of the Dirac delta looks like for two different points. And on the right, you see what it looks like for what we can achieve. Okay, so this one in particular is a little bit tougher, but still they are quite well picked. And this is a simulation which is very hard. So this is in an isotropic random medium. And again, you see some degradation. Again, you see some degradation of the peak, but it's still rather good. Okay, so what can we do with this? Okay, so we can do a lot. Actually, we have a bunch of projects that are now being written. So one project is to obtain quantitative estimate of C, that is finding both the kinematics and the rough part. And this involves rather involved processing with time windows and so on. So I don't have time to talk about that. We can also. That we can also use this for control of illumination from the array for focusing at points. And what I'm going to describe now is a very simple imaging function for estimating the support of the reflectivity. So this imaging function is strikingly simple. All we do is we take this estimate of the Green's function from the Rome and we just calculate the energy of this. And that's it. Okay? And this. Okay, and this is actually a very good imaging function. Now, if you analyze this imaging function, you obtain that this imaging function has this expression. Okay, so there is a kernel here, gamma, which I call gamma, which has an interpretation of time reversal. Okay, so this gamma here is a function that, depending on how good is your bandwidth and the aperture of the array, is going to be picked. Is going to be peaked at x is equal to x prime. Okay, so this guy here is going to be sensitive only to what happens at x near x prime. Okay, now, so if your delta y rho, which is the approximation of the Dirac delta, is well focused at y, then this imaging function is going to be sensitive to changes in the wave speed around the point y. Okay, and Y. Okay, and here is the expression of the kernel. I give you here the expression of the kernel. So it's basically a time-reversal interpretation. You have a wave which originates from the source at x. It propagates to the array where it's recorded. So this is the true Green's function. And then it's observed there. Okay, you time reverse it and then you send it back in the medium and you observe it at X fine. And we all know from the principle of time. And we all know from the principle of time reversal that as long as you have a big enough aperture and bandwidth, the focusing is good. Okay, so I'm going to show you how this works. So this is a setup. So this is the phantom that we want to image. We have sort of like a bunch of reflectors, some slanted, some not. And here is the imaging function that I showed you, but here we're cheating. So instead But here we're cheating. So instead of using delta y ROM, we use the projection of the direct delta on the approximation space. So this we cannot achieve. Okay, but I just wanted to show you that the resolution of this image is really controlled by how well peaked this delta Y ROM is. Okay, so in this case, if you have the projection of the delta Y, so if we knew the snapshots, we would get this nice photographic. This nice photographic image of the media. Now, what we can do is this thing, and you know, so what we don't like this image to display like this, so we take a range derivative of this image. The reason for that is because this carries sort of like the because of energy, so there is a lot more energy here near the surface than below. Okay, so we just take a derivative in range in order to get rid of these shadows. To get rid of these shadows, essentially. So, this is the image that we get. So, we see that we see the targets quite well, except for the vertical one, which is very hard to image. Now, this is what reverse time migration will do in this case. So the image is a little bit noisy and it has some multiples. Our method is free of multiples. And in fact, you can see that in this simulation where multiple scattering between the targets is. The targets is stronger. So, here is our imaging method. Okay, and this is reverse-time migration. So, you see some very strong effects of the multi-scattering between the targets and also the surface. Okay, so, and here is a complicated model. So, this is the Marmusi model. And here, of course, we use the kinematics of the medium. So, we take this medium, we smooth it out. Okay, so we. Smooth it out, okay. So we take as reference medium the smoothed part of this, which is actually only 1D, so we smooth it, so we get a layered medium but very smooth. And then we're looking for the reflectivity, and this is what we get with our method. So you can see a lot of the features actually. We were quite surprised how this easy method works. Okay, so I finish here. There's a paper on what I presented, the second part that Second part that is to appear in inverse problems, and there's a second, there's an earlier paper that describes sort of the wrong construction and all the proofs and all that that appeared last year. Okay, so I think I finish here.