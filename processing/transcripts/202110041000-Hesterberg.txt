And here is, we do have a white paper out, which you can download from the Google Research site. This is joint work with a number of other people. So, overview of the talk is: I'll give an introduction to the problem we're facing and talk about some of the modeling that we did, some of the statistics. We did some of the statistical surprises that came up in the course of that modeling, and then summarize. So we are measuring the performance of brand video ads or other brand ads for context. You think of Google, you think of the Google search page, and on the search page we have search ads, and it's easy to measure the effectiveness of those ads by how many people. Of those ads by how many people click on the ads. But brand ads are different. Brand ad is maybe you see, you visit CNN.com and you see an ad for Google Home, or you visit YouTube and you see an ad for the Google Home. And then you probably don't click on that ad, but that ad may give you an impression, a favorable impression of this brand, and maybe later purchase. And maybe later purchase. So, brand advertising deals in a number of stages. The first stage is we want users to be aware of the brand. Have you seen ads for the brand? Do you know the brand? Do you like the brand? We're trying to move users down this funnel from awareness to consideration to purchasing. And SurveyLift is one project at Google where we are using surveys to measure whether people are aware of the brand, considering purchasing, et cetera. And we use this to measure the effectiveness of display ad campaigns by comparing the answers from people who have actually seen the ads with people who have not. So we're doing a randomized. So, we're doing a randomized experiment. We're doing this partly on YouTube or partly anywhere else on the rest of the web where people are running a display ad campaign. We assign people to a treatment group or a control group. We basically randomize people's cookie, use that to randomize them into one of the two groups. People in the treatment group can see. In the treatment group, they can see a campaign ad. So they visit YouTube or they visit another site where an ad can be shown. We run our auction. The ad for the campaign we're looking at, say Google Home wins, they see that ad. There's also the control group. People who visit the website, we run the auction, the Google Home ad wins, but rather than showing that, we rerun. But rather than showing that, we rerun an auction with the Google Home ad removed, and they will see the ad winner from the second auction. So we now compare people that actually saw the ad to people that would have seen the ad, except they were in the control group. And measure things like increasing brand awareness, consideration, favorability. Any questions before I proceed? Any questions before I proceed? Now, I can't see if hands are raised. I have to actually just speak up. Hi, Tim. I do have a question. There could be social network effects here. So I may not have seen the ad myself, but if my wife saw it and liked it and liked the product and has been telling me that we ought to buy a Google Home thing, then is there the possibility of contamination? A possibility of contamination, but we're not dealing directly with purchases here. We're dealing with responses by people who saw the ad to surveys. So unless you're sharing a cookie, so if you use the same browser and the same device, then there's contamination that way. Maybe she saw the ad, but you answered the survey. But otherwise, we're just looking at the survey responses. I understand. I'm simply saying that. I understand. I'm simply saying that my wife saw the ad, told me it looked really cool. And so when I get the survey from you, I respond more positively because my wife likes it. Yes, that's true. But I think that doesn't cause a lot of bias because that averages out across all the people that we survey. Okay. Thank you. Any other questions? Yes. Hi, Tim. I have a question. Yes, hi Tim, I have a question. So I don't understand the part of the survey data. So since Google has a lot of like strong tracking ability, so is it possible to track the user like came to the Google site rather than using a survey to reflect the metric? Google doesn't necessarily have great tracking. Doesn't necessarily have great tracking ability. In an earlier project I was involved with, we were working with a set of data where we did have ability to track. This was people using Internet Explorer that installed the Google toolbar and gave Google permission to then track what websites they visited and so on. So, there we could use not just surveys as our Y variable, but As our Y variable, but did people who see an ad then go to the brand advertiser's website? Or did they come to Google and do searches for things relevant to that advertising campaign? That source of data is long gone. In general, Google doesn't have that super ability to track people. So here in this project, we're limited to surveys. There is a related project. There is a related project where they are using searches. Do people that see ads do more searches than Google? And that's with a somewhat limited data set. Got it. Thank you. Any other questions? Okay. So moving on to the modeling. In this, we start with a randomized trial, but even so, it's not necessarily perfect randomization, and there are some systematic differences between groups that we're going to compare. I'll talk a little more about that later. But what we do, and this is something that you can do when you have a randomized trial, or it, but you don't have to, or when you have observational data. Have observational data where it's not a randomized trial, then you pretty much have to do something like this, which is compare for differences between the treatment groups and control groups. And the approach that we're using here is basically fitting logistic regression models. For the treatment group and the control group, estimate the probability of answering positively. So the y variable is The Y variable is a zero or a one. Did you answer positively to a survey question? We produce one curve for the treatment group, one curve for the control group, based on various other factors, which would be things like age, gender, how active you are on the internet, any other measures that we have. The difference between these two curves is our estimate of the Is our estimate of the average effect of seeing the ad for people with that for that kind of person, for the person who has that age, gender, internet activity, et cetera? We then average those differences, the averages of the green arrows across groups of people, and that is the estimate of the effect of the cancer. Estimated the effect of the campaign. Now, when we are doing a randomized trial, it's appropriate to average that across everybody. Conversely, when we're doing an observational trial, then we take the average only across the exposed people. Let me make that a little more concrete. In an earlier project, the one that I mentioned where we're using the Internet Explorer data, The Internet Explorer data, we were comparing people who saw ads to observational controls. People who came to the same websites where the ads were shown, and they saw some ads. So we knew they didn't have an ad blocker installed, but they didn't see the ad for the particular campaign. And those two groups were, we tried to make them the same. We tried to make them the same, but they're not the same. In particular, the reason that some people saw the ads and others didn't was that the people who saw the ads gave themselves more chances to see the ad. They did more searches, more browsers. They did more of everything on the internet and hence had greater chance of seeing the ad. So we needed to correct for the differences. And then to estimate the effect of the campaign, we're The effect of the campaign, we're interested in what is the effect on the people who actually saw the ad, not on everybody else who theoretically could have seen the ad. Any questions on this? Tim, how is the x-axis arranged? The x-axis is summation beta j xj. It's whatever linear combination of features best predicts the y variable. So you can think of it as generally further right might be people who are more active on the internet, or further right is people who are more inclined to know this particular brand. Maybe this is an advertising campaign for a new video game, and young males would be more likely to be aware of the game, even without the advertising campaign. So young males would be further right. So, young males would be further right in that case. Other questions? All right. Okay, but we do have some biases. There's solicitation bias. In this, people see an ad, and then some of the people later come back to places where we can. Later, come back to places where we can give surveys. So, if it's a YouTube campaign, some people come back to YouTube, others don't. If it's a more general advertising campaign across the web, where maybe the ads are shown on CNN and the surveys might be given on other sites, some users come back to the sites where we can give the surveys and others don't. Secondly, there's possible response bias. There's possible response bias. People who know the brand might be more likely to respond to survey questions than people who just don't know the brand. So we have biases. How can we correct for these biases? We're using a two-step process. In step one, we work with a subset of the data. We fit the data to people who responded. Responded, and the Y variable is the actual response for those people who responded to the survey. And we use that data then to predict to the group of people who were solicited who came back to a page where we could give the survey, but maybe didn't answer. And for that first model, we only use variability. Model, we only use variables that are available for the solicited people and the respondents. And then we run a second stage model to predict to this larger population people who were not solicited. And we have less data for those people. For just as an example, we don't know how long it was between the time when they saw the ad and the time when they were. And the time when they were solicited for a survey because they were never solicited. So, for this model, there's less variables that we can use, and we are fitting this data now to this middle circle. And the y variable that we use is either the actual y for the people who responded or the Who responded, or the prediction from the first model for the people who were solicited but didn't respond. And then ultimately, we predict the Y hat for everyone and then use that for correcting for biases and for estimating for the larger population. Any questions? Okay. Beyond this, advertisers are interested in overall Lyft. Lyft is the effect of the campaign, the difference between people who didn't see the ad and people who did. Hopefully the Lyft is positive. They're also interested in what part of the campaigns are effective. Is it more effective to reach certain ages and genders? What is the effect of people's Of people seeing the ad more than once. What is the effect of the time between the ad impression and the solicitation? People may tend to forget ads over time, and so we're interested in that decay. And we also are interested in estimating what is the effect, say, 24 hours later as an idea. Okay, maybe it's really not that big a deal if people. People remember the brand for a minute after seeing the advertisement, but if they forget it by 24 hours later, then the effect of the ad wasn't really that good. So we have different methods for doing slices. And one approach you could do is just a raw comparison. If you're interested in the effect on males of this campaign, you just take the average for the males minus. You just take the average for the males minus the average for the male controls, male treated minus male controls. Or there are different model-based approaches that you can do. And I'm not going to spend time on the details between these. You can look for details in the white paper. But I will mention that these approaches have different advantages. In particular, suppose that you have. Suppose that you have a campaign that's maybe more effective for young males, and you want to tease out the effect. What is the male effect versus the young effect? Some of the approaches will confound the two, the male and the young person effect, and other approaches estimate incremental effects. Beyond the effect of being male, what is the effect of being young? And the next issue is advertisers are interested in what is the effect if someone watches an ad for at least 30 seconds. Typically on YouTube, you may see an ad and after a few seconds you can interrupt the ad. And if you interrupt quickly, advertisers typically won't end up paying for Advertisers typically won't end up paying for the ad. They'll only pay if you watch for 30 seconds. So then advertisers are interested in what is the effect of people watching the ad for at least 30 seconds. Well, there's some bias here because why do people watch the ad for that long? Maybe because they're interested in this. So for the people who were in the treatment group, we know which people. Group. We know which people actually watched the ad for 30 seconds. For the people in the control group, we don't know which of them would have watched this ad for 30 seconds had they been shown this ad. So even though it started with a randomized clinical trial, now not clinical trial, a randomized trial, we now have lost the randomization aspect and we do have to compare a treatment group. Have to compare a treatment group of people who saw the ad for 30 seconds to our best estimate of what the corresponding control group would be. And doing that is a little bit tricky. You may be aware of an intent to treat analysis. Basically, if only some fraction of the people watch the ad for at least 30 seconds, you might take You might take the overall average effect and divide it by the fraction of people who watch for that long. Just say it's 50% of people watch that long. And hence get an adjusted estimate, which would be the overall estimate divided by 0.5. And then based on that, you might assume that the standard error for your estimate, the standard error for your delta had a. The standard error for your delta hat adjusted would be the standard error for delta hat divided by this fraction. But that's not correct. We actually have a nonlinear problem here and there are three things that go into this. And I'd like to look at this diagram to understand what's going on. The left bar here is Left bar here is the exposed people and the y bar is the average response for those exposed people. But there are two groups of the exposed. The RE are the really exposed and the PE are the pseudo-exposed. In this case, it would be people that didn't watch the ad for that long. And we can look We can look at the average for the people who were really exposed and the average for the people who were pseudo exposed. And overall, the area of these two rectangles matches the area of this rectangle. Now for the controls, the only thing that we have is the y-bar for the controls. We don't know what the y-bar would have been. Well, what we do is we basically infer that for the controls, that the fraction of people who would have really seen the ad matches the fraction for the people in the treatment group. And for the people that didn't see the ad or wouldn't have seen the ad, that the average response for those people would be the same as this. And based on that, and given that On that, and given that the area of this rectangle, which would be the average for people who really would have seen the ad had they been in the treatment group, but they didn't see the ad, what would be the average response for those people? That's the why bar really controls. That the area under this curve and the area into this curve match the area into this curve, and that height is something that we know. So, based on areas matched. So, based on areas matching, we can compute what the Y bar RC should be. Then the difference, the Y bar adjusted, is the Y bar for the really exposed minus the Y bar R C, which is a nonlinear function of these three terms. And computing the standard error for that is kind of a mess. It's a nonlinear thing. We can do delta methods, but. We can do delta methods, but we can mess that up. So for Stan. Stan has raised her hand if you've got a minute. Yes, go ahead. I have a question. Can you go back to your previous slide? So, what do you describe there as a pretty well-studied scenario in causal inference literature? Color inference literature is the non-compliance to the randomized trials. And what you describe, so you probably know the very famous approach of the Engrants Invent Rubin, the instrumental variable approach, essentially view the randomization itself as instrument. And then they, again, I'm not sure whether you are familiar with that, but it's a, so a lot of things you're talking here is related, but what Here is related, but what they're talking essentially is using like the non-compliance. So the randomization itself is the instrument, and the non-compliance is kind of this, the treatment actually taken. Well, the assigned treatment is randomized. And then the treatment actually taken in your case is watching the video or not, it's the actual treatment, and you have an outcome. So they have, they basically say that you can use. Have they basically said that you can use make some assumptions like monotonicity and you can actually calculate. So they introduce this concept of compliers, never takers. In your case, it's a never watcher or compliance. If you show a person TV, well, the video they will watch. If you don't show them, they would not watch. And the never watcher would be the people who would not watch anyway. And then there would be people who are always watcher, like the people who. Always watcher, like the people who, you know, whether you give them, show them or not, they somehow watch it. So, but there's no defiance, there's no people who do the opposite. So, that's kind of assumption. And on the bad, they show that you can use the two-stage D-square, the standard IV estimator to estimate the treatment effect for the compliers. Quote-quote compliers, basic compliers are the people who arguably that would change their behavior according to this. Behavior according to this intervention, which is giving them the ad so that yeah. Um, let me just interrupt that in this case, we essentially have no useful data that would let us predict who the compliers are. You don't need a you don't need the info, right? I mean, like the simplest IV approach, you don't even need individual, you just you don't need to cover the information. You just Need to cover it information, you just use a moment estimation, you can get something. Of course, that's not the best estimation out there, but you don't necessarily need the covariate information. Covariate information will help you, but that's, yeah. I would just say that like there's no, there are all the covariates you can use. You can still put in that model if you want to build a model to predict the compliance. You never, in reality, you never pinpoint whose individual. Pinpoint who's individually identified who are the compliance, but you do it at the population level, you do it at the modeling level. Yeah, we tried to do modeling, tried to predict who would be the compliers, the people that watch that long, and the models were crap. I'm just saying, I guess you don't need that. We had information that would let us predict who would watch. No, my point is: you don't need to predict that, but you can still have. Predict that you, but you can still have the population level estimate of their effect. You don't need a pinpoint who's full. Okay. Again, I'm just saying that there's a large literature in Colin about this type of problem, just again, in case that you might be interested. But it seems that you have a good handle on what you're doing. So I'll just start the literature out there. Okay, yeah. Okay, yeah. So in the if we had more variable, if we had some useful information, then we could use approaches in the causal modeling literature. Given that we don't, basically, I don't think you can do really anything different than the essentially a method of moments estimate that we have here. And so the standard errors, given that we are doing the two-stage bias correction, that we are sometimes comparing subgroups so that we no longer have a nice randomized trial, that we are slicing by different Asian demographic groups. The estimation all gets fairly complicated, and rather than try to derive Than try to derive analytical standard errors, we bootstrap. One interesting point that I will elaborate on later is that we should bootstrap the observations. Second, for doing confidence intervals, one reasonable approach would be to do, say, a bootstrap percentile interval, but we're not doing But we're not doing that here because we don't do enough bootstrap samples for that to be accurate. So we're basically doing t-intervals using standard errors that we get from the bootstrap. Reason we don't have a lot of bootstrap samples is that the code is complicated and slow. And then the final point on this slide is that so far I've talked about absolute lift. About absolute lift, the difference between the treatment and control group. But what is more standard in the industry is to talk about relative lift, which is the average for the treatment group divided by a baseline, i.e. the control group, minus one. So we're looking at a relative lift, and now we essentially have a nonlinear function, and an appropriate way to And an appropriate way to get standard, to get confidence intervals for this ratio or the ratio minus one, uses an approach by Feeler, which is if you basically create the oval which corresponds to the x-axis being the denominator and the y-axis being the numerator. And you get the covariate information. You get the covariate information for your estimates of those two quantities. Go a certain distance corresponding to the number of standard errors based on the confidence level that you want, and take the tangents of the corresponding oval with lines through the origin. That gives you the endpoints of your confidence interval. So, as your confidence, desired confidence gets larger from Gets larger from 95% to 99%, for example. Your oval becomes bigger, and this top line eventually gets potentially very close to the y-axis corresponding to my dragon. All right, any questions there? All right. A quick question. Is this Nick Feeler that you're signing here? I'm sorry, say again. Who is the filer that you're signing here? Feeler. Feeler was the name of the man who wrote the paper on this approach for confidence intervals for ratio estimates. Intervals for ratio estimates. I think it's feeler. Thank you. Okay. Any other questions? All right. And this is all at Google scale. So we do our analysis on a number of advertising campaigns to figure out what's going on. But ultimately, we automate the whole process from the survey setup to the From the survey setup to the data processing and the statistical modeling. It's a team of statisticians and software engineers. The software engineers write software that processes the logs, pull out some of the data from the logs, combine it with the data from the surveys. We did the statistical analysis in R, produced CSV files that were then used to produce Excel templates for use. Plates for use by our salespeople who work directly with the advertisers and set this up so that we could run thousands of studies in an automated fashion. And then we also had some automated meta-analysis to gain insights of what types of ads tend to appear to be effective. Not surprisingly, for video ads, it's important to have. Ads, it's important to have the brand name fairly early in the video because a lot of people may not watch longer. So moving on to statistical questions. Are there any questions before I move on? Okay. Number one, we're doing regularization. Now, Now, you may be familiar with regularization. Consider least squares estimation, where you choose the betas to minimize the sum of squares, or logistic regression, you maximize the log likelihood. Different approaches to regularization. One is ridge regression, where you minimize the sum of squares plus some constant times the sum of coefficients squared. Trying to prevent coefficients from becoming too large. From becoming too large. And in particular, when you have correlated variables where small changes in the data could cause one coefficient to become very large and one coefficient to become very small, this ends up giving more reliable estimates of coefficients. However, this bridge regression approach can be badly biased. Regression approach can be badly biased. Typically, in regression, you have a small number of variables that are really important, and we would see that by looking at the coefficients of these variables. Incidentally, typically when we do this regularization, you can think of the x's as being scaled before we do it, so that the betas are sort of all on the same scale. So a large coefficient. So, a large coefficient then indicates an important variable. With the ridge regression, where the penalty is the square of the coefficient, that really tries to avoid large coefficients. It shrinks the coefficients roughly, some fraction of the way towards zero. So it shrinks those large, important coefficients a lot. And that has a bad effect on bias and tends to produce poor predictions. Predictions. Another approach is to use an L1 penalty. You minimize the sum of squares plus the constant times sum of absolute values of coefficients. So that's better in terms of not biasing those large coefficients as much. Tends to give better predictions. But it's kind of ugly to deal with. The standard errors become somewhat not mean. Becomes somewhat not meaningful because the sampling distributions are no longer continuous. They have a spike at zero. You have a positive probability of a coefficient ending up being zero. What we're using here is a penalty that looks roughly quadratic in the middle, but then approaches. But then, approach is linear in the tails. So we avoid the bias that's caused by an L2 approach, but we get something that's differentiable. And depending on one disadvantage of this approach is that we now have one coh, not just a single parameter, a lambda that we need to choose, but we have two parameters, a lambda and an eta. And an eta. And this eta basically determines how quickly these penalty curves change from being from the quadratic behavior in the middle to the linear tail approach. Now, incidentally, in the least squares approach, one way to implement ridge regression is by adding dummy coefficients. Sorry, dummy observations. Sorry, dummy observations to your data set with a y value of zero and then x values of one or square root of lambda, depending how you want to look at it. And so when you fit this augmented data set through a standard linear regression solver, you end up with the ridge regression solution. Well, if you use that dummy observation approach, Dummy observation approach with logistic regression, what you end up with is this logo penalty. Any questions? Tim, a quick question. Have you explored any non-parametric regression techniques? We did not here. We wanted something that was somewhat interpretable, and we don't have. And we don't have a huge amount of data here. So, some of the machine learning models that are really great for finding interactions if you've got a huge amount of data would not work very well here. Thank you. Okay. Scaling variables for regularization. Now, the usual approach in regularization is to scale variables by their is to scale variables by their standard deviation. That's not ideal. If you think about dummy variables, 0, 1 variables for factor say, well, some factor levels are rare, some factor levels are more common. And where the variable is rare, whether the factor level is rare, you really don't have a lot of information. And so we don't really. And so we don't really trust the data so much to tell us what the coefficient is. We want to rely more on regularization. But when you divide by the standard error, the dummy variable for that rare factor level is small. You end up dividing that variable by a smaller number. That ends up making coefficients smaller, which gives you less regularization, even though we probably want to regularize those. Probably want to regularize those, that variable more. So, what we do here is we don't scale dummies and 01 variables. And then we scale other variables to match the standard deviation of dummy variables that have the same absolute skewness, basically a measure of how concentrated the variation is in a few observations. So, we're basically scaling things to be like similar. Things to be like similar dummy variables. And the overall effect of this is that we regularize more the variables corresponding to rare factor levels or variables where most of the information is concentrated in a small number of observations. Tim, if I can interrupt the question. Question. I haven't seen that approach to scaling things to match the dummy variables before. Is there literature on this? No, I came up with it and I haven't written it up. It sounds like a very clever idea. Thank you. Thank you. Actually, I think I put it in the white paper, but I haven't published it. Okay. All right. Now, statistical surprise. Statistical surprise, we need to bootstrap observations, not residuals. And well, I got it wrong for a while. So the two basic approaches for bootstrapping in regression problems. One is you bootstrap observations. You resample rows of the data, the y and the x's all together. Well, there's a problem with this. Suppose that you've got a factor that has rare levels. A factor that has rare levels, you could end up with a bootstrap data set that has no instances of that factor level at all. So you can't fit the regression. You can't estimate that coefficient. Or what's maybe worse, you end up with a bootstrap sample with just one or two observations from that factor level. So your software happily gives you an estimate for the coefficient, but it's garbage. So, and this problem has been known for a long time, and Brad Efron knew about it. So, he suggests often bootstrapping residuals. So, you fit a model, say a linear regression model, and then for the bootstrap samples, you keep the same x's, but you let y be random. You let y be the fitted value from the From the original model plus a random residual. And this is a special case of the more general approach of bootstrapping y conditional on x. You keep the x's fixed, and then you resample y from the conditional distribution of y given x. In the logistic regression case, this means that you let y be 0 or 1 with probability. With probabilities corresponding to the prediction from the original model. So, in many regression problems, the residual slash conditional approach is better. It avoids problems with rare factor levels. But what we're doing in this problem, okay, oh, well, before. Okay, well, before we get talk about this problem, if you're bootstrapping for coefficients, then typically you prefer the second approach, the keeping the x's fixed. But in this problem, it's different because the estimates that we produce are of the form delta hat is an average across some set of observations. Set of observations of the difference between the two curves: the predicted value from the treatment model and the predicted value from the control model. And what's important here is that this subset that we're averaging across is random. Maybe it's the people who were actually exposed, or maybe it's the people who were exposed males if we're doing some slicing and dicing. Dicing and dicing. In any case, there's some random set of observations that goes into this average. And if we keep the axis fixed, then we're missing part of the variance in the overall estimate, the part due to the random averaging over a random subset of observations. And so, well, until I realized that, we were bootstrapping. Bootstrapping Y conditional on X. Any questions? Hi, Tim. What about like a Bayesian bootstrap where you just weight everything with exponentials, then the X is shaped around and no data point gets kicked out? Yeah, the Bayesian boot. Yeah, the Bayesian bootstrap would work as well. That would be fine. That would capture the variation in the X's as well. And that has the advantage that you don't have the case where you get no observations at all from a rare factor level. So that might work a little better than just the bootstrapping NATO visions. Any other questions or comments? Okay. Statistical surprise to regression versus propensity for causal modeling. If you're in the case where you have a randomized experiment, the treatment controls are from the same population, the estimates are averages across all observations, typically. Typically, the treatment group and the control group are not that different. And a regression approach versus a propensity approach will get pretty similar answers. In contrast, when you have observational data where the treatment group are from the population of interest, the people who actually saw an ad from a particular campaign, and the controls are people. And the controls are people that we try to find that are similar, but maybe they're not so similar. Maybe they are people who tend to be much less active on the internet. And the estimates that we produce are averages from treatment observations. A propensity approach is we Is we do a model that predicts whether someone is in the treatment group based on various X's that we have. The output of that model is a probability for someone being in the treatment group. And then we weight the observations. If we're in the randomized trial setup, then the weights are 1 over 1 minus P for the control. One over one minus p for the controls, or one over p for the treatment group, or if we're in the observational situation, then the weights for the treatment group are all one, but the weights for the controls are this ratio. Note that in either case, there are denominators that can blow up when the estimates are near one or blow up in this case. Or blow up in this case when the observations are near zero, when the predicted values. And in the earlier problem, where we had observational data and we had fairly different groups between the treatment group and the control group, we observed that some of the weights that we got were huge in the thousands, hundreds of thousands, even. Even. So the weights would blow up, and the weighted averages that we would produce using the propensity approach would blow up. Or there's the regression approach where you fit a model where y is a function of z is were you treated or not. Did you see the odd? Various covariates and interactions between those two. And interactions between those two. And you then produce predictions for the treatment model and the control model. And then you take averages across some set of population of the differences between those two. The interesting thing is that we can interpret the regression estimates as weighted averages. As weighted averages. And this is exactly true for linear regression and approximately true for logistic regression. Think about the simple linear regression case. Beta hat one can be written in this closed form solution, beta hat zero. We can do some algebra to show that when we are producing the prediction at any given x, beta hat zero plus beta hat one x, it's of the form a summation awaits. form a summation of weights times y values where the weights are given here and the key point here is that these weights don't depend on x and let me show that graphically in the top figure on the right we have a scatter plot of points we've got a line linear regression line through the points we can produce the predictions at any value of x well the prediction at this Well, the prediction at this point corresponds to a weighted average of the y values, and the values over on the right get a higher weight, and the weights are actually graphed in the bottom figure. The observations in the right get a high weight for predicting at this value of x, the green value. Observations over here can actually have negative. Here can actually have negative values. And what that means is that if this observation goes up, that makes the slope of the line flatter and that produces a smaller prediction at that green point. So that's what the negative weight means. So we can interpret the regression estimates. Regression estimates as weighted averages of y values, weights not depending on the x. And this now lets us compare the propensity approach, which is a weighted average estimate, with the regression approach, which looks nothing like a weighted average, but in fact is. And it turns out that, well, suppose that we want to find an optimal weighted average, we want to maize the variance. We want to miss the variance of a weighted average subject to some constraints on the x's, and these constraints, constraints on the weights rather. The weights need to sum to one, and the weighted average of x's need to match known values. And this is basically the way that we correct for differences between the treatment control groups by setting these values. The solution is of this form. Solution is of this form, and those turn out to be linear regression weights. So the linear regression weights are the minimum variance weights. And in the causal modeling case, the algebra gets a little complicated, but the basic ideas carry through that the linear, the regression delta estimates. Delta estimates correspond to weighted averages, and the regression approach gives optimal weighted averages, minimum variance weighted averages. Any questions on this? Tim, sorry, I have another question. So you noted about the weighting approach, you basically use two sort of You basically use two sort of weights. One is IPW, the other is ATT weights. So, both are essentially inverse probability. So, it's well known that they are prone to extreme weights. Like, that's why you have those huge weights, right? When you have the extreme propensity close to zero and one. I just want to point out again in contrast literature, there's a lot of recent work actually done, like improved weights, essentially, accommodate, like. Essentially, accommodate, like, deal with that extreme weight issue. For example, I did this overlap weights thing, it's simply one-line change of code. The change the weight form instead of the inverse, it's a weight to the opposite, the propensity to opposite group. And then that's theoretically, like we prove that it minimizes the asymptotic variance of the biting will reduce the variance greatly and deal with the deal with the extreme. Um, deal with the extreme weights, um, situation. So, I again, I just want to point out that maybe it's worthwhile for you to. I read your abstract, I realize you're gonna say probably like preventative is actually score, it's not doing well here. And it's not surprising if you're using inverse weight. But I'm just saying that if you try some different other different weights and a very simple change of code, probably you might get something different. Okay, great. Okay, great, thanks. Yeah. Hi, Tim. There's how much better are the regression weights than like somebody could use entropy or empirical likelihood or like any Cressy-Reed-Renier family, and they would try to keep the weights non-negative. I haven't done any systematic evaluation. Evaluation. What I observed was in the earlier problem where we had observational data and fairly big differences between the two groups, when we did the propensity approach, the weights blew up. And we basically had to truncate the weights to prevent that blow up, but that causes bias. We also tried doubly robust. Doubly robust estimates, which are a combination of propensity and regression approaches, and those blew up as well because of the propensity side of that blowing up. Thanks. Tim, I want to remind you, we have about three minutes left in your session. Okay, all right. We'll be done fairly quickly. This is just a graphical comparison of the Graphical comparison of the regression and propensity weights depending on the problem in the observational case. The purple triangles are the treatment group and the others, the circles are the control group. And the red ones are the linear regression weights and the blue ones are the propensity weights. Weights and the regression weights are linear and they can get negative, but they don't blow up in one extreme the way that the propensity weights do. So to summarize, we're estimating brand effectiveness. We're using surveys. It's a controlled experiment, but it's imperfect. And so we want to use some of the causal modeling estimates anyway. Anyway, we are slicing by different groups, by different demographics, because advertisers are interested in this. Overall, the estimation gets fairly complicated. So we're estimating standard errors by bootstrapping. And we need to bootstrap the observations. We're doing some regularization, which sort of combines advantages of the L2 and the L1 approaches. And we find that regression approaches have That regression approaches have better, lower variance than the simple propensity methods that we compared. And that's it. And there's a link for the paper. Are there any last questions?