So professor, we've got Natasha Morrison next. Natasha's talking about. And this is everything I'm going to talk about this joint work with Joseph Hyde, Alp Miesta, and Matthias Have Sin. So we're talking. Have this soon. So, we're talking about finding, I guess, structures in C to random graphs. So, I'm just going to spend a couple of minutes telling you about the kinds of graphs we're thinking about. So, if you are not used to thinking about pseudo-random graphs, don't worry, I'm going to literally tell you everything you need to know to be happy for the rest of the talk. So, an ND Lambda graph is a graph with N vertices, it's deregular, and the second largest eigenvalue and absolute value of the adjacency matrix is at most lambda. How should you think about these things? How should you think about these things? Well, it's not difficult to show that all the eigenvalues are at most d of the adjacency matrix. So lambda is always at most d. Basically, the smaller lambda gets, the more pseudo-random the graph is. I'm going to say what I mean by pseudo-random in a second. So we're going to think about finding spanning structures in these graphs. So for example, spanning trees. So if D is really big, Is really big, these things are not difficult to find, right? If D is very, very close to N, you can find them you want. So essentially, you're going to care about D being bounded away from N, and in this case, it's not difficult to show that lambda is almost at least, say, root D. So you should think of lambda as being between, say, root D and D, and the closer it gets to root D, the more pseudo-random your graph is. What does this mean? Well, you could think of it as saying that kind of between pairs of sets of vertices. Pairs of sets of vertices, you roughly have the kind of number of edges you would have in a random graph of that density. So the expander mixing lammer, I didn't know why that appeared after that, but it basically says, right, if you have this ND lambda graph and you've got your sets of vertices A and B, it's always going to be the case that the number of edges between these sets, the difference between what you'd expect in a random graph of the same density, is kind of bounded. So you can, I mean. So, you can, I mean, this is not going to be so important for the rest of the talk, but you can think of these as kind of graphs that exhibit many random-like properties. And one particular thing that we get about ND Lambda graphs is that if we have two large enough sets, there's always an edge between them. This follows from the expanded mixing model. So, the kind of question I want to talk about today is: what conditions will allow your ND Lambda block to contain particular spanning subgraphs? Contain particular spanning subgraphs. So, in fact, if you care about optimal conditions, right, what are the best possible bounds on lambda? Very few things are known. So, it's known that if d is at least lambda plus 2, you get a perfect matching. It's known, I mean, so all of these bounds on this slide are basically best possible bounds up to constant. So, it's known, you know, when can you find a perfect matching? It's known when you can. Matching. It's known when you can find a triangle factor, and this very large breakthrough was made earlier this year, and they proved best possible bounds for finding Hamilton cycles. So basically, pseudo-random graphs, but there's been a lot of work on finding thresholds in GNP for some spanning structures. It's a lot harder to answer the same questions in pseudo-macrandom graphs, and a lot less is known. So, what I'm going to talk about is spanning trees. So there's a conjecture of Alan Privilevich and Sudakov from 2009, and they conjectured that for all fixed delta, so imagine delta is some integer, there is some constant that depends on delta, so that if lambda is bounded by d over this constant, then your n d lambda graph will contain every spanning tree with maximum degree of most delta. So this is a very nice conjecture. This is a very nice conjecture. There was a theorem of Han and Yang a couple of years ago that proved some bounds on lambda. So the conjecture says, right, we want, if lambda is at most d over c, and remember this is the bound that was proved for the Hamilton cycle, and that was a lot of work. Han and Yang proved this bound on lambda is at most d over delta is a big O root log n, then you can get every Then you can get every N D, every n-verse extreme with maximum degree at most delta. And we managed to prove a bound that if lambda is at most d over some constant times log cubed n, we can find every spanning tree with maximum degree at most delta. Is it clear what it is I'm trying to prove before I give you some idea about the proof? Are there any questions about anything so far? Really, if you're not familiar with ND Lambda graphs, just think of kind of graphs with random white quantities. It kind of graphs with random white qualities, and everything else is going to be fine. I got mixed up between D and delta. Oh, so delta is going to be the maximum degree of your spanning tree. D is some number. D, your graph is d regular. So every vertex has to be d. So your delta is 2 if you want Hamiltonian, right? Yes, I guess. Yes. Yeah. So I guess when we did this, actually the result of the Hamilton cycle wasn't done. Sorry? Sorry? Delta has to be less than that. Yes. Yes, because the case squared. Okay. Okay, so this is what we prove. How might you want to prove something like this? Let me wave my hands around and give you a very rough outline. So, if we've got an - D Lambda graph and we've got some bound on lambda, and t is a tree with maximum degree at most delta. So, there are various things that are already known. So, in particular, if our tree has lots of leaves, we're happy because Matthias proved a result which says you're done if your tree has lots of leaves. And if your tree doesn't have lots of leaves, then it has a lot of long bare paths, right? A lot of paths with A lot of paths with no other things coming out of it. So, in fact, these previous things basically tell us that the only trees we need to think about are trees with at least, say, n over log cubed n map paths, each of length kind of log cubed n. So, we're only going to think about trees with these properties, because in any other case, we're done. Okay, so we've got. Can I interrupt this condition? So, this is only so n must be small then in terms of d, right? Terms of D, right? Oh, N is very big. But then log N is much bigger than D, so lambda would be pretty small, which is probably very realistic, right? So D can be something in terms of that, if you... No, but lambda is even for Romanogen graphs, lambda is like a round square. like around square root d. So it's oh yeah, lambda can be square root d. But if you if log log cube then is big then this is much smaller than square root d, right? Yes, so I guess it's saying right, your graph has to be very pseudo-randal. But the smaller lambda is, the more pseudo-randal the graph is. Okay, no well I think that Okay, no well I think that that n cannot be too large, but still. Anyway, still good, yeah. And n is just very big. D has to be big. But n has such a graph that it is so too. But D has to be at least long. D can be big too. Yeah. N is very big and D can be big. Pan is very big indeed, can be big. So we only need to find trees with at least at least say n over log cubed n by paths each of length loged n. These are the kinds of graphs the trees we're going to try and find. So how might you try and find these things? Well, if you take one of these trees and we get rid of some of these bare parts, what we end up with is What we end up with is, say, a forest on some, right, say a linear-sized forest with max degree delta. And I'm just going to wave my hand and tell you previous work says that we can easily embed this in our ND Bamboo graph. This is not the interesting part of the proof. What is the path? It's a path where the internal vertices don't have any things coming out of them. So like you've got a tree and you've just got a path, everything inside has to be two. Side has to be two. So, what I've done is I'm going to take, I guess, right, the purple bits of my bare paths. I'm going to get rid of them from my tree. What I've got left is, say, forest, let's say some constant times n size forest. It's very easy to embed this using standard techniques in our ND Lambda graph in a way that the end points of these paths are in some kind of randomly chosen sets. I'm not telling you how to do this. And sets. I'm not telling you how to do this. You can believe me that that's not a new idea that we had, it's just standard things. So, what we have is we've got our forest, the end points are in some randomly chosen sets, and we want to stick our forest together with paths of the right lengths. Okay, how might you want to stick your forest together with paths of the right length? Well, the first idea you might have is initially at the very start of your proof, let's set aside these sets. These sets, a bunch of random sets, each of the right size, and in fact, yeah, it should say, of course. At the very start, we're going to set aside a bunch of sets of the right size, and because we've chosen them randomly, there will be perfect matchings between pairs of these sets. So, in fact, we can find a path factor of these random sets. We can find exactly the right number of them that we want. We can also embed our forest inside the rest of. Inside the rest of the graph, such that the end points are in these sets x and y. X and Y randomly chosen standard techniques allow us to put the rest of the forest so that the end points of the paths are in my set X and Y. So I've said that I can embed the forest and the endpoints are in X and Y. I can join up X and Y with paths very easily. Why am I not done? How am I cheating? The paths are the right length, I've chosen the right number of sets. The right length I've chosen the right number of sets. You don't know how to have joined. Precisely, right? So I've told you I can join X and Y with a collection of paths. I haven't ensured... So in my forest, I need to make sure that this vertex X gets joined to this vertex Y. All I've done is found perfect matchings between each pair of sets. So in fact, it could be the case that this X gets joined to this Y prime, and I'm not going to get my spanning 3 that I want. So this doesn't work. This doesn't work, right? You're we're not very happy. We want to be able to ensure that the paths join precisely the right pairs of vertices. Is that clear at this point? Okay. Why are the paths the right length? I'm confused about that. So I've chosen my sets to be exactly the right size. I've chosen exactly the right number of sets. So I've picked C is some constant. I've picked exactly the right number of sets. Are the bows all the same way? Yes. The paths all the same length? Yes. Yeah. They're all I basically chopped my tree into these paths of exactly the same length, and I can put them down exactly between the sets of things. There's only one red thing along each connection, but there could be more. There could be more. Red. Oh, yeah, there's only two parts here. Yeah, it's, yeah. I mean, there's loads and loads. I mean, there's loads and loads of them, right? There's like n over log cubed n of these paths. It's hard to draw n over log cubed n paths in my tree. Basically, I've got loads of paths, I've got loads of endpoints, and I want to stick them together the right way. And what I'm trying to do here doesn't work. Okay, so let me tell you the key idea in our proof, the kind of our new idea, right? Everything else was kind of other people's ideas, how you embed things. So our idea basically allows us to stick things together. us to stick things together in exactly the way we want. So what is the main lemma that we prove? So if we have an ND lambda graph and lambda is at most d over c, so a and b are going to be disjoint sets of vertices that are exactly the same size and I've written typical. If you chose them randomly, they they would have the properties you need for this. Um typical sort of encodes some conditions you need to be able to do the rest of the embedding. Don't worry about that. To do the rest of the embedding, don't worry about that. So, what does it say? We've got our sets A and B, then there is a subgraph S inside my pseudo-random graph that for whatever bijection I have between A and B, I can make a path factor of this subgraph that joins exactly what I want in A to exactly what I want in B. So, right, I've got my sets A and B, they're fixed at the start. You give me your favourite bijection. Oh, no, sorry, the center face at the start. Well, no, sorry. The center face at the start, I find my subgraph S. You give me your favourite bijection between A and B, and no matter what your bijection is, I can cover every vertex in S in a way that I'm making paths between A and B. So I've covered up, every single vertex of S is used, and I've joined precisely the right end points I want. So S doesn't depend at all on the bijection. There has been some previous work where people have proved something similar where the bijection was fixed and they've Similar where the bijection was fixed, and they've said they could find this S. But the power of this is that you don't need to know what the bijection is in advance. And if you're familiar with the way that people sometimes find spanning structures in graphs, sometimes what they do at the very start of the proof is they find an absorbing structure, and they use this to sort of eat up the rest of the, right? They do some stuff, and they use the absorbing structure to eat up the spare vertices at the end. We don't need to use any absorption because our graph basically, right? What we do is we set aside. Basically, what we do is we set aside this thing at the very start, we use the kind of standard stuff to embed everything else. This has exactly the right number of vertices that we can use to join our tree up at the end. So, is it clear what I want to prove? Are there any questions about what the lemma says? Do you know about the size of S or how long these paths are? Yes. So, each path is gonna be like log squared, log cubed n e no log cubed n, sorry. No, log cubed. N, sorry. They're going to be logcubed and I'll. Well, yeah, because these are the pods are exactly the ones we want to join up with log cubed down. I'm going to talk a bit about that at the end because that's actually where we, that's why we have our log cubed N in our band. If we could improve this, that would improve our bound. Can you kind of choose the L yourself of this order of votes? The L is naturally by the T. The LSM actually can find it to you. So for our pseudo-random graph, well, when I mean, we can't make them shorter. If we could make them shorter, we would have a stronger result. So we've done this as, like, if, yeah, I mean, we can't do it with less than log q d. You could make them longer if you wanted to make them longer. But it all sort of feeds into if you want to get the best possible bound on lambda in our theorem. On lambda in our theorem. You kind of want, like, if we could make this log squared, then we'd lose a log later. So, okay, so I'm going to talk about how we would prove this. So, there are kind of, I guess, okay, two things that you might think. Firstly, right, do we is there even some graph S that has the properties you want? Well, if you think about it for a second, you'll say yes, it's easy. What we do is we just take a complete bipartite graph. Take a complete bipartite graph, and then we make it out of the path. For any bijection between A and B, there is going to be a well, it won't be a path factor, actually. Because I'm not using every edge. Okay, so maybe it's not as completely obvious that there even exists a graph with the required properties. Why do you need to use every x? You need to use every vertex. Oh, I don't need to use every vertex. I'm just using my mind. So, yes, I need to use every vertex. That was a trope question, just to check with you all this. I mean, I get... I'm coming to the title, the sorting network, right? Yes, I'm coming to the sorting networks. Okay, so it's very easy to find a graph S that does what you want to make your path factors, fine. The problem is that we can't find this kind of thing in a pseudo-random graph because pseudo-random graphs with the parameters we're talking about. Graphs with the parameters we're talking about, the shortest cycles are like length or getth. So, if my complete bicarte graph has all these C4s, I can't possibly find a C4 in my graph, so I couldn't possibly hope to find this in my Cedaranthem graph. So, I need to find a structure that I can find in my C. Juran graph. So, essentially, I want a very sparse structure with girths, say, called M. Okay, so how am I going to do this? Well, I'm going to tell you what a sorting network is. I'm going to then tell you how we use. Workers. I'm going to then tell you how we use the sorting network to make a graph which gives us the required path factor, and then I'm going to tell you why we can make that have large graphs. So, a parallel sorting network is actually a really cool thing that I didn't know about until last year. So, if you haven't seen it before, this is quite fun. So, it's a pair R C where R is a set of registers. And each register you can think of holding some value, which is, say, a number from 1 up to n. And each one, say, has a distinct. So, imagine there's sort of a biologist. With distinct. So imagine there's sort of a bijection at the start between one up to n in one registers. A comparator is just a pair of registers, and it acts on these registers where it'll swap the values whenever they're kind of in the wrong order. So if the value in i is bigger than j, but i is actually less than j, it's going to swap. I'm going to draw a picture in a second and this phrases will become clearer. But basically, the comparator sorts a parameter. Sorts a pair of values. We have a collection of sets of comparators on disjoint registers, and that will be C, which is C1 up to C L. And for any bijection, essentially, our collection of comparators will sort my initial bijection. So I realize there's a lot of technical language here. I'm going to just do an example which should make it clear. So, what do we Which should make it clear. So, what do we have? And this depth, which is a sort of technical term we'll talk about in a second, is just the number of sets of comparators. So, here's a picture that will make everything clear. So, we've got some registers, R1 up to R4. We've got four sets of comparators, so this sorting network has depth 4. What does my comparator do? It looks at a pair of registers, so that my pink lines are these pairs of registers that my comparators are talking about. And how does this network work? So we pick a bijection from our registers to 1 up to n. And this network, for whatever order I put these numbers, 1, 2, 3, 4, if I go through and apply these rules, it's going to sort them out the right way. So in this particular example, what happens? Well, 4 and 2, 4 is bigger than 2, so they're the wrong way around, so this comparative 1 will swap them. 3 and 1 will travel here, and they'll also get swapped. All these numbers will travel along. Numbers will travel along, and when they're in the wrong order, they'll get swapped. But now we come here, two and three, they're in the right order, so they don't get swapped, and everything travels to the end. One, two, three, four. So the point is, if a bigger number, like if you're comparing, say, a higher register with a lower register and the bigger number is higher, the comparator is just going to swap them. And the point is, the cool thing about this is that for any bijection that I put at the start, my sorting network is going to sort these things. These things. So the example said 4321, I could have started with any ordering of 4321 and it would have sorted it. So this is very nice because it's sort of saying that it's giving a thing that kind of for any bijection we can somehow find a way to go from one side to the other side. So how is this going to, how are we going to use this to make a graph that we want to find in our pseudorandom graph? Well there's a very nice theorem which says that for all n there is a parallel sort of graph For all n, there is a parallel sorting network with n registers and depth at most some constant log n. And that's basically best possible. It's nice if you think a little bit about it, you can see why you need at least log n. You can have n registers. And so what we want to do is turn this into a graph. So the first naive thing you could try and do is I've just put some vertices at the end, and for each comparator, I'm going to replace this with a kind of switching gadget. So I've added Switching gadget. So I've added things two vertices and two edges here. So you could imagine that now, if I wanted to connect my one side up to some other side via the bijection, if you're going through, say, one particular comparator, then if they swap, you take this edge and this edge, and if we don't swap, we just go along. But in both cases, we use all the vertices. So actually, the way that the sorting network sorts tells you how to move through the comparators and gives a You how to move through the comparators and gives a path factor. So, here's an example of when we did 4, 3, 2, 1. This would be the bijection that takes it to, say, 1, 2, 3, 4, and this would be the path factor. So, we do get a path factor, but the problem is with this particular kind of gadget, we get short cycles because I've made a lot of C4. Right? So, what we want to do is we want to actually replace, instead of using this. Instead of using this gadget, which I've kind of put here for the purpose of illustration of how this is going to work, we want to put something here that doesn't have short cycles, but has exactly the same properties in that you've kind of got two ways to go through it. And both ways, like one way you're going to connect up sort of this with this and this with this, the other way you're going to connect this with this and this with this. There are no short cycles, and you use up every vertex at each time. You use up every vertex at each time because then we can just replace our little gadget with this other gadget and then we'll get our whole graph. So, how do we do that? Well, we just make some gadgets. Well, I say we just, it wasn't totally trivial. We make some gadgets with large girth, and let me explain. I'll tell you what we do, and then I'll give you an example. So, essentially, what we want is a switching gadget which has high girth. So, what we actually can do is for each k, we can Each k. We can make a gadget gk which has girth at least 2k. And this condition here basically says, I guess I haven't written. The point is here that P1, say, connects the top line together, P2 connects the bottom line, Q1 is the path connecting the top left to the bottom right, and Q2 connects the other, the sort of these paths do the crossing thing. And these paths don't have, they're not too. Don't have, they're not too long, and this we can then just plug in this gadget to make our overall graph. So the point is that once we use the lemma about the existence of sorting networks of a certain depth, we can plug these gadgets in where k is going to be like log n. This is going to give us this graph S that we'll be able to find in a pseudo-random graph. Because actually, our graph S will end up. Because actually, our graph s will end up having maximum degree three, and basically, it will be able to have been built up just by kind of adding paths of length log n in kind of a way, one by one by one. And I mean, because it has maximum degree three, and basically you've got a bunch of long paths, it's a very sparse structure. You can use standard techniques to find it in your pseudo-ramp graph. So, what do these gadgets look like? So, this is when k is 6. What we start is with our Six. What we start is with a Hamilton cycle. So my blue thing is a Hamilton cycle of length 12. But then what we do, so that's where the girth of the gadget comes, right? I say it's got guth, I mean it's exactly 2k. We start with a Hamilton cycle of length 2k, which are these blue edges. We then add some paths, long paths, to pairs of vertices in this Hamilton cycle. So because the path has length 2k, it's not going to make the girth shorter because the shorter cycle. Make the girth shorter because the shortest cycle is this just blue Hamilton cycle, which isn't a Hamilton cycle anymore because I've added more vertices, but before I added more vertices. So essentially, this is my graph. Why does this do what I want? Well, you can see that actually the top path and the bottom path cover every single vertex. And if you look at the purple path and if you look at the blue path, they also connect kind of the cross pairs and they also. The cross pairs, and they also use every single vertex in this graph. So, this graph has a large girth and it connects everything in exactly the way we want. So, essentially, what we want to do is use this inside the sorting network to provide our graph S, which we can easily find in our C-derand graph. Yeah, so this is KS6 in general. I mean, it's a kind of generalization. Okay. So, I just put the lemma again here because. So I just put the lemma again here because this is essentially right. We've shown I've explained to you how we find this graph S, and because of the properties of S, it has very large girth and it has maximum degree 3. It's not difficult to find it in our pseudo random graph. So our whole proof, how does it work? Well, we start out, we set aside some random sets, we find S at the very start, we then put the rest of the forest without the bare hearts in everything except for S, and then we use this fact that we can. And then we use this fact that we can connect using any bijection just to stick our clothes together at the end. So, this, I guess, two remarks and then I'm done. This lemma was actually used. So, I said earlier there was this breakthrough result of six authors who proved that if land was at most D over C, you could find a Hamilton cycle. They used this. So, what they did was they found some linear forest, they did some clever posio rotations of their linear forest. Clever like posio rotations of their living forest to make sure they don't have to reduce the number of paths in their forest, and then they use this to kind of stick their paths together. So that's very nice, I guess, because this had another application to find expanded structures, and hopefully it would have other applications. So because all we require for this lambda is lambda most d over c, our theorem requires the log cubed n because Because, I mean, if you sort of go through it, the thing that we construct, we have to have our bare paths of length at least log cubed n, because we can't construct anything better. And that sort of feeds through, and that means we can't improve the bound on lambda in our whole theorem. If you could improve this, where your paths here, I guess, right, your sorting network has log n registers, not log n comparators. Comparators. So you're going to always have kind of a log n coming from here, and I don't think your gadgets are going to have to have Girth at least log n. So, really, doing this, you're not going to be able to improve, say, log key dead to anything better than log squared. So, in the Hamilton cycles, because then they end up, so they have So they they have bologna forest. They do posi rotation, so they end up with only like so they don't use the sort of molecular. They do to stick their paths together. So they have ten minutes for the after the no so they have a lot of paths. They rotate in a way so that they're down to something like n into the I want to say 0.8 parts. So they've got a lot of paths which are like there's not Pods, which are like there's not, they don't have many paths in there, they just have to stick them together. It's a Hamilton cycle, so it doesn't matter how long they put their things here. Like, it's all they're doing is just sticking together a bunch of parts, and it doesn't, I mean, okay, I don't think I've explained that very well, but they it doesn't matter to them that we that these things are log cube down, they that that doesn't hurt them at all. Our trees. Tool. Our tree, right? Our Hamilton cycle is just a bunch of paths stuck up together in any way you like. We care somehow how you stick them together, and we need the lengths to be exactly right in our tree at various points. Why do they not have a loss in the eigenvalue entity? Because os it comes from how this is applied. It doesn't come in from the proof of this lemma. So they just apply this lemma and they. They make some paths, they have some sets, their sets are size A and B are like n to the 0.8 or something. And then they apply this lemma and they stick together their paths. And I mean, this lemma doesn't require that bound on lemma. Yeah, but for you, where does it come from? That lambda has to be CI. In order for this to work for us, we need the For us, we need the blur paths in our tree to have length at least log cubed n. So that's already, if we go back to like where did we I don't want to come the way back it's going to take a long time for the three switch to have these bare parts yes these it's these results here somehow it's if you work through the numbers here right at this point at this point we need this number to be log q down so if you work To be log Q down. So if you work, we can't do this number is smaller. So if you sort of work backwards, we can't do it with less, which means that this is exactly... I'm not explaining that very well. But somehow it's essentially if you try to work out, like in your sorting network, you need the bare path to be at least this long. That means it says something about how many bed paths, right? You need this to be at least long q to n. This to be at least log cubed n, which goes here, which says something about the lambda that leaves. And if you go through, you'll get the bound on lambda to be this thing with a log cubed n. But it's essentially the reduction. Yeah, it's that we need the paths to be long, because otherwise our tree wouldn't be able to find it. Whereas for the Hamilton cycle, it it doesn't matter. They can find the cycle if the paths are long. There's a clarification question: you have a corollary that if your tree has the assumption that you have bar paths, then That you have bar parts, then your bound on lambda would have been the oversee it. Okay, if you had a, yes, if you had a, yes, the whole proof would work if you gave me a tree which had this property and lambda was at most years C, the whole proof would work. Only for some specific trees you need your extra assumption. Yes. Path then for DOC. Sorry? Doesn't what you stated just handle amendron in path for DOC? Or lambda less than DOC? But maybe I'm lying. I find it really hard to do calculations in my head when I'm standing in front of lots of people. Or even say something coherent when you ask me questions. So you should see me add up three and four to my linear algebra. Three and four to my linear algebra students get seven, and them politely not say anything for a really long time will look really confused. Yes. Can I torture you with one more question? Yes. I see where you need log squared because you have log n register and probably your gadgets log n. Yes, so the paths and the gadgets, basically the paths are log squared n. So we've got log n comparator things in each thing we're having log squared n and the path that. So that's the log cubed. That. So that's a logic. But could you hope to construct gadgets of size login that possibly? I don't. Alp thinks that maybe it's possible. I mean, yes, plausibly you could hope to get better gadgets. But I guess the thing is, you're never going to get all the way to you're never going to get rid of the logins entirely. So if you wanted to get the best possible balance, the way to do it. I mean, this is a, I think it's very nice and interesting just by itself. Just by itself. But yeah, so yes, you're right. If you can make better gadgets, you could shape off a little bit some power. Yeah, I'm done. I won't move all my slides again to the end, but I'm happy to stop. I'm aware that we're nothing at lunchtime, so maybe questions is detached. You can just give another round of applause.