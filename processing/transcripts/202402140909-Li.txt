Federated and in addition, new new topics later. Thanks for the introduction. So I decided to change the topic a little bit, which is the work that's more related to privacy. And I want to talk about our ongoing work, which we haven't accepted yet, about GetO Federated Linear Learning with certified data review. Learning with certified data removal. So, this topic may be more related to what Professor Shari will talk about. First, let's start with some background, the background on machine on learning. I want to do a quick survey and wonder how many of you have heard about machine on learning, the definition of machine on learning. And not so many. I'm happy to introduce more about it. So, the motivation I'll talk about. So, the motivation to talk about machine unlearning is highly related to data privacy. Dr. Xiao Li Mo also mentioned that nowadays we care about the right to be forgotten. For example, GDPR empowers the right to be forgotten, allowing that the users to remove their data. So, this kind of removal not only regards the removing your data from the hard disk. If you think about you contribute some data for machine learning. Contribute some data for machine learning training. You also hope your impact to the machine learning model can be removed. So, when we talk about data removal for machine learning model, a naive solution is that I can claim, please remove my data and then retraining the machine learning model using the retaining data. In this case, the new model definitely contains no information of my data. So, however, retraining. So, however, retraining the model using the remaining data can be very expensive when you consider large models such as large ramps model or when you have lots of data sets, lots of data samples in your data sets. And sometimes infeasible, such as federated learning, which are another concept I'll talk about later. So, what is machine unlearning? Machine unlearning is a more efficient way that people recently proposed that can help. Proposed that can help us to remove the data's impact in a machine learning model without retraining. So, here user diagram that New RIPS 2023 machine on learning challenge used. So, here we basically think about a setup that given a pre-trained model using some training data set here, and then you define some data point to be removable, which we name it as the forget set. The forget set. Then you design an unlearning algorithm given the pre-intrained model and the forget set. You do some kind of like magic resulting in an unlearned model. So you hope that an idea, unlearned model, it can produce some similar results as you just remove this forget set from your training data and then retrain the model from the scratch. From the scratch. So, this if you have an ideal unlearning algorithm, this unlearned model should be indistinguishable from your retrained model from the remaining data sets. So, that's the basic idea of doing machine learning without retraining. So, but today I want to talk about unlearning in federated learning, which is another concept. I want to do another peak survey. How many of you have heard about federated learning? Have heard about federated learning? It seems like it's a more popular concept, so I will go through that one quicker. The reason that I care about federated learning because most of our data sets are coming from healthcare domain. So those data are mostly disputed in the real applications, and data sharing can be a challenge. Federated learning offers a solution that people from the different centers can collaboratively train a machine. Can collaboratively train a machine learning model without data sharing. For compare with the other training strategy, for example, individual training or centralized training, but rigid learning can definitely leverage more data so that you may be able to get a more generalizable machine learning model. However, instead of like we share the data to a data center and train a model, which may make data privacy issue and also you require data sharing, in federating In federated learning, we require each client, and the client refers to the data owners, to share their locally trained model to a central server. And this central server aggregates the local model to a global model. Typically, this can be done through four steps. Since most of people know about the radio learning, I'll quickly go through. So we start with a random initialization, and then the local clients, they update their local model for several interviews. Local model for several iterations and upload their local model to the global to the server. As you can see, since different clients they have different data, so after local training, their model can be different. But at the global model side, we do the average so we have a unified model which is further broadcast to the local clients. We typically repeat step two to step four for many iterations until it converges. So, this is a basic operation in Fedora. This is a basic operation in federated learning using Fed average strategy. Now, let's talk about unlearning in federated learning. So, recently, we did a survey about federated unlearning. We collected a batch of paper in this GitHub repo. So, what we have observed is that most of the paper on the unlearning propose heuristic strategies. So, there is no certified proof that. Proof that the data can be indeed removed. So, even further from we talk about differential privacy to protect data information. Give you more concept about what is unlearning, particularly in federated learning set. So, this unlearning procedure can be initiated by the server. For example, when you train the federated model, the server may notice that there are some possibilities. There's some possibility that the clients contribute some noisy data or malicious inputs to our global model. So, after the model is trained, we want to unlearn those malicious clients to get a better model. This unlearning also can be initiated by the client. For example, the client decided to quit the federated learning system and then they are entitled to the right to be forgotten and deleting their data. Be forgotten and deleting their data and also deleting their data's impact from the model. And we also have different levels of unlearning when we talk about rated learning, including the class level unlearning. For example, all the clients they require to drop the data points belonging to a certain class. Or we can do the sample level on learning. So for each client, you define a subset of data as the get set, and those need to be removed. Or we can do the curve. Or we can do the client-level unlearning, which I think is more common to see that maybe one of the centers who was originally in the federated learning decided to quit and also want to remove their data impact. This quite often happened, particularly when you use the healthcare applications, as an example, maybe due to some recent legal regulation, you realize the data cannot be used for machine learning training anymore. So today I want to talk about our strategy of forgettable federated linear learning with certified removal, which is a certified removal strategy on the federated learning field. So our work was based on one previous work, which is called mixed linear forgetting. Let me introduce a notation first. Let use D be the training set and the subset DF be the setting. And the subset DF be the forgotten set. That's the data set that you want to remove from your machine learning model. And the remaining set, we call it a retaining set, which is the complement of your forgotten set in the training dataset. And on learning procedure, starts from a pre-trained weight, W hat, and output, a new set of weights, W minus. So an important example of the So, an important example of data removal under the centralized setting is a linear regression problem with a quadratic loss function as defined here. So, if this is a convex optimization problem, the solution can be very simple if you decide to remove the impact of DF. You basically can achieve the remove the model weights by performing the gradient asset. So, this is a hash. So, this is the hash, and this is the gradient on the weakening set. So, when we come to the deep neural network, this mixed linear forgetting paper basically extends this linear regression case to neural network by utilizing the first-order TALA expansion to approximate the neural network. The idea is super simple. So, following the similar idea, we try to linearize the deep neural networking by radio learning. In fact, regularly. The motivation is that data removal is easy under the linear function with a quadratic loss. So we can use gradient ascent as removal. But if you think about this is a long linear function and you use stochastic gradient descent as training, so it's not easy to remove the impact of data because for SGD, you random sample data points and then you always update the gradient based on the last iteration. Last iteration. So, from a high level, we propose to use a linear approximation of a deep neural network given by the first-order Taylor expansion, which we can see as follows. But W star P is some initial weight, and we also assume that W actually is close to W star P. So, given this first-order Keller expansion, then we can define the linear. Then we can define the linear removal as follows, very similar to the linear regression case. However, there are two challenges associated with using this strategy in federation. The first one is that how to find the good initial value, W star P, and the second challenge is how to calculate this hash and matrix over the decentralized or disputed data in that rated learning set. So, regarding the two questions, we propose solutions. So, for challenge one, how to find the good initialization, we propose this federated linear training, which basically contains two steps. The first step is pre-training, and the second step is find a linearized regime and then perform the federated linear training. Again, our focus is based on the client level on learning. So, in this case, So, in this case, one client may claim that I want to withdraw my data from the federated learning system. I will provide more details about these two steps. The first step, pre-training, is to find the good initial weights. Let's assume that we have a domain client which is served as server in federated learning. So, we first pre-train a deep neural network on its data sets. And then, just through the empirical risk minimization, we get this W star P. We get this W P. And noting here, we use the basic cross entropy laws if we are performing classification tasks and use SGD for optimization to get W star P numerically. And note that this data set also can be some public data that's similar to the domain of your training data. And for step two, we perform the linearization and we federated linear training. Course, federated linear training. If we assume that we have a reasonable amount of data on the server, and now we believe that in the previous step, the W star P is close to the optimal model weight based on the full dataset D. Therefore, we can perform the first-order teleextension of the model at W star P and get the approximated linear model for our deep neural network. And this approximation is equivalent to a curved. Is equivalent to a kernel predictor with the kernel known as neurotangent kernel, which we show as here, as the network with infinite number of neurons, which is wide enough. But actually, people also show that if your data is well structured, you don't need to have a super wide neural network to do the linear approximation. Once we linearize the model, and then we can use the traditional federated The traditional federated learning fashion with MSE loss to update the model with the remaining federated data. And here we add an additional variabrization term for this linear training. Theoretically, we show that by using a good initialization plus MSE loss, we can converge with the federal average algorithm. And the basic here, WR means the federality learning model after the R f iteration. Learning model after the R v iteration and W star is the ground truth optimal. And as you can see here, and this is the initial weight, and M is the local step, and beta is the beta smooth, and the mu is the mu convexity assumption. And after we well train the model, so let's think about how can we make this federated model removable. Given the scenario that the client C requests to The client C request to withdraw the data after the federated training we introduced earlier. Using the gradient assignment strategy, we need to calculate this data W. If we follow the traditional recipe, we require to calculate a reverse hash and left multiple with the ingredients. However, when we talk about complete neural network, calculating this hash can be very expensive. This hash can be very expensive. And also, if you want to calculate this in a federated learning setting, you require all the clients to share their hash and displace additional data leakage. So in this step, for the federated learning removal, we try to propose a more computational efficient and the less data leakage solution that we only require server and the data to be required. The clients require removals data only. So, how can we do that? Following the mixed linear forgetting paper, we basically can write the model after removal as the original model weights plus the starter. And noting that D minus is the remaining data set, and DC is the data set from the client that requires to remove. remove so to retrieve to achieve the removal as i mentioned if we calculate hash it's usually very uh computational uh expensive and also if we rely on the data remaining to calculate this hashing we face some data leakage risk so to do so we basically leverage another work saying that this hash can be approximate Can be approximated by finding the minimizer of this Jacobian vector product. So basically, if we find the minimizer of this new function, we will get the approximation of Hessian. So by taking this as true, another strategy we use is replacing the remaining data sets with our server data sets. And then our problem can be reconsidered as. Reconsidered as finding the data w which minimize this Jacobian vector product, and then we replace w minus the remaining data set with the server data. And then we can easily calculate it based on the clients to be removed and the server data as follows. Also, we theoretically show that through this removal strategy, the difference between our removed model weights and the model weights after And the model weight after retraining will be very similar, actually, bounded by those terms. And also, as you can see, for the third term, that we have this lambda, and the lambda is the smallest eigenvalue of the neural tangent kernel what we use for linear approximation. Okay, let's look at some empirical results of our study. We use several metrics to quantify the results. The first one is on learning accuracy, which is Is on learning accuracy, which is defined as the classification accuracy that's evaluated on the forgotten training set. We hope that our removal will not affect the training accuracy that much. And we hope that our removal will result in very bad accuracy on the forgotten training set. And our second metric is remaining accuracy, which we aim to evaluate the fidelity of machine unlock. And the fidelity of machine unlearning that we evaluate the unlearned model on the remaining data set, and we want the remaining accuracy as high as possible. And we also evaluate the testing accuracy on the unlearned model, and we hope that the unlearning will not hurt the model utility. So, the higher the testing accuracy, the better. And additionally, we use some empirical strategy to evaluate this. This success of our learning. The first empirical strategy is to use membership inference attack success rate. The membership inference attack basically offers the empirical approach that says whether the data belongs to the training data set or not. So the forgotten set should not be considered as the member of the training set. A good unlearning strategy should result in a tech success rate that closely resembles That closely resembles the retraining setting. Another empirical strategy that we use is to evaluate the backdoor attack defense performance. We intentionally inject some backdoor triggers in our data set. For example, here, this is a normal stop sign. When we talk about backdoor triggers, for example, we add some patches on the original image, and then their label will be flipped when we send them into the neural network. Into the neural network. So, no matter what the sign looks like, once your neural network detects those kinds of like triggers, it will output some wrong labels. So, that's the backdoor data points. And we perform that unlearning on the backdoor samples, and we hope that a good unlearning strategy can result in the unattacked model at the end. So, for evaluation, we did a simple experiment on multilayer perception with MLIS and the fashion. With MNIXT and fashion MNIXT, and we focus on the client-level removal strategies. So, first, we show that the unlearning with different number of clients, because this indeed matters. So, as you can see, we change the number of clients from 10 to 30, and then we report the results on both data sets and compare our methods. Here, we use that removal with original training model and the retraining from the scratch. So, our goal is that. So, our goal is that we hope the behavior is as close as retraining as possible. So, if we only look at the second and third bar in those bar approach, actually in most cases, we achieve very similar results. And regarding unlearning for backdoor, so we compare with some baseline unlearning strategies, such as fine-tuning-based forgetting or stochastic gradient assent-based unlearning. So, in this case, Learning. So, in this case, we hope that the backdoor success rate is as low as possible. And as you can see, using our strategy, we achieved the lowest backdoor success rate. Since I'm running out of time, quickly to the conclusion, so in this work, we propose a novel federated framework that ensuring certified data removal. And we also provide the theoretical guarantee by bounding the weight difference between our methods. Difference between our methods and the retraining from the scratch, we can achieve a good balancing on data, on model accuracy, and data removal while outperforming the baseline methods on defending backdoor. Some ongoing and future efforts, we want to see the scaling up to more complex neural network and data sets. And also, here, although we call it as certified data removal, we haven't considered the privacy part of this work, which is very essential. We hope to add the differential privacy. We hope to add the differential privacy in our pipeline and then further preserve data privacy. That's pretty much my talk. I want to say thanks to my students because this is a joint work of my students. And also, I want to show my acknowledgement to my funding agency. Thank you so much, Xiao Xiao. We're a bit behind schedule, so we have around five minutes for questions. For questions to give our sleeper time for your questions. So, are there any questions from the audience? Matthias? I'm wondering your bound for the unlearning or even the approach for linear unlearning. Does it work for over-parameterized models? Or is your maybe lambda needing more bound being zero and affecting GPS? Yeah, there are two questions. There are two questions. Matthew Persa asked whether this linearization works for over-premised neural networks. I have to say, this linearization particularly works pretty well for over-premised neural networks. So in this case... So not the learning theory results, but the unlearning results. The unlearning results, gotcha. Regarding the unlearning results, at least the fear for now.