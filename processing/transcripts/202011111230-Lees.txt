Session five. It's a delight to introduce John Lees, who will talk about hardware accelerated genome sketching enables real-time genomic epidemiology of pathogens. John, excited for your talk. Hi, so yeah, John, I'm at Imperial College in the Department of Infectious Disease Epidemiology. Infectious disease epidemiology. I'm going to tell you about some new or some extensions to some software packages we've been working on over the past year or so. So by way of introduction, I have kind of two parts, the introduction of this. So first of all, some motivating genetics and then some motivating informatics. So thinking about large bacterial populations, many but not all bacterial pathogens have what we can call Have what we can call, there's lots of different names for this, but what can be called strain structure populations. So within a strain, samples are quite similar to each other, but between different strains, there are very large genetic distances. So what this means kind of functionally for a lot of analysis is that if you try and do a whole genome alignment of the whole population, your phylogeny will be quite poor. So it's possible to get quite good phylogenies at the tips where, so within a strain. Where so within a strain, but the ancestral part of the tree will likely be very uncertain, and the ordering of those nodes will be impossible to resolve because there's been large recombinations which you just don't have the information to resolve. So, how does one deal with this? So, well, one thing is that just observing the strains are actually very useful to have. Strains are a good starting point. Epidemiologically, there's strong phenotypic correlation with strains. Correlation with strains in many cases. So, antibiograms, phenotypic profile can be correlated strongly with strain. But also, just in terms of thinking about this as a data problem, if you have enormous databases, splitting that up into sub-problems, which you can then run your kind of properly powered phylodynamic methods on is very useful. So, within a strain, these kind of evolutionary methods are valid. Another thing to mention, which is important in this talk, for those that aren't Which is important in this talk for those that aren't so familiar with bacterial populations, are that they generally have a very significant accessory genome. So, there's been some talks about how that might be driving their evolution, but it's a very important part of how their genome varies. There are many genes and they can come in and out in this pangenome. So, I'm just going to start off by talking about a method we published a year or two ago called PopPunk. So, this kind of underlies a lot of what I'm going to talk about later. But we realized it'd be But we realized it would be by in if we could construct pairwise genetic distances between samples, we could use this to define these strains very quickly. And we wanted to do this in a manner which didn't require an alignment, didn't necessarily require an assembly, and didn't require calling of genes. And this is what we thought of. So here are two sequences. They're shown as aligned here, but they needn't be, they're just aligned to make it slightly easier to see. To make it slightly easier to see. So you can see between the two sequences, there's some SNPs in red. There's a gene that's missing in the bottom sequence, which is kind of the big, the big deletion. And here's a K-mer sequence of length K in green, and that matches between the two sequences. So first observation, if I count the number of K-mers that match between the two sequences, it gives me an idea of how similar they are. So I can roll this K-mer through and think that there are, is it eight or nine K-mers that match in between those two? Nine k-mas that match in between those two SNPs. Leads me to a second observation: that the longer that k-ma is, the more likely it is to hit a SNP and mismatch. If you assume that the mutations are roughly evenly distributed, so like JC69 matrices, simple stuff, you can derive the following: that each additional base you add to the k-ma gives a roughly multiplicative chance of mismatch. So you get this to the power of k. Mismatch, so you get this to the power of k. And then the other observation is that in the accessory region, this deletion is going to be a lot longer than whatever k-ma length you pick. So, be it longer or shorter, it will mismatch in that region. So, it's independent of the length k. So, this is great because what we can do is we can compare the number of matching k-mers at different lengths and fit this relationship. So, this is dead simple because in log log space, it's just linear regression. Look across a number of different k-mer lengths. Look across a number of different K-ma lengths, calculate this Jacquard distance, so that's the intersection of the union of all the K-mers, needn't be aligned, needn't have genes called. We can do that quickly for big populations. Just as an aside to start with, in the original paper, we only considered bacterial genomes, but we've recently started to consider whether this could be applied more generally, so to viruses and eukaryotes, but specifically fungi, which, as a disclaimer, I don't have much experience. Which, as a disclaimer, I don't have much experience with. But kind of the key considerations we think turned out to be these. So, one is that if you're using longer genomes for eukaryotes, you can't use shorter k-mers because they'll start just matching at random rather than due to actual homology. With sequences with a higher mutation rate, so viral RNA, longer k-mers will always start to mismatch. So, if you have loads of SNPs, a big k-mer will just always mismatch. So, there's no information to be gained there. There's no information to be gained there. And also, you know, we're talking about accessory distances where there's a whole gene which is deleted to hundreds or thousands of bases. But in a virus, you might have smaller deletions you want to find. And your shortest K-ma defines the minimum resolution you can get to there. So just to show a few plots to support this, this is an example of using K-ma lengths that are too long. So as the lines go down here, the mutation rate, so from top to bottom, the mutation rate is increasing. And you can see Is increasing, and you can see the long K-mers, the kind of the amount matching just tails off to zero. This kind of shows the perils of using K-merlins that are too short. So, the green and the red lines are for typical viral genomes, but the blue is for typical bacteria. And once you use smaller genomes, you just get lots of random matches. So, it's just complete noise. So, quick background to genome sketching as well. So, for those that haven't come across, So, for those that haven't come across this method before, it's used very commonly, it's really taken off. I think MASH is probably the bit of software that introduced most people in the field to this. It's based off a method that was used in search engines, I think from the 90s, but neatly applied to genetics. And it's been used in lots of fields. So, you can use it to calculate average nucleotide identity with FASTANI, you can use it for all sorts of quality control, you can use it for metagenomics. But the basic idea. Genomics. But the basic idea which underlies it is that you don't need to compare all of the k-mas between those two sequences. It's actually sufficient to take a randomly selected subset of size S, so that's the sketch size. So that does set a maximum resolution of in your pi, so that s needs to be big enough. So to give you an idea, typically for species identification, you might need to use a thousand, but for within species, so strain identification, you need to go up to about ten thousand. Up to about 10,000. These sketches are much smaller to store and much quicker to compare than looking at all of these k-mers. And you can calculate this Jacquard distance, which is the intersection of the union size. The only kind of difficulty is how do you actually make sure that subset is random? DNA sequence is not random. So if you just lexographically ordered it, you know, it has some function. There's going to be things that are shared across like large evolutionary distances. One thing you can do is you can apply hands. thing you can do is you can apply a hash function to these bit strings and these map so everything i mean everything in a computer is a number it's not hard to come up with a function that maps numbers to numbers but the hash function is cleverly designed so that it'll map to integers in a defined range but importantly those integers will be uniformly distributed randomly uniformly distributed so then from there you can take um just say the lowest s that'll be your sketch size and that's actually exactly how the min hash algorithm And that's actually exactly how the min hash algorithm works. It just takes the lowest s of the K-mer hashes and then compares how many of those are in the intersection of the union. So the original version of the poppunk software used MASH, great bit of software, as I've mentioned, but now a little bit old. So the feels come along a bit. So there's this nice review from Will Rao, which I'd recommend to anyone who's interested in these methods that describes kind of some advances since then. And three bits of software. And three bits of software that I think are very good are listed here. And what we wanted to do to start with was update to one of these methods and then use that later for genomic epidemiology. And there's two main considerations. The more objective one is that probably we will sketch things once. So you get your sequences in, you can sketch them and store the sketches. So sketching is less important because it's just done a single time. Just done a single time, and also it's just linear in the number of input sequences. However, the distance operations are probably more important, firstly, because there's n squared of them if you're doing all-by-all comparisons, so they grow very rapidly, and also because we're likely to want to keep recomputing distances. So, if we could on the fly take out parts of our database and just compute the distances between them, that'd be useful. And that's what we want to keep redoing. Want to keep redoing. The second criteria, which is a lot more subjective, is that I needed to be able to understand how to implement it. And I couldn't do that for KMADV. Sure, it's very feather, but it was beyond me. But BinDash was easy enough to understand and had these nice fast distances. So, kind of on those metrics, that was what we went with. So, using that as the base, we developed this new bit of software called Sketchlib. So, it's a new sketching method. Sketching methods. It's targeted at genomic epidemiology, but it is, you know, generally, it could be used for anything MASH is used for. So, I'll give an overview of what goes into that on this slide. So, what we, so there's kind of a few modern techniques and then a few kind of tweaks we make to them. So, the first is that to calculate that hash function, we use a bit of software called nt hash. So, that's specific to nucleotides and is a rolling hash. So, it takes a very Is a rolling hash so it takes advantage of the fact that the sequence is linear and we can move through it by adding a base on and taking a base off, which makes it faster. Then we add into that this concept of non-contiguous k-mers. So rather than having the k-mer as being just the sequence, we can ignore some bases. So this would be a three-mer, and in this case, this is aligned with codons. So this was actually necessary in flu, where the third base varies a lot more because it causes a non-synonymous change. More because it causes a non-synonymous change. So, when you have strong selection pressure in a small genome, that's quite important. So, that's an option you can add in. Another thing we add in is an in-memory filter so that you can take just raw read data, error correct that, and turn that into your sketch. So, if you don't have assemblies to hand, you can just put data in straight off the sequencer. Yeah, at the core, we have a slightly modified version of BinDash, which kind of deals with the rest of the sketching. Which kind of deals with the rest of the sketching and the distances. One of the main things we add into that is we adjust for the random matches. So, this is particularly useful for viruses where we want to get as small a k as possible. But, kind of the basic idea here is that if you know from theory or otherwise what the expected number of random matches is, you can fit to the excess matches above random. So, there's still signal there to be fit to. So, we do that. So we do that, and the way we do that is we developed a parallel random number generator to make random sequence, calculate the expected number of random matches empirically, and kind of cache that. And we also had some CPU extensions in to make it a bit faster. We thought quite a lot about how to store this. So a lot of existing methods serialize the sketches. So it just writes it out as one long thing, which is good if you've got hard drive. Hard drives read things in. Got hard drive, hard drives read things in all in one go, but with solid-state drives, and you can pick bits of memory out kind of at random very efficiently. And again, that's exactly what we want to do in epidemiology. We want to just take our large database and take the sequences we want out of that. And with serialized data, that can be quite difficult. And was kind of an awkwardness of using MASH. So now we can just access sequences by their ID. We can still easily merge them. A couple of extra nice things. Merge them. A couple of extra nice things is that you can store metadata in there so we can store information about sequence quality as well as the sketch. And it's all just in one kind of combined file. And as it's a standard format, it's efficient and there's APIs in most languages. So you can interact with this object, even if you're not using our code. You can use it from the command line. It's still meaningful irrespective of using Sketchlib in particular. In particular, one thing I think the thing probably we spent the most amount of time doing is parallelizing this. So the CPU parallelism is easy enough. We use OpenMP, pretty standard, is very efficient, kind of gets close to 100% efficiency for both sketching and distances. But what took a while and hopefully was worthwhile was porting this to graphics cards. So even a typical kind of standard gaming graphics card will have up to 10,000 cores. So, you know, a huge number of So, you know, a huge number of cores, a bit more difficult to use. But yeah, we ported the whole algorithm across to there. So there's both a CPU and a GPU version. And some of the algorithms can actually use both CPU and GPU parallelism for kind of top speed. And then the final thing is distance matrices, especially big ones, a bit awkward to work with. And if you want to pass them around between programs, that can be difficult. So we have some functions for doing that efficiently. Doing that efficiently. So that's kind of everything in that package. Just a few benchmarks to show kind of what scale we can get up to. So this table, a little bit meaningless, but to show you on an actual data set that we ran on. So this was looking at 35,000 Streptococcus pneumoniae. So the old version of the code, the sketching was pretty fast. MASH is very good. Took about 10 minutes. So we've got a two to three times speed up over that, which is nothing to write home about, nice to have, but doesn't really. To have, but doesn't really change the game in any way. But the distances did speed up a lot. So previously, doing those 35,000 times 35,000 distances was taking five days, but now with the GPU can be run in three minutes. So three minutes to get the core and accessory distances of all of those. And to my knowledge, I think that the biggest data set that's been run on is about 600,000. Just to point out where the GPU programming was useful, so you get about 15 times increase if you. So, you get about 15 times increase if you kind of add the GPU into 16 cores for the distances. But if you've got read data, so this is particularly useful for some of the eukaryotes, you can get about a 50 times speed up of processing that versus using just a CPU. So, the way that's done is the CPU is used to read in and process the sequence data from the disk, and then all of those many GPU calls each take a read, sketch it, error correct it. Sketch it, error correct it, and create the sketch. So that's a big speed up there. GPU programming, if anyone's interested in porting their algorithms, I kind of learnt it this year. It's not the easiest thing, and there's not a ton of documentation because it's quite new. But if anyone is interested, I've got a blog post about it where I give some of my experiences with it. And then just to explicitly compare it to MASH, in case anyone's thinking about how. In case anyone's thinking about how these two bits of software relate. So, roughly, the algorithm is about 100 times faster if you're just using the single core CPU. Much faster if you use the GPU, but we'll just ignore that for now. But what we're doing to get core distances rather than just your card distances, we usually take about 10 km lengths and have to use a tenfold larger sketch size. So, it kind of cancels out that difference. So, for the same amount of computation, although you can make it faster. Although you can make it faster, you'll get core distances and kind of accessory for free. But if you still want to use just the mash functionality and just look at a single K-ma length and a smaller sketch size, you can still do that. That is still possible in this software if you want that kind of maximum speed. So that's the kind of back-end we've done a lot of work in the past year on. The previous bit of software is PopPun, which is kind of more. Which is kind of more aimed at people actually doing genomic epidemiology. So I won't go into too much detail about the way it works, but it's quite similar to what Art described in his talk last night, where you have a network, a threshold cutoff distance, and you use connected components. We use a slightly different method to optimize the point of that threshold. But the kind of idea of it is you have some sequences. So this is an example with about Sequences. So, this is an example with about 600 Streptocox pneumoniae. You just run this on it, it's a whole pipeline. You get out an interactive visualization where you've got here on the left a neighbor joining tree of core distances colored in by clusters, which are, we think, quite good clusters. And then on the right, you've got an embedding of accessory distances. But it's up to you how you want to display that data. You could do trees of both. Just to show you that it does work here, it runs on all the Work is it run on all the flu genomes or on any of the flu genomes in GISAID. And what you can, I mean, this is pretty fast to run. You get this out straight away without having to do any alignment. You can quite quickly start comparing like your HN types with genetic background, backbone. And it wouldn't be a talk in 2020 without showing some COVID genomes. It does work for COVID, some things slightly change, but one thing I was thinking about here is what. One thing I was thinking about here is what is everyone going to do when there are a million COVID genomes, which surely there will be. I've been very impressed by the ingenuity of the people working on viral genomics at this conference, of how they've kind of got around this massive data problem. Everyone still seems to be doing fine. But perhaps this will be useful when things do kind of outgrow methods. So, one thing would be getting distances to start your maximum livelihood tree from. Some software packages I know aren't particularly optimized in that way. So, maybe you. Particularly optimized in that way, so it may be useful if that first step takes a long time. In terms of making it more user-friendly, we're developing a web front end. The basic idea being that sketches themselves are privacy-preserving, they don't have, you can't get back to the genome from them, but also they're small, so can be sent over the network, and they're sufficient for a lot of genomic epidemiology tasks. So, you can run everything in the browser without having any installation. And my student, Daniel, has a talk on Friday where he's going to demonstrate. Has a talk on Friday where he's going to demo this. So he's been developing this. So if you're interested in that, do go to his talk. Kind of standard, I guess, front end for this kind of thing, drag and drop, and then you've got some visualizations. But I won't steal any more of Daniel's Thunder. You can hear more about that on Friday. So I'll end there with a quick summary. So the two packages I presented are, so Sketchlib is the new code written in C GPU code and CUDA, so it's nice and fast, but has a Python API. So you can put this into your packages. So you can put this into your packages. It can be called just from Python. It's a general sketching method that has lots of features of modern methods in there with a number of extensions. If you want to do genomic epidemiology, then you can use the poppunk package, which automatically uses this as the back end. It's a genomic epidemiology method. You can use it to rapidly cluster pathogen genomes and get interactive visualizations out. So I'll end there. Sorry, it's slightly over time. I just want to say particular thanks to Nick Crouch. Particular thanks to Nick Croucher, who I've worked on most of this with, and Daniel, who's been developing the front end. Also, thank you to Ben Langmead and Will, who put some really good reviews out there that got me started kind of with the sketching literature. And also to these software packages, which also really helped me develop this. And in some cases, I've actually been able to use some of the code from them. So I'll end there. And if there's time, I'll be happy to take any questions. Thank you. Fantastic talk, John. I'll take John, I'll take one question. I think there's one in the chat that you can see from Jimmy Liu. Do you see it? Yeah, that is sometimes a problem. So it's kind of like it's a bit of a big data problem. If you're calculating billions of distances, you need some automated way to deal with anything that doesn't fit the model. So we kind of remove points from the regression. Remove points from the regression in that case. So you still get some sensible output. You can see zero distances in there. Where we saw that very systematically was in flu, where you had those third codons changing more. And if you looked at six mers, they were matching much less frequently than seven mers, so you got a negative core distance. So that was what motivated the use of these non-contiguous k-mers. Thank you, John. Thank you, John. And for those interested in talking more, again, there'll be the discussion outbreak room.