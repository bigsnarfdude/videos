So, thank you a lot. Optimal transport, which it's already come up just now in the discussion session. This is a huge topic, not originally in physics, but in mathematics, but it has very recently also sort of leaked a bit into physics, in particular particle physics. And Tudor and I wanted to give you a brief rundown of what you can say at the moment and what you can do with it at the moment. So, what he wanted to do. So, what we wanted to do is to tell you about three different things, and we're going to start with a very brief introduction to the role of optimal transfer. So, explain a bit the backstory, the math side of things, and ultimately how it then relates to particle physics and why particle physicists should care about it. Now, of course, all of this is really nice to hear about, but what you should really want to know is how you solve these kind of problems that show up in practice. What are the kind of algorithms that can? Practice. What are the kind of algorithms, the kind of methods, the tools that people have also already developed to handle these things in practice? And then, lastly, we're going to cover a few, at a very high level, a few example applications, emerging ones in particle physics. And we're going to try to give you both perspectives. I'm going to take that of an experimental particle physicist, and then Tudor, who's sitting up there, is going to tell you a bit about the side of statistics. So all of this is going to be very brief and relatively high level. Very brief and relatively high level, so by all means, we should then go into the details and the discussion of this. So, why should you care about this topic? The answer is very simple. In particle physics, as we have just heard from Lydia, what we do on a daily basis is manipulate probability distributions in all kinds of ways. We have heard examples about this in this workshop from various people. For example, if there's this control region, signal region problem where you have to extract some knowledge about Where you have to extract some knowledge about the background from one region and then extrapolate and sort of apply that knowledge elsewhere to learn about some potential signal identity. We've just heard from Lydia about template morphing. You have generator A, generator B, two separate predictions. How are you going to interpolate between them in some sort of smooth way? And then the other problem that we've also heard yesterday, I think, calibration, you've got some simulation or some model that you think is relatively accurate but it's not perfect, how do you go the last step? How do you go the last step from front it to data and sort of fine-tune it so that it becomes better? And it turns out that optimal transport has something to say about all of those different topics, and it somehow provides a set of useful tools, but even more so, sort of some underlying methodology and the philosophy in a certain way of how to think about those problems in a harmonized perspective. So, let's start. The theory of optimal transport, what is it? So, what is the problem if you want that optimal transport? So what is the problem, if you want, that optimal transport claims to have the answer to? And ultimately, and this is where it comes from in its math background, it's a logistics problem. Okay, so here is the stereotypical sort of hello world optimal transfer problem. You've got a couple of factories that produce certain consumer goods. Okay, so here I call them factory A and factory B. And they're at certain locations. And then you have a couple of stores, sort of one, two, and three, that have a certain demand for these kinds of goods that the factors produce. And so now the goal is. For use. And so now the goal is to take the output of each of those factories and sort of deliver it to those stores in such a way that the total output of each factory is somehow picked up by the trucks and that the total demand of each store is satisfied. Now, of course, this is the real world, and in the real world, transport is not free, but there is a certain cost associated to it. So there's a certain gray cost function, if you want, that lives along these edges that connect factories to stores. And for example, Factories to stores. And for example, here it costs sort of an amount CA, 1, to transfer one unit of mass or one sort of unit of the product from factory A to store number one, and so on and so forth. And the goal then is to find sort of this transportation plan that tells you in an operative way how to shuffle your produce out to those stores such that you minimize the overall cost that you have to pay to a transportation company. And this turns out to be an incredibly rich math problem, and it has a history that threatens. Problem and it has a history that stretches back into the 18th century, and some of it has actually led to very high-profile Fields Medal-winning maths results. So, this is a major topic that not only has its applications, but is also important for maths itself. So, here is just a picture of how one such solution might look like. For example, if you have sort of an output of four units of this factory and then this store needs three of them and then there is another store that just needs one of them, uh for example, what you might want to do is take these three and put them over here and the last one you take there. And put them over here, and the last one you take there, and then you sort of pair up these other two factories which happen to match, I'm sorry, this factory with the store, which happened to match away the amount of stuff that is needed. Okay, that's one potential way of phrasing that solution, an explicit list of how you are supposed to reshuffle these circles. Now, for a particle physicist, you don't necessarily care about factories and stores and so on, but you would really care about its distributions. But in particle physics, as we've also heard, But in particle physics, as we've also heard already, we don't very rarely have the knowledge of the actual functional form of those distributions. What we have at the end is just samples that are drawn from our simulators or from some representation of these distributions. And then that problem becomes relatively similar again to the one that we've just discussed. Because you can ask the question, if you have a certain source distribution that you start from, this blue one here on the left, and you have sort of a sample of events that you have drawn from that, and you have a certain You have drawn from that, and you have a certain target distribution, the red one on the right, and you have a couple of samples that you've drawn from it, you know, what is the way in which you need to rearrange those blue samples such that they match the red ones? Or sort of more generally, what is this kind of function that you can look up that will tell you where to move each of those blue circles such that the positions where they end up encodes the rate distribution. Okay, this is what you actually want to do. To do. And the mass community have looked at this problem and they call it the Morge optimal transport problem. And in fact, this is Gaspard Morges, sort of a super famous French mathematician, and he is the one who actually started this entire field. Okay, so the name Morange is somehow intimately linked to all of this story here. So the name of the game is to construct this function that I've just introduced. These sort of arrows here, which are supposed to be coming from some underlying continuous function that maps this. function that maps this blue distribution into that red distribution and it does this mapping by moving around the samples. So in other words, for any position x where you might have a sample, you can look up, you can push it through this function and the function will tell you the position y that you need to move the sample to such that if you do that to all of your samples, you're going to get the new distribution. Okay, this is what I just said. These distributions are going to change in some way. Effectively, what t does is just a change of variables, so that the densities Variables so that the densities will change accordingly to the rule by which densities change. They will pick up this Jacobian factor that sort of is connected to the inverse of the gradient of this function. But then what we really want to do, this is why it's called optimal transport, we want this function t to not only have this mapping property, but we also want it to be somehow the most minimal mapping. So in other words, if you again proclaim some sort of cost function that tells you how much. Cost function that tells you how much effort you need to put in to move a sample from here to there. What is the overall cost that you need to pay? It's sort of this cost function evaluated at a certain point and its destination, that's computed by t. Averaged over all of your original distribution, right? This is the expectation of that cost, and you want that function t that you select in the end to minimize that. Okay, that's the statement of the problem in particle physics. Physics. And if you have paid close attention, you will notice that this is actually a slightly different formulation of the problem relative to the one that I've just mentioned before with the factories. Because with these factories, you will have noticed that in this case here, we have actually split the samples. We have taken all of the outputs of this factory, and we've taken a certain fraction of them and put it to one way, and we've taken the remaining fraction and we've put it onto another destination. This is something that you can fundamentally not do with a formulation like this. not do with a formulation like this because all of the samples that happen to sit here at a certain location by virtue of this being a function will get mapped to the same final point so there is no splitting here in this kind of mosh formulation but there is splitting allowed in this other formulation which goes by the name of kontorovich okay and now you will have seen the two major let's say formulations of optimal transfer now there is one more Now, there is one more comment that you should make at this point, which is if you look at this integral, what we care about, as we've just said, is somehow the function that minimizes this overall cost. The argument that minimizes this overall cost. But you can also decide to care not about the argument, but about the actual minimal value that is attained at that optimum. And this also has a name, right? In a sense, this is somehow the minimum possible cost that rearranges the one distribution to look like the other distribution. One distribution to look like the other distribution. And in a sense, you can look at this as a way of measuring a distance or as a measuring a notion of similarity between your two distributions. It's a sort of metric of the space of distributions, and it goes by the name of Wasserstein. This is one possible way that you can do that, and it has certain nice properties, as we're going to see afterwards. Now, operatively, and this is now continuing the story that Lydia started before, operatively, if you start from a source distribution and you want to make it look like a ton. Distribution, and you want to make it look like a target distribution, effectively, this just gives you the same results as if you were to bin your density, and then did some sort of re-weighting or some other sort of morphing between the source and the target distribution. You're just going to end up with the distribution that you want it to reach. However, because you don't do any reweighting here, but instead you move your samples around, you gain something that otherwise you would not have had. For example, in those counter examples that we just discussed, In those counter examples that we just discussed, if you go to the very tails, where let's say the ratio may be very large, and this kind of weighting or vertical morphing might be very ill-behaved, these are the cases that you can avoid here, because what you do is actually move the samples around, and there it doesn't matter whether you're in the tails or in the bulk of the display. And this is speaking to some of the important applications that we're going to see afterwards. So now we know roughly what we want to do. We want to morph one distribution into another distribution. One distribution into another distribution in some sort of cost-optimal way by moving the samples around. Now, how to do that in practice? And in principle, you know, there's two fundamental paradigms that you can look at if you start from samples. Okay, so for example, you can do the thing that's shown here on the left, where you take your empirical distributions literally. You say you have your samples, and this is all that you know. And so, for example, you can look at your set of blue samples, which is the ones that you start from, and you figure out what. The ones that you start from, and you figure out what are these errors that take them to the positions of the red samples. How can you sort of pair up your source and target samples in the optimal way? How do you sort of draw these errors here? In a sense, this is sort of totally discrete and you're just using your samples to do the draft. And that has certain problems. If then you want to, for example, evaluate, what happens if you had a sample here, which you hadn't actually had when you derived that map, but which you sort of had to apply it to afterwards. It was kind of out of sense. To afterwards. This kind of out-of-sample evaluation is something that you will need to do explicitly afterwards. There needs to be some way of interpolating in between the samples if you don't specify it. Now, conversely, you can also imagine that there is some true underlying continuous such transport function that you don't know, but that you can use the data to estimate. Much like that we use a discrete billing to fit some underlying smooth statistical model, you can use your data here. You can use your data here to inform somehow your posterior knowledge about that smooth underlying function. And you can, of course, also have to make some assumptions on what the length scales of these functions are and how they behave and so on. But here, fundamentally, the output is a full continuous function. And that makes these kind of interpolations a bit more natural and easier to define. Okay, so now it turns out that in all of this game, regardless of how you do it, somehow this costume. You do it, somehow, this cost function here that you put in has a huge impact on the structure of the solutions that you're going to get out. Okay, so I'm just going to mention one example here, which is you might want to consider cost functions like this ones. You just have two points, x and y, and you take the absolute distance between them, and you raise it to some power p, which is greater than one. Okay, for example, the Euclidean distance squared, x minus y squared, is sort of in that class of distances. And for that case, it turns out. And for that case, it turns out that you can say a lot about what that optimal function that you get out at the end that moves your samples around is going to look like. And specifically, it's going to look like a gradient field. There's some underlying potential g, whose gradient is going to tell you a vector along which you will have to move your event so that it ends up in the right place. This is very much like electrostatics. And so, for a physicist who knows electrostatics, this gives you a lot of intuition about what these kind of transport functions can do. They can move. Functions can do. You know, they can move mass around, so they can move mass away from a point radially like this, but they cannot move your samples around in circles because that would sort of be suboptimal relative to that constant. Okay, now let's close that chapter and let's have a look at a few example applications and then Tudor is going to take over. Let me say a few more words about the problem that we just discussed, template morphing. So we've had our source distribution P, the blue one. Source distribution P, the blue one, and then we have the target distribution Q, the red one. And if I sort of draw them like this in the space of distributions, then each of those two is sort of a point in a certain location in this abstract space. And what we've just discussed now is that there is, in principle, if you just have these two points, there is no unique way of going from one to the other. You can imagine walking along many trajectories that start here and that end there, but which one you have to pick, right? According to some domain knowledge that you might have. Now it turns out, with optimization, Now it turns out with optimal transport you can also do something like this, right? Because you get a vector for each point you get a vector along which you have to move. But what if you decide to not go the entire vector, but you just decide to go a fraction of that vector? Then you will end up somewhere in between the blue and the red. You will have to interpolate in some way. And in fact, what happens is that if you do it like this, you will actually move along a geodesic line, somehow the shortest possible line that connects the Shortest possible line that connects the source and the target distributions. So here is again sort of Lydia's two-peak example where what she did, one thing that she said was this vertical interpolation where you ramp down one peak and you ramp up the other peak and then you have these sort of spurious local minima that appear. Whereas if you move along that geodesic line, which is very much like a horizontal interpolation, this doesn't happen, but instead what happens is that peak actually moves in a smooth way. Peak actually moves in a smooth way. Okay, so this thing over here would correspond to some longer path which is not a geodesic, and the one up here, that horizontal interpolation, is in fact a geodesic. One word about a second big topic that this could potentially be useful for has also already implicitly been mentioned at this workshop, and that is to do with our simulations. So, a couple people showed this plot from Lucas or Jill that has sort of this convolution of many different. That has this convolution of many different probability distributions that relate the underlying theory space to the space of observed events in our detectors, and that consists of many sequential steps that each talk about very different kinds of physics. And now, in particle physics, we have spent a lot of effort collectively building these simulations, and they contain a lot of the domain knowledge we have amassed as a field of law. But of course, our simulations are very good, but they're not perfect and they are imperfect for various different reasons. Imperfect for various different reasons. Some sort of conceptual and some practical. And of course, what you would want to do is correct for those deficiencies, but it's often impossible or impractical to really go into the model and sort of find the root cause that makes this be a problem and eliminate it. So instead, what you do is you sort of apply some sort of ad hoc on-top correction. So instead of going into that simulation model and tweaking it, you take the output of the simulation model and you tweak the output. And you tweak the output. Okay, so for each of your events that comes out of this chain, you compute a certain small correction that you apply to it that then makes it look like a slightly different event, such that this distribution of new calibrated different events is sort of a more closely, is a better approximation of the reality than what the original simulator was doing. And now you can see that this is again very much a case where you have two distributions that in this case are supposed to be very equal. Be very equal or very, very close because the simulation a priori is already very good, but they don't match exactly. And you can use that optimal transport paradigm to compute that small correction that you're supposed to apply on top of what the simulation gives you. So in this case, that minimality that's inherent in the optimal transport is sort of acknowledging the fact that you don't want to totally change and negate that domain knowledge that's awaiting the stimulation. You want to just make the You want to just make the smallest possible adjustment to the output of the simulator that makes it consistent with the data, but not more than that. And then, okay, there's a lot of subtleties that have to do with exactly what you put in as the cost function there, and this is where the physics knowledge enters. And this is a topic that's best kept for the discussion afterwards. So at this point, let me hand over to Tudor to walk you through the rest of the potential developments. Thank you.