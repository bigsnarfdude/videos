The reasons for this invitation, it's been a really educational, exciting workshop. So, I'm going to say a little bit about some computational challenges that we're dealing with as we are working with biobanks. And essentially, one of the goals, one of the interesting me with biobank data is an opportunity to learn about genetic architecture and context rates with these millions of genomes. Although we're not there yet, but we are approaching that. We're not there yet, but we are approaching that. And that brings with it different types of challenges. One set of challenges is: how do we model this data as accurately, as biologically, realistically as possible? And I view those as challenges in statistical modeling. But then, as you start making your models more statistically plausible, biologically plausible, you run into issues of scaling these models to these large biographies. So that brings with it computational challenges. And of course, you also have issues of data access and privacy. You also have issues of data access and privacy. So, all of these goals are intentional. So, how do we actually deal with these trade-offs? Can we actually resolve some of them? It's something I'm very interested in. So, when I'm thinking of these types of problems, I have a few ways of grouping them. So, one set of inference problems that I think of are inference problems centered around looking at genotype data to try to make sense of it and trying to see if there is some underlying structure in this genotype matrix. So, typically, the kind of structure. So, typically, the kind of structure we're interested in is related to ancestries and populations. So, we're interested in inferring population structure from genetic data. And these types of approaches rely typically on models like principal components analysis, PCA, or add mixture models. So, essentially, they're models which take this large matrix and break it up into these lower-rank matrices. And they make different assumptions on what these lower-ranked matrices look like. Turns out, even computing Turns out, even computing these kinds of decompositions, these factorizations on biopax clear data sets where you have, say, million individuals and millions of SNPs is a challenging computation for us. And so we have some work on this, which I won't say too much about, but the key idea there is to view these as what we call major variable models. And we bring in insights from randomized linear algebra to speed up a lot of the computation. So those are inference problems focused on the genotype side. And of course, the other And of course, the other set of problems are focused on the phenotype itself. So, as we've heard in several of the talks, even getting the phenotypes of what it means to get the phenotype is unclear about these data sets. There is big challenges with missingness or not able to measure the right type of phenotype at scale. So, we've been very interested in intuiting phenotypes that you care about from other phenotypes that are collected at scale in these parts. And one of the approaches that we've looked at. One of the approaches that we've looked at is to see how well we can use deep learning approaches for phenotype reputation and to deal with the complex patterns of missingness. And it turns out that these approaches have shown some promising results, and that's something we're also following up on. But for today's talk, I'm going to focus on the link between genotype and phenotype. So we assume that we measure both of these sides of the picture, and we'd like to use that to understand genetic architecture. That to understand genetic architecture. So, when we talk about genetic architecture, it can mean several different things. So, we might talk about how much of the variation in phenotype is explained by additive genetic effects. So, this is typically what we call heritability in the narrow sense. Or we might want to know how genetic effects are motivated by context. So, if the context is environment, that would be gene-environment interactions. Or if it's other genetic factors, this would be dominance or gene-gene interactions. Or gene genes. Or we might be interested in how genetic effects are shared across phenotypes and that leads to things like genetic correlation estimates. And of course, we might want to learn about these parameters genome-wide or in specific regions of the genome or specific connotations. We might want to learn them in one population or see how they vary across different populations. There are lots of different ways of thinking about genetic architecture. So, just to make things more concrete, I'll focus on heritability and then we'll build up towards the Heritability, and then we'll build up towards these other parameters of genetic architecture. Of course, heritability is a very basic parameter, it's of interest in and of itself, but it's going to reveal a lot of the issues with biotax. So, to estimate heritability, we'll start with this very simple model. So, it's a variance components model, where we're going to model the phenotype as arising from a linear combination of genetic effects. And so, we have these genetic effects that are going to be. Genetic effects that are going to be drawn from some distribution. The details don't matter much, except that the variance parameter of this distribution of genetic effects is related to something called the genetic variance of what. And likewise, everything that is not genetic, we call it the environment, and we assume that that's coming from another distribution whose variance parameters, the environmental variance component. So, putting the genetic and the environmental variance component together, we have this quantity called the SNP heritability, so it's a number between 0 and 1. Helitability, so it's a number between 0 and 1. So, traits which have headabilities closer to 1 are in some sense more predictive from. This is a very simple model. You can make it more biologically plausible by saying that the effect sizes of SNPs are not all coming from the same distribution. Depending on where the SNP lies, you might have different distributions. Say from protein-coding genes, you might have a distribution that's different from outside of protein-coding genes. And all that means is now you have different variance components. means is now you have different variance components corresponding to whether a genetic variant falls within proprietor gene. So in general we have a variance components model which is dividing up the genome into k different annotations. SNPs within each annotation they have their effect sizes with their corresponding variance component parameters. And our goal is to estimate the variance components of each of these genetic components and the environmental variance components. That's a classic statistic. So it's a classic statistical estimation problem. Now, the challenge is if you were to use maximum likelihood estimation or REML to estimate these parameters, it turns out to be computationally extremely challenging. So roughly it scales as a cubic in the sample size, which means if you're looking at samples of half a million individuals, these types of approaches don't really scale to estimate these models. So we can look at alternate estimators, and we heard a talk today morning. And we heard a talk today morning which talked about an alternate estimator, which is the method of moments. So, this in genetics was introduced by Case Man Education, so it's also called Etching or Case Man Edison's regression. And as the name suggests, you write down the moment equations. So, for example, you might look at the second moment, the first moment does not have any information in this model. So, you look at the second moment, which depends on the parameters that we care about. And you try to find values of these parameters so that it matches what we see in the data. So, that it matches what we see from the data, which would be something like this. So, what's nice about this approach, the method of moments approach, is you have a closed-form solution. So, you can actually write down your answer as a solution to a linear system. So, that's the easiest problem. So, you're essentially done. You have to just solve this system. We're not quite there in terms of a computationally scalable approach because this method of moments still requires us to compute these genetic relationships. These genetic relationship matrix GRMs. So GRM is essentially an n by n matrix. For every pair of individuals, you have this entry, which tells you how related that pair of individuals is at that set of SNPs. And so essentially computing this, again, is a computationally expensive job when you have these large indices. So our insight when we tried to see whether we could speed up this computation was the fact that if you look at the metal moments equation, which again, I don't trust you can pass. And I don't trust you can pass in this kind of brief talk. Is that although you need these GRMs, ultimately the coefficients of this method of moments equation only depend on some summaries of these GRMs. And specifically, they depend on traces of the GRM and traces of products of the GRM. So essentially, you're building this large matrix and then collapsing it into these small numbers of some of these. So can we do better? Can we avoid having to go through this kind of wasteful computation? This kind of basel computation that builds up these T values. Turns out we can, and this uses again insights from randomized linear algebra, where there have been all of these methods that allow us to estimate traces of matrices stochastically without ever forming. So effectively, what we're doing is we are working with a sketch or a random projection of the geometric matrix. So take this large matrix and multiply it with a small number of random vectors. So this is what we call a sketch or a random vector. So, this is what we call a sketch or a random projection. So, this is a new parameter that we have introduced into our method. So, that is the number of random vectors that we are going to use to sketch this G matrix. So, that's a parameter we call B, so that's a design parameter. Clearly, the smaller the value of B, the greater the reduction in the size of the matrix. So, the efficiency depends on the value of this parameter T. And what is interesting, and this is something which we still don't fully understand, is these methods are accurate. Is these methods are accurate for fairly small values. So for B as small as 10, you get fairly accurate estimates. So just to give you some perspective, you'll start with a genotype matrix, which might be 500,000 individuals by a million, 10 million SNPs, and the sketching reduces to 500,000 by 10, and that really leads to the fission. There's another interesting aspect of this algorithm, and that has to do with the fact that once you do the sketch, it's what we call a stringing method. Sketch, it's what we call a stringing method. So you do a one-time pass over these large genotype matrices, you can read them a little bit at a time and perform these computations. So it's also very lengthy efficient. So let me give you a little bit of a summary of these kinds of results just to see how these methods work. So this is built on HE regression, so we call it randomized HE regression. So the first thing we did was we looked at how much these results were in maximum likelihood or Reynold. So this was implemented. Or Renel. So, this was implemented in GCTA. And across simulations, we see that there is a small increase in the standard errors. I'm not sure if you can see. It's about 10% larger than what you get out of maximum likelihood. These kinds of experiments would only be run on small sample sizes because you can't scale maximum likelihood estimates of the sample sizes that we care about. We can also ask how much of these results are affected by randomization. So, for about 10 random vectors, 10 random vectors, randomization contributes about 20 to 30 percent of the overall standard errors. When you go to about 100 random vectors, it's less than 10 percent. So, again, randomization doesn't have a huge impact on overall standard errors. Then we moved on to larger scale simulations. So, here we are comparing to methods which could be run on large-scale data. So, here you can't run maximum likelihood, but you could run other methods that have been developed. So, these are summary statistic-based methods. Some of you might be familiar with methods like LES code regression. might be familiar with methods like LES code regression. And here you'll see that compared to some of these statistics-based methods, RT is quite a bit more statistically efficient. And it also has very good parts. In terms of the computation, you can see with sample size, if you try to run maximum likelihood approaches, they are scaling like this. So it's much harder to get them to apply to large sample sizes compared to a method like RH. Another advantage is running it not just on a single Running it not just on a single component of the genome. So, I mentioned you can make your model more realistic by having different variance components for different regions of the genome. And this is useful both when you want to estimate genome-by heritability, where you might want to partition biology like allele frequencies and LD, or if you care about functional enrichment of heritability. And these kinds of methods can also be applied when you have lots of components that you want to fit, and that's fine. So our initial interest in heritability estimation was, we thought this was already an interesting problem, a challenging problem to solve. But it turns out this approach where you take a method of moments estimator and you plug it in with these random projections is much more general. So we call this randomized method of moments. And since then we and other groups have applied it to different types of problems to understand genetic contribution. So we talked about deritability. Genetic population. So we talked about heritability. We and others have looked at genetic correlation, looked at dominance, G by E, G by G. So essentially, all of these are models which are much harder to fit traditionally, but now you can fit them using these randomized methodologies. So the next few slides, I'll talk about a few highlight results when we look at these other more interesting models. So here is one such model. So this is a model where we have an additive component, but we An additive component, but we are also fitting a second component, which is a gene-environmental interaction component. So we have an environmental variable, and we are forming all pairwise interactions between this environmental variable and all the SNPs that we are analyzing. And the question we are asking is, in aggregate, are all of these pairwise interactions between SNPs and environment, do they have some contribution with them? So that contribution is denoted by this G by E basically. By this G by E fat surprise. So now we can estimate additive, G by E, and environmental patience. Turns out there are some subtleties when we try to fit these types of models. For example, oftentimes you have noise that has variance that differs across environmental strata. And if you don't model it, that can bleed into your G by E environmental variance component estimate. So it's important to also model that. And we find that other methods that don't do a good job. Other methods that don't do a good job tend to have inflated false positive rates, whereas our method G is calibrated across these levels of environment-specific noise, while still being computationally efficient. We then applied this to about 50 quantitative traits in UK Biobank and four environmental variables. So we looked at sex, smoking, statin usage. We also looked at age, which I haven't shown here. And we are estimating all of these parameters. And we are estimating all of these parameters jargon. And what we find is across all of these pairs of environment phenotype that we analyze, roughly the G by E heritability, so proportional variance explained by G by E, is less than what you find from additive heritability. So that's consistent with previous studies, but it's still quite substantial. So depending on whether we look at array data or reputed data, it can range anywhere between 7 to 15 percent. And in some cases, it's quite substantial. And in some cases, it's quite substantial. It's hard to read here. You find examples like testosterone levels, which have substantial G-by-sex capability. Now, if you remove the environment and plug in another genetic variant, you get gene-gene interaction. And so, this is a model that was introduced by Lauren and Xiang. So, this is called a marginal epistasis model. So, unlike typical models of epistasis, where you're testing a pair. Where you're testing a pair of genetic variants for association with some phenotype. In the marginal epistasis model, you're asking: if I take a particular variant, is its effect on the phenotype modulated by the rest of polygenic type? So essentially, you're doing a test one SNP at a time, so you don't pay this quadratic multiple hypothesis test penalty that you would typically pay if you were doing a static scan. So, again, this fits within this variance components framework where. Variance components framework where you can now have this interaction between a particular variant and the rest of the genome, and you can fit this marginal epistasis of the e-zome. And this all goes within the same randomized method of analysis. So when you're fitting these types of models, the first thing is it's calibrated. So we show that when you have just additive effects and no g by t, these estimates are calibrated. Estimates are calibrated. We also looked at other types of model misspecification, so that's always a big challenge in this field. Where things like unmodeled causal variance, heteroschedastic noise, unmodeled covariance, all of those could potentially inflate your false positive weight. And so we have to confirm that the model is robust to this. You can't see this too well here. And it is fairly computationally efficient where you can actually apply this model to biobats. So we apply this model again to about. So, we applied this model again to about 50 quantitative traits in the nuclear biobank. So, what we did is we looked at traits which were significant in a regular GWAS and we asked, is there evidence for marginal epistasis? So, we find about 16 signals, 16 pairs of SNPs and traits, which are genome-wide significant in terms of their signal of marginal epistasis. We can also quantify the variance explained by marginal epistesis and compare it to the variance explained by GWAS at. To the variance explained by GWAS at the signal. And what is interesting is the variance explained by marginal epistasis is substantially higher than the variance explained by GWAS. So, this is relevant when you're trying to interpret the importance of genetic variation. And of course, another big challenge in this field is whether these kinds of signals actually replicate. So, we first did an internal replication in the UK bioband. We split UK bioband into two and asked whether what we discovered, one half replicates in the other. What we discover in one half replicates in the other, and that seems to replicate quite well. And then we looked at UK Biobank signals that we could find in all of us. Again, there's a whole kind of story here about which traits are compatible, and that took a lot of effort. But we do find all the traits which we were confident are phenotyped relatively well in all of us, we do find replication of those signals in all of us. Okay, so great. So we have a method which is statistically quite accurate, computationally efficient. However, it still needs access to individual ready data. And that somewhat reduces the applicability of this concept. And so the question is, can we now remove this dependence on individual red data? And it turns out, if you go back to the method of moment equations, so this is what the method of moment equations look like, which RHE is based on, we can rewrite. Based on, we can rewrite the left-hand side and the right-hand side so that they no longer require access to individual numbers. So the left-hand side, these trace quantities, are related to a certain concept which we call genome-wide LV scores. So we can essentially approximate them by randomized method of movements. So now you can release these approximations to the trace, which we call trace summary statistics. It's a lightweight summary of the left-hand side. Of the left-hand side. And then the right-hand side are actually standard Jivasum statistics. And now, with these two quantities, you can essentially reconstruct estimates like the Heritage. So this is a mathematical relationship. So this exactly reproduces what you get from RHE just using summary level. And so this just shows what the results look like. So here are heritability estimates compared to ROG. So RHE is kind of the better. Kind of the benchmark that we're comparing to. We're looking at the mean squared error relative to the RHE estimates. You have the individual level maximum by Q S methods that do slightly better. You have the existing summary statistic based methods, and now this is summary ROGs, which we call summary, which is identical to ROG. So it's as accurate as individual level methods, only needs summary data by the So, we are very excited about these kinds of methods. So, we think they are methods that are both accurate, scalable, and are able to work with some individual data. And a lot of this is due to the effectiveness of this idea of random projections. We don't completely understand all of the situations in which they work, but they do seem to work in relatively homogeneous populations like what we see in the UP paradigm. We think it's also useful for populations for which reference genomes are not publicly available, and another direction we And another direction we are going to take is the ability to do federated learning by sharing these trace summaries, which are lightweight summary statistics that we have. And of course, the overall goal is to use this to build integrative models that take into account additive and non-additive genetic elements. I'd like to acknowledge the students who did the work. So, Ali, Ariel, Boyang, Eric, and Johnson. So, they were the students who led a lot of this work. Thank you. 