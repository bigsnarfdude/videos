Talk about estimation and inference for CP tensor vector model. So, probably I can skip the introduction of tensor as we already introduced this type of slide several times. And also I will skip the common motivation examples of I all detect the data, and then remember it and you introduce much more than what I written in the slides. Written in the slides. So, my talk touches two concepts. One is tensor decomposition, and another is factor model. So, the previous talk mainly focused on topic decomposition. My talk focused on a different type of technology decomposition. So, that I would like to briefly introduce, briefly remind you the definition of CP decomposition. So, in matrix analysis. In matrix analysis, singular value decomposition is well defined. And for S V D over X V D over matrix X, F contains all the singular values and it is diagonal. So mathematically, we may write a matrix S V D in this form, and this notation we denote it to be automatically. We denote it to be outer product. There are several extensions of S V D matrix S V D to tensor decomposition. One of the popular one is CP decomposition. So CP decomposition could be graphically viewed in this figure. Compared with matrix S V D, in C P decomposition, the loading vectors U11 to UR1 are not necessarily orthogonal to each other. To each other. And if we write in a form similar to a matrix SVD, then in CP decomposition, the core tensor F is diagonal. But different from matrix SVD, the loading vectors U1, U2, U3, they are not necessarily orthogonal to each other. It could be viewed as a trade-off. And mathematically, Mathematically, formally, we may write CPD combination in this formula, and again, we use this notation to denote tensor auto product. I probably also will keep the introduction of vector factor model because we already introduced a more complicated vector model in the previous talk. So, my talk today focuses on or includes two parts. In the first part, I will introduce. In the first part, I will introduce a CP factor model for tensor observations. And in the second part, I would like to use two real examples to illustrate the proposed models. So, suppose we have p tensor-valued observations, Xt. Each observation XT could be comprised as two parts, the dynamic signal tensor MT and a noise tensor Ipsum T. The dynamic signal tensor MT is assumed to have a CP type decomposition, C P type low-rank decomposition. And just for simplicity, I also write each element of the data tensor XT. It could be written as a summation of SJT, represents factors. And this is the I1 element of the first loading. Elements of the first loading vector, I2's elements of the second loading vector, and QIT elements of the kth loading vector also class certain random noise. And for identity ability issue, we assume the loading vectors are normalized. So, in this sense, all the signals are absorbed in the factors. And again, just remind us. And again, just remind you, this notation denotes tensor auto product. Or for vectors, it is just vector auto product. And this tensor vector model, we could also write it in a form of vector vector model that vectorize the observed tensor data. And then, very similar to vector factor model, the only difference is the loading vector for vector factor model now is a vectorized. Now is a vectorization of rank 1 tensor. So it is outer product of several loading vectors across different tensor modes. And our purpose of the tensor vector model is to try to estimate the loading vectors and also the factor process. They have different interpretation. So why we want to study such models? For example, we consider the text. We consider the taxi pickup and the drop-off volume by regions so that XTIJ represents the volumes of pickup at IC region and drop-off at JC region. So in a one-factor model, in a one-factor model, then the factors Ft could be interpreted as time varying total volumes and the factor loadings A11A21. Loadings A11A21 could be interpreted as percentage of pickup and drop-off in each region. Total volumes change over time, but such factor structure does not change. We may also fit a two-factor model. Then, the two-factors may be corresponding to locals working in New York City and then the tourists. Locals go from train stations to office districts. Tourists go from Tourists go from hotels to attractions. Alternatively, the two factors may be corresponding to rush hours and business hours. In rush hours, train stations are heavy, but we don't have information on type of passengers. However, we could identify hour of the day as additional tensor mode, and this tensor mode could be used to identify. Could be used to identify rush hours and business hours. Of course, we only observe the sounds, we don't know the underlying structure. And the purpose of this talk is to try to identify the latent structure beyond the observed tensor data. So now if methods to estimate the loading vector Estimate the loading vector is we first construct a spiked covariance tensor. The spiked covariance tensor could be viewed as an extension of the spiked covariance matrix to tensor data. And the spiked covariance tensor could be decomposed into two terms. One term has a LORAC CP decomposition related to related to the Related to the CP factors, and another term includes a lot of random noise, such as noise by noise cross-product or factor by noise product. And we could use spike torn tensor to estimate the loading vectors AJ1, AJ2, until AJK. However, JT. However, even in the rank 1 case, even in the rank 1 case, the best CP approximation is MP hard, according to the seminar paper by Hiland Lin. And a lot of researchers working on such problems to see under what kind of conditions this problem becomes manageable. So we would like to impose certain conditions to make such problems. To make such problems manageable and obtain the estimation of the factor-loading vectors. So, in the literature, this is probably the most widely used literature. In the literature, people use random projection to figure out the CP basis for a low-rank CP decomposition. So, let's consider a very tour example to illustrate the toy example to illustrate the basic idea. So we focus on the matrix observations. Suppose capital K equals 2. And then for matrix observations, the coverse tensor Y is a noble 4 tensor. Their idea is to use a random projection B. Usually this random projection B is ID Gaussian vector. And then to collapse the observed and matrix data to be a vector. Matrix data to be a vector. And equivalently, such random projection B could be used to collapse the covariance tensor and format a matrix. By projecting the covariance tensor by random projection B on the second and fourth tensor mode, we will have such formula. And this formula very look this formula looks like matrix S V D. Matrix SVD. But it is different from matrix SVD. In matrix SVD, the vectors A11 to AR1 are orthogonal. In our model, A11 to AR1 are not necessarily orthogonal to each other. So if we force to use SVD to estimate A11 to AR1, then there exists a bias, and this bias is irreducible. There are two major drawbacks. There are two major drawbacks of this procedure. First is signal cancellation. Because of the random projection, the signals could be very small. And the second issue is the bias induced by the collinearity among the factor loading vectors. This is irreducible and it is called coherence in the literature. So we would like to improve a better. Improve a better, we would like to provide a better estimation method. And this random projection method is still useful, but it will be used under certain conditions. So the first method we propose for initialization is called composite PCA. The basic idea is we first vectorize the observed tensor data. Observer tensor data. If we vectorize observer tensor data, then it could be written in a form of vector factor model, as I mentioned before. And then for the vectorized data, we could construct the covariance matrix. And this covariance matrix could be written in a form, in this form, and this form looks like matrix S V T plus certain noise. Plus certain noise, and in the center, this theta matrix is the coerced matrix of the factor process. And then, similar to before, although all these A1, 2, AR, these vectors are not orthogonal, we still could use matrix S V D to estimate them. So, in the first case, we assume the eigenvalues of the population core. State of the population coins, the eigenvalue of the population factor process are different under such condition, then we apply matrix S V D to the Coren's matrix Y star and compute the top R eigenvectors. Each individual eigenvector could be treated as an each individual eigenvector A G could be treated as eigenvector Aj could be treated as an estimate of Aj. The procedure works because we assume the eigenvalues of theta are different. If the eigenvalue of theta are the same, we will introduce a different method where we discussed later. So the benefits of our procedure compared to the previous random projection method is we method is we do SVD on A1 to AR and although all these vectors A1 to AR are still not necessarily orthogonal to each other but because of this formula we figure out they are more orthogonal to the individual loading vectors A11 to AR1. Later I will use a toy example to illustrate use a toy example to illustrate why the also why the collinearity among A1 to AR are significantly smaller. Our population vector AG could be treated as an unfolding of a rank 1 tensor. The estimated eigenvectors Ag had, however, it is no longer However, it is no longer a rank one tensor. So we need to use matrix S E D again to estimate each individual eigenvectors. So the method is we the Aj hat is an estimate of the rank one tensor Aj. What we do is we first formulate the estimated loading vector Aj hat to be a tensor. Tensor and this tensor is close to a rank one tensor. Then we do matrixizing each tensor mode and then do left singular and then do matrix singular value decomposition of 10 the left singular vectors to be an estimate of the loading vectors. This is a very nice initialization. Initialization for loading vectors. So now let me introduce, let me briefly illustrate the basic idea behind the composite PCA. Again, to fix the idea, we consider a toy example. We consider matrix observations, and for simplicity, we fix the number of factors to be two. Then the Then the vector A1, A2, we just introduced the long vector, becomes a D1, D2-dimensional long vector. It is a vectorization of A11 outer product A12 and could be expressed in this formula. So we observe the collinearity or coherence among our initialization is A1 transpose A2. This is the collinearity among the vectors A1. Among the vectors A1 to AR, and it could be expressed in terms of the product of two terms. And the random projection methods in the literature, their collinearity is just one term. So in this sense, our CPC initialization boosts weak orthogonality among A1K to A2K to strong orthogonality among A1 to A2K. A1 to A2. And because of the weaker orthogonality, the bias becomes much smaller. What if the eigenvalues of the factor process are the same? Then we could not have theoretically guaranteed methods for composite PCA, although in practice we find that this performs very well. This performs our will, but theoretically, we could not prove it. So, what we do is we still adopt the idea of random projection. We want to use random projection to create sufficient eigengaph between the largest eigenvalue and the second largest eigenvalue. So, the basic idea is suppose we constructed the coherence tensor y. This is based on the This is based on the original tensor data. And then we obtain a lot of ID normal vectors. Each element in the projection vectors are ID standard normal. And then the covariance tensor y is projected by the random projection, by the random projection at the first and the k plus ones. The first and the k plus one tensor mode. So we choose first and the first and the k plus one tensor mode just for simplicity. We can choose whatever tensor mode as long as we make sure it to be a symmetric matrix, a symmetric tensor type of six. And after projection the current tensor by the first and the k plus one tensor mode, then we do a materialization. This is This is a tenth of order 2k minus 2. And then we unfold it into a square matrix. And this square matrix could be written, mathematically could be written in this formula. Ignore the noise term. It is also very similar to the matrix SBD form. The only difference is we don't know what the star represents. What the star represents is very complicated. I will discuss later. And then the loading vectors is A1 minus 1, AR minus 1 are defined in this way. They are vectorization of the factor loading vectors from the second tensor mode to the case tensor mode. We exclude the first tensor mode because the first tensor mode is projected by the random. Ejected by the Gaussian random vector, Gaussian random matrix or Gaussian random vectors. And then I will illustrate it later. And then using this method, we will create a sufficiently large eigengap between the top eigenvalues and the second largest eigenvalues, so that we can use matrix S V D to obtain the top eigenvectors. Top eigenvectors. The top eigenvectors is what we are interested in. The population version, Aj minus 1, is a vectorization of Ajk from the second to the case tensor mode. So after obtaining A1 minus 1 hat, it could be treated as an estimate of this one for certain factor J. And then we would like to use the composite PCA method as I introduced before to our introduced before to obtain each individual eigenvectors each individual eigenvectors and the obtained eigenvectors could be treated as a candidate of the estimation of the population eigenvectors. However, we did a lot of random projections. The number of random projections should be at least at order of d square. So it is very large. We did a lot of random projections. We did a lot of random projections. Among those random projections, we need to do clustering type of methods to figure out certain centers. And the R centers could be used to represent the estimate of the vector loading vectors. So this method is very complicated just because the situation, the population, the covariance matrix of vector process had the same eigenvalue. Process had the same eigenvalues. So, why our methods work? So, I briefly introduce the mathematical idea behind the random projection. Intuitively, after projecting the coron's tensor by the first and k plus ones tensor mode, we could write the projected matrix in this formula. And if we are interested. And if we are interested in estimated the factor loading factor for the r factor, then we could decompose the signal part into two parts. The first part, we subtract the effect of the r factor. We subtract the effect of the r factor. And then the rest part is the remaining term. After subtract the first factor. Subtract the first factor. After subtract our spectrum, and this is random noise, random noise is not very interested in. So because of the random projection, GJ, so among the R-spector, we have a factor. This is factor loading. This is the random projection vector. This vector could be used to either enlarge or To either enlarge or make the signal smaller. And our procedure are based on a very nice property of Gaussian distribution. For Gaussian distribution, uncorrelated means independence. So we use Gaussian random projections to create independence. Intuitively, this vector, this vector is created by Is created by random projection gj. It could be rewritten in this formula. So this circle with a null represents a chronicle product. So Ai1 chronic product Ai1 transpose a Gaussian matrix Dj is equivalent to this one. And then the second term, the second term is orthogonal to the first term because the correlation The correlation are zero. The correlation of these two terms are zero, and GL is a Gaussian vector. So zero correlation means independence. And this term is the leading effects of the second term. Although there are still some biased terms, but the biased term could be negligible. And then, because of the intuition, this term and the This term and the leading term of this term are nearly orthogonal, then we could choose a lot of random projections. If we choose a lot of random projections, maximum of Gaussian will increase with the number of maximum. So because with high probability, maximum of so many Gaussians will be proportional to square root of 2 log L. In total, we have L. In total, we have L Gaussi. So, this is the basic fact for Gaussian distribution. And then, and then, if we identify a L star, a projection L star, which maximize this term, if we identify an L star which maximizes this term, then because they are independent, we substitute L star into the second term. The second term is the second term is independent of the L star. So choosing whatever L, as long as we figure out the L star, the second term is irrelevant to the first term. So the noise term, so the noise term of the second term will not increase with S L increases. And intuitively, the noise term is proportional to the number of factors. Proportional to the number of factors. So, based on the property of Gaussian distribution, it is proportional to the square root of 4 dot R. And so that we could create a sufficiently large eigengap, increase the number of projection L, the top eigenvalues will increase with L. But the second largest eigenvalues will not increase. Eigen values will not increase with number of projection increases. Of course, we could still need to control the bias and need to control the noise term, but mathematically they are negligible terms. However, this procedure also had a restriction. So the procedure, we could not allow too large an eigengap between theta. Between theta. But as I mentioned before, we are only focused on the case, the eigenvalues of theta are the same. So the restriction of random projection methods has been excluded. That's very nice thing. And later, I will illustrate the theoretical properties beyond this procedure. So right now, we only introduce the initialization. And for tensor problems, they are empty hard. The problem they are empty hard. They are empty hard. So, initialization is also the most important. If we have a good initialization, and then we can refine the initialization with many well-established local algorithms, such as gradient descent, coordinate descent algorithm, to refine the initialization, to make sure the final estimate will converge to a stationary point. To a stationary point close to the global optimum. So we propose an iterative refinement method called iterative simultaneous oxalization. This method is very similar to tensor power iteration. We use one initialization as introduced before, and our idea is to isolate each individual eigenvector. And in the ideal case, it will become a rank-on vector model. Factor model, and because it will reduce to a rank one factor model, so we could eliminate the coherence issue, the bias issue. So, let's consider the idea case to fix the basic idea behind our iterative procedure. So, in our factor model, the loading vectors A1, K2, AR. vectors A1K to ARK, they are not necessarily orthogonal to each other. So we first construct a matrix BK. This matrix BK is the right inverse of AK. And then intuitively, D1K could be treated as the residuals of loading vector A1K projected onto the column space expanded by A2K to ARK and then normalized and then Then normalize and then do certain normalization of B1K. And mathematically, it satisfies A1K transpose B1K equal to identity matrix. So then we consider a toy example to fix, to illustrate the basic idea behind the iterative production methods. So consider a toy example with capital K equals 3 means we first equal to 3 means we focus on 3 V times times 3 and we focus on the case rank of the vectors equal to 2 and then suppose we have the true V vectors Bj2 and Bj3 and our purpose is to estimate the loading vectors AJ1. Then what we want to do, what we do is, okay, so first we can rewrite the tensor-tuxer model based on this assumption. On this assumption, capital K equal to 3 and RAM R equal to 2, the T11 is constructed by the original tensor data projected by the two E vectors, V12 at the second mode and V13 at the third tensor mode. So graphically, it could be wheeled in this figure, and then mathematically, it could be treated in this way, and because of the definition of B vector, this tool. Of E vector, these two terms are 1 and these two terms are 0. So we eliminate the irrelevant factors. And the G11 could be written in this formula. Because of this formula, the signal parts contain in the factors. So I probably need to, this is the slide I used before. So in the current slide, So in the current slide, I observed the effect of W1 into F1T. So you could treat W1, F1T to be the factors absorb the factor strength. So the factor strength does not change after we're doing such projections. And the loading vectors A11, what we want to estimate, also does not change. But the noise comes from the noise. But the noise contents in the vector model has a much smaller dimension. It has only dimension 1. So under many assumptions, the noise is significantly reduced. And in this sense, we keep the signal unchanged, but reduce the noise, enlarge the signal-to-noise ratio, so that intuitively we could obtain much better estimation. So in practice, we don't, of course, in practice, we don't know the true loading vectors, so the true B vectors. So what we need to do is we need to estimate the B vectors based on the previous estimation of the loading vectors, loading matrix AK, and then we could construct the estimated Z vectors, and we construct And we construct a coherent matrix and use a matrix S V D or eigen decombidation to obtain an estimate of the loading vector aging k-pat. And the algorithm will converge after sufficient iterations. So now let's talk about theoretical analysis. In a theoretical analysis, for simplicity, we focus on the case the noise tensor are independent. Of course, it could be Independent. Of course, it could be extended to the situation. The noise tensor are seriously correlated, but if we assume independent noise tensor, it will significantly simplify our select analysis. And we also assume the eigenvalue of the noise is upper and lower bounded. This is a very common assumption in the literature of fact analysis. And we also assume, so we assume that. So we assume that noise follows from a linear model and the tail of the linear model follows from a generalized sub-exponential tail. This is a very common assumption in the literature. We also assume the factor process has generalized exponential type of tails. And of course, for factors, we can no longer assume they are independent. So we adopt. They are independent. So we adopt FA mixing process for the factor process. And in terms of this formula, because of the unknown signal strength, we normalize the factors. Now let's move to the theoretical analysis. Let's first discuss the convergence rates for the initialization. Probably, this is the most important part because for non-convex optimization problem, this is the most Optimization property. This is the most challenging part. So we establish convergence rates of the initialization for two different scenarios. The first scenario is we have different eigenvalues for the population coherence matrix of the factor process, or if the population matrix if the eigenvalues of the population coherence matrix of factors have the same order. Of vectors has the same order and the eigengap is zero. So, in the first situation, if the eigengap is not zero, we could establish the sine theta distance of the estimated loading vector and the true loading vector. Intuitively, this means the angle of the estimated loading vector and the true loading vector. And for a vector, this scientific distance could also be written using. Could also be written in this formula. And it includes two parts. The first part is induced by the bias, by the non-orthogonality among the loading vectors. The second part is the stochastical error. And the bias is significantly reduced because it is related to delta, and delta is defined as the product of the bias for each. The bias for each tensor mode. So delta K is the bias for one tensor mode K, and delta is the product of delta K. So it is much smaller than the bias for each individual tensor mode. However, if we have zero eigengap, if we need to reduce to random projection methods, then we have much weaker results. This bond is much worse. Bound is much worse than the previous bound. But still, we could guarantee the angle of these two vectors are smaller than 90 degree under certain conditions. And in this bound, the stochastic error, the side 0, is dependent on the total dimension, d1 to dk. And after using the iterative procedure, we will find. Using the iterative procedure, we will find this could be significantly improved. Another issue I want to illustrate is in the stochastical arrow, it includes a term, one of lambda one. This term is induced by the population coherence matrix of the noise. If the population coherence of the noise is an identity matrix, like we assume. Identity matrix, like we assume the noise tensor is the ID, then this term could be eliminated because an identity matrix will not affect the eigenstates. But in general, because of the arbitrary of the noise, then we need to include this term. After iteration, we will achieve much better statistical rate. So, compared the So compared with the previous rate for initialization, the bias term, the bias term related to non-arthagonality among the loading vectors are eliminated. And we also find the stochastic error term is much improved. Previously, the stochastic error term is dependent on the total dimension d. Now, it only depends on the maximum dimension of d t. And if we ignore this term, And if we ignore this term, if we ignore one of them, actually we can prove the first two terms are also meaning max optimum. So we also figure out this term, one of lambda r, could not be improved because this term is related to the population coherence matrix of the noise tensor. And such information. Such information cannot be improved. We also compare our reads with the strong factor model. In the strong factor model, the eigenvalues actually are proportional to dimension D. Then we could simplify the final statistical bond into this order. It could be, it will be square root of dmax, the maximum dimension, the maximum of dk divided by d and By D and total sample size T plus one of T. So the second term is the same as the strong factor model in Ju Shan Bai's work. The first term is significantly improved because we utilize the tensor structure. Now let's move to the asymptotic distributions. So a lot of researchers, especially A lot of researchers, especially in economics, are interested in the asymptotic distribution of estimated factor loading vectors. So we could do teleexpansion of the estimated loading vectors and figure out the leading term. So this is the leading term of the estimated eigenvectors, and this is an negligible term. So we use the, there are a lot of papers discussing title expansion of singular. Tyler expansion of singular space. So we adopted the method developed by Dong Xia. And also, I think this tailored expansion, in some sense, answers Tenghui's question. How could we establish a syntactic distribution for singular vectors? So it is in some sense related to that question. But under the factor model situation, so the title expansion will be much more involved. Will be much more involved. And then, based on such head expansion, we could establish asynchronous distribution based on this. The asynchronous distribution is just the async distribution of the leading term. If if the uh noise term is negligible, the noise term is negligible when t divided by dk of theta ri goes to zero. Of zeta i goes to zero. And of course, just for illustration purpose, on the strong factor model, theta ri is the factor strength. The factor strength will be proportional to d and the CLT will be satisfied under the condition time t divided by d square goes to zero. So this results is proportional to the results in. To the results in Question's econometric papers. Then we do some naive simulation studies to illustrate the proposed CLT. So we consider toy examples, we set the number of factors to be three. We also set the non-orthogonality among the loading vectors to be there. The loading vectors to be their 0.2. So 0.2 is a very large number, means there are huge orthogonality among the vector loading vectors. And then we assume matrix times 3s have dimension D1 equal to D2. We consider three different cases. The first is a 20 by 20 matrix times 3s. The second is 60 by 60. 60 by 60 matrix times 3's, and the third is 100 by 100 matrix times 3s. And we consider different linear combinations of the estimated loading vectors. So we consider the distribution of u transpose the estimated vector loading vectors. The first linear combination coefficient that we consider is even a space. is evenly distributed across all D1 dimensions. The second U vector we consider is we just focus on the first element of the estimated factor loading vectors. And U2 means we just focus on the second element of the estimated factor loading vectors. And all these figures are normalized to make sure the To make sure the wireless to be the virus are standardized and all these figures look very nice, very close to standard normal. Okay. And also this is very, the queue plot is also very nice. Now let's move to the theoretical analysis. So we first consider the empirical application of characteristics, density prophilius, and the response. Profit failures and the response is YTJL, which is excessive return of J's characteristics at different times. And then our observations are monthly portfolio returns into a 10 by 10 matrix time series. And this is the observed matrix time series. Then, using some generalized eigenvalues. Using some generalized eigenvalue methods, we could select three as a number of factors. We compare different methods. The first method that we consider is our method. We focus on contemporary quarantine matrix. And we also use the same methods with auto-coverance matrix. And then the third method, TPCA, is just do tensor PCA for the observed tensor data. The third method is generalized eigen ratio-based eigen. Ratio-based eigen proposed by Jingwei and Qiwei Yao. And also, the last method is: we also want to compare our method with talker-type vector models. Because talker vector model usually is much larger classes than the CP vector model. This is the R-square of different methods. It could be one thing I missed is for target factor model, we use For talker factor model, we use three by three talker factors. And in Cp factor model, we focus on number factor three. So in this case, the Cp factor models are a subset of the toggle factor model. And we find the toggle factor model has a best R-square, but the improvement over our proposed R method is not that much. And it improves. That much. And it improves, our method improves a lot over TPC and the generalized analysis method by Xi Wake. And then we also do rolling squares. The basic idea is we want to see the stability of our model. So we focus on 10-year rolling window. For each 10-year rolling window, we estimate the factor model and calculate the rolling window. Calculated the rolling R-squared. The first figure concentrated on the results of TPCA and our proposed method based on contemporary coherence matrix and the Tucker-based method. It could be our proposed method based on contemporary coherence dominate talker method under a certain period. Under certain periods, like the period before the beginning period of 1990s, and then the period after probably 2008. And of course, it's much better than the Toker methods. In terms of these two methods, we find the generalized eigenvalue method proposed by Chi Wei and our method use autocorrelation matrix. We find it fluctuations a lot. So it is not. A lot, so it is not that stable. Intuitively, it could be the time series, the autocurrency matrix are not that stable across time. So that it performs very bad compared with the previous three one. Okay, so the last one I want to introduce international tradefolio network analysis. So the observation. So, double derivations are monthly import-export volumes across 24 countries and regions. And we again fit CP factor model and toggle factor model. For CP factor model, we select six CP factors. For toggle factor model, the rank is 4 by 4. So, because the rank of the toggle factor model is 4 by 4. So, in this case, toggle factor model is no longer. Toker factor model is no longer a subset of CP factor model. And we figure out: okay, the R-square of Tokyo factor model are much worse than the proposed CP factor model. This illustrated probably in the fifth and sixth CP factors contains information beyond the top 4x4 cotensor factor process in Talker model. And of course, the estimated R-square are much better. Are much better than generalized eigen analysis method in the literature. And I don't know, we don't know why the GPCA method in the literature performs so badly. So now, so the last thing I want to illustrate is the treating volume network. So we focus on five-year sampling period. For each five-year sampling period, we fit the CP factor. Factor. And we fit the CP factor model and then estimate the CP factors and the factor loadings. This could be used to illustrate core international trade goals over time. The size of the circle represents the total volumes of each factor. And those countries are ranked by the trading volumes. And the left represents the export. The export volumes and the right column represent import volumes. So 1995, this means we consider a sampling period between 1993 to 1997. And we find, okay, during this period, United States is the most, United States has the highest United States has the highest import, export, and import trading volumes. And it concentrates a lot on the first, second, third, and fourth factors. And China just concentrates a lot on the third, ten factors.