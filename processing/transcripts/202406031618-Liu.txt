Like the days. Which is very hungry. Sorry, can we be one room, please? Sorry, sorry. Excuse me. Sorry. Apologies, wait. Apologies. Can we be one room, please? Okay, so as the panelists, I get to ask one question. It's going to be an easy one, and then we'll open it up to everyone else. And then we'll open it up to everyone else. So, I think something that I noticed with Dean and all your conversations was this notion of framing and making more concrete the way people interact with these systems. So, like, Jessica, you were sort of talking about like framing the problem and being very specific about how the problem is sort of perceived and understood. You were mentioning sort of like framing the assessment of these. Of these, of these, of the evaluation metrics and making that very clear to the decision maker. I have like two questions. So, one is like, you know, do you feel like this requires, because Jessica, you were sort of calling for like modeling human decision, like human decision makers as a way to, I guess, like capture their formalization or how they're conceiving of, you know, the task, the decision task. The decision task, um, and you're just you're you were discussing sort of like throwing away past metrics and coming up with new metrics. I'm curious, I also think Ben's might have to do with framing as well. Oh, okay, hope so. Oh, yeah, no, well, I guess that also did have to do with sort of the hero, hero versus like different types of questions that are more suitable versus not suitable for human decision makers. Yeah, but what you're evaluating, I thought this was funny. Yeah, you're ultimately evaluating for yourself. You're ultimately evaluating the expected score. Yeah. As soon as you frame your evaluation metric as the regret, that automatically turns the statistics. Yeah, you're right, actually. Yeah, right. So we frame things in such a way that these things look better. It's less important. Statistical bureaucracy is what I would say. Yeah. But I guess, do you guys all believe that this tension between clinical and statistical decisions is really just a framing? Statistical decisions is really just a framing problem in the sense of like humans make decisions and there's sort of implicit reasoning behind how they make decisions. It's just like the real task is just to present to them the task, like, you know, like frame the decision or formalize the decision in such a way that we can start working with it and we can start understanding it. I don't know if that makes sense, but I think that that's sort of like something that I'm curious to hear more. That I'm curious to hear more. And then, related to this, is also: none of you guys talked about incentives of the decision makers because over lunch, I had a very interdisciplinary table of folks with backgrounds. And like, you know, like Talia was there talking about lending, Monki was there talking about education. And, you know, I was sort of like, oh, like, you know, in healthcare, the decision maker is very aligned with the impact of population. Maybe, who knows, you might have a different view. You might have a different view. But I like, you know, in general, like, you know, doctors make decisions sort of on behalf of their patients, and ideally, they want their patient to live and their patient wants to live. But in other settings, you know, criminal justice, education, a teacher might make a decision, it affects a student, and they might be less aligned, or like, you know, a judge might make a decision, and the defendant might want a different outcome, but the judge's priorities are sort of, or it's. Like the judges' priorities or sort of, or incentives are very different. And so, yeah, I was curious: like, since you all talked about how to frame decisions and how to formalize decisions and how that was such a key factor of like our misunderstanding of the way people make these decisions or how we assess the role of risk assessments in decision making. I'm curious if you guys have reflected on the different types of decision makers and their different incentives. That was my point: with like, you have to have a well-defined. My point was like you have to have a well-defined problem, like that includes a scoring rule. I think often we are trying to evaluate human decisions, like with models, and we're not either giving people a scoring rule, like if it's a study that we're running ourselves, we're not actually, like we might give them one scoring role and evaluate them with another one, or if it's like, you know, we have some observational data, I think we're often assuming some scoring role that we're going to use to evaluate things, but it's not necessarily what they're using. And I think that, I mean, you just can't, you can't. I mean, you just can't assess people under one. When you say scoring rule, you mean like metric of success? Just metric of success is a way that you're assigning to different disciplines. And even the experimenter might have a different metric of success. Utility positions. So I think it's extremely important, and I think we have to invest. I mean, if we're trying to actually study decisions in the wild, we have to invest in understanding. We have to invest in understanding how much heterogeneity is there in different decision makers and how they're making those. What appears to be how they're weighing different errors. Otherwise, we're going to draw the wrong conclusions about how well humans are doing this. So, do you think all decision makers are equal just about how much they're weighing different factors? No, I think you, yeah, I mean, I think that's like a normative question. I mean, should they, like, in a medical setting, are there cases where you know, we want. Are there cases where you know we want to give everyone like we want everyone to be on the same page? I think like in a lot of cases I would not expect everyone to be using the same rule but if we're just ignoring that piece when we're trying to evaluate humans making decisions like we're not going to get anywhere so so I think it's like an open question you know like whether in the actual real world decision context people should all be using the same rule like that's I'm just saying like to understand what people are doing yeah there has to be some consistency there. There has to be some consistency there. You have to evaluate them against the rule that they're actually using. I mean, yeah, the problem formulation is like 99% of the problem, and then the rest is just there. I think most problems. Problem formulation is the problem. Well, it's subjective. There's no concrete real thing. I guess what I've started the talk by saying that I think history performs a lot of how we classically formulate things. Of course, other things do as well. I think there are sometimes disincentives. I think there are sometimes disincentives to change, and that could be other forces as well, capitalistic, whatever. But, like, there are, yeah, I think problem formulation is kind of key here. I'll let Ben also chime in. I'll just disagree with you. Okay. I love that. I was supposed to get asked. I thought you're not a controversy. Oh, no, yeah, no. I actually strongly disagree. I think something I wasn't able to. So I was defending meal. We're talking about meal. I don't actually agree with you about. I don't actually agree with you about everything that he says. He became very dogmatic about this distinction. Almost said that he would rather, I mean, to be fair, he had Kaiser. I might agree with him that he would rather have all of his medical decisions done by an actuary than the actual physicians. But I think it depends on your healthcare setting. He will make that decision too. I think the issue that we have is that the statistical decisions are all coming from a big bureaucratic system, a big bureaucratic governmental system. Bureaucratic governmental system. I think the only time we even care about the statistical decisions are like in a hospital, they want to cover their ass for insurance claims or want to make sure that everything gets billed. This is when we start talking about incentives. I think incentives are bullshit unless you're an administrator. But they shape the framing. They shape the framing of the problems. If you look at actual decisions that people make, which we make all the time, we don't know what the decisions are solving. There are plenty of decisions that I can make frame. I couldn't tell you what the problem we're solving is. Like, I take my I take, my plane takes off from Guardia Airport. I hit two birds, both of my enemies go, how do I land it in the like? You're not going to come up with a statistical framework to figure out how that guy landed in the Hudson. Well, no, I feel like that system, for medical systems, people feel these things are very unjust. But let's start with that. I think that this is one of the biggest issues that this, whatever decision theory community. What our decision theory community has stumbled upon is that there's like these like institutional decisions. We can talk about those. And maybe we can just focus. Let's talk about institutional decisions. But there's like decisions that we do all the time. There are all sorts of other things that people do way better than computers ever can. Like decisions of what I'm going to do when I see a ball drive roll into the road or like a kid run after the ball on the road. Like self-driving cars. There's decisions after decision after decision. You can't make decisions. And so that distinction I think is really. Distinction, I think it's really important for us to suss out what we want to talk about. So when we talk about incentives and populations and justice, justice is funny because justice, I think you could have, that's another funny one where you could have justice between you and me. You can have a sense of justice in our own relationship. But then there's a lot of time when we talk about justice, we're talking about justice from a government and a country. It's more institutional. I don't think that we're actually disagreeing. I think when people don't like, like, when there's a problem. Like when they, when there's a problem with, or they think there's something to be solved, that's where I think problem formulation kind of the thrust of it. That's where people disagree. It's not about like statistical or what other kinds of decisions. How did you formulate this problem? Do I think there's a problem with it? So, Aisha, just to push back, I do think you disagree. I was just going to say, because at the end of your presentation, you talk about the sort of cost-benefit analysis and how that's really. And how that's really how people are making decisions in the medical setting. Don't you think that there's sort of, in the American medical system, financial incentives for people to think that way versus other systems make decisions this way. It's not just American. Not to be anti-American. And I'm just like, pro-honesty. If you're making decisions this way, then be transparent about it. Now, maybe we shouldn't be making decisions this way, but I feel like showing me this courage. Like, showing me this curve, this is not actually, like, I asked someone how they went from this curve, and they basically turn it into an economics curve and then use that to actually discern which thing to use. It's just like, well, then do that, present that result. I don't know. I think it was just more very convoluted and trying to bury the lead. It's like, if you're going to be economic and bureaucratic about it, then just be transparent about that. I'm not saying it's the right thing to do. I just think that's. Oh, no, no. Yeah. I'm just saying, sort of, like, do you feel like there's And I'm just wondering, sort of, like, do you feel like there's a way in which the incentives of the decision makers sort of expressed in that framing that you kind of landed in, where it ended up being this cost-benefit analysis? I think that's like trying to get a hundred percent. Okay, open field. We'll start with Ben because he's up with Brown. Any chance then, you know, incentivizing this. So, first of all, I love the provocations, and maybe I think there might be some. And maybe, I think there might be some relation to Depp's first question, but this relates to Asia's and Jessica's talk, I think, and maybe the connection between them, which is, like, Asia, I was wondering, like, whether this is about what humans understand and thinking about how to communicate to humans statistical insights in a way that enables them, empowers them to make better decisions, or whether, because to me, like. Or whether, because to me, like where you ended up, which was similar to Jessica, like sort of attaching, committing to a utility function and attaching a cost to each of the four quadrants of a confusion matrix or something like this. That to me is like you're choosing a direction in some Pareto space, and then you're just like finding a point at the ROC curve. So you end up somewhere on an RC. So, like, you end up somewhere on an ROC curve, but maybe that isn't the best way to communicate to doctors what you're doing. But, yeah, I guess I'm still like. Yeah, but like I said, it's about transparency. Like, I'm not, I don't think that there is a good way to summarize all this information. Like, integrating under the curve is obviously reporting one number is not going to capture the full thing. But all I am saying is, if at the end of the day, you're making a decision based on costs. Decision based on costs, then you should actually state that, and that should be actually how you're reporting these things. But there's something also nice about saying, look for any weighting of costs, like between false positive and false negative. One algorithm beats the other. I know that isn't always the case with ROC curves, but sometimes you get this nice thing where you can say, I don't even know how we. Just to follow up on real quick, just jump in and ask to that. Quick, just jump in and ask, add to that. Also, do you ever come across how you determine the AUC of an individual person? I mean, I don't think you can. For that other one, if one curve dominates the other, it will do so in almost any format, precision in the Switcherson plot, and it's just better. And so, like, there's no controversy there. Most curves cross at least once, and that's where I think it becomes, like, how do you assess which area of this is actually more important to your relevant. This is actually more important to your relevant decision-making circumstance. And I think there are easier, better ways, more transparent ways to express that and communicate that. But I feel like we've worried a lot of medical papers, they don't report a lot of things and you're left to infer, like, I don't know how this did all different subgroups. I don't know how this did, like, it's just an AUC, we've just gotten too lazy, I think, in reporting and conveying information in medical settings when it comes to motivating your need. To pull the main charm to the answer. Okay, we're going to move on to other questions very quickly. Go ahead, Stretch. So it's a question that came to my mind when you were talking about the framing, but I think it's applied. I'd love to hear what the panel thinks about this. So I know we managed to survive till 4.30 without mentioning LLMs, but I'm going to do that in a minute. Oh, no, no, no, sorry. I promise I won't tout that from the table. One of the arguments, one of the big discussion by Alison Gropnick and Nalvi Mitchell, like this came out two days ago, talking about, you know, what does it mean to do concept formation in humans? What is it that we do when we are coming up with concepts to understand the world, how to interact with the world, and how we predict what's going to happen in the world? And they were trying to make Right, and they were trying to make the argument about what LLMs don't seem to be able to do regarding that. But the question, I think, for me was: when you were talking about this, you talked about, you know, are we better than statistical prediction machines? And when you talk to a doctor, again, I'm not a doctor, but I would hope that when a doctor is looking at the symptoms I am presenting with, they have some mental model of human sort of physiology or anatomy or whatever the particular, or, you know, or my bone structure, whatever. They have some model, you know, maybe not a physical. Model, you know, maybe not a physical model, but some kind of model of the world, and they're basing their assumptions, their predictions for me, not just based on statistics, but on some kind of conceptual model of the world that drives how they think things are going to play out. They have some kind of counterfactual causal model or whatever you want to call it that. And that seems to be fundamentally, that seems to be non-statistical in nature, right? Or is it not? And I guess my question is: is the fact that we seem to have conceptual models of the world that we use to interact the world non-statistical, or do you think that is still statistical? Or do you think that it's still statistical but of a different kind? I think the statistical thing is just funny how we, I think it's pattern recognition of a different kind. Okay, just a little bit. And, you know, I think there's a lot of fascinating psychology work on what exactly people are doing, making decisions, especially for high-stress scenarios. And yeah, a lot of it is playing through in your head what might happen and looking for anomalies and relying on past stuff and you. Relying on past stuff, and people will just say, I just saw it. And I think having read enough of that literature on heuristic formation and human causal explanation, just to see that it's like no one, myself included, has been able to take what I see in that naturalistic decision-making literature and turn it into something that a computer can do. Me implies it's not statistical. I think it's kind of, I think of it as like pseudo- I think it's kind of I think of it as like pseudo statistical in that like I mean there's often some compression of experience and we know that people are bad at that in lots of ways if we compare them and evaluate them against statistical processes. But I do think there's like interesting things that people can do that are hard to formalize. So yeah, I guess the thing I always come back to is that most of these formulations, I always get stuck in the post-war period. Post-war period. That's where most of these formulations happened. The number of algorithms that we all in here use today that were invented between 1945 and 1950 is 95% of what we do. And then us figuring out how to map those things back onto people is this bizarre thing that happened in the 50s. So we first decided what computational rationality was. We defined that. And then we were like, well, how are people? People clearly aren't rational because I just showed you what the perfect rationality. And we've been dealing with that misconception. With that misconception, that we built an improper model of rationality and then try to shoehorn everything into that. But I think we still always need a model to check against. And I think that's what's tricky is that even if we think people are doing different things, it's very hard to get insight into what they're doing without some comparison point that we can understand. That counts. I think that's why I like within the decision theory stuff, this naturalistic stuff has never really, it's never been in close dialogue with. Been in close dialogue with the sort of more statistical stuff because there's like we can create things that are just like fundamentally we can't understand, but then it's hard to reconcile or move forward from that. Are you saying it has to be a discrete decision in order for like those kind of methods to work? I guess what do you mean discrete decision? It could be a prediction of some type. I mean to analyze things we have to discretize actually at some point. Oh like in terms of a decision maker's actions or no it doesn't have to. I mean it could be a probabilistic I mean, it could be a probabilistic forecast that we're evaluating them on. Yeah, so I guess my question is just trying to get at the mechanism that leads prediction models to do better than humans. So I'm interested in this partially because, like, many of you claim right that there's 80 plus years of just 80 plus years of just evidence showing that actor mission is giving clinical judgment every single time. Or same. Well, consistently, right? But then, like, for example, Jessica pointed out that people are actually, you know, I mean, it's just, you might observe that, but maybe some of that is just basically people are not doing tasks that they were actually doing. And so some of that might be, for example, like another possible explanation for why machine learning models seem to be. Why machine learning models seem to keep during these studies is just because people are doing people are making decisions in this adahog, haphazard manner, and then when you do a monthly model, maybe they're going through some like step-by-step process and it forces structure if you think that's what structure not immediately. And I guess, so this is kind of why I want to get into this mechanism of like what is it that is helping the answer. I think I have the answer, and I think it's weird. Have the answer, and I think it's weird. I know it annoys everybody, it's like annoys clinicians in particular, but I probably annoys everybody else in the room what this answer is. But it's just like you pick an evaluation metric, once you pick the evaluation metric, then there is an algorithm that maximizes the evaluation metric. And it's going to be people. That's why it always does matter. It's like I pick the evaluation metric, and what's the optimal thing to do? It's use statistical prediction. Oh, shit. So you just lose, you've lost as soon as you've picked your metric. Right, yeah, I agree. It's, yeah, we study problems where it just makes sense. Yeah, we study problems where it just makes sense that humans are going to do worse, but we actually often think that humans are valuable precisely because we think our assumptions must be wrong. And so then it's hard to ever really show in some static evaluation. But the hard part is that as soon as we patch them, well, see, we forgot about this one feature or this one thing, we put that in there, and then the computers do better again. Right. So it's always like we're going to, as soon as you find the corner case, put them, for example, how to set up. But then for example, how does that get into this framework where, oh, like decision maker has some additional information that we haven't encoded, it's not even the data. So now you can formalize it and you can optimize for that, but clearly that's some optimal root part to all of the information needed for that. I think humans can improve over models. You can get a human with a model doing better than the model alone. I think that's totally feasible. It happens a lot. So I think that's the weird thing about models. Neil talks about this, the idea that somehow there's this other thing. The person's coming to the thing, you can see a little bit more. So, his first thing, Neil's first point is he would like to say that he believes. is he would like to say that he believes that the, and I think this is true, the statistical thing is going to do better if the human and the statistics are constrained to the same information. That's the first thing. Then there's this argument that clinicians can bring extra pieces of judgment to the table that can actually inform place. And he says that if those things were really happening that frequently, one, you could add the information ways, and also you would see it in the statistical cases. So it's complicated and frustrating. It's complicated and clustering. Okay, sorry, go ahead. Yeah, this is a follow-on on Jeff's earlier question about problem formulation and formalization. So it struck me that we were kind of talking about two different stages. So one is like, how do we translate a decision into like a prediction policy model, right? So like to build the model in the first place. And then some of the other work today was about like how do we formally create a framework to evaluate how those models are doing and how interactive, which are actually sort of two distinct things that we're talking about. Things that we're talking about. And so I was thinking about a lot in these contexts in public policy, for instance, where we have a lot of multiple objectives and biggest goals, like in child welfare, we're trying to prevent maltreatment, we're trying to strengthen families, we're trying to ensure monitor placements are safe. And so then, you know, we might have a model, say, that's just predicting maltreatment. But I guess my question is: given a recognition that often we are trying to, you know, jump on multiple goals, to what extent should we, at the evaluation stage, like, you know, do those Do we need to formally sort of align what the criteria are for saying this was a good decision at both stages? Or is the choice that we make at the when we translate into a prediction task, that's exactly the basis on which we should then evaluate whether or not we produce some decisions? If in fact there's other things that we care about too. I don't know if that makes sense, but do those need to be perfectly aligned in terms of how we're going to. Yeah, to solve the problem well. I think there's cases, though, where they're just never going to be, where you can't redo everything and redefine the problem. So I was restricting myself to those because it's easier. Do you have an example share of just what you mentioned? What I just mentioned? Oh, yeah. Anywhere where you have some human and they have access to a model prediction, but you can't necessarily redesign the whole pipeline so that now you're only going to give the human certain instances in the whole. Give the human certain instances and the model certain instances. Like a lot of these medical ones with risk scores, it's like they exist and they're sometimes proprietary models, and you're not going to easily just redesign the whole thing. But I totally agree that that's a different setting than when you have access or you have the ability to redesign the prediction problem to begin with, and you probably will get a lot farther every time. And you start there, if you can. I'm a little bit skeptical. I'm a little bit skeptical of this idea that once you define the problem, statistical prediction always outperforms humans, and that's been true for over eight years. Nowadays, it's so much easier. I guess for two reasons. One, it's so much easier to write code and build things of how many posters were at this MPH thing, which makes it all the more easier to commit malpractice. I said, there's probably a lot more bad stats papers written now than there are. Written out, but there are like the word 1920. And like, second, I mean, for a lot of times, like, the truth is sort of like inherently human defined. Like, I always found these comparisons, like, human versus machine accuracy on the image net. Like, I always find that very hard to reason about. It's like, how could you beat me at a game in which I define the truth? It's like, you can't beat me at Calvin Ball. Like, how can you say that it's better at recognizing images when, at the end of the day, it's like a human decides what's in the picture. Like a human decides what's in the picture, right? Usually, I think they're not considering those types of problems, but the ground trees is actually human-defined and the kind of work Neil's talking about. Yeah, I mean, the things that he has very specific examples, but Neil's specific examples, let's be clear of what they are, is like recidivism, predicting educational outcomes, application, diagnoses, and like these are all things that were, just to point out, like, there are all things that were talked about this morning. Things that are in this 1954 book are all things that were talked about this month. And they are all things that we would say are a little bit more. I mean, again, it's 54, so he's talking about two variable regression. So simple rules came out. You should, do you want to defend the conference you enter? No, I'm not going to be able to do that. Was there statistical malpractice? Yeah, I mean, yeah. There were several models in which the base waves were like, there's, you know, five cases, and they're reporting AUCs of 0.98 so as to promote their machine learning method. It's just like, but, you know, we've lost the plot. Where's the context? It's not even on the poster that they'll be easy. You have to actually talk to them. So it's just, yeah. Go ahead, Robin. Go ahead, Maria. Sorry, in asking the panelists if all of you or any of you might want to advocate for a certain alternative to statistical prediction. They think maybe in Ben's talk, statistical prediction is kind of juxtaposed against clinical prediction, and maybe the implication there is you should. You should just leave certain decisions up to the human. So I'm not sure if there's space there to develop a more this is the alternative to sets of scope prediction that we should use inside the applications that we talked about. Or if there's an interest in proposing something like that. And I think in Aisha's provocation, we kind of talked about You kind of talked about alternatives in terms of how we measure statistical prediction and kind of developing this idea of economic value and making that more rigorous. Well, if that's the way we want it aside, then just like being rigorous about that. I'm not saying that necessarily should be the way. It might not be the objective at all. Yeah, it's a little bit different than Ben's kind of thing of whether it should be used. Thing of whether it should be used, just what else is there to do outside of statistical prediction? Me? Yeah. Well, I mean, I think that I'm, I think the only thing I'm arguing about is just that, like, there are things, there are things that just don't bend to technological solutions. What are these things? Yeah. Oh, if you're gonna do like a science-based approach to teaching jiu-jitsu. Science-based approach to teaching jiu-jitsu. I think that would be ridiculous. Honestly, I think you can take that jiu-jitsu analogy and push it into a lot of education. It also becomes a little bit different. Why education? What? Why jiu-jitsu? What's like, for those of us that are not like. I think martial arts. What's the size-based approach to teaching martial arts? What do you think? What's the part of the data, though, I think it would probably include, right? Like, if you had lots of data and lots of different teaching approaches. There are definitely a bunch of people who claim this, and I think that it's mostly not the case. So, what is unique, I guess, about the human there? Are they like somehow sensing individual students' needs? I'm just saying, you tell a professional fighter that he has his decisions, someone in a ring moves in a fight. How are they making decisions? Take that one out because I think as you start to, I think as you start to. To, I think, as you start to take decisions. I think, again, we have all these decisions that we end up talking about here that are like these population, justice, legal-ish decisions that are quantified. But then there's a whole other side of decision-making that ends up being mainline decision-making, critical decision-making, what an EMT has to do when they come onto a scheme, what a surgeon does in emergency surgery, what Does an emergency surgery? What, like, you know, people in the military have to do with something kind of elevation of threats or alerts are going up, and they're having to make a decision right now on the fly there right now. I don't think any of these things end up being about statistics. I think you can have a whole huge, huge list of things there. Go ahead, Mark. Do you think they are? So, like, I've wanted to. I didn't do residency. And a math PhD who works in healthcare said something to me once that I've never forgotten. Something to me once that I've never forgotten that was clinical training, residency, and fellowship are the application of the law of large numbers to clinical practice. Like it forces you to sample population distributions until you reach a point where you can see individual patients, see where they fall in the distribution. Like, so I just want to really push on this notion that they're not. I don't think that's how doctors. I don't think that's how doctors think. That's certainly not how any of the doctors I talk, but it's how they train. Like the training process. So Ziad is not going to quote me. I'm going to quote Siad. Yes. It's recorded, by the way. Is it? Yeah. We can verify. Siad will now verify, though, quoting him. Go ahead. The modern medical school is the greatest destroyer of intelligence that institution that we have exists. Because you take very smart people who are very high achieving and you turn them into low-falling agents. I think that's a product of the way that we've designed our system. I do think that we have to bend the curve away from, in the 50s, as I was talking about. I mean, you go into the 1930s and people were dying because they were putting antibiotics in antifreeze. They wanted a liquid suspension of antibiotics, and the way they figured they could do that was by dissolving the antibiotics in antifreeze, and then they sold out to people. And then they sold that to people. This killed 200 people and caused the formation of the FDA. So our modern FDA now exists. The FDA had to figure out how to regulate things. And I think you see this sweep post-war of this huge regulatory state that brings us to today. So we go 54 to 2024. The biggest difference is this massive regulatory state. Okay, so I'm going to go to this side and then pop to you, Brian. I still have a question. Oh, okay. I started a question at the beginning. I thought that was his question. Okay, let's go to his. Okay, let's go to the hardware. Okay, all right. Wow, that's kind. Go ahead. No, I wanted to ask a question about problem formulation. I think especially for your talk, where it's stated as important, but I want to actually make explicit, like, what does that mean? Because to me, like, we do a lot of work where we start with problems, and then we have to go define the problem. Define the problem. And I just want to make sure, like, it's not like a person sitting in a room thinking about the problem. Like, there's a great deal of user research and talking with people, observing workflows, mapping processes. Is that what's meant by rigorous problem definition? Yeah, so I'm talking specifically about we're trying to study people using model predictions. And so I think in that context, a lot of what is done is often Is done is often we have people, we try to, like, if we're doing an experiment, we try to, you know, somehow get some like simulacrum of like what we think the real world decision is, but often things are not well defined in the problem that we give people to study. And so we think we're learning about how well people can like use models, and we're trying to ultimately improve that through better interfaces, etc. But that's what I'm arguing against, is that you can't get anywhere until people are given a well-defined problem, even if we don't think that they can. Defined problem. Even if we don't think that they can solve that problem themselves because they're not rational. But how do they do that? How do you do that? You define your problem using decision theory, whatever. You define a problem that some rational agent could solve. Because if you give people a problem and even a rational agent would have no idea how to solve that problem, people have to fill in the gaps. And so how they're doing that is you don't think you're analyzing that, but you're getting all this kind of noise. Do you have an example of a wildfire problem? Of a well-defined problem. Or, like, even understanding what the rational agent is and what he's doing. So, a well-defined problem, like I said, like you people need to know what's the action space, like, what are the allowable actions? Like, what is the scoring rule? So, you're giving them sub-signals. They should have the prior, because the signals are informing them sort of about this payoff relevant state, which is uncertain. So, some of this is like statistical decision theory plus like information economics. Plus, like information economics. Basically, like it should be well understood. Like, it, you know, in theory, they should have the same prior that you're going to use when you analyze your decision, which often works sort of using the form prior or whatever. But, you know, they should have the same prior and they should, you know, at least have the opportunity to update their beliefs and choose the utility optimal decision under the same scoring rule that you're going to use when you analyze their decisions. So that's what I'm saying when I say a well-defined problem, like the humans have to be doing the problem we think they're doing. And I think a lot of times we legal. And I think a lot of times we leave a lot of it implicit, both in what we give people, it's not well defined, like even if they were trying to optimize, they couldn't. But then when we go to even analyze a problem, I think sometimes like we're just like, you know, there's always a scoring model. There's always some way we're evaluating decision quality, but sometimes, you know, we're not even being very upfront about that or like why we chose that role and why that's exactly the right way. But like that has so much implication for what we interpret as like, how well are people doing here? How much could we? How well are people doing here? How much could we improve things? So, that's what I mean by saying well-defined problems. I'm not talking necessarily outside of this context of like trying to understand how humans are interacting with models. So, I'm not saying like let's define the problem of like, you know, what the outcome should be to begin with. I'm assuming that's all kind of decided. If you have a question, can you raise your hand just so I can tell? I have a finger that is. Oh, two finger. Okay, sure. Yeah. Yeah. So, can you clarify why decision theory though versus like topology or some other form of language? So, for instance, in philosophy of science, people argue you should be mapping out the problems faced with topology. So, Kevin Kelly, Konstantin Jinin, they argue that. Why are you privileging decision theory? And secondly, this is more of a comment. I have a skepticism about this claim that you have to map out the Map out the question space like this. Like, what about something like Kuhn's thesis? That, like, oh, it's like all these embedded, you know, theories and presuppositions and axillary hypothesis. And just in practice, it would be impossible to even do that because the nature of questions and theories are such that they're so value-laden in this way that it's not even a possible task. It's unrealistic. Impossible task. It's unrealistic to even suggest. No, I'm saying if we want to make normative judgments that people are not doing as well as they can, people are biased, etc., like we must understand the problem they are solving. Oh, hold on, hold on. So are you claiming that we have not made scientific progress to now, where people have not been doing this, as far as I can tell, in decision theory? I'm pretty sure Darwin, when he was laid out, but Darwin did not laid out evolution in decision theoretic terms. I imagine it was a very good scientific theory. Very good scientific theory that led to a lot of scientific theory. I think I'm talking about something much more specific than you're talking about, perhaps. Okay, well, so I'm talking about how do we intervene? We have a model of human using some model, some statistical information. How do we intervene to improve their use of that information? So that's all I'm talking about. I think there's a difference between exploratory research, like we're going to observe some real humans making a decision. We're going to try to understand how they're making this decision. That's, you know, we can do descriptive or exploratory research. You know, we can do descriptive or exploratory research, but what I'm talking about is when we want to make normative statements about people could be doing better here because we want to intervene and improve the humans' use of the model, like we really need to be rigorous there. And why statistical decision theory? Because if we have a human using information, I don't know any better way to say, like, to actually be able to quantify, like, what is the value of the information that we're giving this person to use decision-making? And so, like, you. Decision making. And so, like, yeah, it's not the, I mean, there's like critiques of like subjective expected utility theory, but I have yet to see anything. Well, topology, again, is a whole literature. I don't know what's topology. Topology is a type of math. It's a type of math. How do you apply that to human decisions? They're using it to map out question spaces. Your claim was that we need to rigorously clarify the question space. Characterize the question space, and I'm saying that's been done by people using topology. And I'm saying, well, why not that? And again, you did also didn't answer my question about this other business. I said, like, about, okay, well, just because people are doing some statistical question, are you thinking that they don't have like values and theories and oxides? No, they totally do. I'm just saying that to understand what they're doing, we have to start from somewhere. We have to have a point of comparison. I'm saying, I'm claiming that they can't do that. I'm saying, like, they don't have to be able to do that. I don't. They don't have to be able to do that. I don't want them to be rational. I want them to do what they're doing. I just want some framework in which I can understand how they're different from some well-defined model prediction because I can't learn anything about that. And I think there's a lot of HCI type research or research on humans where people get confused. Maybe they just want to describe humans, but they're making these evaluative judgments. That's curious from what is the mechanics. People think you've done you got a lot of scientific knowledge. Yeah, I was gonna say, also, you mentioned auxiliary hypotheses and sort of theories and instruments. I know, Ben, you've blogged a bit about that. I don't know if you want to say, add anything before you move on to that question. I think that's interesting. I'll try to theorize this about the I think that there is a weird discussion I'm framing myself in, which is that the algorithmic decision theory world is stuck in this rational. Stuck in this rational decision theory prison that I thought it's the fault of economists, it's the fault of big bureaucratic government systems. But it is a ridiculous, like, it's not handed down from anywhere. It's something that has only existed since World War II. It's something that we've kind of like built a huge framework around, but yet there are so many other possibilities. To Alex's point, and I will just come back to this all the time, we made decisions. We made decisions, many of them good, before I came up with this math. And it is this kind of funny thing that, like, for whatever reason, the community that we are engaging with, that's the dawn telling us one most of our time. Yeah, we'll get science question. But yeah, I'm just going to say, but I do think that that's something that the fat sphere community around it, whatever it is, and the economics community around it is trapped and cannot. But we don't have a better alternative. But we don't have a better alternative. But we need one. Yeah, I think we need one that fulfills all these purposes that we're using decision theory for. I think there's a lot of interesting descriptive frameworks, but I have not seen anything that we can learn from. Let's go to Sayash. Awesome. Actually, I want to challenge the notion that even within this sort of menu examples where we would expect actuary decisions to do very well, we actually see that happening in practice. So it seems like. That happened in practice. So it seems like Neil's observations were based on machine learning, which is using observational data to sort of model these decisions. And so I have like three, because we've been talking about medicine, I have three sort of examples which go counter to that. The first is EPIC's use of sepsis prediction. You would think that EPIC with 250 million samples of when people have and have not suffered from sepsis in hospitals, they would be able to do very well. It turns out that one of the features they used was whether a phenician has already. Use was whether a clinician has already described an antibiotic. And so that is one of the features. And this thing was not discovered for five years, like after the rollout of the system. A second is this sort of article example of asthma predictions, where you have this very similar example of people coming into a hospital. A healthcare worker has to decide whether they need to send the person away or whether they should be hospitalized. And it turns out that you train a model which predicts that people who have asthma. Predicts that people who have asthma should be sent back home. The reason is that traditionally, on the data the model was trained on, people who had asthma were sent to the ICU and therefore they had very low probabilities of actually having adverse medical effects. And the third example is from science. So across medical fields today, where machine learning has been used, I point to this review by Russ Koljak and others, which looked at how people are developing machine learning tools. And I think Machine learning tools, and I think this goes back to Hansi's point about you know, like statistical misapplications. Out of 100 papers that they reviewed, 45 of them use only in-sample statistical sort of goodness of tests or whatever to say that these methods should be used. So within this context of these tools being used in the real world, and within, you know, with this observational data being used to train machine learning models, we've seen these models continue to fade. I guess, like, based on the discussion. I guess like based on the discussion. I think this is the thing that's funny. You could find those things. But I mean, I presented three meta-analyses, and I will find more. You could find examples where clinical decision rules are bad. You could find lots of examples where doctors are also pretty bad. But my point is that experts claim that they succeeded, right? Like up until 2021, if they had included TEPEX results, the self-reported results, in a way. The market has set this result. You know, I mean, and one more thing, I'll say it's like. My question was: sure, okay, we might say that, look, EPEC made a huge mistake, it impacted millions of people. There should have been someone who looked at the features used in the model. The kitchen sink approach that we heard about, like first disincentivizes some of that. So, with this kitchen sink approach to statistical prediction, no one is actually even looking at those features. And second, once we get to that level where you have to look at every single feature, like, how do you compare the human decision-making values there? Human decision-making value is there, right? Because it is then at the end of the day, a human decision. The human decision is just shifted about which sort of features we want to use and what data we want to collect, as opposed to the actual decision. So, in some sense, we are still back to square one if you look at the system as a whole, about what we want to market in the first place. And that's where clinical decision making comes in, because any clinician could tell you that don't include asthma as a predictive, it goes with it is the sort of main predictor, whereas these data scientists will. Whereas these data scientists would these models would probably miss out as you see obviously. That's why we need humans in a loop. Still. Also, sorry, it's a really quiet room, so we can hear if you're whispering. I'm sorry. Yeah, sure. What's about sepsis? Yeah, go ahead. Go ahead. No, I can send you stuff. I can send you stuff. So, like, I don't want to forget the outcomes. So, sepsis is the number one killer in American hospitals. Most American hospitals for CMS publicly reported. Hospitals for CMS publicly reported metrics like are below 50% for compliance with what the standard of care is. So put machine learning to the side, even like most of the work we've done on sepsis, it's about identifying when sepsis happens and making sure the standard of care happens. So it's like there's regardless of what Epic did, sepsis is like a public health issue globally. And then what sepsis did Did people know about the antibiotic thing? Other people, we use antibiotics as an input to the feature space for the model. Epic's very proprietary about their documentation. They give it to their customers. They don't make it public. That's a different argument to have: like, should there be public reporting of input features for all healthcare models? But, like, I just think it's the framing is particularly. The framing is particularly negative when, like, a lot of that was available to people using the models. Sure, but yeah. But the system was still late. It did not work. Have they replaced it after five years of shooting compiles? No, they pushed out a new version, but to say it didn't work compared to the performance metrics, that they there was a discrepancy in the performance metrics that they had in their model review, and then what. Model for me, and then what a team found. But then there's other teams that even found that in their settings it performed different than it did in Michigan. So, like, I'm not, and I'm not going to defend EPIC for everything that they do, but like it is a toll bill for the number one killer in American hospitals. At the end of the day, like, you can say, like, okay, it's not as good as some others, but like opportunity to improve human decision. And I'm not questioning that. I'm just simply questioning the claim that we have seen evidence over and over that clinical decision making. You can deny it, right? Meil Dilbo did for his entire life since he wrote that book. But it really is one of the most robust findings in social science in 100 years. So we could deny it, we could keep denying it, but a lot of, there's just, I mean, I don't know how many papers you want me to send you. But like, you know, to Sayasha's point, though, there's sort of Like to Sayash's point, though, there's sort of this common sense role of clinical decision-making where a doctor can look at a set of features and identify in some cases. No, you don't agree? I mean, like, clearly, there are some assumptions that were happening in real studies that are being violated under your case. And I think that we need to figure out what it is. Like, under what conditions can we actually expect this to actually happen? Because they don't really map to what's being done right now. Like, hey, I mean. I mean, and the comparison wasn't human decision making, it was like other performance measures that they reported in a different context. Absolutely, and I think that's one of the issues is we don't really have that data to be able to experiment or validate these models against human decision-making context. Valid. Okay, Brian has been waiting for a long time, but then you're the last question. Well, you'll have to go to figure. Okay, you have a two-figure? Yeah, it'll be here in quick. So, just in response to the ethics stuff. Just in response to the ethic stuff, I think it comes back to problem formulation again. Because, like, the ethic model was probably optimal for the data distribution it was defined for. And the Michigan study, the Michigan medicine study was like an external validity study evaluated on a different population, which is why it didn't perform as well on that population. And so, like, it's not that the model was like. The model was like it's how you define the problem, right? Like you define the problem with a certain data distribution. That model is whatever, like you can find the best statistical model for that, whatever expectation you're trying to minimize that you're trying to minimize. But then, does that model have guarantees like in different settings? Once you have different data, if you use different covariance, whatever, like that's you change the problem and your evaluation, like the evaluation metrics you need to not consider. Metrics you set up not consistent with the model, or the problem for me, which you reset up, which is why these discrepancies come up. And I think, yeah, it's just a problem for me. Like, once you define a problem in a certain way, I do think you can find a statistical algorithm that solves your original statistical problem. The issue is that maybe the statistical problem you wrote down isn't actually the thing you care about. Yeah. Quick comment, Ben. Ben, you point out this like two domains of things. This, like, two domains of things where they shouldn't be formalized using these statistical methodologies, and you're trying to give examples. I thought education was like a really interesting one. Oh, I thought, you know, because many people would complain about the bureaucratic kind of bureaucratic of education without making metrics of education. My son spent the last two weeks of his fifth grade in testing. It was fun and great. Yeah, it's fun and great. There is kind of something to that argument with respect to certain things. I'm just trying to figure out that like separating hyperphone where you don't necessarily want to gamify the entire kind of thing by placing a metric and then saying oh, a statistical algorithm is going to outperform a human in these