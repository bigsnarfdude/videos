We'll start with our sessions, but today I'll introduce our first speaker, Amanda Kosten. So Amenda is a postdoc with Microsoft Research in Statistics and Machine Learning, and she's also an incoming assistant professor at the Department of Statistics at UC Berkeley starting in the fall of 2024. Her research uses techniques from causal inference, human-computer interaction, and the social sciences to investigate the validity, equity, and governance of data-driven algorithms in society. Driven algorithms in society, consequential science. So, we are very excited to have Mandy present and thank you. Please Lydia Way. Yeah, thank you so much. Hi, everyone. It's so great to be here. Thanks so much to the organizers, Lydia, Angela, and Doug, for organizing this awesome event. I'm really excited to be here and meet you all and chat with those of you who already know. I'm looking forward today to presenting my work on validity, problem formulation, and data-driven decision-making. And okay, so to get us started, I'd like to share this quote from Albert Einstein that somebody once asked Einstein if he had an hour to solve a problem that could save the world, what would he do? And he said that he would spend the first 55 minutes thinking about the problem and the last five minutes thinking about the solutions. And I think that this quote is really insightful, especially when you compare it to how a lot of algorithms are built. A lot of algorithms are built in practice in these high-stakes settings where it's so easy to use, you know, a Python library at our library to build a bunch of models that I think the times are inverted, or maybe even people spend less than five minutes thinking about the problem and spend most of the time tuning model parameters, comparing different models. So today, I'm going to focus on some of the things that I hope we could be thinking about and lose fifty-five minutes of problem formulation. Problem formulation. And the setting has already been introduced, the kinds of algorithms that Deb introduced in the introduction. So I won't go through any of that, but we'll go through some of these examples. We'll come up as kind of working examples throughout the talk. I'll just note that I'm using the terms algorithm and predictive model interchangeably. And we know from practice that problem formulation is often not sufficiently well thought out in a lot of these settings because we'll see algorithms to. These settings, because we'll see algorithms deployed, but that in practice, the kinds of quantities these algorithms are estimating don't actually align with what they were intended to do and what we thought they were going to do. And so we're going to look at this today through the lens of validity. So I'll start by talking about validity and especially what it means in the algorithmic decision-making context. Then we'll see how problem formulation can go wrong and jeopardize validity. And then we'll discuss some ways to identify and address these issues. Okay, so validity broadly is about whether we are assessing the intended quantity. So in the algorithmic decision-making context, this means we're interested in whether the model predicts what we intended it to, what it's supposed to. And this kind of very basic question of whether the model is doing what it's intended to, it doesn't always hold. And in practice, it can actually be harder to assess than we might think. Than we might think. And I'll get into some examples to make that really concrete, why this doesn't hold in just the next slides. But first, I want to emphasize that it's really hard to draw a line and say, like, you know, up till this line, validity holds, but past that line, it doesn't. And instead, following the precedent of thinking about validity in other contexts, like the social sciences and experimental sciences, I think we can think about this as whether there is strong. This is whether there is strong or weak evidence in support of validity in a particular context. Okay, so what are some examples to make this a little bit more concrete when validity doesn't hold? So the first example is in the healthcare setting, and this may be familiar to some of you. So algorithms that are intended to assess the severity of patient needs are often built to predict a more readily observable outcome, like healthcare costs on insurance billing. On insurance billing. And in work by Thean Obermeier and co-authors, they showed that these outcomes are different, especially for historically undertreated groups, and that using this algorithm can systematically underestimate the health needs of those groups and cause harm. The second example is in consumer lending, and it concerns election bias. So, lending algorithms are So, lending algorithms are often trained to predict whether someone will repay or default on a loan, but we only see those outcomes for applicants who actually got a loan. So in practice, what people often do is just train and test the algorithms on data from that subset of applicants who got a loan. But that's not a random sample of the population. So then, when we go to actually use the algorithm on all applicants, the kind of performance guarantee. The kind of performance guarantees may or may not translate. And we investigated this in a work with Ashish and Alex, my co-authors. We found that in a wide variety of settings, the best algorithm, and by best, that could be the best performing one or the one that has the best fairness accuracy trade-off, but oftentimes the one that looks the best on this bias subset of approved applicants is not actually the one that would be the best if we were to evaluate it on the full set of applicants. Okay, so those examples. Okay, so those examples are just intended to give a little bit of more concrete motivation, but now I'd like to get into how we can more rigorously think about the variety of threats to validity in the algorithmic decision-making context. Where just as a reminder by validity, the central question of validity is whether the model does what it purports to do. Okay, so our approach. Okay, so our approach to coming up with a framework for thinking about validity issues in the algorithmic decision-making settings was to start with how validity has been discussed in the social and experimental sciences and then translate that to the algorithmic decision-making settings. And I'll go through each one of these now in a little more detail, starting with the idea of construct validity. So construct validity from the social sciences Validity from the social sciences concerns the extent to which the measure captures the intended construct or concept. And then, translating this to our algorithmic decision-making settings, this means that we are asking to what extent the algorithm predicts the intended quantity. And of course, we just saw an example where this doesn't hold when we're using a proxy outcome that differs in important ways from the target outcome. From the target outcome. And that's one type of threat that we call target misalignment that can threaten construct validity. So, target misalignment can also happen for other reasons, maybe if the problem is not mathematically well specified, but one of the most kind of common reasons is this use of proxy outcomes. And even if we have a reasonably good alignment in the target, we can still have validity issues in practice. So, to see that, I'll move. Practice. So, to see that, I'll move on to the next concept of validity: internal validity. So, in the social sciences, internal validity concerns the extent to which claims hold true in a study setting. And because these claims are often causal, this is really asking us whether those causal claims hold true in the study setting. Then, in algorithmic decision-making, this concerns the extent to which the predictive relationship holds true. For instance, reflecting For instance, reflecting a plausible causal path, and that could be both indirect or direct. So it's perhaps easiest to think about internal validity by looking at an example where this doesn't hold. So for that, I will use the example of predicting, trying to predict criminality from photos of faces. So photos of faces don't contain predictive signal for predicting. For predicting criminality. And then, because of that, we would say that this fails internal validity. And more generally, attribute misalignment threatens the internal validity of algorithmic decision-making systems. So attribute alignment occurs when there's no plausible indirect or direct causal path between the features and the predictive target. And the predictive target. So, in the example from the previous slide, we can't write down a causal graph that we can plausibly defend that has an arrow either from the photo of the face to criminality or in the other direction. And so, if we can't do this, then we have attribute misalignment, and this threatens the internal validity of the system. This is really, I think about this as a very low bar for validity. If you're familiar with concepts like face validity, that's related here. Then, the analog to internal validity is external validity, which is another very important type of validity from the experimental sciences that concerns the extent to which claims hold true in real-world context. So for algorithmic decision-making settings, this is the extent to which performance metrics reflect how well the algorithm will perform in real-world settings. Or put a little bit differently, to what extent do performance metrics that we observed on Performance metrics that we observed on the sampled evaluation data reflect how well the algorithm will actually do once we deploy it. And we can bring back in that second example I introduced from consumer lending on selection bias here. So selection bias is one of the big threats against external validity. When we are doing evaluations on a different sample from the data population that will actually deploy the algorithm on, then external validity can be jeopardized. Validity can be jeopardized. More broadly, we say that population misalignment describes a set of threats that can jeopardize external validity, and this happens when the data set is not representative of the target population. So this can also happen when an algorithm is deployed in a different country or geography from the one in which the data was used to build that algorithm. That algorithm. And okay, so we can put those all together, and this gives us a framework for thinking about validity in the algorithmic decision-making settings. Of course, as I mentioned in the beginning, we're never going to have perfect alignment on any of these. Instead, we're really interested in the degree of alignment. And this brings us to the next question of how we can identify validity issues in practice, assess them, and hopefully address them. Address them. So I'd like to present some of our work in trying to make progress on this question, where we envisioned an early stage deliberation on validity and problem formulation that would involve a variety of stakeholders who would, at the end of this deliberation process, release a report that summarizes their thinking on these different validity issues that could be made available for review or public release. Public release. And so, to make progress on this, we conducted a co-design study with a variety of participants from public sector agencies who either manage, develop, or use algorithms in their day-to-day work, as well as community advocacy groups. And in these sessions, we did semi-structured interviews and a co-design activity. So, the co-design activity is represented here in these screenshots, where we asked participants to think about the Where we asked participants to think about the kind of pitfalls they should watch out for from the very start in building algorithms. And the result of this was a set of 138 co-design deliberation questions falling broadly in these four categories, goals and intended uses, societal and legal considerations, data and modeling constraints, and organizational governance factors. I'll go into the goals and uses section because that's really one of the most Use this section because that's really one of the most relevant areas for what we're discussing today on validity and problem formulation. So, some of the examples that were co-designed in this section were what pain point is the algorithm intended to solve, what evidence exists that supports the claim that this pain point actually exists in practice, and what evidence is there that the algorithm may correct this pain point or offer a remedy to it. Then, as part of our work, we also laid out how this could work. Work. We also laid out how this could work in practice, and this is an iterative process that would involve identifying the goals, selecting from that set of 138 questions, conducting the deliberation and synthesizing that, and potentially iterating on that until it gets to a point where it makes sense to release this public report. Okay, so so far we've really focused on these early stage problem formulation, thinking about the let it be there, but I'd like About validity, there. But I'd like to take a moment now to say that validity is actually really important to think about throughout the life cycle of building and deploying algorithms. And to do this, we need a variety of tools. I think things like the deliberative framework that I talked about in the previous few slides, as well as statistical methodologies specifically designed to address validity issues that commonly arise in each of these stages. So, to give a sense of that in the remaining time, I'll go through some. Time, I'll go through some work we've done to address validity issues in evaluation. And to start us off, we'll look at like a standard approach and I'll call out why I think there's an important validity issue to address here. So we're back to the consumer lending setting where we've already talked about the problem that we only partially observe outcomes. We only see those for applicants who were approved. We don't know what the default or repayment outcomes are. What are the default or repayment outcomes for in this example rows two and four because those applicants were rejected? So, what's the kind of common approach here? We already talked about this a little bit, but we do just evaluate an algorithm on the subsample where we observe the outcome of interest. And this is problematic because that may not reflect how well the algorithm would do on the full population. So, you know, in the stylized example, we might miss the fact that we are actually making mistakes on rejected applicants. On rejected applicants. So, how can we address this? We proposed a solution that uses counterfactual estimation. I'll take a moment now to give some high-level intuition for what we're doing, and then we'll get into a little bit of the math and then wrap up. So, what we're going to do is use counterfactual estimation to impute those missing outcomes. But we may or may not do a good job with that, right? We're not going to perfectly know what those outcomes are. There's going to be some bias in our estimates. There's going to be some bias in our estimates. We can't actually estimate that bias because we don't just use those, the rejected applicants, because we don't know what the outcomes are. But we can compare those imputed outcomes as if we were to, if we pretended like we didn't know what the outcomes were on rows one and three, and then look at what the estimated outcomes would be compared to the ones we actually observe and use that difference to do a bias correction in our performance. Correction in our performance estimate. So that's kind of high-level what we're going to do, and then I will go through this in a little more detail to make this more concrete. So I'm going to use the potential outcomes notation. A superscript here denotes the outcome we would observe under the decision little D. And then we'll denote the algorithmic prediction as S hat, which we'll think about as a score falling between 0 and 1. So the mean squared error in terms So the mean squared error in terms of this potential or counterfactual outcome describes the error between Yd and the algorithmic prediction, the squared error cost of population. And this is a counterfactual quantity because we've written it in terms of this potential outcome, Yd, which isn't actually something that we observed in the data. So we need to make some assumptions in order to be able to write this in terms of things that we can observe with the data. So the main, or one of the big assumptions that we need to make. Or, one of the big assumptions that we need to make is that there's no unobserved compounding. And this assumes that conditional on our measured covariance x treatment assignment is as good as random. Then we also need to make an assumption on overlap with positivity and consistency that I'll skip for the sake of time, but I'm happy to talk about those later if someone's interested. And then together, these assumptions enable us to write that counterfactual mean squared error in terms of observable quantities that we can actually estimate in the data. We can actually estimate in the data. So now we've identified our performance metric, and all we need to do is figure out how we're going to estimate this thing. Okay, so that's our task. We want to estimate this quantity that we've just identified. And the first part, we're going to estimate that inner expectation. I'm just going to call that eta for brevity. And so we'll come up, we can use some kind of flexible or non-parametric method to do this. And then a simple And then a simple way to estimate mean square error would just be to take our estimates of ADA and average them on an evaluation data set. So, this is what's called a plug-in estimator. And the issue is that if we want to use flexible, not parametric methods to avoid making assumptions on the data generating process, then in general we're going to get slow convergence rates that will make it hard to get confidence intervals around our estimate, which is something that we want if we're trying to be able to say something about lead squared error. Be able to say something about lead squared error. So, what can we do differently? So, we're going to do something similar, but there's just an additional step. We're also going to estimate the propensity score, which is the probability of getting the little decision little date conditional on X. And then we're going to use that propensity score to add this bias correction term when we're averaging eta. So, this is where, this is what I kind of introduced a few slides ago, but if that, if we've observed the outcome yi, which happens when the decision was little d, we can actually compare. little d, we can actually compare what the error is on that particular case, and then we're going to inversely weigh it by the propensity score to make it look like the overall population. And we'll do these steps one and two on a different sample of the evaluation data set. And then the reason we go through all this trouble is we can get nice theoretical properties like root inconsistency and asymptotic normality under relatively mild conditions on estimating the propensity score and beta. And this means that we can use flexible methods to estimate. That we can use flexible methods to estimate a pi in beta, but still get a valid confidence interval on our mean square error estimate. Okay, cool. So I will wrap up now. Just a recap. So in today's talk, we really focused on validity, whether the algorithm does what it's supposed to do, and looked at how misalignment in the target, predictive attributes, and the population can threaten validity. We considered the question whether deliberative tools can improve the Whether deliberative tools can improve our ability to identify the validity issues early on, and saw how statistical methodologies can address specific validity issues like selectively missing data. I think there's still a lot of exciting and interesting work to be done in this area around things like that many of you are experts on, like addressing proxy outcomes, unobserved confounding, post-deployment monitoring, and its implications for reliability, which is closely related. Reliability, which is closely related to validity. So, these are all things that I have a lot of fun thinking about and would be interested to chat with you all on. And I would like to also acknowledge my collaborators, Alex, Edward, Hoda, Ashesh, Anna, Ken, Hai, and Alan. And thank you all so much for your attention. I guess, is there time for questions? I guess, cool. Questions. I have a question to start. Oh. So those are questions back. Yeah, we can take off. So my question was for the countergradual estimation component, do you access the data set where the decision was essentially randomly assigned? Or can you there are also two ways to make the kind of To estimate the counterpart quantity to that part. No, no, yeah, so you don't, you essentially need the decision to be randomly assigned conditional on your measured covariance. So not like just randomly assigned overall, but essentially like a stratified RCT kind of thing in theory. And of course, like in a lot of these settings, we don't have randomly data from RCTs. Some data from RCTs. So the hope is that you can have enough X's that you're mostly capturing the confounders. I think in practice, there may be only a few settings where we can really plausibly defend that. So that's why I think one of the important open questions is how to do better. What kind of assumptions can we make that are more realistic? So we have some follow-up work where we're kind of relaxing this assumption and you just kind of bound the amount. Of bound the amount of unobserved confounding, and then in that setting, you're going to get bounds on. You're not going to be able to point identify the counterfactual mean square error, but you can identify upper and lower bounds, and then you can estimate those. And so there's a trade-off because at some point those bounds may be not that informative. But yeah, if this assumption doesn't hold, then the validity of this whole process is called into question. So yeah, it's a good question. Yeah, that makes sense. Yeah, we'll have a question from Martin and Brian. Less of a question than like trying to go deep into what this looks like in healthcare at the health innovation. So it's like adding. Yeah, Mark, but yeah, add some nuance to this. So like the idea of like proxy variables and target variables, the reality is. Like the reality is, most outcomes that are modeled are proxies. And so, even in Ziod's work, shifting from cost to documented diagnosis codes and billing, that still doesn't get you to true outcomes of diagnosis and patients. So, just how to reconcile that, because it's a proxy for something. And then the other one around external validity in healthcare. Validity in healthcare, oftentimes you use the model to intervene to prevent the outcome you're trying to predict. So, like one case that I've seen that's a good example of this is HIV, where people are using circle models to prescribe pre-exposure prophylaxis, which if taken appropriately would like 100% eliminate the enzyme. So, it's like the model performance in the real world looks just like World looks just like precision recall goes to the model and implement it, and you even have the outcome from the model. So it's just a totally different, you need different tools to evaluate external validity. Yeah, I guess it sounds like you're saying maybe precision recall is not the right metric to be looking at. Other measures that people look at prior to implementation or real-world use are unless you don't want to implement. Yeah, that's a good point. That's a selection of metrics, it should be thought about as part of that. I have a delayed question about proxies where in many settings you're never going to observe the actual thing. Any help with this one? There's a lot of other settings like this where it's like hard to just get at the actual thing. So you will always have to rely on epoxy. Do you have, are there tools that you've been exploring around sort of what makes a better versus a worse proxy for some downstream variable? Let's say that you have. Too variable. Let's say that you have very little measurements of what you're actually getting at, and there's a lot of options in terms of the cost. Are there sort of heuristics, either formal or informal, around what makes a better versus a worse observable process or something? Yeah, education. There's a bit of that. Yeah, I think there's still a lot of open questions. And a lot of the work I've seen has been more around mathematical models. Like Luke has a great paper on this, on outcome measurement. Paper on this on outcome measurement. So, when we can put down a model of the measurement of those proxy outcomes, then we can kind of start to correct it. But, in practice, that in some settings that may be easier to do than others. And there's also some emerging work that does a nice shop when you have like a series of proxy outcomes, how you can combine those in a nice way. But I think there's still a lot of open questions there. So, that's why I flag that as one of the areas that I think. Great to work on the hand. Yeah, so I'm curious about the interaction between the sort of identification and proxy autocrats, because I feel like a lot of the time practitioners are sort of ostensibly have a predicate question, but they're using it as a proxy for something that's kind of a cost of the question. They're saying, I want to predict the rest of this outcome because I think then if I target according to that outcome, I will change something, right? And people will then have questions are going to change our parameters. And the processing is too is often very kind of fuzzy and continuously, right? And so I'm curious, kind of, like in the sort of like the framework we talked about for just go bank questions, like it's curious thought about how people thinking through the link between their sort of like possible goals. Yeah, yeah. So I think the first step I think is really using the right connotation and language. So that's why it's really important to think about. So that's why it's really important to think about potential outcomes because, like, if the decision is affecting the outcome, then the outcome we see under one decision is going to be different potentially than the outcome we see on another. And so, just kind of being precise about the targets that we're estimating, like the outcome under a proposed decision. And then you can, once you have that in place, you can start to think about things like that will measure causal effects, like contrast, like what would the outcome be under one decision versus another. And is that kind of. I have to I think it's related. Um but yeah, I I think part of this process is often that like, you know, people are making these decisions right you know before they implement the system and it's like there's not really like a one existing evidence based though on right like what decisions are on something. Right, so you're gonna be like you're making those decisions under like a large amount of uncertainty. I don't know if there's gonna be sort of any useful practices you've seen or anyone or just a kind of Different type of organizer deciding whether that decision is actually going to be useful for yeah. I mean, I guess if you wanted to do it in a data-driven way, ideally you would get some kind of data and be able to estimate these causal effects, and that would tell you whether it is. If you don't have something like that, maybe analogous to the deliberative framework I described, where that was more centered on the algorithm, but you could have similar questions for the decision, like what evidence exists, but the decision. Like, what evidence exists that the decision would actually reduce the likelihood of the adverse outcomes? Those would be kind of how I would start to think about it. But I haven't actually thought about that much before, so I'll keep the momentum over to. Great, thank you. So we'll break for coffee and return at 10:30 before talk from Anish.  She was saying that we're just saying,  Yeah. Oh, yeah, I'd like to scroll down after like my friends drafted like draft officers like  I'm really worried that they won't have stays in. I followed, so I did a couple things ago because I followed three trials one night, and then I tried to follow people. And it's only because we don't actually want a deal we talk about her now, but we can do some sentence to the corner. I mean, we should say this in the comments. Like, yo, like, do you have the original watch that was matching? Because, like, it's a whole bunch of hacker on its computer.