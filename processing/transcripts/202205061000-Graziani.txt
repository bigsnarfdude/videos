Her PhD in the group of Hendri Hulde at the University of Geneva in Esson, uh western uh Switzerland. And uh she now joined uh our group of computational assistant biology at IBM Research Zurich, but she's also jointly holding a postdoc position in the same lab where she did her PhD. Today she's gonna talk about deep learning interoperability and whatsoever. What surprising, and specifically to discover biomedical patterns. Thank you very much for the introduction. I have to be honest, after the great talk of before, I'm a bit frightened of giving this talk, but I will try to do my best. Yeah, so as Anthi mentioned, I just finished my postdocs, and these are some ideas and results that I have from the six months after finishing the PhD. Finishing the PhD, and obviously, this is ongoing work and it's also done in collaboration with some of the people that I work with at Azure SSO and at IBM, particularly Nicolo, Nicola, Nikita, and obviously under the supervision of both Henning and Maniel. And so I would like to start this talk with a general introduction about what have we learned so far thanks to deep learning interventability. Learning interability. And I would like to start with this concept that we also discussed yesterday, actually, that is very common in statistics: that models are approximations and none of them are actually exactly true. So what are we trying to do here is not to come up with the perfect model that is spotless, but we're trying to come up with a model that is good. Obviously, it has to be good enough, so it has to be a certain degree of accuracy, but also we want to understand whether the model. Also, we want to understand whether the model is useful and in what application and in what kind of context. And we understood so far that the optimization of soil performance in terms of accuracy is not sufficient, it's not a sufficient type of evaluation to ensure the reliability of the model that we are building. And some examples that we observed show this, like performance drops when you shift to a real data set, little robustness to adversarial examples, hidden biases in the networks themselves, like this paper coming out. Like this paper coming out, AI is racist. And also, the fact that models memorize samples. So in some cases, it might be something that we don't really want to see because it might hardly overfit a type of data. So postdoc approaches actually helped us to understand and to learn something new about the models we're working with. And so they gave us something different from simply. Something different from simply testing accuracy. We found, for example, that some data correlations reflect reality, but they may be harmful. And I put this reference to the work of Ben because I think it's kind of interesting to see that we can look at the data, we can have a model, we can have an interpretable model, and find out that, whoa, the model that we have of the world is real, but it's still harmful. And then I wanted to put some of my work, I was thinking of whether. Put some of my work, I was thinking of whether talking about the previous work that I did for the PhD or not during this talk, and I will, I couldn't resist, so I put a couple of references there. We learned that interoperability can help us improve the performance of the models we have, and I will talk a bit later about this. And we also learned that we can use these Postag methods to actually look at particular types of models that are hardly memorizing the samples or that we would like to generalize better to other types of samples. And with interpretability, we can try to open this about. We can try to open this box and see what we can do and what kind of patterns we can focus on to improve the generalization and reduce minimalization. So, going deeper into this idea, what did we learn? So, there are some architectural biases in convolutional neural networks. For example, we know that low layers and CNNs extract simple features of color and texture, whereas higher layers focus on concept representations that are more complex. So, this is the sample. That are more complex. So, this is an example of OLA. It's very common, it's feature visualization. So, you do some activation maximization, you put a random noise as input, and you try to see with the gradient descent how can you maximize actually the activation of a neuron, of a challenge, of a channel, or of a layer. And here we are at very low layers, so we can see the convolutional neural networks, so this type of textural patterns, and then going deeper and deeper into the matter. Sorry, I haven't put here the depth, but it is an effective. Put here the depth, but these are different levels of depth of inception. You can see that you can start seeing something that looks like a dog eyes moving from this texture and then going even into something more complex. And another thing that we have learned that is scary maybe is that texture features actually represent a strong bias in CNN. So whenever you're using a CNN, be aware that they are biased to learning textural patterns because these are very deeply encoded from the low layers. From the low layers, and then all the features that are built on top are built on top of texture features that come from the low layers. Another thing that we've learned that I think is interesting to put the point is that a lot of people are doing transfer learning because it gives a lot of benefits, but actually, for some types of images or some types of domains, for example, medical imaging, this transfer learning is using some features, but only some of them and very little of them. And very little of them. So there is this paper by Ragu that is showing that actually Gabriel-like filters, the GABR-like filters, the TNNs were not crucial in medical images, and you could actually recover performance just by appropriately initializing the weights before you started training. And here there is some of the work I did where I really tried to take this idea of visualizing activational maximization. And here you have another example of this from low to high layers and the features that maximize the impact. And the features that maximize the inputs that maximize the activation of a channel. And as you can see here, this is a network that was trained on medical images. And there are only the first two early layers that are almost similar to the early layers trained on image network images, but the rest is kind of lost because the network is so much more powerful and there is so much m less variability in medical images that there is actually no point in reusing more complex features. So those features are just amiable. So, those features are just amitable. And well, we talked about transformers, so I thought, yeah, okay, we also learned something about transformers. Let's talk also about this. I mean, we're kind of starting to learn a bit more. There is this new paper that came out, and I thought it's really nice because I'm actually trying to make my mind about transformers. Do I want to use transformers? Do I want not to use transformers? What is my position about it? And, well, they have nice things. So, with this result, this kind of comparing the similarity. The similarity of layers in transformers across themselves. So, if I take a layer, early layer and a deeper layer, I can compute the similarity. And this one is doing the same for a resonate 50. And as you can see, the representations read by transformers are much more similar in depth, which means it reads much more complex information at all layers of a transformer. And I think this is actually what makes them so powerful. So, here in this experiment, they have tried to train a transformer. To train a transformer on a large input data set and on a small input data set. And I think this also explains, that teaches us something about this architecture, that is that these architectures need a lot of data to actually become very useful. And the reason is to be found in the way they pay attention. So the transformers, attention heads, need a lot of data to learn that the attention should be paid at things. Should be paid at things that are far away in the input from the beginning. So, this is this: each point in this graph is kind of telling you the distance. So, there are multiple attention sets, so there is an attention that is computed among two parts of the input, and these points are measuring how distant these two points in the input are in each of the attention heads. And there are attention set at the low layers, and attention set at high layers that are the Layers that are the colored green-colored ones. And in the large data set, you can see that there are low layers that already start paying attention, there are some attention heads that pay attention only at things that are in a locality, whereas there are some things that pay attention also at inputs that are not in a locality, so that are far away. So this is what makes them so powerful to actually generalize better because they look at Actually, generalize better because they look at things that are also far away from the beginning already at the early days. At least you don't have it in CNNs. So, a bit about, I said, like I was thinking about introducing a bit about my work, and I was just mentioning it. And if you're interested in it, we can talk about it later. So, one of the things that I was working on, and I think is also one way to go, is to use interpretability together with domain expertise to kind of improve the models that we have. And I think that this is. Models that we have, and I think that this is also a pattern that we saw during this workshop. So, we saw that there is the need for the main experts to actually understand if the model is doing something that we would like to do. And once we check with them, we need a way to correct for model behavior and to make the models better and to represent better what the experts want. And I came up with this idea of exploiting essentially the inductive bias that is done by multitask learning to introduce these changes in. Introduce these changes in the network in a softer way that is maybe less harsh than what Rich was proposing, just drawing a draw line, a straight line. Well, I'm not drawing the straight line, but I'm trying to make my model draw it for me. So the idea is essentially to have a network, in this case with a CNN, because I was working on micro images. I to have multiple tasks as output. So I would like my model to obviously solve my main task, the one that is being trained for. The one that is being trained for at the beginning, but then I also wanted to take into account that there are a series of parameters that the model should learn. So I was working with isopathology images, I wanted to classify cancer against non-cancer images, and I wanted at the same time the model to pay attention at the nuclear size, at the nuclear density, and so on and so forth. So these are some concepts that are very relevant for pathologists. And you can really use this inductive bias in the model. This inductive bias in the model to make sure that your optimization converges to those solutions that take into account both of them. And well, I put this here just to show you that it's essentially what we're doing is just optimizing multiple losses. So we have just a multi-objective domain optimization problem. And the complexity of this is just how to weight each loss and how to make sure that. Each loss and how to make sure that none of them is overtaking actually what we really want to do. And another way, actually, that I think is a bit simpler, more intuitive, to use interpretability to improve the performance of our models is to actually look into again this transfer learning idea that is highly used in medical imaging at least and look at the problems that are in there. So I was mentioning that sometimes that gap. Sometimes that GABA filters are not needed for magnetic imaging, and this is because we mostly are working with textures. And another problem is scale. So, in natural images, we want to recognize an object category at multiple scales because the object itself is actually still the same. But in medical images, the scale is actually a relevant and determinant factor because if you're a tumor at a given size, you probably go to a certain category, whereas if it's larger, you will go to another category that is. Who will go to another category that is riskier also? So, the bigger you do, more. So, the scale is really important. And one of the things I was doing was to look at my latent space of the network and learn these regression planes that would be representative of the direction of increase of a given measure that I could compute in the input. So, in this case, I would take my input, image-like images, compute the bounding box, and use the bounding box as a measure of object scale. As a measure of object scale, and then see whether I can regress this information, and at what layer in my network I can find this information the most. So, here you have all the multiple layers of my RESTNet, and as you can see, there is the scale information is learned by the network through the layers, it peaks at a certain point, and then the model starts removing this to obtain the scale invariance that is needed and essential for network deliverance. And so we were like, we looked at these results and we were like, okay, let's just. To me, we're like, okay, let's just drop that, just drop all of those layers that are learning these invariants, and let's see what happens. And, well, long story short, it is actually very, very effective. It just reduces the error rate of your model and it works faster. So, yeah. So, this was a summary of what we know. And I think it's only the tip of the iceberg so far. So, when I was I finished my PhD and I was like, okay, now what? And I was like, okay, now what? What am I going to do? What do I want to do? What is worth investing on? Well, I thought after a while, Maria knows about it, I thought actually it might be worth focusing on all of that we don't know yet. So not using interpolability only to find and to confirm things that we know and that we want the model to do, but actually to use it as a means to find out what is it that we don't know. And an example of this is, for example, the AlphaGo. Example: the AlphaGo that beats the World Go champion in 2016, and then Oxford does an unusual move. So, this is one of those examples that might be fun to work with because it might be interesting to know what the model was doing to actually pick this move 37 that is very unusual, that puts the adversarial the the the player in a very uncomfortable uh position, but then it actually ended up being a winning strategy for the model. Strategy for the model. So the idea is there is now here: what is between what deep learning can achieve now and what humans cannot achieve? What is in the missing up? And can we learn something about it? And then can we use interpretability to find out? I think the answer is yes, it's just that we need to work on it. So I think that this is a summary of of like my position so far is that there is a knot a lot that we learned about deep learning, but there is so much more that we don't know. But there is so much more that we don't know and that we should try and find out. And to convince you about this, I just thought of doing one-minute trivia also because this was a very heavy week. So we can just try to answer these questions. Could you predict the gender of the patient from this retina image? And could you tell me if it has a risk of cardiovascular issues? If any of you can do this, please raise your hand and I would really like to talk to you. But a model can do it. So there is a CNN that can predict the gender. CNN that can predict the gender, smoking habits, and the risk of cardiovascular diseases from eye phones images. And when I saw this, I was like, I don't believe it. And well, do we believe it? I don't know. I think we have to use interpretability to actually make sure that this is something that we can believe and trust. And second trivia is: could you predict the current distribution of pollen in Switzerland from this satellite image? And a similar example is if you could predict gene mutations and express. If you could predict gene mutations and expressions in this light of human tissue. And just to let you know, this is a small image because I had to fit it in here, but the actual size of these images is probably as big as the entire campus over here. So we would need like a giant carpet to cover all the town center to actually have a good estimation of the machinery of the data that we're working with. So these tasks are above. These tasks are above the capability of many of us and of many domain experts as well, so it's not something that domain expertise would really help you. You could make maybe a better guess, but I guess you cannot really guess these things exactly. And well, actually, there is a previous paper that says that we can learn from human tissue microscopies, so from those types of images, patterns of gene mutations. So then, like when I was thinking about what to do with, I was like, okay, I know. To do, I was like, okay, I know what to do. I want to see whether I can use deep learning and interpretability to model some of these gene expression patterns and gene interactions from different type of data and see whether there is something that deep learning can do that we cannot do, but from which it can learn from. So, I'm working on this project on colorectal adenocarcinoma, and just to give you a brief introduction of what my problem looks like: so 80%. So, 80% of colon cancer is adenocarcinoma. So, this is a type of cancer that affects Tibetelia cells. And, well, sorry for the image, I'll try to make it look prettier, but cancer is never nice to look at. It looks like this when it arrives at the laboratory. And you can see here that there is some regular tissue structure, so this looks actually very nice. But then you can see that here, over there, there is some really irregular and ugly growth. Ugly growth, and this is at the macroscopic level. So, here we kind of already have a very good guess that it's going to be colorectal cancer because 80% of the cases is colorectal cancer, and because it does look like adenocarcinoma, because sorry, we already know that it's adenocarcinoma because 80% of the cases is adenocarcinoma, and it also looks like adenocarcinoma. So, there is this abnormal growth essentially, an inversion of all the structures of the tissue. So, now the thing is that there are So now the thing is that there are molecular differences in the patients that differentiate prognosis. So if we were able to know the gene expression in this tissue, we might be able to actually stratify patients and divide them in different categories that have different prognosis and that react differently to treatment. So knowing which patient belongs to what category would actually be helpful for treatment of these patients. And so the idea here is to actually use the So, the idea here is to actually use deep learning interpretability to understand if there is any relationship between the microscopy of the images and the molecular patterns. Because the gene expression is difficult to access, it's rarely integrated, it's very expensive, it takes lots of time, and it has high variability. So, if we can do anything about it, I think all the pathologists would be really happy because they can take it into account in their tumor bores, and oncologists may make more informed and better decisions in the patients. So, going to the technical perspective, what I want to do. Technical perspective: what I want to do: I have this input image, I want to learn actually what are the gene expression patterns. But here, I'm kind of giving the idea. I want to know where, given a gene, I want to know where this gene is highly expressed, or is lowly expressed, or somehow mildly and not relevant. And I want to see how this can help us understand the molecular subtype of patient and how the patient is going to then behave. Behave and it's going to have what kind of a prognosis it's going to have. So, to do this, well, the human genome is like 60,000 genes more or less. So, we have a lot to work with. And I thought we need to take a step back. The naturality of the problem is way too big. We need to understand how to select genes and find an upper bound. So, I started with from this result of the paper: there is 40 gene signatures of CMS that is colored. Of CMS, that is colorectal molecular subtypes. And I found some more papers where they say there are some genes that actually act as a biomarker. So these are previous studies that are published and well accepted in the community. And when I was here, after listening to the talk of Rich, my hands were real, like I couldn't resist it. I needed to do something, so I was like, okay, I'm going to just try their machines. Because before I wanted to have an idea of how these genes were predicted of concretive cancer, and I was. Predicted of colorectal cancer, and I was using gradient-boosted trees. And well, I'm really happy with it. They work kind of nicely. They gave me a very nice AUC one versus REST, and I didn't need to tune them, whereas for the Radiant Booster 3, well, I think I needed to do a bit more high parameter search and stuff like that, and I really didn't want to. And well, one of the things I'm thinking is also that there is a lot of context actually. Also, that there is a lot of context actually that might be introduced, so that could be one of the nice outcomes of this workshop, thinking about how to introduce context. But going to the results of this, so I run this, and with the 47 genes, I get very good LUC, so it means I can really well predict from gene expression the molecular subtypes of the patients. And I looked at interpretability, and so these are the top four genes that are selected by Are selected by the EBMs. And these are their partners for physician. So you can see here that there are some, so for example, FSNC1 is known to be underexpressed in CMS2. And I think you see it here that if it's low, then it gives a higher probability for CMS2. So I like this kind of results because it kind of makes me understand what's going on. And there are other types of genes that are also known, for example, Are also known that, for example, there is another paper that our partners did that shows that mucin is highly present in CMS3. And here's what you see: if there is high mucinin, then the variability of CMS3 grows. So this is a nice intervalability outcome that I have from the sole gene expression analysis. Now, the question is: how do we introduce images? And I think we need to pay attention. We need to pay attention as the model does. So I won't go into details of working with these images because it's kind of a nightmare. Long story short again is you have these gigantic images that will cover the entire bound center and you extract smaller patches, so smaller images size, and you feed them into the network as a set. And you predict also as a set. And the idea here is to predict the gene expression value for a patient as a set, which is the only thing that we have a As a set, which is the only thing that we have a label for. So we input all of these images and we say, well, the gene expression of this patient is for this gene is na na na. Do your best to try to figure out how to guess this from the images. So we have a single model per gene, which means in my case I have 47 models per gene. And I'm doing fivefold cross-validation. So I have five products. I have five prototles per gene, which means something like 47 times 5. That's my total number of models that I have. And well, what is very hard to do for a pathologist is actually not random for the deep learning predictions that I get, because my mean average percentage error is less than 60% when it's average across all genes. And the CMS predictions from images, obviously, the performance. Obviously, the performance going from the ground truth gene expression to the prediction that I get of the gene expression from the images is highly deteriorated because there is a lot of noise involved and there are some genes that we cannot actually predict from the images themselves. So the assumption here is that if there is a gene dysregulation, the tissue is also going to look different from a normal controlled tissue. And so we can predict this from the images. But there are some genes that maybe just don't. But there are some genes that maybe just don't follow this kind of assumption. So the performance is deteriorated because of that. Going a bit deeper in the results, but I think I don't have too much time anymore. Some of the genes that were reported as top genes for EBMs actually are not well learned from the images. So they actually have a higher rate. And I think what we need to see here is whether it is possible to learn these things from the images, as I was saying. The images, as I was saying, but learning from images points us to other genes that actually have low error rate and high correlation. And so, what I thought was interesting doing was interpreting the model. And this kind of goes and follows the discussion that we were doing yesterday. So, how to generate new knowledge? Well, you train multiple models, and the idea here is to train it on different speeds of the patients while keeping the test patients always the same. So, I have this ensemble of models that are. Of models, there are five models that have seen different patient splits, and I test all of them on the same test data, and then I take their patchwise attempts and I ensemble it for the test patients. And so I have a robust estimate of what the model is paying attention to, because essentially I take five models and I look if they're paying attention to all the same things in the same patients. This kind of also follows the work that Nikola presented. Folllows the work that Nikola presented at the beginning of the workshop. So, a way of visualizing this is actually cherry-peating some of the results that we have. And yeah, you can talk to me if you want to see more images. But the idea here is that you have these two images for a single patient, and these are the predictions of the model here right for the gene expression of axiom2. That is one of these genes that. Of Axim2, that is one of these genes that we know are highly expressed in are underexpressed in CMS-1 patients. And you can see that the model is predicting both areas where the gene is highly expressed, it is in red, and where the gene is underexpressed, it is in blue. And then we can look at the model attention for this gene, and we can see that the attention is also kind of distributed both to high prediction areas and low prediction areas. And just for the sake of convincing you, For the sake of convincing you, I'm showing you the attention of the model to a different type of gene, so to show you that the model is not paying attention in the same locations for predicting the distrust of different genes. And this, I think, is insightful because it's what we would like to expect, right? If some genes are expressed in different regions, the model should also pay attention to different regions. Obviously, this is cherry-picked, but I have more results. But I have more results, so I mean, come see me if you want to see more. But I think there is another way of going that is actually by measuring and not only visualizing the images. And well, this is a lot of content, but ideally here I'm visualizing one of the top gene resulting from EBMs. And I'm looking at the different histograms of the gene expression levels. So from low gene expression, predicted gene expression to high predicted gene expression. Gene expression to hybrid integer gene expression, and I'm sorted the top five patches that had the highest attention from the model. So the model is paying very high attention to predict low gene expression to this kind of patches from the top to the bottom, like first one to the fifth. And one thing that I saw in this is that, so TP53RK is known to be underexpressed in CMS2, but CMS2 is highly underrepresented in our data set, and what I think is It in our data set, and what I think is happening, and what I think is why the model is not predicting very well this gene from the images, is because there are not enough images of CMS2 to actually learn this pattern. And you can see this because, well, you cannot see because the text is very small, but trust me, there are a lot of CMS4 images that appear in the top-predicted attention. And I think that if we were to introduce more CMS2, then the model might actually learn this button better. And one thing you can do to show And one thing you can do to show this is to evaluate the structural similarity index between the images of the top attention and see how these images differ. So we would expect to have that the images differ widely from low gene expression to high gene expression because there must be a difference in the features in the image that is looked at to actually guess whether it's low or high gene expression. And here I think the model is a bit confused because this is actually not happening. But if we look at another example where the gene is highly expressed in CMS4, that is highly represented in our data set, what we can see is actually something nice, but the high gene expressions and the attention is being paid to a lot of CMS4 cases, and so a lot of images coming from CMS4, and the structural similarity index is actually doing a better job at separating the visual features between all these images. And obviously, this is very preliminary because I only Obviously this is very preliminary because I'm only getting started. So if you have better ideas of how to do this, I usually have a couple of ideas that are better than structural similarity index, but that's what I go to so far. So as a final remark to finish this talk, I think there is so much we do not know yet and we need to understand. And interpretability might actually help us fill the gap. And I think this is a very exciting time to live in, and it's a very exciting way of using. It's a very exciting way of using interpretability. So, we could use interpretability to uncover new patterns, but then my questions still are there. How do we assess, verify, and test new knowledge? I think we need a formalization for that. And how do we entangle relationships that are real in the data from spurious relationships that our model might just be picking but it actually knows? Especially when we don't have any prior knowledge about this. So, I think attention mechanisms are. So I think attention mechanisms are a nice way to go. They could teach us where to pay attention because the model is looking at those things to actually make predictions. But yet we need to understand how to actually translate all of this into new discovered information. So this is all preliminary work and I just have an advertisement now, a moment. If you guys are working on medical images, priestess workshop, I think I 2022, there is a lot of fun. I started from there and I started from there, and so if you're working in practical s just. That's the end of my talk. If you have questions, yes, I have two questions. One is a bit general, like I would like to move you to think about so let's let's let's talk about the learning greediness of this system. Of those systems. Like, for instance, the radina example, but it also applies to the cats. Like, imagine that you take all images only from healthy people, and then you make up the labels, and you say those are healthy, and those other ones are not healthy. And then you embed smoking values on it. There will be differences that will be learned by the system, even if you do have a lot of. By the system, even if you do have all the healthy. So, like, but I like, you know, the question is: how, to what extent you can be fooled by the difference that are learned and the labels that you have. So, what, you know, what is the jump that you can make, like, for instance, here as well, like in the boretal transfer case. I mean, I was a bit puzzled when you say in this region the gene is highly expressed. I mean, not really. Like, this is a region that is learned because you Because you said that image has a hype. So it's something something different, et cetera. So there are two things here. Thing number one is to address your question about giving me false labels and all that. And I think there's a lot of interesting work in memorization patterns of the network that is really worth looking into because they actually tackle this problem and it is really interesting. So they give the net the model false labels, like they just truffle the labels to break the real patterns in the data. Break the real patterns in the data. And what they observed is that you can look at your loss curve, and the loss curve will be widely different if the model is memorizing, so trying to fit noise, from whether the model is actually trying to fit the real distribution that is underlying the data. So the assumption here is that if you give them real data without corrupted labels, then there is an underlying distribution in your data, and your optimization problem should be able to recover that first because it's easy. Because it's easy to learn what is real in your data. Then, if there is anything that has been broken because of corruption or noise that happens, then those samples would be the hardest samples to learn and the loss will actually converge much later on. So, I didn't put it in the slides, but we did some experiments with that. And you really have two different loss curves. And actually, when I was presenting this paper, there was another guy that also was working in that. And I was like, Yeah, you can model it with a bimodal distribution. It with a bimodal distribution. So it is really a bimodal distribution that has two different means. So that's kind of a way to go. If you're fitting noise, you look at your loss, but you have to do it sample-wise, and you will see that some samples follow one type of distribution with a kind of mean, and some samples follow another distribution with mean that is shifted, that takes longer time, more echoes, essentially. That's for the first point. And for the second point about the gene expressions, so The gene expressions. So I didn't give my model hard labels of where the gene was highly expressed in my images. So I just give the model the images and then a patient label. So the idea is that, and this is attention-based model for distance learning, and the idea at the basis of this model is that it's leveraging the fact of having only a global label to then go and pay attention to different RAs in this image, and the way it's computing the final. And the way he's computing the final label is a weighted average based on the attention of all of these inputs that he's looking at. So, obviously, that is a noisy prediction, and that's what I was saying, right? If you look at my average percentage error, it's not the best result on the planet. It is kind of high. It's lower than 60%, but it's still 60%. It's not 20%. So, obviously, there is some error involved, but I think that it is. It is in some way much more robust than what exists right now in the literature. That is using the model to predict batch per patch the prediction and then just doing an average of it. I'm doing an average but I'm using the model attention as a weight and why I think this is better is because we can actually look at the attention and try to use some interpretation of it to understand what is going on using obviously the the help of some Help of some metrics that we can compute from the images, which I haven't done yet, and also asking for help with the pathologists to actually ask them: do you see any differences between these images? So, all of what I was doing here is kind of a validation of my model to say, well, if this assumption of attention as a weighting combination, as a weighting function, is correct, then I want to see a couple of things. I want to see that as I go from lowest gene expression to high gene expression, there are different patterns in the data. Because if there aren't these different patterns, then Because if there aren't these different patterns, then maybe my model is just being lucky. Find some similarities between the histological patterns that are learned and the genes that those patterns correspond to. Maybe some genes that are maybe co-expressed or maybe have some functional implications, they are pointing to similar isological patterns or similar regions. It's on the tool list. It's on my tour list. It's actually one of the things we were thinking of doing to validate this idea of attention. So, yes, as you said, like if some genes are co-expressed in the same regions, then my model should pay attention to the same regions and have a similar prediction. I haven't done that yet, but I want to do it yet. And do you know if there are special tasketomics for color hats that you might use to validate even more? Yeah, so I'm not working with this yet. I don't know if there are. With this yet. I don't know if there are special transcript plannings, honestly. For TCGA, I don't think there is anything. But there is another way to go that is immunoistochemistry. So you could have some antigen that is being injected in the slide and then it colors some parts of the slide. And the idea is to actually look at these kind of images here and to see if these red areas correspond to the color of the immunohistochemistry. That's some kind of validation. Instrochemistry. That's some kind of validation that we were thinking, but obviously it involves lab testing, so I first want to dissect my model until the very last, until I'm sure that it's worth doing some lab testing. So at some point at the beginning, actually, you were talking about transformers and how they differ from ResNets, the way they look. And can you go back to the slide? Because I said the closest. Can you go back to the slide because I said the close question? I didn't want to do all of this while you were talking, so we could paint it for the eyes. So actually what yeah, what it shows on the left, top one, so it shows that the resonant is actually kind of lower part and a higher part. And I wanted to know your opinion on what's the role of supervision here, because the transform is an unsupervised task and the present panel guessing is a supervised task. Uh I don't remember what uh I think this is a vision transformer, so it's still trying to learn the classification. I think you can put in a vision transformer you can actually put the class label and just you add it they just did some kind of trick for it. But I think they actually studied this this I don't remember how they call it, but it's some kind of black box effect, right? That you have these black boxes here at the and they studied it in another paper. What is happening What is happening is, and I think what they also claim in this paper is that the reason the transformer is like this is because it has lots and lots and lots and lots and lots of skip connections. And the skip connections actually help diffuse the information from the top to the bottom and to keep all of it. ResNet also has some of them, but not as many. And so that's, I think, what is happening. And the next question is is that something we want?