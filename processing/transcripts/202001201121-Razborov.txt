Several contacts, as we will see, and at uh Jacob's request it will be a more or less a technical talk. Part of it will be on the blackboard, real proofs, actual proofs, and to compensate for it, the first part will be quite relaxed, as you can see from the first question. So we all love restrictions, there are no questions about it. If you don't love restrictions, it only means that you are in the wrong place. means that you are in the wrong place. So why do we love restrictions? There are restrictions. There are many reasons why we love restrictions. We know we can apply them to many things. Right, we know that we can Know that we can do lots of stuff with them, both in circuit and proof complexities. And I will not even try to survey, by the way, my survey will be always, will also be quite lost. So if I missed some words, I'm sorry about it. It's just an illustration. I don't even try to attempt what we can do with random restrictions. Let us better look at what they are. What they are. So, a random restriction, a restriction is written here. That's what we all know. However, we would like to view it slightly differently. Namely, it's a substitution that not only leaves some variable, it does not leave anything unassigned, but it assigns every variable to some value from this list. And when we start thinking what properties this function pi is. Is having here, we actually see here two basic properties. First of all, it is rate once. The variables of pi of xi are mutually disjoint, and the second property is that it is a projection, which means that pi of xi is illiterate. And if you think for 30 seconds, then turns out that these two properties, they can actively describe restrictions at all. Restrictions, so permuting variables and negating them. So, one and two is precisely means restrictions, and in the simple part of my talk, I will basically look at what will happen if you now relax these properties and remove one of them, and then we will remove both of them. So, and it is also helpful And it is also helpful to look at what I am saying now in terms of two main parameters. So we have contradictory formula tau or usually CNF and we have proof system we are working against. And when you apply your substitution, then different things may happen to either one of them. Namely, the complexity. Namely, the complexity in a sense may either increase or decrease or stay the same, and the same with the power of the proof system we are working against. And let me note that when I use these errors, I always use them in a constructive sense. For example, if you restrict your formula, which if you apply hashtag switching grammar, you restrict your formula, parity formula, it reduces the number of variables, but you can but you can neither use this fact nor it is neither helpful nor detrimental to you. Which means that you really don't care that it got decreased. We really care here in those increases or decreases that we care about or we will see examples. So and this is the most detection of what I just said. If we look at the classical results At the classical result, Larbal Ciginst constant depth freaky Earth systems, then that's what happens. Descriptive complexity of tau does not change. It is at the same CNF. The number of variables decreased, but we control it. For our curves, it does not change. Well, the point is that the strength of the proof system we work against drops by one. And then you recurs, and you get your result. That's a typical thing. So let's now. Think so. Let's now look at projections as the first step. Let's look at the projections. So we still apply our now we substitute every variable with a literal or with a variable say and the point is that now that this function j of i is no longer injective, some of the variables collapse. Some of the variables collapse. Actually, there are not that many applications of this case. So, again, I will be very thankful for any additions or corrections. But the only application that I will show on the next slide, why it is better than restrictions, it is better than in this way, particularly if you do it recursively, it helps you to keep track. Helps you to keep track of what's going on. And the only application that is worth mentioning is lower bounds for cycing tautologies that Paul already mentioned in his talk. We have cycling tautology for a graph G. We hit it with variable substitution, with a projection. So you will get second tautology on the new graph. Second tautology on the new graph. If your pi is a restriction, then it will be a subgraph, spanning subgraph of G. And in our paper with Michelle long ago, we somehow were able to handle this case for a problem we were interested in because if you do it naively and your graph G has constant degree, then of course it's no good. You will simply kill your setting tautology, which we don't want to do. You have to think about it. Don't want to do you. You have to think about it. You can do it. But for example, in our with our substitution spike that we designed, you really don't see how does the structure of G change. And this is the ultimate, I think, application of projections. I mean, projections as opposed to restrictions. Are these two papers that Paul also referred to? That Paul also referred to in his talk. Because if now you apply a projection to G, say it doesn't matter, then what will happen, and it can be easily seen, that G, folks, ask questions, please. Ask questions. I want to guess. Oh, sure. Okay. Okay, you can ask me. Next time, ask me. All right, then. Alright, then it will be in general, it will be a minor of G. Well, maybe not quite in general, but it will be sort of minor of G. And they applied this in the situation when you get only a graph homeomorphic to a subgraph of G. For example, what Johan did, he worked with the grid. So, and he wanted to reduce the problem not with some stupid graph, but with a grid of smaller size. I create a smaller size, and then you have to do arbitrary paths. But that's the only application of this technique I am aware of. So if you have any additions, then let me know. Let's now go in the opposite range, namely let us now do read once substitutions. So now fi can be complicated, but But we substitute formulas with mutually disjoint variables. And as it turns out, so far at least, I am not aware of any situations where these formulas of i for any reasons should be different. So we substitute the same formula, well, in different set of variables, but the same formula for every individual xi. So that's what. xi. So that's what Jakob called in his survey that was also mentioned here pi of f. Now since f is the same for all variables we can call it substituted formulas. And these things they were used many times. Let us first look at them at our scale. So now as you can see everything lifts. Everything lifts. So obviously, after you substitute anything non-trivial here, then the complexity of tau gets increased. And the hope is that the proof system we are working against also gets decreased. But we view these two increases in absolutely opposite ways. So we don't like the fact that the complexity of tau increases. Of tau increases. Ideally, we would like it to stay the same. First of all, you can overshoot and simply get a formula that is not expressible in the language of P, and then you are in trouble. Or simply aesthetically, someone said, I don't know whom, that mathematics is predominantly aesthetic subject. So under these guidelines, when you write. Right. Apply a substitution, then you would better not do if you can avoid it. So, it is it's explicit, but somehow anyway, so I it is it's very strange because Elea referred to Michel Likhnovich and me that we had applied this trick. I have absolutely no memory of it. Unfortunately, we cannot ask Nisha anymore about this, but Elegant also in many situations like this one, he can be trusted. So the first thing you can do, and then it was used by many other people, is that, for example, if you have just in resolution proof system, if you have lower bounce against widths, then you convert them into lower bounce against signs. Against size, resolution size, by this simplest possible variable substitution. And the proof is very easy. It uses technique from Bimen Pitazzi, or I think Basin-Pitazzi. Let me skip it. Then there was an app there were many excellent applications of this technique in space complexity when you combine it with uh graph pairing. With graph pairing, celebrated papers, and I am sorry I cannot mention here all people. Let me again refer to Jacob's wonderful survey for examples and for a complete account. But this technique is extremely important in space complexity. You can do things that you were not able to do before, and of course, everyone expects this word. This word. So here is the word. So these days this technique is mostly associated with lifting, but somehow I was completely sure before coming here that there will be many talks about lifting. I will just leave it to other people. It already started. So lifting will be somewhere else. So let's now switch to the next one. Switch to the topic to my topic of hardness continuously. In hardness continuization, and it is essential, we forfeit both requirements. We substitute, well, not arbitrary, we will see what we actually substitute, but it is not literals and it is not read ones. So, in similar things, Similar things were used by Jan Rajik in 2013 in a slightly different context, but let us stay closer to our talk. Here is what is our goal and what is our course. So now as the word condensation says, we want to condense or preserve well Or preserve, well, in particular, we want to preserve our hardness. So the proof system we are working against is the same here and there. It is actually, I hope it is seen here. It is not exactly the case. Actually, at least in one application, it also gets lifted, gets increased, but it's not the point, because you can get it lifted in a much simpler way. We really want it to stay the same, not to drop. To stay the same, not to drop. What is actually important is for applications, and it is our primary goal, is to reduce the complexity of tau. And reduce for total applications that I will be able to short to trade means simply reduce the number of variables. Which is our goal we are trying to achieve. Let's now be slightly more difficult. Let's now be slightly more technical. So, this paper by Nisa and myself, which preceded Atterius NÃ¼ller, one of the... So I tried to stay close to the notation of this paper. It used sort of condensation technique only with this caveat. Why did we use it there? We used it not to reduce the complexity of tau, but just to... Complexity of tau, but just to bring tau, it's the rotation from the paper, for our purposes today. It does not matter what are these things. So, or this thing. What matters is that our goal was different in that paper. Namely, you can define what do you have here before making substitution pi. But it turns out that this thing simply does not exist. That this thing simply does not exist. I mean, it exists, but it has exponential size. So it is somewhere over there. So we used this hardness condensation technique to reduce the complexity of tau from something which is exponential or we cannot handle it to something which is a polynomial size, so it's slightly different. But still, what is typical is that the proof. What is typical is that the proof system stays the same. So, we wanted that hardness in the resolution proof system. It gets very served. Any questions so far? Then let us now move to our current applications of this technique. So we need, like everyone, improve complexity needs we also need expanders. We also need expanders. This is mostly to fix our annotation. We work with ordinary bipartite expanders, and I do prefer to denote them and to use them as 0, 1 matrices. They are incidence matrices. So, and I will keep reminding the slide as we go along. But anyway, the point is now that M is much larger than N. So, V, it's not actually an expander, it's a condenser. It's not actually an expander, it's a condenser or whatever they call it. So M is the number of vertices on the left-hand side, N is the number of vertices on the right-hand side. So it's a marketing S will be left degree. In most applications we don't care about the right degree, it's left regular. And C is the ratio coefficient, and R is the upper bound on the size of the set. Upper bound on the size of the sets to which the expansion condition can be applied. So every set of size at most R expands by at least a factor of C. Good. So now what we do? This is just for reference. I don't think we are going to formally use it anywhere. It anywhere, so it is a typical range of parameters we are interested in. The left degree is relatively small, but not a constant. Then you can find very good expanders with these parameters when m is almost polynomial in n, almost n to gc, like n to g 1/3c or something like that. And it is obvious that it is the absolute. Is the absolute, it is the best you can do, and the expansion coefficient is almost also of order s. S is the main parameter, left degree is the main parameter. And this is the basic matrix, the basic substitution we are going to do. It is in theoretical complexity, it is simply called Nissan generator. And in group complexity, we just substitute basically every xi with the linear form. Every xr with the linear form, linear modulo, which is given by the matrix A. So entities, the notation, most of our results will work with this, all except for one or two, will work with this particular substituted formula. So again, tau of A is, you apply for tau, tau is a CNF, for example, you apply this linear mapping given by the matrix A. Mapping given by the matrix. What is R? R? R? This R. Yes. What is R? What is this example? For this generator. What? For the example. Parameters. What is R? Given the generator, this generator. R is the size up to which the set is expensive. Yeah, yeah, yeah, yeah. You're asking what it is in the typical case. It's good enough for all practice. Typical case. It's good enough for all practical purposes. I mean, it's interesting that it's sub-linear, right? Which is sort of a slightly non-standard expander setting. It's sufficient for all. I think it's a roughly error. I mean, it will be n to the. So it's not sublinear. Sublinear in what? Jakob mentioned that R is usually sublinear in terms of N. No, no, no, no, no, no. It cannot be sublinear here. No, no, no, no. It cannot be sublinear here in terms of M, because M is much larger than N, which would be at most R. I think it can be sublinear in N. And divided by 10 or something. So it's good enough, fortunately. It's good enough. All right, more questions. Very good. So, and let me brief Let me briefly mention one variation that is used in one of the papers. It actually is a combination of Nissan, Victor's engineerator, and multi-variant logic. That is what I already defined in a slightly different language. Namely, these are those elements in A which neighborhood of I, where it is a function, and we apply this parity function. And we apply this parity function to those distinguished as elements, and it is my generator, each is my substitution. So what we can do, we can change parity to an arbitrary function g and instead of 0, 1, we also can consider an arbitrary finite domain. So then this value will be in G, which means, alright, you probably don't. Which means, alright, you probably don't want to read it, but anyway, so it's this mapping, it transforms formulas of d-valued logic to Boolean formulas. And that's what we need in one of the results. And in another result by Berkhard and Nordstrom, so the So the formula is sort of implicit because they work in the context in the context of descriptive complexity, and by the time they get to applying the substitution, there are no formulas anymore. Formulas are gone. So they work and do all these things where it's inside already the world of descriptive complexity, but more it is just the same substitution. So it is our The same substitution. So it is our basic object. You apply linear transformations and the metrics must be a good expander to your variables. So now then in this part of the talk, very briefly what we can do with this technique. With this technique, we can mostly prove, with one exception, we can mostly prove what I called supercritical. What are called supercritical trade-offs, which I think are cool objects. So, when you prove an ordinary trade-off, so then what you do, all right, here is your one parameter, here is your second parameter, you trade all of them, and there is a proof of this form or that form, but you can't combine them together. This is something that was discovered in three steps. First of all, I wrote a paper, fortunately, a draft of a paper, where it was claimed that the Where it was claimed that the opposite is obvious. As the second step, I thought that perhaps I should be slightly more generous to my readers and forwards. And in three weeks, then it happens. So what happens in subject very bizarre? We have small bridge refutations of tau n and And no, we have tau n has refutations of small widths and small and it has something like refutation. We don't care which one but if you make uh the parameter tau n uh if you so w is uh the smallest width. Is the smallest width. However, if you now try to combine these two restrictions, namely look at both small grids and good three-like size, then you overshoot to the second exponent here. Which means that if Which means that if you are economic along width, then you cannot beat not only large widths perove, but you simply become double exponential in the size. So you really don't need to say anything about the seconds kind of rules because everything every formula has to really refutation of single exponential signs. And so the first application is First application is this trade-offs for between three-like and three-like resolution and widths. Then another application also between widths and cutting planes, lower screen reproof systems. You can define widths for those roof systems in the obvious way, simply require that everything you write, all cuts you do. Right, all cuts you do all cuts on linear constraints of small grids, and you will get the same thing. So we will see it in the open problems session, in the open problems, on the open problem slide. It's way more complicated and I cannot combine these two results into one. So proof techniques are slightly different. It will be an all-bunk algorithm that I know. It will be a low one that I will mention later. Yes. Is the number of graphs really double-exponential or are the number of graphs polynomial? Yes, exponent must be removed here, but still it's right, superlinear. We are not used to it. And yeah, thank you. This miscreant. So now we have these two results. So it is a supercritical trade-off between width and class phase by Christoph and Jacob. So here it works only within this range. So it gives you trade-off between reads W and W log W, but in this range it becomes separate. It becomes super critical. It is a wonderful application that I already mentioned. It is in descriptive complexity. Very, very different area. And I think it was an interesting problem with that area. You have k variable logic, so you have two model isomorphic structures. What is the quantifier depth in K-variable logic? You need to separate these two structures. These two structures, so and so they were able to give essentially a tight answer. So, in k-variable logic, you obviously have to also restrict all your predicate symbols. They must have at least k variables, at most k variables, that's why their bound grows when k grows, because you are allowed to use predicate symbols of different sizes. Different sizes. And this is also something between variable space and resolution length. So if you restrict where, and it is just the same pattern, if you restrict variable space to squared of optimal, so it is here the quadratic gap between a second and a squared. gap between S and S squared, then it becomes crazy. So it is obvious theoretical information upper bound on the proof of on the size of the proof of any variable space as refutation, just because it is the number of Boolean functions in S variables. Turns out that it is uh it becomes uh as bad as this. Which code system is this? Good, it does not matter. Variable space is invariant. Semantic. That's why it is lovelier, proof measure because, or proof complexity measure, because it is invariant. It's mathematical. It operates with arbitrary Boolean functions as long as some variables is constrained. More equations. And here are three open questions. I already actually mentioned almost all of them. So it would be great to combine the first two results into one. Because for cutting planes three-like loops, we do have good lower bounds. And we do know how to prove lower bounds, so it's not out of our reach. You can do it. And the results that I mentioned on the previous slide and on this slide, they also have this gap, which is less impressive than four-colour results. And to what rescuers are to improve it, and of course to find more applications of this technique. So that was an introductory part of the talk. Now I am going to, or in the remaining, what that What's that? 17 minutes or 18 minutes. So the chair is skipping, so we can go over time as much as we can. So in the last 18 minutes, I will give some tutorials as I was. Not asking is a good idea because it's a bit technical. So I will show my last slide. So this is just the reminder of what we are doing. So we substitute this thing with So we substitute this thing with linear substitution. So and this is the core of the result which looks precisely like lifting results. And it says that if you have three like refutation of the substituted formula, Of the substituted formula, then you can get back small relaxed refutation, then you can get small depths, resolution depth, refutation of your original formula. Alright, so let me now attempt at lifting. You have to take the projector off. What? Uh go to the panel. Go to the panel and turn the projector. No, I don't want to turn it off. Oh, you want to mute the projector? Can you mute? I want to have this light on the board. Oh, you want it? Okay, fine. Okay, start. Right, so let's now let me try to show in 15 minutes. I thought that I will be able to do just the same justice. There's a light for the board that you should turn on. Right. So let me do it. So either this is the world of y variables and this is the world of x variables. And let me note that I have changed the notation. Now y became x and vice versa. It is to make it compatible with the paper. If someone wants to look at the more details in the paper. Into more details in the paper. That's how it is there. So, what do we have in the X world? In the X world, we have to really like refutation of this formula where EI where EI are x terms, are my original x terms. And we want to construct from it a good refutation of the original formula on this part of the work of the board. Alright, so the way we do it actually it resembles very much Resembles very much lifting techniques. So there are two caveats. The first caveat is that I am an old-fashioned person, I will do it bottom-up. Well, you probably can recast it as a game, and for many, or for some it will be more instructive. I like bottom-up things. We start from the actions and we move to the right. So what is more important is that now, according to the general recipe, we see here some clause E in this refutation. And we somehow want by induction, we want to come up with the both clause C E here that has, so if E has a small tree like some Small tree-like size proof, then this guy will have small depth proof, and then we just proceed by induction. That's what leafing techniques do. You should somehow be able to reword this mapping and construct CE from here. Now comes what I think is the first difference, or maybe it's not a difference actually, is that CE Is that CE is constructed by adversary? So CE is non-deterministic. I simply tell you what can you do to construct your CE and as long as you, the adversary, play along these problems, then I promise, and it will be our inductive statement, that CE has small. That CE has small depth refutation. So, now what are the restrictions on CE? C is a class. Everything on this part of the board are clauses. Actually, CE is a clause. So, a clause is specified by its set of variables and it is specified by its value. Specified by its values. Which will define two total different conditions on CE that I require. Let me start with the second one because it is similar. So the second one C on the right. The second one simply means The second one simply means that, let me first write it down, then I will explain it to you that after I apply the separator A to I C E, then it will be mutually falsifiable, which means that there is a truth assignment that falsifies both of them. It falsifies both of them. So you mean C E or C E? Instead of C, you mean E? C E of A and what's the C? Where does the C on the right-hand side come from? You mean E? You have C E of A and C. What is C? Oh, you want E instead of C on the right? That's all right. I want a borrow where I will tell you that I want seems to coffee everything. Ah, okay. Okay. Okay. Good. People follow. I'm surprised. Alright, good. Alright, it means that if you are applying. Alright, let me make a dictionary so that I will not do it anymore. So people mutually falsifiable. So if we are a class, then mutually falsifiable symptoms consistent. Two classes are mutually falsifiable if and only if they are consistent. Think about it. Uh two classes, regardless of their set of variables, are mutually falsifiable if and only if they are consistent. If and only if they are consistent, regardless of the set of variables. Any objections? Yes, so but this thing is not a clause. That's why I use it in this way, because it turns out that it is the simplest way to say what I want to say. So they are mutually falsifiable. Second is easier. The first one requires The first one requires the standard game we play with expanders. Let me here, since it was done, counting many times in both complexity, here I will be only brief. So what we do, so if I have any set of any set of colours. Any set of columns, which means every set on the right-hand side. Then we define as a kernel of J is basically all vertices on the left-hand side that completely map to J. I can write down the formula. I can write down the formula. K of j is all vertices on the left-hand side that map into j completely. And for every. No, it's easier for it now. So answer. So, answer now what we can do. If I have a set of columns or verses on the right-hand side, then I can remove them from my graph. And at the same time, I remove everything which is in this kernel. So, obviously, after I remove j, then this vertices will have awful expansion, namely, expansion. Expansion, namely expansion zero. I remove them from the game. And the point is that again, it was used so many times that I really don't want to stop on it. Is that if C is not very large, and now that's the place where we use the fact that my refutation has small widths. So if uh C has small widths, right, just keep my conventions. So if C has reads at most r divided by 4, where r is the expansion condition, then you can find, actually it's the adversary job to find it, but we will help him or her because we will also need it. So then you can always find a set of columns. A set of columns J which first of all will contain all variables in C, it will cover variables in C. Second, it will not be very small, it will not be very large, it will still have size at most car oversold. You slightly enlarge the number of variables, and after removing this J. J's and all corresponding. All corresponding. I think I forgot to say what is. Ah, and all corresponding vertices, you will still have an expander. So C walls 2, C slightly drops. This parameter slightly drops. It becomes This parameter slightly drops, it becomes a slightly worse expander than it used to be, but it's still good enough, and then you will have it. So, and that's what we need, because the second thing that the adversary can do is to take this any J and it's non-deterministic because this J need not be unique with. This J need not be unique with these properties, and we require that for his choice of J variables will also contain variables of EC, will also contain everything in the kernel of J. J is in there in the hand. J is in the hands of the adversary, but after the adversary picks it, everything she must include all variables from Kerr J. And that's basically it. So the claim is that it is proven by induction that if I But if I am in my tree and I have a clause here, C, and the size of this tree is save age, size of this artery, then wall matter, water culture. How that were the depth of resolution depth of the refutation is stays as R by bubble by log H. Alright, so I have five minutes left, so I think what I will do, I will show the most I will show the most complicated case, but I will give I will give so it's it's it's probably the case analysis I will do the most complicated case but I will I will only give some clue how to do it so what is complicated case first of all it's inductive step we have here C1 we have C2 C1 comes over there we have here C1 V have C2, and V have the closed C. So, and third verse repeat this pair, which resulted in the class E C. We want to show that E C has a small resolution depth perfutation, resolution proof from our axioms. And then Then this choice of the adversary comes with j and well comes with his choice of j. This is the variable xj on which on which we resolve. And the most difficult case is when j is not actually When j is actually not in j. When the small j, the variable in which we resolve, is not in the set j chosen by the adversary. So what we do? Let me just describe basic steps. First of all, we take a falsifying assignment which exists by Which exists by our inductive assumption. That's what we also do, and like in games, at least one of these sub-trees has at most half-size of the original tree. I look only at the branch where it has half size. In many cases, it's very much like lifting. And now I assign this new variable j. I assign it accordingly. accordingly either to 0 or to 1, accordingly to what part of my tree we have chosen to work. And it turns out that another can be ignored. That's why we get our regression on the size of the tree. So after assigning this variable and enlarging j my expander properties, it can get slightly worse, but not much worse. We will still have one half. it will still have one half here. And at this point, everything which is larger than zero is good enough. So it is still maybe very lousy, but still expandable. After we fix the chain, and after that, what we do? We ask the adversary to come up with, so assume that we went along this line along this line. But along this level along this branch, we ask the adversary to come up with arbitrary values to arbitrary class ECZR that corresponds to that branch. And that's why we should care about this, because the adversary can do it arbitrarily, but for the argument to work out, we have to be sure that it exists, at least one choice. Exists at least one choice. But after the adversary picks out this choice, so we will have a new set of variables j hat corresponding to this choice. It's the new choice. So this is J. These are J uh J hat. They have the kernels I and I hat uh and And what we prove? We prove that if I take my class, well, the adversary class that corresponds to E, and if I extend it a bit warrior to new variables from i-hat, we don't have almost any control over them, but we do have control over the numbers. They are at most r over. Kerr here. We do have a control over the number. We can show that any false infrastructure. We can show that any falsifying assignments of these variables to the remaining variables, it can be actually satisfy all my properties. Therefore, the plus corresponding to this falsifying assignments, by inductive assumption, it will have low Depths proof. And after that, after I have all extensions of my closed cutis variables and all of them have low Depths proof, I simply combine all those. I simply combine all those proofs in the brutes or search and it will give me precisely this increase in the depth because there are just R over to all of them. That's roughly how the technique works. The simplest case it is more complicated in other applications, but I hope that the these techniques it may find Techniques, it may find applications elsewhere. Thank you. Questions? Is that the only place where you use three lines? So is that the only place where you need to have a tree? That step? Because otherwise you cannot control the size. Because otherwise you cannot control the sizes. And uh and uh it is simply not true. It's not true. It's not true because uh the greedy algorithm that's the point that if you have any refutation of with W then you automatically have a refutation of size and with W just because. Just because. And it grows exponentially if you If you consistently thank Alexander again. And then photo detection. Thanks. 