Disappointed if I don't see a couple of sheep during this talk, Frederick. They were here a second ago. Maybe they'll come back. Yeah. I hope so. All right. So, well, Frederick Jens from Rice University, he will talk about Eul's nonsense correlation, Wiener-Caus analysis towards asymptotics and testing. Thank you very much, Professor Sammy. Much, Professor Sammy, and all the organizers of this workshop for inviting me. And of course, I'm disappointed that I can't be down in Oaxaca with everyone. But so this is joint work. I don't think I have it here on the slide. This is joint work with, oh, here it is, with Philip Ernst from Imperial College. Some of the things I'm going to talk about are with Suskaya Duisi, who's at Kariyad University in Marrakech, Morocco. And the plan of the talk is that I'm going to introduce Ewell's non-senator. Could you try to be on full screen? Let's see. Yeah, let me do that. Is that better? Yes, much better. All right, great. So the I'm going to introduce you all as not. I'm going to introduce Yul's nonsense correlation with a little bit of detail because it's maybe not very well known. I'm going to talk in particular in this second section here about a chaos representation for the components of this correlation. And it's a very elementary object, so it won't be hard to follow at all. And the one thing that we're, can you guys see when I move my mouse over the screen like this? Is this okay? When I highlight the, or is this not picking up on your side? Yeah, that's good. That's good. Up on your side, yeah, that's good. That's good. It looks great, okay. Yeah, okay. So, you know, this term here, the variance of this correlation, is an object of interest, and I'll explain why. We'll present a formula for the variance of this nonsense correlation for indiscrete time, and then a question of conversion speed for the law of this correlation, and I'll explain why that's important. And then we'll conclude with really, it's kind of the topic that I want to talk about, maybe the most, but it won't take the most time, is conjectures on the S. Time is conjectures on the asymptotics of the fluctuations of rho n and what that has to do with practical implications for statistics. And so if you consider two time series, two sequences of data points, you can call them S and T if you want, and assume that the two sequences, two time series, if you will, they're identically distributed, not necessarily independent, not necessarily stationary time series. Not necessarily stationary time series. And in this talk, we'll discuss mostly random walks, which are not at all stationary. So imagine that S and T are random walks. But whatever they are, you can always define a Pearson correlation coefficient the way that people usually do when they're trying to understand a correlation between two sequences of data. But these are time series, so it looks exactly the same whether these are time series or not. And it's the standard correlation coefficient for any two sequences of n data points. Data points. And if the data are IID, in other words, a very, very specific, sort of very simple dependence structure among the data points for each of the two series of data, if they're IID with two moments, then of course we know that the Pearson correlation converges to the true constant, which is the correlation between the two sequences of data. And a central limit theorem will hold for the fluctuations of rho n. And this result also persists. And this result also persists for short memory stationary time series. Maybe you've, yeah, I mean, there's a classical, I guess, result that people see sometimes when they study probability theory for M-dependent sequences. That'll work here. But it's not at all true that rho n will converge to the correlation of s and t when s and t are far from stationary. So remember, s and t have the same distribution. They're two time series, if you will, but they may be far from stationary. And if that's the case, then you can't rely. The case, then you can't rely on that correlation coefficient. This is something that the British statistician Udney Ewell noted in 1926. He noted it using an empirical analysis using random walks. So he said, you know, what happens with time series, which are, I think the way that he put it were strongly correlated over time. He may have used a different word, but random walks was the example he chose to use. What he noticed is that the law of the What he noticed is that the law of the Pearson correlation is diffuse over the interval negative 1, 1. It doesn't concentrate no matter how many data points you seem to take. In his original paper, he started with 50 data points and then he went up to 600 for the length of these random walks. And things are looking more and more. I don't have any pictures in this presentation, but what he showed is that the law of rho n seems to tend to a law which looks a bit like a beta distribution. It's not a beta distribution, but it looks like one. And it certainly does not concentrate. And it certainly does not concentrate at all. It doesn't tend to Dirac delta at all. It tends to something which is completely diffuse. In fact, for random walks, rho n is going to converge in law to this expression here, this rho right here, where these the numerator and the two denominator terms are defined using this expression here using Wiener processes. So this is simply because random walks converge to Wiener processes. So when you write Walks converge to wiener processes. So, when you write it all down, you end up with a little bit of scaling, you end up with this expression. So, that explains, you know, in some sense, you know, we could say, all right, you know, the study is done. We figured out what the limiting law of this row would be. It just takes a little bit of elbow grease to prove that rho n converges to rho n law. This was done in the 1960s. And the law of rho is whatever the law of rho is. And it looks a bit like a beta distribution, but it's not because it's not the ratio of two correlated gamma distributions. Gamma distributions. It's the ratio of in the numerator, we have an expression here with the two correlated W's correlated, or they could be uncorrelated. They could be independent. But you see, you have an integral of their product, then you have the product of the integrals. And the dominator, you have the same thing, except you don't mix the i's and the j's, and you take a square root. So that's the story. So that could be it. But so that could be the end of the story, right? And in fact, some people in statistics think that this is it, that there's nothing else to ask about. Is it that there's nothing else to ask about? But so, as I said, rho is clearly diffuse. But you'll ask in 1926 if one can compute the asymptotic variance of rho n, if we could figure out how much it's equal to, in the 60s, as I said, probabilists started getting interested in this problem. And of course, because that asymptotic variance is nothing more than the variance of rho. And rho, I have it on previous slide again. Rho is explicit, it's got its own variance, that's it. So, how much is that variance equal to? Well, this was a problem which. Well, this was a problem which, you know, in some sense was defined in 1926. In the 1960s, probabilists started working on it. Nobody could get anywhere with this problem until in 2017, Philip Ernst and a couple of other co-authors published a paper in the Annals of Statistics with a proof of a formula for the variance of rho. So they closed a 90-year-old, it was a 90-year-old mathematical problem on a statistical question. They closed it. The two other, the two co-authors on Philip Ernst's paper, one is a Philip Ernst's paper. One is a climate scientist who really doesn't do any mathematical statistics. So his contribution was, you know, whatever it was in terms of framing the problem. And the other person on that paper is somebody who had died two years before. That was Philip Ernst's PhD advisor two or maybe three years, maybe more. In any case, Philip Ernst put his name on the paper because without his advisor's help, he wouldn't have figured out which direction to look into. So it's all fair. Into. So it's all fair. With the Sukhana Dewise and Harifa Subai, in a paper that published, we published last or yeah, last year, I guess, in Electronic Journal of Statistics, we noted another interesting thing, which is that if you look at, I guess I have my I's and my J's and my K's and L's. I don't know what. Yeah, the K and the L are just the same as I and J, and they are equal to either one or two. So now So now consider the same expression. Let me just scroll back real quick so you can remember. You see that there's that expression there for integrals between 0 and 1. But what if you decide to integrate between 0 and t instead of integrating from 0 to 1? And then you have to, to scale things properly, you should divide by t over here so that things have the same physical dimension. And then you write down your correlation coefficient this way down here. This is the same as the row with t equals one. It's just you can ask yourself. Equals one, it's just you can ask yourself what happens if we have continuous paths over an interval of length t instead of a length one. What happens? Well, what we noted in this paper is that it's very simple to see actually that if that for random walks, this quantity here is constant in law. The law of rho t does not depend on t. And even if you replace w by self-similar processes like fracture boundary motion or other processes that we've looked at today in this conference, you're also going to get just by the self-similarity condition that By the self-similarity condition, that the correlation œÅ of t has a law which is constant as t changes. So that means to us, that means one thing up here at the top of this slide. There's no concentration because the law of rho of t is constant. And asymptotically, as t tends to infinity, that's not the place to look for any asymptotics. As t goes to infinity, nothing happens to your rho or to your rho n. If you're looking at rho n where you extend. If you're looking at row n, where you extend your number of data points to infinity without shrinking the size of your interval, without keeping your size of your interval fixed, in other words, if you don't look at the limit of high frequency of observations for your row n, but if you just let your data points go to infinity without interpreting them as higher frequency, nothing will happen. You will not get any asymptotics at all. But the first question that that brings to mind is: can the moments of Rho n be computed like Ernst did for Rho in 2017? Also, at what speed does 17. Also, at what speed does rho n converge to rho, and in what sense? Those convergences are actually meaningful. It's not as t goes to infinity, it's as n goes to infinity with maybe t fixed or t equals one. So you're talking about the convergence, what we call infill convergence. Could that convergence be the basis for a test of independence of the time series? All these questions are the ones which we are motivated by, and we're going to answer some of them. Why are these important in practice? Now, because we're limited in time, I'm not going to say too much about the practicality of all this. Say too much about the practicality of all this, uh, but um, oops, I think I hit the wrong key here. Uh, let's see here. My apologies, everybody. Uh, hopefully, this comes back on your screen here in a second. So it's easy in practice for practitioners to see that they're going to compute their rho or their rho n, and they're going to realize that it's far from zero, far from zero, and then they're going to. From zero, and then they're going to reject the hypothesis that the two time series are independent. That's a big mistake if the time series is time series or accumulations of data like you would have in a random walk, because rho is diffuse. It doesn't concentrate towards zero as n goes to infinity. So you would never want to do that. And people have done it many times. This happened famously in climate change attribution. I can talk to you a little bit about the details later. One case that's close to my heart is the case of the so-called lake. Hard is the case of the so-called Lake Chad Basin, which is in North Central Africa. There's an S missing here after farmers. This is supposed to be farmers of Northeast Nigeria. Why am I interested in the livelihoods of farmers of Northeast Nigeria? Well, you can ask me later, but Philip Ernst and I are interested in this problem, and we just were awarded a grant from the UK government in a branch of the UK government, which is similar to the National Endowment for the Humanities in the U.S. Endowment for the humanities in the U.S. So it's not something which is meant for statisticians or scientists of any sort. It's for people who study the humanities and social sciences. So we're very proud that the people of the UK have decided that our connection between Ewell's nonsense correlation here, oh, it's called nonsense because it doesn't concentrate towards zero, it's diffuse, and that's why it's a nonsense to think that having it be non-zero would indicate lack of correlation. So we're pretty proud of this issue. So we're pretty proud of this issue here, but you can ask me later about that. Maybe not today because our time is short. So here's some of the details here. Again, we have your two time series S and T or processes, continuous time here because it's easier at the beginning. Assume they're Gaussian. So the numerators and denominators of the expressions for rho and rho n contain products of normals. Therefore, anybody who knows a bit about we are chaos. Anybody who knows a bit about Wiener Chaos knows that their centered versions are going to be second chaos objects. And you might wonder: well, are we sure? And how do you represent it in a second chaos as a double wiener integral? It's not very hard. Let's work with rho first. Well, first of all, simple stochastic calculus leads to this expression for the numerator of rho with this kernel k, which is completely explicit. We're in continuous time here, so we're working with the Brownian motions w1, w2. So very, very explicit thing. Now, strictly speaking, you might say, you know, this is a product of You might say, you know, this is a product of two different W's, so that's not, strictly speaking, what you would get when you're doing ordinary double Wiener integration. You can't mix your W's if they're independent. Well, that's not a problem. All you do is if your W's are independent, then you can write them as a sum of a difference. You can write the double integration as a sum of a difference, as a difference, a scale difference of W's this way. And so then you really recognize the object as being a member. Really recognize the object as being a member of the second chaos, but it's on the Wiener space on 0, 1 squared. And that's for the numerator. For the denominators, you have these two terms in the denominator. Same sort of scenario. In the case of the denominator, you have to also realize that each one of the two terms in the denominators are inside of square root, so they're not negative. So you got to do something to figure that out in terms of second chaos. Because second chaos object right here has mean zero. And so you see we're subtracting a term here, and that's how. You see, we're subtracting a term here, and that's how you end up with something which is so. This is something which maybe this should be a plus rather than a minus. Something which, yeah, this should be a plus rather than a minus. And so you end up with an expression here, which is almost surely positive. If the correlations are non-zero, which of course is a very real possibility, then you can just use some ordinary linear regression to introduce the The innovation part of W2 connects it to W1 and little R, I'm using that letter to denote the correlation coefficient between the two W's. It's all good. Just makes the calculation more complicated, but it's all good. It could be all represented neatly in the second chaos. Sorry, did I go back to the previous slide or is this the next slide? Sorry. My apologies. All right, so now let's assume we're in discrete time. Let's assume we're in discrete time, and what does that look like? Well, now instead of variant motion, we're going to use Gaussian random walks. Gaussian random walks because we want to try to do this. Is remember working with Philip Ernst here? And we're trying to do calculations by hand as much as we can. So we have to use very specific models. And so instead of using ordinary random walks, I'll use random walks where the innovations are Gaussian, as you can see here. That means that we can use the same Wiener Chaos tools in order to represent the numerator and denominator terms. The numerator and denominator terms in the nonsense correlation. So you remember we had the kernel K, which was on the previous slide or two slides ago. You see down there, the last line, it's a very simple formula. Over here, it's the same kernel, except you discretize it in time. And when you do that, you can use that kernel without any other changes, and you obtain with the same representation that I have on these slides here, everything the same, just change the kernel, and you got yourself the discrete version of your. Yourself, the discrete version of your EU's nonsense correlation. You don't have to change the W's, just need to change the kernel. So that's pretty handy. Now let's see about, I see that there's a note in the chat. Oh, it's just a comment. Let's now talk about the thing which we really wanted to get figured out early, which was computing the actual variance. You see, it took probabilistic and statistician 90 years to. Probabilist and statistician 90 years to figure out how to compute the variance of rho. And then, once Philip Ernst figured out what tools he needed, then he thought to himself, I want to compute the variance of rho n. And so then, you know, we started talking. And then, you know, because the paper is co-authored, you know, I can say that we did it, but I think everybody can understand how the result was actually proved, given the past success that Ernst had. So we're going to. So we're going to remember the kernel K here. We're going to take only the values of the kernel at these discrete points for 1 to n minus 1. So only n minus 1 times n minus 1 values here for k. Call that a matrix Kn. Consider this thing, which is known as the alternative characteristic polynomial for Kn, a function of lambda here. And then ask yourself, you know, I. Then ask yourself, you know, I have to figure out the joint distribution of the numerator. The numerator of the Yule discrete correlation is this thing here, y1, 2n. The denominator is the square root of the product of these other two, 1, 1 and y22. That's a trivaria thing that I have to understand to some extent. Well, so Ernst's idea previously was: let's try to figure out if we can compute the moment generating function for this thing. This is one way to write down the moment generating function for. To write down the moment generating function for that triple of variables, and remember they're all in the second chaos plus constants. So, you know, second chaos is not, I mean, it's a little bit exotic, but it's not horrible. Can it be done? Well, it turns out that the first thing you see is each one of these Y's can be written as this bilinear form, if you want, using the kernel in the middle and two IID variables on either side, which are N01. So that gives you a representation. So that gives you a representation which is similar to the chaos to the kernel representation for objects in the second chaos, but this is just very discrete. It's the same thing, though. And then we use the formula on the next slide here. Uh-oh, what happened? Oh, yeah. So the following formula holds. You see, on this slide, I haven't. So this is the same K as the old K that I was talking about before. The old k that I was talking about before, and it leads to this matrix Kn. But the next formula holds for any matrix Kn, as it turns out. If you want to compute the moment-generating function of that triple of variables defined here on this slide, well, all you need to do is compute these expressions. The S1, S2, and S1, 2 are just the variables of interest in that MGF. The MGF is equal to one over the square root of the product of the two alternative moment-generating functions. Generating functions. And so that's a general fact. It's a general fact about any matrix when you decide to create these variables the way that they're written here in this expression here with the same thing for y11 and y12. It's a completely general thing. Just has to do with the matrix and its alternative characteristic polynomial. And then, even still, at the very general level, still don't have to tell you what Kn is, even though we know what it is. It turns out you take the second. Second mixed derivative with respect to the middle coefficient of your MGF, integrate that against this power function here over the upper right quadrant. There's a universal constant here. And this gives you the nth moment of that ratio. I mean, it's nuts, but it's true. And this was established in a paper that was written in 2019 by Ernst Rogers and Joe. I think they're having difficulties getting it published. I don't know why. Having difficulties getting it published, I don't know why. It's brilliant, brilliant paper. You should look at it. Awesome, awesome paper. And this is one of the things that they start with. Our constant when m equals two is equal to one quarter, so that's not a mystery. And the variance that we're looking for is just the second moment. And so therefore, based on all of this, all we need to do, you know, I've put the word all in quotation marks, is to find dn and hope that the moment calculation based on this expression can actually turn out. Can actually turn out to be tractable to some extent. And so, this is what we do in the paper: we compute dn, and here's the formula for dn. It's a formula, right? It looks pretty crazy, but in order to prove this formula, again, because I'm working with Philip Burns here, we're after, we're trying to prove exact expressions. The proof requires an explicit determined operations on the scaled determinant here, which is a scaled version of the Is a scaled version of the alternative characteristic polymer of the matrix Kn. Now, this is the specific matrix Kn. We can't do this for any matrix, it's just we were able to do it for this one. And then it's, you know, once you get there, the idea is to try to show that this expression satisfies that an affine, an affine transformation of this expression satisfies a second-order recursion relation. And if you can get to that by doing all your line and column operations on your determinant, then you can actually solve. Then you can actually solve that second-order recursion explicitly. And when you put it all back together, it gives you this crazy formula. So, with this extraordinarily complicated determinant calculation out of the way, applying it to the moment formula takes what we like to call here in Redneck talk, we call it elbow grease. It's not super easy, but it's actually easy in comparison to what it took to prove this expression for DN. Well, so with the elbow grease here, so this. So, with the elbow grease here, so this looks worse actually than this formula, but this is actually easier to show than that because this took some real ingenuity and that's just plug and chug. So, we end up with a formula. To me, this is what I would consider a semi-explicit expression, but it's a completely unambiguous. Everything here is completely explicit. It's just that there's a double integral involved. So, you have to then, you know, you can try to use Mathematica to get an answer, but you certainly can approximate this to any level of precision. Can approximate this to any level of precision that you want. And there's a shorthand here, which is that there's this function here, which you recognize from previous formulas that you have to plug in and figure all this stuff out. So it's explicit. There it is. That's the formula for the second moment. The formula for the second moment for the original problem of Ewell's nonsense correlation, which Ernst solved in 2017, was looked a bit like this, but it looked a lot simpler. Looked a bit like this, but it looked a lot simpler. And so this is in discrete time, so everything's worse, but it still can be done. You know, this our paper shows that. And I completely forgot to say that this is a paper which is written with three co-authors. The third co-author is a former PhD student of Philip Ernst Dong Shou Fuang. The next thing that we want to talk about is Is the speed of convergence of rho n to rho in law? So let me just set up the ideas real quick. The first thing we do is we couple rho n and rho n on the same probability space, but that's this construction that I already showed you. Then we use this very simple upper bound on the Wasserstein distance. It's just because we're now working with a coupled set of random variables, the L1 norm is an upper bound on the Wasserstein distance between the LOS, turns out. Very nice. So it's all about just finding the right coupling, which we already knew what. About just finding the right coupling, which we already knew what it was. There's some technical tricks that we need in order to make the whole calculation work. We need badness of the first negative moment of the denominators terms for all n in the discrete time case. In the continuous time expressions, we need negative moments for all them to exist for the numerator part. And we also need mean square error convergence speeds for. Square error convergence speeds for the discretization of the numerator and of the two denominator terms. We need those convergence speeds in order to give us the convergence of the whole entire rho minus rho n in L1. In other words, in Massachusetts distance. To prove the negative moments, we just use a very simplified version of the things that we did before using moment-generating function. Using this very simple form, it turns out, did you guys know? I didn't know this until Philip Ernst told me. I didn't know this until Philip Bernstein told me that you want to compute the negative moments of a random variable. Well, here's a way to do it: you use its, if you wanted, this is kind of like a moment of the moment generating function, but in some sense, you know, integrated against this parameter, like, interesting, that gives us a negative moment. That's nice. It turns out that when we use our matrix Kn, this is very general again, we're going to get Again, we're going to get explicit expression using the dn for these functions phi that we need. And in continuous case, it gives us a hyperbolic sign. And then you need just a little more elbow grease after that. And here's the results that we get. Because we're doing exact calculations, it's not necessary for us to be able to prove these two things. For the numerator term and for the denominator terms, we have an exact. Denominator terms, we have an exact expression, exact expression for how their variances go to zero. Exact expressions. There's no additional terms here, turns out. We just need some classical analysis formula to figure this out. We didn't even need to know the first term. We just needed to know how big these things were, but we have everything here. And then the final result based on all this stuff is that the vast in order to prove this, we did not need something as precise as that, but it does come out of our calculations. Does come out of our calculations. We end up with this speed of convergence for the Wasterstein distance between rho and rho n. The constant is actually explicit, even though it's complicated, but it depends on all the negative moments that we needed to have exist. And then finally here, and I guess I have like maybe five minutes left or something like that. What do we want to do with all this? We would like to explore these computations to understand fluctuations of rho n. Of rho n. As I mentioned, we're doing infill asymptotics. This is the expression that we found on the previous slide. Because the scaling is n rather than square root of n, it calls into question whether a central limit theorem would exist. In fact, we think it does not exist. We already proved using Newell Art-Picati theorem that a CLT is going to fail. And in fact, we have a little bit more, which is that CLT, well, we T, well, we don't even need the theorem because it's in the opposite direction that you would need it here. But we've looked at the fourth moment. It's supposed to be equal to three, but it's bigger than three. And so that's why we are sure that a CLT does not hold. Similar for the other terms here in the denominator. Our strategy is to work on. So from this point on, these are all conjectures. Our strategy is to work on the denominator term first because it's easier and then we can polarize. If things work for the denominator, by polarization, they're going to work for the denominator. For the denominator by polarization, they're going to work for the numerator too because we're using Wiener chaos kernel representation, so that's why we can do polarization. So there's a theorem of Noordan and Poly, which appeared in EJP in 2012, which implies that if a second chaos sequence converges in law, then the limit has to be a convolution of a normal and a second chaos law. That's just a theorem. It's just a fact of life. You cannot have a second chaos sequence that converges in law, and the limit be anything other than a convolution of a normal and a second chaos law. Impossible. Second chaos law. Impossible. In our case, we may need a trivariate version of this, but that's okay. We'll figure it out. We think that the second chaos component is non-zero because it could be zero. You could have a CLT. We don't think there's a CLT. So the second chaos component is non-zero. We also think that the first chaos component, the normal component, is also non-zero, which makes it more interesting. In our case, we think we can couple things in such a way that the following things will hold. We'll have a decomposition of the kernels that we have that show explicitly where each component comes from. This is something which is not. This is something which is not explained in the Nordan Poli work. It just has to be done by hand in every case, case by case. We think that the second chaos component converges not only in law, but also in L2 of omega. This is critical. I'm going to try to explain why in a minute here. This happens if and only if the kernel, the piece of the kernel that we identify is the right one, converges in L2 of 0, 1 squared. So being able to show that that works, as long as we find the right kernel, it's going to be relatively elementary. And these components would have to be the ones in the neurotic. Components would have to be the ones in the neuronal poly decomposition. So, therefore, they would have to be asymptotically independent. And that's a big plus as well. And so, to simplify notation here, let's consider this. This is the function that our nonsense correlations are based on. A, B, and C are my Y1s, Y2s, and et cetera. And so, if U is equal to ABC and UN equal to A and B and C N, then you see that rho is just F of U and Rho N is just F of U N. This is just to write things quicker. Write things quicker so we can keep track of stuff. So, U and UN are trivariate. We conjecture, so what we're conjecturing is that there exists V in the second chaos, such that not only is it a second chaos variable, but it's in the same second chaos as our Brownian paths. In other words, it's going to be some sort of explicit double Wiener integral. And when you take u n minus u multiplied by n, which we know has a constant variance asymptotically, subtract this term, which we identified. Subtract this term, which we identified as the double Wiener integral from the Neordampole decomposition, you end up with something which converges in law to a normal with a specific sigma here, covariance matrix of this three by three situation. So we believe that this conjecture will work. Because we think we can identify this V in the same space as a rounding paths, it means that it's actually measurable. And if you use with respect to the path that we have, and if you use a delta method conditional on U, which we have, and V, which we think we can identify and observe. V, which we think we can identify and observe, then n times rho n minus rho will be asymptotically normal. This will be its mean. This is just coming from ordinary delta method. And this will be its variance right here. This is the sigma that was on the previous slide. And therefore, and so here's the punchline. One could reject the null hypothesis that W1 and W2 are independent if n times rho minus rho n deviates enough, not from zero, but from its mean value, which is this observed expression. mean value, which is this observed expression here. It's just that we have to be sure what v it is. So the whole thing is to find v. We could work directly with the trivariate n u n minus u minus v without using the delta method. That's fine too. Or even just with the first component, that's fine too. But correlation coefficients we think are going to be more robust when we're doing statistical applications. So we want to just work with n rho n minus rho. That's just our hunch based on our experience with these things. This is the last slide. There's also a couple other things which are interesting. There's also a couple other things which are interesting to statisticians. Asymptotic power of this test of independence that I just talked about on the previous slide. Under the alternative hypothesis, we think we can prove something like this, which is a specific rate of asymptotic power. And asymptotic, I'm sorry, this is a typo. This should be asymptotic exactness, not normality, asymptotic exactness of the test. So, because we're out of time, I won't say more about that, but it's Out of time, I won't say more about that, but it's also something we think we can obtain. Um, uh, but it's not this is less clear to us what exactly the speed will be. But if you're doing, if you're working a standard setup, the speed is always like one over square root of n and maybe log n over square root of n. But since here our normal convergence comes about in such a non-standard way, it's very exotic because we have to go through the second chaos and the Neur Dampile decomposition to figure this whole thing out. This may not, the speed may not be what we think it is. The speed may not be what we think it is. But for this one up here, we think it's going to be this particular speed based on our calculations so far. And thank you all very much for your attention. I really appreciate it. And that's it. That's my last slide. Thank you for it. Okay, some questions. And sorry for giving you such a complicated talk at the end of the day. That was nice. So it looks like you were interested in applying this to real-world data and see that. Oh, yeah, sure. So I mean, the things I had here. So this thing about climate change attribution. When let's see. So it turns out that the It turns out that the global average temperature on planet Earth is not a stationary time series, and it may behave more like a random walk than it does like a stationary time series, even with very long memory. And if that's the case, it would explain why people who are trying to show that climate change, and especially the rising global average temperatures, are attributed to Are attributed to greenhouse gas emissions and their concentration in the atmosphere. That would explain why when you try to do those correlations going back in time, you know, for a very, you know, you can do those correlations going back in time for millions of years because we have proxy information and we can try to understand the correlations that way, but it's the wrong tool to use if there's any chance at all that global average temperatures behave more like random walks than they do like stationary. More like random walks than they do, like stationary time series, than our work shows. And if it was, of course, known a long time ago. But here, what we would be able to do with these tests is we would have a different way of trying to attribute global average temperatures to external variables that are like forcings, like solar activity or greenhouse gas emissions. So, this is stuff which there are other methods to do this, but it'd be very interesting to actually apply this test, which I'm talking about here. Which I'm talking about here is simply this test rejecting the null. If n rho minus rho n deviates far enough from this mean that we'd have to figure out, that would be an application which works directly with the time series rather than trying to understand their innovations. And one of the reasons why this is interesting is because when you have data, which is real data, and you say, well, your real data is a random walk, so just take its increments, and then you'll find some IID data. And then you'll find some IID data. That's not how the real world works. If you have a time series that looks like a random walk, it may look like a random walk from far away, and that may be good enough for this test to work. When you start taking it apart and calculating first order finite differences, you're going to get things which are very messy, and you won't be able to apply traditional tools on traditional short-memory stationary time series because your data. Because your data won't look that good, and you'll have to go through all sorts of contortions in order to apply those regular tools. So, we think that working with paths, if they happen to be random walks, is a much more robust thing to do than to work with the random walks increments in practice, because it's not robust to miss specifications of how those increments actually behave. I don't know if that was helpful, Sammy. Yeah, it was. It was. Thank you. Are there more questions? Questions? All right, if not, thanks, Perry, and all the speakers of the long bit.