Okay. So our next speaker is going to be Elna Lemra from the University of Bristol, who's going to be telling us about classifying boundary fluctuations where uniformly random Gelfand sets are going to distribution. Thanks, Arjun. Thanks, Argent. And thanks, Timo, for introducing me to this beautiful subject of random growth models and KPZ universality, and for your guidance and friendship over the years. Many thanks. So I'd like to take this opportunity to Take this opportunity to talk about the forthcoming joint work with Courtio-Anson. Most of this research was carried out when I was at KTH. For a few months in spring, I also worked on this project while visiting Bashkent University in Ankara, Turkey. I would like to thank Nizami Gosilov and the computer engineering department there for their hospitality. Department there for their hospitality. So let me begin with discussing the model we study in this project. This will be uniformly random Gautam-Settler patterns with fixed but arbitrary initial level. First of all, what's the Gautman-Settlin pattern, or G D pattern for short? G D pattern for short. This picture illustrates a G P pattern G of depth n. So this is a triangular array of real entries. It has n levels, which I label with integers starting from 0, going from top to bottom. And I denote entry i on level L by gli. So there are entries distributed over levels. Distributed over levels such that they're exactly n minus L entries on level L. And the entries satisfy an interlacing constraint, which is indicated here. So entry I on level L must be between entries I and I plus 1 on the previous level L minus 1. So you have already seen the discrete version of this notion in Nico Stigora's toll, but it's the continuum version. Version. This discrete version is connected to the RSK algorithm and this naturally arises from Hermitian matrices if you look at the eigenvalues of the leading principal sub matrices. Alright, so I'll be actually studying random GT patterns, construction. Random GT patterns constructed as follows. So I'll take an increasing sequence A of some finite length n and I'll place this increasing sequence on level 0. So that will be the level 0. The remaining entries will be uniformly, will be randomly chosen so that the resulting GT pattern is uniformly distributed over all GT patterns having the A. All GT patterns having the A sequence on level 0. Again, so there's a natural reason why one would want to look at uniform measure. So I will not write this down, but this is related to a result of Borishnikov from 2001. So if you look at the unitarily invariant random permission matrix and look at its eigenvalue minor process, conditioned on the eigenvalues of the matrix, the eigenvalue minor. Matrix, the eigenvalue minor process would form a uniformly random GT pattern having the A sequence on level A. So there's some motivation from the random matrix side. But we do not really make use of this connection in this project. So what we are interested in, one can pose many questions, but what we are interested in Many questions, what we are interested in is multi-level limit fluctuations of first particles. So I'll just take some D many levels, K1 through Kd, and mark some points on these levels, so T1 through Td. And I'm interested in the probability that the first particles on these levels, they stay to the right of the corresponding point. That's a gap probability. Point. So this is a gap probability. And this probability I denote it with f, so this is a CDF function that captures the joint distribution of first particles on those levels. We will be interested in the scaling limits of this function. And our approach is based on the fact that this process is a determinantal point process. This structure was worked out by Tony Metkelf around 2000. Tony Metkelf around in 2013. And what that means is that the the C D F function can be expressed as a freton determinant, which is this expansion here, with some explicit correlation kernel, okay. So this correlation kernel is some four variable function, and k and l here are level variables, and x and r are positioned variables. Variables and x and i are position variables. So this k can be written as a sum of two components, which I write indicate by phi and i here. And phi part has this closed form expression. And I refer to it as the heat part because it scales to the heat kernel in the asymptotics. And the other part is a double counter integral given by this. Given by this formula here. And this is the part where you see the dependence on level zero parameters come in through these products within the integral. And there are some requirements on the contours. So this Z quantor gamma plus, it must enclose exactly those AIs that are to the left of X. of x and the W counter gamma minus, it must enclose gamma plus and the variable y. And if you look at the structure of this integral, it's actually not so far from the structure of the kernel for inhomogeneous exponential LPP, and there are many analogies with that model in this world. Okay, so now let me begin to Okay, so now let me begin to describe our contribution in this work. I'll initially start a bit informally, but progressively get more precise. So our contribution is that we identify five regimes of multi-level fluctuations for first particles. Let me say right away that this is not exhaustive, so there's at least one regime that we did. We did not investigate. So, in this list, I'm sorry, in this list, the first one, the bounded level regime, concerns the situation when the set of levels you consider, they stay bounded in the asymptotics. While the remaining four can be considered as large level regimes where the levels themselves are scaled to infinity in the asymptotics. Cells are scaled to infinity in the SM5s. So in this list of two, number two and number four, they recover the baked Benaroux-PÃ©chet transition for this model in a special case. And this area regime is where one sees the area process. And this third one, what I call the Weierstrass regime, seems to be completely normal. Okay. Now here I wanted to describe informally how these regimes differ from one another. So I tried to simplify the result and in the process I might have lost some accuracy. So you should take this table not so seriously. So especially the second row here, I think would be open to interpretation. So it turns out that these regimes will differ from each other in terms of scaling and in terms of whether. So, what we are looking at is that we will be taking a sequence of uniform G D patterns with some given level zero sequence AN and of some arbitrary. Of some arbitrary length ln. So ln indicates the depth of the GT power. And so the regimes will differ from each other depending on whether the individual terms in the level zero data will contribute to the limit process or not. Contribute individually or in some aggregate, in some aggregate fashion. In some aggregate fashion through some macroscopic quantities. So, those that make an individual contribution, like you can see this individual parameter in the limit, we can think of them as outliers, outliers like spikes of the system. And one feature of our result is that our setting is that we allow infinitely many outliers. And it's not clear a priori which of the terms I. A priori, which of the terms act as an outlier and which do not. So it's part of our result to sort this out. Alright. So now I'll begin to describe what these regimes look like. So each of these regimes corresponds to a set of two or three conditions. So to state them, I need a bit of notation. So I'll, for some finite significance, For some finite sequence A, I can define it as negative Cauchy transform as this sum here. And it will turn out that all of the regimes can be described entirely in terms of this Cauchy transform applied to the level zero data. So next I will introduce this level function. Level function in terms of the Cauchy transform. And the purpose of this function, I mean, this has some significance in the analysis of the model, but for me, at the moment, all I need is that it's a decreasing bijection from the interval from minus infinity to a1, the first parameter a1, onto interval from 1 to n. So I will use this bijection to just track where I am on the I am on the boundary of the GT pattern. So I need something like this because, under our assumption, it will not necessarily be true that there will be a limit shape. So I can't really describe where I am on the boundary relative to a limit shape. So I need to do this in a more convoluted way. I also need something that I call the curvature function. So this is again, you know. Function. So this is again, you know, you see defined in terms of the Cauchy transform. This curve, what it does is that it sort of computes how curved the first particles look like. Of course, strictly speaking, this doesn't quite make sense when we are talking about a discrete set, but you can imagine that if we have a limit shape, then this function would converge to a limiting function, we would actually measure the curvature of the boundary. Now what I will do, now I have the sequence of um GT patterns, uniform GT patterns Gn with given level zero data A n, and I will pick a sequence of I'll pick a sequence of levels on this GP pattern. And what I will do is I'll just map them on this interval. So this is a a n1 now, on this interval from minus infinity to a n1, Minus infinity to a n1, so it will be mapped to some point zeta that I call zeta. I refer to this as a critical point because in the saddle point analysis, this just is indeed a second-order critical point, but that will not be important for this talk. So as this Pn moves on the boundary, it just means that the Zeta moves on this interval. And it turns out that it's more natural to state the regimes in terms of Zeta. Now as I told you there are five regimes that we study and there's one more that we do not So it would be boring to go through conditions for all of them one by one so instead so I will actually skip over most of them and I will just pause on one and try to explain what 101 and try to explain what the conditions describe. So, in each case, what we'll need to do is we will need to rescale the parameters in an appropriate way. And this rescaling, for example, in the bounded-level regime, will be done with this Cauchy transform A n computed at the epitome point zeta. On the Gaussian regime, On the Gaussian regime, we rescale by the square root of the first derivative of the Cauchy transform. And the remaining three regimes, the scaling is done with cube root of the second derivative of the Cauchy transforming value at that critical point. So you see, so when we look at these rescaled parameters, they lie between 1 and infinity. So our assumption. And infinity, so our assumption is that they converge to some limit parameters beyond. So this is necessarily an increasing sequence, but we allow that some terms are infinite, but we assume the first one, the first one, is finite. So this assumption gives me some information about how far this critical point is to the first parameter of the level zero data. And in some convoluted And in some convoluted way, it also tells me how far this PN is from the top level. So this PN is the plate that you're interested in, or where does that come from? Pn is a sequence of levels that I consider. So I'm interested in... And that's why you want to study the... Exactly. I'm interested in local fluctuations around PF. Okay. And then this first condition here, here I'm doing the rescaling with respect to with the factor that I use for the Gaussian case. And I assume that the limit is infinite. So in the Gaussian case, this would be a finite constant. That would be the part of the assumption. And then I have also a third condition which separates this regime from the BBP regime. So here, I look at this. So here, I look at this ratio of the curvature function divided by this first term in this expression. And this quantity, I assume that this limit exists, and it will always be between the sum of 1 over vi cubes and 1. And this is an extreme case where this limit is equal to this sum. So what this captures is that, so these BI's, you could think of them as the counter. them as the contributions of the individual parameters to the limit process. And what this condition is testing is that whether the overall contribution of the level zero sequence comes as the sum of individual contributions of the terms of the level zero sequence, or if there is some extra contribution that's not captured by this. And we have a similar set of conditions for very Conditions for various cases. Let me say a few words about maybe this error regime. Here, we rescale the parameters using the same factor, but now we assume that this rescale parameter converges to infinity instead of a finite constant. And then in this assumption, we again look at the ratio of the curvature function to the first term here. And this is between 0 and 1. And we assume that this. 0 and 1, and we assume that this converges to some constant kappa 0, which will necessarily be between 0 and 1, but we also impose a technical condition that this kappa 0 is strictly positive. So this technical condition enables us to proceed with our analysis. So it can possibly be weakened. So this error regime possibly is wider than what is indicated here. Here. The reason why we put this is because it was just more difficult to work with scaling with this function instead of just the first term index function. Okay. And the last regime that I call degenerate regime, so that which we do not study, that comes from just complementing the technical condition in the error regime. Now breaking the fourth wall for a minute, I can speak to the online viewers. Now if you were in my talk in Anger, France, last week, you might remember that I didn't have this text crossed out. So at the time I shared my belief that we do not have something interesting in this case. But since then I spoke with Alexei Bufetov and he explained to me that there are indeed some processes. Some processes that would appear in this case. If one has a limit shape, this regime sort of corresponds to the situation when we are looking at the very bottom of the limit shape and one can see GE kernel under some scaling there. So that we do not cover in this result. Okay, so I have about ten minutes. So let me now describe what the limit processes look like. So I'll just describe the kernel. So we have explicit expressions for the kernels in each case. So I'll just look at the case of Weierstrass region. But we have similar expression for each case. Okay. So in the wire stress regime, so we had this so maybe I also should make it clear that if you take any uniform sequence of GPAT and GM and any sequence of levels, then there will be some sub-sequence along which one of these six sets of hypotheses holds. So for the first five, we show convergence and We show convergence and describe the limit process, and the sixth one we do not split. Okay, so the limiting kernel will have a structure that mimics the structure of the Metgaus kernel. So it will have some heat part and an integral part. And we had these limit parameters B. The heat part will just be a heat kernel, one-dimensional heat kernel, computed at some point. It's not so important what these series are for this talk. So there are some expressions that in terms of the limit parameters and the level variables u and v. So here, u and v are limiting level variables, and s and t are limiting position variables. S and T are limited position variables. Okay, so in the integral part, I would like to point out something interesting. So we have this double counter integral over the broken line segment. And here, within the integrand, we have these exponentials. And the exponent here has this expression, where the leading order term is what I call this W2BC. Is what I call this W2B. So this is a Weiersch sum of degree 2. So in the Gaussian regime, so this would be a quadratic expression. So in the BBV and AR regimes, this would be a cubic expression. But here, what we have is we have something, an intermediate function between just. And of order 2 is nothing. Weier sum is nothing but a logarithm of a wei-schless product. So if you exponentially list, you get a wei-schless product. And if we have that the sum of 1 over bi squareds converge, then we can separate this quadratic terms, and then the limiting kernel would just be the same as in the Gaussian case, but if the Gaussian case, but if this is not summable, then we cannot do the separation, and this limit process is genuinely unique to this region. And the closest thing in the literature that I'm aware of is a kernel that appeared in the context of exponential LPP with growing and homogeneous parameters, which was studied by Johan, so in 2007. That does not form in Um that does not formally fit in our framework, but they're in a similar sphere. Okay. So in order to state the convergence result, so I need a little bit more notation. So I need to estimate the position of the first particle The first particle at a given level p. So, for this, we have this deterministic estimate given by this variational expression. And this level function that I used before, its significance is that this critical point zeta is an extremizer of that variational point. So, our setup is this. So, we take a sequence of This, so we take a sequence of uniform GT patterns and a sequence of levels Pn, and then I fix a dimension D, fix and choose some level variables UI from a compact set. And I assume that there is some gap between consecutive levels. So this is just to make sure that the levels are separated from each other in the asymptotics. And I pick some position variables also from some compact sets. Also from some compact set. So I'll now be looking at the CDF function, which is what I looked at before, but now this is just this rescaled version of that. So I'll take the level variables that I picked and I plug them in into this level function, a rescaled version of the level function. So I get some certain levels. Then I also put some marks. These marks are computed from this boundary. From this boundary function, a rescaled version of the boundary function, then I just look at the probability that the first particles on those levels stay to the right of those miles. So this is just a rescaled version of the CDF. And in each regime, we'll have a result that says that that rescale CDF converges uniformly to some limiting CDF, which has the straight-on determined expression with the corresponding kernel. So for example, So, for example, in the YSH regime, the precise statement would look like this. So, we have these conditions characterizing the Y-School regime. And I have, in the way I set things up, so the first parameter is necessarily bigger than this largest possible value of the level variables, but I just have an extra epsilon room, and that's just to guarantee this uniform convergence in this statement. So we have we have analogous statements uh for each region. Okay, so now we have this classification theorem of sorts and now what would be interesting would just find some interesting applications and one application one immediate application would be to look at a situation where you have a limit shape and just describe what And just describe what processes you see along these limit shape. And in order to have a limit shape, we need to assume a bit more. So we need to, for example, these are essentially minimal conditions to have a limit shape for the boundary. So we just fix a n1 at some point a0 and assume that the empirical measure associated with the level 0 days. With the uh level zero data empirical measure of the level zero data, that converges in the wake topology to some to some limiting measurement. And we assume this measure is non-trivial in the sense that it is not zero and it has at least two points in its support. So then we have a shape function boundary. A shape function as a boundary, and it has this variational formula, which is just the limiting version of the boundary function I use to estimate the boundary in the theorems. And here it is A hat, so I use hat decoration to indicate limit variables. So A hat is just the Cauchy transform of the measurement. Okay, so now we will have a weak shape theorem as a consequence of our result. So if we just take a sequence of levels and divide by the depth of the And divide by the depth of the Gaufland settling pattern, and assume that that converges to some ratio R, the shape theorem is that the rescaled first particle level Pn will converge to this shape function. So this shape function is convex in general, being the supremum of linear functions, but it can have flat spots initially. So it takes some notation. Uh it will it takes some notation to explain where this is, but one can compute this from A0 and the limiting measure. And we have this flat region and a strictly convex region. So in the strictly convex region, we can get this area universality result from specializing up the classification theorem we had. So we would just have uniform convergence of the area classes. And finally, I'd like to look at the model quickly where we see this Weier Schless regime and we have a flat limit shape. So I'll just take the parameters of this form, so i minus 1 divided by the depth raised to the power 1 over q plus 1, where I take this q to be between 1 and 2. And then a0 will just be 0, and the limit when I Be zero, and the limit measure will be will have a density of this point, the power density. And then the shape function will have a flat segment because I chose q less than 2. So we already know that on the curved piece we see a ray process. And on the flat piece, we will see a Gaussian regime, and a very special case of that, so uh one where V two parameter is just infinity. Parameter is just infinity, and B1 can be computed in terms of R. So, in this case, this is just Browning motion. And we have a purely technical condition. This also is in the classification theorem that can be removed more work. And finally, if you take R to be exactly equal to R0, R to be exactly equal to R0, this is the critical point, then we need to actually blow up, look at a zoom in around this critical ratio. And it turns out for this model, so the magnitude of neighborhood we need to look at is this depth raised to the power 2 divided by 1 over Q. And then I need to describe where this chosen. Describe where this given sequence is relative to some fixed sequence in this critical neighborhood. And as this fixed sequence, I just pick something that I know is there. And then we'll have this convergence to this wirestress, we'll have this uniform convergence to the process in the wire stress case, and we can compute what the limit parameters are. So they are given by these expressions, but the important thing is that they are all non-trivial. They're all finite. They're all finite. And then we have this y variable here that can be computed implicitly in terms of this x variable. So it's given by this formula here. And one final remark. If you sum 1 over bi squared, if you sum 1 over bi squared. Sum 1 over bi squares. So this bi squares are basically like 1 over j to the power 1 over 1 plus q. So if you sum them, you get infinity because I picked q bigger than 1. Okay, so then this limit process is really specific to this Reichstag regime. So it's not something we would observe in the Gaussian regime. It's not something we would observe in the Gaussian rich. And it is the novel, it's the most interesting case because we have these, all of the parameters are fine. All right, so with that, I'd like to thank you for your attention. Questions? How do you get the conditions that what kind of regime do they uh do they uh come from the set of particles? How? Yeah, so I mean it took us a while to come up with the final set of conditions. We were aware of earlier works, for example, Bay, Fichet, and Beneroux work. We knew, for example, that if we had finite For example, that if we had finitely many outliers, what we would expect, we knew that we would need to see that. And I was also aware of, you know, Kut, I mentioned that work. We knew that we needed to also see something like in the West trust regime. So we needed a set of conditions that would capture them all. And this involved a bit of reverse engineering, trial and error, and it was a laborious process. It was a dangerous process, but yeah, in the end. So, I mean, I can't claim that. There was the intuition behind this equation. Are they related somehow to the saddle point equations or how? Yes, exactly. Yes, exactly. So they come from saddle point analysis. Ruth? So having done this now, do you have So having done this now, do you have a guess for what should happen in the critical direction in the exponential model? When you have one? Yes, I would expect that we would have a very similar situation. A virus transfer chip? Yes, so if we allow infinitely many inhomogeneities, we would I would guess that we would see it there. But I haven't done the test yet. So we'll meet in five minutes for twenty points.