Yeah, thanks a lot, René, for the very kind introduction and invitation. I'm really sad I cannot be with you guys. Yeah, and nonetheless, that I'm allowed to speak, even though I really don't, I wasn't able to come. I'm really glad about this opportunity. Opportunity. Yeah, I'm going to just start. Yeah, now I see a few participants. Yes, I'm going to talk a little bit about work that has been done in my lab on Max LP martin classifiers in high dimensions. And we'll see how to motivate this first. And then we'll see two results that might be. Results that might be somewhat surprising given what we know from classical high-dimensional statistics. Okay, so this work is joined with my student, Konstantin, and some master student, Stefan, and who is now also a PhD student, and Nicolo, who was a PhD student in my lab. So the motivation comes from basically the phenomenon of double descent on. The phenomenon of double descent on neural networks. I'm just going to go through this very quickly since I'm guessing most of you have seen this before. But to go through this quickly, we have the on the x-axis, we will see the width parameter of the RESTNet. On the y-axis, we'll see the test error of a neural network that is trained on C410 with 15% additional label noise, just to make sure that there is label noise. Sure, that there is label noise. Otherwise, it could be that there's just no noise, and then there's no problem of overfitting anyway. Okay, so for small resNets, for small width parameters, we are in the classical regime where we see a U-curve, right? So the error, test error goes down as we increase the model complexity and goes up again because the variance goes up as we increase the model complexity. So this solid line here. So the solid line here is the training error and the dotted line is the sorry, I don't want to say. The solid line is the test error and the dotted line is the training error. And so add with parameter a bit larger than 10, we are able to fit noise. So we start to fit noise. So the training error is below 10%. And that's roughly when we say we are in the overparameterized regime where we can essentially interpolate. Interpolate the data or at least interpolate parts of the noise. Okay, so this picture should not be any surprise. And the second part is a bit more surprising, right? So the question would be, is this U curve going up further or not? And we see that actually the test error starts to decrease again if we increase the width parameter even further. Okay, so this double ascent curve, some of you might have already. Some of you might have already seen before, and there's multiple things that one could find interesting. I'll just go through two first that we're not going to discuss in too much detail in this talk. The first one is the fact that it descends, right, that it doesn't shoot up further, this curve, but it actually descends after this interpolation threshold, is a bit astonishing. Now people basically understand where this comes from. You can think of just in one line TLDR, it's that the larger the model, the more you can also. The larger the model, the more you can also absorb noise in many directions that are not hurting your signal. Okay, so this is somewhat well understood, second descent by now. The second point is that I will not dwell that much on is the point of harmless interpolation. So basically, if you look at this point over here, first of all, what you see are many lines. All the lines correspond to, no, each line corresponds to. To each line corresponds to the test error of a neural network of different widths, but cut off in terms of training at a particular epoch. So the yellow line here is cut off after one epoch, and the more blue it gets, the later the solution is. So basically, at a later epoch during training time, we look at the During training time, we look at the solution and the tester of that. And so the red one here is the lower envelope of all of these lines. And you can think of that as the best stopping time for each with parameter that you pick. Then you get the test error of that neural network that is stopped at that time. And so here we can see that if you train until convergence, which is this blue line, this very dark blue line we saw before, that is a U-curve and that descends. That is a U-curve, and then descends again. We see that there is a huge difference for around the interpolation threshold between the best early stop solution and the solution after training until convergence. Whereas if you look at much larger models beyond interpolation threshold, then basically whether or not you stop or you just let a train until forever or until convergence or directional convergence. Convergence or directional convergence, then you actually see that the test errors match. So basically, it doesn't help to early stop. So this is another interesting phenomenon that I discussed in many other talks before. This is also not what we're going to dwell on too much in terms of story in this talk. So the third one is: well, you want to understand, okay, so five. Okay, so fine, there's harmless interpolation, but when does the model actually have good test accuracy? A large model, large overpriced model that interpolates the data, when does it have good test accuracy? So this is what we're going to focus on today. And I should say, please stop me at any time, especially it's difficult not being able to see. Difficult not being able to see the audience. Yeah. Okay, so what we're going to do today is we look at instead of neural network interpolating models for which the overparameterization is in terms of the width, for example, of hidden layers. And we're going to look at linear models. Okay, of course, the reason, one of the reasons is that. One of the reasons is they're easier to analyze and because they're just minimizers of convex losses or maximizers or minimizers of convex losses. And here the over-parameterization would be the dimensionality of the linear model. And that is D, and that's much larger than the number of samples. This is the over-parameterized regime or high-dimensional regime that we will focus on. Regime that we will focus on. The other reason why we look at linear interpolators is that a lot of the double descents that we saw for neural network interpolators could also be observed for linear interpolators. And hence, understanding those could at least perhaps give us some insights for what could happen in the neural network. Of course, this might not actually transfer, but if we don't understand the neural interpolators yet, then there doesn't seem to be that many. Then there doesn't seem to be that much hope to go to the more complex models. So, this is a reasoning why we'll focus on the new models. Okay, I just also want to note that the goal is not to find better interpolators in practice or any sort of methodological reasoning behind, like, oh, you should use this kind of interpolator in practice, but more like we try to understand when in which situations this. Rich situations, this interpolation can be benign. We'll also look at a second question when we don't have noise, and this was hopefully will be new for those of you who have seen me talk before. Okay, so what is the setting? To look at sparse linear classification, and we'll study max LP margin linear classifiers. Then we'll ask these two questions. For one, if we have noisy observations, One, if we have noisy observations, what are the generalization or prediction error rates? In particular, we would like to have tight rate, we would like to know tight rates, like upper and lower bound. And the second question is, if the observations are noiseless, is the max L1 margin linear classifier adaptive to sparsity? So I will explain what that means when we get to that. But this is basically the plan for today. Okay, so what is Bassini classification and what is the setting? We assume that we have a ground truth W. That is the unit vector that determines our decision boundary. We assume that it is hard sparse. So it has L0 norm equal to S, and that's smaller than N. And our measurements that we get from W star. That we get from W star are basically xw star, where x is a Gaussian matrix, and then we take a sine of that and additional label noise. So x here consists of xi of rows, xi's, that are standard Gaussian, okay, of dimensionality d. They're all independent. They're all independent. So, Gaussianity is quite important here. And even though for linear regression, we were at least now able to go to sub-Gaussians, these results for classification are still only valid for Gaussians. And that's a big limitation I would like to note before we go on. Okay, so I already alluded to it. What are our observations, our actual measurements? Why? So you can think of that a bit like one-bit compressed sensing. One-bit compressed sensing if you're coming from that community more. So essentially, we measure W star via this XW star, not in terms of the actual values, but after doing a one-bit compression. So this one-bit compression is in terms of this sign. So either we observe minus one or plus one instead of the actual real value. And instead of actually observing the Instead of actually observing the exact sign, we observe the sine times some label noise, ψi. And this ψi could be basically flipping the plus one or minus 1. The true sign would be flipping it in your observation. You can think of that basically both in the compressed sensing way, one bit compressed sensing, or classification. Right, so and how does the label noise look like? What are some of our models that Are some of our models that we can incorporate in our results? So it could be completely random label flips that don't depend on x, or for example, a logistic regression model where the further you are from the true decision boundary, the larger, the smaller your probability of label noise. So the noise can also depend on xi through the projection onto W star. Projection onto W star. What is our performance measure? So, for regression, we usually look at the prediction error, classification, we look at the classification error on the right-hand side here. So whether the sign of our estimate, estimated classifier w hat, x, whether this sign is Whether this sign is equal to the sine of the true w star, x. So we want to know whether basically directionally it is aligned. And if you for small, relatively small errors, that is similar to the L2 norm, the difference of the normalized w hat and w star. Right, so for classification, really. Right, so for classification, really, it only matters where your sorry, here we assume w star is also of norm one. Right, so all that we care about is how close are the vectors in terms of their angle. We don't really care about the lengths in order to have the same classification accuracy. All that matters is the decision boundary. So that's why the performance measure here looks at the difference of the normalized solution. The normalized solutions. So, this normalized solution minus the W star that we assume is already norm one. Question? Yeah. Am I correct to understand that if all the entries of Casi are positive one, then there is essentially no label noise. But otherwise, I see. Okay. Thanks. Yeah. Yeah, exactly. So psi i equal to minus one would mean that we have noise, right? So here, this is why I have. So here, this is why I have this psi equal to minus one as the probability of noise, and psi i equal to plus one if it's deterministically plus one, then there is zero enable noise. We look at both, and the first part we look at when psi can be minus one, and the second part we look at when psi is always one. Okay, so I hope the setting is clear. In classification, there are two different kinds of major models. One is if you have something like a mixture model. is if you have something like a mixture model where you have separation and this one here which is the discriminative model where you have actually a blob of X's a blob of inputs and there's a lot of points that are close to the decision boundary here right so that's if you look at different kinds of classification models you get very different results and yeah so this one is is very crucial here this is our model we have a lot of points near the We have a lot of points near the decision boundary. Okay. And that's also closer to the regression model, and that's why our results are also very similar to the regression case. Okay, so we wanted to get a sense for how this measure here, this error here, behaves if we look at interpolators w hat. interpolators W hat, max LP margin interpolators. So before we go to classification, let's just get some intuition from the regression case. What we could from there, we would like to kind of transfer our intuition to classification and see if we can learn anything. So if we go to sparse regression, which is much better understood, we were quite surprised that for classification Quite surprised that for classification, there's relatively little, even for the noiseless case. A very basic question is not even answered there, which is like totally well known for regression. So for regression, we have instead of x w star, the sine times noise, we have actually a real-valued observation, xw star plus some noise. Noise could be Gaussian or sub-Gaussian, whatever. And in the noises case, we look at the At the minimal L1 norm interpolator, to well known to be optimal in terms of sample complexity to achieve perfect recovery. This is called basis pursuit. Here we can totally fit the data perfectly. There's no noise. There's no reason to sacrifice data fit. In the noisy case, we cannot just eat, well, we could, but we do not want to fit the data. To fit the data perfectly because there's noise, so there's we should allow some slack, and so this is why now it becomes part of the loss. You have the square square norm of y minus xw plus some penalty term for the L1 norm. And so this last cell basically achieves minimax optimal rates. And all of this is well known. And so the question is for classification. question is for classification can we also utilize the l1 norm if we know that the ground truth is sparse can we try to encourage a small l1 norm of our final solution and then also get closer to the ground truth than if we just search for example um for the minimum l2 norm or or something else so can we utilize this the structural um additional sports Structural additional structural knowledge. Okay, so here again, classification setting, back to that. We have this model for our observations. Now, there has been works that study this non-convex problem. So we want to minimize sparsity such that sparsity, sorry, that we want to minimize the L0 norm, so the number of non-zeros, such that all of the points are perfectly. The points are perfectly classified. So, this is a noiseless case, that's what you would do, right? So, you want to find a w such that the sine of w, xi is the same as this yi for all points i from one to n. And then you need to add this constraint just so that you don't get into the regime where w goes towards zero. So, you want to have some unique solution, so you just add this constraint. So, you just add this constraint. Here again, in Noiseless case, we would like data fit, a perfect data fit. There's no reason not to. And we can get a minimax rate of s log d over n. So that's a very fast rate for this directional error, essentially. This is well known for noises. How about noisy? If you start thinking about actually, it's when we were starting. Thinking about actually, when we were starting to look at this, really we're quite surprised how little work is done on this. For 90 case, what is known is this particular kind of, there's multiple versions that are kind of similar problems as this one, optimization problems, where you maximize the average margin subject to L1 and L2 constraints. L2 constraints. Okay, so if you do this, then you get a rate of order square root S of D O brand. Of course, it's slower than for the noiseless case. But the other thing that changed here is also that you changed, instead of maximizing, for example, the min margin that enforces perfect data fit on all points, here you're maximizing the average margin. So, if you think about it, what this allows you is that if you have an error, you can compensate that by being very, very good on other points. So, this is a quite a different objective function than a max margin solution that is enforcing perfect data fit. So, I also actually discussed it in my introduction to machine learning lecture now. Lecture now. I find this quite interesting that you can, yeah. There are reasons, of course, why we do not want to use this in the noiseless case in particular, because of this compensation thing. But in the case in the noisy case, it could actually be helpful. Okay. So, okay, so this talk will be. So, okay, so this talk will be about the, oh, I don't have this yet, but MaxLP margin classifier. And so what we're interested in after looking into this kind of known results is here we have the problem of, first of all, this is a non-convex interpolator. It is an interpolator, but it's non-convex. Here we have something that sacrifices data fit. So what we actually want to know. This data fit. So, what we actually want to know, if we go back to our motivation, we want to know if we have, in the first part, we want to know if we have noisy data, is there an interpolator or which kind of interpolator of the data, meaning perfect data fit, can generalize well? Is there one interpolator that can generalize well? And we would like that to be a minimizer of a We would like that to be a minimizer of a convex problem, okay. Um, because that's uh, that's actually achievable, but also in our case, the one particular ones that we look at, they're solutions of first-order methods on logistic loss. Okay, so the first question is, how about if we try to fit data perfectly when we have noisy data? And the second question is, how about the performance of these minimizers? Performance of these minimizers or these solutions are noiseless data. Okay, so what are the solutions that we're looking at? So, we're looking at the max LP margin classifier defined as this. So, when we first look at this optimization problem, so here we have again the min margin, so the minimum of all the margins yi times w transpose xi. W transpose XI. We want this to be larger than one. So if you remember hard margin SVM, that's exactly the same constraint. And instead of minimizing the L2 norm, we minimize the Lp norm where P is in between 1 and 2. So what I motivated previously was probably P equal to 1 could be a reasonable thing to try if you have sparse round truth, but we'll see later that this might not be. See later that this might not be optimal. And one reason to look at those, besides just being extensions of the hard L2 margin SVM, is that these are also the solutions, or these are also the final results of running steepest descent on logistic loss with certain step sizes. Step sizes. Okay, so these are natural solutions or natural interpolators to look at for classification. Okay. Another way to look at this, other than from the optimization perspective, is you can, if you consider this known result, we know that for the sparse problem, like the hard sparse problem, where we actually have a non- Problem where we actually have a non-convex problem where we have L0 norm here, and we know that this should work well, right? So we have results that have error of the order S dot D over N. The question is, if we relax this, the L0 norm, to something convex, the L1 norm, how does that behave? So that's another motivation to look at Lp from P from one to two. P from one to two as a convex relaxation of this non-convex problem for which we know results already. Okay, so now the first question for the noisy observations, what are the rates for such max LP margin classifiers? Okay, so previous works have focused on a p equal to two. So one result by Vidya and co-authors. Vidya and co-authors. They have focused on p equal to 2 and could show consistency when the covariance is spiky, similar to the regression case. But if you have isotropic Gaussians, which I assumed in the beginning, x is standard normal, then for p equal to 2, you do not achieve consistency as n and d go to infinity. So this is high-dimensional consistency. So you can see. So, you can say generalization is very poor, and the original motivation was to understand when generalization of interpolators can actually be good. So, this is not satisfactory, p equal to 2. So, we go and check p from 1 to 2, if there is something better. Okay, so p equal to 1. There was our result for regression. We got the square root 1 over log d over n. 1 over log d over n. Then there were some works already, non-asymptotic bonds, but they focused on adversarial vanishing noise. Adversarial means that you can adapt your noise to the particular distribution that you have and your inputs. And in that case, of course, upper bounds are going to be worse than in the identifier. worse than an identifier identical noise case where they're just iid they're not adaptive but um um yeah but these works also conjectured that in the case where we have iid noise that the bounce would not be better okay so we had this kind of kind of a clash right and here you don't have to focus on on the details here the the main On the details here, the main thing to look at is this performance metric here for our classifier and this norm difference, or this norm difference, the norm of the difference of the unit vectors scales as one over square root log d over n, and this is a tight bound. So it's equal to some constant over the square root log d over n plus something that's of order smaller. Smaller okay, so um, how do you interpret this? Right, so the best way to interpret is just to imagine d is of the order n to the beta. Beta is bigger one, then d is bigger than n. So we have over parameterization. And if we plug this in, n to the beta into this bound, then we get a rate of one over square log n with some beta floating around, which we don't care about that much right now. Around which we don't care about that much right now. Okay, so it does decrease with beta, but that's not the main focus here. Okay, so how good is this bound? So I started saying in the very beginning that previous work already achieved one over square root n, some other algorithms, that they can achieve one over square root n in the noisy case. So one over square root log n is Of a square log n is quite bad, and the other algorithms here are not interpolators. So these were the max average margin solutions. So for these algorithms, we don't know. They usually do not interpolate. Okay, so we have interpolators with p equal to one is one over squared log n rate versus non-interpolators which have a one over squared n rate. So we're still not good in terms of a generalization compared to non-interpolators. Compared to non-interpolators. So, can we do better? This is the question. Before we go there, let's just see what the problem is for p equal to 1 and 2, for which we know both test errors are large. So, this was previous work, and the reason why p equal to 2 doesn't work is because if we have a sparse solution, we have b much larger than n, if you just use If you just use the Max L2 margin solution, then you have way too little data to guide you towards the right solution. So that's the same as for regression. You have a very high bias. You have no idea where to go to, even though the variance is small. And so then we were wondering whether p equal to one would be better. In regression, it is a bit better. One over square log, and it's the same for classification. So basically, if you're So basically, if you've seen the regression talk, the rates will be exactly the same. Okay, but what we compare with is quite different than the regression case, which is much better understood. Okay, in the p for the one case, what is the problem? We also have a very slow rate, one over square log n. So basically, it's just a tiny bit better than p equal to two. The problem there is not the bias anymore because the L1 is really. The L1 is really encouraging the correct structure of the solution, right? I'm searching only in sparse direction, basically, and that's effectively constraining my search space in the correct way. But the problem is that because I have to interpolate noise, noisy labels as well, the variance goes up. And so the, yeah. So this is kind of we have a essentially. Essentially, a new bias-variance trade-off as a function of p. So, if we have bias-variance trade-offs, then the question is: Is something, is there some intermediate p where the generalization is much better? And the solution is, or the answer to the question is yes. P between one and two, you can get much better rates that are of polynomial order. So, in particular, So, in particular, I will not show the complicated picture of the entire theorem, but for beta bigger than 2, and if p is close to 1, then you can achieve alpha equal to minus 1. In this case, just means that you achieve a rate of 1 over square root n, like in the max average margin case, which previous algorithms could achieve, and that there's also a lower bound that's That there's also a lower bound that says we cannot do better than one over squared n. So we get exactly matching upper bounds to the lower bound up to log factors. So this was interesting. And the difference to regression here is that we get exactly one of a squared n, whereas in the regression case, we get only arbitrarily close to it. So for those who know the regression results. Who knows the regression results. Okay. So this is the summary. I have a question. Yes. Is it a sharp transition from one over squared of n to one over squared of log n? Like, does it switch once p below that? Yeah, it should not. Okay, so here's the caveat. Our analysis only holds for p larger than one plus one over log log d. One over log log d. And the problem is that we need concentration, and that's where this concentration of LQ norms, and that's where the one over log log d comes in. We cannot close the gap between one and that one over log log d. Just don't have the tools for it. So we would expect that it would be continuous, but yeah, we're not able to show it. So that's a very good question. Yes, you would expect that it. Yeah, you would expect that it it essentially goes from one over square root of log n to one over square root of log. Yeah, yeah, it should be to some extent continuous, yeah, but hard to prove. Yeah, we were not able to prove in between one and this little bit bigger term. The other caveat is that, you know, this one plus one over log log d, that to be smaller than this term over here that requires. Over here that requires D to be really large as well. So these are caveats of these results. Essentially, we are limited by the concentration that we're relying on to a large degree. And even in the newer results that doesn't use Gaussian covariates, doesn't use the CGMT, there's also this problem, which comes from the bounding the LQ norm and continuing. Q norm and from the concentration of the LQ norm. And yeah, just not able to get rid of that limitation. So is that just because the CGMT has a little bit of slack that you induce a little bit of, like you get end to the one-quarter rates for like progression and stuff using the CGMT? Sorry, can I say again? Is it because the CGMT gives you just a, it induces a little bit of slack, so it gives you kind of, it doesn't give you optimal rates, but it gives you the right. Rates, but it gives you the right limiting results. Is that the reason that you can't go below one over log log d? It's not the CGMT. And because, as I said, like in the new paper, we're not using any CGMT, and we still have this problem. It comes from the concentration of the LQ norm, which is by duality with the LP norm. And we use the LQ norm very heavily in our analysis. Yeah. And yeah. That was indeed also. That was indeed also what we were wondering: is this due to CGMT or is this something else? But really, it's not. For sub-Gaussians, where you cannot use CGMT, you also get the same. So yeah, there's more to be done, I guess, to bridge that gap. Thanks for the question. Okay. So yeah, I don't think I have that much time for this. Time for this. So, this is just a summary of what I just discussed. We looked at new bias-variance trade-offs. So, one of the questions I had was whether or what, in which cases, what kind of interpolators can generize well. And so, if you kind of take a step back on this max LP margin problem, you can abstract out high-level takeaway. Take high-level takeaway. So, what we have for p equal to one is a strong inductive bias towards sparsity or kind of the structure of the ground truth. And on the right, we have no inductive bias towards that sparsity. The high-level takeaway could be, so whatever your strongest inductive bias is, that is best to interpolate the noiseless data. So, knowing some ground truth structure, essentially. Structure essentially, you can think of what is an appropriate or corresponding strong inductive bias. Whatever that is, if you have noise and you need to interpolate for whatever reason, or you just do interpolate, then you can expect to still do well if your strength of inductive bias is not too strong. If the inductive bias is not too strong. Bytes is not too strong. So if you have only medium strength. So if you are in this regime here. And this could be one takeaway. And we found that interesting because, okay, this is called deep learning workshop, right? So we're trying to see this theory, can this actually predict anything for neural networks? And in order to do this, we had to take steps back and abstract out how do you translate our stories. Translate our story to something that can be tested with for images, for example. Okay, so we actually tried this. I'm only going to show the experimental results for neural networks. So going away from linear models from P, we went back to ResNets with different x-axis. Not yeah, res nets in one case, in the other case, is MLPs. So, what are some x-axis? Piece. So, what are some x-axis for neural networks that we could study? Before we go to the experiments, I had two questions that are much higher level. I'm wondering why this is the right problem to solve. And I know you said at the very beginning that the assumption of Gaussianity on the data is restrictive. I guess we would like to have more general distribution than Gaussian, but what I mean here in But what I mean here in particular is that if the task is classification and the data is Gaussian, then there is no structure in the data that supports that you've got two classes to begin with. So there is no expectation of having a margin to begin with. And therefore, this problem formulation with a margin in it is probably not the right thing to analyze. Yeah, that's a very good point. I agree to some extent. It was. It was so this, okay. So, why do we look at this? That's one way is to motivate us just by the optimization, right? So, if you have data, if you have a classification problem and you run some standard first order method, that's what you might get to, right? But of course, if you know beforehand your data structured the way that I described, you have a Gaussian, you have this Kriminov model. Gaussian, you have this criminal model, maybe that's not the right thing to do, right? And that's kind of also what you could conclude from this exercise to some extent. And we go even further for the noise, this case, where this basically supports what you were saying. And related to that, if we are okay with doing Gaussian and we really are interested, I was curious if you could comment a little bit more on the More on the role of the noise model here. The reason being that you didn't say this, I think your noise model is different, but we could have thought that the psi vector is sparse, in which case we would have put a loss that is specifically targeted to that psi being sparse. Or we could have thought of psi is equal to minus one with some probability, and we would have used a loss that is more targeted. So, in other words, I'm arguing given the non- Given the noise model, isn't there a better way to put the constraint instead of a margin constraint? Yes, no, I agree again. So totally. Okay, so I think this is a philosophical, let's see. Okay, so one again, similar answer to before, why max margin is primarily we, okay. Primarily, we okay. We come with okay. These are solutions of first-order methods when we run this on logistic loss. That's why we're interested in these kinds of classifiers. If you have indeed prior knowledge on your distribution of your data, then you could leverage that and use a different method. I'm not saying that we want to find the best method or best interpolator, even to Even to kind of give to practitioners to use. Again, that's maybe the most important slide here. It's more like if we look at known interpolators or classifiers, when can they actually do well? So we started from the opposite side. We started with the interpolators, classifiers. Then we looked at the Gaussian noise model or the Gaussian discriminative model. You could say this is kind of Could say this is kind of, you don't expect it to do that well. It might do much better with mixtures. And indeed, this is also something we would like to look at for max LP margin classifiers for different kind of classification models. For some, it clearly will do much better than others, right? So that's also an interesting question. But we're starting with a bunch of interpreters that are motivated by optimization, not by the particular distributional model. Particular distributional model. So, this is kind of a philosophical thing. And again, if we were to actually try to come up with new interpolators that are good in practice, then we would, of course, have to adapt to the actual prior knowledge on the distribution. I hope this roughly answers the question, but it's a very good question. Why would you even, you know, why would you? You know, why would you even consider this a problem? The main reason is that it's not a methodological approach, but it's really a curiosity-driven question. Just because we know very much about regression in this case, in this case where we have Gaussian data, and we have the min LP or L1 norm interpolators, we know very much about these. We don't know much about the classification case. The classification case, and so that was just out of curiosity what happens there. Great, thanks. Yeah, thank you for the question. Um, so now I have 15 minutes for the second part, the noiseless case, where you will also see that this max margin may not be the right thing to do. So it's actually basically following this question, okay? So, um, yes, so. Yes, so this is just the same thing again. Recapping our no, this was not okay. Wait, I go actually, this is good. So, just to recall, this was the noise model, right? So the or generally the distribution of the data. And now our question is: if ψ is always plus one. Okay, so we don't have any noise. So then, here is a fundamental question, really just out of curiosity, mathematical curiosity. For regression, it's well understood that if you do basis pursuit, then you do a convex relaxation of the originally non-convex problem, which is minimizing the L0 norm, relax to minimize the L1 norm. And you get basically, you don't lose anything in terms of your statistics. anything in terms of your statistical efficiency. So you need samples of the order sparsity s times of d to get perfect recovery with high probability. And basically the takeaway here is L1 relaxation is as good as L0 and it is adaptive to hard spores W star, right? So if you don't have hard spores W star, then this is not going to give you a perfect recovery at all. So this is very crucial here. All. So, this is very crucial here. If you have hard sparsity, then you get very big gain n of the order s instead of n of the order d. That's necessary for recovery. Okay, so this is the picture for regression. For classification, we actually don't know this yet. So, what do I mean by this? So, what we know is this I showed you before. I just rewrote the problem a little bit, just looking at the dual essential. Just looking at the dual essentially. And we know that in this formulation, you can rewrite this as arc min W zero such that min Yi etc. bigger zero. I've just rewrote this a bit. So for this problem, which is non-convex, the error is s log d over n for noiseless. Again, I won't stress this is all noiseless. And so what happens when we relax? And so, what happens when we relax this and convexify essentially this non-convex set here? If we do an L1 relaxation of the set, so now then we can throw away this L2 norm constraint. What is the error here? Okay, for noiseless. Do we also have, first of all, do we get the same error as for the L0 case? And the other question is. And the other question is: Are we adaptive to hard sparse W star? So, do we gain if W star is hard sparse? So, both of these were not known. And that's what I found a bit more interesting, perhaps, just out of mathematical curiosity. Okay. Okay, so surprisingly, the answer is: first of all, we get much worse error than L0. Than L0. And we also don't get adaptivity to sparsity. So, what's known before is again this upper bound for this adversarial noise case, but still the best that existed. We have an upper bound also for the IID noise case of order W1 norm, L1 norm, squared over n to the power 1 over 3. So, important here is this exponent 1 over 3. So, we have 1 over n to the 1 third, which is a much slower rate. A much slower rate than all the other rates we've seen for noiseless. And this holds for any W star. So now this work by Sarah van der Gerdal, they were conjecturing that if we have a sparse W star, then maybe the rate could be faster in terms of n. And they thought this should be possible. And we showed that it's actually not true up to log factors. Up to log factors, we can show actually equality to some constant times exactly the same term, where we have some poly log terms or log to the one half d over n something. I just didn't want to write it here. We have the same rate, essentially, even if we assume hard sparsity. So, this is for any sparse W star with sparsity smaller than this n to the two-thirds. Than this n to the two-thirds, et cetera. So, including w star, which is like 1000. So, even for that, you cannot get a faster rate for noises. So, if you look at this, well, this is the same just writing the dual version of the argmin of the L1 norm. This is basically the max. Basically, the max L1 hard L1, or how do you say, hard L1 margin SVM. Okay. The error would be, is actually equal to, meaning a tight upper and lower bound, this one over n to the one-third. Okay, so if we go back, sorry. Sorry, if we go back here, right, if you compare for regression, actually the convex relaxation was totally fine. For classification, the error is much larger if you do the convex relaxation. Okay, this is something that we didn't expect. We thought at least you would get one over square root n for this. For this for max L1 margin, which would basically be equal to the noisy case for p between one and two. But that was not possible. And it's much worse than the one over n fast rate for this non-convex problem. Okay, so this was quite surprising to us. And so, what is the intuition behind? And so, what is the intuition behind this? I think, and the intuition here really crucially relies on the distribution model. And that's related to the question before. So, the reason why L1 relaxation is bad or this max L1 margin solution is bad is that the max L1 margin solution is Is has a large min margin, which is at least n to the minus one-third. Okay, if you have n samples. And if you take the ground truth, you can do some calculations with Gaussians, but it should also hold for sub-Gaussians. Then you get that the ground truth, let's say it's 1000, just in the very easy case, then this. then this min margin will be smaller than n to the minus one one over square root n for all points right so for all points this value here will be smaller than um n to the minus a half so so the margin so the min margin for the ground truth is much smaller than the margin the min margin for the max L1 margin solution. For the max L1 margin solution. Okay, even though W star is sparse. So, and that is, of course, related to the fact that you have this Gaussian concentration around the decision boundary. If you didn't have that, if you had more separation like in a mixture, then you might not have this problem for max LP margin or max L1 margin. So that's one note here. So actually, if you have mixtures, then max LP would be even much better suited. Would be even much better suited. Yes. Can you explain where the one-third is coming from? That's rather technical. I mean, this is kind of when you are trying to, this is basically the same one-third that comes in here. They prove this rate already before here. For this one-third, they go through the max margin, and it's just calculations. Yeah, I can. Just calculations. I cannot give a very intuitive answer right now in this time. There's also a chat. Okay. So I wanted to use the last few minutes to kind of suggest some interesting questions that popped out just by just out of mathematical curiosity. That came up from our analysis. So, one thing is we just saw that in the case when we have noiseless data, somehow MaxL1 margin or the convex relaxation is not good. And in particular, here we have this max min, right? So maximizing the min margin subject to the Z L1 constraint. So the question is: can this formulation here with the maximizing the average more? Here with the maximizing the average margin, do better. And one reason why it already does better is that it already gets one over square root n in the noisy case. So this is not anymore an interpolator. So this is going away from the motivation to study interpolators, just purely in terms of compressed sensing, one-bit compressed tensing. Can you do better in the noises case with this L1 constraint? L1 constraint, but with a different objective if you loosen up your interpolation constraint. You should be able to do better, but can you reach something like one over n? That's completely unclear. Second question is I motivated the max LP margin classifier here by steepest descent on logistic loss. So what if we stop steepest descent somewhere? Stop steepest descent somewhere before it converges directionally. Will we get the max average margin solution, for example? Also unsolved. And then now moving beyond one, p equal to one, let's go back to the whole picture. So we know for p equal to zero in the noiseless case, this is all noiseless, you have this rate one over n, that's possible. You have this rate one over n to the one. You have this rate one over n to the one-third, and in between you have the rate one over square root n, because that holds for noisy case, so it should also hold for the noisiless case. So if you then just kind of, this is an illustration, right? So if you just look at the error, it seems that there is a maximum at one, and then it goes down again and goes up again. So this is a kind of unexpected picture, perhaps, at least for me, I was very confused. At least for me, I was very confused when I was putting all the results together that you would see something like this. Okay, so one question is, okay, in contrast to regression, somehow the regime between zero and one, there's something actually happening there, so it's getting worse. So why does it do better here in contrast to regression? And the other question is the second part that there is a dip in between one and two. Somehow, between one and two, two, two, two, two, Two somehow between one and two, it's better, even in the noiseless case. So, that's also something I don't understand because, uh, right, like the best inductive bias you should always use if you have noiseless data, why should you ever not use it? Right, so this is something I don't understand. And so, I actually was doubting our results again. Like, is this, can this be true? But then we actually ran a couple of very fast experiences. Of very fast experiments, and there we could actually see this. So, this is a linear model. For different piece, we look at the test error or the error that I wrote, the directional error. And we see indeed for the very high-dimensional case, this actually happens even in experiments that p bigger than one gives you a smaller error in the noiseless case. And that's very, very puzzling. And that's very, very puzzling. I don't have any intuition for this, and but I find this also very exciting that we don't have any intuition for it. And that's something I would find interesting to look at. Again, this is moving a bit further from explaining neural networks, but these are interesting questions that popped up that were motivated by the double descent and looking at binographer fitting, then looking into the noiseless case. Then looking into the noises case and becoming more and more of a mathematical question, that is something quite puzzling. Of course, here the question could also be, this is something very pathological and it only happens for Gaussian covariates. We should try for other covariates as well, at least experimentally, but I would expect that it's similar. Yeah, and then this why is P bigger one better than P for one? Very interesting question, at least from my perspective. Okay, so yeah, these are all the things I wanted to say. So here are the results. The results are in these three papers that I discussed in this talk. The first one is on P between one and two. The second one is the noiseless results and also the Result and also the result for p equal to one for benign overfitting are in the second paper. And I didn't really get to the experiments in the end because, yeah, we didn't have time, so I skipped them. If you're interested in looking into how we translated this whole story about inductive bias into experiments for neural networks, you can look at this paper here. So, yeah, then I would like to thank everybody also for the great question. To thank everybody also for the great questions. And if you have any more questions, happy to answer. Thank you. Thank you very much for that wonderful talk. I think there is a large number of questions, and then for the online participants, we also need to take a photo. So please stay tuned for the round of questions. Xi Shing, I think you were first online for questions. Is that correct? No? No? Did you have a question? No. No, I don't. No? Okay. Any questions from here or online? Okay, I hope this is working now. So this might be a silly question, but it pertains to the first part of your talk on the setting with added noise, label noise. So, you know, it's like I'm trying to make an extrapolation from the regression setting where we understand very well. Setting where we understand very well how L1 relaxes in L0, and under some settings you get matching rates and everything's great. Now, if I take your result of using P slightly larger than one, how should I understand this in the context of thinking of an L zero, minimum L zero interpolator in the noisy setting? Like, is there a result for that? I'm not quite sure. I completely understand your question. I can go to one slide and you can say if that answers your question. Sure, let me try to rephrase it. Yes. Is there a result for the min-L0 interpolator in your label noise setting? Min-LP. For classification or for classification? Classification or for classification? For classification, yes. Min L zero in the interpolator in the noisy setting. Noisy setting. Okay, that one. Because that would be, in that case, your result would be the relaxation of that estimator, right? Yeah, yeah, yeah. So you're saying this one, right? So this non-convex L0 thing, and you put in noisy. Yes. Yes. This one, okay, good. This one, okay, good question. So, for classification, nor for, no, also not for regression, there is, but basically, okay, so for regression, we would expect that it would not get better than L1. So, for L1, whatever. And for classification, it's an interesting question because of this weird noiseless behavior, right? So, you could think maybe that's also helpful to go beyond. That's also helpful to go beyond one. So, that I can't answer. It's a very good question. That would be question number five for future work. Yeah. All right. Thank you. Yeah. All right. Any other questions? Any other questions online? Okay, so if you're online, the organization here at Oaxaca has requested that you show up your face because they're going to take a photo of you. They've requested that of us here. They've requested that of us here in person. We're taking a photo at 12 noon today. So please show up your face and they'll take a photo. So I stopped share, right? It's already stopped the ticket. Perfect. All right. Thank you very much, everybody. We're going to take a break now. Ready. I'm going to take a break now for 25 minutes, and then we have Gita's talk. Thanks.