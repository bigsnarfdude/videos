The picture work. Okay, so it's uh unfortunate that I cannot be in Banff. I would have very much like to meet you all, but thanks anyway that I can give the talk in this way. The talk is certainly on the more analytic side, but at least the idea comes from conformal geometry. There will be a sigma two curvature that you will try to minimize and at least. And at least in the conformal class of the standard metric on the sphere. And I'm curious to hear your feedback, and perhaps some of the things, some of the techniques that we develop will also be helpful in some geometric context. All I'm going to tell you is joint work with Jonas Petter Andel, who is a PhD student with me in Munich. And let me join. And let me jump straight away to the main result. So I'll present this on this slide, and then I will go backwards and motivate where this comes from and why we think this is interesting. And then afterwards, I will give you some ideas of the proof. Okay, so here things are very, very sketchy, but I think we've heard a lot about sigma 2 curvatures. Yes. Yesterday, so I'll just use that notion. So we have a metric G on the sphere. The dimension will always be strictly bigger than 4. The metric is assumed to be conformal to the round metric and to have positive scalar curvature. We're interested in the total sigma 2 curvature, which is simply the integral of Which is simply the integral of the sigma two curvature over the whole manifold. And then to normalize this thing by scaling, we simply divide by the volume of the metric to the appropriate power. And the question is, can we make this total sigma curvature? How small can we make this total sigma 2 curvature given the volume? Okay. Okay, and so this question has already been asked and answered some 20 years ago in joint work by Guan Wyaklowski and Wang. And really, this is a combination of two earlier works by Wyaklowski and by Guan and Wong. And what they showed there is that this infimum over here is obtained. Obtained precisely at the round metric itself, and then because we have an invariance under Merbius transformations, it can also be an image of the round metric. Okay, so the stand-up metric on the sphere minimizes the total sigma 2 curvature normalized by the volume. Now, the question that I want to ask in this talk is: what happens if Is what happens if this quotient, so the volume normalized total sigma two curvature, is close to its minimal value s? Can we then say that necessarily the metric G is close to the round metric or maybe it's image thereof? And the answer is yes, and that's expressed. And that's expressed in terms of the following inequality. So, this is the volume normalized total sigma 2 curvature. If it's close to s, then the left side of this inequality is small, and therefore the right side of the inequality is also small. And that means precisely that the conformal factor u is close to one or a conformal factor. To one or a conformal image of the one. This holds with certain norms and with certain exponents that I will tell you about later. But that's the general idea. So we want to say that if the total sigma 2 curvature is small, then the metric is necessarily close to the round metric. And now, as I said, I want to backtrack a little bit and I want to give you. And I want to give you some general introduction to the topic of quantitative stability for functional inequalities. Because this is sort of where this question and answer belongs to. And so perhaps the simplest version or the most the easiest version is the isoprometric inequality, right? So what do we want to minimize there? We want to minimize the surface area. Want to minimize the surface area given the volume. This can be expressed as an inequality: that the surface area is greater or equal than a certain constant times the volume to the appropriate power. And we all know what the answer is. The minimizers are precisely balls. The stability question is if the isoperometric ratio, I mean the surface area divided by the Surface area divided by the appropriate power of the volume is close to its minimal value. Is it then possible to say that the set is close to a ball? Okay, so it's the same idea. We want to have some sort of energy, some number close to its minimal value. And from this closeness of numbers, we want to get a closeness in function space. Before it was the metric or the conformal factor had to be close to something. Now the set. Be close to something, now the set has to be close to some other set. That's the general idea here. And so there have been many works on this question. And so already 100 years ago, there have been first inequalities of that type. But then a paper published in 2008 by Fusco Macci Buratelli really brought this to big attention. And since then, there has been a huge amount. Has been a huge amount of work on stability inequality. And what was shown in this work was that this difference between the isoparametric ratio and its minimal value controls the distance of the set to the next possible ball. Okay, that's what the delta here stands for: the symmetric difference, and what we see here. And what we see here is a power 2. And I'll talk about this a little bit later. Now, let me go to an inequality that has a little bit more to do with today's topic, and that's the Sobolev inequality. So I have a function w on Rd. D is strictly greater than 2. And if I know that the function tends to zero at infinity and that its gradient is square integrable, then this controls. Then, this controls a certain LP norm of the function. Again, there is an optimal constant. I didn't write down the value because the precise value isn't so important. And we also know that there are certain special w's for which equality is attained. One of these guys has this form, 1 plus x squared to an inverse power. 1 plus x squared to an inverse power, but again, this is not so important. It's just these are sort of the analogs of the balls in the problem with the isoparametric inequality. So then back in 1985, Reesis and Lieb asked the first question about stability. They were saying, what if the Sobolev quotient, so again, the gradient of W squared. U squared integrated divided by the corresponding Lp norm. What if this thing is almost equal to the smallest value it can be? Does this necessarily imply that the function w is close to this function q or you know there are some symmetries going on or a version of it when we apply symmetries, and that was. And that was answered affirmatively in work by Bianchi Egnell in 1991. And so, what they showed is, again, the difference when that is small, we have to be close in here the good topology. I mean, that's the gradient, that's the strongest norm that's in the game. And in this strong norm, we have Norm, we have closeness. Now, what is remarkable about both these results are the powers two. Okay, you see on both sides the distance to the manifold of optimizers vanishes quadratically with the smallness of the energy. And that should not come as a surprise if you. And that should not come as a surprise if you think about ordinary calculus, right? When you have a generic function, its minimum will be quadratic. So that's the same thing. You have a power two, that you have a quadratic minimum. Your left side has a minimum and therefore it vanishes quadratically. That's the idea behind this power two. And as soon as you get a power two, you're happy. Okay, so the two things that are important. Number one, you want to get a good Number one, you want to get a good topology with respect to which you measure the closeness to the optimizers. And number two, you want to get a good power outside so that the order of vanishing is optimal. All right. So now let me tell you what this Sobolev inequality has to do with conformal geometry. Conformal geometry, at least on SD. It will become a little bit more geometric a little bit later. But this is probably all well known to you. This is the inequality that we've talked about so far. That's the function w on Rd. And now I want to introduce another function, a little w. That's a function on Sd. And the capital W and the little W there. And the little w they are related by a stereographic projection, right? So I'm just using that rd plus a point is conformal to sd, and then I can move from one picture to the other picture. And then just the L2 norm of the gradient becomes this functional on the sphere. And the L P norm stay on Rd becomes the L P norm on the sphere. Okay, so now Okay, so now we have this functional here on the sphere for the little w. And now, what I can do is I can view the little w as a common formal factor, and then I can write this functional as the scalar curvature, or since I want to get later to sigma k curvatures, I can also write it as the sigma one curvature, the total sigma one curvature. Okay, and then the Sigma 1 curvature. Okay. And then this p-norm, that's nothing else than the volume of this metric G. So what this argument tells us is that the ordinary Sobolev inequality on Rn is really nothing else than minimizing the total sigma 1 curvature normalized by its volume over all metrics that are conformal to the standard metric on the sphere. metric on the sphere. And we also know this was the thing that I didn't tell you on the previous slide, you know, this function q and so on. This function q, this really means just the round metric on the sphere. So again, optimizers are precisely the images under Mibius transformation of the round metric. And now having said that, I can phrase this Bianchi-Ignell theorem from the previous slide as As the statement that this deficit between the total sigma-1 curvature and its minimal value is bounded from below by the distance of a Merbius image of the conformal factor from the standard function one. And the things that I want to emphasize is again the topology is the W12 topology. W12 topology and the vanishing is with the power 2 of the distance. So now this inequality looks very much like this inequality that I showed you at the very beginning, you know, where we have the sigma 2 curvature minus its minimal value and, you know, bounding from below a distance from the set of optimizers. Now, before I go back to the Sigma 2 business, let me tell you a little bit more about stability inequalities. So, the things that I'm going to tell you first here, these are recent developments, meaning they come from the last two years or three years. And the first one does not have terribly much to do with what I Much to do with what I will do, but I think it's still interesting, and there are some open questions that I wanted to mention. So, this is some work that I've done with Jean-Dolbeau, Mariel Steban, Alessio Figali, and Michael Loss. And the point is this proof of Bianca Genel, this has really been super, super influential. There are many, they have devised a strategy that can be. A strategy that can be really applied for almost every stability inequality that you want to prove. It's a very robust strategy. Now, the price you have to pay for this robustness is that it uses compactness and therefore it does not give you any bound on. You know, I used this twiddle symbol, I didn't explain that. Didn't explain that, but what I mean here is that the left side is greater than a small constant depending on d times that infimum. Okay, so there's a constant hidden. And the point I'm trying to make is because Bianchi Guinel use compactness, they do not get any value, any information at all about this constant. It's just compactness says there is a positive constant, but we have no idea what it is. And so, what we could do in this work is we gave an alternative proof where we got a number without using compactness. So like an explicit number that you can compute, that you can write down, and we could, in particular, track its optimal dependence on the dimension. And that's particularly interesting. Interesting when you look at the high-dimensional limit, you know, there this inequality becomes a log-Sobolef inequality, and it's absolutely crucial that you get the good, the optimal behavior of the constant with respect to D. And as an open problem, I should mention this. You know, there was this inequality by Fusco, Maggi, and Bratelli, this inequality here, which, you know, gave this. Which gave this big impetus for all this study. And in this inequality, the behavior of the optimal constant with respect to the dimension is not known. So that is an open problem in case anybody is interested in that. Now, let me describe the second recent development, which concerns Sobolev inequalities, where we have the gradient now in LP. P being between one and the dimension. And of course, as soon as people started looking at these stability inequalities, people have started working on this. And the final answer was given by Figali and Shang. And what they showed is, well, by now you know the idea. This is the quotient that you want to minimize. This is its minimal value. On the right side, Value on the right side, you have a distance from the family of optimizers. Now, here comes the surprise. What Figalia and Shang found is that this stability exponent, which I call alpha here, that this stability exponent is no longer necessarily equal to two. More precisely, when alpha is greater than two, then the stability exponent becomes p. Stability exponent becomes p. So, meaning you do not have a quadratic minimum, but you have a degenerate minimum. So, the distance vanishes faster with well I mean it's not a quadratic minimum, but it's a higher order minimum. The energy landscape is flatter around the manifold of optimizers. Around the manifold of optimizers, and they show it's optimal, right? I mean, this is not an artifact of the proof, but for p less than two, you get it with stability two, and with p larger than two, you get it with stability exponent p. The third development that I want to mention is due to Engelstein, Normai, and Spolauer. And this is now geometric. So now we're in a manifold M with a conformal class. With a conformal class described by metric g, and what we're looking at is the Yamabi functional. So I'm now writing r for the scalar curvature, not sigma 1. It's the same thing. Again, you try to minimize this by definition. This is the Yamabi constant, y. And then this difference is bounded from below by the distance to the manifold. And now you see there's a stability. And now you see there's a stability exponent, and once again, there's a stability exponent which depends on the manifold. And they show that generically this stability exponent is equal to two, but there are cases where the stability exponent is strictly bigger. And then actually I did a little computation, and you see that if you have this. And you see that if you have this particular manifolds, I think Rick Shane has introduced them in the setting of the Yamabini problem or studied them in detail. And so you have an S1 plus Sd minus 1, and you have a free parameter, which I take the radius of the sphere, the S1. And then there's a critical value. And when you add the critical value, you have stability exponent 4. Value, you have stability exponent 4, and when you're off the critical radius, then you have the ordinary stability exponent 2. Okay, and this might remind you of some works on the convergence or the rate of convergence of the Yamabi flow. You know, when you have stability exponent two essentially corresponds to To exponentially fast convergence of the Yamabi flow, whereas if you have here a stability exponent alpha which is greater than two, then you only have a polynomial rate of convergence. And this was studied by Carlotto, Cholos, and Rubenstein. So these were three developments about stability of functional inequalities. There are many more. I even in the audience said. In the audience, several of you have worked on this, and I'm sorry that I did not have time to quote all this. I just wanted to set the scene, give you some ideas of things that will be relevant in what follows. But as I said, this is by no means comprehensive and there have been many more important works. So now I want to really come to the topic of this talk, which involves these sigma 2 curvatures. So once again, a quick So, once again, a quick run through the definitions. There's the Schouten tensor, which involves the Ricci, minus a multiple of a scalar curvature times G. And then you take the k-thelementary symmetric polynomial of the eigenvalues of the Schouten tensor, and the total sigma k curvature is the integral of this sigma k integrated. Sigma k integrated over the manifold. And as we have seen several times yesterday, if the k is equal to 1, then this simply becomes a multiple of the scalar curvature. Another important case happens if k is equal to the dimension, then we have the determinant, the product of the eigenvalues. Of the eigenvalues, but here we're interested in this k equal to 2k where, well, it can be written in this way. The manifold that we're interested in is only the standard sphere with its standard metric. We're looking at, and the dimension is always greater than four, as I told you. And we're looking at the problem of minimizing the total sigma k curvature. The total sigma k curvature divided or normalized by its volume. Now, as I mentioned already, there is this work 20 years ago by Guan Yakovsky and Wong, where they show that this minimization problem is solved precisely by the round metric and its images. Now, how does this proof work? Let me give you a quick breakdown. Breakdown. Vyhaklovsky has shown what the critical points of this functional are on the sphere. He has completely classified them. These are exactly the ones that appear as optimizers. But in order to deduce that those are the minimizers, we need to show that there is actually a minimum. And this was shown using parabolic methods by Guan and Wong using, yeah, as I said, some parabolic. Using, yeah, as I said, some parabolic methods, flows, and the behavior, the long-term behavior of this flow. This is quite serious mathematics that's going on there. These works have had an additional condition so that not only was sigma one positive, but also sigma two had to be positive. I noticed that I'm not consistent in my notation, so I should. Annotation, so I should erase that. And so, in a later 2013 work, this additional assumption on positivity of sigma 2 was dropped, which is useful for us. Anyway, an alternative proof is given by Jeffrey Case, also a couple of years ago, which Which I guess I particularly like and which attracted my attention to this whole business. As I said, what we want to ask is if we don't have a precise minimizer but an almost minimizer, are we then almost round? And now, what I will do is I will frame. And now, what I will do is, I will phrase this inequality in purely analytic terms. Meaning, I will write my metric as u to some power times the round metric, and I will express everything in terms of this conformal factor. And so they will be functionals, and they have rather complicated expressions. Here, I introduce something called sigma 1 of u, and that is if you And that is, if you make this parametrization, then up to a factor, this sigma one u is really the scalar curvature. But, you know, there's these, this is not such a nice expression because we use a parametrization that's not the natural one, so to speak. I'll come back to that point a little bit later. Point a little bit later, and then there's a term that's even more complicated, lots and lots of pieces. The meaning is that if you integrate this E2, I want to think of this E2 as an energy density, then you obtain the total sigma 2 curvature. Okay, so let me emphasize what we should focus on when we look at this term. So, on the one hand, term. So on the one hand, we have here a term that is gradient u squared times gradient u squared. That I want to emphasize. And then I have another term which is sigma 1 u times gradient u. These are the two terms that you should really focus your attention on. The additional terms, this one and that one, those you know they are necessary, but those do not present. But those do not present such a big problem. Anyway, the good thing is that all terms that appear here have positive coefficients. So there's no cancellation between the different terms. We also see that this whole functional is four homogeneous, right? Everything u always appears to the power four. For that reason, the relevant inequality. This is the relevant inequality, right? I mean, I can now write my sigma 2 inequality, I can write the inequality that is for metrics as a functional inequality. Namely, that this E2 functional that I had on the previous slide with the complicated coefficients, that this controls a certain LQ norm of my function. Q is 4d over d minus 4. The point that I want to make is The point that I want to make is this inequality is very similar to an inequality that we've seen just a few slides ago, namely the Sobolev inequality with a gradient in L4. Remember, I said that the main term that you should focus on is the gradient of u to the power 4. It's a four homogeneous inequality. And the power q that you use to control is the same one both in this inequality on the sphere. In this inequality on the sphere and this inequality on Rd. There's a major difference, however, between these inequalities, these two inequalities, namely the inequality on Rd, the Sobolev inequality with gradient in L4, that is not conformally invariant. This is just by construction. This inequality is conformally invariant. And what exactly do I mean by this? So if I have a Merbius transformation of the sphere. So, if I have a Merbius transformation of the sphere, right, then I can compose U with this Merbius transformation, and I can multiply by the appropriate power of the Jacobian. And if I do this, then my functional stays invariant. So, I have this huge symmetry group coming from this group of Verbius transformations. This is an invariance. This is an invariance that I do not have for the standard Sovolev inequality. The price, so to speak, to pay is that, remember, this inequality, the geometric inequality requires my scalar curvature to be positive, and that means that this rather complicated functional sigma one that I introduced on the previous slide has to be pointwise positive. To be pointwise positive, and that's very different from such an inequality where you do not have a pointwise constraint, right? And well, I'll talk about the remaining things later. So, now here's again the theorem from the first slide. This is the inequality that we prove. So, this thing here. So, this thing here is the total sigma 2 curvature volume normalized minus its minimal value. And what I show is that this can be bounded from below by a certain notion of distance to the manifold of optimizers. So let's look at this more carefully. There are two terms appearing on the right side. The two terms come with two different norms. Norms. One is the W12 norm, one is the W14 norm. And together with these two norms come two different stability exponents. Remember, that's what I emphasized before all the time. The question is: how do we vanish as we get to the manifold as optimizers? What we would expect is a quadratic vanishing, or that's the naive guess. Vanishing, or that's the naive guess. And what we show here is that indeed, when you look at the W12 norm, you have a quadratic vanishing. Whereas if you look at the stronger norm, namely the W14 norm, then you only have, or then you have a much faster vanishing, then you have fourth-order vanishing. And what we show in our paper is that this vanishing is optimal. This vanishing is optimal. What I mean by this is when you insist on using the W12 norm, then the vanishing is quadratic. When you insist on using the W14 norm, then the vanishing is quartic. Okay? So, for two different notions of distances to the manifold of optimizers, you have two different You have two different vanishing powers. It perhaps does not come too much as a surprise that you have this four power, because remember, this is what Figali and Shang got when they looked at the Sobolev inequality in W14. It's exactly that power 4 that appears here, and it's not this power that comes from Engelstein. That comes from Engels' dynama aspolar that I mentioned before. Okay. I should also say that the W14 norm, if you just ask in what kind of norm can I estimate the distance from the manifold of optimizers, then the W14 norm is best possible. And that's just when you have bubbling, meaning when you have concentration of this conformal factor U. Of this conformal factor u, then you see that you cannot have any other law. All right, and so what's new about this inequality is I think it's the first inequality, first stability result of a functional inequality where the underlying Euler-Lagrange equation is fully nonlinear. Okay, so so far we had, we started with the Sobolev inequality with good. Started with the Sobolev inequality with gradient in L2. That gave us a semi-linear PDE. Then we had the gradient in Lp, with P different from 2. There we had a quasi-linear PDE. And now for here, the underlying PDE is a fully non-linear PDE. Okay, and it's the first time that there is such a stability result. And the difficulty in the proof is In the proof, is on a technical level, is this second term that I showed you here: the sigma one times gradient u squared. That's a contribution to the energy that is not controlled by the remaining terms. I mean, we only know it's positive, but there is no function space sort of in which we can control this. On the other hand, this other term. On the other hand, this other term, this gradient u to the power 4 term, that's, you know, that's similar to the term that we have in the Sobolev inequality on Euclidean space. So that's, I mean, one has to deal with it. And as I told you, Figali and Cheng only were able to show how to do this a couple of years ago, but that there we can use techniques that they have. Use techniques that they have. Dealing with this sigma one term in the functional is really a new analytic obstacle that we overcome. And I think one thing I, that's my first point here, my first comment that I didn't really emphasize. Of course, the original inequality is conformally invariant, meaning when I act with the Merbius transformation on the U, then the left. On the u, then the left side of my inequality doesn't change, and so by construction, the same is true on the right side. Okay, so if I if I transform the u, you know, then this just by the by the group property doesn't change this infinite. Okay, so I agree that this is perhaps not the geometrically most natural notion of a distance, but it does. But it does have the required conformal invariance that is useful. Anyway, so now the remaining time, I want to explain you a little bit on how our proof proceeds. I should say that the proof is not on the archive yet. We have a preprint that's almost ready, and I hope that it will appear on the archive within a week. Will appear on the archive within a week or two. Anyway, here's the general construction. The general construction is very much like what Bianke Ignel suggested. Bianchi Genel say we have to argue in two steps. The first step is saying that if we are away from the set of optimizers, then the bound is. Then the bound is trivial. That's step number one. That's the global to local reduction, right? So if you're far away, then everything is good. And the second step is now proving the bound really when we're close to the manifold of optimizers. Okay, let's go through this in more detail. So the first thing that's really compactness statement. Compactness statement, meaning you have a minimizing sequence for our sigma 2, total sigma 2 problem. U n is a minimizing sequence for that. And what I want to show is then I'm necessarily close to the set of optimizers. Okay, so I have convergence, strong convergence in W14 of minimizing sequences. And the second ingredient, that's now proving the bound locally. So now we assume that we are very close to the set of optimizers, and what we want to do there is prove the bound. Okay, but I don't do it uniformly, I just say that for a sequence that gets closer and closer, I eventually get a boundary. Okay, and then it's not hard. I don't want to go through these details here, but using these two things, I think it's obviously, I mean, intuitively clear that these two things together imply the stability argument. Okay, if it would not hold, then you argue and so on. So, the point that I want to make on this slide is there are two things we have to do. We have to prove a compact. Have to do. We have to prove a compactness theorem for minimizing sequences. And number two, we have to prove the bound locally close to optimizers. I was particularly interested in the proof of the compactness theorem, and this is motivated by the previous meeting in this series where this question came out there. This question came about there. There was a very interesting talk also about sigma 2, now sigma 2 trace inequalities. And there was the question, can one prove convergence of minimizing sequences? Or more generally, can one prove existence of a minimizer? More specifically. Now, I've been thinking about this problem for a long time. Thinking about this problem for a long time, and this seems to be really hard. I could not really come up with a nice solution. That was my, I mean, one of the big things that I want to do. Usually, you can prove compactness of minimizing sequences using some concentration compactness arguments. That's a very standard methodology. But the interesting thing about this, the analytically interesting thing about this, analytically interesting thing about the sigma 2 problems is that these concentration compactness arguments seem not to work. At least I've not seen anything in the literature. There is, however, we found a way around it. It's a little bit, I don't know, a cheat or I mean it's a nice inequality, but I mean what I want to say is we did not manage to do what we originally wanted to do. Anyway, we could prove what we Do. Anyway, we could prove what we needed, and this is based on this nice inequality by Jian Wang from 2013, and that's a monotonicity. Okay, so what this inequality says is that sigma 2 divided by sigma 0 controls sigma 1 divided by sigma 0. There's a monotonicity in the sigmas. Okay, that's a really powerful. That's a really powerful inequality. And for us, this is useful because if I have a sequence of metrics such that this thing converges to one, I mean, if I have an optimizing sequence for the total sigma two curvature problem, then this says that this sequence of metrics is necessarily a minimizing sequence for the total sigma one curvature problem. sigma one curvature problem okay right so i'm this s prime i haven't said what that is but that's by definition the sharp uh yamabi constant okay that on the sphere right i mean that's just e1 remember e1 is the sigma one curvature or the scalar curvature and so this is the right inequality here is the sharp yamabi inequality Inequality. And so if this thing tends to one, then necessarily this thing tends to one, and we can use everything that we know about the sigma one problem and deduce the compactness therein. There's a little bit of a small print, namely when you work with scalar curvature, you parametrize the matrix as w to the 4 divided by d minus 2. d minus 2. Whereas here for the sigma 2 problem, we parametrized by 8 divided by d minus 4. And so really you get the wrong, I mean you get the convergence from this inequality, you get in the W parametrization, and then you have to translate it to the U parametrization. That's a little bit of a headache, but it's not it's doable. And I don't wanna wanna bore you too much with these details. Bore you too much with these details. Let me instead move on and talk about the second step. That's the point where we want to prove that close to the set of optimizers, we do have our inequality. And remember, that's where the funny thing happens. The funny thing being that, depending on with which norm we measure the distance, a different A different power do we get. When we measure with respect to the W12 norm, then we get a power two. When we measure with the W14 norm, we get a power four. So that's a little bit strange because, you know, usually when you think when you're close to the set of optimizers, you make a tail expansion, right? And well, then you keep terms of order two or whatever, terms of order four. Whatever terms of order four. But how can it be that there are terms of order two contributing and terms of order four? That's the difficulty that we have to get our hands on. All right. So we expand. I mean, nevertheless, we expand. This is our functional now written in additive form. Okay. And I expand in the quadratic terms, cubic terms. Cubic terms, quartic terms. There is one term here which is not homogeneous, so I cannot really tell you which homogeneity it is, but it's at least a cubic term. And anyway, everything else is more than fourth order, so that we can ignore. So now we have these various terms, order two, order three, order four. Order three, order four, and this mixed order. And there are constants. I didn't write out the constants so that I hope you can focus on the important ones. All right. So there are various difficulties here when we look at this. The first thing, that's not really a difficulty, but when you look at the quadratic term, right, that's a gradient squared. That's a gradient squared minus d. So the quadratic term is not positive definite. As I said, that's a minor problem. Next, we have the cubic terms. And we don't know what to do because there is no reason that Rn has a sign, so they can go either way. Finally, well, we have the force, the quadratic terms, they're fine because they're all positive here, so that's good. And lastly, we have here a big negative term, where we also don't know what to do. Now, perhaps your first unaïve reflex would be to do a Schwartz inequality for the cubic term between the quadratic and the quartic term. But that does not work. But that does not work. Okay, so you really cannot absorb the cubic terms between the other terms. It really, I mean, the coefficients just don't work out. So we have to do something smarter. And so the question is, again, how can it be that a quartic term can be as big as a quadratic term? Okay, and that. Okay, and that can only happen if our sequence sort of runs away to very high frequencies, okay, when it's oscillating very much. And how do we capture this? We capture this by decomposing this remainder Rn, right? Rn, I didn't emphasize this, but Rn is the difference. I mean, you write your optimizing sequence Un as the one guy. Un as the one guy, that's the fixed guy. You assume that you're closest to the function one, and Rn, that's the remainder. And so this remainder you decompose according to spherical harmonics. And we do this here. We have very low degree spherical harmonics. I mean, only degrees harmonics of degrees zero and one. Then we have medium degree harmonics. Medium degree harmonics, those are up to some degree capital L, which is a parameter that I have at my disposal. And finally, I have the high frequency contributions. Those are really the big ones. And now the point is that if you have, so this capital L, that's a momentum cutoff, okay, a frequency cutoff. And the space of 12 gharmonic. The space of swerve harmonics of degree less or equal than L, that's a finite dimensional space. So, on this finite-dimensional space, all norms are equivalent. So, in particular, it doesn't matter whether I work with the W12 norm or the W14 norm. So, this has the consequence that we can drop all low-degree spherical harmonics in the Q. In the cubic and the quartic terms. Put differently, in the cubic and quartic terms, only the high spherical harmonics contribute. Okay, that's sort of the key point in the proof. Whereas in the in the for the For the high, so the high frequency components, for them, the first term is very big, the quadratic term is very big, right? Because that gradient is very large, because you're oscillating so fast. And because this term is so big, we can now actually apply a Schwartz inequality and kill the cubic terms. Terms so that the only term, the high frequency component that remains, is the gradient to the power 4. I understand that this was all a little bit fast, but as I'm getting to the end of the talk, I do not want to explain this anymore. It is a little bit technical, but the point I wanted to emphasize is that the fact that the size depends on the size of the size. That the size depends on the norm is captured by using spherical harmonics. And with this, I come to the conclusion. Let me briefly summarize what I've done. I've considered a sharp Sobolev type inequality on the sphere that is related to an L four Sobolev inequality, but in contrast to the ordinary L four Sobolev inequality, it has conformal invariance. Inequality tests conformal invariance. I've shown you a stability inequality that respects this conformal invariance. Here it is once again. And the remarkable thing about this inequality is that the vanishing as you approach the set of optimizers depends on the norm you're using. I didn't tell you the argument why these exponents are optimal, but you have to. Are optimal, but you have to believe me that you cannot improve either one of those. Okay. And the open questions are, of course, I mean, why did I do this for sigma 2? So I could do it for sigma 3 volume normalized. I could even try to do it for sigma 2 divided by sigma 1, sigma 3 divided by sigma 2, whatever. I mean, there are many similar questions. There are many similar questions that can be asked. We have not been able to do this. I mean, we believe that something similar should work, but you know, I mean, I've put under the rug a lot of explicit computations and expansions. And we really use this form that I have on the slide here, these positive coefficients and how they're related. And we have just not found an efficient. And we have just not found an efficient way to get this in the general setting. And then the last inequality, there I want to go back to this question that Engelstein, Neumann, Spolauer asked. What about general manifolds? Again, you can now have, say, quartic stability exponent due to the Exponent due to the fact that you have a gradient to the power four, but you could also have this power four, this increased stability exponent due to zero modes of the hash that do not come from symmetries. And this is, I think, very much, or I believe that this is related to convergence rates for the sigma 2 Yamabi flow, in particular, slow convergence. In particular, slow convergence rates. But as far as I understand, this has not been explored yet. Anyway, that's all I wanted to tell you. Thank you very much for your attention. Thank you very much for talking. Actually, close this close.