And I will talk about some joint work with Richard Nicol, who spoke already on Monday, and my supervisor, Gabriel Pattanine. And I'll start with... Oh. Oh, this works. Okay. I'll start with two overview slides just to give you a general idea sort of of the things we're doing in our research groups. And I'm also talking about extract transforms. For me, this just means some inverse. Just means some inverse problem where the forward operator G, as it is often called, is given in terms of some integration problem. So it can be just integrating along lines or along GD6, or more generally, solving some ordinary differential equation along lines. And this is typically the same as solving a PDE of transport type. And there are some examples of this that you're familiar with, like the Radon transform or SPECT, which are linear maps. And then in the talk here, I'm And then in the talk here, I'm going to focus also on non-linear X-ray transforms. And there are some examples that are called the non-abelian X-ray transforms, or PNT stands for polarimetric neutron tomography. And I will explain in more detail later what exactly I mean by this. So especially in the non-linear setting, of course, we know sort of there's the challenge that there is simply no explicit inversion formula. So instead, we try to tackle some other questions from a theoretical perspective. Questions from a theoretical perspective. And the ones to start with are obviously: you know, is the forward map G injective? Then afterwards, we ask whether we can establish some sort of stability properties. And then yet another question would be, can we say something about the range of the forward map G? And stability and range is what I want to talk about in this talk. And then, of course, if you take a bit more of a statistical point of view, as in Richard's talk, for example, on Monday, you ask about the performance of statistical algorithms. Of statistical algorithms, for example, about their consistency or computational complexity or uncertainty quantification. And all these statistical properties are typically very closely connected to the underlying analytical properties of the forward map gene. And so here's some contributions of people in Cambridge and also for Soi Monan. These are just very recent results from the last couple of years on the class of non-linear expert transforms. X-ray transforms, these non-abelian X-ray transforms. And in the talk here, I want to focus on two papers, the ones that I've highlighted here at the bottom: one with Richard Nickel on computational complexity, and one with Gabriel Patternine that has a bit obscure geometric title on the Ocker-Crowd principle, but essentially focuses on the range of the non-abelian excel transform. And so, here's an outline of the talk. And so, here's an outline of the talk. I'll first only talk about linear extra transforms and present some very classical results on range characterizations. So, these are all well known and there's nothing new. This is just to give you sort of an idea of what we mean by range characterizations and what we could possibly hope for in the nonlinear case. Then, in the second instance, I will introduce the class of non-linear problems that we're interested in. So, these are non-abelian X-rays or polarimetric neutron tomography. And then And then three or four is sort of talking about our results, and three is going to be about a non-linear range characterization, and four about local curvature estimates for the likelihood and an associated initialization problem that we run into. Okay, so part one is just linear setting. So we look at the actual transform here just in a two-dimensional setting. So the question is whether we can. The question is whether we can recover a function f given integrals along lines or along gd6. And we assume here that we could be in a geometric setting, so mg is some Riemanni manifold, but we only consider some special cases, namely A, that we're just on the whole plane, B, that we're on the flat unit disk, and then GDC are just straight lines. And then the bit more geometric picture would be that Mg is what we call a simple surface. Simple surface. So, this, if you're not familiar with it, you should just think of as sort of a slight perturbation of the disk where GD6s are like slightly different from lines. The formal definition would be, I've written it down here, that the boundary is strictly convex. GD6 always reach the boundary, so they don't get trapped inside, and there is no conjugate points, which you can think of as GD6 only intersect once morally. Okay, so what question are we trying to answer? It's sort of the rate. It's sort of the range question or range characterization. So assume you have some function q here that depends on lines or in GD6. And then I ask you, okay, could this possibly have arisen as measurement data in this actual procedure? Is this in the range of this operator I? And so, for example, in a smooth setting where you assume that we take a sort of a smooth function on m, then we ask, okay, what is the range of this linear map here? f goes to if. And I'm using And I'm using G here sort of just temporarily as a space of GD6. This is always going to be a two-dimensional manifold, but sort of in the next slides, we're always going to fix a specific parametrization. And so in the next few slides, I'm going to present three possible answers to this question in the linear setting. And the first one is very classical and goes back to Hergers and Ludwig. And that's an answer in sort of the setting where we're on the whole plane. So Mg is just all of R2. So, Mg is just all of R2, and the GD6 are straight lines. And then typically, you parametrize your GD6, which are just lines in that setting, in sort of a parallel beam parametrization. So that means in order to specify a line, you pick some unit direction omega, and then you can sort of shift lines back and forth, sort of that are perpendicular to omega with some parameter s, as in the picture here, such that the X-ray transform is then a function of omega and s. Transform is then a function of omega and s. And then we have this theorem which goes back to Galfan Grafe in 1960 and Helge and Duttwich in 1964. And I've heard you the following, namely, if you have some sort of candidate function q, so something that lives in sort of the data space. And so here this is just a Schwarz class, so we assume smoothness plus appropriate decay. Then we say, well, this candidate function actually lies in the range of the X-ray transform. The range of the extra transform if and only if two conditions are satisfied. What are those two conditions? The first one is just some sort of symmetry condition, and that just arises because we sort of overparametrized the space of lines and that we go in both directions. And of course, the integral in both directions is the same. So that's what we have to assume right away. And then the second one is that contains sort of the actual like non-trivial content. And it says, well, you know, given Q, you can compute these moment functions. Moment functions. So you take the moments in the s variable, and then you have a function depending on omega or on sort of its components omega one, omega two. And if this happens to extend to a homogenous polynomial of degree m well, then we know we're in the range. Yeah. So that's the like very classical Helgerson-Ludwig answer to this question. Then next, there is another characterization that is a bit more recent and goes. Characterization that is a bit more recent and goes back to Pastof and Uman. And that holds true in quite some geometric generality. Namely, here Mg can be an arbitrary simple surface. So the GD6 don't have to be lines, they can be slightly different. And in this case, we like to parametrize that GD6 in this fan beam parametrization here. So, what does it mean? In order to specify a GD6 gamma, as drawn in the picture here, we just give its The picture here, we just give its starting point x and its starting velocity v. And so the collection of all possible starting dates we name D plus SM, and this is called the influx boundary. So that's the set of all points, X V is such that X is in the boundary, and V is in what's pointing. And in that case, you get GD along which you can integrate. And then your X-ray transform is a function on this influx boundary. Flux boundary. And then the theorem from 2004 also gives a characterization of what it means to line the range, namely, starting again with a candidate function, this time on the influx boundary, because we choose a different parameterization. We know that Q lies in the range of the X vote transform if and only if, again, two conditions are satisfied. The first one is a symmetry condition. It's again because we parametrized 2D6 for both directions. Uh, for both directions, and the second one is the interesting one. Um, and here we say, Well, Q has to lie in the range of what they call a boundary operator, so that's some linear operator P that has a domain dp lying in this smooth functions on the influx boundary. And boundary operator essentially means that you can compute it without ever having to leave the boundary, so you don't need to know anything about the interior structure of M. The interior structure of M, you can just make computations at the boundary. And there's a very explicit formulation of P, it's given in terms of three other operators: this adjoint of A minus, then H minus and A plus. And A plus and A minus star, they're quite harmless. They sort of tell you how to extend or, in a sense, restrict functions between D plus S M. So that's why velocities are sort of restricted to a half circle to DSM, which means that you have velocities sort of running in the whole circle. Velocities sort of running in the whole circle, and they're just a sort of bookkeeping operator, so there's not so much interesting happening here. The interesting part is happening here: namely, H is essentially the Hilbert transform, and the subscript minus just stands for taking the odd part of it. Yes. And then there's a third answer that is very concrete and that you can do if you have a special geometry. You can do if you have a special geometry. So, if Mg is a simple disk with constant curvature, then there are some very concrete answers that François Menard found and then later sort of generalized together with Michael in 21. And it says, well, a function lies in the range if and only if one, again, symmetry is satisfied, and second, Q is killed by some other boundary operator C. And this other boundary operator looks very similar to this P. The only difference is that here we change. Difference is that here we change the way we extend functions, so we go from a plus to a minus. And so, what's the merit of that? Using the C, you can actually construct, or you can find that the projection onto the range, the auto-orthogonal projection onto the range, is also given in terms of a Hibbert transform. So you can use this operator to construct this projection if you want. And here, of course, things are very special. We're in constant curvature. And then, and so that's what Mona Michael did. And so that's what Mona Michael did. They constructed explicit singular value decompositions of this boundary operator P and also used this then to construct the boundary operator C. And then also as a side note, if you're in a flat case where Helges and Ludwig becomes available, then you can actually check that these Helges and Ludwig conditions can be viewed as orthogonality conditions in this SVD basis. So that means that all the three conditions that we've just, so all the three range characters. Three range characterizations we have just seen are essentially equivalent. Okay, so as a sort of an overview or a summary from a sort of meta point of view, we always want to characterize within some big ambient data space that I call Q here. And so these are all little Q's, all possible candidates of something in the range that have the right sort of symmetry. And within this, we want to characterize the range R. That's in this case. R. That's in this case going to be linear subspace because R is a linear transform. And then we saw three different characterizations. The first one was over the moment conditions. And that you can essentially think of as some sort of spanning set for the orthogonal complement. Then the second characterization was this boundary operator, which essentially gives you some sort of parameterization of the range over the boundary. And then the third was an explicit orthogonal projection onto the range. Onto the range. So, why is this useful? I think sort of classical things that are mentioned in the context of range characterizations is that you can check for data consistency. You can check whether some data can actually arise through a certain transform, or you could project noisy data onto the range. And we'll see later that there's some hope that we can use this also in the statistical context for initialization, but that I will talk about later. And so, what we're interested in is whether we And so, what we're interested in is whether we can pass to a non-linear setting. So, here's some little cartoon that's, of course, grossly oversimplifying what's happening. But so, in the linear case, we just have some linear subspace, and there's all these options. We can characterize things by their orthogonal complement, or we can project onto it quite easily. But there's also this parameterization, and then in the nonlinear case, of course, not all of those conditions here even make sense, and it's a lot. Even makes sense and it's a lot harder. And in general, for non-linear mass problems, it's not well understood how to characterize the range or what to do with it. There's one exception for the Calderon problem in two dimensions. So there's some results by Sher Futinoff in 2011 that sort of cleverly uses the River mapping theorem. But for other inverse problems, it's not so much is known. And sort of the upshot of the work is that we can get some sort of pestof-Uman type characterization, so some sort of parameter. Characterization, so some sort of parametrization of the range also for a class of non-linear X-ray transforms. Okay, so and part two, now we'll talk about the kind of X-ray transforms that we're looking at, the non-linear transforms. So this will be a transfer of matrix fields. So we start with some function phi, again defined on our manifold and taking values this time in n. And taking values this time in n times n complex matrices. And it will be put out a function c phi that's again a function on d plus s m, so it depends on gd6 or on the initial conditions of gd6. And it will take values in invertible matrices. How do we define this? So if you're given some phi and you're giving some starting conditions of a GOD sig, what you do is you solve an initial value problem along that GOD sig. And this is just the simplest initial value problem. Is just the simplest initial value problem you can think of. You have a linear ODE where phi appears as potential here, and you start at the identity matrix. So the solution U is then going to be sort of a fundamental system of solutions for this ODE. And you essentially, for any possible initial data, you can find the solutions. And what we record is the end condition of the PDE, sorry, of the ODE. So tau is the point where. So, tau is the point where the GLSI leaves the manifold again, and we said Cφ of XV equal to this fundamental system at the exit time. So then this tells us for any possible initial state, if we evolve along the ODE, what is the corresponding outgoing state? And since we started at the identity matrix here, this is going to be an invertible matrix. And then we look at this nonlinear map here: phi goes to C phi. non-linear map here phi goes to c phi and that's the called the non-abelian x-fit transform um so let's look at some special cases uh the first one is sort of a like a dummy example just to to tell you that it's actually non-linear namely if phi happens to be r valued then of course everything is abelian things commute and it turns out that uh it just reduces to the normal x-ray transform just the exponential of the x-ray transform and of course in this case you can just take logarithms and you're back in the in the linear setting The linear setting. But if you're in a matrix case, this is not possible anymore. And this is not possible exactly because matrices don't compute anymore. And then there's another important special case, namely the unitary case. So if you assume some symmetry on phi, namely that it's adjoint equals its negative, then it turns out that the data C phi will always take values in unitary matrices. And so this is, of course, just a condition of lying in the corresponding V algebra of Un. Corresponding V algebra of Un. And this latter example also shows up in applications, namely in polarimetric neutron tomography. So that's some novel imaging method that I think is not widely used at the moment, but at least physicists have built machines and are making experiments with it. So it's somewhat going in the right direction. And the idea here is that you want to measure the magnetic field inside of some sample by probing it with a neutron beam. It with neutron beams, so you shoot neutrons through it. And these neutrons have some sort of spin vector that changes as they pass through the magnetic field. And by having some clever arrangement of polarizers before and after the sample, they manage to record sort of the whole rotation matrix. So for any possible initial spin direction, they know how the corresponding outgoing spin direction will be. So in this case, the unknown is a magnetic field. So B. And you can put And you can put this into a skew Hermitian matrix just by arranging the components into a matrix like this. And then C, the non-abelian action transform will tell you what the spin rotation is. Yeah, so this is going to be an SO3, actually. It's just going to be a rotation, which is, of course, a subset of U3 of the unitary group. And then the first question, of course, is whether this is an injective map. And this is by now whether. Map. And this is by now well understood. There's a bunch of very quite old results by now, going back to Wertkaim in 91, and then later over Novikov and Eskin. And sort of the most general result that we have by now is due to Paternine, Zalo, and Ullmann. And they solved the bulk of the problem in 2012 under this symmetry condition. So in the unitary case. And then finally, two years ago, they managed to remove this last condition. And now we know it in all cases. In all cases, and it says that if we're on a simple surface, so that's sort of the same geometric generality as in the Pasto-Uman range characterization we saw earlier, then the transform sending phi to C phi is actually injective. Yeah, okay. If there's questions at any point, please just interrupt me. I should have said this earlier. Yeah, so part three now talks about the non-linear range characterization. Non-linear range characterization. And that's sort of our main result of the paper with Gabriel. Namely, now we also can say something about the range. And this theorem here, without reading it in detail, it's essentially saying, oh, there's very much the same characterization as in the linear case, where we have sort of the pastof-Uman point of view and characterize things over a boundary operator. And we have the same sort of geometric generality. So Mg can be any simple surface. So, mg can be any simple surface. And then starting with the candidate function q, so we assume here that it's taking values in unitary matrices. Then, this is the of the form C phi, so it's in the range of the null be an excellent transform for some skew symmetric phi or skew Hermitian phi, if and only if it lies in the range of some boundary operator. So it's the same sort of statement as we've seen in the linear case beforehand. Beforehand. And this boundary operator, I call again P is again a boundary operator because you can compute it at the boundary. You don't have to go to the interior to do anything. And it has a very similar form. It's given in terms of three operators. And this B and A plus, there again, some sort of extension and restriction operators that are sort of bookkeeping tools. And the magic is happening in the middle operator. And this is something we've called the non-linear Hilbert transform. And I will explain on the next slide what exactly it is. On the next slide, what exactly it is. And sort of the main theoretical input we needed to prove the theorem was the existence of so-called matrix holomorphic integrating factors. And that's something that actually had been open for a while and now we managed to prove on simple surfaces. Okay, so this transform H or script H is defined in terms of symmetric Riemann-Hilbert problems. Symmetric Riemann-Hilbert problems. And I will just explain this in one fiber. So later, we're going to have you want to apply this for a function that depends on a point x and some velocity variable v, but everything happens just in the velocity variable. So here we just fix on that. And we assume that we have sort of velocities that live on sort of the complex unit circle. And then if you start with a function f, can be taking values and matrices, could be anything else, you can write down the Fourier expansion. So you have a So you have, you can write in terms of its Fourier modes, fk. These are going to be matrices then. And then we use the following notation. We say f is holomorphic if the negative Fourier modes vanish. Okay, so where does this come from? It's of course just saying that you can extend the function to the disk holomorphic fashion. So it's the boundary value of some holomorphic function. And then further we use the notation here, her n and her n plus for her mission matrices and Hermitian matrices and Hermitian matrices, which are also positive definite. Okay, so this is just the setting. So, what are Riemann-Hilbert problems now? There's two sort of fashions that I want to talk about. The first one is the additive version. So, say you start with a function g on a circle, taking values in Hermitian matrices. Can you decompose it as the sum of a holomorphic function and its adjoint? Yeah, so this is just point-wise taking the adjoint of the matrix. Pointwise taking the joint of the matrix. And this is quite easily possible because since we assume that she is Hermitian, the Fourier mode satisfy the sort of symmetry condition. So taking the joint of the kth Fourier mode gives you the minus kth Fourier mode. And then you can just write down f. It's just where you take all the positive contribution and you take half of the zero Fourier mode. And then if you add to it the adjoint, then you will end up with G again. You will end up with g again. So, this was quite simple. Then you can ask a similar question in the multiplicative setting. So, assume now you have a function g that now is also invertible. So, it's taking values in invertible Hermitian matrices. Can you decompose it as the product of f and f star, where f is holomorphic? And this time you also ask for the inverse to be holomorphic. And this is a lot harder problem, but it's well studied, and there is a solution in the form of Burkhouse factorization theorem. Burkhouse factorization theorem that tells you, yes, this is always possible. And we use this to define our Hilbert transform, namely H is then going to be fiber wise. At least it's going to be map that's taking functions on the circle taking values in Hermitian positive definite matrices. And its output is functions taking values in invertible matrices. And what we do is we just decompose a function according to the Birker factorization. According to the Birker factorization and then send it to its factor f. So this decomposition is not going to be unique. There's some sort of a gauge you have to fix, but once you fix that, this is well defined. Okay, so this is how we define our non-linear Hilbert transform. So we call it like that because it sort of fulfills the same function in the range characterization. In the range characterization, you can, of course, ask whether it's any more related to the standard Hilbert transform. And what you can do is, for example, just take the derivative of h and see what kind of linear operator this is. And then it turns out you can write it in terms of the Hilbert transform. And it's again just the projection onto holomorphic parts. So it's not exactly the Hilbert transform, but it's closely related. Okay. So now I want to talk about these matrix holomorphic intercurring factors, which are sort of the key ingredients to prove the range current. The key ingredients to prove the range characterization. And to define them, we have to solve some equation on the unit tension bundle. Oops. So the unit tension bundle is what we call SM here. So these are tuples of points X together with directions V having unit length. And in all the cases we're interested in, M is just a topological disk. And then this is going to be this disk times a circle. So we have some coordinate x and m and some direction. coordinate x in m and some direction v in s1 and then a holomorphic integrating factor for phi is a smooth function f that's defined on sm so it's a function depending on points and directions taking values in invertible matrices such that three conditions are satisfied so the first one is um you can write it in terms of a transport PDE but here I've just wrote what it actually means it means we have to solve an ODE along every GDSIG and An ODE along every GDSIG. And the ODE is the same we've seen before. It's just the simplest ODE. So dtf plus phif equals zero. And of course, so f is a function that depends on points and directions. And what it means here is we just evaluate it at the gd sig and at the velocity of the gd sig here. So the second condition is we want that f is everywhere invertible. Yeah, so just taking values in the vertical matrices. And the third one is that for every point x, Every point x, if we fix x, then we just have a function on the unit circle, and then we ask f and its inverse to be holomorphic in a sense I've talked about on the last slide. So the negative Fourier modes are supposed to vanish all. So why do we care about them? So this boundary operator P, oh sorry, I think I changed notation earlier, it was some calligraphic P, but that's what I mean. This boundary operator P naturally produces only files that already Produces only phi that already admit holomorphic integral factors. And in fact, it's quite easy to see that P parametrizes the whole range if and only if every phi admits holomorphic integral factors. So that's sort of what we have to prove in order to get the range characterization. And so of course you can ask what's out there already in the case where everything is abelian in the case n equal one. This is a well-known result. That was proved by Nico Salo and Gunter Ulman to the By Nico Salo and Gunter Ulma in 2011, and has been used in many, many instances and is very, very useful and very important. But it's only the scalar case. And the reason why it's easier in the scalar case is essentially because you can get this, you can cheat with the invertibility by taking exponential. So you can just solve. So in general, if you look at the three conditions here, it's quite easy to satisfy any two of them. So if you pick any two of the conditions, it's easy to find a solution, but it's hard to get all three together. Solution, well, it's hard to get all three together. And in the abelian case, you can essentially solve an equation and make sure that the solutions are holomorphic. And then by taking exponentials, you get something that is invertible and still solves an equation. But in the non-abelian case, this will not be true anymore. And in the flat case, there's also been some progress for n larger than one, namely Novikov in 2002 and Eston Ralston in 2004. And Eston Ralston in 2004 proved that there are some, so that's sort of my terminology here: weak holomorphic integrating factors. And what this means is that the F is only going to be continuous, and the equation here is only going to be satisfied in some limiting sense. So they're not honest smooth solutions. And since then, they have been open, you know, whether you can actually solve it in like the, whether you can find smooth holomorphic innovative factors and whether you can do this in sort of more geometric generality. In sort of more geometric generality. And the original motivation behind this was to prove injectivity of the map. So, what happened then in reality was that injectivity on simple surfaces at least was proved using only scalar holomorphic interconnected factors. And what we do now is we prove that holomorphic interconnected factors exist using that. This is injective. So, we're going the other way around now. Okay, so I just said it already. So, now we have a proof. Said it already. So now we have a proof that on simple surfaces, indeed, every matrix fields on M admit holomorphic factors. And the proof is conceptually quite simple. So here's a very brief sketch. There are sort of two important insights that underlie it. The first is that we can reformulate it as transitivity of a certain group action. And the group that we look at is a group containing maps F, as on the previous slide, satisfying. As on the previous slide, satisfying two and three. So let me just quickly go back. So we assume that we are taking values in invertible matrices and that for fixed x, f and its inverse are holomorphic. So this will form a group, okay? And then you can let it act on phi's in a natural way, but you will be taken out of the space of phi's, and you turn out you have some action on some larger space, mo. And I don't want to write down what it is, it's some space of attenuations. So it's essentially phi's that allow to have some direction dependence. Allowed to have some direction dependence as well in a certain way. And if you write down this action, you see that it's equivalent for phi to admit a holomorphic integrating factor that's equivalent to phi lying on the orbit of zero. So in order to prove the theorem, what we have to show is that the action, the group action, is transitive. And so the analysis that you can do, and that's sort of the second insight is, is that if you take this action map A and differentiate it at the identity. And differentiate it at the identity element, then you get a map that is onto, and it has a right inverse that has good, good boundedness properties. It's tame. So, this uses some results that were already known on attenuated X-ray transforms on a micro-local analysis. And it sort of used the same results that also underlie the injectivity proof. So, that's why it's not independent of the injectivity proof. And so, when we were looking at this problem, this was actually what we first found out, and we didn't really know what to do with it. First, found out, and we didn't really know what to do with it. And then it turns out: oh, actually, if you view everything as a group action, that's all you have to prove. Namely, you can introduce the inverse function theorem of Nash and Mosa that tells you, oh, if the action has surjective differentials, then the orbits must be open. And since we're acting on some connected set here, if all the orbits are open, there's only one of them. Yeah, the g-action has to be transitive. If there's only one orbit, it has to be the orbit of zero. And that means that every phi action. Of zero, and that means that every phi actually has a holomorphic indicating factor. So, it all like the hard bit here is really sort of showing that you have holomorphic, sorry, that you have a surjective differential in this setting here. And also as a quick comment, so this is of course a heavy canon, Neshmosa. But at this point, there is not really a way around it because these 10 estimates, they come with a lot of derivatives. So, there's no solar. With a loss of derivative, so there's no Sobolo space in which you can run an inverse function theorem. But we, in this case, really need sort of the Frichi space version. Okay, so that was all I wanted to say about the range characterization for now and holomorphic interhanging factors. And now I'd like to switch gears a little bit and talk a little bit about the paper with Richard on local curvature and the initialization problem. And so here we assume. We assume that we're in a very, well, simple geometry is the wrong thing to say. In a very easy geometry, we just assume that we're on a flat disk and that everything is real-valued because we want to do statistics. And so the question that we are interested in now is sort of the one of whether we can actually compute Bayesian estimators. So what we do here is we discretize. We go to some finite dimensional spaces on which we try to solve the problem. And these finite dimensional spaces I want to call ED here. Ed here. So D is sort of a proxy for the dimension. It's not quite the dimension because we have these matrices here. But ED is defined as the space of all phi's that satisfy sort of the usual symmetry condition. And each of the matrix entries lies in the span of the first D Cernicke polynomials. So Czerningke polynomials show up in the singular value decomposition of the linear X-ray transform. And that's a well-known result, but we use it here for the non-linear transformation. But we use it here for the non-linear case, and we use it even if we're far away from the zero attenuation, where the linearization has a different S V D. And so these Chernica polynomials, they're very classical, and it's known that they sort of form a complete orthonormal set of polynomials on the unit disk. Here's sort of a picture from Wikipedia. And what we do is just we enumerate them, one, two, three, four, and so on. And then we say phi lies in ED if its matrix components lie in the span of the first D. Matrix components lie in the span of the first D polynomials. Okay. So now I want to tell you about two stability results and the meaning in a statistical context. The first one is a Huldar result that goes back to Frozois, Richard and Gabriel from 2019. And it's some conditional Huldar stability result. That's actually much more general than what I've written down here because they prove it actually on simple surfaces. And it's also true in some. Simple surfaces, and it's also true in some infinite-dimensional setting. But if you restrict to ED to these finite-dimensional spaces, then this is what it looks like. Namely, you get Höder stability with exponent gamma for any gamma you like. You can choose gamma as large as you want to one, and you have to pay a price by choosing more restrictive a priori bounds on fine psi. But choosing gamma for any a priori bounds, you get a Herder stability estimate. And then also what they show. And then, also, what they show in this paper is that this stability estimate is a driver of posterior consistency. So, if you try to analyze the performance of basin posterior or the posterior mean, then this is what is responsible for consistency. And there's a second stability result that we proved in the recent paper with Richard. And that tells you a stability of the linearization. So, in this case, So, in this case, we look at the linearization here. So, this is the derivative of the non-Abelian external transform at some phi naught in direction h. And we're looking here at directions, so in like ways to perturb it, only within this finite dimensional set ED. And then on this finite dimensional set, we find that the derivative satisfies some lower bound that degenerates as d goes to infinity. And this sort of is sort of a proxy for the Of a proxy for the ill-posedness in this case. And the conditions we have to put on this phi naught is that it has to have some regularity. So we can go all the way down to H5. So at least it's not smooth. And we have to assume the usual symmetry condition and that its support is compact within the domain. And that we have some a priori bound. And then depending on the a priori bound and the compact support, we get this lower bound. We get this lower bound on the gradient. And so, what we found in this paper is that this gradient stability estimate is what is responsible in the end to get curvature of a corresponding likelihood function. So, it turns out that if you that this would be the leading term of the expected likelihood curvature near phi naught in sort of the correct setting. So, this is then used to prove another result. Been used to prove another result that I will like show you in a second. So, here's just to summarize again in very informal ways what Richard had talked about previously, and I think that also Sven will talk about later. So, if we consider a Bayesian approach to inverting this transform, where phi is assumed to lie in this finite dimensional space, under sort of the usual conditions, as in data arises from noisy observations of C phi, we make n observations and we assume that there is a true underlying phi naught, and we assume there is a Naught, and we assume there is a suitable prior that has been chosen. Then we get the following result. And this is stated only very hand-wavily because Sven will talk about this in much greater generality later. So morally, if n is large enough, then we know that with high probability, the posterior concentrates on a set where its density is lock-concave. So there's some region B. So, there's some region B that's a neighborhood of the projection of the truth to this finite-dimensional set where its density is log concave. And this is great because if, that's a big if, I will talk about this in a second, if we can initialize in this region, then we can actually sample from the posterior with fast, gradient-based methods. And this is sort of the key feature that allows you to get polynomial time sampling results, as what Richard talked about earlier. What Richard talked about earlier, and more you will hear in Sven's talk later. But of course, that's a big if because you have to initialize in that region, that's hard. And the goal is, of course, to find some sort of initializer that you can compute in polynomial time. And this we can't do at the moment, but there is some hope, or there's at least some hope that maybe the range characterization will help. So just going back to this previous picture, in the linear case, Picture in the linear case in this cartoon, we had some sort of projection operator onto the range. And you might speculate that you can do something similar in the nonlinear case. At the moment, we can parameterize the range over the boundary, and that's great, of course, but we don't have this projection. So that would be like a great question whether there is something like that. Can we actually project onto the range at least approximately in some way? And then the next question is: can you use this in some way to actually get a good initializer? A good initial answer. But these are open at the moment, and yeah, we have to see. Okay, that's all I wanted to say. Thanks.