Sorry about that, I had to unmute myself. All right, so this is something kind of completely different than what we just talked about. Hopefully, show off a new imaging method for imaging the middle of the mantle. So I'm a seismology grad student. I actually work with Jeffrey Park, and I'm interested in using SS precursor waves to detect some interfaces in the middle of the mantle. And just a quick talk about. We talk about seismic body waves. Just a quick review for anyone who's not familiar. There's kind of these like P or compressional waves, primary waves, sometimes they're called, that have this compressional motion in the direction of propagation. So there's like pressure just propagating. It's just like the acoustic waves in air. And then we also have these shear waves where you can shear a solid, sometimes called secondary waves. I'm going to call them S waves here. And these are what I'm particularly interested in working. I'm particularly interested in working on. And my kind of application is to understand the middle of the mantle. So the actual SS precursors are these SS phases that bounce off the free surface at bounce points in between a source and receiver. So if we just follow these black ray paths here, first we have an earthquake represented by the star. It sends out energy that bounces off the surface either here or here, depending, you know, you have this wave front, but these are like what the rays look like. What the rays look like, and then it is eventually recorded at these receivers. And there's these SDS phases, as they're called, where D just simply stands for depth, where if you have some sort of interface of depth, you're going to have a reflection as well. And this is really handy because the unfortunate reality is most of Earth's surface is ocean, and we obviously do have ocean bottom seismometers. But they're pretty hard to install, they're hard to recover. Install, they're hard to recover, and they're obviously a lot more cost-associated with them. So, this is kind of a handy way to measure kind of in between a seismic source and a receiver. So, if you have like earthquakes coming off of, you know, you have a bunch of seismic stations in the continental US or Canada, and you have a bunch of earthquakes coming from the, you know, western extent of the Pacific or from very west of here, you get bounces off of the middle of the mantle or in the middle of the mantle in the Pacific. Or in the middle of mantle in the Pacific, and then you are able to observe them on the stations on like continental North America. And these data are, or these data are called SS precursors. This imaging method is called SS precursors because these SDS paths are faster than the SS phase, so they arrive before it. This is just an example of some synthetic seismic data for a specific 1D Earth model. This is what our SS phase looks like. This time here is relative to the SS. Like this time here is relative to the SS phase. And then, you know, in this particular model, there's three interfaces that reflect a significant amount of energy at depth. And these are the SDS phases here. These are just the depths and kilometers of 670, 400, and 220. And these are really useful for imaging the mantle transition zone. And if we look at this data plotted slightly differently with respect to every central distance, so this is like how far away from the seismic source the receiver was in degree. Seismic source of the receiver was in degrees. You're able to really clearly see these phases. This is just amplitude, positive, and negative. And you can see each individual phase really clearly. Now, in real data, in just in the time domain with very little pre-processing, just a little bit of bandpass filtering, you're also able to see these. This is the main SS arrival. And then we have these phases here, which you have to squint a little bit, but they do pop out pretty nicely. They do pop out pretty nicely. And this method is really handy for imaging what's known as the mantle transition zone, which these days is typically considered 410 to 660 kilometers of depth. And just to kind of give the geology reason for studying this is we're really interested in imaging these mid-mantle low velocity zones generated when you have water flowing or hydrous rocks flowing through the solid earth. So rocks can hold a little bit of water. So, rocks can hold a little bit of water in them, and in the mantle transition zone, they can hold a lot of water relative to the rest of the earth. And they make these low velocity zones that will be observed when you have this displacement of material associated either with like this upwelling plume here. This is like hot material coming up of the mantle, or some subducting slabs represented with the gray here. But, and this is like really important for imaging or really important for understanding kind of this long-term cycling of water. Kind of this long-term cycling of water and earth. And we can also just look at the depths of these interfaces, which can vary by a few tens of kilometers, that just can indicate if we have a warm or drier region or a colder or hydrated region. And the SS precursor methods just in time domain stacking have been used for a really long time and are really useful, but they do have some drawbacks, but they are able to give this information pretty well. And one of the big issues is that they have a lot of time domain size. Is that they have a lot of time domain side lobes? So I'm not talking about side lobes like you know when you think about the sensitivities of the kernels and the frequency domain, I'm talking about side lobes in the time domain. So here, this is just an example of some SS precursors calculated by you et al. for just kind of a line of bins of seismic data, these bounce points in the Pacific Ocean. And it does a really great job actually of picking up the 410 discontinuity here, these positive pulses across this line. Positive pulses across this line on the 410 discontinuity. You could see the 660 really clearly as well. It's clear here in this signal, which is just synthetic data. Perhaps there's this mid, this feature in the middle of the transition zone called typically associated with like called the 520. Typically, it's around 520 kilometers depth. And it does a great job of imaging this. But notice how around these positive signals, these unfilled areas are negative signals. And these are these time domain side lobes, and they're quite significant. Side loads, and they're quite significant both around the 410 and the 660. And if we're looking for these low velocity zones, which are actually going to create these negative correlations because they are a reduction of velocity, these side loops can really be complicated. And they are generated partially because they have to use a pretty tight band pass here. And within you have to use the tight band pass, you have relatively limited vertical resolution. We're talking 30-50 kilometers, which is. 30-50 kilometers, which is pretty limiting compared to some other similar methods like receiver functions or some other seismic scattered seismic wave techniques. But in the past, people have successfully actually argued that they do see stuff within the side lobes that indicates these low velocity zones. This is a particular study where they just did time domain stacking, where they essentially looked at the differences in the side lobes and acknowledging that the side lobes around this main. That the side lobes around this main pulse, I mean, the entire pulse is just a summation of all the other signals that are coming in. And here they basically compare in this panel A, they have this kind of typical SS pulse that they've scaled to their stacked data and their stacked data for this SDS phase. The scatterer depth is in the red, and then this dashed black line is their synthetic pulse, which looks very similar to their real stacked SS pulse, scaled SS pulse in blue. And what they note. SS pulse in blue. And what they note here is that they get a much bigger negative than they would expect. And they interpret this as a low velocity zone. These data are from the Pacific Ocean. And then here in B, this is just an example where they don't claim to observe it. So, and they actually don't have the side lobe. And this is actually quite a clever way to do this and is really, really useful and was actually used across most of the Pacific Ocean here. Here is Hawaii for reference, where they have. Hawaii for reference, where they have this more blue, they argue they see them. But, you know, still, there's limited vertical resolution. The side lobes, even though that you can arguably see some stuff in them, you know, this method has some limitations that I think really, you know, could be, you could exploit a more sophisticated method of instead of just stacking these data in the time domain, actually looking at them and performing a frequency domain deconvolution. So, what I've actually done here is start to Done here is start to just apply a multi-taper simply to these data. So, this is just an example of like applying some of these tapers to this data. We find that obviously we have really good spectral leakage qualities, particularly compared to other tapering methods that people use in seismology. And we're able to maintain good sensitivities across the entire region. And we are able to include some damping with some pre-event noise. Some damping with some pre-event noise, and it works quite well. So, just to kind of give an idea of how we do this, we apply a data high pass. So, initially, we do high pass the seismic data, remove some of these low frequencies because there are issues with drift or some other long period noise. We try to maintain as much data or as much signal as possible for a reason that I'm going to show you later where we get some ringing when we have a high pass. But essentially, we then But essentially, we then set these windows via a 1D Earth model, just calculating how long it would take these rays to arrive in an idealized Earth. And so, here, you know, this is an example in this panel A of targeting this 670 kilometer depth feature where it does include some of the signals from, like, for example, this 400. But because we keep the windowing consistent with the main SS pulse, we're able to deconvolve and get a correlation. We're able to deconvolve and get a correlation with this. And hopefully, near zero time, we get a peak here. After, and then we so we apply these tapers. We then go into the frequency domain with the fast Fourier transform. We calculate the spectrum, and then we perform this spectral deconvolution here. This S naught factor is just for noise. These complex conjugates help to stabilize the result over k tapers. Typically, I found that five works really well. And then we are able. And then we are able to stack them via the variance, which is just a function of the coherence and the actual deconvolution. And then we can filter them once they're in the frequency domain. We can filter them. We use a cosine squared filter here. You could really use any filter you wanted, any flavor filter you want, but we find that this works pretty well. And the things that we're thinking about when we think about how to apply these multi-tapers are kind of the very standard multi-taper parameters for just the Celebian tapers. Parameters for just the Celebian tapers, you know, time bandwidth product and basically how we window them, how long they are. We essentially use a window of 150 seconds because that's about as far as we can push it without getting some overlaps with some competing phases that doesn't necessarily destabilize the deconvolution. We actually don't have any divide by zero problems, but we do just have problems of correlations blowing up when you have some specific interfering phases. And but we found that like that the Found that, like, that the tapers typically around p equals four work pretty well with the five tapers. I'm going to show some results in a bit. And we also mess around with the cutoff frequency because ultimately what we want to do is get as high frequency as possible. I mean, these low frequency stacking methods that are just stacked to the time domain where there's no deconvolution applied, you know, people are typically cutting off at around 10 seconds period. In the best case scenario, sometimes lower. A lot of people use 15 seconds. So there's a really poor. So, these are really poorly resolved regions vertically. And we really think that this, you know, with receiver functions, which is a similar method, scattered wavefield method, multi-tabers have been shown to work really well. And so one of the reasons why we really like this p equals four is because over our time window, where we have a good stability and we don't have any issues of any solutions blowing up, is we have this shape that looks pretty good in time domain sensitivity and actually pretty similar to. Sensitivity and actually pretty similar to a cosine taper, like a 20% cosine taper. This works really well at, this is like a taper that works really well in other seismological methods. It's very popular in some of these correlation techniques in seismology. And, you know, we're glad that we're able to maintain these sensitivities and it makes us feel pretty confident in the way we're able to window our data. So we apply this to some of the synthetic data I show and some data recorded on the global seismographic. Recorded on the global seismographic network, which was talked about earlier when Frank Verdum was talking about doing some testing of that post hole data and doing some benchmarking there. And this is just a map of where our bounce points are. So we have all these bounce points in the Central Pacific. We use magnitude six or above earthquakes here. And we wind up with about 7,000 that we can identify clear waveforms on. This is just a visual identification. And we focus on these. And we focus on these bins here, A, B, and C, because they represent kind of the ideal, the average, and the worst-case scenario in this data set. We can definitely add many more data into this. And actually, as I am presenting this, I'm downloading a ton of data right now, hundreds of thousands of seismograms to do a more global study. But this is just a really good way to be able to test the limits of this method in a situation where you have a lot of data and very few data, as well as bench. Very few data, as well as benchmarking with synthetic. So, mainly, what I'm going to be showing is some results from bin A just to show the promise of this method, but I'll also show some data from B and C. And these are our actual deconvolutions. So these are our estimated SS precursors calculated here in this top column for PREM and down here for bin A. And these are for some varying time bandwidth products, varying the high pass frequency, and varying the actual cutoff frequency. And we just have a scale. And we just have a scale here. And we do add some real noise to these synthetics, but still the scale on the synthetics, I just want to know, is much higher than, or a little bit higher than that of the real data, which is to be expected just because of how coherent these signals are. And we find that, you know, pretty consistent across multiple time bandwidth products, even when we go all the way down to one, which is kind of the extreme high spectral leakage case, just because of probably how coherent this data, you're not going to have any issues with the Any issues with the phases in this. And it's actually pretty similar for Bin A, where we have many data, size, actual real seismic data we can include here. With high-pass frequency, we find that like there are some changes. I'm going to talk about this a little bit in a slide that's coming up, some of the pitfalls with this. And just one thing I want to note: this zero frequency result where we have zero being no high pass frequency, having a very low amplitude is actually because of an interfering. Amplitude is actually because of an interfering phase that is. We may have lost you for a second then. You've lost your sound. Oh, he's frozen. He'll come back. Okay. Give him a moment. Yes. The curse of a Zoom meeting. Meeting okay, am I talk? Am I act? Yes, okay, perfect. Sorry about that. Um, so uh, I'm not quite sure where I cut off right here, but right here, okay. So, what I just want to really emphasize from the slide is that we're able to go to these really high cutoff frequencies, so we're able to really resolve a lot of data, so or a lot of signal and keep it, which is really promising about. Which is really promising about this promising for this method because we're able to get all the way up to like 0.25 hertz. This is four seconds period. This is far higher than previous results. And we're actually able to go higher with real data. The reason why I limit it here, cutoff here, is just because our synthetics, these are full waveform synthetics that are very computationally expensive. And this is actually the limit of our resolution within the synthetics we calculate. Now, just to talk a little bit about the high. Now, just to talk a little bit about the high pass problem, we create generated some moving window SS precursors. This is to try to create like a depth picture of the mantle from like the top to the bottom. So, you basically can I am able to calculate these at various depths shown here on this axis. And I just want to note the scale is non-linear, and that's actually because the move out, the relative delay times for the precursors is non-linear. But I didn't want to stretch or squeeze it at all for the sake of preserving the actual signal quality. Preserving the actual signal quality or the signal characteristics here. And this is for some of these PREM synthetics and for this bin A. And we do this for up to 0.25 hertz cutoff. And what I just want to note is that as we increase high pass frequency shown here on the x-axis, we lose what I like to call and what actually Jeff likes to call. I'm not sure if this is common in this time series community at all, but we've just been calling it losing the zero. It's like, you know, I've heard it's kind of similar to like ring. I've heard it's kind of similar to like ringing, like I think Gibbs-free ringing is a term that people use a lot, where we just have this oscillation around a mean. We're able to preserve these large interfaces that are in the data, both in the synthetic and the real data, but we get these oscillations. And obviously, when we're trying to detect something that is sometimes like a side lobe, it's very or sometimes interpreted, could be interpreted as a side lobe or is hidden in the side lobes. We obviously want to minimize this. So, what we do when we do these. Want to minimize this. So, what we do when we do these calculations is we keep the high pass as low as possible just to avoid adding any of this spurious signal to our data. And we find that's very helpful in actually some geologic interpretations as well. And then to try to kind of push the limit of this method, we go all the way up to 0.5 hertz here, shown in the red on this. And these are for real data. These are for bins A, B, and C targeting 410 kilometers. Targeting 410 kilometers depth. So that's the top of that transition zone, and 650, which is the bottom. When we have a high number of data, we actually get really good results for 0.5 hertz. This pulse looks a little shaky. You know, it's not a perfectly clear pulse. It's not perfectly symmetric, which this is the 410. But it actually, in kind of this realm of seismology, this is a pretty clear pulse. I certainly am very happy as an observational seismology with. Happy as like an observational seismology with the signal, I feel confident that that's a good thing that's been resolved well. We find the same thing with the 660, but then as we have fewer number of data, which is here in N, you know, this bin C, we have a lot of oscillations, a lot of unstable nature of the signal. The pulses are splitting. There's a lot of reasons for this. One is there just not being a lot of data. Others are like complicated 3D structure that just can't be resolved by this method. And unfortunately, that's like a limitation of just. Like a limitation of just reality and working with this real data. But of course, we in the future we want to be including a lot more data in these calculations. This is really just to test the limitations. And just to kind of give an overview of what our results look like for kind of general geology, you know, the topography of the mantle transition zone here, we just show the depth of the 410, that's the top of that transition zone, the 660, which is the bottom, and the thickness of the 100 zone. And the thickness of the mantle transition zone, which these are generally used as proxies for temperature. And in general, we find a thinner mantle transition zone, which is pretty consistent with a lot of the results from old school methods and lower frequencies. These particular calculations are at 0.2 Hertz. This is about two to three times the typical frequency content or two to, excuse me, two to three times the vertical resolution of typical methods. We also show the amplitudes here just to get an idea of the correlation values. To get an idea of the correlation values, you know, the actual topography isn't exactly the same as a lot of other studies, but it's kind of consistent within the variation that is found in this region. And just to kind of circle back to my true geologic goal, which is to image this low velocity zone, these are the individual calculations across this line here, eta A prime, eta A prime down here. And these are just targeting 385 kilometers depth. And what we find for these And what we find for these precursors, which are at 0.2 Hz and 0.5 Hertz with uncertainties, is as two sigma uncertainties with these dotted lines, is we find this big low negative pulse above what we would interpret here as the 410. And we think that this is not a side load, and this is actually a true low velocity zone. And this is a similar region where the previous study had identified this low velocity zone, but it's at about an order of magnitude better resolution than they have. Magnitude better resolution than they have. So that's a really promising result and really gives me a lot of hope to use this method in the future. So we kind of want to go global with this study, which is kind of something seismologists like to say all the time. Oh, we want to do it globally, where we download basically as many data as we can. This is just an example of a SS bounce point data set used in a recent global study and potentially use the double P bounce points because the The double P bounce points because these are also like pretty plentiful and just basically using a slightly different data set with a similar multi-taper decompolition method and potentially in the future try to account for some of these complicated finite frequency effects with some of this relative timing and add a little bit more complexity to the method. And just to quickly conclude that applying the multi-taper to this like kind of long used seismic method and a relatively simple application. And a relatively simple application of such has really, really, really still improved this method that's been used for a while. We reduced side lobes, which is just kind of nice to do in theory, and it just makes you feel more confident about your data, but also lets us detect these low velocity zones. We found this low velocity zone out in the central Pacific using this multi-taper correlation method. And even though this was, you know, multi-tapers being used in seismology goes back to Jeffrey Parkin. Back to Jeffrey Park and Frank Vernon, like in the 80s, there's still new places for it to be used and to improve our ability to kind of just image the Earth. And with that, I am done.