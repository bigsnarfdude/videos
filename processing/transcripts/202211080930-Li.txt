Great. Thank you. It's really great to be here. My talk is a little bit not quite fit the theme, but so I'm going to talk about deep learning for predicting biosynthetic gene clusters in bacterial genomes. So I work on the microbiome. I think the next phase of microbiome research is really try to understand the function of the microbiome. Function of the microbiome. So, one way that microbiome can affect human health is through generating secondary metabolites. And the secondary metabolites are the molecules produced by the organisms that are not required for growth per se, but they can provide a very significant advantage of those organisms producing them. So, therefore, it is really important to identify the secondary to learn what. They secondary to learn what type of secondary metabolites those microorganisms can produce and understand their biosynthesis and the role they play. Also, the interaction among the bacteria and their host are also likely to be mediated by those small molecules. So, some really important. Really important secondary metabolites include some of the well-known antibiotics or anti-cancer reagents. And those secondary metabolites really are coded by genes, coded by genes that cluster together in a genetic package. And this is referred to as biosynthetic gene clusters. So, what you see here is an example of four of them. So, those are the four different Them. So those are the four different biosynthetic gene clusters produced by different bacterial species. And you can see those species, they have different number of genes. They have different structure. However, if you look at the function of those genes, you do see some similarities. For example, all those four BGCs, they carry this black colored genes called transport. Genes called transport genes and also the blue color genes. So they have some similarities. And the first paper that was published in literature that try to predict all the BGCs in the bacterial genome was the paper published in 2014 in SAL, where they propose a hidden mark of model approach to identify all those BGCs. So, what are the data? What are the information we have? There is because the BDCs and the secondary metabolites are so important, there is indeed a repository of the currently known and experimentally validated BGCs. So, the database name is MIBBID. And if you go to this database, you type in one of the BGCs, you will see the structure of You will see the structure of the BTCs. The structure of the BTCs, the genes involved in the BGCs, and they are biosynthetic class that this BGC belongs to. And also, all the gene sequence are available from this repository database. And in fact, there And in fact, there is a minimal information required when you deposit those VGCs into that database. So you can find more information about the BGCs, for example, what class they belong to, the number of load size, the operating structure. You can find all those information in there. And you can also find information about the particular compound coded by those BTCs, some BGC, some detailed information in this database. So, just as an example, usually when we try to predict the BGCs, we instead of look at the DNA sequence, we often convert the nucleotide sequence into a string of contiguous p-protein family domains. And here, because there are a lot of talk on this co-production, Of talk on this COVID-2 virus. I just show you this is the predictor of the COVID-2 and the PFAM sequences. So that's the data we're going to convert the DNA sequence into a protein family domain sequence. And the literature, there are some of the really well-known tools to identify the B disease. Probably the most commonly used tool is this anti-SMES. Is this anti-SMAS? IT SMASH is a really powerful computational tool to identify the BTCs. But the limitation of IT-SMASH is they can only detect the BTCs of known types because this is really based on rule-based prediction based on the underlying profile Hidden Markov model. And so this algorithm cannot identify the new BGCs. And I mentioned this. And I mentioned the cluster finder is the algorithm, it really tries to identify new BTCs. It's based on the frequency of observed protein family domain inside the BTCs and outside the BTCs. It's a very simple two-state Hidden Markov model. And the key assumption of those algorithms is that even the biosynthetic pathways for unknown compound family are very different from Family are very different from the known secondary metabolite, but the secondary metabolites they utilize a broad enzyme families, so broad protein family domains for the key reaction. So that's basically the assumption. They share some common essential protein family domains. So again, the goal of our project is to try to identify and Identify and characterize the biosynthetic gene class of all the BGCs in the bacterial genomes that we have complete genome sequence available. So at the time when we did the analysis, we looked at about 5,666 bacterial genomes. So the training data set, because this is really a prediction problem, the training data set we have is that particular MIBIG database. Particular MIBIG database version 1.4, where we have 1,984 BGCs. And as I said, we can identify gene sequence, convert them into protein family domains. So altogether, there are about 3,685 protein family domains, and those VGCs are from 1,094 bacterial species. So those are the true VGCs. The true VGCs that we know in the database. So, because this is we treated this as a prediction classification problem, we also need to create some controls, non-BCCs. So, as a background, we look at this 5,666 reference genomes, and we have their DNA sequence, convert them into protein-family domains. So, for the reference genome, there are about So, for the reference, you know, there are about 11,426 protein-family domains. And so, we have all those domains. Then, we create this pseudo-controls. So basically, look at the size of the known BGCs in terms of number of protein family domain they contains and generate controls of similar size distribution. Just randomly combine the BGCs from the From the genome. So, altogether, we create about 10,000 controls. So, now you have cases which are the known BGC, the protein-family domain sequence, and we have controls. We just want to build a learning algorithm. So, this is my first try of a deep learning algorithm for this type of problem. So, in the literature, there was a paper published in 2019 where they proposed the first. Where they propose the first deep learning algorithm for BCC prediction. But the algorithm, they only use very limited information, the information about the names of the protein family domain, names. So they essentially use this natural language processing learning algorithm to build a predictive model. So what we did, we further extend their algorithm to incorporate more information about those species. Information about those species. So, besides the protein family domain names, we look at PFAM clans, basically, tell us the similarity of some of the protein family domains. And the PFAM function, basically, for each of the protein family domain, there's a description of the function of the PFAM domain. So, there's a function information. So, all together, we encode. We encode all those information into 1126-dimensional numerical vector. So, just to give you some idea about those known BGCs in term of number of the protein family domains. So, this is just a histogram of the known 1984 BGCs. You can see most of the BGCs, they only include a few protein-family domains, about most of DACA 3. About most of that three, a few of them you have, they include about 300 protein family domains, but majority of them are short in term of number of protein family domains. So the control, the artificial control non-BGC we created have similar size distribution. Just a quick clarification. Maybe I missed the controls. The controls, how do you know that they're not VGCs? Good question. It's hard to, we just randomly pick those protein family domain, put them together, create artificial controls. Okay. Not in the order, just randomly pick them, but have similar size distribution like this. Yeah. Yeah. And again, there is another database which Another database which tells us the similarity of some of the protein public domains, the similarity based on sequence, based on structure, based on functional similarity, and similarity of the underlying profile Hedgehog Markov model. So this basically tell us the similarity of some of the protein family domain. It's almost like tell us the meaning, the similar meaning of those words information. Information. So, this is additional information we use. And as I said, without going to too much details, the whole idea of deep learning in handling this type of data is how to embed all those non-numerical information into numerical vector. And so, what are the data? So, what we did is we look at a contiguous segment of protein functions. Segment of protein family domains, right? Because each of the PGC can be summarized as a segment, a sequence of protein family domains. But remember, some are really short. So we just use the so-called zero padding. If they are really short, just do the zero padding. So all the input to the algorithm has the same length in terms of the number of the protein family domains, 256. Domains 256. And the embedding includes the domain names that you use, they use this so-called word to VAC. You convert the protein family domain, each of the protein family domain name into a vector. So that's one information. Another is a PFAM clan function description information. So there's a way you can numerically call. Numerically code those information. And this is, I think, the power of those deep learning algorithms for handling non-numerical input, non-numerical data. So after you set this up, so your input basically are the sequence of protein family domains, 256 of them. And each one of them is represented by 1126-dimensional numerical vector. Dimensional numerical vector. Then you just run the standard so-called long-short term memory recurrence neural network to learn the BGC. And then you use this softmax function to make a prediction. So we studied two type of prediction. One is just to predict whether for each of the protein family domain as an input, you predict whether they belong to BGC, no BGC. So that's a binary prediction. So that's a binary prediction. And another prediction is a multi-class prediction. Basically, you predict the type of BGC class. There's, I think, there's six different type of broad biosynthetic class you can make a prediction. So here, the algorithm-wise, there's nothing new. This is a standard recurring neural network algorithm. So one idea we tried in We tried during this learning this idea of data augmentation. I think it's quite useful. So, the idea is when the sample size is relatively small, in our case, we have about 2,000 protein family domain, we have 2,000 BGCs. So it's a relatively small training data set. So the data augmentation idea is during this learning, during this machine learning, during this iteration, During this iteration, this stochastic gradient desat iteration algorithm. And each of the at each of the iterations, you can replace the input. Our input, remember, is a sequence of protein family domains. You can randomly pick one of them and replace with small probability with another word of the similar meaning. Remember, each of the PFAM, we can potentially identify the other PFAM in the same. The other PFAM in the same clan, then we just randomly replace them. So, sort of you perturb your input a little bit, and that actually also helps to improve the prediction performance. So, that's the data augmentation step. So, let me just show you this embedding actually is quite informative. So, what you see here is this is training in the training data set. In the training data set for those 2000 BGCs. So each dot represents one of the BGCs. What you see here is a TSNI plot based on the learned numerical vectors just prior to output of the class label. So this is learned. This is for the training data set, learned features, and you just plot a teacher. And you just plot a testing plot, and you can clearly see the embedding and the learning can separate the BGCs of different types. Now, this is six different types. The other type, the yellow ones, are the BGCs that may have, may belong to multiple classes. So they sort of spread out a little bit. But all the other BGCs. BGCs, they indeed cluster together quite nicely. But this is a training set. This is learning training set. And if you just look at binary non-BTC, this is for the testing data set. I'm going to explain a little bit next about how we create a testing data set. But again, you look at the learned feature, and you can see the testing data set are separated. Data set are separated from the case, BGC are separated from non-BGCs. So it's the algorithm really learning the difference of the features. So how are we going to evaluate the prediction result? So what we did is, so we have training data set, we create a testing data set. The testing data set are the Are the 13 contexts from nine genomes? And in this testing data set, there are 291 known BGCs that are never used in the treating. And to answer Jesse's question, we cannot just use those and treat all the other areas as non-BTCs because they may have new BTCs, right? So those are the known BTCs. Then we just sort of implant. Then we just sort of implant those known BGCs into the artificial genome. So we create an artificial genome with those known BGCs in the original genome, but replace other part of the genome, replaced with non-BGCs. We create those non-BGCs in the training, the controls. So we just replace the rest of the genome with non-BGCs. Know is non-BTC. So we definitely know some are the BTCs and some are non-BTC, but non-BTCs we assume are not the BTCs, right? Yeah. Okay. So that's how we control, we create the non-BGCs, and we did this 10 times. So this is the artificial genomes. And here's just a comparison. So we can predict the outcome, you can evaluate the prediction. You can evaluate the prediction both at the protein family domain level in terms of BGC, non-BGC, or after you predict for each of the protein family domain, you predict BGC, non-BGC. Eventually, you have to do some post-processing to have the segment of the protein family domain sequence. So that's what I call the PFAM BGC level prediction. I just in general, you can see the first column is the original DBGCs and Original DPDCs, and the next three columns are the method based on our algorithm. The last column is the ones without data augmentation. And this column is the binary prediction result. And what you see here is the AUC F1 score, and particularly for the predicted BGCs, the way we evaluate. BGCs, the way we evaluate the performance is to look at how well the predicted BGCs overlap with the known BGCs, with the true BGCs. And you can see this is the FM score. So in general, we do see a really good improvement over the original BGC by incorporating additional information about the protein family domains and by data augmentation. So, this is for binary prediction. Binary prediction, and this is this is a prediction at the gene level, the similar conclusion. Um, then for the body class prediction, what we did is, so after worse than 1.4, in the worst than 1.5, there are 160 new BGCs that were deposited into the database. And we just want to see whether we can predict those new BTCs. And this is a similar type of plot I showed you for the. for um for those of 160 new BGCs that are separated, but not completely separated. And one of the reason is the training data set for some of the classes, the sample size is quite small. So it's really hard to learn them. And here, again, the first column is the performance for predicting the BTC. Predicting the BTC types. Second column is for the binary prediction, just say BTC or no BTC. And you can see we can achieve a good F1 scores range from 85 to 95. Even the one we predicted, 100% overlap with the true BGCs, we can get F1 score of 85 or 88%. So that's basically the performance of the algorithm. The performance of the algorithm. Then we apply this algorithm to predict all the BGCs in those 5,666 bacterial genome and made 170,000 predicted BGCs. This seems like a lot, but it's really, if you look at the distribution of those predicted BGCs, one class, this is the RIPP class, we made a lot of predictions because that class is. Because that class is very easy to make a prediction. They are very well conserved, they are very similar. So it's quite easy to make a prediction. Even the sample size in the original training data set, we don't have many VGCs that belong to that particular class, but make a lot of predictions. But just in general, a basic summary, those predicted VGCs account for about 10 to 20% of the genomes. And in terms of number of the BGCs, And in terms of the number of the BGCs per genome, most of them get 20, 30 BGCs. In terms of how many genes involved, it's a small number of genes. And what we can do is for each of the five different bacterial phylum, and we look at the bacteria belong to those phylum, and we can summarize what type of BGCs those bacteria. Those bacteria in a particular phylum can generate. And this can be useful when we study, for example, the function of the microbiome communities. So that's basically what we did. If you ask me now, how are you sure about this 1070,000 prediction? I think it's really hard to verify those. To verify those. But I just want to mention this number is not huge. Some other algorithms predict even more of the BDCs. So one question our group is working on now is we try to build a similar algorithm to make, to discover more BTCs based on shotgun metagenomic sequencing data, not just limited to those 5,666 bacteria with a known sequence because With a known sequence because there are a lot of new bacterias potentially in the human microbial communities, and we want to see whether we can identify more BGCs. So just to summarize, what I present is just a simple deep learning algorithm. We call this extension of the BGC, which can be used for multi-class prediction. And we observe better performance than the deep BGC. BTC. And we now have a database, basically to a database with all the BGC predicted for each of the species. And the result can be used to quantify the BGCs and BGP relative abundance based on species abundance. So that's all. Thank you. Hello, hello. Thank you, Hungji. I'm interested in kind of out-of-distribution predictions with these deep learning approaches, and particularly. Approaches and particularly finding new classes. If you could go back a couple of slides. Yeah, like that one. Okay, so you're finding more elements within these known classes, but I'm wondering, for example, if you took the terpenes out, you didn't have them in your training set at all, would you be able to predict BGCs that are outside of the kind of classes of BGCs that you've seen? And also, then once you've verified whether it can do that. Once you've verified whether it can do that, do you see any other classes that you can you use this to predict new types? Ah, that's a great suggestion. That's a great suggestion. No, we haven't done that. That's good. Basically, you take one out in the training and see whether you can find something that's okay. Because sometimes it's these deep learning approaches, they're finding just very specific things about similar, similar to your training. That's right, that's right. But I'm sure there's other information in there. I'm sure there was other information in there, and you could even make it a requirement that it be able to perform well at that point. That's a really excellent suggestion. Yeah, I'll try that. Yes. Thank you. Thank you. Any other questions from here in Oaxaca? Is there anything on the line? Yep, sure. Yeah, it's one, it's one to ten. So sure, I'll ask it. So I guess you gave us a bit of a teaser about your work on metagenomic sequences. So I'm curious, what's the average? The average length of a PFAM domain and relative to like a short are you working for metagenomic short reads? And can you get, you know, can you get a PFAM domain from a short read? So basically, what's the size? That one is not. Yeah, that's right. For this very short read, we are not. Very short read, we are not trying to convert them into the PFAM domain as an input. Yeah, we need to look at a DNA sequence and maybe at the protein sequence level, but not the P5 domain level. Yeah, yeah. Yeah, general assembly, but the choice to use PFAN domains is sort of a natural unit. I mean, as a unit, like, what if you just use it? DNA sequence or something like that, and maybe stuff through the other methods too. Most of the methods actually base on PFAM domains. The hidden market model I mentioned and another deep learning algorithm, they are based on the PFAM domain sequence. So, good. All right. Thank you. Okay, thanks, Anji. 