The same approach as Hans already introduced. So, I want to start with an application that we did. So, this is with Real Motorizations, and it's in the back then operational setup of the German Weather Service. So, back then it was still COSMO, now they've replaced it with ICAN, and with the Kenda system, which is based on the LXPF, so also an ensemble common filter type algorithm. And we estimated the roughness length. The roughness length. Roughness length is a two-dimensional parameter which accounts for sub-real scale ruggly and land use. So we can see here our domain, so it's Germany and surrounding countries, and we can clearly see that the elves here are highlighted. And then we do our parameter estimation for two weeks. So we update hourly with observations as it's done operationally. And this is what we get for the roughness length after two weeks of parameter estimation. So we can see that the features are mostly highlighted. But we have to be very careful when we do parameter estimation with real observations because there will be lots of model error, both known and unknown model error. And therefore, we cannot really assume that this representation of the roughness length is a better approximation. Is a better approximation than the default value here. Because we project all the model error that is caught between the observations that we have and the parameter, all that model error that is caught in between will be projected onto the parameter and the parameter will compensate for it. So this parameter will correct for, in this case, the surface flux flow. And here I have plotted in the solid lines the spatially averaged parameters. Averaged parameter increments as a function of time. And we can clearly see the diurnal cycle. This diurnal cycle clearly also follows the momentum flux, which is in the black delta plank. So we have now a roughness length, which is basically flow-dependent, because it compensates for model error in the surface cluster. So we have to be careful with what the goal is when we look parameter estimation in a real setup. And in our case, In a real setup. And in our case, we're interested, as Tiana already mentioned before, we're interested in convective scale, so in short-term forecasts of up to six hours of clouds and precipitation. So let's have a look at here at six-hour forecast. So here's a verification against visible satellite images. So we're looking at clouds, and this is just a snapshot. So we have our observations here. So we have our observations here, our reference simulation where we don't do parameter estimation, and here where we do estimate the roughness length. And I've highlighted like this region here where you can see a difference, where there are some clouds in the observations. Not in our reference simulations, but we do have it when we estimate the roughness line. So we can also see that in the ensemble representation. So here is a little bit more of a quantitative result. So here I've plotted the fractions. So here I've plotted the fraction scale score. So fraction scale score, we often use that to verify things like precipitation and reflectance to avoid the double penalty feature. So basically you specify a certain scale and a certain threshold and then you look at how many grid points within that scale exceed the threshold of the quantity of interest. And then you compare that to the observations. And so in this case, I showed a I showed the FSS score averaged over 60 forecasts, but in a relative sense, so it's like an improvement with respect to the reference simulation where we don't estimate the parameter. So red is an improvement, and blue is that we're making it worse. And as you can see, when we divide our domain into the northern part and the southern part, you see that we do very well in the northern part. Very well in the northern part and bad in the southern part. And I was very puzzled at first, but then I looked at what observations were actually assimilated. So this is a plot of all the wind observations that were assimilated in this two-week period. And the blue dots are surface wind measurements. And we can see that barely any surface wind measurements were assimilated in the south. Probably they were rejected by the quality. Probably they were rejected by the quality control because of their water fee. But in any case, this explains why we're doing so good in the north and so bad in the south. So it's really important to have the good observations that constrain your parameter good enough to get good results. So what have we seen from this application? So first of all, that well, we have to be careful because the parameter will compensate for any one error. Parameter will compensate for any model error, so you have to be very realistic about what your goals are. We've seen that in this case, estimating the roughness lengths can improve short-term forecasts of clouds. Also, precipitation didn't show this, but for precipitation we get similarly good results, even up to twenty percent improvement at forecast hour three, but only when we have enough surface width measurements. Surface width measurements. So it's really important to very well constrain the parameters, as you're well aware. So, how can we better constrain the parameters? So, of course, you can increase your observational coverage, but unfortunately, we usually cannot do that. We're limited. So, instead, we can think of ways to use the observations that we have more effectively. For example, by reducing sampling errors. So, again, increasing the ensemble size. So again, increasing the ensemble size is something that we can usually not afford, but we can look at smart ways of doing localization or reduce the degrees of freedom by introducing some correlation length scale. I believe Ansel also did that, and I also tried that with the Rothnet length, and it indeed improved the results in the southern part of the domain. But I want to focus for the rest of this talk on choosing data sets. Choosing data assimilation that can alleviate the Gaussian assumption that the ensemble-common filter has. Because we know that at convective scales, what we're interested in, and also specifically for parameter estimation, we're dealing with non-Gaussian error statistics. So, there's many algorithms out there that attempt to deal with non-Gaussian error distributions. I have looked into the I have looked into the productive filter, which was introduced by Daniel Rodis in 2012. And the idea is that instead of what the Ensemble Dammot filter does, assume that our analysis is a linear combination of the innovation, or the innovation is the observations minus the background. Instead of assuming a linear relation, it assumes a quadratic relation, like here. Here. But pay attention that this is now an element-wise multiplication instead of the cross-product. So, what the reason that Daniel Holis did that is to make everything computationally feasible. So, in effect, what this does is that you are ignoring cross-skewness and kurtosis of between three or more variables. So, you only More variables. So you only take into account these higher-order statistics of at most two different state elements. And when we plug this into the best linear unbiased estimate, we get this expression for the quadratic filter, which is very similar to the ensemble-Calm filter equation, except that we have that our matrices are twice as large, and they now contain also information about the skewness and kiltosis of the. Skewness and the kurtosis of the error statistics, both for the observations and the background. So I've applied this quadratic filter to the modified shadow water model. This is the toy model that Tiana introduced earlier that we've been working with a lot. And what I've done here is a parameter estimation with the quarantic filter. And here I've plotted just the results. And here I've plotted just the results for the rain variable. And we've simultaneously estimated three parameters. And here I've chosen to show just one, but they all behave fairly similarly. And we have the RMC as a function of ensemble size, where in blue we have the ensemble count filter, in green we have the quadratic filter, and then this colour that I don't know what it's called is where we Is where we apply the ensemble common filter for the state and the quadratic filter for the parameters only. And we can see for the state that the quarrel filter is way more sensitive to the ensemble size than the ensemble comment filter, which makes sense because we're using higher order statistics, so we are more prone to sampling errors. So we need a certain ensemble size to beat the ensemble count filter. The ensemble count filter. And in this case, it's about 100 members. But when we apply the ensemble count filter for the state and the corrupted filter only for the parameters, this sensitivity increases and already for small ensemble sizes we're doing better. And also for the corruption, we're doing better. And this is definitely a feasible option to do in real cases. It's not that expensive. But But we may be able to do better. So, this is very recent work that we did together with our colleagues in Mainz, Bettina Mieber and Maria Pachola. Paper here is on the review. And we've been looking at polynomial chaos. And this was also already introduced yesterday. So the polynomial chaos method that we're using is the stochastic. That we're using is the stochastic lurking. So, where you basically, instead of representing the uncertainty through an ensemble, we represent the uncertainty within the model itself. So, we end up with a stochastic set of partial differential equations. And then we can use the stochastic alerting to solve these. So, basically, we have our uncertain variables and/or parameters. Here, I call them theta, and we have them depend on some stochastic. And we have them depend on some stochastic variable omega, which in this case is a normal distribution. And then we approximate these uncertain variables with the polynomial expansion with Hermite polynomials, because the Hermite polynomials are orthogonal to the Gaussian Pf. Then we plug this approximation into our original deterministic model and so we And so we get a stochastic model. But then we can use the weak formulation and this quartographinality property to derive a deterministic set of partial differential equations to solve for these coefficients of the expansion. And so once we have the coefficients of this expansion, we have the full PDF, right? So we have the entire distribution and we can Distribution, and we can even also, if we like, draw a huge ensemble by sampling from omega. So we can do whatever we want with it. So our colleagues, Bettina and Maria, have implemented the stochastic lurking for a cloud model, a two-dimensional cloud model, which they couple to a dynamical core. And here I briefly show some results that we have in our paper that is on the review. So on the left, there are some So, on the left, there are some convergence properties. So, here I don't do any data simulation, but we perturb the initial water vapor field by 20%. So, in this case, we have a fully correlated initial error, such that the degrees of freedom in the stochastic space is just one. And then we do a forecast both with an ensemble and with the stochastic version. Stochastic version. So our ensemble is of size 100,000, so enough to represent the true distributions as converged. And for the stochastic Bellurkin, we truncate at 10 polynomials. And then we look at the convergence. So we look at the error with respect to these, to the converged solution, as a function of ensemble size for the ensemble. Of ensemble size for the ensemble and as a function of truncation for the stochastic Galurkian. And we see in the solid lines the ensemble and in the dotted lines the stochastic Galurkian for the mean and the standard deviation. And the stochastic Galurkian easily beats the ensemble. So already for a very small truncation, so heavily truncated polynomial expansion. Polynomial expansion, we beat ensemble sizes of thousands. At least in this case, we've also seen cases where the convergence is a bit worse for the stochastic lurking, but it solidly is better than the ensemble. Then in this work, we also coupled the stochastic Gallurkin to data simulation, in this case specifically the ensemble count filter. So instead of running an ensemble, we run a stochastic An ensemble, we run a stochastic alert simulation, and then we use these coefficients of the stochastic alert simulation to get our backward error statistics. And that's how we get the analysis. And then we did a twin experiment with this model. We compared it to the Ensemble Common filter. So for the stochastic alurkin, we truncated at six polynomials. Six polynomials, and we compared it to an ensemble-Columb filter with 10 and 50 members. And then we can see that the stochastic Lurkian is doing much better. Now, as I already said, our initial error here is only in the water vapor field. And the error is also fully correlated. So, we have only one degree of freedom in the stochastic. Degree of freedom in the stochastic space, which is of course not realistic for real applications. So the idea that we have is we want to use this combination of the stochastic Lurkian with data assimilation for parameter estimation. And the beauty is that since we have the full PDF available and we can cheaply generate a huge ensemble if we want, we are not necessarily restricted to using the ensemble code. Restricted to using the ensemble-Cambo filter. We could use the corrective filter or maybe even a particle filter. So, this is the idea that we have to how to do the parameter estimation with the stochastic Delurkin data simulation hybrid in like an operational setup. So, in our case, what we eventually want to do is estimate cloud parameters in ICAN. So So here we have in black just the regular weather prediction system. So we have our background ensemble, we have our observations, we plug it into CENDA, which is using the LTTF, we get our analysis ensemble, plug that back into ICAN or the American weather prediction model, etc. etc. But now instead of using a deterministic microphysics schemes, we replace Physics teams, we replace this with a stochastic version of a microphysics scheme. And this stochastic Bellurkin version will represent both the parameter that we want to estimate and the cloud parameters as stochastic. And to make sure that the degrees of freedom in the stochastic space are small for the stochastic blurking, in this case we want to estimate the global parameter. So that would be just the degree one. Would be just a degree one, but we should be able to increase it to at least order ten if we want, but we'll start with one. So when at each assimilation cycle, we re-initialize our stochastic lurking simulation with only the parameter as initially uncertain. And for the cloud variables and the coupled deterministic dynamical core variables, we take the We take the analysis on some who mean as given by the LET gap. That's this arrow over here. And then we run the stochastic Galurkian in parallel to the regular ensemble. Then we get our background PDF. Now we have now PDF for both the parameter but also the cloud variables because the cloud variables are modelled as stochastic, so the initial uncertainty of the parameter will have. Initial uncertainty of the parameter will have propagated onto the cloud variables. So we have the full error statistics of the cloud variables and the parameter. And then we have, for example, radar satellite observations that we can combine with this full error statistics and we can do any data simulation algorithm that we choose to get the analysis PDF for our parameter. And then we use every time the expected value. Every time the expected value of this parameter PDF in our ensemble of the regular system. And that's what we plan to do now in the future. So we haven't tried it out yet, but it seems like a cool idea and maybe also applicable for some of you guys. So in the summary. So we've seen that estimating the roughness length improves the short. Length improves the short can improve the short-term forecast of clouds and precipitation, but the parameter needs to be sufficiently constrained to be successful. And we've also seen that we can better constrain the parameters by reducing sampling errors or use higher order statistical moments to calculate the analysis with the productive filter. And we have the idea to use the stochastic lurking. To use a stochastic lurking instead of an ensemble to obtain accurate pure error statistics, and this hopefully may open the door to use particle filters to do the parameter estimation. So, here are the references of the papers I've talked about. Thank you. Concerning your focus on constraints, it also comprises the presentation from Jancic. Why are you so keen to constrain a system like clouds, which are itself not constrained in some ways? In some respect or other respect. This is an attempt to compress my question in a single sentence. Okay, so first of all, I don't think I've tried to... So in my presentation, I haven't tried to constrain anything. No, in your conclusion, there was both constraints. No, no, I mean like constraining the parameters, like in terms of identifiability of parameters. So I just mean that we need to have sufficient observations that are completely correct. That are completely correlated to the but maybe you can ask the question to Tiana because I think that's I unfortunately didn't hear it. So nobody can repeat it. Or Ivan, can you repeat it? I can hear you very well. Yeah, I try to compress my thoughts in a single sentence. So why are you so keen to constrain a quantity like The quantity like mass of clouds or especially conductive clouds, which are itself by their dynamics, the environment not constrained in many respects. So basically the idea here is just to get rid of the artifacts which data simulation is producing. So this is something where we have seen that we can Have seen that we can improve on the classical results. So, basically, let's say with the Ansemo-Kahn filter, that they somehow within our settings they reach a plateau, so there is not any more reduction of errors if we do nothing. And if we add this physical constraints, so we have now added it in several different Added it in several different toy models, and also we have added now in this little bit more complicated so idolized radar setup as well. We see that this actually improves the analysis and the forecasts. So it's about removing the artifacts of the analysis schemes. Well just a quick question. Uh it's very nice to see. Uh I was wondering about the assimilation window. Do you use any type of smoothing over time? Or is it just a filter currently? No, so for the for the application of the roughness length we did not do anything antsy there. So it's just hourly just exactly the same as the state. Because you would expect maybe that to be quite constant or high, right? There are strengths. Yeah yeah exactly, but so that's the point. So it's not But so that's the point. So it's not just so in retrospect, maybe we should not have estimated the roughness length. In retrospect, we should have directly maybe have estimated the surface boxes. Because that's the point. I mean, with the model error in the surface boxes are projected onto the parameter, so we get this flow-dependent parameter. Yeah, that's possible, but we actually embraced it. But we actually embrace this because we are interested in short-term forecasts, and we've seen that this is still very beneficial in that case. But yeah, you're right, so if we would be interested in longer-term forecasts, then we would have to do something like this, because otherwise we'll probably have big biases. Okay, great. Thank you so much for both of you. Good time, again. Yeah, so now uh let's move on a bit away from uh parameter estimation and then come back to machine learning. So yesterday we didn't have time for our discussion, so now it's uh it's a discussion. We we stay here in this room uh in such a way that the online uh online people can