Yesterday, but happy to still have the opportunity to talk to all of you. So, I'm going to talk a little bit today about, I think the things that I'll be in sort of a theme is quite similar to some of the stuff that Amanda was talking about, but I think the specific concerns that I'll be addressing are, in a sense, different. I think I'm going to assume away a lot of the complexity that Amanda is trying to deal with, but we'll see what sort of payoff we can get from doing that. So, I'm going to talk about trying to integrate human expertise with algorithmic. Expertise with algorithmic forecasting and algorithmic prediction. This is joint work with Rohan and Devavrat, who are at MIT, and then Lauren Lane, Derek Lee, and Dennis Shang, who are at the Yale School of Medicine. And I'm going to talk to you about a canonical scenario, which is going to be a running example throughout this whole talk, where a patient shows up, in our case, to an emergency room. And there's a doctor that examines this patient, right? Gets to see, talk to the patient, get some characteristics about them. Patient, you get some characteristics about them. And there's also an algorithm running somewhere that looks at this patient's electronic health record and looks at certain particular features and forms a prediction of risk. And somehow jointly, these two pieces of information, the doctor's intuition or expert judgment and the algorithmic risk prediction, have to come together to make a decision. In our case, the decision is: should this patient go home or should they be admitted to the ER? They be admitted to the ER? Again, I'll tell you exactly more about what that setting looks like. And so, right off the bat, it's not exactly clear how you should make this type of decision. You get a statistical risk estimate from an algorithm, and then you have a doctor who looks at the patient and says, I have some intuition. I know I've seen tons of patients before. I have access to information that perhaps the algorithm does not have. How should we combine these two sources of information? That's really going to be our focus for today. Now, there's Now, there's a few sort of different lines of work trying to think about this problem. There's, you know, we might think that maybe the doctor should just do whatever the algorithm recommends, particularly if the algorithm ends up being statistically more accurate in some cases than the doctor. There's some evidence actually that this happens both in our setting and in the sort of medical clinical decision-making settings more broadly. But there's also some amounts of intuition you might have perhaps that doctors should override these types of algorithmic. Should override these types of algorithmic recommendations when they feel confident about that they have some other information that's decision relevant. And they shouldn't feel bound to following these algorithmic recommendations. And in some sense, the reason that we want to still have an expert clinical decision maker in charge of the final decision is because we don't trust these algorithms to really meet our goals and especially to be sensitive to the particular case of an individual. And so the key questions that we're going to be asking in the setting is: how do we even know if Dr. Is how do we even know if doctors have useful information that our algorithms do not? So, how can we test for the presence of that useful information? And if it's the case that doctors, in fact, do have information that algorithms do not, how should we update statistical risk predictions using some sort of forecast from a human or some sort of feedback from a person on whether that clinical risk prediction from an algorithm is too high, too low, is omitting some key feature. There's a lot of flexibility we have to try to. There's a lot of flexibility we have to try to combine these things together. And so, what I'm going to talk about today is one particular framework that we've been developing on how to do this integration. Now, I mentioned that we're going to be thinking about emergency room decisions. In particular, we're going to be talking about gastrointestinal emergency room admissions. And so, a patient arrives at a hospital with a GI problem, and the doctor basically has to decide whether to keep them in the ER or to send them home and then schedule a follow-up later, right? Schedule a follow-up later. And the primary thing that you're looking at here from the doctor's perspective is: is there a risk of some complication if they go home that we would want them to be in the hospital for if that happens? So GI bleeding is kind of the number one thing that people are worried about here. And the standard algorithm, the state of the art, or the sort of standard of practice or standard of care is to use what's known as the GBS or the Glasgow-Blatchford bleeding score. I've actually, MD-Calc was mentioned earlier. MD-Calc was mentioned earlier. I've actually pulled up this screenshot from MD-Calc on the right. You can just see you put in certain characteristics of the patient and you get back a score. I believe it's between 0 and 21. Now, we have in our setting, we have a bunch of data from a large hospital that contains a lot of information about these patients. So, it has some patient characteristics, which are exactly those characteristics used to calculate the GBS. The GPS. We have the decision made by the admitting ER doctor. So, did they hospitalize or not? And then we have a measure of what I'll call adverse outcome. And this is where I'm going to sweep under the rug, under the rug a lot of the complications that Amanda talked about. In a sense, the outcome that we're interested in is, is this person going to develop some sort of bleeding where it would have been better for them to be in the hospital had that occurred? Which is, so we're not necessarily interested in the causal. Not necessarily interested in the causal impact of them being in the hospital. The primary motivation behind this decision is: if something unexpected goes wrong, it would be better if they were physically already in the hospital than if they were at home and the time that it takes them to get from home to the hospital might actually be a problem. Now, there likely is some causal impact of them being in the hospital, and that's the piece of this that I'm going to ignore. I'm going to assume that hospitalization does not actually influence bleeding risk. We can take a lot of issue with that assumption. I think this is a sort of a I think this is a sort of a worked example of what it might look like to do this. I'm not going to sit here and defend this lack of causal influence assumption, but I'd like you to suspend your disbelief just for a little bit. And I'll try to argue to you why it's actually kind of reasonable in our setting and why the results that we get aren't necessarily that affected by it. But yeah, happy to talk about that assumption, particularly in the setting and then another setting that I'm going to talk about later in the talk. Okay, so let's see how we can answer this initial question. So let's see how we can answer this initial question. Does this doctor have information that algorithms do not? How do we know if, when they make their decisions, these Y hats that I have up here, these hospitalization decisions, are they somehow using information that is not captured by the algorithm or perhaps not even captured by these measured patient characteristics? Because remember, the doctor can talk to the patient, they can directly examine them. They have lots of information that is just fundamentally never going to be recorded in our data set. Is that information useful? All right, so that's going to be. Information useful. So that's going to be the first question that we're trying to get at today. Now, there's a few ways that you could, right off the bat, try to assess this. You could just say, are the doctors more accurate than the GBS? Are their decisions sort of a better predictor of actual outcomes than the GBS? Now, if that's the case, that would be interesting. That says that the doctors' decisions are doing something that the GBS cannot. One implication is that maybe the GBS is not very good. It's not optimally using all the information. Optimally using all the information in the data, it's kind of hard to disentangle the quality of our model from whether or not the doctor has information of any kind that is useful. And you could say, okay, what if we tried to build a more accurate model or the best model that we could build? In a sense, the GBS is the best global model that we can build, or at least the one that is sort of validated to be the best in some of these clinical settings. But this really has the same problem: that we don't know if we've built the best possible algorithm. Maybe we just needed more data. It's not really clear how to. Data, it's not really clear how to interpret that. A useful fact in our setting, though, which is going to allow us to at least make some progress, is that X, the patient characteristics that we observe, comes from a discrete space. And actually, in practice, it's kind of finite. So there are just discrete patient characteristics, and they're all within reasonable ranges. And so, in practice, you kind of see everybody clustered in some space. And what that tells us is we're actually going to observe lots. And what that tells us is we're actually going to observe lots of x's that are identical. We're going to see lots of people who have precisely the same characteristics who observationally are identical in our data set. And this allows us to do something kind of interesting. If we look at two identical X's, are doctors better than random at distinguishing between their actual outcomes? One way of framing this is: conditioned on X, are decisions independent from outcomes? And again, we're assuming that for the sake of this talk, We're assuming that, for the sake of this talk, there's no causal relationship between the decisions and the outcomes. So, any correlation between them must therefore be because somehow the doc that depends on y through some pathway that does not go through x, right? So, it suggests that the doctor has information about outcomes that is not captured by x. Okay. Now, this would be great. If we could find enough pairs of patients with identical x's, we might actually be able to do something useful here. Okay, so this is our plan to do it in. Our plan to do an initial test, right? Let's just group together all the patients with identical X's. Within each group, let's look at the correlation between Y and Y hat. Whenever that correlation is positive, this tells us that doctors have useful information. In fact, whenever this correlation is non-zero, this should tell us that doctors have some information about why there's not captured in X. Now, hopefully that correlation is positive, which means that they're actually making productive use of that information. But it's also interesting if that correlation is negative, because that suggests that somehow they have information about why and they're kind of doing. They have information about why, and they're kind of doing the opposite of it. Now, we don't really see that in our data, but you could imagine that you would get something that looks like this as well. Now, an important consequence of this formulation is that we're not only just learning the presence of useful information or not, we're actually learning on an instance level where doctors have useful information. So, for these particular values of x, this is where doctors are going to have useful information. Now, this could be really useful for decision making in the future, and we're going to come to that later in the talk. Future, and we're going to come to that later in the talk. You might want to know: are there particular groups of patients or particular patient characteristics where doctors are actually doing a really good job? And are there particular patient characteristics where doctors don't really have any additional information? Okay, so this is going to be our plan. And this is basically you can just sort of directly implement this plan. So I'll tell you, let me try to interpret for you what's going on here. On the x-axis, we have axis we have uh the size of each group of patients so every dot here represents one particular feature vector right and so the the you know towards the very left-hand side of this plot you have feature vectors that appear exactly once right there's one unique patient that has exactly these characteristics and then towards the right you have more and more common uh patient characteristics okay so there's lots of patients that big green dot you see on the right hand side there's uh two to the power seven to eight ish Seven to eight-ish patients who have exactly that identical feature vector. Okay. And just for context, I've also included the GBS in color, just so you can see which ones of these are predicted to be high risk or low risk by the GBS. One thing that you notice here is that you have a big point cloud above this zero correlation, which suggests that it looks like doctors are actually have information that is not actually captured. That is not actually captured by X, right? There's conditioned on X, there's correlation between Y and Y hat. But it's really hard to get any sort of statistical claims here. And the low sample size is really going to be an issue here. And in fact, if you correct for multiple hypothesis testing, only one of these actually remains statistically significant. Okay, so we have a lot of suggestive evidence maybe that a lot of stuff lies above this zero correlation line that suggests that there actually is correlation between what doctors are doing and actual outcomes. But we don't really have. Outcomes, but we don't really have enough data to draw valid conclusions from it. And in a sense, this is not really that helpful because any one of these dots could just be a result of noise as to whether the doctor is doing something productive or not. This doesn't really help me inform policy downstream because I would like to actually make statistically rigorous claims about this. Okay, so far, at least our initial idea gave us some suggestive evidence, some mileage. But this idea of trying to just use identical X's is not really going to work in practice. X's is not really going to work in practice. And in a sense, this was just a feature of our particular instance or our particular setting where we actually do have a non-trivial number of identical X's, but in the real, or maybe I guess this is the real world, but in general, we shouldn't necessarily expect this to happen. For instance, if X's can take continuous values instead of discrete values, we're already out of luck. We're just not going to find people with identical X's. But it still feels like there's something here. It feels like we're onto something. And so our next question might be, what do we do with this? Question: Might be: What do we do with this? Right, how can we try to deal with this sparsity issue to generalize beyond our discrete-valued case and deal with potentially continuous-valued x's as well? Now, one idea you might have looking at this is say, well, why do we treat each individual x as the sort of unit of observation? What if we just stratify all these x's by the output of the GBS, right? The GBS produces actually a discrete valued output between, I think, 0 and 21 is what I said. Can we just stratify by that and run and do run? Stratify by that and run and do roughly the same plot, right? And this kind of corresponds to the following intuition: our initial goal might have been too ambitious, right? Does the doctor have information that no algorithm can capture? Because remember, if that information is not contained with X, within X, no algorithm can capture it, right? Not just the GBS. But a slightly scaled back version of this goal is to say simply, does the doctor have information that the GBS in particular does not capture? If I conditioned on the GBS, is there a relationship between Y and Y hat? Is there a relationship between y and y hat? Okay, and so we can make that plot as well. And again, you get some suggested evidence. I believe this one actually is already corrected for multiple hypothesis testing. And so clearly we have some relationship between physicians' decisions and outcomes, right? This is the Matthews correlation coefficient between y and y hat within each value of the GBS, right? Now, again, some of these are. Now, again, some of these are above the line of zero, which means that for that particular score, there is a relationship, a correlation between what the doctor says and the actual outcomes. And so this is great. This gives us some statistically rigorous evidence that we can do, that doctors have valuable information here. It actually tells us where that information lies, right? For which values of the GBS is the doctor's intuition perhaps the most useful. So conceptually, what did we actually do here? We went from this really big original. We went from this really big original feature space, which was, you know, because it was discrete and finite, it wasn't intractably large, but it was still too big to get statistically rigorous claims out of. And what we said is, well, we can just stratify by the GVS, which has a much smaller space. And within each value of that GVS, by definition, patients who have the same score are indistinguishable from the perspective of that score, right? So this particular algorithm is incapable of distinguishing between patients who both have a GVS of seven. Have a GBS of seven, for instance. Sorry, there's a fire station right by my house, so this happens often. Okay, so what's our status so far? On the one hand, we tried to do this thing where we treated each X independently, and we were testing for the presence of information relative to what any algorithm could do. And this suffered from the problem of having low statistical power, right? We just couldn't get rigorous statistical evidence here. But on the other hand, we did this thing where we picked a particular And we did this thing where we've picked a particular algorithm and said, let's just only look at the output of that algorithm. We'll stratify by that. And we'll basically say we've conditioned by conditioning on that algorithm, any claims that we get are relative to the information content of that particular algorithm. And so on the one hand, we said we're going to do something that's robust to any algorithm or getting information relative to any algorithm. On the right-hand side, we said we can get the presence of information relative to a particular algorithm or particular model. Or a particular model. What lies in between these? And that's going to be the goal for the rest of this talk: to try to figure out what lies in between these. How do we sort of smoothly interpolate between these two cases? And what sort of insight does that give us? And so our plan is going to be relaxing our definition of identical, right, to do something somewhat more nuanced, right? This initial goal, as we mentioned, was strong in an information theoretic sense. No algorithm can distinguish between identical X's, and this gives us robustness in an information theoretic. This gives us robustness in an information theoretic sense against the class of all possible algorithms. When we analyzed the GBS, we were robust against this singleton class of models or singleton class of algorithms, which is just the GBS. We're going to try to interpolate between these. And more formally, the way I'm going to do this is to say, suppose that we have some family of models or some class of models. And we want to ask the question, does the doctor capture information that no model in that model class can capture? All right. And this basically says. Can capture. And this basically says one of two things must be going on. Either the doctor has to be computing some function that lies outside of our model class, or the doctor has to have some information that is not contained within x. And so what we're going to try to formalize is this notion that a doctor is capturing information that no model in F captures. And this can be difficult, right? F might be an infinitely large class of models. We can't be testing against each one individually. So there's a sort of a learning problem embedded in this. Learning problem embedded in this. Now, we're going to, I'll give you a slightly more formal definition of this, and we're going to call this algorithmic indistinguishability. And this comes from some notions of computationally bounded indistinguishability that show up in the theoretical computer science literature. Our goal here is going to be partitioning the X's into little subsets, such that within each subset, no model can reliably distinguish between, let's say, the positive instances and the negative instances, or the people who require care and the people who do not. Care and the people who do not. Okay, now this is again clearly true if each X is distinct and we just partition it into each X being in its own little subset. Now, of course, no model is going to be able to distinguish between them. It's also true if we're stratifying simply by the GBS and we're only interested in the class of functions that only includes the GBS. Because again, the GBS cannot distinguish between its own level sets. But what we'd like to do is do this for general f, for general. General f for general model classes in a way that doesn't produce too many subsets, right? Obviously, we could again do this by including each x as its own little subset, but this is going to blow up the number of subsets too much, and we're not going to have statistical power. Okay, and so visually, what I'm looking for is something like this. I take the entire feature space and break it up into little chunks, such that within each chunk, no model has any idea what's going on. Everything kind of looks the same color, the same for any of these models. Are the same for any of these models looking at them? And I mentioned this comes from, in some sense, from the theoretical computer science literature. If you've seen multi-calibration before, this idea of computation-bounded indistinguishability, this is almost exactly what multi-calibration requires, right? So a multi-calibrated partition for a model class F satisfies the following property. For any subset in that partition and for every single model in your model class, Single model in your model class, there's no correlation or the covariance is low between the predictions of that model and the actual outcome of interest. So within each set of the partition, no model is very correlated with the outcome, right? If we could get such a partition, and if that partition were not too big, we would have basically achieved our goal. Because now we can say, well, none of these models seem to have information within this partition. If the doctor does, then clearly the doctor is doing something that none of these models can do. Doing something that none of these models can do. And so, what you might think is in one of these partitions, suppose the sort of pluses and minuses are distributed like this. And let's say our model class is a class of linear functions, right? Basically, what I'm asking for is to come up with this partition such that no sort of linear classifier within this partition is very good at distinguishing the pluses and the minuses, right? And if that's the case, if you look at this picture and I tell you, but it turns out that the doctor actually can tell the difference between pluses and minuses, you might look at this. Pluses and minuses, you might look at this and say, okay, either the doctor is using something highly nonlinear, or the doctor is just using information that is not captured by X, right? Again, either of those two cases are going to be interesting to us and might inform our decisions about how we involve human expertise moving forwards. And so this is going to be our starting point, right? Suppose we have a multi-calibrated partition P. We know that within each subset, no model is correlated with the outcome. So, within a particular S, suppose that the doctor is correlated with the outcome. This means that at least one of these two things must be true. The doctor is computing something that the models cannot, some f prime that is not contained in f, or the doctor has information that is not contained by x, right? Either of these, or at least one of these, must be true. And of course, we can scale the complexity of f as much as we want. So, the more complex f is, the more we might believe that the doctor actually has information that x. Doctor actually has information that X does not. Okay, so I'm not going to tell you the details of exactly how you get such a multi-calibrated partition. There's a nice paper that we sort of directly use, which you can go read. The thing that we do in our experiments is we consider F to be the class of regression trees of depth at most three, right? So shallow regression trees. And we're using this boosting algorithm that Ira and co-authors had come up with to learn this multi-calibrated partition. To learn this multi-calibrated partition, basically, what you do is you sort of iteratively boost a predictor until you can't really extract any more predictive performance. And then you take the level sets of that predictor that forms a partition. We choose the size of the number of level sets we take to be 11. You could take more or less if you wanted to. We just sort of bin it, as I'll show you, between sort of deciles of the score. And now within each of these partitions, we can look at the correlation between the doctor's decisions and the actual outcomes. Uh, decisions and the actual outcomes. And so we do this. Uh, I didn't fully label this axis, um, but on the x-axis, you have the level sets of this predictor. So each bin is between 0 and 0.1, then from 0.1 to 0.2 and all the way up, the last bin is just deterministically one. Some of these are missing because some of these buckets are actually just empty. There's no patients that actually fall in these deciles. But now we can see, again, within each of these deciles, you have provable correlation, at least, or statistics. Provable correlation, at least, or statistically rigorous correlation between doctors' decisions and outcomes. And importantly, this is not just with respect to one particular algorithm, right? You might think this looks kind of like the plot that we got for the GBS. That was specific to the GBS. This tells us something about any possible shallow regression tree, right? There's no shallow regression tree that's going to capture the information that the instructor needs. So, in the couple minutes I have left, I'll try to take stock. I'll try to take stock and tell you where we can go from here. We kind of figured out how to answer at least this first question: right, does the doctor have useful information that the algorithm does not? This gives us a principled way to answer question number one for a particular function class. And of course, we can scale the complexity of our function class depending on how much data we have, how strong the signal is, and so on. But this is a natural way to trade off statistical power against model complexity. But this doesn't actually answer the second question, which is, what do we do with this information? Question: Which is what do we do with this information, right? From a decision-theoretic perspective, should we automate these decisions? Should we not automate these decisions? Are there subsets where we should and subsets where we shouldn't? How should we think about this? One nice feature of our setting is because both the outcomes that we measure as well as the decisions are binary, the policy space is actually quite small, right? Within a particular partition, we can either always send patients home, always keep patients in the hospital. Always keep patients in the hospital or do whatever the doctor says. Now, in principle, you can do the opposite of what the doctor says as well. That's the sort of fourth available policy option to you. This is never a good idea in our setting, and so we're not really going to consider it. And as a hospital in this case, you might have essentially three objectives that you really care about. Mostly, you actually only care about one of them, which is you really don't want these false negatives where you send people home and where they actually should have been hospitalized. Where they actually should have been hospitalized, but to some extent, you also care about false positives, and you also care at the fraction of cases where human judgment is actually required. And we're going to look at that as a piece of this trade-off. Now, as I mentioned, there's three possible policy choices within each partition that leads to three to the size of P possible policies overall, but most of these are Pareto-dominated. And so I'll show you a plot that shows what these Pareto, these non-dominated policies look like, and what is the sort of menu of policy options that you get. So there's three, my three. So, there's three, my three objectives are kind of all on this one plot. You have false positive rate on the x-axis, true positive rate on the y-axis. So, this might be a sort of familiar ROC curve to you. And then color actually represents the fraction of decisions that are automated. And so you can see this top right point over here, the yellow one, basically says always hospitalize everybody. Don't even ask the doctor. You've completely automated everything. You have all the true positives in the world, but you also have all the false positives in the world because nobody's actually getting sent. In the world, because nobody's actually getting set point. So that's one potentially interesting point on this, or maybe not that interesting point on this curve. The ones that I think are slightly more interesting are, one, the red dot over here is basically what hospitals are already doing in a non-automated way, which is defer to the doctor's judgment basically all the time. This gives you a 99.7% true positive rate. As I mentioned, the hospitals care a lot about false negatives. It gives you a false positive rate around 77%. Positive rate around 77%. Basically, what we find is that you could still automate one of these partitions because, in that partition, the doctor makes exactly the same recommendations as what your algorithm would, right? This is, so I mean, you could ask the doctor, but because the doctor will sort of, at least in our data, always says the same thing as the algorithm does, it's sort of equivalent. The other point that I think is worth highlighting on this plot is a policy. Highlighting on this plot is a policy that kind of does the opposite, but only asks the doctor for their feedback in 14% of the time or only in one of these partitions or these subsets in the partition. This has a slightly less true positive rate, 97.4% in our setting, but a significantly smaller false positive rate. Now, from our conversations with the doctors, this is still probably not a desirable point on the policy space. The true positive rate is still a little bit too high. But this basically tells us sort of what the trade-offs might be. Sort of what the trade-offs might be. If you really wanted to automate heavily in this setting, or you wanted to set strong defaults, for instance, in the setting, you might end up closer to this blue point. Whereas the current policy, existing policy, looks much more like the red point. And again, we can still do some amount of automation here with basically zero cost relative to what doctors are already doing. I'm running out of time, so I won't tell you what the sort of more general recipe is, but basically we never really relied on the fact that doctors actually produce predictions of outcomes. Predictions of outcomes. What we're working on now is a more general version of this where doctors can provide sort of more arbitrary feedback. It could be just, you know, I think the prediction is too high, too low, about right. It might be freeform text notes. It might be something else. You can basically run the same recipe and it works in those settings. We're currently working with a hospital, a local hospital here, to get this sort of feedback from doctors where they get a risk prediction before performing a surgery. They look at this prediction. Performing a surgery, they look at this prediction and they tell us, I think the prediction is too high, too low, or about right. And we're basically trying to figure out: okay, how should we interpret that form of feedback in this decision theoretic context where we actually figure out should these patients, what should the post-operative care be for these patients? I'll leave the slide up or very briefly talk about it. Of course, I told you from the beginning I was going to ignore this causal relationship between y-hat and y. Maybe there's something that you could do methodologically here that would be interesting. Here, that would be interesting. A lot of the feedback that we get is actually from heterogeneous doctors because it's not just one doctor providing feedback. That might be some doctors might have more expertise than others. Maybe they have different areas of expertise. That is sort of an ongoing question that we have. And then there's this kind of interesting question that I think is worth pursuing more, which is when doctors actually do have valuable side information, what sort of methods can we use to try to elicit what that side information is? Like in instances where they do something to the algorithm. Instances where they do something that the algorithm is unable to do, what information were they rely on to do it? I'm going to wrap up there. These are all the wonderful co-authors, including from the CS department at MIT and then the Yale School of Medicine. I know I'm running over, so apologies for that, but thanks for being here. Sorry, again, to be having to present remote, but hope you all enjoy the week. And yeah, looking forward to any questions. Let's take it any questions. Yes. Any questions? Yes. I'm actually curious if you explored risk assessments that have any best features. And so you would probably have a bigger group of identical X's, if that makes sense. So is that something that you guys explored? Because I feel like the risk assessment you chose has so many input features that that's probably one of the reasons why none of the identical groups were like large enough to analyze with your pre- Large enough to analyze with your framework. And so I'm curious if you guys looked at, I guess, groups that had, you know, maybe like five, three to five features. So there are some of those kind of risk assessments in MDL. And then I had a second really quick question around when you guys stratify the groups based off of the risk score, you know, at times you can have someone getting the same risk score, but you know, like, let's say, for example, if age increases the risk score and race decreases, The risk score and race decreases the risk score. You can have someone that's like in the same bucket but they have different features. I'm curious how you guys thought about that when bucketing by risk score itself because that doesn't necessarily mean the features identical. It just means that the sort of algorithmic combination of those features are identical, right? So it could be like higher age, lower race, or lower race, higher age could lead to the same bucket. So yeah, those are my two questions. Okay, great questions. Question number one, in a sense, in a collaborative. In a collaboration with a bunch of GI doctors, you don't choose the risk score. The risk score chooses you. Yeah, so I think you're absolutely right that in a world where you have more sort of dis like a smaller feature space and everything is discrete, you don't really need to do anything. I think you should just run the very first plot that I showed you, which is like conditioned on every possible feature combination. What is the relationship between predictions and outcomes? That, you know. Outcomes. That, you know, if you're you that I think that's a good starting point, if you get enough statistical power there, great. You don't need to do anything else. Because, from an information theory perspective, that's actually the right thing to do. It's only because of statistical limitations that you don't. So that'd be my suggestion in a case where you have sort of a smaller feature space. I think ours is kind of a median case. There's actually instances where you have a much bigger feature space, like anything continuous value, this automatically goes out the window. And so our hope is that our methods would sort of adapt to those as well. So the second question. Well. So, the second question: what do you like? Does it make sense to stratify the risk score? To be clear, I don't actually think that's a good idea. I think that is a useful pedagogical device to think about what does it mean to sort of coarse in the feature space using a particular model. But now you're subject to the limitations of that model because precisely, as you said, like there's lots of people who get the same score, but actually might have meaningful variation in their features. And so, but then when we say, okay, what does it mean to have meaningful variation? The way that we define that is variation that keeps us. Define that is variation that can be captured by some model in some broader model class, for in our case, shallow regression trees. And so the same critique applies to whatever we learn, right? There's if there's some, we stratify things. I showed you those 11 buckets. There are patients with non-identical features who fall in the same bucket. That might be meaningful difference. All we can guarantee is like, if there are meaningful differences, they cannot be captured by depth three shallow regression trees, right? Now, there might be depth four regression trees that can capture. Depth four regression trees that can capture that information, we're not able to speak to that. So, the way I'd view it is like the more you're able to add complexity to your model class, the more confident you'll be that you're not lumping together people who are actually meaningfully different in some capturable way, right? But of course, there's always going to be some, unless you're doing the very first test that I mentioned, which is just run a different, look at the correlation for every X individually, you're always going to have to group them together in some way. And now you have to hope that you didn't sort of destroy meaningful information. didn't sort of destroy meaningful information when you did that. Thank you. One more question. And oh, Surish Mankut, Brown University, Director Center of Technology. Manish, thanks for the talk. I had a question about, so your argument overall is this idea of the position having information. I want to focus on that word, information that the algorithm may not have. And then when you talked about this interpolation between all algorithms and GPS, and you talked about your Algorithms in GPS, and you talked about your depth three regression trees, for example. It sounds to me more like it's not so much about doctor information, but doctor, I don't know, internal computational model. So you have a Dev3 regression trees, they do something. Dev4 regression trees, as you just said, could do something different. Why do you argue this is about information rather than about the expressivity of the model alone? Is the major issue that you're trying to probe? Yeah, it's a good question. Yeah, it's a good question. A couple. So, the reason that I think information is a useful lens is, at least in the case where we saw that suggestive evidence at the very beginning, this was not about internal computation model, right? This is purely about information. This is just X only captures some information about the patient. Doctor sees information that is a superset of. That is a superset of this, right? They actually talk to the patient, they can physically examine them, whatever. That stuff doesn't make it into the X, right? And so, if you believe that this plot has more points above zero than below zero, which visually seems true, then it has to be a story of information, right? It's not at all a question of computation. The only reason it has to become a computational question later is because we can't, this information, if I had, you know, maybe 10 times as much data, we could just do this. But in order to get statistically rigorous. But in order to get statistically rigorous claims, we have to relax our initial goal to not purely be about information. But I guess the reason that I keep talking about it through the lens of information, that's why that was my motivation, is unless you believe that doctors are doing some very complicated decision tree or some computing some function that's outside of our class, and you don't believe that there's any signal in this plot right here, then it has to be at least some informational component, even if it's not 100% a story of information. And I do agree that the more you start to restrict the model class, the more you're blurring the line. The more you're blurring the lines between is it actually a question of information or is it a question of computability? And so, yeah, so this is one extreme which I think still suggests strongly that there has to be some informational component to it. Yeah, with that, we'll conclude this talk. Thank you so much, Mani, for this. Thanks so much. Take care. Thanks for joining. 