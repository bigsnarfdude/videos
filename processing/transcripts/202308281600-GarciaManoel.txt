Um, can you hear me? Sorry, yes, thank you. Okay, just a second. I'm trying to, so can you see my slide? Yes, perfectly. Yes, perfectly. Great. Thank you very much. So I want to express my gratitude to the event organizers and I'd like to extend a special thank you to Jawad for the kind invitation. In this talk, I present a study of singularities of vector fields using graph theory. So the singularities are equilibrium points in the scenario. In the scenario of a network of individual subsystems that are coupled together, so the vertices of the graph represent the individual subsystems or cells, as I shall say, and the edges between two vertices represent the interaction of these cells. Now, about synchrony, these singularities, they are expected to occur in these special subspaces. These special subspaces that are invariant under the dynamics, which are the synchronous subspaces. We can say that the synchronous subspace is in the context of network what corresponds to a fixed point subspace of an isotropy supergroup in the context of equivariant systems. I will come back to this idea a little bit in this talk. So I will talk about Laplacian networks. About Laplacian networks, but let me start speaking briefly about the first notions of networks in general. So I start with this example. So I start with this picture representing an interaction of two identical robots in which one is giving instructions to the other. To the other. On the right, we see the graph representing schematically this interaction. So this is a graph with two identical vertices, which I draw as two identical circles, right? And one edge, which is in this case a directed edge, because it's just the cell number one that's sending information to the cell two. So this is a very simple example. So this is a very simple example of what we call a feed-forward graph, because it's a direct graph. So now this graph can represent other coupled networks of two individual systems. So what are the possible dynamics that are represented schematically like that? Like that. Here. So here is one possible ODE associated with this graph. In this case, an ODE on R2. X is the variable of cell 1. And I'm calling Y the state variable of cell 2. So the graph shows that cell 1 depends only on its internal. One depends only on its internal dynamics. So, in the ODE on the left, on the left, the variation of x depends only on x and the variation of depends on y and also on x because y is receiving because of the input cell two from cell one, right? Okay, so yet another example: the Dauphin oscillator. This is a second-order non-autonomous equation that can be another example of a dynamical system associated with this same graph. So, notice that I think you can see it. If we rewrite it as an ODE in the variable. As an ODE in the variables u, v, x, and y, so on R4, which cannot really be identified as an admissible vector field in these two cell feed-forward graph, where UV are the state variables of cell one and XW the state variables of cell two. Okay, well, let me just Well, let me just move to a different graph. So this is a five-vertex graph. It's regular, right? In fact, every vertex has degree two. And if we are to write an admissible vector field for it, so assigning the state variable xi for assigning. xi for a cell represented by vertex i on the graph, a direct investigation should tell that what? That each component should be like. Well, first, look at vertex one in the graph, for example. The input set of cell one is cells two and five. Okay, so x1 dot is a function of x1 itself. Itself and also x2 and x5. Second, the cells are identical, so the function f we see in the component must in fact be the same in each component. And there is only one type of edge in the graph, which means that all couplings are identical. And so the function f must have function f must have this property, this invariance under the permutation of the last two variables. So what is the formalism behind that? So for a general network, for a general coupled network, we encountered the notion of what we call an admissible vector field of a graph. So, as I was saying, we have these relations x1 dot and all the orders defined from the same function f and they depend on the input sets, for example, x1 depending on x2 and x5. Depending on x2 and x5, and f invariant under the permutation of the two variables. Okay. So the notion of a demisible vector field. So the graph underlying the graph is what represents the subsystems and how they are coupled. Okay, so each vertex represents a Represents, oops, sorry. Sorry. I have to silence myself. I'm sorry. Okay. Sorry about that. Yeah. So each component of the vector field should be like the variable of the cell C, which is governed by this equation, should depend on X C itself and all the other variables in the input set of the cell C. Of the South Sea. Golobitsky Stewart, Martin Golobitsky, and Stewart and many collaborators, they started in about 2004 to develop this formalism to study networks via an admissible vector field associated with a given graph. So, one of the more interesting facts in this formalism, as I was mentioning in the beginning, is that Beginning is that this is in general similar, but not the same thing as an equivariant problem with respect to the symmetries of the problems. In this case, the obvious symmetries would be the symmetries of the group of automorphisms of the graph. Okay. But there are, yes, these symmetries and but But in this context, they do not form a group, but they form a groupoid of symmetries. And this is what defines an admissible vector field associated with the given graph. So for people familiar with the quivariance under a group of symmetries, for the coupled network formalism, the notion of an admissible vector field associated with a graph is then related. Is then related to the notion of an equivariant vector field for dynamics invariant under a group of symmetries. One of the major problems in this setting is that there is no standard way to define the group of coordinate changes in the classification of terms or of bifurcation problems, mainly because of the obstructions of this groupoid complicated. This brupoid complicated structure. Well, these are the first ideas of the very nice algebraic formalism behind that, which I'm not going to talk about. So let me move on to the next slide. Here is the The notion of adjacency matrix. So, there is a very practical and useful way to work with graphs, which is to look at it as an algebraic object. So, from the linear algebra point of view, a graph is a matrix. This is the adjacency matrix. So, the matrix is the The matrix is the entries are for if the graph is unweighted, so this is a one-zero matrix, and it's one if there is an edge from vertex I to J and zero otherwise. If G is a weighted graph, so we just replace one by the weights of the graph. So this is a one-to-one correspondence between graphs and matrices. For example, Graphs and matrices. For example, bidirected graphs means that the adjacency matrix are symmetric matrices and so on. So proposition. Here is the first result relating a graph adjacency matrix with the dynamics behind the graph. The dynamics behind the graph network. So I'm using this slide for these two ideas together. So we have the adjacency matrix, and then I write the ODE, the linear ODE, which represents an admissible ODE associated with the graph on the left. Well, there is a theorem behind the power of the adjacency matrix related to this proposition. Related to this proposition, but we will see it on one of the next slides. So, for now, using the linear algebra as an example, let us talk about synchrony. So, synchrony between cells. So, this synchrony is this configuration where cells behave setwise identically. So for an ODE x dot equals f of x governing the network dynamics, a synchrony of two entries of x, so between two cells C and D means that X C of T is X D of T for all Xd of t for all t. So this is the same as saying that the solution X of T is in the fixed point subspace of the permutation C D. Let us go back to the previous slide. Let me show. So, this is the synchrony in this case is in the invariant subspace of fixed point under the permutation 1, 3, T2, 4. So, that's what I want to. So, what we see on the right is this coloring, defining the. Defining the synchrony configuration one equals x one of t equals x three of t and x two equals x four of t and x five it doesn't matter okay so Let P denote the total phase space and a balanced equivalence relation. Yes, so a balanced equivalence relation. What is that? This is an equivalence relation. So when we define a balance equivalence relation, this means that we want to identify cells. The main idea is to identify cells so that they are invariant under the dynamics. So what we do. What we do, we define this subspace being the set of X such that X of C equals X of D whenever C and D are equivalent. So assign a color to each synchrony class. The resulting coloring of vertices encodes the corresponding pattern of synchrony. So the definition is So, the definition is this equivalence relation is balanced if vertices with the same color have isomorphic input sets. Let me explain what this means. So, the input set of a vertex V is the set of what? It's the set of vertices directed to it. Okay, so there is a match from each of these vertices directed to V, and being isomorphic means that. Being isomorphic means that for any two vertices C and D in the same class, the number of directed edges from vertices of each class directed to U and to V is the same. Well, I think it's easier to see that in the picture. So look at the previous slide. Yeah, here. Yeah, as I was saying. The input set of any red is one green, one white, and the input set of any white is one red, one green. Right, now two theorems. Now two theorems, the two main theorems, I think, regarding the synchrony, the idea of synchrony. First, the first theorem is saying that the relation, the equivalence relation is balanced if and only if the synchrony subspace defined from this equivalence relation, the synchrony subspace. The synchrony subspace is invariant under any admissible vector field F. So, if and only if F of delta is contained in delta. So, it assures that in coupled dynamics, any synchrony subspace is invariant under the dynamics. The second theorem is of more practical use. So, it says we can find this invariant. It says we can find these invariant subspaces through linear algebra. So, an equivalence relation on the set of vertices of a direct network G is balanced if and only if the network adjacency matrix leaves this polydiagonal subspace invariant. So, anapa. So, an apologie is Maduela Guillar from Universidad do Porto, they gave in a paper of 2014 a theoretical algorithm, algorithm based on linear algebra to this theorem. Entia Guamor√≠, a student of here at ICMSA, implemented in a practice code with using the software mathematical. With using the software Mathematica. Part of the results of my last slides are actually deduced from this code. So here I present an example. Actually, this is an example where the code was extensively used. So what I present now is the graph. What I present now is the graph G6. This is the name of this graph and its resulting list of synchronic configurations I will show in the next slides. So the G6 is a homogeneous graph where we have six identical vertices, cells, and each is connected to its neighbors, two neighbors. And the next nearest The next nearest neighbors. This is the kind of couplings that defined what we call the G6 graph. Okay, so let's look at what are the possible synchronies for this graph. So this is the first one. X1 equals X4. So the synchrony subspace is the fixed point subspaces of the permutation of 1,4. I'm coloring 1. I'm coloring one and four in red with arbitrary colors for the others. And then we have another synchrony configurations. So number two, these are X1, two, four, and five. This is the permutation of one, two, four, and five. And a third configuration, a fourth one. Fourth one, number five, number six, seven, eight, and nine. Nine is the full synchrony. Every vertex with same color. Okay. So I'll give an example. This is a very classical. Very classical, impossible not to mention example in the context of networks in general, and especially in my talk, because I'm intended to talk about Laplacian networks. So, Kuramoto model. This is the classical one. By classical, I mean that he considers the O to O coupling. In this case, the picture we see is a 12 vertex graph. So, this is the So, this is the vector field. This is, I'm just picking one of the equations of the n equations of the Kuramotu model. Each of the oscillators is considered to have its own intrinsic natural frequency. You can see that omega i for cell i, okay, and each is coupled equal. And each is coupled equally to all the other oscillators. And the coupling is a sine function of the phase of the differences. So this is an example of a Laplacian network. So what is a Laplacian network? First, a definition, an n by n matrix, L is said to be a Laplacian if it To be a Laplacian, if it is symmetric, and if the element in the diagonal is minus the sum of all elements in this line. That's the definition. This is a Laplacian matrix. And a vector field F on Rn is said to have a Laplacian structure if the Jacobian Jf of X is a Laplacian matrix for all. Matrix for all X in Rn. So the first thing we investigated was what is the general form of a Laplacian vector field. So first a lemma if H is If h is a function from Rn to R is such that the sum of the partial derivatives is zero, then this is a function of the differences. So in this function depending on n minus one variable now. Okay. Theorem. Well, this theorem is a kind of pre-normal form for a general form of a Laplacian vector field. It's saying that f is a Laplacian vector field if and only if its component is the partial derivative of a function of n minus one variables t1 t one t two etc t n minus one T2, etc., Tn minus one, and the last one is this expression for some G. It's easy to say, well, there is no special thing about the component N, just a matter of picking an FI, which I'm calling here an N. It's just an arbitrary choice, okay? This corollary is just so, oh, sorry, oops. Oh, sorry. Oops. Where am I? Oops. This is just a characterization of this space of Laplacian vector fields. This is isomorphic to this direct sum. And the last corollary we see here is the characterization of a Laplacian. Characterization of a Laplacian vector field. F is Laplacian if and only if is a gradient vector field. So it's gradient of G bar, where G bar is a function given by G of the differences plus K Xn. Okay. In fact, what we see from this theorem is that, in fact, any version of the Kodamoto network is Laplacian. Here we consider what I mean by another version of Kodamoto. We have some functions that sometimes this is not signed. Sometimes this is not signed. There are much more general networks called Kuramoto networks. They are not necessarily all-to-all, etc. And here I bring as an example a network with coupling given by that graph G6, which is a bidirected graph of six cells. Each cell is coupled with its nearest neighbors and also. Its nearest neighbors and also to the next nearest neighbors. In this case, well, this is the vector field I'm choosing. This dependence on the phase differences implies an S1 diagonal symmetry on X. And the domain because of the sine function is the torus T6. T6. An important general observation about the Laplacian networks is that the defining condition of a Laplacian network implies that the graph must be in fact directed. Okay, in other words, if we have an edge from I to J, we must have an edge from J to I. I okay, so let us now turn again to linear algebra behind this, particularly about the linear algebra of bidirected graphs. So Laplacian matrix of a bidirected graph. So what's the Laplacian matrix? So, what's the Laplacian matrix associated with a graph? Okay, if you notice, I was talking about Laplacian matrix in general. Then I defined a Laplacian vector field, and now I come to the notion of a Laplacian matrix of a bidirected graph. Okay, so what is this? Just recall that this is given by D minus A, where A is the adjacency matrix of Is the adjacency matrix of the graph and g is the diagonal matrix of the degrees of this the vertices of this graph? So, for example, for this graph G6, this is the Laplacian matrix. So, every vertex has degree 4. So, G is 4 times the identity of order 6 minus the ones representing giving. Giving the IJ states in matrix for the same graph, we can look at a Laplacian matrix with weights. In this case, it should be instead of zeros and one, we have those omegas ij's and necessarily in. And necessarily, in the diagonal, we must have the sum of all elements in that corresponding line because of the definition of the Laplacian matrix. So, the sum of each line must be zero. Okay, so What was the idea we had? To use algebraic graph theory to study the stability of singularities in Laplacian networks, we used the Jacobian of the Laplacian to be the weighted Laplacian of the original graph. So let me say it again. We have a network and this has its graph, which is an unweighted graph, but now our network. But now our network has a Laplacian structure, and we take the Jacobian of this vector field. And now, this Jacobian being Laplacian, we can look at it as a Laplacian graph associate, a Laplacian matrix of that graph. Well, now we have a Laplacian matrix of a graph with weights. So, although we start with, this was the main idea, actually, we started with an Actually, we started with an unweighted graph and we use this algebraic graph theory of Laplacian matrices of graphs with weights to study singularities. So let me explain what we how we do it. So let's see of age denote the So let's C of H denotes the number of connected components of any graph H. G plus and G minus shall be the subgraphs of G with the same vertex such as G together with its positive, respectively, negative weights. Also, we associated the absence of an edge, Ij. So if there is no edge, I use the zero to Use the zero to be the weight of that edge. Okay. So any zero, any minus, and any plus are the numbers of zero, negative, and positive eigenvalues of the Laplacian matrix. And the theorem is, if G is a possibly disconnected graph, then we have these lower and upper bounds for the A lower and upper bounds for the number of positive, negative, and zero eigenvalues of the Laplacian matrix of this graph. So any plus is a number between this difference. Well, I will not read it, but I think we will have an example. It's much easier to understand what this theorem is saying. This theorem is saying in an example. So I'll move to yes. So for the Kodamoto network, okay, we compute the Jacobian. These are the entries of the Jacobian. And this is the IJ entry. So we see that the We see that depending on x, I shall have negative, zero, or positive weights. So you see, this graph is a bidirected graph without weights. But now I will write this matrix as its Laplacian. As I have just said, this can have zero, negative, and positive weights. And this is the new. And this is the new Laplacian matrix I want to look at for this graph. So for this, if we compute the singularities, this is the slide that are summarizing those synchrony subspaces I shown one by one. I put all together here, and this is the latches. Is the inclusion of one fixed one sub synchrony subspace to the other is used by these arrows? Okay, I think it's clear this way. And what we have, we have proceeded with this calculation for the singularities of every synchronous subspace and this the last. This, the last slide summarizes the calculations. I will show it. So, for example, choose pattern number three. We have this singularity. This corresponds to the synchrony picture. This is the matrix, which is the Jacobian, and this is. And this is now the Laplacian matrix of the graph. So, what we see, I cannot, can you see the graph on the right? I see just myself on the right. So my picture is just for my screen is just on the graph. But anyway, so we have this graph. And can you so you see that this graph is now disconnected, okay? Disconnected. Okay. It started, it's a connected graph, but in the search of the singularity, I want to look at this disconnected graph. So this is obtained from the original G6 assigning Jf of X as the weighted Laplacian of it. Okay. So C of G is the number of connected components of this graph, which is one, two, and three. The subgraph GP. Uh, subgraph G plus has the same vertices, so six vertices, and it's a disconnected graph. So we just remove the edges with negative sign. So we end up with G plus being a disconnected graph with four connected components. And the same thing for G minus. If we go to the formula in that theorem, the lower bound. The lower bound and the upper bound for the number of positive eigenvalues for this singularity here x is 1 and 2. Okay, and what we did, as I said, we proceeded doing the same thing for every configuration, and then we have this big list of all these critical points, and we see. And we see what we see from this last column is what I'm most interested in mentioning here at this moment is that only any plus is at least one for every configuration. In other words, only asymptotically accepted the total synchrony. Okay, so what I'm showing here is all of them accept the total. All of them except the total synchrony. Okay. 228. I called nine. It was the number of the total synchrony. So only asymptotically stable equilibrium is the total synchrony and vice versa, but in this case, okay. So yes, we have other examples where asymptotic stability holds for others. Stability holds for other synchrony configurations other than total synchrony, but I won't show. This is just one example I chose to bring here. Okay. And that's it. These are the references. And muchas gracias, thank you. Muyo Rigardo. Thank you very much. Is there any question here? No. And online, do we have some questions? No? Well, thank you very much, Miriam, for your time. Miriam for your talk, for being so clear and post. Thank you, thank you very much. We have now a coffee break.