Equations, and this is part of a successful program of Scott, Tuomo, and Charles on elliptic stochastic homogenizations that has had a lot of resonance in the last years. So, Scott, please. Okay, so thank you, Rosario. I will say that two things you said were wrong: one is that it's not successful, and one is It's not successful, and one is that it's not stochastic. But other than that, yes, it's joint work with Tuomo and Charlie. Okay, oh, at least this, I know, okay, at least this is okay, fantastic. It's neither successful nor stochastic, but I hope it's still a little bit interesting. So it's about optimal doubling inequalities for periodic equations. So let me start with the open problem that we were trying to think about. So it's actually an old question that goes back decades. Question that goes back decades, and it's been studied or attempted, I guess, by a lot of spectral theorists, a lot of Russian spectral theorists, but it's still open. It's a very simple question. Suppose you have a periodic matrix A of X. It's symmetric, say, and it's uniformly elliptic. Can you find an embedded eigenfunction or an embedded eigenvalue? That is, can you find the eigenfunction? Or an embedded eigenvalue. That is, can you find an L2 eigenfunction for the operator, the elliptic operator, in the whole space? Okay. The conjectured answer is no, at least if the coefficients are smooth. Probably if they're Lipschitz. I mean, the conjecture should be if they're Lipschitz, the answer is no. And as a caution, if the coefficients are only slightly less than Lipschitz, then we know it's not true because you can build a Then we know it's not true because you can build a compactly supported eigenfunction. And this is work of Philonoff, who builds the eigenfunction using ideas of Pliss, who kind of made the best counterexamples to unique continuation for elliptic equations in the 60s. So using Pliss's argument, you can kind of build an eigenfunction which is compactly supported, but in order to do so, you have to make the coefficients be not the shits. Okay, so this question. Okay, so this question is related to unique continuation. Um, which okay, so the idea being that if the if you have NL2 eigenfunction, then you try to show that it decays fast. And once it decays fast enough, it means you're kind of small far away from the origin. And if you're small enough, then you shouldn't, then you should be small everywhere, and that should be a contradiction. So that's kind of the idea. By the way, I know it's kind of a short talk, but please stop me if. Short talk, but please stop me if I'm unclear if there are questions. All right, so this is the open problem that we wanted to solve. You can think about this in terms of exponentially growing A of X harmonic functions, that is solutions of, okay, so now I have a problem because my iPad is running out of batteries and it's apparently not charging. That's bad. Okay, well, we'll see how far I get, and I can open the PDF if it if it closes. PDF if it if it closes. Right, so if you okay, so now something stopped. Okay, so everything is not working now. I'm sorry about this. Let me open the PDF. My iPad is not enjoying my computer right now. So let me try to reshare the PDF. Okay, all right, so I will share screen drink, and now I have to change to full screen mode somehow. Maybe I just how do I do this? There we go. Okay, so I won't be able to now write with my pin on the screen, unfortunately. So, okay, so where was I? Okay, can you still see it? Okay, can you still see it? Is it still there? I guess if it's not, you'll stop me. I think it's okay. Okay, sorry about that. Okay. Anyway, you can add a dummy variable, which just takes away the right-hand side and makes it have a zero right-hand side. But then your solution is growing like this exponential in the D plus one direction, the dummy variable direction. And your new matrix has a one in that slot and it is blocked. Has a one in that slot and is blocked diagonal like this. Okay, therefore, we can think about this as a more general question about classifying sort of harmonic functions, A of X harmonic functions with at most exponential growth. Okay, and so we can think about it if you want as being a unique continuation problem for exponentially growing solutions. Okay, let me talk about how you would prove such a quote delocalization result for the Laplacian. So, in the case that A of X. Laplacian. So, in the case that A of X is the identity, we obviously know how to do this. It's kind of so easy that it's hard to even separate the proofs. But let me talk about at least four different proofs that we have, which are all very related for Laplacian, but if you tried to carry them out for the periodic operator, it would lead to different kind of different programs. You could prove a Carloman estimate. A Carloman estimate says that you multiply the equation by some weight. You multiply the equation by some weight, and then you try to prove an L2 type bound, or this is one kind of Carliman estimate: an L2 type bound for the solution in terms of times the weight, in terms of the L2 norm of the operator evaluated at the solution times a similar weight, or maybe the same weight. Okay, and so this is what a, so you could prove a Karlovin estimate, and that would give you unique continuation, and it would, it would rule out the presence of embedded eigenvalue. The presence of embedded eigenvalues. You could take the Fourier transform of the equation and see rather quickly that it's not going to work. You could prove a three-ball theorem or a doubling inequality. You could use Almer's monotonicity formula to prove a three-ball or a three-sphere theorem, and that would be enough to rule it out. Or you could use analyticity. So you could, if the eigenfunction was decaying fast, you could. function was decaying fast you could find a place where it was small you could open a you know you could use the ck estimate the the pointwise ck estimates for the laplacian for large k to approximate the function by taylor polynomial and then argue that actually the taylor polynomial is very small so that's incompatible with the function actually existing okay so we have four sort of options here the carloman estimate is not obvious how to make it work because make it work because the way that the proof of the typical proof of a Carloman estimate works is you it's there's some commutator that's involved in the calculation and you can't commute past this A of X operator even if it's Lipschitz. Fourier is not going to work or at least I guess you could replace Fourier by flow K theory and that's what a lot of what the Russian school tried to do but that hasn't solved the problem and so we're going to try our best And so we're going to try our best to use the last two dots here to see how far we can get. And the answer is we can't get very far, but we're going to at least see what we can prove. So if the equation has x dependence, obviously you can't prove analyticity in the same sense as you can for Laplacian. Because even if the coefficients are smooth, I'm really interested in the behavior of solutions on large scales. And so the oscillating coefficient matrix A of X will. Oscillating coefficient matrix A of X will preclude us proving analyticity. So we have to interpret this correctly. Okay, so let me explain what analyticity estimates I'm thinking of for harmonic functions. It's just the basic estimate that you would find in the book of Evans, okay, or any PDE book that the first thing about harmonic functions that you find really is that you can estimate the kth derivative of the solution at the origin by a constant divided by r to the k times the l. Divided by R to the K times the L1 norm of U in the ball of radius R. Here I'm underlining my LP norms. That just means that I'm putting a slash in all my integrals, because otherwise I get confused by all of the volume factors that I'll probably be dropping if I don't do that. So I put slashes in all integrals. And so I remember that by putting an underline on these LP norms. So if you do that, then you see that, of course, it should scale like inverse power of r to the k. Of the k and the constant is actually k to the k, also known as k factorial, or you know, c to the k k factorial or c to the k to the k. They're equivalent. Okay, so this is the the this is what I mean by analyticity. I mean like a quantitative CK estimate pointwise. In terms of polynomial approximation, you can restate this and say instead of saying what is the function, you know, the pointwise value of the derivatives at the origin, you can ask how. Origin, you can ask how well can I approximate the function by a polynomial in a smaller ball? And the answer is: the best polynomial of degree k, which is basically the Taylor polynomial of degree k, is close to the solution u and L infinity in a small ball of radius little r. And the error is, again, the underline, the normalized L1 norm of the solution in the big ball divided by the ratio of the link scales to power k plus one. Okay, here. Okay. Here, this PK, this sort of blackboard bolt pk is the space of polynomials. Okay. Okay, so now why can't I scroll? There we go. Okay, so let me give a cartoon of the proof for how you prove, you know, delocalization or the absence of embedded eigenvalues for Laplacian by following a proof that we're going to try to use for the periodic operator. So suppose you have an eigenfunction. A eigenfunction, suppose it's not identically zero. You can, without loss of generality, assume the eigenvalue is positive, because otherwise, it's easy to rule it out, or if it's zero, really. So the first thing you do is you find the place where the eigenfunction is non-zero. You bring it, you know, the maximum, say, you bring it to the origin and you normalize it so that its size is one. And you go ahead and assume the rest of it is decaying exponentially for. Rest of it is decaying exponentially for some large constant A, which you get to choose. It can be as large as you want. And I say this as a result of Kuchman. I mean, using Fourier, then you can kind of reduce to this, or using floquet theory in the case of a periodic operator, it's okay to reduce to this kind of decay. So you don't have to rule out an L2 eigenfunction. You can simply rule out an eigenfunction that decays exponentially, and that's good enough. I don't want to get into the floquet theory, though, so I'll just skip over this point. Theory, though, so I'll just skip over this point. But the first step is a reduction. I'm going to use the w variable trick to put on the function u tilde. And the idea here is that I'm going to find a place over here in the top right of the image where u tilde is very small. And I'm going to open a Taylor expansion there and try to argue that it can't be equal to one at the origin. Okay, so let me give a little bit. Okay, so let me give a little bit more of a cartoon. There's going to be three balls involved. There's kind of a big giant ball of radius capital K times R where you anchor your estimates, your regularity estimates. You have a small ball, which you're going to use to kind of estimate the sizes of the coefficients in the Taylor expansion. And you're going to see that that polynomial cannot be too big. That polynomial cannot be too big at the origin because it's so small in this ball. But the function is equal to one at the origin. The polynomial is basically 10 to the minus 6. And so they're a part by one, but they can't be a part by one if this capital K is really large. And if the solution U has exponential growth in the X D direction, which is smaller than the exponential decay constant. And so you can rule this. And so you can rule this out. Okay, so that's sort of the cartoon proof. So it's a kind of application of unique continuation. There are three balls in this argument, but it only uses the analyticity estimates for the Laplacian. You can also use the, of course, the three-ball theorem. It works really well here, and it gives you the same. So let me talk for a second about doubling inequalities because doubling inequalities are a nice way of quantifying. These are a nice way of quantifying unique continuation. For harmonic function, if you look at the ratio of the L2 norm of U into the normalized L2 norm of U in two balls of comparable radii, then this, and you look at that as a function of R, this doubling ratio is monotone. This is almost the same as the Almgren monotonicity formula. You get it out of Almgren monotonicity formula in one line, but it's this is not. Line, but it's this is not very hard to prove. You can use the Alemgren monotonicity formula calculation, or you can just, it's basically a consequence of the orthogonality of harmonic polynomials, that a harmonic function is in kind of infinite harmonic polynomial in a ball, and harmonic polynomials of different homogeneities are orthogonal in the ball, and that leads to this thing being increasing. Anyway, this is called a doubling inequality, or more precisely. A doubling inequality, or more precisely, it's a doubling inequality would be some inequality for a function which says that the doubling ratio on a large scale controls the doubling ratio on a small scale. So, in particular, that's a quantification of unique continuation because if a function is zero in a ball, then that sort of means the doubling ratio is infinity because the small ball is in the denominator. And if this, if the doubling ratio on a big ball controls the doubling ratio on a small ball, that can't. Controls the doubling ratio in a small ball that can't happen. Okay, and this is actually what you need: such a doubling inequality. You could rewrite the previous argument of the previous slide in terms of that. So we want to prove such things for periodic operators. And again, I said that this is a consequence of the almography monotonicity formula, but the almond monotonicity formula is not going to work for periodic operators because even if the coefficients are smooth, because I want the capital R. Smooth because I want the capital R to be, or the small R, I guess, to be much, much bigger than the period. So you can't try to adapt the algorithm monotonicity formula. You have to instead adapt the proof that harmonic polynomials, you know, the orthogonality of harmonic polynomials. So anyway, we're going to have to talk about A of X harmonic polynomials, whatever that means. And so let me talk about that. So regularity for periodic options. So, regularity for periodic operators. So, this is the work of my colleagues, Avaloneda and Lynn, when they were postdocs, I think, in the 1980s. If you have a periodic operator, then you have a notion of, or you can classify, let's say, all solutions of the equation in the whole space, which grow at most like a polynomial. And this is a linear space, and it's a space. Space, and it's a space of solutions, and it's a nice space to use. You can replace the role of polynomials for the Laplacian with these things. And I call them, this is probably a terrible name, but I call them heterogeneous polynomials because they're like polynomials with little wiggles in them. Okay, this is where I would draw a picture if I still had my iPad working. Anyway, this space AM here is the space of solutions which grow at most like a polynomial of degree m. It has Polynomial of degree m. It has the same dimension as harmonic polynomials of degree less than or equal to m. And you can prove a quote large-scale CM1 estimate, which is kind of analogous to a bound on the, I guess, the m plus 1th derivative at the origin or something. And it says that the closest, that if you have a solution now in a finite ball BR, that you can approximate that solution by. You can approximate that solution by one of these quote heterogeneous polynomials in smaller balls, and you get the ratio of the link scales to the power m plus one on the right-hand side with the normalized L2 norm in the largest ball, but there's a constant that depends on m and you don't really know the dependence on m. At least they didn't know at the time because their proof was by compactness, and so they didn't see, they didn't know what any of the constants were. Okay, but they're but for. Okay, but there, but for every m, there is such an estimate. And of course, if you want to speak of analyticity, you need to quantify exactly the dependence of m of this constant. And so this is what Charlie and Tuomo and I decided to do, was to try to quantify the dependence of m in this cm. And if it would be like Laplacian, then the Cm should be some C to the M. It should be exponential in M so that I could then put the C sub M inside. I could then put the C sub M inside that parentheses there and get C little R over big R to the M plus one. So that's the goal. Scott, yes. One question. How do you normally prove this? You rescale by a power of radius equals to m or m plus one and apply. You can't rescale. No, you can't rescue because it's periodic. So when there's a periodicity, you fix the scale. You fix the scale. So how is this proved? So, how is this proved? Well, okay, the basic idea is this. If the coefficients would be smooth, so this estimate only, you only care about it when the radius r, even the small r, is at least 100, because you really only care about this on large scales. On small scales, you don't see so many periods of the periodicity anyway. And so, what happens on those scales is just shouter, and you go with what your assumptions give you as far as the smoothness of the coefficients. As far as the smoothness of the coefficients. But we're speaking of periodicity. And so here we're really thinking about it in terms of large scale. So you can't scale. What you have to do is you have to do a shouder-like iteration starting from a very large scale down to a still large but smaller scale. And in the course of that iteration, in every ball, you replace U with a homogenized solution. So it's like shouter, whereas instead of freezing the coefficients to get your Instead of freezing the coefficients to get your comparison function, which is harmonic, you use homogenization to get your comparison function, which is harmonic. And you use the homogenization estimates to see that it's close, and you iterate down the scales. So that's how you prove it. Is that clear? I'm kind of fast. Yeah, it's like, no, no, just to understand how the marmonic polynomial comes up. And okay, so yeah, so you have these. Okay, so you have these. You have these, okay. So, you have these polynomials, and I thought that you were some induction in the after scaling. I mean, well, I'm going to reproof it and then I'm going to kind of sketch the reproof in the talk, so maybe I'll explain it. Yeah. So, these things are just, I mean, you want your set of polynomials to be solutions. So, when you subtract them from your function, you still have a solution, right? That's the first thing you want. And the second thing you want is that you have enough of them that you can approximate your solution. Yeah. Okay. But the proof of this is not. Okay, but the proof of this is not too bad. Oops, what happened? Okay, sorry. The proof of this is not too bad. Okay, something still is. I'm sorry, I'm having all kinds of computer problems. If you want to just get this with some implicit dependence in M, the proof is not so bad, is all I wanted to say. Okay, so as I was saying, you need to quantify this in M. The previous proofs, if you were to quantify them, Proofs, if you were to quantify them. So Marco and Fengwa's proof was unquantifiable. But later, Charlie and I found a proof in the stochastic case, which allowed this whole thing to work in stochastic homogenization and led to the regularity theory and stochastic homogenization. And if you use our proof on the periodic operator naively, then you would get that the dependence is C to the M squared. The M squared is obviously all in the X. The m squared is obviously all in the exponential of c. And so that would give you c to the m times little r divided by big r, all raised to the power m. And that's a disaster because that's only useful you see when this big parentheses is less than a half at least, or at least less than one, because otherwise it's a big number, not a small number, and that's useless. So you at least would need the ratio of the link scales to be C to the M. link scales to be c to the m exponential in m. So if you're looking at a scale r, the order of the Taylor polynomial that you can make there is proportional to log r, and that's a disaster. And that doesn't allow any of the arguments on the previous slides to work. Okay, so we're hoping to get that m squared to be m and in that much we succeeded. So that's at least we at least succeeded in doing that. So we prove for the periodic operator. Proved for the periodic operator that it can be C to the M. And so, and so that is, we proved a kind of large scale analyticity. So, we call it large scale because it really is talking about scales larger than, I mean, in this case, actually, the theorem has it explicitly. So this only works when the little scale R at the bottom in dark red is bigger than some large constant times m. So, as you increase the order, you have to the You have to, the scales on which this work increase. So you're allowed to take an nth order Taylor polynomial to approximate a solution in the ball of radius proportional to m. So instead of log m, you get m, and that's what you should get. And that's not as big of a disaster, I guess. And this matches the bound for the harmonic function in terms of the constant on the right-hand side. And so in that sense, it's sharp. And we can prove that this. We can prove that this dark red box at the bottom, which restricts the little r, that's sharp. You cannot reduce that to little r equal m over 100. That can't be true. There's a counterexample. Okay. And the reason there's a counterexample is that I've nowhere, this does not use the smoothness of the coefficients. So we have proven analyticity, but the coefficients here can be just measurable, periodic, but just measurable. Periodic, but just measurable. So I'm not looking at small scales. I'm only looking at large scales. Okay, so on large scales, it's quote, acting like I have analytic estimates, even if the coefficients are measurable. And if I could get this CM to be epsilon m, then I would actually be able to apply it to the eigenvalue problem and disprove the counterexample that Philonoff built. So Philonoff's counterexample is actually a counterexample that I can't improve this theorem. Example, that I can't improve this theorem. So, in that sense, we proved the best theorem for regularity for these periodic equations. I should say that you can think about these polynomials. I think Rosario, you asked me how do you think about these polynomials? These polynomials can be thought of as very, very high order two-scale expansions. So, the affine polynomials are the correctors in homogenization. No, no, their role is clear. Their role is clear because. Everyone is clear because you subtract higher order and then you get so these polynomials, the polynomials in this AM are really like mth order corrector expansions in homogenization. And so this in some sense is proving like so I'm going to build them in a second. Well, quickly, I mean, I'm starting to, my talk is not long, but these things are, so in other words, this is like an error. In other words, this is like an error estimate in homogenization for very, very high-order two-scale expansions, which is sharp. So, in this sense, this theorem is really the best possible result in periodic homogenization that you could ever prove, at least for this simple equation. Okay, so it's like the sharpest, it's like the absolutely sharpest result in periodic homogenization. It leads to three ball theorems. So, when we first did this, this is an easy corollary that comes in half a page. In half a page. So you see that you estimate on the left-hand side, the norm of U in the middle ball by the product of the square roots of the small ball and the big ball, almost. There's this little delta here. In the subsequent paper, we remove the delta. But when we first wrote the paper, we had the delta. And this was the title of my talk was based on this: is that we have a new. Based on this, is that we have a new result, which now uses the theory that I'm discussing, this large-scale analyticity, to prove an absolutely sharp doubling inequality and three-ball inequality for the periodic guys. Okay, just really quickly, let me go over this quickly. I'm not going to be able to give more than basically half of this talk, but if you assume that you have some large-scale doubling ratio under control, capital. Doubling ratio under control, capital M here in red at the top on the largest scale. And your balls are not balls, they're actually a bar, where a bar is the homogenized matrix, ellipsoids. And you have to use that, right? Because this is going to be the set on which you have the mean value property almost for your equation. You have to make the ratio be very small, so theta. And it's only going to work on link scales on which the doubling constant is a small x. Is a small exponential of R. So it's exponential of little c times r. Okay. And that's the point. This is where the main point I should have given it a color is that this works on scales in which it's the logarithm of the doubling constant. That is, your solution is really exhibiting exponential-like growth, but only for a small C. So what do we get? We get that you can go all the way down. You start with the assumption on the big ball. You can go all the way all the way down to as small a ball as you'd like. To as small a ball as you'd like, provided it's still bigger than log m. And you don't even really change the doubling constant very much. It's almost a power of the doubling constant, but it's not quite. There's an iterated logarithm, which we don't aren't sure if it can be removed, but we think it can't. And this iterated logarithm is due to some higher-order homogenized tensors that pop up starting at order four. And at some point, we were doing the calculations and we got a Point, we were doing the calculations, and we got a fraction that was nine over eight, and we needed it to be less than one. So, if nine eight over eight was less than one, then I could remove this iterated log, but it's not. So, we're and we were not sure if this is an artifact of our proof or not. Anyway, at least up to this iterated logarithm, this is sharp. You certainly can't go below scale, C log M. Previously, Kinnig, Zhu, and Zug had used our large-scale analyticity to try to prove a result like this, but they had an extra. Result like this, but they had an extra exponential factor. And so instead of having m to constant log log m, they had exponential of m. And the exponential of m kind of renders the inequality much less useful. So we went back and then said, okay, we're going to get rid of the delta there so that it's really square root, square root, and get the constant correct. And so that's what we did in July. So that's kind of the fine, that's the best you can possibly do using period. Do using periodicity. So, what does this give? Well, it gives you no embedded eigenvalues at the bottom of the spectrum. So, it's not nothing. You can rule out that there are, you know, you have delocalization at the bottom of the spectrum. The reason that this is kind of sad is that I think after we figured this out, the guy in the next office heard me talking through the walls because I talk too loudly when I'm on the phone. Jonathan Goodman, my colleague, and he knocked on my door. Jonathan Goodman, my colleague, and he knocked on my door and said, I heard the theorem you proved through the wall. I think you can prove that with block waves. And then he subsequently hand wrote on like, you know, 12 handwritten pages a proof that uses floquet theory and block waves. And so I think this result is actually folklore. We were happy when we proved it before we realized that probably it was already known, but we can't find it written anywhere. But anyway, this is what our result gives. It gives the bottom of the spectrum. And to go higher in the spectrum, we need new ideas. Spectrum, we need new ideas. In particular, we're not using the smoothness of the coefficients. And there's a counterexample if you don't use that. So our result has to be restricted to the bottom of the spectrum. So the problem is still open. In the Russian school, they have many special cases solved. So like if the coefficient, I mean, if the matrix has a very particular form, they can solve the problem. Like if in the, like if the matrix is in a block diagonal form and there's one It's in a block diagonal form, and there's one special direction such that in all the other variables it doesn't depend on that direction and things like that, then they can solve it. But it's still open. Okay, so what needs to be done is to figure out how to combine the smoothness of the coefficients with the, you know, the periodicity is giving you doubling kind of down to some scale, right? It gives you doubling, looking at the previous slide, down to scale C log. slide down to scale C log M. But if you're in the case where your function is growing exponentially, C log M is still a big number. It's still very large. So it's not actually going down to very small scale. And you may think, okay, like, you know, by Garofilo Lin and things like this, we have unique continuation for Lipschitz coefficients. So what I should do is I should use periodicity to go down to some scale and then use Lipschitz to go down all the way down to the pointwise level and have, you know. level and have, you know, and have my thing proved. The problem is, is that I can't go from scale C log M to scale one. And C log M can be a thousand. So there's like a little intermediate range of scales that I can't cover. And that's the reason why we're stuck. Anyway, let me talk a little bit. So I have, I've been talking for half an hour. I'm supposed to talk for 35 minutes. So I have five minutes to tell you about the proof. So maybe I won't tell you about the proof, except I'll skip. Proof, except I'll sketch it. So, in order to do the proof properly, I have to introduce tensor notation, but I won't. I'll skip it and I'll just flash some slides. So, you have to build these polynomials, and you build the polynomials by constructing the correctors. It turns out that the homogenized equation they satisfy is not really just the second-order homogenized equation that people from homogenization know. There's actually this homogenized equation with infinitely many. Infinitely many homogenized tensors in it with very, very high order effects, which are sort of dispersive terms somehow. And these terms don't matter typically in periodic homogenization because they affect the very high derivatives, but they have powers of epsilon in front of them. So I'll look at the next slide and show you that if you scaled everything with the typical epsilon that people decorate their results with in periodic homogenization, you would see that the operator, the A-bar K on the on this right-hand side, The a-bar k on the on this right-hand side has epsilon to the k minus 2 in front of it. So if your solution is not, it doesn't have derivatives blowing up or something, then this epsilon to the k will kind of allow you to ignore it and you can prove homogenization and not care. But if your function is growing exponentially, then this kth derivative of p can counterbalance the epsilon to the k, and actually this thing can blow up. And this is sort of natural because really you shouldn't be able to use. Sort of natural because really, you shouldn't be able to use homogenization to approximate solutions which are growing faster than exponentially than a slow exponential. Anyway, there are these dispersive terms. So you build, okay, I'll skip the construction of the correctors. I have to parametrize them somehow. I just want to, okay, I'll skip this. Let me just tell you the two basic lemmas that you. Tell you the two basic limas that you need to do, or these are two basic limmas that are used in the proof. One is that I have this estimate, which looks suspiciously like the analyticity estimate. This estimate is only for finite differences. This capital D is now the unit finite difference operator. Like it's the same size as the period. So, because of you can use periodicity and you can sort of do finite differences with exactly the period, and then you then you can sort of find. You then you can sort of you know finite difference right through the equation because um the operator is periodic. So if you shift the solution by one, it's still a solution of the same equation. And you can use that to get estimates on these finite differences, which because we're thinking about very large scales, the unit finite difference operator is still almost kind of like an infinitesimal derivative. So we can easily control using periodicity these low frequencies, the frequencies that are, you know, I can get a bound. Frequencies that are, you know, I can get a bound on the finite differences of the solution with finite difference one. And then I need to control the very small scales. And we do that with this sort of like small scale Poincare, where you can control the L2 norm of the solution by the discrete L2 norm, where this hat guy is the kind of average over one cell. And you pay a price of the gradient. However, this gradient does not have an R in front of it, right? Front of it, right? For so for large capital R, this has a good scaling compared to the left-hand side, and we should be able to use catchopoli to eat it. And that's actually how the proof works. I really don't have time to explain this. So let, I mean, I had a nice slide here that explains at least how it would work in the case of C01 and C11, where you just, the whole proof is on one slide. So this is a proof of Avelneta Lin without using the proof I was telling earlier in answer to results. Earlier, in answer to Rosario's question, it's all here on the slide. You just use catchopoli to absorb that error term, and you get an approximation of u by a constant is decaying like you expect, no matter how large this r is. Oops, what is going on? Okay. And this can be done. So, the talking for the C11 estimate, you can do the same proof. I just barely changed anything. I just barely changed anything. I just erased constant and put elements of A1. But then, when you have larger than 1, when you start having CK for K bigger than or equal to 2, you have a problem because you need to. Yeah, the problem is that if you choose the right polynomial so that you make a Taylor expansion, you don't know that the right-hand side is actually zero anymore because these polynomials that I constructed actually. Polynomials that I constructed actually have right-hand sides, which are constants or are actual polynomials. And they're not always zero. If you want to choose the ones that are zero, then you can't do the proof like this. So if you want to choose the polynomial that has the same sort of finite differences as u, which are like the Taylor polynomial, then you have to worry about the right-hand side, and then that messes up your catchopoly. And so we have a funny estimate that estimates the right-hand side by the left-hand side, and you need to do that to finish the proof. And this is analogous to. And this is analogous to kind of like doing a blow-up. You know, if you want to prove that the Taylor approximation for a harmonic function is harmonic, you have to do some kind of blow-up argument to see that. And again, use the homogeneity or something of each piece of the Taylor series. So, okay, so I'll skip the rest of this. I didn't really get to the punchline, but thanks for listening. The punchline, but thanks for listening to my talk, and I'll stop here. Okay, thanks a lot to Scott for this beautiful and psychedelic, I would say, talk with all these colours and I'll take it as a compliment. Of course, it is. Finally, something, I mean, colorful. Are there questions, remarks? Yeah, maybe I have something more to say. Yeah, this is the this one. This was exactly the point I was wondering if you have a okay, first order is trivial, but higher order, then there are more things because first order is just catch up. And the first thing I thought is that you could use something like Lubid property or after blowing up. Well, this we're proving the leval property, right? The level property is like the qualitative version of the C of the CK1 estimate, which is a quantity. Yeah, exactly. The Levit property typically comes when you have a suitable catch-op for your type of IOT. Then you can prove some rigidity. When you have some growth, then you can prove that there are only finite number of possibilities. Let's say these are the polynomials that you're talking about. Yeah. But I was wondering. But I was wondering, yeah, but yeah, okay, so yeah, at some point, the funny part about the iteration, I'll just tell you, is that you can't use the iteration in balls. It doesn't work. You have to use the iteration in balls or cubes, which have a rounded kind of fattened boundary on the square root scale. If you use more than the square root or less than the square root, your proof doesn't work. We were stuck on this for several weeks. And why this? This is an interesting technology. So the reason that it has. Interesting technology. So, the reason that it happens is that you need to approximate a polynomial by its gradient in the same set. And if you try to use, because you're not allowed, you're not allowed to inflate the ball by a factor of two. You're not allowed. And so you have this inequality that you need to prove. So, what you need is this inequality in white here for just normal polynomials. Okay. And this turns out to not be true. Actually, what's true is this inequality. Actually, what's true is this inequality if you have m to the fourth. And that breaks our proof. And so, instead, but if instead of, and the reason is that you can have a polynomial, like a Chebyshev polynomial, that does crazy things near the boundary. But if you kind of smoothen out the boundary, then you can kind of make that not happen. And so that we had to do that. And that's the most interesting part of the proof, I think. And after a year, we haven't succeeded in simplifying that. Actually, is it longer than a year? Yeah, it's more than a year now. It's more than a year now, and we thought that this proof we'd eventually find a better proof, but we haven't found one. So it's a bit of a strange point. Anyway, okay, are there further questions or people in Europe are about to okay? So, if this is not the case, I would like to thank Scott again for this very nice talk. And Andrea, I think we can update. I think we can update to tomorrow, I think, right? Tomorrow, yes. Yeah. Tomorrow, seventh.