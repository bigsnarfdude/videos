Workshop. This is not an area that I'm very familiar with. That's why I really enjoy the workshop that I've learned a lot and motivated me to work a little bit more in this area before after this workshop. So I want to thank Liang Wu for invited me initially and thanks Leila for asking me to give a talk so that I get opportunity to present. I get an opportunity to present my very limited work related to surrogate endpoint, in particular the progression-free survival. And this is a joint work with long-term collaborator Richard Cook and also Lang and Audrey were former graduate students at the University of Waterloo. I know everyone here are very familiar with progression-free survival. With progression-free survival, but as a prelude, I will give a brief intro. In particular, I want to talk about a conventional approach that being used to deal with the intermittent observation of the progression component for this competent endpoint. And I will explain why that is a problem. And then I will formally investigate the issue, in particular, looking at the asymptotic bias. At the asymptotic bias and the loose of power associated with these conventional approaches. And then lastly, I will talk about the design of the clinical trials with the PFS as a primary endpoint that can account for the intermittent observation on the progression component. So in many cancers, So, in many cancer, actually, in many clinical trials for chronic disease, often the events of interest include the disease progression or comobate conditions and death. So, here I'm using this three-state diagram to illustrate the disease process where at the baseline, everyone are alive and progression-free. Then, later on, we may progress. They may progress, they'll move to stay one year. Of course, they may die after the progression, so move on to the death state. Of course, the patients could die without the progression, so this zero to two path is possible as well. Now, in terms of evaluating the treatment effect, the primary endpoint of interest is overall survival, which is a time from randomization to Time from randomization to death. So, in this diagram, we'll be getting into state two here. We know that it takes time to observe death, and also it could be rare. That's why an alternative or convenient endpoint will be the progression. One question we ask when we use progression as the endpoint is: how do we deal with deaths then? Then, typically, the patients who The patients who die without progression will be right-censored at the death time. Then, the problem with that approach is then, you know, the interpretation is harder because we basically assume people can progress after the death. And in addition, if the death and the progression are related, this approach induces what we call informative censoring and invalid inference. Inference. So, this leads to the rise or the popularity of the composite endpoint progression-free survival, defined as a time to the first of the progression and death. It became increasingly important and popular, commonly used endpoint in cancer trials. In terms of analysis, the In terms of analysis, the standard survival techniques are typically directly applied on the competent endpoint, the PFS. However, there's some implicit assumptions that worry or need some careful consideration. In particular, if we're thinking of fitting a Cox pH model on the COMPS endpoint, then that proportional hazard assumption. Assumption might be validated. So, here again, in this three-state diagram, if we assume the proportionality holds for the hazard or the intensity for progression, and it also holds for the death, then we know the hazard for the composite of the two, the PFS, is a sum of these two intensities. This then become really clear. then become really clear that the proportionality for the PFS is only true under one of these two conditions. That is, if we have a common treatment effect on progression and the death, or the baseline intensities for the progression and deaths are proportional to each other. If none of these conditions are satisfied, If none of these conditions are satisfied, then the fitting the Cox pH model on the comma's endpoint won't be appropriate. There has been much discussion in the literature about the utility and interpretation of using the PFS in clinical trials. I listed a lot of papers here, discuss the implicit assumption on. Implicit assumption on you know PFS as a surrogate for that overall survival, and there are also other papers look into the structural nature of the relationship between PFS and overall survival. I want to clarify here that our intention is not to debate whether PFS is the most suitable endpoint to evaluate the treatment effect in cancer trials. In cancer trials. Instead, we assume, yeah, we're using PFS regardless. And also, let's assume the common treatment effect on progression and the death so that this Cox model is still valid to use. Our focus here is really on the issue related to that intermittent observation of the progression component and investigate the impact of that on analysis and the design. That on analysis and the design of the trials. So, as I mentioned, the intermittent progression observation. So, in practice, I mean, it's really hard to monitor the progression continuously. And most of the time, it's periodically evaluated at a set of fixed time points. Therefore, progression is really subject to interval censoring. On the other hand, the vital status is often monitored continuously. Is often monitored continuously and subject to right censoring. Now, therefore, for this composite endpoint, it's under this sort of hybrid censoring scheme. Then the in practice and the conventional approach to deal with this in the PFS analysis is to use a observed PFS time, which either take that assessment time if progression is detected there or the At there, or the death time, if no evidence of progression prior to death. So, we'll assume the death occurred first. So, this I call this surrogate time, but the surrogate here really means this is an observed version of the true PFS time, which is not necessarily the same as that. Then, this surrogate PFS time is routinely used as a true PFS. As a true PFS time in the standard survival analysis in practice. This is obviously problematic, but I don't think there has been too much work in looking to how big a problem that really is. As an example here, I'm showing you a paper published in New England Journal of Medicine on a prostate cancer trial. Prostate cancer trial where the progression was evaluated based on the MRI and the bone scan at scheduled at some time points. The results were reported in these Kaplan-Merrick curves on the progression, time to progression on the left and on the progression-free survival on the right-hand side. You right away see this stepwise drop in the survival, which In a survival, which mainly because they're using the surrogate event time that basically use the right endpoint imputation. So, using the test, the assessment time when the progression is detected. So, that's why you see it look like this. So, our first objective is really to investigate the asymptotic bias. Bias in the estimators associated with the progression, the conventional approach where progression is intermittent observed and the surrogate time is used for analysis. We do this using this illness death model framework. And thanks to Emily, that she already talked about this yesterday. You're not very, you're not. Not very, you're not familiar with it. So I give a little bit of notation here: that T denotes the state occupied at a time. Then H T is a full history of that disease process up to but not include time t. Now this three-state process then is fully characterized by this transition intensities defined here as lambda j k. You understand it as a You understand it as an instantaneous risk making that J to K transition at time T. Now, often we like to use Markov models here in the sense that we assume the dependence of this transition intensity on the history is only through the state occupied right before this time t. In that case, we can simply write this transition intensity as. We write this transition intensity as lambda j k t here. Now, the nice thing about Markov model is that then we can use the standard Como growth forward differential equation to get the transition probabilities based on this transition intensities, which facilitate the construction of a tractable likelihood function for the parameter estimation and that allow mixed types of sensoring schemes. Censoring schemes on the different components. So, now then, under this Jonas-DETS model, we can derive the distribution for the PFS time. We know that the progression-free survival time is simply the time of leaving that state zero. Okay, so in that sense, the survival function of the true PFS time. Of the true PFS time is simply the probability of staying in that state zero at time t. So not being progressed, not by yet. This has an explicit expression in this exponential form. Therefore, you can easily get out the hazard for the PFS endpoint. It's simply the sum of these two transition intensities for progression and the death. So, as I showed previously. So, now what is the distribution of the surrogate time under the right endpoint imputation when we have intermittent observation on the progression? I use the S denote the surrogate time, which will take one of this discrete assessment times if progression is detected, or it takes that death time if no progression is observed prior to death. So, to derive the distribution. So, to derive the distribution for this surrogate time point, event time, we consider two different scenarios. The first one will be that randomized trial setting where we pretty much preset the fixed assessment times. And then this surrogate time S is really a mixed type random variable whose cumulative distribution function is a sum of the two parts. The first part is a continuous part. The first part is a continuous part, which basically represents the probability of observing a path that the individual will stay progression-free all the way to the last assessment prior to deaths being observed. Though here we have a summation just to account for the possibility of either the individual will progress before the deaths but not observed or not progressed at all. Then the second part. Then the second part is the probability mass of observing the progression at one of those finite set of discrete time points. Well, fundamentally, this CDF function is a time-varying function that involves all the transition intensities that I just defined for that illness-death model. So, with this, then we contrast the Then we contrast the survival curve for the true progression-free survival time, and which is in this black solid line, and then for the surrogate PFS time. Now you can see that very distinctive stepwise downward pattern. The curve for the surrogate time sits above the true curve. That means we're going to overestimate the survival. Overestimating the survival if we use a surrogate even time under this observation scheme. And this, the bias could be less if we have more frequent assessment on the progression by moving from left, there are only four assessments to double that eight assessment case. So the next second scenario we look at is a cohort. We looked at is a cohort studies where it's more likely the assessment times are random, not fixed. In those cases, we extend our multi-state model framework to jointly consider the disease process and the assessment process. So, the way to look at this is at the baseline, the patient is alive and progression. Is alive and progression-free. So from there, the patient may firstly progress and they will move to this P state, or they may die without progression, so move to the D state, or their next assessment happened. So they will move to this V state. Now, if the individual progressed first, then after that, another assessment can occur that detect this progression that will To this progression, that will be this VP state, then the assessment process on progression will stop because we already observed the event. Of course, the patient may die without before the next assessment happens so that they will move to this D state. Now, if the individual neither progress or die, then this assessment process can continue. So, have the next assessment, then as indicated. Than as indicated as this horizontal arrow here. So, with this more like elaborated or extended multi-state model, we will be able to write out the survival function now for that surrogate time S, which basically the probability of being in this VP state, that means progression detected at one of those assessments. And or And or they're in this one of the C states, that means that's observed after some assessments without any indication of progression. So again, this function will be a complex function that involves the transition intensities for those disease, for that three-state model for the disease process, and also the intensity for the assessment. Intensity for the assessment process as well. So, with that, we'll be able to again compare the survival function for the true PFS time, which indicated by the solid curve here, and then for the surrogate PFS time in the dashed curve, which is much smoother in this case because we're assuming the assessment times are random, therefore. Therefore, we can see again the overestimation of the survival becomes clearer. In particular, the vertical bar here denotes the estimated median survival time, which can be overestimated by a factor of 2.6 times if we use a surrogate event time to construct these curves. Well, of course, in the quality of Well, of course, in the clinical trial, we're interested in the treatment effect and just estimating the survival. So, as I mentioned earlier, in order for this Cox pH model to be valid for the composite PFS endpoint, we have to assume a common treatment effect on progression and death. So, we do here assume that's true. Then, this beta will be that true treatment effect on PFS. However, you know. However, you know, under the intermittent observation, if we're using that survey time as a fit of working cox model here, then this will be a misspecified model. Then to differentiate, I use gamma here to denote that naive treatment effect that you're going to estimate from this misspecified model using the surrogate time. Now the standard partial score equation can be used to estimate the naive treatment effect of gamma. naive treatment effect of gamma denote by gamma hep here which will now converge to some value of gamma star which will be different from that true treatment effect beta then this difference is asymptotic bias we can also work out the asymptotic normality for that naive estimator gamma hat get the asymptotic variance covariance matrix now with this result then we'll be able to Result, then we'll be able to display the asymptotic percent relative bias on the top here against the average number of assessments on the left and the probability of a progression on the right-hand side. So, mind you, here, the top is zero here. So, we can see the asymptotic bias reduces as the number of assessments increase. Increase when we're using the surrogate time for the analysis, but the bias increases if the probability of progression increases. We can pretty much observe the same pattern for the asymptotic power displayed at the bottom. The nominal level is actually 80%, so there's definitely lots of power. When you use surrogate time S for the analysis. Time s for the analysis, the lowest can be 65 percent. So, 15 percent would drop in the power for the worst scenario in the settings that we considered here. So, that really brings us to the oh, I'm running out of time. So, you started late. I'm almost at the end. Hanging there. I don't want to stand between you and your lunch. Um, so the last part is really about the design. The last part is really about the design. So, the question is: how do we design a cancer trial that you're going to use the PFS as the endpoint, but the progression will be really assessed intermittently. Now, in a conventional approach where you're thinking of using Cox model, but analysis eventually, and then the standard sample size calculation for the world test for the treatment effect is given here. Treatment effect is given here. Now, this is only valid to use if we assume that event time will be exactly observed or right-censored. But this won't be the case for PFS because, as I said, progression is intermittently assessed. So if we use this calculation, then we'll be not getting the desired power that we're going to have with this sample size. So, what we propose then is really to derive What we propose then is really to derive the sample size based on this, the multi-state models that can jointly consider progression and deaths also account for different mixed type of sensoring on the progression and death component. So we will be able to write out the likelihood function, which is the probability of observing certain paths in that disease process over the observation period. Um, and then we can get the facial information matrix, which is ultimately a function that depends on the parameters in a multi-state model, a number of assessments, and the censoring rate, and the study duration. Now, with that, if we're interested, particularly the treatment effect on the PFS, then we just need to pull out the asymptotic variants for the Asymptotic variance for that estimator beta hat, which is one of the diagonal elements in that fisher inverse of that fisher information matrix, then just plug into the standard sample size calculation calculator for the world test. So here is this, we did empirical studies to evaluate the performance of the sample size and also compare it with other approaches. It with other approaches. This is just the settings that we considered for simulation. So just spend a little time explaining the results here. So we consider three different approaches for calculating the sample size. The first is a conventional approach using the Cox model, assuming we will exactly observe PSS time. Okay, now so the sample size reported here under the N star. Here under the n-star. Then we simulate the event time used according to the sample size, and we exactly fit that Cox model on the truly observed PFS time, report the empirical power here. Then you see that's close to the nominal level as one would expect it. But that's not the reality. Reality is we have intermittent observation on progression. Therefore, we analyze, we create the surrogate event time using the right endpoint expectation. Using the right endpoint imputation, as in a conventional approach, then we test the treatment effect reported in piracy power here according to the sample size. You see, this is underpower here. Lastly, we did the correct analysis using the multi-state model, account for the intermittent observation. Well, this approach does give us the unbiased estimates, but you see a slightly lower power. That's mainly because of the loss of information. Because of the loss of information due to the interval censoring on progression, compared to this approach where we assume that we exactly observe the PFS time. Now, the center part is really related to our proposed sample size calculation based on the multi-state models. You see, the sample size here is larger compared to, you know, when you assume you won't know the PFS time. And then the empirical power is close to the nominal level. Close to the nominal level if we use a multi-state model to analyze data and do the test. Well, lastly, we also considered a sort of sample size justification based on the misspecified Cox model. This approach also works well in the sense that you can see the integral power here is close to the nominal level. The only thing I want to point out is the sample size ends up to be bigger than the one based on our multi-state model. So finally, we applied this to design a future trial based on the observed data from a metastatic breast cancer trial. In this case, the 378 breast cancer patients under the treatment or control and the progression was evaluated through the radiographic surveys. Geographic surveys that was done at some prefixed times. There is an appreciable proportion of progression and mortality in that data. So we fit an Iona stats model to the data, then get the estimate of those baseline transition intensities and the treatment effect. Then we use this to design a future trial in the sense that we want to. In the sense that we want to test the effect of the drug on the progression-free survival, where we assume 890 days of total follow-up. We assume a random censoring due to loss follow-up. We consider five or ten assessments on the progression. So, this table reports the sample size required and the different settings. Different settings. So, just to illustrate that it works, we do not really have a package for the sample size calculation, but I'd be happy to provide the code for a function that just requires some standard plugins and to give you the required sample size. So, here are some concluding remarks or messages that is the impact of Impact of using a surrogate time with the right endpoint imputation can be substantial with the loss of the power range between 3 to 15 percent. The multi-state model is a more natural framework for the joint consideration of the multiple events, and it can accommodate a different censoring scheme for the components. Therefore, the sample size can be derived based on such models. So, I think that's all I want to talk about. Thank you very much for your attention. Thank you very much. Are there any questions for Hi, Lele. This is quite interesting. So, I have been working with survival time, especially in top-centered data law. So, I wonder what you're thinking about. So, this is really interesting framing this like intermediate observation time as like a surrogate marker. So, I wonder what's your take about like different approaches for intosensor data. So, I guess the yeah, I think there are approaches available for. Approaches available for dealing with interval sensoring if you just look at one event in a typical survival setting. So, this is more for the case where you have multiple events and you may have different, you know, the absorbing state, the deaths is often only right-censored, but the other transient states that in that disease process can be intermittently observed most of the time. Observed most of the time in practice. So, I guess the message is: you know, if you think there are associations among these events or the conditions that involved in this disease process, you do want to consider them jointly. And then you can use a multi-state model where that can account for different types of censoring in those components. So, that would be one of the selling. Uh, one of the selling peach for money state models, uh, yeah, thanks for your talk. I think this is a really important consideration. And I had a question about sort of a related consideration. Maybe in the setting where you have information about progression-free survival for a certain amount of time, someone may drop out of the study, but you may still have information about death from like death certificates. Like death certificates. I wonder: have you either considered that in your method or seen that in practice with trials or cohort studies? Yeah, that's a very good question. So one way you can with the auxiliary information about the disease process, you can think about the combining different sorts of data that augment that likelihood by the group where you. By the group where you only know the deaths, but you don't have any information in between on progression. But in terms of multi-state models, you can think about there could be possible paths of either going through the progression or not going through progression. Of course, this auxiliary cohort does not have too much information on progression, but if you combine that with the data that you have information on. That you have information on both, that certainly will help in terms of, I think it will increase the efficiency in the estimation. So, I was looking at the approach to combining the information from different cohorts, especially for multi-state data, because different cohorts may be sampled differently. I think Tanya's talk was interesting because she was mentioning people may start at different stages of that disease. So, you can think of that. Of that disease, so you can think of that the sampling condition if you put in this multi-state framework, and then you have different follow-up lengths, and then you may observe different information. You will have different information related to different transitions in that multi-state model framework. Some cohorts are more informative for certain transitions, some are less so, but try to combine them and take into consideration the sampling conditions. Consideration of the sampling condition, I think it's one area that I'm currently quite interested in. Yeah. Are there any other questions? Not, thank you, Lily. And lunch. We'll be back at two. Stay tuned. 