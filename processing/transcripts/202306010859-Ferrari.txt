Uh first this day would look like I was instructed to do an extra team out of you. I am really happy to be here in this meeting, especially for Timo. I met the team in 1995. I was looking at the emails from 95 to now about the simplest structure process and other process hydrodynamics, several things. It is was really great to have a It was really great to have uh film as a comrade in this group of uh ISL hydrogen and its etc. Okay, this today I will talk about something that is slightly out of uh because the hidden temperature in the KMT model. Somehow I am using some things, some tools that are uh cut from the HammerFlex process. So, this is work with Anna Damasi and David Gabriel. They are from Blackwilla. This is a round big blackwilla, what you said is blackwilla. And I will go straight away to the model and then at the end I will talk about the references. So the step up is the problem. You have The step up is the following. You have the graph V union D, that D is the boundary. So the it you can think in this graph, but what I am doing is works for any graph. So these black dots are vertices in the vertices and the red dots are the boundary. I oriented the graph, the edges, to define the dynamics. To define the dynamics, but this doesn't matter, it doesn't imply that the dynamics depends on the result depends on the orientation. And we fix what we are calling positive temperatures in boundary vertices, that will be in this case t0 and tn plus 1, so that you have n boundary vertices. Okay, now you have the usual. you have the usual Marco Poisson process that I will uh denote the Nij associ there will be two types of Poisson process Nij associated to the edges edge has this Poisson process Nij, that is a set random set of points, okay and then I have another Poisson process for the for the vertices in the boundary. The boundary. This red Poisson process. All of these then are independent and right one. But the blue points of the Poisson process have a mark that this uniform random variable in 0, 1, and the boundary has exponential random variable of mean 1. So the KMP dynamics is the following. You have this process Z T indexed by the vertices of my graph. It's governed by this market quisson process that I denoted NUP. There will be others. This is a reason of the notation. And this boundary conditions TB. So if you have a tau belonging to the Poisson process of the edge ij. Of the edge ij that will have a mark u and then you have two values, zi and zj, that are real values, positive. And then there are these two values, this zi and this zj, and they put them horizontally, because it's easier to see the thing. And then you erase this mark and distribute this mark uniformly in this interval using the random. In this interval, using the random variable u. Okay, so the zi will go to u times the sum, and zj will be to one, go to one minus u times the sum. Is it more? And then in the boundary, one thing that you could do is nothing in the boundary. Just fix the initial condition, and then you will have a conservative process. Conservative process, no? Because here you conserve mass. But for some reason that I don't understand, well, we prefer to put to renew the boundary putting an exponential random variable of parameter TB. But the way we do that is take this B that is exponential parameter 1 and multiply by Tb and then you get exponential of mean TB, right? Okay, so the product of Okay, so the product of exponential one is reverse three at the moment because if this is exponential, dependent of this exponential, then the neutral point is uniformly distributed in the interval of the sum. So this will be exponential and this will be exponential. So if you look at the invariant, after each iteration, you get the same notion. Well, so this is the case where. So, this is the case when the boundary conditions are called. So, the goal here in this talk is to describe the invariant measures when you have different temperatures. In this case, things are more complicated because product of exponential doesn't work, but we we will make them work in some sense in the following way. So I need to introduce another Introduce another model in the same graph with the same boundary conditions that I will call OI of t is an opinion model. So OI of t is the opinion of individual i at time t. And the boundary, you have two extremists that have always the same opinion. So extremist zero has opinion t0, and extremist n plus one has opinion n plus n, and they will never change. In Placent, they will never change. But the others are very friendly, and then they work like that. When the Tao says to IJ to agree, they say, okay, we will agree. And where do we agree? We agree to an opinion that is between our opinions chosen uniformly. Okay? When I told this at some friends, this doesn't happen. Friends, etc. This doesn't happen. Okay, but this model happens. And OB, so this is clear, no? So both of them go to the same value, but the OB agrees to do that, but then doesn't do it. So B is in the boundary. But the other one, yes, takes the opinion of B in consideration. So this process is finite, everything, so you have an It's finite everything, so you have a stationary measure that I will then construct, even construct. So I call Ostat the vector that has that distribution. Okay, the theorem is the following. So you have the invariant measure for K and P. This is a to be archived, but not yet finished. Yet finished. So take O stat, the law of the invariant measure for the opinion model, x by vector x i id exponential one, x i and O stat independent, and define Z as the product coordinate i, the product of x i times O i stat. And the theorem says that the log uz I do. The non-muz of this vector is invariant for the model. So, in other words, the invariant measure for T and T with that boundary conditions is a mixture of exponential random variables with parameter o i distributed according to the invariant measure for the opinion model. So I wish this theorem is not difficult, but I need a couple of the tools. It doesn't need one dimension. Any dimension. Any graph. Okay, so you want to read that? Oh, but the computing the invariant measure for the opinion model? Is this somehow easier? No, no. This is better by not it's not as easy. I guess in the case where they're the same it becomes but in that case I don't need it's But in that case, I don't need this. But we have some information about that. I will tell you that. So let's start with a Galilean KMP that is XT. I will call this process when you have boundary conditions 1 and the invariant measure is a product of exponentials. And then this is also reverse. This is also reversible, not locally this is reversible. And I will call D of T and A of T the relationship in that picture. So B of T will be the thing that makes this partition. But this depends on the process X, right? And A of t so B of t is the relation between xi and xi and xj just before the jump. And A of t is the value of the boundary vertex p just before the jump. And then I claim that the market points minus P, you see that the reverse is. Minus p, you see that the reverse is defined as minus p, minus. So to make it catalog, so and this x of t I define here for all t and R. So this is to not to care about the finite inter time interval, it's just it's easier to define. So uh the remark here is the points minus t v t Points minus T V T and minus A T govern the reverse process. So the so-called dual point that Grunbone and Kator and Brunbone introduce the first time, I think. But okay, but in the Hammer-Lay process, for those that know it are the point the other points when when you have the point one point is a Poisson process and the other point is the dual point. Is the dual point. So you have, here is something very similar, and they are the ones that govern the reverse process. And then you can show that the law, well, for this reason, since that they go down, you just go there and inspect that this is the case, and that's it. And then the low of N B A, A L this match. A L this match is the same as the law of the initial Poisson. It's like a birth year. It's like a birth view. Not sure. Well, it could be a yeah I'm not could be could be but you know, but I'm not sure. Anyway, the same proof as you have in for for the dual points. For the dual points. And then one interesting thing is that x of t is equal to x star minus t is independent of the future of x star minus t because and the future are given by the Poisson process with the dual mass. This is crucial. But all of this for those that worked with Dual Marx is normal at least. Okay, then a coupling, of course. So you take X of T equilibrium TMP government by MUB, then you take the opinion model covernette by N D, like the dual mass, and then And then you have the following proposition. If the initial conditions are independent, then Xt and O T are independent at all times. The proof is one line. X is independent of the dual max in the past of T because of the previous lemma. And now T is a function of the dual max in the past. In the past of the past years. So now the theorem is that if you have Xt equilibrium KMP governed by NUB, O T is the linear model governed by the dual max NB. The dual max N D and boundary T D, initial arbitrary initial condition O of zero. Then you define Z bar T, the vector Z T as X I T O I T and then you will have that Z T is KMP with boundary T B governed by A U P and initials the initial condition. Initial condition. So, if you take the initial condition independent for the previous dilemma, you have that they are independent at all times. Here's the proof. So, here is your independent exponential. Here you have the opinion model with boundary condition P1, T3. With boundary condition T1, T3. And these are the opinions of one, vertex one, and two, that are the only two that can change opinion. And then appears a Poisson map in this edge. The edges are this point, right? In Ij. This is one. The edges one, two corresponds to this value. And then at that point, you do the following. At that point, you do the following. You take an amount of mass data and you equal the two heights, but without losing or gaining much. So this O1D plus O21 minus D is the height that you can have in order to conserve the mass C1 plus Z2. So you already see that the opinion O1 and O2, both of them are going to the complex combination given by D, that is the dual mark correspondent. Because this D is defined here, is depends on these two. It's depends on these two values. But this is just a geometric thing that high school geometry, right? Okay, now you do the following. You erase, so you keep these opinions and you change this boundary according to the uniform map that you have at that time. In this way, I change this simultaneous. Way I exchange simultaneously the value of the homogeneous process with the non-homogeneous one with the same unicorn. Copy. Just a couple of. Well, everything is a copy. Everything is done with the the U, that is the mark associated to the tau that uh was appearing here, no, in the in the boundary of these two s two sides, in the edge. Two sides, the edge. Okay. I I hope I solved it to you. So this is a cutting. What I proved that if I take Z equal to the product of X and O, Z will do the non-homogeneous K and P. A and P. Okay, so this is at this point I could close here, but then you ask Ivan asked me what happened with the opinion stationary measure. Well, now there is no explicit expression. So at some point we found ourselves trying to use properties of Properties of KMP to show properties of the opinion measure, but hopefully you will get some progress on that. But for the moment, I will tell some things. One of them is that you can do the one-dimensional case, one dimensional and one individual case, so you have two opinions. Two opinions, and then the other opinion is is uh moving this interval. You don't two, okay? And the thing is the following. So when when you when O is here, with probability with rate one, you go to a point uniformly distributed here, and with rate one, you go to a point uniformly distributed here. Uniformly distributed here. But the rate does not depend on the length of the interval. So it's symmetric, the jump is symmetric, and then the choose, the way you choose the estimation point is uniform in the interval where you are going to. And if you do that, there is a computing balanced measure, and that is piecewise. It's not uniform and zero one. Okay, for arbitrary n, this computation seems to be very difficult to know how to do, but we still have the duality. That always comes to classic unduality. Classic unduality, so for two. For classical duality, I have to construct a family of random walks backward, going backward in time, starting at time s indexed by s and k, started at time s at point k and are governed only for by the edge for some points. And I put I and I put I govern them also with the D march and I need an extra U march in order to to decide where the the walk will jump and then the the walks is the following so when when you arrive to so you arrive to what some they are going up no time is going down so the work is going up so the work the work has rise to a point L and the marked tau And the mark tau corresponds to this the mark tau corresponds to this, so you go to j with probability 1 minus t and to i with probability d and d the the point associated. But if you are here, you do the same. You go to I with probability I D and to one minus D with probability D also. Minus V is probability V also. So you coalesce, random box coalesce in V is explaining there. And in the boundary, you just stay there. When you arrive to the boundary, the work arrives to the boundary, so if you integrate U and D, you just have a symmetric random work, right? With rate one, two, jump one. With rate one to jump one unit to the right or left. But in fact, since you have, if you fix the environment, but you integrate over u prime, then you have a random work in a space-time random environment. So, this proposition tells you that you can express the opinion model in terms of the expectation of the random. The expectation of the random walk. So you want to know the opinion model at time s. You run your random walk up to time zero, so you know the configuration at time zero. So this is time zero, and then this is time s. So you know zero, and if you want to know. And if you want to know O of S of O S of K then you go to the walk back and then you will have some probability to arrive to each one of these points and this weighted probability of the initial cond you weight the initial condition with this probability. Initial condition with these probabilities, and then you get exactly the random position of essential function of the random walls. And then, if you define the fitting time of the boundary starting from i and s, where is the fitting time of the boundary, then you can construct the stationary distribution by running the works Running the works up to minus infinity, also up to you are absorbed at that point, so the initial condition doesn't matter anymore. And then the boundaries will give you the weight of that. And this weight, conditioned to ND, will be exactly the explicit construction of the vector that is stationary. Of the vector that is stationary for the random work. So, if you believe this formula, the stationarity comes because this formula is invariant for translation by X. So, it's necessarily invariant. So, this is the meaning of this proposition. Okay, yeah, this is something that. Okay, yeah, this is something that you can get, but then we are trying to compute quantum correlations, it seems that we will succeed, but then we don't have yet the thing. But one thing that you can get from that is that the mean in this, in our case here, in our case, is linear in n, that we would expect anyway. And so we investigating this model, we Investigating this model, we found another model. That is the model that you get when you look at the disagreement between two neighboring vertices. I call Bij, the indicator that the Oi and Oj disagree at time T. So if you have different boundary conditions, the bonds, so this process is indexed by A. So this this process is indexed by edge by edges. Okay, so the edges that have a boundary vertex in the string will always be disagreement edges because you can cannot agree to a given value. You will always agree to a mean value. So the thing is the following. When you have a situation like that, so this is idea. So this is ij. Say it like that. So here you will have a zero because disagree, agree, disagree, disagree. Let's say that here you don't agree again. Okay? So if this is the opinion model and this is the disagreement process. Okay, now if you have the this bond This bond, this bond, the tau belongs to the Poisson process in this bond, then you hear you will agree at this point. No, at some point here. But then immediately you will disagree with this guy that you were agreeing. Okay, so this is a typical spatial process in neural but in a mathematics side. But the the nice thing is this uh if you take one coordinate and it's Markov, and you take two coordinates that it's Markov, and any finite number of coordinates is a Markov process. So this permits you to, for instance, to compute the expectation of disagreement at the given bond is to thirds. You have two neighbors. It will be something related to the number of neighbors. Because you have all the neighbors that make you disagree when they spike. And when you spike, you make all the neighbors in disagreement. Okay, so well, this is also a curiosity. This disagreement processes also can go to Ebronian webs if you want. Because you have creation of Creation of disagreement at all points. These zeros behave like random walks that are quales. I have no time to discuss that. Well, the final thing is just one minute is that we can show our theorem for the invariant measures, also for the discrete case. Also for the discrete K-m mode. So once we found the solution was easy, the solution is the form. The way that you do it, you know, the discrete K and P, you take discrete uniforms to redistribute this discrete mass k plus n that you have in the two bonds. So you have secularity to get 0 here, 1, 2, 3, etc. And then the one the thing is, you do the coupling between the discrete and the continuous. And so this will be bound zero, same one, two, three, four. So this will be the boundary things. Things and but you do the following and so this these are the continuous so these are the z and these are the k okay the discrete so when you decide to interact or your Poisson process tells you interact these two things you erase the boundary remix the points The points that are here and choose a random uniform point again, and this will give you a uniform distribution not only for the continuum, but also for the discrete. And then you can export the result here for the same distribution of the vignon. So you have the same The same that's it, I guess. I have here I will say I will so let me have a minute, no, no, but so the KMP model was proposed by Titanic Martio and Presutti when I was a student in the 80s, I think. Yeah, 18. Before I met them. And then And then now this model appeared again because people studying large deviation that are not familiar with and the rate function for large deviation coincided with the mixture of exponential for the KMP model. But the it su suggested that instead of our invariant measure for the opinion model, the mixture would be for the The mixture would be for the statistics of n uniform random variables in this one-dimensional case. I have to go complete here, I have also to complete all the dual maths bibliography. And finally, this opinion model has a lot of literature. was introduced by Defuine a couple of decades ago. But they were studying a very different thing. Defuine proposes that if your opinion differs more than delta from your neighbor, then you don't change. They don't discuss with you. And in that case, the opinion tends to coalesce, to condensate. And depending on the parameters, Depending on the parameters could condensate to only one opinion or to two different opinions. An hour is something that you don't let them condensate because there are two opinions that will never change in the finite case. So some people studied that, but it seems I just some person seems that they don't know how to or they didn't think about that. Probably from the point of view of the point of view of the opinion model is not realistic. The things are compensated, in fact. Well I think it is some reference. Thank you, Pablo. Any questions after that? So your opinion model is really interesting. It seems like a It seems like a continuous version of one of these random average processes, or a special case being the beta polymer model. And so for the beta polymer model, you should actually be able to explicitly solve for the stationary merger with these two style-inhabitable conditions having this curve on top. So I wonder, there might be a way to... Okay, thank you. And you need to plus an eyes and get to some. So can you get the models and paper? So the general T-mosion random average. Are from the Bellies random average process. And then the special case being the beta polymer model, which is a solvable model. Okay. Thanks. Else? So I was wondering if you just dropped the line and you have two opinions. So you say that the expectation is a straight line. So the opinion. But where are the the opinions? But where are the the uh the opinions? The two on the fixed ones on the side, right? Fixed opinions. Yeah. On the side, and then the expectation in the middle is just the straight line. Yes, yeah. Yeah. So intuitively I would think that if you have more and more points on the graph, then the the variation should be smaller and smaller. Well, yes, uh this this is where th this is kind of very named, that that promises, but I don't dare to to say anything. There to say anything. You convert to the diffusion essentially with this inclination. But just like small, small variations. Yes, yes, yes. We are trying to prove that in fact. Yes. Yes. I think that's what's saying. I'll do again. We'll meet at twenty-two. 