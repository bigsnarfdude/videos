So, my talk is not auxiliary space preconditioning on surfaces. So, it is about iterative solvers for HD and H curve problems, both on flat domains and on surfaces. Okay, so this is the outline of my talk. Well, the motivation of this research is Of this research is the computation of harmonic vector fields. So, first, I will introduce the harmonical vector fields and the computation. And then I will briefly introduce the DRAM complex on flat domain surfaces and the finite element discretization of DRAM complex. And in the end, I will present and describe the user-friendly User-friendly iterative preconditioners for solving this HQ HD when harmonic problems on flat domains and surfaces. Okay, so here is a very simple example about a harmonical vector field on Euclidean domains. Suppose uh we have a 2D polygon omega and uh we want to compute uh the curve free and the divergence free vector field. Free and a divergence-free vector field on it subject to some natural boundary conditions. Then, by using some numerical method, here I plot two of them. And these two are orthogonal in the L2 norm, and they are and they form a basis of the space of harmonic vector fields on this polygon with two holes. Polygon with two holes. Because it has two holes, the betting number is two. So we know the basis should have two harmonic vector fields. Okay. Then let's consider similar questions, but on surfaces. For example, we consider this two-dimensional torus embedded in 3D space and the harmonic. And the harmonic, then here we need to consider harmonic tangential vector fields because it is on surface. We do not consider the arbitrary vector field. We need to consider the tangent vector field. And then by harmonic, we mean that the vector field is tangential to the surface and it has zero curve and zero tangential divergence and zero tangential curve. And here is a picture about this. Here is a picture about this harmonic vector fields, and they form a basis of the space of harmonic vector fields for this torus. And so, why do we want to compute harm harmonic vector fields? My motivation is from the so-called finite element exterior calculus. Calculus. So the model equation in that area is called the Hodgella Plazian, the Hodgella Plus equation. It is formally written here. So d delta plus delta D applied to U equals F. So here F is the given data and U is the solution. Here D and delta, they are derivatives. They are generalizations of cur and the D and the gradient. Here, this equation is formulated. Is formulated a module or HK because this HK is the generalization of space of harmonic vector fields. It is called the space of harmonic k-forms because it consists of differential k-forms and the differential forms in this set has zero dk derivative and zero delta k derivative. They are generalization of current. They are generalization of current V operators. And this delta K is the L2 adjoint of D. Okay. And here, we need to modular HK because HK is the kernel of this second order derivative, D delta plus delta D. If we do not modular it, then it may not have a unique solution. Just like we need to modular the constant when we are solving a Poisson equation with the Uh, Poisson equation with the No Eman boundary condition. Okay, then the finite element, the finite element method for this formula plus is a mixed method. It is written here in its fully general form. So here, and it is developed by Dr. Arnold Richard Falk and Ragnar Winter. And it has influences. It has influences in many areas, and Dr. Arnold was a plenary speaker in ICM 2002 in Beijing. So here, in order to solve it, they need to introduce an auxiliary variable sigma, sigma h. And they rewrite this equation as a system of two equations. And for each equation, they have test functions and integration by path. test functions and integration by parts so they have these two they have these two equations and they have a third equation in order to impose this uh this condition so uh in order to guarantee the existence of solutions you need to you need to impose the condition that the solution either the continuous solution or the finite m solution is orthogonal to the harmonic form or discrete harmonic form okay so so the third form. Okay, so the third one is an orthogonality condition. And in the discrete level, this curly edge, curly edge, HK is the space of discrete harmonic k form. And each differential form in it has zero k derivative and has weakly zero delta k derivative because the derivative. Because the second condition delta k equals zero, delta kv equals zero in the discrete level is weakly imposed, is weakly imposed by integration by parts. So this is the space of discrete harmonic k form. So as you can see, to efficiently implement this method, we needed to develop an efficient numerical method for computing a basis of this space HHK. So, this is the one motivation. But of course, the harmonic forms may have applications in other areas. I will briefly mention it later. Okay, so let's forget about the differential forms and the exterior derivatives and go back to the vector fields and curve and the D operators. Operators. So on that 2D polygon, when computing the discrete harmonic vector field, just as the analog mixed method, we need to introduce an auxiliary variable sigma h. And we need to write two equations here, two equations here. Okay. So this can be viewed as a special case of the aforementioned. Case of the aforementioned ano-fok winter mixed method. And I'm kind of sloppy in the previous slides because I didn't explain what is VHK minus one, what is VHK. They are both finite element methods. They are both finite element spaces. Okay. And here, these two spaces, VH grad and VHQ, they are also finite element spaces. In particular, this VH grad is just the space of linear. Is just the space of linear Lagrange finite element. It is the space of continuous and piecewise linear polynomials. And the VHQR is more complicated than VHGRAD. VHQR, I will describe VHQR later. So you cannot arbitrarily. So in order to obtain correct harmonic vector field, you cannot arbitrarily choose V. choose you cannot actually choose VHQ. For example, you cannot use a Lagrange finite, you cannot use the most common Lagrange finite element to compute this solution. So from this variational formulation, if we set the test function tau h to be sigma h and we set the second test function v h to be uh, then we obtain this equation. Actually, then we obtain this equation. We obtain this equation. And after adding up these two equations, we obtain this one. And this one implies that the discrete harmonic vector V or the UH is the curve real. It has zero curve. And it implies that the auxiliary variable sigma h is also zero. And then the auxiliary variable sigma h is zero means that this discrete harmonic vector field u h is weakly divergence free. Divergence free, weekly divergence free. And as I mentioned before, we need to suitably choose this finite element spaces, VH grad and VHQ. And by the analysis of Arnold Falk and Winter, it is proved that the space of discrete harmonic forms is isomorphic to the space of the continuous. To the space of the continuous lines. So this H, H, and H, they are isomorphic. If you choose VH grad and VHQ in a suitable way. And more surprisingly, they are isomorphic no matter how coarse the mesh is. They are always isomorphic. Even if your mesh is very coarse. So this is algebraic topology. Algebraic topology. This falls from topological theorems. But of course, if we need to compute accurate harmonic forms, then we can choose to refine the mesh. So if the mesh size h goes to zero, they also prove that the gap or the distance between these two subspaces is converging to zero. Let's reconsider the computation of the discrete harmonic vector field in matrix form. So, if we let if we consider the coordinate vector of the finite element solution. of the finite element solution sigma h, u h, then this coordinate vector satisfies the linear system of equations a tier h x equals zero. Well, this a tiota h is a symmetric but symmetric and indefinite and a singular matrix. It is not positive or semi-definite, it is singular and indefinite. This etiota and it is the matrix. And it is the matrix that represents these operators. If you write this discrete gradient, discrete curve with respect to the canonical finite end basis, then you get this H or H. It is indefinite and it has zero eigenvalue. And the eigenvectors corresponding to its zero eigenvalues is the coordinate. Values is the coordinate of the harmonic vector, of the discrete harmonic vector field. So we need to find an efficient way to compute its kernel, to compute the kernel of this indefinite symmetric singular matrix. And for the more abstract Hodgela Plausion, the structure of A T or H is the same as the more concrete one. Okay, so but these derivatives are replaced with the X T. Derivatives are replaced with the exterior derivatives d, h, k. And before our research, there is a few papers considering the computation of harmonic vector field. For example, I carefully read Alan Demlo's paper, which is about adaptive finite element method for computing harmonic forms. But of course, in in his paper, he didn't uh consi uh he didn't consider this uh fast solver. Uh he just used a general purpose eigen solver in MATLAB, which is SVDS. So if you use SVD, then you can you can find the singular ve you can find the singular vector corresponding to the uh zero singular value of A T L H, then you get this kernel. Edge, then you get this kernel. You get the kernel of ATOTIH. But according to my experience, this general purpose test VDS is quite slow. It is slow even if you, for example, it is slow if your mesh has if your mesh has an twenty thousand vertices, then it is already very slow, twenty thousand vertices. 20,000 vertices. Then let me describe my approach to computing the discrete harmonic vector fields. So I found that we can use minimum residual method to compute the kernel of the singular matrix H or H. So my strategy is to randomly generate, is to is randomly generate Is randomly generating a right-hand side vector, b. And I consider this equation, a Tiota x equals B. Because A T or is singular, this equation is almost surely incompatible, right? It has no solution. But although it has no solution, we can still apply an iterative method to it. We just write To it, right? We just wrong iteration for this incompatible system. Okay. In particular, I apply min rest to it. Because min rest has this minimum residual property. It will generate a least squares solution for an incompatible linear system. And because this least squares property, the iterated residual, the iterated residual B minus A tilde H times the iterated solution Xn will converge to this exact residual corresponding to the least squares solution X which is contained in the range which is contained in the orthogonal complement of the range of this singular matrix at your then so by a simple result from linear algebra it it will converge to some vector It will converge to some vector in the kernel space of A T O T. This is the observation. Then, because I'm talking about fast solvers, we need to analyze the convergence rate of this min rest for incompatible system. While the convergence rate, while the classical convergence rate, The classical convergence rate will mean ras for non-singular symmetric system is something similar to this, unless that the condition number kappa is the usual condition number. But now this etiota is singular. We cannot uh the classical condition number is infinity, but uh so here we need to consider the effective condition number. Consider the effective condition number, which is the ratio of the biggest absolute value to the smallest non-zero absolute value. That is the effective condition number. So this convergence rate of this iterative SOR depends on the effective condition number of this singular matrix. Of course, in general, it is very large. And in general, it is like h to the minus two. Minus two as h goes to zero, right? Because this is a second order differential equation. So we need some preconditioner to speed up the convergence of this mean res. So this is the goal of our research. So we need to construct a precondition of BTO type such that, so our goal is to construct So our goal is to construct a precondition of B tilde such that the effective condition number of this product matrix B tilde A Tilda is uniformly bounded with respect to the mesh size H. Okay. Then the preconditioned mean race for this singular problem will uniformly converge. So the space of harmonic forms is not only used in finite element methods or solving partial differential equations. It was also used in some statistical or statistical applications. For example, if we have a combinatorial Hodgela part, if we have a combinatorial graph, G, so it has a vertex set and the edge set, and you can define gradient on this graph. You can also define this. You can also define this discrete curve on this graph, and similarly, d and then you have this k and you have curve-free and d-free vector fields on abstract graph. And you can compute the harmonic forms on the abstract graph. While the aforementioned framework applies to this case, and recently we did. And recently, we did some computation. In my opinion, I think the computation of this differential operators on this differential equations on graph is easier than the differential equations on the true grid. Yeah, for example, I have read a paper and this this Lehan Lin from University of Chicago, he developed some new ranking algorithms. A new ranking algorithm, which is called a Hodge ranking. So he will compute, you have to have some data, and he draws a graph, and he computed the Hodge decomposition on this graph to quantify some incompatibility between local pairwise ranking and global ranking. Okay. The harmonic form is the obstacle there. So Okay. Let me put the previous core and d variational formulation in a more rigorous way. So to this end, I need to briefly mention the very famous exact sequence H1, Hcore, Hdv, L2 on a 3D solid domain, omega. If this If this, so this H1 is the usual H1 sobre space, and this HQR and Hdiv are defined here. So they are sober spaces for vector fields. So well, this capital D is core, it corresponds to this H core space. When this capital D is divergence, it corresponds to this H div space. And this core and div are understood in the weaker sense. So they are vector valued sob level spaces. Vector value the software of spaces. And just like the preceding talk, he talks about the natural model reduction or structure-preserving model reduction. In this case, to correctly approximate harmonic vector fields, we need to use structure-preserving finite elements. Okay, so in the discrete layer, So, in the discrete level, we have an exact sequence when omega has no hole. If omega has holes, then this sequence is not exact. It means that a curve-free vector field may not be a gradient field, and a d vector field may not be a curve field. There is some extra harmonic component. So, if this one So if this one uh if this finite element subspaces VHQ and VHD are constructed in appropriate ways, then the cohomology structure of this discrete finite element sequence would be isomorphic to the cohomology structure of the continuous ones. In particular, it means that their harmonic vector field, their harmonic vector spaces. Their harmonic vector spaces are isomorphic. They have the same dimension. And the discrete one will converge to the continuous one. And the vertical mappings, pi H0, pi H1, pi H2, Pi H3, they are some local interpolation with respect to the degrees of freedom of these finite eminent spaces. If these spaces are chosen in an appropriate way, then Appropriate way, then this is a commuting diagram. It means that the interpolation of the gradient is equal to the gradient of the interpolation. And similarly, the interpolation of the current is equal to the curve of the interpolation. So it is a commuting diagram. So why do people develop structure-preserving finite elements? Because these structure-preserving finite elements can preserve very important physical quantities. Preserve very important physical quantities like the incompressibility condition in fluid dynamics. And this cannot be preserved by this projection method. But of course, the computational cost of these structure preserving schemes is higher than the projection schemes. And when solving Maxwell type or MHD equations, this This structure preserving finite elements can preserve the divergence free of the magnetic field, which is also essential. Okay. So the VH grad space in 3D or in NED, it is just the nodal finite element, or as I said before, the Lagrange finite element of degree one. So it's globally continuous and Globally continuous and piecewise linear polynomials, and its degrees of freedoms are located on each vertex in the mesh. And the VHQR space in its simplest form is called the edge element or net-like edge finite element. And its degrees of freedoms are associated to the tangential direction of each edge in the current mesh. And the VHD space, in the simplest case, is called the Revatoma net legacy. Is called the Reviatoma netleg phase element, and its degrees of freedom are associated to the normal direction of each phase in the current mesh. And so in particular, the VHQ and the VHD are not continuous piecewise polynomials. They are the finite elements in there are discontinuous, but weakly continuous. For example, Uh for example, in 3D, a finite element function vh in this vh curl space is locally of this form: is a cross x plus b. Well, x is the position vector, is the coordinate vector x1, x2, x3, and a, b are constant vectors. And when crossing the boundary of the tetrahedron, this component of the finite annual function is a single value. Anion function is a single valued or it is continuous across each phase. So this n cross V H cross n is the tangential component of VH. So VHQR, the finite element function in VHQR has continuous tangential component. Similarly, the finite element function in VHD has a continuous normal component. So it's VH dot the outwater unit normal to the phase. It's a single valued. Is a single valued is not changed, it's not changing when across this phase. And locally, this VHD finite L is of this form, A X plus B, where A is a scalar, B is a vector. In 2D, they are similar. In 2D, these are VH core and VHD in R2. Okay. Okay. Similar to 3D case. And on the surface, right? Because my talk is about preconditioning on surface. So for example, we have a triangulated surface. And if we zoom in, let's consider a pair of adjacent triangles. And then the VHQR on the surface is locally of this form, is locally of this form. Well, this X is This x is x1, x2, x3, and this nh is the outward unit normal of the triangulated surface. And this t, and v edge dot t is continuous across, for example, across this edge e. So this t is the tangential, is the tangent vector to this edge e. Okay. So this tang tangential component is continuous across this edge. Across this edge. Well, on this surface, the VHD is locally of this form. And I would say the conormal component of the VHD will find an element function is continuous across the edge. So here, this NE plus and NE minus, they are polynomials. So they are perpendicular. Normals. So they are perpendicular to both this edge, E, and the outwater unit normal. Now, let's go back to solvers, in particular, multi-gree method. Well, it is well known that classical simple iterative methods. Classical simple iterative methods are not efficient for linear equations from discrete PDEs. The reason is that they can only efficiently damp out the high frequency error, and they are inefficient for low frequency errors. So we need a nasty grid to resolve medium and low frequency errors. So you perform simple iterations on the finest grid and then you transform. then you transfer the error to the core circuit, and then you perform the classical iterations again. And the medium frequency error becomes high frequency error on the cost of grid. So it can be damped on the cost of grid. And then you redo this algorithm, then you have multi-grid algorithms. Okay. But as you can see, the multi-grid is not user-friendly for User-friendly for unstructured meshes, right? Because the implementation of multi-grid requires a nested geometric grid, which is not available in many applications. For example, on a flat domain, for example, this unstructured mesh for Lake Superior, it is quite complicated and it's not easy to construct a nested causal grid sequence. Grid sequence. And the same situation arises for our surface application. Well, if you, when you triangulated a surface, a surface by a sequence of meshes, they are never nested, right? Okay. So when solving when solving discrete PDEs, Discrete PDEs on unstructured grids, it's better to use algebraic multi-grid, algebraic multi-grid. Okay. So in practice, AMG should be used. So first, here, let's consider the model equation: this dUH DVH plus UH VH equals F V H. And this D is a derivative, which could be gradient, curl, and D. And this is. Curl and div. And this is a symmetric and positive definite problem. So, why do we consider this problem? Because the solver for this problem is a building block for more complex problems, including the Hodgella plasma and the harmonic vector field. So, this is the building block. And so, when the mesh size is small, the stiffness matrix AH has a very big condition number, and we need a precondition. And we need a preconditioner to speed up the convergence of quantitative gradient method. And unfortunately, AMG is not applicable for HD and HQR problem. So when the derivative D is divergence occur, AMG, AMG preconditioned conjugate gradient simply diverges. It does not converge. But the AMG preconditioned conjugate gradient Preconditioned quantity gradient converges for gradient problem. So, for song or for second-order elliptic equation, it is converges, but it diverges for divergence and curve problems. And so around 2000, Hibdenmeier and Doug Arnold, they and Winslow and Falk developed a classical geometric multi-grid. So the multi-grid requirements. So the multi-grid requires a sequence of grids for the core and the BIM problems. But as I said before, they are not user-friendly because the geometric multi-grid requires a sequence of grids. So later on, in 2007, Ralph Hidema and Jin Chao Shu developed the HX precursor. HX preconditional HX preconditional for this cur and du problems. Okay. And this BH is a matrix, is a matrix consisting of sparse components. And it is implemented on a single unstructured grid. So So, in view of this work, then we need to generalize this HX precondition from Euclidean domains to triangulated surfaces. So, here, let me review those derivatives on surfaces. Well, the extrinsic way to compute the derivatives on surfaces is to first extend the function. Extend the function living on the surface to its tubular neighborhood along the normal direction of that surface. So VL is the lift, it's the extension of V in that way. And then when taking a tangential gradient, you can first take the four gradient of this extension and subtract the normal component from it. Then you get a tangential gradient. And similar, we have other operations on the 2D surface. On the 2D surface. Okay. Then, well, as I said before, this curve and the divor operators are built in the classical Hodgela plausion, and this Hodgela Plausion is also naturally defined on surfaces. And besides it, the Stokes equation and the Poisson equation in mixed form can also be. form can also be written in curl and d form and there are some works about it. So this numerical method also needs some efficient solvers. So now here is our HX preconditioner on surfaces. So here I just give an example because the preconditioner corresponds to a lot of different operators. Different operators. No, I just consider the current operators. So when the derivative is the tangential curve on the triangulated surface, the HX preconditioner B is of this form, is of this form. Well, it has three components. The first one, SH, corresponds to simple classical iterations like Gauss-Sadel or Jacobi. For example, if we use Jacobi, then this SH is the inverse of the diagonal of the stiffness matrix. Of the stiffness matrix. But of course, the single line, this diagonal will not efficiently be efficient. So we need the next two terms. The second term, okay. It consists of a discrete gradient, so it's grad mh, and an inverse of a surface Poisson plus identity. So while this surface gradient, it is a subset matrix. It is a sparse matrix. Okay, it is a sparse matrix. And in fact, the entries of it is given here. So this is the discrete surface gradient. And the inner one, this negative Laplacian plus ID, okay, it is written in an exact as an exact inverse, but in practice we will not exactly invert it. We will not exactly invert it because here it is a second-order elliptic equation. It is a gradient operator and it is similar to Poisson equation. So it can be efficiently, the inverse of it can be efficiently approximated by classical AMG. So in practice, just replace it by classical AMG. And the third one, this border face Laplacian plus ID is a block. Is a block diagonal is block diagonal second-order elliptic operator. And each block, the inverse of each block can be efficiently approximated by classical AMG. And the pi h in the third term is also a sparse matrix, but the entries of it are more complicated than the entries of gradient mh. But I just plot the I just uh plot the sparsity pattern of this pie H it is a sparse matrix. Okay and so the action of this operator on a vector is cheap, right? Then you just need to do several sparse matrix vector multiplications, and you just need to call one AMG cycle, for example, AMG V cycle or W cycle once to replace this. Cycle once to replace these inverses. And well, similarly, we have a precondition for BHD. And in theory, we proved that the surface X preconditioner is spectrally equivalent to the inverse of the stiffness matrix. So their eigenvalues are comparable. It means it implies that the condition number of this product. Condition number of this product is uniformly bonded. The condition number is uniformly bonded. So up to now, I briefly explained how to precondition the symmetric and positive definite problem. How to precondition this problem. But our goal is not to solve this SPD problem, right? SPD problem, right? So for the saddle point in the mixed problem, for example, this one, this because we need to precondition this matrix to get a fast min rest iteration. So, well, it turns out that this block diagonal preconditioner is an efficient preconditional. Is an efficient precondition for this matrix. The reason is from the info soup theory from finite element methods. But of course, here this matrix is singular. So it's singular, but we can modular its kernel and we consider some singular infusible conditions. And then we can prove that the bond. The bond, the effective condition number of B T or A T O T is uniformly bonded by one. Because the infusible constant is uniformly bonded by the mesh size. So in theory, we proved that this block diagonal preconditioner is very effective. And then in practice, we can just replace this inverse by any fast Poisson solver. Fast Poisson solver. If it is on, yeah, by any fast Poisson solver. If it is on the if it is on the structured grid on the rectangle domain, we can just replace it by FFT. But now we are on a triangulated surface, then replace it by AMG V cycle or W cycle. Okay. And similarly, we have two, we have several AMG V cycle inside a BHQR. Okay. And as I said, Okay, and as I said before, if the preconditioner B tilde is identity, this residual B minus A tilde XM will converge to the kernel of this matrix. Now, because we have a preconditioner, so well, if you multiply this residual by the preconditioner, it will converge to the kernel of A Todd. And in the end, let me show some numerical experiments. The first one is the computation. The computation of this source problem, this SPD source problem on the 3D sphere embedded in 4D Euclidean space. And you can see that the number of iterations only mildly grows as the number of degrees of freedom grows. It can easily solve this discrete problem with several millions number of degrees of freedom. Okay. And the next And the next one is the computation of the tangential harmonic vector field on the 2D Torres at the very beginning. And I recorded the number of iterations. And you can see that when you have this n is the number of vertices in the mesh, you can see that when you have such a fine mesh, the number of iterations is still quite small, and the accuracy, which is the relative residual, is quite. Relative residual is quite small. And I should say that general purpose eigensolver, SBDS, it even does not work for this one. Okay. And here is some literature that relates to my work. So thank you for your attention.