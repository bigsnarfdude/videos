Thank you, Damien Majorga-Pena. Thank you, Damien Major Pena, for accepting to give this nice talk and your work. So, let me just say a few words on Damien, what he has done so far. So, he actually obtained his bachelor at the Universidad Nacional de Colombia some years ago, and then he moved to Bonn, where he did his master's in. Did his master's and PhD under the guidance of Jens Pitanilas. And then he moved to make a couple of postdocs. He started at ICTP with Fernando Quevedo. Then he moved to the University of Guanajuato, where he worked together with Oscar Le Sabrito and Anna Cabo Bizet. And right now, if I understand correctly, he's at Vivata Front University working with Vishnu Jejala. He has worked on different topics on heterodox. On different topics on hydroelectric orbital phenomenology, on F-theory phenomenology, on non-geometric flux compactifications, and more recently on machine learning, that is actually the topic of this talk. And that's the reason why we invited him to work on, to tell us what his work on neural network approximations for collaborative matrix is. I guess it's complementary to what Fabian already presented to us. Thank you very much, Tamien. Thank you, Saul. Thank you to the organization. Thank you, Saul. Thank you to the organizers for putting up together this nice conference, for letting me speak here. And yeah, it's great to be back in Oaxaca at least virtually. So my talk is entitled Neural Network Approximations for Calabi Geometries. And  Is there something that doesn't work? Yeah, it wasn't moving somehow. Okay, so basically, what I want to tell you is about neural networks that approximate metrics that to a certain degree are close to the rectified metric that one needs for many phenomenological and physical and mathematical purposes. So, in this talk, I will discuss some recent work of You discussed some recent work on machine learning approaches to obtain numerical approximation to reach a flat calibration metrics. And instead of looking at the killer potential, what we would do is to approximate the metric directly by an array of neural networks. And we apply this approach to various examples, such as the quartic K3, some members of the dwarf family of quintics, and I'll make some comments also on the Tianyao manifold. Also on the Tianyao manifold. This work was done in collaboration with Bizner Yeyala and Challenger Mishra from WITS. And I'm also talking about some ongoing work with Argo Chateau Padai, who is also a post-off at WITS. So basically, Calabi-Jao manifolds appear in many scenarios. One of these are string compactifications where Where they come as suitable compactifications of the 60 extra-dimensional space. And they are relevant to obtain the corresponding low-energy effective field theories with some degrees of supersymmetry, but these theories, of course, in the context of this conference, would be important for addressing certain phenomenological issues and to obtain, well, let's say, realistic models. So, Calabrija. Calabi Javs, some features of these Calabi Javs are that they have some special parameters that define the dimensions of the Kayler moduli space. They come with a lot of structure. When people carry systematic analysis and constructions of such manifolds, one obtained pretty pictures like this, which led to what today To what today is known as the basic idea of mirror symmetry. And that has made them become a very appealing object also for popul mathematicians. And since they are so interesting and so beautiful, they have also made their way into popular culture and people are in general amazed by these spaces and they give names to techno albums, to comic strips about people who turn into Calabria. About people who turn into calabujaos to explore the extra-dimensional space, and they also made beautiful lamps out of them. The definition of Calabilla manifolds, as Fabian was already introducing the topic, they are complex manifolds, which are also Kayler manifolds, and they are a set of equivalent properties that define them. So, one of these is that the first chain class. So one of these is that the first chain class of M is zero. And of course, we're focusing on compact Calabria manifolds for our purposes. M has a Kayler metric with vanishing cliche curvature, has a nowhere vanishing holomorphic M-form, and has a Kayler metric with local holonomy SUN. Needless to say, please feel free to interrupt me whenever you have questions or comments. And some And some examples, even though, as it was already said, the metrics are not known, they are relatively easy to construct. So the simplest way perhaps is to build them as hypersurfaces in projective spaces. One simple example is the Fermaquartic, which is just built as a quartic polynomial that cuts a That cuts P3. And further, since we're interested in Calabija threefolds, one has the famous quintic treefolds, and then some complex structure deformation leads to a one-parameter family of such quintics, and those are known as dwarf quintics. There are other constructions now in products of weighted pros of projective spaces. Pros of projective spaces. And one of such examples is taking an ambient space to be P3 times P3 and cutting the first P3 by a cubic, the second P3 by a cubic, and an additional equation that is bilinear in both P3s. This is known as the Tianyao manifold. And somehow it's interesting because due to the symmetries that it exhibits, it can be quotiented by a set-tree symmetry. It can be quotiented by a set three symmetry, leading to a three-family model. So, similar to what one expects in particle physics. There are other examples, many more examples. One of these is the Shore manifold, which admits these two different presentations, for instance. So there have been various approaches to obtain the Ricci-Flat metric. So, as Fabian mentioned, the proof of As Javian mentioned, the proof of Jao is not constructive and therefore one has to find ways around in order to engineer a richie-flat Calabi-Jau metrics. And Donaldson's algorithm basically suggests the construction of a particular family or type of Kayla potentials that are described by this equation. So H alpha beta is basically a Hermitian matrix. A Hermitian matrix and the S alphas are a basis of holomorphic polynomials over the manifold M up to a given degree. And the task is then to find a suitable Hermitian matrix at every degree K that best approximates a Richie-flat metric. Okay, so if we take So, if we take nk, the dimension of this basis, then we can obtain something that is called the balance metric from integration of these products of polynomials divided by the thing that enters the logarithm in the definition of the Kayla potential. And this process can be done iteratively. For instance, you could start with H alpha beta to be the identity. You could start with H alpha beta to be the identity and then proceed. And after some steps, you would see that if you take this, let's say at step two, you take H alpha beta to be the inverse of this quantity that you obtain here, what you're going to get is that indeed this H alpha beta stabilized, and this is what's known as the valence if you're duplicate. And as the polynomial degree increases, then the metric that you obtain from some Then the metric that you obtain from such scalar potential, once you have included the corresponding balance metric, approaches the desired recipient metric. So the idea in this work is to approximate Calabi geometrics using machine learning. And let me just set a little bit of Let me just set a little bit of the stage on how things are at the moment. So, there's no known analytic expression for Richie-flat calorie geometric, with the exceptions of some K trees, where some recent works have explored the fact that K trees are also hypercaler. The metric has been accessed numerically in some cases, and here are some references. And more recently, there have been some machine learning efforts to obtain the Richie flat color. Obtain the Ricci-flat calabi geometric in some examples and more systematic efforts from what Fabian told us this morning. So being a Kayler manifold, the Hermitian metric can be obtained from a Kehler potential in this form. The metric is also related to the Kehler form in the compactification. And then the Ricci tensor can be simply. And then the Ricci tensor can be simply obtained from the derivatives of the logarithm of such a metric. The simplest case of a Keller metric is the Fouini-2D metric. And of course, using the defining equations of the Calabi-Jiao, this is a metric in the ambient space, but it can be pulled to the hypersurface. So the idea is to approximate. Is to approximate this metric by means of neural networks. And basically, neural networks are nothing but arrays of artificial neurons. And an artificial neuron is something that basically emulates how a neuron in the brain works. So basically, you take certain inputs, so and stimulus to this neuron, and depending on Neuron, and depending on what these stimulus are, it either fires or not. More specifically, what one is interested is in a set in some sort of non-linear response. And to describe that, basically what one considers is an activation function. And as examples of activation functions that are typically used in the machine learning literature are the logistic sigmoid, which looks like this, the hyperbolic tangent, or a sign. Hyperbolic tangent or a simple reload that basically becomes linear after the input x is positive, otherwise be zero or near to zero. Then one stacks up these neurons in what are known as layers, and basically the layers are feed from previous layers such that in the end your neural network basically takes some inputs, some here. Basically, take some inputs, some hidden layers, and some outputs. So, in our case, one naively would want to have inputs, for instance, as also was mentioned in the discussion, as coordinates in the Calabri Jao. So dilute the Calabi-Jao into a given number of points, and then feed those points and try to obtain something that looks like a metric, and in particular, that exhibits some of the properties that. Properties that I just mentioned. So, in our case, of course, we are interested in a metric that is Hermitian. And linear algebra provides us with a very simple and interesting decomposition. So, this is called the LDL decomposition. Basically, you take L to be a lower triangular matrix with ones in the diagonal. And then a diagonal matrix. Of course, we're interested in Euclidean matrices, so we have to ensure that the eigenvalues are positive. And basically, L D L DAC is going to give you a Hermitian matrix with the desired properties. Given this modular approach, we thought of using neural networks, different neural networks, to approximate the matrix L or the The matrix L or the entries of the matrix L and another neural network to you to describe the entries of the matrix D. And this is the architecture that we considered. So as inputs, you have here the coordinates of a given point in a given patch, and they are fed to both neural networks. From the first neural network, we obtain the eigenvalues, and from the second neural network, we obtain From the second neural network, we obtain the L2 entries, and from these two outputs, we construct the corresponding matrix. Since the eigenvalues have to be positive, this is not guaranteed by the neural network itself. So instead of taking the output directly, we exponentiate in order to guarantee this positivity. And for instance, the outputs of the neural network of the second neural network, we can take. Second neural network, we can take to build L, and L, in the case of K3, is going to look like this. So, basically, you have only one complex entry in such a way that the neural network, which gives you real outputs, is going to give you two in this case. So, we have taken real and imaginary parts of the coordinates separately. Separately and provided them as inputs in a given patch. And the number of neurons we have kept fixed throughout our work. And somehow, from experimenting with different architectures, we found that an architecture that suited for these purposes was one with three hidden layers. Each of these layers has 500 neurons inside. 500 neurons inside, and these are basically the pictures that describe the neural networks. Of course, there are other things that one has to take into account, such as what activation functions suit better for this purpose. So we considered the logistic sigmoid that I was mentioning, the Rilo and the tangent hyperbolic activation function. And since we want to take derivatives of the output, Take derivatives of the output. Basically, we need to take the derivatives of the output metric. We found more appropriate to take smooth activation functions, which also shown to work better. And in particular, what we see is that a tangent hyperbolic activation function seems to be the best for our purposes. The data that we used was prepared. That we use was prepared in Mathematica, and the neural network implementations were done in Python. So, in learning the metric, as it was already mentioned, we don't know the actual output. So, this is not really supervised learning. Instead, we have the points, we get a neural network approximation function, and we want to find a way in which this neural network approximation gets close. Approximation gets close to the metric we want. And then we have to ensure for some properties of this metric. So we have to ensure that the metric is flat. That's our purpose. And this can be described in terms of what is known as the sigma laws, what Fabian called the Montchamper laws, that basically takes the ratio. The ratio of the determinant of the metric divided by the Calabijao volume, sorry, by the Kajalabijao differential volume or the determinant of the Calabujao metric that's known. And then the idea is that in the end, after training, this ratio with proper scalings gets as close as possible to one. Similarly, we have to ensure that the neural network. Clearly, we have to ensure that the neural network approximation is as close as possible to KL. So we have to come up with a measure of KLRicity. These are some rescalings, but ultimately what we want is that dj or dg is actually zero or close as possible to zero. And similarly, we would have different patches. And if we want to patch up the metric correctly, we expect that indeed up to the corresponding. That indeed, up to the corresponding Jacobians of the coordinate transformation, both metrics agree. And this is taken care of by this quantity here, which is computed just as a Frobenius norm of the difference of the metrics corresponding to the same point in different patches. So, here we have used these indices M and L to denote. And L to denote the patches in our construction. And then we want these three losses. You can add more. And there are reasons for that. We just consider these three. And the total loss, of course, is going to, we're going to take as a superposition of these losses. And what we want is that after training, all these losses drop significantly so that we. Significantly, so that we actually well satisfy the properties that we want for our metrics. So the first problem is to find a set of points that could be used to describe this Calabri Joe or to fit to the neural networks. And basically, the procedure that we applied is one example that is based on this theorem that Farian was discussing from Schiffmann and Celdich. And basically, what we want to build are lines in the ambient space. And to do so, the first thing that we want to generate are a Are a set of random points, right? So we start building random points in an interval. And of course, this twice to the complex dimension of the ambient space. And this gives you ultimately a set of points that live in a hypercube. So the first thing to do is to reduce. So, the first thing to do is to reject those points that do not lie within the hypersphere, similar to the analogy of Fabian of throwing stones to a lake. And to do that, basically, we build a norm out of them and reject those which have norm bigger than one. And now we have a distribution of random points inside these hypers here. And one option is first, well, it's Is first, well, it's to project them to the surface of the unitary sphere and complexify. So basically now we have complex points in this PN. Since these points are living in this hypersphere, we expect them to respect this SUN plus symmetry that is the same symmetry that we see in the Venus-2D metric for the PN. For the PN. And then we can take two unitary vectors to construct a line. So basically, what we do is we add this parameter lambda. And basically, now these lines that are sketched here are going to intersect in the case of the quintic the hypersurface at five points and then we take those points. And then we take those points as part of our sample. So there are some variations to this approach. For instance, when we consider the Tianyao manifold, we have to find solutions now to three polynomials. So we take this product of P3s and we proceed identically as before in order to sample points in the two corresponding hyperspheres. Then we use these points to build now not two lines, but a line and a plane, so that we have enough freedom to solve for the three polynomial constraints. Then we cut these lines and planes with the corresponding equations that define the Tianyao manifold. And for symmetry, since there is, and we are considering a particular example of these two cubic equations and the bilinear. Two cubic equations on the bilinear, we can exchange coordinates between these two between these two p trees in such a way that we obtain double as many points as we need. So the first simple, the simplest example that one can think of is the Taurus that's called as a cubic hypersurface in P2. And this is nice because it allows for a pretty visual image of what we're doing. Image of what we're doing. So basically, we sample the points, and in these pictures, we have projected these points to one complex plane. So basically, here we have the freedom to set one of the coordinates in the ambient space to one. That defines the first index that you see here. And we can also use this defining equation to solve for one coordinate in terms of the other. Solve for one coordinate in terms of the other. So, if you look at all the possibilities that are there, you have six possibilities for the case of the Taurus, and this is precisely the regions, these are precisely the regions that are sketched here. So, these are the six different patches that we would have for the Taurus case. And to the right, you have the distribution of points that we have obtained from sampling according to this method in the torus. Source. Similar procedure was applied for the FermaQuintic. So, again, the patches in this case are described in terms of these coordinates I and J. I labels again the affine coordinate set, the set I that we set to one, and the dependent coordinate set J, set J is obtained from solving for P equals to zero. In this case, we Equals to zero. In this case, we find 20 patches, and when we project the points that we have sampled, we obtain this log density plot of the points, and you find this nice bare footprint in this case. So the similar procedure can be followed for the dwarf quintic. In this case, we have set psi equals to minus one over five. And here, these five points that These five points dilute, and this is what you basically obtain. So, the sampling in the points for the Tian Yao, as I mentioned, was slightly modified. So, in this case, what you have to consider are these three different equations. And we have considered these simple cubic polynomials in both P3s, and then this bilinear combination for the third. For the third, we have set now the coordinates to run between one and eight in order to find a compact notation for the patches that we find in this case. And basically, here what we have is that we can set two affine coordinates to one. So basically, this is defined by i and j. i is below four and j should be above four. And similarly, if I solve copy. 4 and similarly, if I solve for p1 and p2, I will get the coordinates k and l one below 4, the other above 4, and m that can be basically lying in any of the two p-trees. So basically, we end up with 192 patches. And before something that I forgot to mention is that all these patches for the Quintic, the Taurus, and the Quartik K3 were. Tree were essentially equivalent up to permutation of permutations of coordinates so that the metrics are expected to be identical up to such permutations. That helps a lot in the computations because we can basically focus on one patch and then compute the corresponding coordinates in the others and work with everything locally except for the matching of patches. But in this case, we consider all the symmetry that. Case, we consider all the symmetry that we have left, and this is not enough. Therefore, we are left with four inequivalent classes of patches. And in general, the metrics in inequivalent patches do not need to agree. So we have to take care of that in a different way. So now that we have the points, we have to find a way to compute integrals in order to compute. Integrals in order to compute the corresponding loss functions. We're lucky that the way in which the patches are constructed intersects over zero measure sets, and therefore the numerical integration is simply a sum of points in the corresponding preferred patch. And that would be the case if the points would be uniformly distributed with respect to the Calau geometry. It with respect to the Calabo geometric. Again, the Calaboi geometric, we don't know. We cannot think of computing some sort of density in order to balance for these weights. And therefore, we have to take into account a bias because our sampling was done not with respect to the Calabi-Javic, but with respect to the Fubini-Sturi metric restricted to the Calabi-Jav. The numerical integration, therefore, requires. The numerical integration, therefore, requires us to integrate with respect to the finished tool, and this is done simply by considering a weight associated to each of the points. And this weight is just given in terms of the differential volume for the Calabria divided by the differential volume associated to the Fubini speak. The simplest case one can think of again is the Taurus. For the Taurus, many of these. Taurus for the torus, many of these things simplify because you don't have to take care of scalaricity, etc., etc. And basically, the only well, I mean, the metric here is just a scalar. So minimizing for the sigma loss is sufficient. And here, what we find is this is the true flat metric, and this is the predicted. And this is the predicted flat metric. They look similar, in fact, they are almost identical. The only thing is that here there is a rescaling that one has to take into account. So if we look back at the way in which the loss function was constructed, basically it was taken as a weighted sum of the three different losses and then Three different losses, and then the tuning of these parameters has to be done in such a way that along as training evolves, sigma, kappa, and mu loss, they all decrease. That's the idea. And basically, this is what we obtained for the quartic K3. So, as training evolves, sigma decreases, mu decreases with some. μ decreases with some jumps that we blame to the learning rate that we have chosen. And similarly, for the kappa loss, we obtain values of sigma of around 0.18. So basically, starting at roughly 1.2 means that we are dropping by one order of magnitude. And if we would ignore that, I mean, we are away from Kayler and that the metric doesn't. That the metric doesn't exactly match among the different patches, then this would be comparable to k equals to 6 in the Donaldson algorithm. So I was mentioning that we considered different activation functions. And basically, we tried training. Basically, we tried training with these activation functions, having fixed the number of neurons in each layer, as I mentioned. And something that is a symmetry exhibited by the metric is a set for symmetry in this case. So basically, if I take a coordinate and I change it by means of these transformations, I shouldn't expect the metric to change. Metric to change by virtue of the symmetry. So we built some quantity that measures this extent of symmetry learned, by which we took a sample of points, we evaluated our neural networks at such points, and then obtained the difference from the metric evaluated at the image of such points. And what we see is something that we also notice from the way in which the losses evolve. Whereas the RILU and the sigmoid activation functions do not seem to be sensitive to this symmetry, the tangent hyperbolic shows a significant decrease as you evolve. So this is sort of an additional check that allows us. Check that allows us to select a better architecture, let's say, for these purposes. So if we move to the Fermacuintic, we could think of different experiments to do that are related to the tuning of these weights in the overall loss. So, for instance, if I would take one. One loss only, let's say I take sigma or mu or kappa, and train only on that loss and see how these losses evolve, how the other losses evolve. This is what I would obtain. So for instance, when I trade with sigma only, I managed to get sigma to decrease, but at the same time that sigma is decreasing, since I'm not controlling the other losses, what you see is that an increase that you observe both in mu and kappa. When you train with mu and When you train with mu, something that happens is that basically the mu loss, the sigma loss seems to basically work, behave erratically. But somehow nicely, you have that both mu and kappa decrease, even though you are not controlling kappa. And the same situation occurs when you change with kappa only. You see that there is a decrease in mu as well. As well, but sigma again behaves erratically. So, this suggests that there is some correlation between μ and kappa, and this shouldn't be a surprise as for Kayla metrics that are derived from a Kayler potential, one expects a matching of patches as well. So, but of course, there is also a tension between sigma and the other losses. If you make sigma to decrease too much, Make sigma to decrease too much, then the other two are going to increase and vice versa. So basically, if you train with mu or kappa only, what you would see is that even if you manage to get some decrease in mu, this can at any point change erratically and bring you back to where you started. So here are some results for again different experiments. For again, different experiments. So we took 28,000 points for one experiment. The learning rates basically tell you the length of the step that you take when you do the gradient descent or when you proceed with whatever optimizer you are considering. In this case, we considered Adam type optimizers. You see that in this case, we have tuned alpha, the alphas that The alphas that appear in the overall loss function, such that in all experiments, the losses decrease. And we manage in this case to get roughly to 0.2, 0. Yeah, between 0.2 and 0.3, depending on the learning rates that you find. Something that is interesting to show here is that Here is that at least the number of points that you choose doesn't seem to be so much of a relevant ingredient, even though, as you can see here, the things lie within the same, roughly the same order of magnitude. Okay, so a similar result we obtained for the dwarf quintic, we have this parameter psi fixed. This is just for 10,000 points only. But here you see that following the same experiment, choosing the learning rates definitely determines certain features, especially here where it seems to be some sort of overfitting after you end up with six after the sixth, seventh epoch. Seventh epoch. Okay, so before I wanted to work with the Tianjiao manifold, so in this case, we need to modify the architecture because we need to predict simultaneously the metric in four different equivalent patches. But as a preliminary approach, we just consider training with sigma only. Even we could consider adding Kler as well because those are. As well, because those are local measures. But whenever we want to get serious and consider the matching of patches, we have to adequate our architecture in such a way that we obtain the global metric in such a way that we are getting neural network approximations for the metric in these four different patches, and then computing the overall sigma loss as well as the overall K-Ler and the overall new measure. We see that there's some decrease, but of course, the results are not as good, partly because we're taking a small number of points, only 5,000 in this case. Maybe we can again get better results with a higher learning rate. But here, basically, what I wanted to sketch is that these are the results for two different activation functions. Okay, so These are some issues, and these are still, these results are not to be taken too seriously. So, we need to train on the full loss, include other measures beyond one single patch, and we hope to have some results soon on that direction. And also, we do not know the killer class, right? So our approach. Our approach is slightly different. Actually, Fion put it as the worst possible way to do it in the list that he had, because we are initializing our metric at a random point. So that means that the metric that we start with is by no means close to Keller. And hence, the algorithm is going to take much more work in order to get. Work in order to get closer than if you would start, let's say, with a seed, something that you do with variations of the Monchan-Per equation. However, since we are not keeping the Keller class fixed, we hope that even though we don't know it, as the algorithm evolves and we get losses that are close to zero, we are getting one. That we are getting one close to Kayler metric, close to some Kayler class, and that indeed this approach could work, even though there are possible issues with sampling. I guess that as you increase the number of points, these issues are going to be less relevant, particularly because we are not starting within, let's say, a KLA class and keeping in it. But this is also something to be seen. So, our main interest was, as I mentioned, that Was, as I mentioned, that quotienting the Tian Yao by a freely acting set three, you can get to a three-family, a three-family family model. And this is, of course, very interesting from phenomenology. But also in the 80s, there was a lot of work done on that Calao-Jau. And Candelas and Calara came up with computations of the Yukawa couplings. And they managed to boil down everything in that manifold to a single integral. So this. This seems like an ideal test for checking how close these metrics could be to giving us reliable information about the Jukawa couplings to tell us, for instance, if we think of Donaldson's algorithm, what k or what degree of polynomials is desired in order to get a reasonable Jukawa coupling or reasonable textures in our physics. On our textures in our physical models. So let me just digress from our results and tell you a little bit about Fritchy flow. So basically that was our original idea when we started working in Calabi Geometrics. And the idea was to use inspirate draw inspiration from Ricci flow in order to come up with In order to come up with a loss such that following some Ricci-Flow recipe, the loss would be minimized whenever the neural network approximation would be close to the Ricci-Flat metric. So, finding the Ricci-Flat metric, as Fabian mentioned, amounts to solve the Montcham-Perry equation for this is the Montcham-Perry equation for a The Monchampere equation for a smooth and real phi and this e to the f times the determinant of the metric being proportional up to constant to the Calabrija to the Calabrija Now one option that was explored in Fabian's talk In Fabian's talk, was to consider phi and minimize or find a suitable phi that gets as close as possible to this to solving this equation. So in a way, we could think that this phi exhibits some parametric dependence in such a way that I'm following some particular track in this space of metrics. In a way that for a given lambda equals to t. For a given lambda equals to t, we obtain the desired Richie-flat metric. This can be thought of as some sort of flow in the space of metrics that suitably correlates between g of lambda and f of lambda. Of course, this is basically Jau's proof that for every f up to a constant, there is a unique phi up to a constant. This is reminiscent of Ricci flow, and Ricci flow is the gradient flow of the Einstein here. Flow is the gradient flow of the Einstein-Hilber action and is governed by an equation that essentially looks like this. For real manifolds, we have a short existence theorem for the solutions. And basically, the problem with that is that as you evolve in lambda, you reach a point, very likely, where your manifold starts developing singularities. Of course, there you have to stop, do some surgery, and then go on. Go on for Keller manifolds. The results are more robust, and in particular, there's a long-existence theorem that guarantees that if you start from an arbitrary Kayla metric, you do your Ricci flow, and it will take you to the Ricci-Flat metric in that class. Additionally, something that we have dealing with already is that keeping the Kayler class fixed as one. Class fixed as one evolving training is not an easy thing. And from Richie Flow, what one gets is that the Klaus class is preserved throughout the flow. So the drawbacks of these are, of course, that I don't want to solve Ricci-flow explicitly because that would mean finding a family of metrics. And I'm just interested in the desired Ricci-flat one. This would also mean that I have to add an additional dimension, lambda, and already dealing with. Lambda, and already dealing with six real dimensions, three complex dimensions is not that easy. So, numerical solutions would involve iteration errors. These iteration errors are going to propagate as lambda evolves, and there are some interesting results that show that these iteration errors ultimately would take you away from the desired retrieval metric. One also, if you Take Kao's long-existent theorem. What you see is that you need a seed to start with. And as people were commenting out this morning, that might be not the case in some particular cases, especially in cases where not all the devices are inherited from the ambient space. And in particular, you need to get a positive representative in order to describe the metric. So we have. So, we have some small success in trying to apply Ricci flow to the torus. And basically, we took the flag torus basically on a square, and then we described an initial metric basically given by some periodic function such that the boundary condition we met, positive, of course. And then we evolved the Richie flow, and what we obtained is that indeed, after a certain time, the curvature. The curvature of the metric drops to zero. But that's in a way very trivial because, of course, the metric will converge to some constant as expected. So instead of looking for a Ricci-Flow solution, what one would like is, as I mentioned already, to find a potential for some sort of bounded loss function such that it has the Ricci-Flan metric as a minimum. As a minimum. So, to draw some inspiration, let's just consider Perelman's entropy function. So, Perelman adjusted the Einstein-Hilbert action and added this function f and basically with this expression integrated over the manifold. And then, the first thing that we see is that there's some sort of normalized volume. Of a normalized volume. And the idea of adding this F is that the differential volume doesn't change as the richie flow evolves. We are physically speaking, well, physically speaking, we have introduced a dilaton. And of course, from demanding that the differential volume is fixed, we get a differential equation that governs the evolution of F. Similarly, if you look at the variable Similarly, if you look at the variations of this or the gradient of this entropy functional, what you get is now an expression that governs the flow for this entropy functional. And here, it looks very much like each flow. We just have added this Hessian of the function f. At first glance, solving these two equations doesn't seem so easy. It's a coupled system of partial differential equations. You would think that the first one, this one, That the first one, this one, could be solved, but then you immediately look at the second and you see that this is a reverse heat equation. So that means that if you start with initial conditions for G and F, you might not necessarily find a solution. Instead, what you have to do is solve for G with an initial condition for G and solve for F with a final condition for F. There is a way to decouple these equations in such a way that we recover the Richie's law. Way that we recovered the Ricci flow plus a constraint on the evolution of F. Again, we have that this is a reversit equation, but now that they are separate, setting the boundary conditions just does the job in principle. However, this function f that we defined, the permanent entropy functional, is not bounded from below. And like this, it doesn't seem to suit our purposes. So basically, what we want is initially what we wanted was to take f as loss function. But instead, if one looks at how f grows along the flow, one sees that it grows monotonically. And basically, the derivative of f is given by this term, by this positive term. So if you ignore f, this will look something like a Ricci loss, but somehow. But somehow, what F is doing is controlling the volume such that there are no weird divergences as the flow evolves. So, maybe a possible loss would be this. It gets to zero basically when you reach the flat metric and it's positive definite. So, maybe having a neural network now that approximates G. Now that approximates G and keeps F as obtained from the constraint on differential volumes could do the job, and this is essentially what we are contemplating right now. We are taking inspiration from how the training evolves, take lambda as some term that is proportional to the number of epochs. To the number of epochs and take an answer for this metric, we have learned a lesson somehow that not putting seeds definitely gives you worse results. So you take a perturbation of this neural network, a perturbation of this metric described in terms of this neural network. So you start basically with the metric being Kayler at lambda equals to zero. And as lambda evolves, the neural network contributions become. The neural network contributions become more and more accentuated. So, and what we want for F is to remain as a bookkeeping device. So, as we compute volumes with g of lambda to evolve, we can obtain f from equaling it to the calabital differential volume. So, So again, we will take a neural network that takes the coordinates points as inputs. At every time in the process, we'll be computing f, obtaining the corresponding Hessian that enters in the loss function, and obtaining a final neural network. We have encountered some obstacles. So, one of those obstacles has to do with the fact that away from zero, the metrics are not strictly KL. And in fact, we observed that a straight And in fact, we observe that as training evolves, F starts to develop some divergences at some points. So, whenever we compute the Hesian, this thing is pushing us away from the desired retrieval metric. We can think of correcting now the loss function by adding extra penalties for f. This is one option. But as we saw, if we look at the Montcham-Perry question, having Equation: Having not this neural network metric as an addition, but instead approximating phi as the output of the neural network would give automatic helicity. So, this would be some other avenue to explore. So, basically, that's what I wanted to tell you about what we have been doing. As it was mentioned already, there is some need for interpretation. Is some need for interpretability in a way, or at least some need to understand what are the best architecture, the best machine learning practices that could lead to optimal solutions or optimal approximation for these metrics. I guess that, well, we are far away from the using analytic expressions for the Calabri Java metrics that are somehow inspired by the results from the neural networks. Symmetry learning is Symmetry learning is some feature that could hint towards base architectures, base activation functions. Maybe this could also be used as measure for architectural selection. The structure of our approach suggests, and you saw it already implemented in Flavian's and his collaborators' work, to other To other examples, such as complete intersections, tourist constructions, and beyond. And it would be interesting to see when all these metrics are going to be put to work towards getting more refined phenomenology in general. So, with that, I just wanted to thank you. Thank you very much. Thank you very much, Damien. So there is plenty of time for questions and discussion. Please go on. So here, I see Paul. Thanks for the nice talk. I have just a small stupid question. A small, uh, stupid question, like a bit of a technical question. When you were computing the um the metric for um the specific cubic in P2, like in one of your earlier slides, um, yeah, I was just wondering why, so when you when you do it and you do it patch-wise, I was just wondering why do you use uh six six patches? Not, I mean, from the ambient space of P2, I would have thought, okay, maybe you just do it in three. Just do it in three? Why do you use six? Yeah, that's a very good question. So the thing is that, well, I mean, you would have to compute the corresponding differential Taylor volume, the differential Fourier Studio volume, as well as the differential Calabija volume, right? And for that, you have to basically. To basically, well, choose some specific coordinates. So you are left with one. But then, yeah, there are some arguments regarding numerical stability that somehow suggest that there is a preferred patch for a coordinate. So computing the volume form implies derivatives, right? So basically, you want the dependent coordinate to be the one that has the higher value of the derivative. The higher value of the derivative so that you don't hit presentations, let's say, of this metric that are close to zero. Okay, so that's why you would not take the, say, z equals zero patch. Well, z equals one. Yeah, exactly. Yeah, I think I need to add extra information in that. Thanks. Okay. Any further questions? Questions so while people are getting their questions, so let me just ask a very, very stupid question, one of the most stupid. When you showed this plot, when you had the hyperbolic, the tangent hyperbolic function and cellular and some other function, I had a feeling that your 10H. Your 10 age went down. So, does it actually mean that your algorithm was unlearning? What does it mean? When it goes down, the tan h function, the blue curve goes really down. What does it actually mean for information? Since you expect this symmetry to be there in the metric, the ideal situation is this delta is zero. Is zero. So, as your metric description improves, you expect this quantity to get as close to zero as possible. Right? Right. So, this is what is happening here, essentially. No, it is minus two. Ah, this is logarithm. Ah, okay. Yeah, I didn't read that. I did logarithmic. Okay, thank you. Yeah. Yeah, stupid, I told you. So, some other question. So, then again, I can go with another. Okay, Nana. Thank you, Amian. Very nice talk. I wanted to know, in general, I mean, how to study Calabiao metric, numeric Calabia metrics with my. Metric numeric calabiometrics with machine learning, how this is better to previous numerical calibrate studies. So, why machine learning improves on that? Let me see. So, so, for instance, one option would be to use Donaldson's algorithm, right? So, what it was done, for instance, in So, what it was done, for instance, in the work of Ashmore and collaborators was to use decision trees to extrapolate. So all this calculation become more and more costly. And somehow, one way to see machine learning entering at least at first order would be to simplify those components. Those computations, the computational time that it takes you to work. Okay, so it saves you, so it makes the things faster somehow. Yeah, and the other thing that it's very interesting, as Fabian was pointing out, is this modularity, the fact that now you can survey a lot of calabills coming from Tauric varieties, etc., which otherwise take you really the so. Take you really the so, for instance, computing derivatives. If you would like to learn the Ricci tensor, you'll have to take derivatives, and then, for instance, the numerical works of Hedrick and collaborators would have to fix a lattice and take numerical derivatives and things like that, that are just costly. The fact that here you have an approximation function, you can take very easily derivatives, gradients, everything you need. Everything you need, so that's that's definitely a computational advantage. Um, but yeah, of course, of course, there are some limits. So, do you hit, for instance, a limit where you cannot drop the K-Lericity loss any further? Are there ideal architectures where this can be improved? What is one question and actually is what's the best that one can do with this machine learning? That one can do with these machine learning approaches? Is it sufficient to do all the computations? Okay. And what is the best strategy? No, because, of course, approximating the metric directly is something that is costly. You have to match patches, you have to ensure that it's scalar, you have to ensure that it remains in the same scalar class, etc. etc. There are maybe. There are maybe better approximations if you take, if you approximate Keller potentials, of course, at the cost of adding additional derivatives. Okay. But then for applications of this numerical AO metric, you think that machine learning works better? Well, it's simpler. It's simple. Thank you. Okay. Okay, some additional questions, doubts, comments, discussions. I see Anna Maria Font, please go ahead. Hi, Damian. Thanks for the talk. Yeah, I have a question. You talked about the quintic, what you were calling the dwarf quintic, or let's say the mirror quintic, but only one. Mirror quintig with only one complex structure parameter. There are several other models similar to this that one can consider. I guess you could also treat those, let's say, and do something similar to what Fabian did for checking the swamp line conjectures and so on. Have you thought about doing something like this? Yeah, somehow I think that. I think that the quality of the metrics that we obtain is not good enough for such studies. So we, I mean, the drop that we get in the loss functions for sigma, I mean, basically the Ricci scalar that we get for the approximation matrix is not enough. Actually, from what I remember of Fabian's paper, they did it following Donaldson's algorithm to obtain the Calabi geometric, but I'm not sure. Sure. Yeah, and similarly, I guess going to, I mean, you did this and you said the methods can be applied to other toric or other complete intersections. But I mean, is this something that is going to be done? Is this something that is going to be done soon, or it's just something that in principle it can be done? Well, at least, Yania, we are working on. So, yeah, as I was mentioning, the problem with our approach is that we rely a lot on symmetry. So, this would not be problematic for what you mentioned already of looking at other families of candelaus as long as this Z5 permutation symmetry is there. There is this classification of candelas and. Classification of candelas and collaborators of what complex structures can be tuned in order to preserve those symmetries. So, in here, our approach is local. And what I was mentioning about Tianyao is, of course, that there, not all patches are just equivalent up to permutations of coordinates or the P3s. So, there, we would have to take four separate local approximations, and this is what we are talking about. And this is what we are doing right now. Of course, this is a little bit costly, but somehow we are in a good direction. I hope that we are able to report on this before the end of this year. How general it can be beyond that? I guess that it just becomes More and more expensive from the computational point of view because basically, so how we do it is we approximate, we have one metric for every patch and then matching patches according to these losses. So, yeah, in more complicated cases, of course, you would get, I mean, already at the Tenyao you are roughly 200 patches. Okay, some more questions. Well, I have another very naive question. And your final comments, final remarks, the very first point somehow tells me that you don't have, with these methods, you don't have a semi-analytic expression for the Calabio matrix. Expression for the Calabio matrix so that you just simply arrive at, say, really numbers. So the metric has this value at this point, and the metric has a solar radio value at a different point. Is that correct? Or do you obtain by patches at a different patch a different metric that you can say it's already semi-analytic? No, in fact, you can so. You get a so so you're training a function, right? I mean, the neural network gives you ultimately a function that's an ugly function that depends on hundreds and hundreds of parameters that you fix. And then basically you do the composition as the number of layers grows. So you have a function, just not a bunch of numbers. Now, one of the questions is if this function works as good on points that have not been. Good on points that have not been seen by your network. Those are these test validations, etc. Test validation sets, etc. Somehow it seems that since you are dealing with, I mean, basically regions that are open but bounded. The neural network approximation function would extrapolate. Neural approximation function will extrapolate well to other points. So, in principle, you do have, I mean, for instance, for the torus, this is a function, right? This is a function that looks very close to the actual metric function. You do have expressions, actually. Not only numbers. The problem is that these expressions are not so easy to interface. Okay, very good. Okay, very good. It's a little bit like any function by polynomial or polynomial, whatever degree. Yeah. Paul has another question. Yeah, just really quick, now that you're on this picture, maybe understood. What are the three knobs that go out? Are these similarities? No, it's just that they are not suitably described in this part. So if you look at the so basically I'm plotting everything but but you see that they belong to other parts but the is actually fine so these are these two points here but they live somewhere else right okay okay great great so an additional comment remember this is the time for opening uh your microphones To your microphones and saying all what you have in the heart. So, whatever you think that machine learning can work on, you can just bug Damian and ask him everything you want. His WordPress sign. They should have talked to the previous speakers. Also, you, also, you. Okay, very good. Well, at this moment, I don't see any urgent question. I invite everybody to. Question: I invite everybody just simply to plug to the talks that are already online. The slides are already available and also the videos. So please go there. And if you have further questions, there will be also time for further discussions. As we announced before, there are also additional rooms, discussion rooms in this workshop. So in case you have a very important question, please do not hesitate to use them. Hesitate to use them. I shared with you already the links. So, if there is no further question, I guess it is a very good time to thank you very much, Damien, for this interesting talk. I think we learned a lot on Calabios and their metrics. So let us see what we can force here in the future. So, thank you very much, Damian.