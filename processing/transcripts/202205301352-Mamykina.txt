So everybody can see and hear, right? Alright, so again I was asked to give a 20 minutes introduction to a field, still there, I'm gonna try. And it's gonna be brief and the way we've decided to split it, so I said my background is in computer science and human-computer interaction, so I'll talk broadly about HCI as a field. And then Mary Shierinski, who is on Zoom somewhere, she will give an example of a project that uses Project that uses computational techniques in sort of within the HCI reality. So, between those two, I think it's going to be a nice introduction. So, I'll talk about what it is, what we can gain from it, and I'll start more broadly, and then I'll talk specifically about HCI in computational modeling, machine learning, and AI. And so, we just had this discussion with Rogershore for lunch. This discussion with Rajesh Rolanch, how do we call all this computational wonders? And I'm going to use AI, even though I don't really feel comfortable using it as describing something we do rather than historical tendencies in the past. But I don't know a better way to call computing that produces new types of decision support for people. So, for the human side of it, it may not necessarily matter. Of it. It may not necessarily matter whether it's mechanistic computational modeling or whether it's machine learning or whether it's Guildhall rule-based AI, but they do get something from it that helps them to make better decisions and choices that more traditional software doesn't. So I'm going to put it all under one umbrella, which is probably not the right thing to do it, but I'm happy to revise it as we go along. So, HCI, generally speaking, has Generally speaking, has two complementary traditions to it. On one hand, it's a sort of engineering design enterprise. It's a design of technologies that useful and usable technologies that support people. And on the other hand, it's a sort of a tradition of scholarship around how people engage with technologies. So when you look at HCI literature, some of it is going to have engineering slant to it. Going to have engineering slant to it, there are systems being built and tested, and some of it is going to be how do people engage with Facebook? What kind of interesting behaviors and traditions emerge on social media? So, not all of it is design-oriented. But why? So a little bit of history, it emerged somewhere around World War II and it sort of spun off of another discipline which is older called Human Factors, where the focus is really on organizing. The focus is really on organizing workspaces. Strong emphasis on ergonomics and physical design. A lot of it evolved around cockpit design during World War II and the whole idea of human information processing, of how people deal with information, react to information, make mistakes with information, and so forth. So that's kind of the origins of HCI and where it really became its own discipline is with Became a own discipline is with the introduction of not just computers in general, but of graphic user displays and personal computers. Because before that time, that was a little history. My dad was one of the early computer engineers, so I used to play as a child with punch cards, and that was the mode of interaction between humans and computers. But when graphic displays became available, that became more of a real interaction, and there became more opportunity. And there became more opportunity for a design of this interaction of how people actually communicate their intentions to computers and what they can get back out of it. And obviously, when, but that was still very much in a professional world for a while, where you could train operators to do whatever it is that needs to be done. But when computers moved out of the professional world, that's when visibility became sort of a household item because all of a sudden everybody knew to be able All of a sudden, everybody needed to be able to interact with computing systems with no specific training and no tutorials for how to do it. But why, again, why is there a whole field? What's the need for this? Design seems to be sometimes straightforward as a discipline. The way kind of we tend to talk about it in HCI is because technologies can have very complex trajectories. Very complex trajectory. So, this is one of my favorite quotes, right? This is a very creative person, and this is the extent of his imagination of the impact of his technology on society. Now, when I look at my kids, who when they were toddlers and using iPads and smartphones, it is not easy to anticipate how technology will evolve and how it will develop over time. And it is also not necessarily So not necessarily the sort of, it doesn't necessarily always fall on the designers to envision how technology develops. There is a very interesting interaction between those who design technologies and those that adopt it. So this is not an example. This is a fascinating book if you're interested in this. We beiker, anthropologist of technology, does a lot of research with archival materials. And this is a book that talks. And this is a book that talks. I have no idea, why fluorescent plates are the way they are. If you're interested, it's an interesting tension between power companies and engineering innovation, and it's a compromise between the two. But another idea is the bicycle, right? So, this was a technology that was shaping over a hundred years, and its form changed so many times because it was a Because it was a tension between two groups of users, one who wanted bicycle for racing, and another one who wanted it as a transportation, who wanted to be dressed like so, and who wanted to be comfortable. And it wasn't clear for a while, for a long time actually, that you can design the same piece of equipment to satisfy both of those groups. And the bicycle went back and forth, and eventually it did settle on the form that we know now. That we know now. But again, just because we're designing our artifacts and technologies the way we think they will work doesn't necessarily mean that that's how they will work in the real world. And technologies medicine is not really that different from other technologies. So clinicians in the room, or when I ask this question, typically a stethoscope is the symbol of a medical profession, right? So when kids want to play a doctor, they Want to play a doctor, they put it around their neck, and I'm a doctor. It was invented almost by an accident where a physician couldn't examine a patient's chest, so there was a piece of paper rolled up, listened, and it actually turned out to be much more superior in terms of his ability to examine. And it took 50 years for that physician to convince medical establishment against very, very strong negative reaction that it is a useful. Reaction that it is a useful piece of technology that should be adopted. So that was not necessarily obvious that it will succeed. 50 years, that's exciting. So and that actually continues to be the case with more modern technologies. Even though EHR now is being very firmly established as a Established as a set of technologies that clinicians use, it's still not without its controversies, right? There is a lot of concern about unintended consequences. We have a whole movement in our department looking at burden of documentation because it created a lot more work for clinicians and for nurses who now need to document a lot more to make it available. A lot more to make it available. There is increased information overload. It's not clear that all this information is actually being used in a way that it can improve clinical practice. And the thing that we all sort of kind of hope the most from EHR for clinicians is the decision support, the ability to make better clinical decisions. There was a lot of concern that the adoption rates are very low and it's not having the kind of impact. And it's not having the kind of impact that everybody hoped it would. So, this is why HCI has a chance of making a positive sort of impact on technology and health and medicine. So what is HCI? It's a discipline that emerged somewhere around the 70s, I think. One of the first HCI conferences was, I believe, in the early 80s. It is an interdisciplinary... An interdisciplinary field that combines computer science, cognitive science, design, anthropology, linguistics, and borrows methods from all of those different disciplines. And again, the idea is to, with the focus on designing systems, computing systems for people and studying how people engage with these systems. This is a very quick overview of different things that kind of how HCI structures. Of how HCI structured. There are theories, and some of them come abroad from cognitive science and cognitive psychology, and some of them are actually native to HCI as a field. So here I'm giving an example of distributed cognition. This theory comes from cognitive science, and the idea there is to view any kind of intelligent system as a system consisting of both humans and artifacts. Of both humans and artifacts. The idea being that cognition doesn't reside in the human mind, in the human head. People don't think and don't operate in the world just by thinking inside their heads. A lot of early research on cognition was based on studies of chess masters and mathematical exercises. But when people use cognition in the real world, they use artifacts. They outsource a lot of the cognitive processes on artifacts. Processes on artifacts and tools that they use in the world. So, when distributed cognition came along, it suggested that studying human cognition inside human mind is less practical and useful, and it's more useful to study how people use artifacts to help them with more complex cognitive tasks. You can imagine that is very appropriate for medical environments where typically decisions. Where typically decisions are made within clinical teams, where each person has their own training, set of skills and responsibilities, and they are used artifacts very heavily. There are a number of methods that the discipline came up with. Typically, they sort of align along three different directions. There are methods for studying human practices and behaviors in order to identify requirements for computational systems. For computational systems. So I spend a lot of time interviewing people with diabetes and understanding what makes diabetes self-management hard, why and where can they use support. Then there are design methods that very much focus on design as an iterative process and ways to engage participants or people who are intended to be users of technology in the design process. There are many different techniques that have been developed. Techniques that have been developed over the years to make design less of a, you know, let's put a bunch of creative people in a dark room, close the door, give them a lot of coffee, and then ask them what happened, and expect some magic, and make it more of a systematic process where there are steps involved, there are practices, and there are skills that can be learned. And then there are many ways to evaluate systems where it starts from something as simple as usability. From something as simple as usability studies, can somebody use this software to accomplish the goals that they have? To controlled experiments where, for example, if we want to use predictions or if we want to use computational inferences, can people make better decisions and choices based on some criteria of better that we've identified? Two, deployment in the wild and study of user engagement, where we develop prototypes, we deploy them. Prototypes, we deploy them with people and we observe how people engage with those prototypes in their own way. And there is a over the years again, the discipline has developed a collection of design patterns or interaction techniques that have been shown to be useful in different contexts that are now available for reuse in commercial systems, for example. So that's kind of a So that's kind of a very worldwide tour of HCI as a discipline, but I wanted to talk a little bit more about HCI and AI as broadly defined. Interestingly enough, it has a lot of shared roots and very common history, where many of the early thinkers sort of were considered to be founding fathers of both HCI and AI combined. I don't know how many of you have. I don't know how many of you have read this paper, so this is a seminal paper in HCI, which you read as a first-year graduate student. Vanovar Bush wrote this paper, As We May Think, where he described Memics, a thinking desk that helps its operators to organize information to make better choices and decisions. So the idea of computers helping humans to reason, think, and make decisions goes way back. Even before computers. Even before computers were widely available. Then, their idea, JCL and Nicolider, another founding father of HCI, the idea of human AI symbiosis, where the two of them who together can do something when combined that's better and more than either one of them independently. And there is some kind of a complementing set of skills and abilities between the two. And the last one, which is actually much more One, which is actually much more recent, the idea. So, this is another article. Mark Weiser was a computer scientist at Xerox Park, which is kind of was this green farm, I suppose, the greenhouse for a lot of HCI research in the 90s and 2000s. Articulated this idea of intelligent environment, calm computing that surrounds people and quietly supports their decisions, choices, and behaviors without them even. Without them even noticing it. It's the Star Trek door that opens just when it needs to and never opens when it doesn't need to. It's the environment that understands human intentions, senses their context, can make inferences based on past, based on the current context, and can provide assistance and support in just the right way. So all of those things kind of have been informing it. Things kind of have been informing HCI for a long time. And these are very sort of similar ideas that have been persistent in medicine also over the years. So some of the early work in AI actually was done in health and medicine. This is a paper that was published before I was born. Knowledge-based AI to support clinical decisions. So these are early days of rule-based and knowledge-driven AI. And knowledge-driven AI that was aiming to assist clinicians make diagnostic and prescription decisions. And what happened over the years, I think, is interesting because it sort of the AI and medical decision making kind of split ways a little bit. And there was a lot less emphasis in clinical decision support on using complex computation. Complex computational models or any kind of models to support difficult decisions and choices, and much more emphasis on helping clinicians avoid silly mistakes. And then, of course, recently there is a reintroduction of all the computational intelligence back into the medicine and health, where particularly with the introduction of machine. Particularly with the introduction of machine learning and data-driven AI. We are seeing, as Nouimi said, predictive analytics is one of the kind of very strong emphasis in biomedical informatics. There is a lot of research and work on using data to form inferences and predictions that can inform clinical decisions and patient decisions outside of the hospital. But all the data. But all the data-driven AI advances brought with them new challenges for HCI. For example, the poor alignment with human reasoning and the four black box model. The more complex data-driven model became, the more difficult it became for people to understand how they make inferences, how they make predictions, and how to engage with them. There's an interesting way to describe this: enchanted determinism, where these are systems that where these are systems that, strictly speaking, are deterministic because they've been programmed, so it's not metaphysics entirely, but they do behave in an unpredictable way that creates this metaphysical aura around them, which actually the paper argues that releases in a way creators of those systems for responsibility for those systems, embedding some of the biases and making, having a negative. And making having a negative impact because we don't know how those systems behave, so it's not really our responsibility. There's a lot of emphasis on fairness and bias, and closer to my work, their impact on health disparities. I'm close to being done. So I wasn't sure whether it's worth working, but I'll give you a flavor. There are many different sorts of ways to. There are many different sort of ways to describe this relationship between humans and AI, and I think my field and both of my fields, biomedical informatics and HCI, are still trying to figure out what does it mean and what is it that we study. So there is the human-centered machine learning. A lot of the emphasis there is on figuring out how to create the right combination of machine learning techniques to meet human goals and aspirations. Then there is the human AI. Then there is the human-AI interaction, which is a somewhat different type of sort of concept. And that has more to do with more complex AI or computational models where closer to the new foundational language models, which could be trained to do things that the creators of the systems did not anticipate. So it's less about. So it's less about we want to be able to classify our phenomena and we want to classify, and more is we have this foundational language model and we want to teach it to arrive at certain inferences. So that's a very different way of interacting between humans and the models. There's a whole field of explainable AI and there is also the notion of human-centered AI which is Centered AI, which is sort of how to create AI systems that are responsive to human values and understand human values. So just to end with some of the open questions for intelligent systems that, for example, I've been working on and many of my collaborators looked at. What kinds of forms of support are appropriate? If we want to support, for example, clinicians with computational models, with machine learning, Computational models with machine learning. So far, we kind of came up with inferences, predictions, and recommendations as the three main types of decision support. They all have their advantages and they all have their limitations. For example, the closer we are on this side, the higher they are on human agency and control. When we generate inferences, people can understand what to do with those inferences and apply them in the way they need to. them in the way they need to. When we give users recommendations, those are much more directly guiding action. Those are easier to work with. There's a lot less of cognitive burden, but there's also a lot less agency and a lot less of control for regular users in how recommendations, for example, are generated. So there are some trade-offs between them and understanding what is appropriate or how to combine. What is appropriate or how to combine them is still a pretty big question. And something that we've grappled with with Dave is the translation of model outputs into the human language. For example, how do we convey inferences in a human understandable language? I think we're going to be talking about it quite a bit here, is when we have computational models. You were talking about how hard it is to have physiologists and mathematicians now take. Mathematicians now take clinicians into that equation, and it becomes three completely different. What is it? The three blind men and the elephant? Because they all see a different part of the system, and it's hard for them to reconcile the different views. And it's also hard to build models that account for the three different perspectives. How do we make predictions actionable? There's a lot of emphasis on predictions. On predictions, and a lot of times predictions are mind-bogglingly useless. Because if there is a wonderful system that predicts mortality but doesn't explain why and doesn't suggest how to change the course of action, it's very frustrating because when you know that your patient is likely to die in the next five years, but you don't know why or what to do about it, I can only imagine how frustrating. Imagine how frustrating something like this could be. But obviously, it's very useful potentially information as long as we can make it actionable. And then how to make recommendations actionable. That's another thing. Right now we are working in a project where we're trying to make meal-based recommendations, make recommendations for next meals. So what do we do? Do we give people exemplar-based recommendations? Exemplar-based recommendations, eat this meal, or do we tell them how to make this meal a better meal so that they can find their own way to do it? So, even there, there are so many different ways to do it and lots of different opportunities to study. Now, this is an area that actually Mary can talk a lot more, probably, because she has training in cognitive science. I do not. But that's a whole other Do not. But that's a whole other field. I'll just say a few words about it. There is a long tradition on cognitive studies in health and medicine. We have a faculty in our department, Vim Locatel, who spends a lot of time studying it. A lot of it has, so there are different flavors of it. There are studies of reasoning, for example, how clinicians reason with information. For example, distinguishing between forward reasoning and back. Between forward reasoning and backward reasoning, where you go from data to hypothesis or from hypothesis to examining data. There is decision-making is another tradition. A lot of it focuses on normative approach to decisions and on biases. There is a lot of research in health behaviors, how people choose their behaviors and how people change their health behaviors and what motivates it. And there is an emerging area of research of how people make decisions with data. So, discovery, self-experimentation, and so forth. I talked very fast and said a lot. So, I think this is pretty much it, but I got close to 20 minutes. That's it. So, I'm going to stop here and I'm going to now sniff. And I'm going to now ask Mir, are you still with us? Can I ask your question? Yes. Can you hear me? Yes, hang on a sec. There's a question in the audience. I was wondering, because it was more now the time came about how the decision making for the decision. But I was wondering if it could also be if this also falls also in the same category use the machine kind of to double the mental. the mental health of the um the patient just through attraction basically. They are those like robots that are basically used to produce the feeling that they're not alone or something. For example we have a robot lab and like the PhD students, they start to really talk to our robots like they are with dogs or something. Because also the robots are not designed by default to give information. The design that you thought you make a hammock when you find five women or something. And then, like you just yellow that work, I like, but it's a really cute interaction. And then I was wondering, I was always wondering how people have looked into this and how they can extend it, because I find it worth something. So, yes, funny enough, this is Transition Timeristov. She's going to be talking about something similar. But yes, there is a lot of research in Sort of AI-driven systems in mental health. Many of them take the form of conversational agents where you can have a conversation, usually typed, sometimes it's a spoken one. That work goes back to the 60s. Again, another early work in AI and conversational AI was ELISA. I don't know how many of you have heard this. It was a relatively straightforward, rule-based conversational issue. Rule-based conversational agent that pretended to be a psychologist that you could talk to and it kind of provided therapy and over it spun off like the whole branch of research in AI and mental health. So yes, that's a great big area of research right now. But also even if they're not designed, like let's say I remember when I had for example a Tamagotchi as a kid. For example, I tamagotchi as a kid, like it's not designed to directly address a specific mental health issue to do action or re-approve health just by like randomly basically. So I think what you're asking is: does it have to be specifically designed to improve mental health or if it's not? If it's not designed, it could go either way, right? It could be just as detrimental to mental health if it's, yes, potentially, but accidental support may or may not work. So there's a whole area of research in how to make those systems useful and usable. And yes, there is a lot of research on robots. A lot of research on robot-human interaction that sometimes works, sometimes doesn't. I don't know if that answers your question. It seems like that kind of accidental Sephora would be really important in this age of physician burnout, like a supportive robot making suggestions about how to adjust diabetes management to the provider instead of some terse, like coarse recommendations? Absolutely. I mean, my current project with the Project with Dave and Noemi and George is a chatbot that provides coaching in diabetes self-management. So people text with people, it provides suggestions, recommendation, it asks them, how are you doing? Did you meet your goals today? So yes, there is a great opportunity for that. And more questions? All right, Mary, how are we going to do it? I think you can share your slides from. I think you can share your slides from your Zoom. Yeah, I'm going to try. I don't use Zoom, so just so you all know, I'm a Zoom novice. Let me see if I can share my slides. Okay. But it doesn't matter. Yeah. I'll mean when you can actually see my slide. Yes. Yeah, we got them. Yeah. All right. Awesome. All right. Obviously, this is. Oh wait, those are Linus. No, those are Lina's. Lina, I think you have to push video on the touch screen. Push the touchscreen one, like top right, top left, video, top left, that one. Hi Ann. There we go. We're back. That worked. Okay, that's done. Alright, everybody, thank you. I'm sorry, I have an injury, and so I'm sitting on my couch with my leg up, so that's why it looks kind of weird. And I wish I could be there in person, but I'm not. I wish I could be there in person, but not. I love Leanna to be able to work with it. So, thank you for having me. Today I'm going to talk about some of the work we've been doing in the space. So, thank you for queuing me up, Leanna. And we're going to talk about an app we built, a mental health app that was built for your phone that helped seriously suicidal participants learn some skills that actually could help them. So, the study we ran was actually on 100 suicidal patients. Hundred suicidal patients who literally could kill themselves. And I'm going to talk about some of the data-driven implications for the app and how the participants use the app. And as you can see, we had quite a big group of people working on this. I actually unfortunately left out Marsha Lenningham who invented the therapy that we translated to the apps. That was my goal. Mental health disorders, obviously. Mental health disorders obviously are a leading cause of disability and death, and evidence-based psychotherapy is effective for many and has been shown to be clinically efficacious and really helpful. How do I get? I can see my slide there. Okay, there we go. So, DBT is a kind of therapy called dialectical behavior therapy. It was amended by Marshall Linehan, and Marshall. Linehan and Marsha actually reached out to us. She, for whatever reason, loved Microsoft, and she thought Microsoft should take this therapy and design it onto a mobile phone so that we could help people who are suicidal, that can't get access to this therapy. There are very few therapists that are actually trained and clinically trained on this kind of therapy. Most people know cognitive behavioral therapy better. DBT is a little bit different. It has elements of cognitive behavioral therapy, positives. Of cognitive behavioral therapy, positive psychology, mindfulness, but it really is the best therapy when people are suicidal. So it teaches you how to cope and it's effective for a wide range of disorders, including any generalized anxiety disorder as well. However, it's difficult to quantify the effectiveness of skills. It's also difficult to quantify how good a therapist is once they're trained in the therapy. So Marcia was very interested in that as well. In that as well. So, the cool thing is: if we move to a mobile phone, we reduce financial and time barriers. You know, you can access this kind of therapy wherever, whenever you want, and possibly increase engagement. And some research has shown that people are more likely to be honest with a digital therapist than with an actual therapist. That's very interesting. However, fewer than 10% of the mental health therapies that are Health therapies that are available online or on the phone are actually based on evidence-based principles. So we did not want to follow that trend. We wanted to do it the right way. So we took Marsha's DVT manuals working with her PhD student, Chelsea Wilkes, and translated them into a very professionally designed and easy-to-use app on a mobile phone. And once you can translate these things on the phone, you can actually These things on the phone, you can actually track what people actually use, how they grow their skill set, and whether or not they're actually feeling better because we can ask them. So what we did was we built Pocket Skills. This is just a couple screens from the Pocket Skills app. It just basically had a little menu that you went into and you see there's E Marsha. That's Marsha Linahan. She's very, anybody who's in DBT therapy knows Marsha very, very well. She has quite a commanding personality. Quite a commanding personality, which turned out to be one of the more engaging aspects of the act was actually taking Marcia and having her look like she was vigilantly watching you when you learn these skills and use the app. And then on the right side, you see the modules on the left side, and on the right side, you see the skills. So for instance, understanding your emotions. We did gamify it a little bit. The more skills you used and practiced, Skills you used and practiced, the more points you got. But interestingly, in the second generation, we stopped paying users money to use the skills. That was the first round. You actually pay people to use more skills. The second round, we just let them unlock videos of Marsha talking to them. And that was way more interesting to suicidal patients than actually getting paid. So, obviously, Marcia is a very effective therapist. And just to give you a little: And just to give you a little bit more, here's an example of check the facts. It's one of the emotion regulation skills. You're feeling a particular way. Marcia asks you what's the threat. So we made this very conversational. It was not using machine learning to generate what Marcia said to you. That was actually just pretty rule-based. It asked the user, what do you think the threat is? The user responds, let's stop being friends with me. Responds, this is stopping being friends with me. And the market goes, Great, you've labeled the threat. So we don't really understand what the user said back, but we ask you, you know, on a scale of one to ten, what is the probability that that's actually going to happen? And then the user makes a guesstimate, and then we say, well, can you think of any other possible outcomes besides the threat? And it gets the user involved in a conversation that makes them cognitively reappraise what they might be going through and teaches them a skill. Spill. And so we took all this paper-based stuff that Marcia had developed in her books and made it into a conversational agent kind of user interface that users could very easily use. It got super high easy to use gradings. And obviously, Marcia is such a commanding figure that putting her online as the actual conversational agent, even though she didn't move, she was a presence. She was a presence, and users told us that was a very compelling thing to do. We didn't know that at the time. It was a happy surprise. So we published this work at our big conference on computer-human interaction in 2018. With every clinical psychology skill that clinical psychologists use to see if there's actually improvement with a patient, we got significant improvement. It's not even recovery. Improvement is not even recovery with these suicidal patients. And we believe what happened was this beautiful circle of self-efficacy that happened. Users would, because it was engaging and easy to use, they would engage with the app, they learned the skill, they cried it out, they actually saw that they got better, and they didn't have to call a family member or their therapist. They could do it all on their own, which made them feel better and like they could actually handle the situation on their own. Handle the situation on their own, which made them use the app more. So it was this beautiful circle of self-efficacy that we discovered. So we went back after that four-week study with 100 suicidal patients and we analyzed what was going on in terms of usage of the app using statistical methods and shady wood. And this was Gina Sue's work as part of her dissertation. And we have all kinds of background information about these patients, their demographics. Patients, their demographics, how they were scoring on mental health scales, what kind of disorders they had. But obviously, we could quantify how they used the app and how they felt before and after they tried these skills and learned these skills. And we had a lot of research questions, but in the sake of priority, I will just like let you read them here. We're going to walk through these one by one, but obviously we want to use it. Obviously, we want to figure out which skills are the most effective and which skills to people use. We had always already learned from previous research that you have to personalize the skills that work for people to handle their general anxiety or stress or depression. But we'd never worked with suicidal patients before, so we actually really wanted to get down into the details about this. So, when did study participants use the skills? Well, most of the time they use them when they're just They used them when their distress ratings, which they had to rate for us, were high. So, on a scale of one to five, most of the time they were going in when their distress ratings were higher than three. And these skills were used in the moment. This is not the first time we've seen this, probably nor will it be the last in our research. People really prefer just-in-time interventions when they are in their moment of distress. And so, what we're trying to do here in this talk is. What we're trying to do here in this talk is pull out design implications for designers. So you need to design for emotional context. You really need to design for being able to help people in their moment of need. For the second research question, we looked at did self-soothe skills result in more or less skill improvement? Turned out that those skills weren't that good. They resulted in less skill improvement. So that whole module maybe is a So that whole module maybe is a less important module in the dialectical behavioral therapy than maybe Marcia even knew. What turned out to be super important was that the skills are helpful if you can complete them. So think about something, you're on the bus and it tells you to put your head in a bowl of ice water. You can't do that. I mean that's just a really radical example, but that is one of the skills and if you're about to commit suicide, that You know, commit suicide that people are taught to do. And obviously, if you're on a bus, you can't do that. So, you have to design for environmental context. And then, were some skills more or less effective for different subgroups of people? Yes, absolutely. So, we know that the higher education you have, usually the better able you are to handle these stressful situations and learn the skills. Situations and learn the skills. But for whatever reason, having just a bachelor's and not an advanced degree meant you had less improvement with some of these skills than other education groups. But we saw lots of variance across different kinds of background demographics for the ability to uptake and really improve with these skills in DBT. And then adding to that, your distort type. Your dysfunction type, the medication you're taking, whether or not you have family close by, all of those factors were really important. So, individual characteristics are super important for skill use and effectiveness, and you have to design for that. So, again, it just keeps coming back again and again in all our work that you need to personalize using machine learning, what kind of skills you proffer up for just in-time interventions. And then, finally, or sorry, second semester. And finally, or sorry, second to us, did skill level effectiveness influence the overall depression and anxiety or skill use improvement? And yes, the largest, best skill improvement resulted in significantly more depression improvements. So skills that work, that you like, that work for you, really, really helped. And as I said earlier, for the people that started choosing the skills that were working for them, they got significantly better in clinically efficacious. In a clinically efficacious way. So the last question here is: Can we predict a particular skill's effectiveness? We did use machine learning classifier to try to do this. We work way better than average. So effective skill use for a particular individual can be predicted. Therefore, we can use machine learning to kind of recommend skills to a user in a just-in-time basis. But obviously, you still have to take into account the environmental. Account environmental context. So, to wrap up, skills that are used in the moment are super powerful and some are more useful than others based on the person. And skills that work lead to overall improvements. So the more we can personalize this stuff, the better. We have to design for all these factors, going back to what Lena said. You have to design for emotional, environmental, and personal context. You can't And personal context. You can't just use machine learning. You have to take all these different characteristics into account. And obviously, this kind of work, it takes a village to do this kind of work. You can't do any human-computer interaction work anymore without machine learning experts, but you certainly can't do any mental health machine learning work without people who are very expert in AI and people who are very expert in clinical psychology. Very expert in clinical psychology, and then people who are very expert in psychology and HCI. So it takes a small team. This was the team, and we continue to work together to this day. I'll take any questions now. Thank you. I can't see if there are questions. I can't see the chat either. I can't see the chat either. I have a question. This is really nice work, and how are you going, because you said you're going to continue in this work, what's basically the next immediate step or what's the long-term goal that you're also at? Yes. You know, it's hard to work with suicidal patients. And I'll be quite frank with you, the Microsoft lawyers shut this whole project down. This whole project down. One of the aspects of working in this space is that the legal teams might be afraid that someone would commit suicide using our technology, even though obviously the opposite was happening. So we had to give this technology back to Marsha Lane and take all the Microsoft name off of it and just give it to her. And Marsha has since retired and it's now sitting on a shelf. And it's nest sitting on a shelf, which is a real, real bummer because during the pandemic, there's been a huge uptick in suicidality and apps like Pocket Skills could really have helped. So it's kind of a sad outcome for this particular project, but on my team, Gina and I in particular, continue to work in the mental health space and continue to build apps to help people. Apps to help people. We've been particularly interested in stress in the workplace because it's more, I don't know, it's just more palatable at Microsoft. And we've been able to really help people with spent time interventions in the workplace. So that's been a very rewarding outcome of all of this. And again, the data keeps showing that personalizing these interventions is super important. Actually now I have a question for the whole room. How, like, for the decision support, how do we have you ever encountered legal problems? Because I guess this is not only like better decision support tool. Who's at fault when this is not suggesting the right thing? And there is a So there is kind of new regulations that have come online from the HHS around decisions for it, where it's viewed as a medical intervention. So I know, for example, in our institutional review board, when we submit doing decision support, there's additional work that we have to do when we're doing the evaluation. When we're doing the evaluation of the implementation, about how we handle clinical decision making and clinical decision making or decision support. And there is talk of treating decision support as if it's a device, having it to be regulated by the FDA the same way they regulate it. That hate has different phases of validation for ISIS. So there's talk about that started just before the Trump administration. We all died during the Trump administration. So that was one of the positive outcomes of the Trump administration, I guess, from a research perspective. But I suspect that people will start thinking about that if that makes sense. Also, I guess different in every country than the either word. Oh, for sure. Oh, for sure. There's also the question of how do you personalize it in all these crazy places. Like, how do you personalize the HR? How do you personalize decision support? It's very complicated and cheap. Excellent. Should we keep trucking? So, yeah. Thank you. That was awesome. Thank you. That was awesome. Well, that actually made me super worried because part of this we turn the ventilator over to the robot and then now if I kill someone I think I'm going to jail. Alright, this thing.