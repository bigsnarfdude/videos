You're gonna see more drivers later on and based on the talk by Jeffrey Anderson this morning I would also want to say you're gonna see plenty of nice colorful pictures but also plenty of equations. So you can just decide what you want to like and what you want to follow and when you want to maybe relax or whatever you want to do. So my work is based on the developments of Ishar, but it's also motivated by the fact that we have an increasing number We have an increasing number of observations, of model forecasts, of climatologies, or whatever you can think of. And I just use the word data sets, which includes everything of that. For me, it doesn't matter if it's a forecast, if it's an observation, or whatever it is. Just a data set. And part of the things I will present will refer to co-located data sets. And with this, I mean there's these data sets that are available at the same time and at the same location. Time and at the same location. And I also noted here the word optimal, because optimal can be everything depending on how you define it. But in my case, I'm taking a statistical point of view. So everything that you're going to see is just statistically view. And I assume I have enough samples from all the co-located data sets that I will be looking at. Okay, so let's start. It's similar to this thing that you saw before, it's a statistic. It's a statistical view about data selection. So let's assume we have a background state somewhere, which is in my case data set one, and we have an observation data set. Of course, they are not at the same place because they are not the same. And in the Hilbert space that you saw before with the char, this distance is actually the innovation covariance. So here I just have one innovation covariance that I can estimate from these two data sets. So the problem that we all So, the problem that we all have is we want to know the truth, right? Because if we know the truth, the distance from the truth to each of the data sets is the arrow covariance, and the angle between the two lines at the truth is actually the arrow cross-correlation. So if we just stick to this, we basically can't tell what the truth is. We could move this point all the way all around, changing the error covariances, change the cross-correlations. You just can't tell from the index. You just can't tell from the information that we have. So, this problem is underdetermined by two degrees of freedom. Now, the assumption that we're all doing when we're doing data assimilation, if it's common filtering or variational assimilation, oh, so just to finish this, depending on where the truth is, we want to know the optimal analysis, which is a linear combination of the two data sets that we have. But as we don't know where the truth is, we don't know where our optimal analysis should be. But now, we But now we assume that the arrows are uncorrelated. In this visualization, we have a right angle between the arrow covariances at the truth. That means all possible solutions of the truth are now aligned in this circle. We still can move the truth along the circle, so we still don't know where the truth is. But we reduced the problem that it's just underdetermined by one degree of freedom. And now, if we want to do the analysis, so as soon as we have an idea. So, as soon as we have an idea about the ratio of the error covariances, this gives us the weight where to put the analysis. And once we have the analysis, this distance is actually the analysis error covariance. And that's what we're all doing in, I call it the classical data assimilation. Like, you can think of a common filter if you want to. So, you should be all familiar with this. Should be all familiar with this. You have the analysis state, we have the column gain, and we have the analysis error covariance. But deriving these equations, we have to assume that the errors are uncorrelated. If we don't assume that, we have to introduce the error cross covariance, which is just x. So it's a statistical estimate of the errors between the background and the observations. And then we end up with these equations. So we have basically the same analysis equation, but K1 changes. K1 changes a little bit. We have additional terms which refer to the error cross-covariance here and also here in the denominator. And here also the analysis error covariance changes because we're getting these additional curves. Now we can also, what we usually do is we define an innovation covariant, gamma, and then the Kalman gain looks like this, and the inverse analysis error covariance. And the inverse analysis error covariance can be written like this. And you can do something similar with the generalized formulation. First, we have to substitute the error covariances to something that are called b tilde, r tilde, and a tilde. So we subtract the error covariances by the cross-covariance, and then we have the observation operator to account for the different dimensions. And in addition to this, we can now define the substituted or generalized innovation. Substituted or generalized innovation covariance, which becomes a very similar form as the original one, but using the substituted error covariances here. So if you write it out, you get these additional terms again. And you can also define what I like to call asymmetry. It's basically the cross covariance minus its transpose. So the cross covariances are not symmetric and not even quadratic. Even quadratic. So we can transpose it, and then we have the observation operator, of course, to account for. And then we can come up with something which is usually quite small, but it's not zero in general. So if we use this, the Kalman gain formulation looks exactly like the original form, but again using the transformed variables. And the inverse analysis covariance looks similar, so you still have the same terms again substituted. Terms again substituted, but we get an additional term which includes this symmetrically, but it's scaled by the two error covariances. So we can assume that, at least from theory, this is quite small, but there is an additional term that theoretically we should account for. So I was asking a question: what happens? How sensitive is our solution? If we use the count for how sensitive is our solution to Sensitive is our solution to the actual correlation if we assume that there are colours. So, this is a very simple example. I just use a scalar case, I just throw out the observation operator, assuming that I have both information available at the same location at the same time. And I just took the derivative of the analysis variance with respect to the correlation between those two. So, rho is the correlation between background and observation. And then I get this formulation. So, basically, you can see. So basically, you can see this is again the innovation. And you can see similar terms as before. But it becomes more obvious if we actually formulate in another way. So now formulating in terms of the sensitivity to the cross-variance. So not the correlation, rho, but x. And x is rho times standard deviation of the two data sets involved. And then we can write this simple equation which just depends on the correlation rho. The correlation œÅ and on alpha, which is the ratio of the standard deviations. And we can throw out all the standard deviations or covariances. So now we can plot this in a two-dimensional space. That's what you can see here. So we have the correlation here, and this is basically alpha, so the ratio between background and observation standard deviation. The first thing you can see is that this sensitivity plot is symmetric. plot is symmetric around alpha equals 1, which is the white line here. And if you want to make it a little bit more simple, and let's say we assume that we have no correlation, which means we are on this vertical line, then our solution or our assumption becomes critical here, because here it's the largest and the sensitivity decreases if the observation error is small compared to the base. Arrow is small compared to the background and also the other way around. On the other hand side, if we look at the ratio which is where the arrows are not that different from each other, so they're kind of in the same regime, this whole plot looks quite linear. So we have a linear sensitivity function. What I want to point out here is you can see the isolines and the colored lines, and everything which is on the right-hand side. Everything which is on the right-hand side from these thick black lines here, it's also like dashed a little bit. These are negative sensitivities, which is not that bad in theory. But if you think what means negative sensitivity, if we go back to this equation, this only can become negative if either this difference or this becomes negative. That means that our cross-correlation, sorry, our cross-covariance gets negative. Cross-covariance gets larger than the smaller variance that we have. And if we would plug this into the common gain, we would get a negative common gain. So we run all kinds of problems with this. So we have to be careful when we have in this negative area here. And moreover, here, this is really dark blue. So we get very, very large negative sensitivities. If we have If we have high correlations and medium, like moderate differences between background and observation. So this is really an area in this simple setup and we have to be careful because our sensitivity is super large. If you make a little error in our assumption, it will be totally off in our analysis later. So this was for two data sets. But now we can go back to the sketch. Can go back to the sketch, and now this is a three-dimensional point of view. And we add the third data set, which Vishar also had before. So now we can estimate three innovation covariances between each pair of the data sets. And we have six unknowns, because we want to know the three arrow covariances from each of the data sets, as well as the cross between each pair of the data sets. But if we do again the assumptions that they are independent, the errors, Dependent, the arrows, we can actually define the truth. It can only be on this side or on the other side, which would be somewhere up here. But the important thing is we can actually statistically estimate the optimal arrow covariances. And this holds for the full matrices, as long as the data sets are co-located. So we can estimate the optimal length of these three distances, which are the arrow covariances, under this assumption. Variances under this assumption, which is kind of nice, and some people maybe have heard it before about three-cornet head or triple co-location method. So this is known for quite a while. And again, going back to the cross-validation approach that Richard talked about, the optimal analysis in this case would be here. And in the difference to what Richard presented, we now have all information available in the same space. So we cannot only estimate directly Directly the optimal analysis from the optimal error covariances, but we can do the same between data set 1 and 3, and between data set 2 and 3, which give the other open circles here. And moreover, we can do an optimal analysis including all three data sets, which is even closer to the truth. So this is what I call triple analysis. It's somewhere here. And you can imagine that this is, if you have a plane between. If you have a plane between dataset 1, 2, 3, this is the point with minimum distance to be tripped. So it's really the optimal linear combination of all three data sets under the assumptions that the errors are uncorrelated. Again, going back to the equations. So just generalizing the common filter equations, but now including three data sets under the assumptions that they are uncorrelated. So now I'm not using k. So now I'm not using k, but I'm using w just to make sure that it's another quantity. So they are kind of weights. So the analysis state equation just looks like this. We add another term for another observational data set or whatever you want to have. And we also just have another term in the analysis, error covariance. If we now derive from this the weights, the individual weights of the three data sets, this is an example for the first data set. First data set. You can recognize the common gain in gear, but the weight is reduced by a factor which is proportional to the weight of the second observational data set. And if you plug this in again and you define some kind of Cal and Gain-like matrices, which are denoted as KD1 and KD2, they're defined here, then you end up with a weight for data set one like this. And you can do this similarly for the other two data sets. This was a direct use of all three data sets in the assimilation, but of course you can also do it sequentially. So you first assimilate your background with one data set, and you use this analysis to assimilate another data set. Again, you have different weights. The analysis error covariance matrix takes a similar format here. You can show that it's the same. And you can also show mathematically that the weights that are given here are. Weights that are given here are exactly the weights that you can derive from the direct formulation, which basically tells us it doesn't matter if you assimilate one data set after each other or if you put them all together and just assimilate them all together at once, which is quite nice. You can show the same from a variational point of view. So, this is how free you are. In the cost function, again, we just add another term over here, and if we derive the gradient, we get this other term over here. So, it looks quite similar. This diagram over here, so it looks quite familiar to us. In the direct formulation, and also in the sequential formulation, if you assume that after your first assimilation, you achieve a point where the gradient vanishes, and you get the optimal analysis after the first assimilation, and then you combine your first analysis with this new data set. You get in a cost function which looks like this. So the other term is exactly the same as we saw here. Is exactly the same as we saw here. And we also saw another term. But if you look carefully, you only have xb and y1 in here. So actually, this term is independent of x. So it's constant. We can just throw it out of the cost function. And you can also see when we derive the gradient, it's just the same as above. So again, it doesn't matter if you assimilate everything together or you do one after each other. This was for three data sets. But now we can think, okay, what? But now we can think, okay, what happens if we have more than three data? I don't want to go into detail about this. I just want you to focus on the blue line. It basically shows you the number of error statistics that you can estimate if you assume that your errors are uncorrelated. So that means if you have one or two data sets, even if you assume that they are uncorrelated, you still have to have another information. You still have to have another information to close your system. That's why this is negative. It's minus one here. We saw for three data sets that, under the assumption, everything is determined. So that's why D is zero. But if you go to higher number of data sets, this increases quite rapidly. And a positive number actually means, in addition to estimating the optimal error covariances, you can estimate cross-correlations. Cross-correlations. And this is quite nice. So you can estimate more and more cross-correlations, but you'll never be able to estimate everything. So I have to disappoint you in this you always have to make some assumptions. But at least, having the idea that we can now estimate cross-correlations, we can also use this in the assimilation, right? So first I was thinking about having a direct formulation of the common further formulation. Formulation of the common future for multiple data sets, which includes the correlation. But this becomes very complicated. Just to give you an example, for three data sets, even if we substitute with a lot of variables, the weight of data set one gets long like this, and compared to the formulation that we have for two data sets, it's much more complicated. And if you want to generalize this to a number of i data sets, I ended up with this equation where the weight depends on the sum of the other weights. Depends on the sum of the other weights here and there. So that's the point where I decided, okay, I'm just going to stop here, because this will never be, probably never be easy enough to implement. So I would assume, at least from the point where I'm now, that it makes more sense if you have correlations to just estimate one after each other. Of course, you have to update the correlations of your new analysis, but I think it's still easier than just trying to handle equations like that. But what I do want to show. But what I do want to show you is again the simple Scala example that I had before, but now we only have a background and n observational data sets. And to be able to show this again on a two-dimensional plane, I assume that all observations have the same variance or standard deviation, sigma r, and that we have no cross-correlations among the individual observations, but we have cross-correlations to the background, which are Correlations to the background, which are all the same. So rho is the same, and it's the correlation between the background and each of the observation data sets. So we can define again rho and alpha. First, we have to calculate the analysis from the observations only, where we can use the simple equation because we don't have any correlation. And we have to update the cross-variance from our observations. And then we can plug the And then we can plug this into the equation. And this is a generalized form of the equation that we saw before, which includes n now, which is the number of observations. But besides that, because of the simple setup, we see it only depends on alpha and rho. So the right-hand side plot is the same you saw before, so it's the plot that you get for n equals 1. But if you increase n to 2, 4, and 9, for example, you can see there's something happening. Something happening. The first thing you can see, if I just go back and forth, that the white line, the symmetry line, moves downwards. Because it's basically the line where sigma r A R is equal to sigma B. And if you have more observations, these become smaller, so the line goes down. We have a common line between all cases, which is this line where the sensitivity almost vanishes. It's around here, it's a bit. Around here, it's a big, a thick black line. So let me just go back and forth. You can see everything is moving around this line. And this is the line, it's actually the one-to-one line in this plot, which in this setup also doesn't change. But the problem that we can see, if I just go to n equal 9, that this weird area of negative sensitivities, which are huge, comes closer to, goes to lower correlations. So even if we have correlations, Correlations. So even if we have correlations above 0.2, 0.3, we can run into the problem of having these huge negative sensitivities because we have a lot of observations and these are including a lot of cross-correlations, we have to be much more careful in this case as in the case of one observation. But just shortly to conclude, concerning the error estimation problem, we saw that if we're using multiple data sets, we can. Data sets, we can, from a statistical point of view, estimate optimal error statistics, so full covariance matrices, and we can even estimate some cross-correlations or cross-covariance matrices, which is really nice. And again, to remind you, when I'm talking about data sets, you can think of everything that you want, as long as it's co-located in the scale. So it can be forecast, it can be another analysis, it can be different observations, or whatever you want to combine, especially if you can. Especially if you can estimate the correlation, so you don't have to assume that they are all uncorrelated. You can use similar setups from the same model and just run it again. The number of estimated error statistics increases if you have more error statistics, sorry, more data sets, but always some assumptions and also some conditions remain, which I didn't show, but you can find them in the paper which is currently under review. Concerning the assimilation, theoretically, the more data set The more data sets we have, the better gets our analysis under the assumptions that we're taking. If we stay with uncorrelated data sets, we can directly assimilate all of them together if we want, which is still quite easy, and we can show it's the same theoretically as if we do it sequentially. If we start including correlated data set, the assimilation, at least the equation, is very complicated, becomes very expensive, so there's probably a lot of things to do. A lot of things to do, and we have to be careful with sensitivities if we go to high correlations and large differences between the arrows. So, I'm going to finish with maybe a little provocative statement of saying we can't have too much informational data sets in this case, but we do have to have the right assimilation algorithms. And for now, we can't include any correlated data sets, so it's problematic, but getting the idea. But getting the idea about what the cross-correlations are, we can include it in your simulation. But there is a lot of work, I think, to go. So, with this, I want to close, and I'm happy to get back to chance. Recognize that you are looking at me. So, Anika, great presentation. Why are you so modest? You include your formulation of the Kalman filter, the observation operator H with no constraints. So formally, your request to be collocated is to some extent obsolete, as you can include the less correlation or less conflict. Correlation or less conflict, less quality easily directly, isn't it? Yeah, you're generally right. So the equation that I was shown doesn't include the assumption of co-location. But of course, if you don't have the information from the optimal error statistics, it's difficult to estimate your correlations. So if you want to use the error estimation, you need this assumption of co-location. And that's actually a point where I'm And that's actually a point where I have some ideas where I'm still thinking about how to generalize this error estimation problem if they are not perfectly co-located. But another thing that you have to be aware of, there is the pseudo-inverse observation operator appearing in my creations and the inverse of it. So Of x. So, where is it here, for example? This plus t, it's the transposed pseudo inverse of the observation object. That means you would have to go from observation space to model space. And if you only have a few observations, you have to be very careful in how you do this. So, in this wringer paper we once met 2010 or so, we use this for estimating. For estimating the correlation length, so this would not be in contradicting to you what you are presenting? I would have to take a look at the paper. So I'm curious about, I guess, what I perceive to be an asymmetry in those rainbow rope. Asymmetry in those rainbow row plots where having a small positive correlation can. Well, so now I'm going to. So you're referring to this asymmetry or to the plots? The plots. Okay. So, yeah, it appears that, and maybe I'm over-interpreting, that a small positive correlation is somewhat disastrous, whereas a relatively large negative correlation is Large negative correlation is less so. Yeah. And if you think about it, at least for me, it makes sense. Because if you have negatively correlated errors, that means, for example, if your background is too high, your observation is too low. So your truth is somewhere in the middle. Whereas if they're positively correlated, they might be both too high or both too low. And then it's really difficult to get an idea what the truth or what's really going out. So that's my interpretation why So that's my interpretation why large positive correlations actually are the big problem. But I think that's probably what we have in reality more than negative correlations. Yeah, I mean I wonder with things like CO2 and methane retrievals whether I would guess you get a little bit of both. It would definitely be interesting to see because this, you know, this work is like super theoretically, but I'm curious to see how it works. But I'm curious to see how it applies to real cases looking for. I think, can I ask a question? Sure. I'm other, just related to what Brad mentioned. So it looks like to me it is more on the observation space, right? Think about my data set to be the CO two and CO and methane. They're collected, but they are correlated. They are intercorrelated. Error correlated. Now, yes, we can understand the error. We can estimate the error correlations. Can you actually project that or infer the correlations or the parameters that you want to estimate? That's a good question. I think it, as far as I understand, it refers to the question that Vishard had before. I'm not very sure if we can really estimate. Parameters. Because we can't observe parameters. So we would have to. Yeah, I'm not sure how to create the different data set even for parameters. What I would have to think about is more to have a better guess on this. Thanks for the speaker again.