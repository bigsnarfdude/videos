The next speaker is Leo. He would be talking about modeling of sparse conditional spatial extremes processes. Okay. Thank you, Rafael. Thanks, the organizers, for this great workshop and taking the time to put this and bring us together. This is joint work with my post, Lakrishikesh Yadav and Nicholas Beck. And really, if you've been, I hope you've been following Emma's talk because I'm basically going to take just one step from whatever she did. Step from whatever she did. So it's going to be basically the same framework. I don't think I need much motivation, but still, if you've looked at the news recently, there's some wildfires in Alberta at the moment of unprecedented surface area burning. And this is taken from the BC government. So this aerial picture taken from Helicopter to assess floods that happened in November, which is not a particularly rainy season, but it turned out that there was an atmospheric river that brought a lot of rain. And unfortunately for the people there, it's a sort of dried lake. So it's a bit below the rest of the sea level. And there were quite a bit of damage for agriculture. For agriculture. Okay, so the Canadian broadcasting company called this, you know, one 100-year return event. A lot of rain fell and the place where they recorded the most rain, these were preliminary estimates, was about 252 millimeter over sort of two and a half day of precipitation. So, if you're not familiar with the geography of the west coast of Canada, this is BC, so there's This is BC, so there's Vancouver Island here, and Abbotsford is very much near the border with the U.S. in a sort of flat plain over the lower mainland. So there's quite a bit of rain over the range here. It's mostly driven by the topography, but this sort of west current coming in. So this is a motivating example of what I'm talking about. Example: What I'm talking about is work in progress. We haven't gotten yet to the data, but there's strong evidence, this is rainfall, that at least for sites that are far apart, we have asymptotic independence. So I'm going to be focusing on the conditional spatial extreme that Emma and others have introduced and just look at how I can handle this rainfall. Can handle this rainfall data. Okay, so the challenge here, of course, is that rainfall is zero-inflated. So I'll have basically a bunch of non-events. And the previous model, I'll basically have point mass, so I'll need to do censoring. And with these models, the challenge, of course, is that every time you have a realization of the rainfall field, the patterns change. And so the computational cost, if you just have a Gaussian process. Cost, if you just have a Gaussian process, well, you can sensor, so you have to evaluate high-dimensional distribution functions of Gaussian, which you can do relatively efficiently. But the challenge is, of course, as you scale, the dimension of this becomes increasingly costly. And every pattern I see, I have to recompute the marginal distribution, the conditional distribution, the precision, sub-precision matrices. And that's basically where the bottleneck is. Basically, where the bottleneck is: these linear algebra calculations have to be done for each time replicant, and as the field grows larger, then the cost also increases. So, that's my slide of the Waltz-World-Ton model. The details I refer you to the previous presentation. So, I'm going to basically look at for now a The process with exponential tail. So we've assumed we've done some sort of marginal transformation, and I'll come back to the marginal model in a minute. And so we assume that, say, at Abbotsford, where there was most of the population who does most of the damage, I have some exceedance, and I'm going to be interested in looking at what happens in other places if there's a big rainfall event. Okay, um, and uh, as Emma just talked about, so I'm going to assume basically that in the limit, um, the behavior at the conditioning side, okay, suitably normalized. This is like a generalized Farito, but once standardized, this converges to unit exponential, is independent of sort of the conditional field. Field with the restriction, of course, that I can recover exactly what happens in the conditions. Okay, so this is the formulation. So I'll have two scaling functions A and B. And this is my residual field, which I'll take to be a Gaussian process here. And I'm going to start first. So, what I want to do is deal with the sensoring in the way I Deal with the sensoring, and the way I will do this is: I'm going to go into the Bayesian world and I'm going to do data augmentation. And that's going to move the computational burden up. And to do so, I'll borrow an idea from Li Kun, Ben Sheby, and Jenny, which is to consider this process slightly modified. So I'm going to add an additive. I know, so just a nugget to them. I know, so just a nugget term in here, and that's going to allow me to shift the burden of the inputation. So, this is not quite useful. Usually, if you are to do inference, and Emma pointed this out, so the problem here is that if I just look at x of s naught, it's Gaussian, but every time replicate, and if b is a function that also varies in space. A function that also varies in space, you kind of have to change the precision matrix, which changes from one time iteration to the next, so that's a bit more costly than, say, just having here a constant. So you can pick for the a function, it has to be equal to x. So I'm just going to take x times some correlation function. And for the simulation experiment, I'm just going to pick. I'm just going to pick an exponential correlation function, but you can pick your favorite model. And one thing that I will be doing, so we've started exploring with this is because of the way my model is built and with the topography in my application, I'll introduce inisotropy directly in this A function. So, because I don't expect, for example, rainfall. For example, rainfall in British Columbia, there's the Cascade and the Rockies, and of course, what happens sort of in the mainland as I go north to south is pretty much dependent. But then rainfall tends not to cross on the other side. So there's a lot of rain basically close to Vancouver, but then when I get to the Okanagan Valley, that's a desert, and rain doesn't cause any. Any questions so far? So there has been quite a lot of recent work on this conditional spatial extreme. So it was introduced, it's been published in 2022, but it's been around for five years or so. Okay, so I think from what I've seen, the literature is evolving fast, so maybe I've The literature is evolving fast, so maybe I've missed some. Most people use two stage estimation, right? Everyone standardize, take their data, standardize them to Laplace margins, and then fit the model to the standardized data. Except for some, so whatever Evan was talking about, and Silius also has an application with INLA that's mostly frequentis inference that's being done. That's being done. And in the original paper by Wadsworth and Tone, there was a mention that this model scales well relative to other threshold exceedance model. And it's mostly true, but the bottleneck in most extreme value model is the censoring. So, of course, if you don't censor that model, it's computationally cheap. But if you want to fit a Brown ResNIC process without sensoring, it basically amounts. Censoring, it basically amounts to up to the normalizing constant, just like a Gaussian lighting. I'm not the first person, we're not the first person to consider censoring. So Jordan and Simon have also two papers with Jonathan Tan where they use composite likelihood methods, triple-wise composite likelihood, for the sensoring in the UK. So what's my objective? So, what's my objective in this work? Well, we're trying to sort of establish something that's computationally feasible. So, I'd like to do high-dimensional spatial inference, but with sensoring, so I'm not using anything fancy like neural networks still, but good old likelihoods. I want to work in a Bayesian paradigm, so I kind of want to use full likelihoods and not composite likelihoods. And I want And I want, and maybe I'll scale down on my emissions to estimate the dependence parameters and the margins jointly. Okay, and my reasons for doing that is if you think about the way the process is built, the normalizing constants are a function of the value of the conditioning side. And that value That value is in the tail, and the tail is highly uncertain. So, if you just perturb these x-naught values, it perturbs the a and b function. And so, I expect that some of this uncertainty, you know, in principle would percolate. So, by fixing them, you kind of reduce the variability in a way. Okay, the second reason for trying to do this simultaneous estimation is, of course, the Estimation is, of course, the margins are not Laplace in the model. But I'll come back to this point in a couple of slides. So what will I do if I want to do the sensoring? I'm going to use INLA, but I'm actually not going to use INLA. I'm just going to use the SPD approximation for the precision matrix. So I'm going to build a mesh, extract the Extract the SPD approximation and use this in an MCMC. Okay, so this basically is what Emma was presenting. So I'll extract the particular form I have is, so I have these basis functions over the defined triangulation. There's some Gaussian weights here, and I have a second nugget term. So this one is basically to a Is basically to account for the fact that I have this finite element method approximation, and of course, the data doesn't fall exactly in their interpolated value. So my residual field is going to be basically some variance times the A matrix that projects the observations onto the tessellation plus the nugget term. So, how do we build this mesh? Okay, so I have this constraint that the conditioning site be zero, so that the value of the random field be zero at the conditioning site. So, what we do is, following work by Silius, essentially you create the triangulation and you force the mesh to have a vertex at the moment. A vertex at the conditioning site, and then you shed it, right? You just remove that and you extract the self-precision matrix, and that means you satisfy the constraints. And so why do I use the SPDE method? I mean, Doug talked about it in his first lecture, but in a way, if you want things to scale, you need sparsity. Whether it's warranted or not is a different question, but if you have sparse matrices, If you have sparse matrices, everything, you know, the dependence is local. So, when I'm going to do my imputation, things depend only on a few neighbors. So, instead of conditioning on a very large number of observations, I can condition on a few. So, in Lique's paper with Jenny and Ben, what they did was to say, okay, well, we add the nugget and at every itch, and we basically. Every and we basically impute a latent field at every point in space and time. And the trouble with that is, of course, if you're imputing using, say, so in the paper they were using Metropolis-Hasting step, you know, you have the cost of evaluating a Gaussian process density at every point in space and time, and the process. Time, and the process is dimension n. And so you've basically shifted the burden, and it doesn't scale when you increase the number of sites. Whereas if here I use this SPD approach, well, it really just depends on the weights. If you're inside a triangle, it depends on at most the weights of the vertices that are around. That are around. Okay, so this is actually, you know, at most a trimari, normal problem. It was before, well, it's univariate, but it depends only on three elements of the W. And so I can basically do this very efficiently. And you can also think about removing the W's and working directly with the Z using some linear algebra tricks. So you can use Sherman-Morrison to scale this. Scale this, and this turns out to work quite easily. So, the trick for you know, you can do the imputation with the nugget, but really, the trick is to scale it is really to make the dependence local. And then the imputation is quite fast. So, here's the sort of IRA formulation, okay, with the sensoring. So, what happens is if I've observed the value at the site, well, I just get a, you know, you know. Will I just get a univariate normal density contribution and the CDF if it's censored? The Z process comes from my SPD approximation. And here I've kept the W, but I could also integrate over. And I have some priors. So in practice, what I'll do is I'll just take the data and I could just, you know. Uh, and I could just you know truncate the zero, but I can also truncate if I don't believe the model is a good approximation for all the data, I can truncate at a much higher level and then just do sensor, sensor these values and use this in the hope that it's going to provide a somewhat better fit just for the tail. So, in simulations, we've just generated models at least to see if. Generated model, at least to see if it works. We generate data from the model and we sensor observations below the 75th percentile. We've tried modulating signs over the W's. It didn't seem to help much. So the W's is a conjugate normal normal. So we use guest sampling. We've used random walk, metropolis, Hastings, and MALA. And I also sometimes do second-order approximation. Do second-order approximations and then some of the parameters. So the A and B functions, they're quite correlated, so we block them. Okay, and it seems, so these are the model parameters with four chains. Of course, if you internationalize them at stupid values, it takes a little bit of time to converge. But this is quite fast. And the W's and the Z's, the Z's are a latent variable. They're kind of one-off, so I. They're kind of one-off, so I don't expect to necessarily retrieve them exactly because I only have one observation to infer them from. And the marginal fit seems to be okay. The more parameter, the more complicated the functions you put in, of course, this kind of becomes harder to estimate. In particular, so Emma was talking about this x naught to the power beta or anything to the power beta. Beta is very close to zero, all of a sudden I. All of a sudden, I end up with a model which has two nuggets. So the sum of the variance is identifiable, but not the individual term. And likewise, if I have anisotropy and there is no anisotropy, well, the angle is kind of uniform. So that's also, there's some identifiability issues that do not exist unless some of the values are restricted. But in practical settings, then the model tends to behave for you. Okay. Okay, so I'd like to come back to the margin. So I said people fit Laplace to the whole distribution and then kind of focus on exceedance. Basically, you filter out the data and you just keep the ones that exceed at the particular conditioning site you're interested in. Now, if you look at the model formulation, so I know that the conditioning variable above its threshold is unit exponential on the standardized scale. Unit exponential on the standardized scale. And I've also specified that if this conditioning variable takes the value x naught, which is an exceedance, I have some form of normal. The parameters don't matter, but this is exponential, this is normal. And so this is kind of awkward because, unlike max stable models, where you know that you have a distribution, the margins are GeV, in conditional models, the margins are. Conditional models, the margins are specified at least partly, but there's no closed-form expression. Okay, so what I could do, of course, is if I say, well, okay, I'm going to take these to be the distribution functions. Okay, for now, they're just unspecified. So this is going to be my real data scale, right? Rainfall zero plus something plus that potentially heavy tail. That potentially heavy tail. And this is the standardized scale with the on an exponential scale. Okay, I can just basically add this marginal transformation with a Jacobian inside my model. And in principle, this goes through. And so I could estimate jointly both the margins and the dependencies. There's a caveat, of course, is that I know that sort of the district. Sort of the distribution conditional, but if I wanted to know the marginal of x, which is hidden in there, I'll have to somehow get it numerically. There's no closed form. So what I can do, so what do I need? Well, I need the quantile function of x of s given not. Okay, so I need basically this guy. This guy So I need the marginal this which for now we are so I need the marginal density and for that I also need the quantile function for the Jacobian and for the data on the real scale I need the distribution function and the density and that but that's less of an issue you can fit whichever model you prefer and then you get Whichever model you prefer, and then you get those for free. So if I want to get the quantile function, what we've been doing so far is just simulate data from the model and use the empirical CDF as a sort of Monte Carlo approximation. And the annoying part, of course, is that the margins are different at every site and every time replicate. So you really need to do this kind of pointwise for the entirety of the data. The entirety of the data matrix. So, if I want the density, I can do this pretty efficiently, sort of in a Monte Carlo way, but that turns out to kind of give a computational model next, to which we don't have a good solution for the moment. So, this can be, of course, parallelized, right? The data are conditionally independent in space and time. We don't have a time-varying model, but in a number of observations. Model, but you know, in the number of observations, but still, this takes a couple more seconds than the rest of the process. I could also approximate, if I wanted the whole density function, I can also do a Laplace approximation. And the purpose of this picture is to show you. So, this curve in blue is a Gaussian approximation to the marginal of x, which is not good. This is the Laplace approximation, and under it. Laplace approximation, and under it, which you can't really quite distinguish, is the true distribution, the true density function. And so if you need it, I don't need it, but if you needed the marginal density of x of s, turns out that if you do the Laplace approximation, the solution for the conditional mode is very nearly linear. So it looks flat because this is a constraint optimization problem. So it's basically u until some. So it's basically u until some point, and then it's linear in x. And you can solve this very efficiently for a large number of x. So if you want the entire density, you can get this efficiently. But for the purpose of the MCMC evaluation, I only need it at one specific value of x. So this turns out to be way faster. Okay, so before we introduce the marginal component. Before we introduced the marginal component, it was, I would say, relatively fast. So, with the SPD approximation, sampling about 500,000 draws from the posterior took about 10 hours without parallelization on a single PC. Now, if I consider, and that's about the order of the application I'm interested in, a thousand spatial sites and 100 time replicas. Spatial sites and 100-time replicates, the marginal quantas and the density using Monte Carlo adds about 20 seconds to the likelihood cost. So this is quite a big drawback of adding the margins. And this is still work in progress, so we're trying to get this to work. Okay, so just to basically wrap up, so we're combining tricks from Combining tricks from mostly people in this audience and different parts of the model. And really, what we're using is trying to do this amputation, but using Gaussian market random fields for the residual process to kind of keep things tractable at the amputation level and use MCMC to do sort of the full likelihood. And my hope is that we're going to be able to assess, for example, whether there's much. For example, whether there's much added uncertainty due to the margins or not. If not, then we can be happy with just doing this two-step approach. And otherwise, then we have to rethink a bit how we carry this through. So some work in progress. So we're working on the data application, but it's still a bit rough. So I thought I would rather skip it in the interest of time. More efficient implementation and trying to reduce the bottle. And trying to reduce the bottleneck, and as I said, just compare it with the two-stage approach to see if there is a lot of uncertainty that's getting leaked from the second part. So these are some, I'd like to acknowledge some funders. Thanks for your attention. And if you have any questions, I would like to answer them. Thank you for the work.