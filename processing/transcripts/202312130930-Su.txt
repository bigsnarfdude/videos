And so this talk will be about privacy and its application on sensors. So there's no need to motivate the importance of privacy because our data contains lots of our personal information and the data now can be accessed to many tech giants. So this could be there's some potential issue. There's some potential issue. So, and so there's already some ad hoc approaches to deal with this privacy, possible privacy leakage. And one simple approach is just to delete the name, make it anonymous. So, this is the Netflix competition where the users actually, their identity are removed. But, however, there's an algorithm developed by these two data scientists showing that by relating different data sets, okay. Different data sets. Okay, so this is IMDB, another, so a public database for image for movie rating for movies. By relating multiple data sets, possible to identify users, unique users. And so as a result, the second Netflix competition was cancelled. And another, yet another ad hoc approach to hopefully to protect privacy is to only release the average statistic. Average, the average statistic. For example, to release the average minor alien frequency. However, a paper a while ago showed that this simple average statistic can contain some information. We can identify whether some user is in the data set or not. Suppose the data set consists of diabetes patients. So if we know that somebody is in the data set, so there's some privacy, there's some issue. Still, some issue. So, it looks like we have to live in a world without privacy, but however, this is terrible. Okay, we have to live with privacy. Privacy is important. And so in 2006, for computer science, since work, MacShaffe, Frank McShaufe, and Bobby Newsy, and Adam Smith introduced differential privacy. And looking back, later, people realized that in a paper by Larry West. Realized that in a paper by Lei Westman and Shu Henzhou, showing that actually different privacy is rooted in statistics, especially through hypothesis testing. So now let's first lay out the setting for differential privacy. So in differential privacy, the data set is just a data frame. So each row is a data set, is a person, individual. Each column is some feature. Some features can be released. It's fine. It's fine, and but some features are very sensitive, maybe for example, salary. So, a typical mechanism in differential privacy is to release the noisy version of the original value. Okay, suppose I'm interested in the average salary of people in this data frame, data set, and then what we can do to protect differential price is to add noise on top of the true original salary. We add a noise. How much noise do we need to add? How much noise do we need to add? Okay, so this is what we need we will discuss today. So, this is basically the setting for differential privacy relating to hypothesis testing. So, in differential privacy, we assume that we know everybody. We know that Bob, Charlie, Dave are already in the data set, but we don't know whether Alice or N. We don't know whether it's S or S prime. Okay, so this is a really natural for us to set this, frame this as a hybrid. Set this frame this as a hypothesis testing problem, right? And on the now, Alice is in the data set, and on the alternative, it's S prime, it's any. And the basis for us to do this hypothesis testing is to use the averages, is use their outcome, it's the outcome, output of the algorithm. For example, it's the noisy salary. So, intuition is that if it's difficult to do this hypothesis. To do this hypothesizing in the sense that it's hard to be very confident to say that we have very small type 1, type 2 errors. And in this case, we can say that the privacy of Edison and is well preserved. So this essentially underlines the intuition of differential privacy. So far, everything is old. And now let's try something new. And so based on these papers, especially we will focus on the first paper. We will focus on the first paper most of the time. So, this is joint work with lots of amazing collaborators: Jing Shu, Aaron, Chi Chi, Qi, Ling Jing, Wu Xing, and Chen Di. So, this is old. This is new. This is what we want to use today, Apple Differential Privacy, and this is Epson Data Differential Privacy. And first of all, they are both interpreting privacy through hypothesis testing. And now comes the first difference. The first difference that we are The first difference that we are using the type 1, type 2 average trade-off in high-versus testing as a privacy measure, and but in epsilon data, they are using the worst case likewhere ratio. And so our privacy parameter is a function defined on 0, 1 interval to mapping to 0, 1. So essentially mapping type 1 error to type 2 error. But in epsilon data, they are just using two parameters. And the typical way to achieve privacy here is to add causing noise, but in Epsom there's about plus noise. Okay, now let's try to introduce FTP. And so and so this is the hypothesis testing formulation, right? On the null is S on the alternative as S prime. So now let's use probabilities distribution. Suppose under the now the every Now, the average salary, the noisy average salary has this distribution p, where p is the probability distribution of the outcome if the reality is h naught. On the alternative, if the reality is h1, and then the probability distribution of the noisy cell is q, p and q. So it's, and we are deciding whether it's from p or q. So we have defined type 1, type 2 errors. Type 1 error is that the reality is H naught, but we reject it. But we reject it, we choose q and we choose h1. Likewise, we have type 2 error. So then the trade-off function between two probabilistions defined on the same space, measure space, is defined as follows. Given any alpha between 0 and 1, we can think alpha, alpha can be sort of as the type 1 error. And the mapping is the minimum possible value of type 2 error such that the rejection rule is. The rejection rule is at level at most alpha. Okay, the type two error, the type one error is a catch at alpha. So, this can be realized by the infinite can be achieved by using the Neyman Pearson lemma. So, this is just the likeworld ratio test, right? And later throughout this talk, we will use f, a single light f to denote a generic trigger function, p of p cubed. And f is trader function if only if f is convex, continuous, non-increasing, and the moreover is smaller than or equal to one minus. Is smaller than or equal to one minus of so this now we can define the uh of differential privacy FDP randomized algorithm n is said to be FDP if and only if for any neighboring data sets S and S prime which differ by only one person for the for the other people in the data set they're all the same and for any two neighboring data sets S and S prime the the channel function is always larger than or equal to f Is always larger than or equal to f pointwise for any alpha between zero and one. So this means that on the left, the type two error is always larger than the type two error on the right, right? Having a larger type two error here, larger type two error means that this hypothesis testing problem is more difficult. So this means that, okay, it's not that easy to break the privacy between S and S prime. So there's a lower bound, which is given by F. Which is given by f. So, this shows that to tearing apart at n is at least as difficult to test t against q. And here, by the way, so the randomness of this mechanism comes from the algorithm m. So, m is randomized. Instead, s and s prime are fixed. So, this let's recap what is epsilon delta dp. Epson delta dp. Epson delta dp looks as follows. Okay. For any metropolitan space, E, it must hold. So take a look. If data is zero, if data is zero, there's no this term, right? If there's no such term. So this means that the likelihood ratio between S and S prime is between, is no more than e to the epsilon. So the log likewise ratio is no more than epsilon. So this is a dp. So the result you So, the result due to diversity and the shui touch can be adapted to our language of FDP, which shows that actually epsilon del DP is a special example of FDP. And especially in the sense that the trader function f here looks like this. It looks like this. So, at this point, the length here is delta, and the slope here is minus e to the epsilon. Here is minus e to the epsilon, and everything is symmetric with respect to 45-degree line. Of course, by then there was no FTP, so we are just translating their result to our own result. So, but however, it's a little bit ad hoc, right? Why should it be this four segments piecewise linear? Why, right? And it's a bit ad hoc. So, that's why this motivated us to develop our FDP. But now, let's try to gain a Okay, but now let's try to gain a more insight between Fdp and the epsilon delta P, their relationship. So, for a simple fixed algorithm, F M, there could be, it could be described by multiple pairs of epsilon data. We can always make epsilon slightly larger and data smaller, so on and so forth. So, finally, we can draw many, many such piecewise linear functions, and their upper envelope is just the true function f. Just the truth of function f. Okay, so given a trigger function, we can recover epsilon delta. We draw a line here. So here, this point has height one minus delta. We draw a tangent line and do everything in symmetric here. Okay, so we get we get a many, many case. And finally, we can recover all the epsilon data pairs. So this shows that they are somewhat equivalent, right? Epsilon delta and f. Right? Epsilon data and f, but we should be very careful in interpreting this equivalence. F is only equivalent to epsilon data for infinitely many pairs of epsilon data. A single pair of epsilon data is not enough. It's not enough. But the good thing that here we are using a single F, single parameter to encode all the necessary information of epsilon data, infinitely many pairs of epsilon data, which is technically. Data, which is technically more convenient. So now f is too general, right? Can be any convex continuous function. So now let's try to focus. So we focus on Gaussian differential privacy. Gaussian differential privacy is where the chiller function is given by the chiller function between Gaussian, this and Gaussian, this Gaussian with mean mu. Okay, and this admits a closed-form expression. And an algorithm is said to be mu G D P if the child function. GDP if the child function between any two neighboring data sets is larger than or equal to g mu and we will explain why this is important. This is because of some kind of universality. To achieve different privacy, the easiest way is to use this simple template. Given any statistics you are interested in, we just add noise on top of it. How much noise do we add depends on the sensitivity? Sensitivity means that if we perturb Sensitivity means that if we replace one individual by the other, what's the largest possible difference you will see? Yeah, excuse me. And also depends on your priority budget, mu, okay. And we have this relationship. Add on this identity holes, and we can achieve mu GDP. Sigma is noise. Okay, sigma is noise and this relates a celebrated result due to David Blackwell. So David Blackwell is basically saying, so roughly speaking, so this results basically saying that the trailer function is basically the most informative information for us to do to do hypothesis testing. If the hypothesis testing will be pro is invariant under Is invariant under post-processing. However, this is not the case for other differential privacy definitions, such as which are mostly using ready divergence. So this definition of random divergence, when gamma has Q1, this is basically KL divergence. And so there are lots of privacy definitions. They are all based on Randy divergence. However, this simple reduction. However, this simple result shows that when the for one epsilon, so this is a Bernoulli distribution, this is normal distribution. And first of all, radio divergence will think that the Bernoulli are closer, having smaller divergence than that of the normal distribution for any order gamma larger than one. But however, recent total variation, existing the obvious. Right, this is the opposite. And the total variation perspective will say that the normal distribution, the balloon distribution will have a larger variation than normal distributions. Okay, it's opposite. So this shows that terrain divergence is not, is a little bit, is lossy. It does not contain all the information we need to do post-processing. So now let's move on to competition. So, competition basically is where we analyze the assembly data set multiple times. And as we ask more questions, privacy will degrade, right? But how much privacy, how does privacy degrade, of course, is a question. So, this is the composition. Roughly speaking, what's the composition? Composition is that we ask two questions. Of course, the second question can be adapted. And then there are And then their answers are putting together M1 and M2. And the composition is just put everything together. I ask you what's your height? And what's your weight? And we have now it becomes two-dimensional. You can further increase in the dimension. And we can define a trade-off function between a test product of two tradoff functions. Suppose your first mechanism Their first mechanism M1 is FDP, and the second one is GDP. Okay, and then their composition is also FDP. And the shield of function is given by this mutual function. This mutual function is the tensor product between P times P prime versus Q times Q prime. So when each G, for example, it's mu G D P and then there comes And then their composition is Term U GDP, and the resulting mu is the outnumber of all the parameters. So, this is our theorem saying that under composition, everything will still be FDP. And this is most important theorem in our paper is showing that, okay, under very general conditions, suppose your algorithm can be written as a composition of many, many. A composition of many many different steps, and the number of steps is increasing, and then the ultimate final privacy lost is basically given by mu GDP, Gaussian differential privacy. So this really doesn't look like a central limit theorem, but I can ensure you that the proof actually is based, it's basically based on the central limit theorem through the log likelihood ratio. likelihood ratio. So the interpretation is as follows. If we are dealing with a very complex algorithm, very complex algorithm in the sense that it's the combination of many small steps, for example, deep learning. Why deep learning follows this example? It's because deep learning is basically medium of back propagation, right? Each back propagation can be sorted out as an algorithm. So it's a combination of many, many. So it's a combination of many, many back propagation. And then their overall privacy loss can be characterized by GDP. Can be characterized as Gaussian differential private. So this shows that now ultimately everything is reduced to estimate a single parameter mu. So GDP to general FDP is just like a Gaussian random variable. Just like Gaussian random variables to general random variables because of central limit theorem. Okay, and we also have a final sample guarantee. So, this is exact. There's no approximation, there's no symptoms, but this only applies to pure DP. Okay, so data is zero. So, in this case, we have this bound. But this approximator bound, by the way, if we want to exactly compute this value, it's going to be sharply. Compute this value, it's going to be sharply complete. What's sharply complete? Anyhow, it's more complex than NP-complete. So, here's the approximation error is basically one over k. But we know that for central limit theorem, the approximation error is basically one over root k, right? Why is it to be very essential? It's really because here, in order to leverage the layman-PSM lammer, we have sometimes we have to do randomization at some single time. Singleton. So this randomization was smooth out one over root k to one over k. So this is an example showing that when only we have the number of compositions only 10, we can already have very good approximation. Red is the exact composition and the green is given by GDP central meter theorem. You see, red and green are very close to each other, right? But if we are using absolute data, then of course it's impossible to enclose this small. it's impossible to enclose this smooth curve using piecewise three segments piecewise linear segments okay now let's move on to maybe i only have 10 minutes and now let's move on to applications so first of all it's deep learning so deep learning is uh used in high-stake decision making already so privacy is a very severe issue so this is a this is a version of private deep learning so roughly speaking in private deep planning so roughly speaking in private deep learning first step is to do first modification is to use gradient clipping in the sense that we do not want if the gradient is too large we project it to the ball of radius r but without changing the rate the direction and the second is gaussian mechanism we add noise to to the to the average to the average gradient so deep learning is basically sub something class composition Subsampling class composition subsamplings because we have to use each time we need to sample maybe 60 for images. Composition basically we need to do back propagation for millions or billions of time. So we have this theorem. This theorem is leveraged using the stop sampling theorem and as well as the composition theorem for GDP for FGP. So this results saying that the overall privacy loss of private deep learning is. Overall, privacy loss of private deep learning is mu GDP and the mu is given by this closed form expression. So, this is very convenient. It only involves the mini batch size and the total number of data points, and the number of iterations and the sigma the noise level. So, this is our results. And so, this shows that when the privacy loss is, this is on MNIST. So, basically, it's changing accuracy 95%. So, this is our privacy estimate. This is our privacy estimate. This is given by Google Brain's Moments Accountant. So, Google Brain has a technical account for privacy. And you see, our red curve is uniformly above the blue dashed line. The higher, the better. The high means that our privacy guarantee is stronger. And our estimate is far better than the one by Google Brain when accuracy is. Accuracy is 97%. In this case, Google brands using Moments accountant has epsilon 7.1. This means that the lack of ratio is more than 1000. So in this case, of course, we won't think there's privacy. But however, our analysis shows that mu is only 1.13. So basically, it's still with one standard deviation. So there are still decent privacy left. Okay, so this shows that we can reduce. Okay, so this shows that we can reduce noise, right? We can add less noise since we get better privacy guarantee and we can add less noise to match the same privacy guarantee. So this gives us a better test performance. Okay, now I only have five minutes to talk about another application, the 2020 US census, which is a decennial US census every 10 years. So, and this is the first time where differential privacy is used for the first time in history. For the first time in history. Okay, and the statistics should be released at many different levels from national level all the way to the like some county checked group and the block levels. So for example, but most statistics are integer values. For example, I'm interested in how many individuals are older than 18, are more than 18 years old in New York. Okay, so this is an integer. So in this case, we cannot add Laplace No. This case, we cannot add Laplace noise or Gaussian noise. How can we do it? So, this is a way to achieve it developed by Kanem Kamak and the standard. So, basically, they introduced the notion of discrete Gaussian mechanism. So, basically, this is discrete Gaussian distribution. So, basically, the same probability distribution, but now it's only supported on integer values, integer, negative, and positive integer values. And so, the mechanisms just to add a So the mechanism is just to add a Gaussian noise, the three Gaussian noise as integers to the original statistic. So this is really used in 2020 descending on US census. So they show that this gate can ensure privacy, but what they do is to first use central concentrated differential privacy and then translate it to epsilon debt. So our question. So, our question is: Is it possible that the FDP can make a difference? So, this is possible because here just competition, right? Because we have to release the statistic at national level, state level, and many different levels. And all combined, actually, we will be releasing many, many different statistics. So that's why there's composition. And so, to this end, we first need to evaluate the cheetah function for Gaussian for this loop-Gaussian random variable. And we have this expression. And we have this expression. So, just no worry, but what we need to do is that it's basically the closed form expression. And now, use this closed form expression. We can account everything reduced to compute the tensor product. So, this tensor product, so FK, each is given by a discrete Gaussian, discrete Gaussian distribution. And their overprivacy loss has this expression. This expression is epsilon data dp, and the relationship between epsilon data is given by this formula. Okay, of course, we will be wondering, right, is approximation, and what's the approximation error? So we can ensure that the approximation error in the case where the k is this number, why we are using this number. So this is really because this is the number really people used in sensors, in decennial sensors. And in this case, the approximation error is smaller than. The approximation error is smaller than 10 to the power minus 100. Okay, so this is essentially zero, right? So there's no need to worry about the approximation error because the approximation error will be even smaller than the machine precision. And we can have this approximate error at different levels. So, most of the time, as long as the noise level is more than one, and the approximation error is small enough. Enough. And so now let's just look at, I don't have time, but now let's just look at the results. The results showing that, so we don't change anything. It's still the original 2020 US decennial sensors. But what we can say is that sensors underestimate their privacy guarantee. So this is the sensors' claim about the privacy level, about their statistic for the national level. Statistic for the national level. So, epsilon here is 2.51. App data is this number. But actually, what we can see is that actually, here epsilon, the real epsilon should be smaller. Epsilon here is only 2.2. Okay, so there's a waste of resource here. Indeed, sensors protect privacy too much. Okay, they underestimate their privacy guarantee. So the implication is that actually we the implication is that actually we can add less noise okay we can add less noise in order to achieve to match the original privacy guarantee okay so here we are adding less noise for example here and less noise and what we get is as follows so we can reduce the mean square error from 91 to 28 so excuse me 82. okay so there's a really stringent there's a very severe trade-off between Very severe trade-off between privacy guarantee and utility. If we add noise, of course, meanwhile, we protect privacy, but we will also sacrifice utility, mean square error. But actually, our study shows that we can add less noise, we can improve privacy and the utility guarantee for free. And the increase is basically 10%. Okay, this is all free. We don't need to do anything. It's only enabled. It's only enabled. It's enabled by analysis alone. Okay, so I'm down here. So everything goes to central limit theorem by using central limit theorem goes to GDP. So this brings back Gaussian to our center of our key. Thank you.