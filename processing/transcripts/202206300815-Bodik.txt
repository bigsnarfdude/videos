First of all, I would like to thank the organizers of this conference or the workshop. I personally find all the talks super relevant and interesting, so I wanted to say thank you. About the work that I'm going to present now, it's a joint work with Linda and Valerie, and this work combines the causal inference and extremal value theory. We are doing causal discovery for the extremal dependent structure. And yeah, a little more. Yeah, a little motivation in multivariate extreme value theory and in general for extreme dependence structure between two variables. In many applications, it can depend on covariates. So in winter, it can behave differently than in the summer. And we want to include that into modeling. So there have already been work how to do this statistically. So which cover it can significantly help? Can significantly help with predictions and which cover rates are irrelevant, which covariates are, in fact, very significant. And in this work, we are trying to move this to another level, and we are trying to say which of the covariates are not only significant, but which of the covariates are actually causal for the extremely dependent structure between two variables. How do we do the causal modeling itself? We are doing it via structural causal modeling. causal modeling. In particular, there are the method called invariance causal prediction that was worked by Jonas Peters and co-authors. And we are trying to implement that and we are trying to use these ideas for the extremal dependence structure itself. So first I will start with classical statistical modeling of extremes. So far no causal inference at all. And I will start with something very simple that I'm sure you already heard many, many times. It's multivariate extreme value distribution. So, for IID random vectors, if we take their component-wise maxima, then after some renormalization, if it converges to something non-degenerative, then it has to converge to multivariate extreme value distribution. Now, we will consider only the we are concerned only with the dependent structure itself, so we standardize our margins to unit fresher distribution. So, you need fresher distribution. And I believe that it's not necessary to talk about it more. It was already mentioned many times this conference. So in this talk, we will consider only the bivariate case. And in classical literature, usually we do not directly work with the distribution of this multivariate value distribution, but we work with so-called exponent functions. We work with so-called exponent functions, so the transformation such as this one, it can be shown that this v, this exponent function, is minus one homogeneous. It means that it can be fully characterized by its behavior on the unit simplex. This behavior of unit simplex is that's what Pican's dependence function represents. That's a function between zero and one. And in bivariate case, it fully characterizes the extremal. Characterize the extremal dependence structure between z1 and z2. Now, what is our quantity of interest? It will be theta, which is the extreme coefficient. This theta is defined as the z-coex function in one particular value in 0.11. And arduability is the most important value. It characterizes the diagonal of the extremal copula. It has a connection with Picant's dependence function. Has a connection with Pican's dependence function two times this Pican's dependence function in 0.1 half, so in the middle, that's exactly this extreme coefficient. And for those of you that I went too fast for the rest of the talk, you can imagine something that is much easier to interpret, and that's extreme correlation. So it's two minus this extremal coefficient. And this extreme correlation is defined as probability that the first margin is larger than z. The first margin is larger than z, given that the second margin is larger than z for this small z going to avail. So, okay, these are definitions, and this is the quantity of interest. This is this theta. So, how can we model this theta? And I believe some of you may heard about max projection or mean projections. So, this is particular adaptation of that in order to fit the causal methodology that I will talk about later. Will talk about later. And okay, so what does it say? For vectors at one, z2, such as we guilt before, so bivariate simple max stable random vector with extremal coefficient theta, then we define the so-called log mach projection, this y, as log max of z1, z2 minus an Euler's constant. Now, if we do the transformation like this, it can be shown that this y is equal in distribution to logarithm of theta. logarithm of theta plus centered gamble noise. Okay, this is a notation for centered gambling noise. So in particular, expectation of this log mach projection is logarithm of theta. And because we can directly observe this y, we can directly model and do statistical inference based on this very nice property here. However, as I mentioned in the motivation, in many applications, the dependence structure depends on the covariates. Depends on the covariates. So let's introduce covariates. Let's say we have x1 up to xd. And because we are not dealing with the entire dependent structure, we will consider only the extreme coefficient here to be a function of x. So what I wanted to say is that from now on, we assume that the theta is a function, theta of x, and we try to model this. So in total, the last two slides can be summarized in a The last two slides can be summarized in a statistic on our regression model where we have this y given x is equal to logarithm of x plus center gamble noise. So this is our model. And I believe that many, many of you, that all of you can imagine of 10 different ways how to estimate this theta of x, how you can actually model it. So what we chose, we chose modeling via generalized additive models, so using GAN. Gam. This was kind of a natural choice with our dimensions of x, which are relatively small. And we know that this gamma works quite well and it's quite general. So, well, this was our choice for the modeling theta of x itself. Okay, so this is the statistical modeling of theta. And we are asking the question, which of the covariates are causal? So in GAN, we can say which of the covariates are significant, which of the covariates has. Has which of the coverts has how large effect, but we want to know from now on which of them are causal. So, as I mentioned before, we are using structural causal model for the mathematical interpretation of causality. And just for notation again, so we have x1 up to xg covariates. We have y some response variable. We denote s star as the indexes of the direct causes of y. What does it mean? Causes of y. What does it mean? So, in the data-generating process, the y is generated as a function of, let's say, g of x star plus some noise. We assume the additivity here. And we don't really impose, we don't really care about the structural dependencies or causal dependencies between axes themselves. We only care about the response variable here. And our goal is to estimate this S star. is to estimate this star. This is our final goal that we want to achieve. Oh yeah, just a simple observation. If you take y minus g of x star, the distribution of x is invariant with respect to the distribution of x. What do I mean by this? It doesn't matter what distribution x has, it can be discrete, it can be continuous. This quantity here is the same. Is the same regardless of the distribution of x. So, this is quite a simple observation. However, it's kind of the core of the invariant causal prediction method, again developed by Jonas Peters and co-authors in 2016. And just to put it a little bit more rigorously, invariance property represents that the conditional distribution y given x star remains invariant if we intervene. invariant if we intervene on other covariates other than y. So by intervention we mean changing the structural causal structural equation for some of the covariate. For example, do intervention means that we set some value of x to one particle value. And again, for this star for the true direct cause, this conditional distribution remains invariant. Remains invariant, this is still the same. If we intervene on one, two, three, five, all the covariates, this remains the same. However, of course, we do not know this star, we want to find it. So it's quite natural to ask which subsets of our covariates do satisfy this invariant property. So in other words, what we are asking when the conditional distribution y given xs remains invariant under a change of environment. Under a change of environment. Now, what do I mean by environment here? Environment can consist of several interventions. For example, we can observe a random sample from one environment from observational environment, and then we can intervene on some of the variables, some of the covariates. And after this intervention, we observe another random sample, and we say this is a different environment. So environment is a change. So environment is a change doing something, but in general it can be even more general than just interventions, but we are changing in general a structure of our coverage itself. This is what I mean by change of environment. I hope it was clear. In a little more detail here, so let's assume that we have k environments. In each environment, we observe a random sample. So we observe Observe a random sample. So we observe covariates every time possibly with different interventions and our response variables. Now, the goal is to test whether a subset of our covariates, S, fulfills the invariance property. We know that the true S star do fulfill the invariance property. So it's quite natural to ask which of the subset do satisfies this invariant property itself. And how can we do this? And how can we do this? So, it's not super important that you understand each of every step of this, just to give you some idea of the main steps here. So, first, we can pull all the data together from all environments. We so far do not look which interventions we made, and we get one statistical estimate, g hat of this link function, g hat of then next. Then, next, we compute the residual. So, we compute y minus this estimate g hat of xs. And now we finally look which environments these residuals are. So, we split this residential with respect to their corresponding environments. And then, finally, we are looking, we are using a case sample test to test whether these residuals are equally distributed. Residuals are equally distributed through the environment. As an idea for a case sample test that we are using, it can be even Kolmer-Gos-Schmir test. What we are using is Anderson case sample test of Anderson-Darling, but it's not really that important. So, this is the main steps how to test whether S fulfills the invariance property. And we repeat this for each and every subset of S. So, that's two to the power of the. Subset of S. So that's two to the power of d times. In practice, we don't really need to do them all, but again, it's just for the computational purposes, it's not super important. And finally, our estimate S hat of this S star, we define it as an intersection of those subsets S for which the K sample test was not rejected at a given level alpha. So just to make sure that we do not. So, just to make sure that we do not make a mistake, we take the smallest, the intersection of all of those subsets for which this case sample test was not rejected. We know that for the true S star, the invariance property should hold, and it can be shown that this case sample test should not reject these hypotheses for the true S star. So, just to make sure we are taking an intersection of all sets that satisfy. All sets that satisfy this property. Okay, so that was invariance causal principle prediction method. And now we are moving to context of extremes. So for the y itself, for the response variable, for the response variable, we are using our load max projection itself. Okay, so y is So y is a log market projection. And if you remember the lemma at the beginning, it's therefore very reasonable to assume a structural equation that y is equal to logarithm of theta of eta star plus a center Gabon noise. So this is a structural equation that we have for our logmax projection. And for this, it actually holds that if you look at y minus this log of theta of x star, this x star, this remains invariant if you intervene on the covariates x. So we can directly use the previous methodology, we can directly work with log nach projection as a response variable and our covariate, we can use the previous methodology, the causal methodology. Just to give you maybe very, very concrete and very simple example how it works in practice. It works in practice. So let's take x1 to be, it can represent the temperature, let's say. It can be normal, it is normally distributed. X2 will be another covariate, which is equal to x1 plus some other noise, let's say vibral noise. And Z1, Z2 are in the context that we had at the beginning. So it's a bivariate equality distribution. And we are using a parameterization with logistic copula. So for those of you that Logistic copula. So, for those of you that don't remember exactly what logistic copula is, it's a parameterization of this extreme value dependent structure. It's kind of the most simplistic one. It has one parameter, parameter alpha. If this alpha is close to one, it means if alpha goes to one, it represents independence in extremes. If alpha goes to zero, it represents full dependence. It represents full dependence indexes. So, okay, for this alpha, we choose this alpha to be a function of x1. So, x1 is the true direct cause, not x2, only x1. And for the link function, we choose this inverse logic function with beta times x1. So, what is this beta? This is some hyperparameter that represents the strength of this line right here. If beta is 0, it represents... beta is zero it represents it means that x1 is independent of the z1 z2 if beta is larger then it means that x1 and y okay what is this y here uh this y is log max projection of z1 z2 as before and if beta is large it means that x1 has larger and larger uh strength that the dependence between x1 and y is larger and larger now we take the second one variable Now, we take the second environment. We make an intervention on X1. We take X1 to be P plus the previous noise, epsilon 1. So you can imagine we put a heater in the room that adds exactly P Celsius into the room. And with such a setup, we again observe the entire structure again, and we have random sample from this second environment again. This second environment again. And finally, we observe also third environment when we make a do intervention on x2, we set it to be minus 10 always, no matter what. And again, we observe a random sample from this third environment. Now, we are using this previous Causal methodology, and by this Causal methodology, we obtain this estimate S hat star. estimate S hat star. We know that the true direct cause is one. So we are asking question: so how many times is this as head star actually one? So how many times the methodology works? So we are changing p between minus two and two, we are changing beta between minus two and two, and we are taking n either 500 or 2000, 1500, and for each combination we make 100. Combination, we make 100 repetitions, and these are the results. So, if this P is zero, it means that we make no intervention at all. And the larger this P is, the larger heterogeneity there is between the environments, the larger intervention we make. And you can see right here that the larger intervention we make, the easier it is to find correct results. Also, for beta, that's not surprising at all. The larger strength X1 has on Y, easier it is to find the correct result. So the beta is larger, the easier it is to find the more often S hat star is actually equal to 1. I want to point out this other axis right here. It just represents kind of where we are with alpha. If P is 0. If P is zero, it means that we made no intervention at all. And expectation of alpha, if I go back here, if you compute expectation of alpha, it's exactly 0.5. And if we intervene by adding the x1, adding the temperature, we have p positive, the alpha increases. So we are in more independent setup. And if p decreases, we are in. And if p decreases, we are in more dependency. So the change of alpha is kind of crucial here. And in fact, we are not, okay, this may sound quite hard to understand maybe, but we are not directly looking at heterogeneity of P or beta here. We just want this alpha to change between environments as much as possible. And that's the change that we are looking at. That's how. That's how we make the inference and the CASO methodology itself. Okay, in the last five minutes, I will move to application. And the application corresponds to NO2 data set. We have 21 stations in England, which have at least 15 years of measurements, of hourly measurements. Years of measurements of hourly measurements. This is one particular station. So, in Cambridge, it started measuring NO2 in the air around 2002. We can see that there are some missing values and there are some quite extreme values that, yeah, NO2, extreme values of NO2 can be quite dangerous for humans. So, it's very relevant to detect some causes of this. But more importantly, you can also see. But more importantly, you can also see some seasonality here. In winter, this NO2 seems to be larger than in summer. And also about these stations, we have some other information. We know their location, when they are located, we know the time of the measurements, of course, and we know the type of, if there is a traffic, if it's in the city, for example, or if it's a background station. So we know. So, we know this type of the station for our stations. And what is our variable of interest? So, that's extreme dependence of NO2 measurements between stations. So, when it's very localized or when it's spread out very, very far. Our goal is not again only to say which covariates are significant, but which covariates are causal for external dependence. So, first of all, So first of all, let's define what are our covariates. Even before, let's define what are actually environments in our case. So one environment, we define one environment at one pair of station. What does it imply? What are our assumptions if we do this? So we are assuming that there is an underlying physical mechanism that is the same regardless of the location of our station. So in principle, if we want to do So, in principle, if we want to do an intervention on, for example, the traffic there, one possibility would be to say, oh, the people, let's go there and let's increase the traffic there so we can measure it. Of course, that's impossible to do in practice. So we assume that we have one pair of station where we measure it and when there is small traffic. And we assume that at the same time we measure a second pair of stations when there is everything the same. When there is everything the same, but the traffic. So, what does it mean everything is the same? Only the covariates that we have, they are the same. However, we do not really imply in our model, and that's maybe one error that we can make, one assumption that it's very hard to say this is definitely true, because it probably isn't really. Different things can. Different things can change between two stations. For example, if we take one pair of stations, it can be the altitude of the station is very, very low. The other pair of the station, the altitude is very, very large. And we did not include the altitude in our modeling. Our coverage that we take are only these three. So it's the distance between two stations. It's a year of measurement that we want to put there a time. And again, the type of the station. So we are taking. Station. So we are taking pair of the station. So what the type means, we are always taking a pair of stations with the same type. So both have either a lot of traffic or both are background stations. So these are our assumptions. And if something else really changes between the environments, our assumptions are not really fulfilled. However, we hope that these covariates are kind of enough and describe a lot of the And describe a lot of the structure itself. The algorithm itself, I just kind of repeat all the steps that we have to do in order to get an estimation. So first, in each station separately, we feed the GEV to the weekly maxima in order to transfer the margins into unit fresh air. Then, between in environment, so between every pair of stations, we compute the log mark projections. compute the log mark projections. The third step is the causal, the causal part, where for each of the subsets we compute the four, we follow the four steps. We put the data together and we calculate this theta hat of Xs itself. In the next step we compute the residuals, so y minus logarithm of theta hat of Xs. Then finally we look at the corresponding environments between these corresponding environments between this residual. So where, in which environment this residual corresponds to. Why is this five? I will get to that later. And finally, we use K sample test with significance level 0.5. And we are using underson darling test for this kSemp test. And finally, we have a p-value which represents to the test if the all if To the test, if all the residuals between the environments are the same or not. So finally, this S star is an intersection of all the subsets where this case sample test was not rejected. And to give a result, even before really giving you the results, I want to say how we picked the station. So all the time we take five different environments. So in total, 10 stations. We make sure that no stations. Stations, we make sure that no station repeats itself. And we randomly pick those 10 stations. It's not exactly random. We had to make sure that there is some heterogeneity between the environments. So we take one environment where there was a traffic type and another environment where it was a background station. So we made sure that there are different types. However, we However, all the other things we pick randomly. We repeat 50 times with 50 different fifths of environments. And finally, we get this plot here. You can see here that if x1, the covariate x1, was in our model, we typically did not reject the hypothesis of invariance. However, if one was not present in the model, we did rejected the invariance assumption. So what does it mean? So what does it mean? X1 corresponds to distance between two stations. So this means that the estimation at star would be X1. And so only X1 gives us really strong evidence of having causal effect. So if you think about it, this is quite a trivial result. If you have two stations and you pull them away from each other, the extremal dependence decreases. So yeah, that makes sense. So yeah, that makes sense, right? So yeah, this is true. However, I still think it's very nice to show that even on such a complex setup as an environmental data, we can see that this methodology can work and that it can work also on such a, I would say, complicated quantity such as extremal dependence. Yeah, so thank you. Thank you for your attention and happy to have a discussion or answer questions. Thank you. Thanks, Yuru, for your talk. Are there any questions for people outside or for people online? Yes, there is one here and I have another one. So while I walk to Mila, I will ask you in your output. In your algorithm, I think it was slide 23 or 24. Yes, so in item four, I was wondering whether you really have to go through all the subsets because you're looking for an intersection. So isn't that possible to have sort of a path through certain subsets? Because if you know that the subset that these tests were not rejected for some subset, then the S star would be a subset of that subset. Star would be a subset of that subset. So you can basically maybe search only within certain subsets. Yeah, you are completely right. That is exactly what we are doing. If we have just three coverheads, that's not so complex. So we actually can do it all. But yeah, you are right. We can impose this kind of stopping rule and we don't have to really go through it all. And typically, we just need a first few that we think. We think they are relevant, and when we find out that the intersection cannot be larger, then we can stop. So, yeah, you are completely right. Hi, this is Mila. Thank you for your great talk. I have two questions. The first one is, is your method be able to quantify the causal effects? And my second question is: you possibly said this, but I. You possibly said this, but I missed it. You didn't assume any like the all these covariance should be independent or dependent. So would it be true that even you detect some covariance that has some causal effects, but this causality might because of the other variables. Of the other variables, that means it's not a direct effect, it might be because of the other variable cause has a causal effect, and then it goes through this variable. And then, therefore, this one also has some causality on this outcome. Thank you for the question. Both are very good. So, for the first question, in order to somehow quantify the effect, so now this methodology. So no, this methodology, I mean, the basic goal was to estimate this star. So yeah, it can be very interesting to do that in future. However, I must say that we have not looked at that yet. For the second question, so if let's say X1, if I understood your question correctly, so X1 is a cause of X1. So x1 is a cause of x2, and x2 is a cause of y itself. And we will detect that x1 is a cause or not. So in limit, if we have a lot of data, then if x2, if we observe x2, then we would say x2 is the direct cause and x1 is not. However, if we do not observe x2, then yeah, we would directly say x1 is a direct cause. Say x1 is a direct cause, although there is something in between. However, this between we don't know what it is because we do not observe it, and we would, because of this, we would say that yes, x1 is direct cause of this y. Is this an answer to a question? Just a second. Yeah, just uh follow your uh The following the X1, X2 things. So your method could, if we both observe X1 and X2, so your model could also detect both X1 and X2, right? Yes, yes, that's true. Okay, so that means, but it won't give you any information like who, like the um like the the the the who who caused who right about the covariates themselves between them each other no yeah that's true we are basically looking only at the at y itself and we don't kind of really care about the covariates themselves that they can have very complicated causal relationship between each other and we do not model them we don't even impose any really restriction that for that we only care about We only care about their effect on Y itself. So, yeah, you are right. We cannot detect that. Thank you.