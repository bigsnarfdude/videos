You were paying attention to Rick's talk just before was the part that he mentioned at the very end. So are people's videos sort of blocking the slides? I can't tell. No, we can see them. Okay. So to me, the world of differential categories is sort of split up into four parts. So and Rick mentioned pretty much all of them, except maybe this third part down below. All of them, except maybe this third part down in the corner. So you've got differential categories, which is sort of you've got differential categories introduced by Bluke Cocketseali, Cartesian differential categories, also by Blu-Cocket Sealy, differential restriction categories, this time by Cockett, Crotwell, and Gallagher, and tangent categories, which first by Rosiski and then popped up again by Cockett and Crotwell. And so each of these four parts sort. Each of these four parts sort of takes a different aspect of differentiation, and you can do different things with them. So, for example, differential categories are more the algebraic foundations of differentiation. And as Rick talked about, they go hand in hand with differential linear logic. In fact, differential linear logic is sort of the prologue to this whole story of differential categories. What I'm going to focus on is Cartesian differential categories, where you sort of get more on the analysis side. Where you sort of get more on the analysis side of things and talk about multivariable differential calculus over Euclidean spaces. You can do differential restriction categories where instead of working with total functions on Euclidean spaces, you work with smooth functions on open subsets. And sort of, and this is the parts, and these two parts have been picked up by computer scientists, so in particular, are being used in differentiable programming and machine learning. Programming and machine learning, which I'll mention very briefly at some point. And lastly, which is probably the biggest component of these workshops, is tangent categories, which deal with smooth manifolds and the tangent bundle. So the other cool part about the world of differential categories is that all four parts are connected to one another via various constructions. So if there's one thing you're going to take away from my talk, So, if there's one thing you're going to take away from my talk today, it is this slide right here and all the connecting factors. So, what this means, for example, is that if you start with sort of an algebraic model of differentiation, say somewhere in a different starting with differential categories, then you can build your way up to a more complex model of smooth of sort of abstract smooth manifolds via tangent category model. And Rick sort of in his talk already mentioned one of these constructions, specifically. Already mentioned one of these constructions, specifically the one that goes from a differential category, taking the Cochleisley, to get you to a Cartesian differential category. And then Robin, in the talk after me, is going to talk about how to go backwards using the embedding theorem. And then I'm certain that Jeff and Ben and Richard and whoever else is giving a tutorial on tangent categories will probably deal with these constructions linking up tangent categories with differential restriction categories and Cartesian differential. Restriction categories and Cartesian differential categories. Probably the only construction you won't see in these tutorials is this Kohalenbird-Moore arrow, which is the latest one to be added to the story. So before I move on, are there any questions about the map or the world of differential categories? Does this diagram commute? So, could I understand Ko-Eilenberg more by Co-Kliesley? Eilenberg Moore by Koch Livesley composed with the subset arrow? Yes. So most of these arrows do commute. Some of them don't exactly commute on the nose, but in general, you can assume that they do commute. Or there's an adjunction. So for example, Cartesian differential categories and the tangent category, that's an adjunction between the two. All right, so as I mentioned, this talk will focus on a very specific part, chapter, the one on Cartesian differential categories. So Cartesian differential categories formalize differentiation in multivariable calculus over Euclidean spaces and also provide the categorical semantics of Erhard and Ragnier's differential lambda calculus. So the main reference to Cartesian differential categories. To Cartesian differential categories is the original paper by Bluke, Cocken, and Seeley. So now let's go over the definition. So a Cartesian differential category can sort of be split up into two parts. The first off, it's a Cartesian left additive category with a differential combinator. So let's start with the first part. So a left additive category is a category which is skew enriched over Q. Is a category which is skew-enriched over cumulative monoids. So, this notion of being skew-enriched or skew monoidal is an up-and-coming notion in category theory, and it's getting lots of steam and traction. But specifically, skew enrichment, there's a paper by Campbell on it. But you don't need to understand necessarily skew enrichment to understand left additive categories, because all you need to know is that a left-additive category is just a category whose hom sets are Category whose homsets are commutative monoids. So that means that I can add two parallel maps together and I also have zero maps. Such that composition preserves addition in the following sense. So if I post-compose by a sum of maps, then I can just split it up. So if I do f plus g compose with x, then I just got f compose x plus g compose x. And if I post-compose by zero, I just get zero. So this allows me to have. So, this allows me to have maps that. So, if you're familiar with enriched category theory, you might be expecting the other version as well, where pre-composing by some is also preserves addition. But that's not necessarily true in this case, because we're going to want the ability of adding maps together, but having maps that don't necessarily preserve addition. Maps that do are called additive maps. So, a map is additive if it So, a map is additive if it preserves the sum and zeros. Then, a Cartesian left additive category is a left additive category with finite products such that the two projection, that the projection maps are additive. And I'm going to use the, I guess, the computer science convention. So, I'm starting my indexing at zero. So, I've got pi zero as my first projection and pi one is my second projection. So, maybe the better way. So, maybe the better way to understand Cartesian left additive categories is via examples. So, any category with finite byproducts is a Cartesian left additive category, which if I haven't mentioned, I might abbreviate or just call a clack because that's a mouthful. So this is an example where every map is additive. So, for example, the category of vector spaces and linear maps between them is a Cartesian left additive category. Category. You can also take the category whose objects are again vector spaces, but this time whose maps are just arbitrary set functions between them. And this is a, and in this case, you have lots of, you can add two maps because you can just take the addition inside of the vector space. But in this case, you have lots of maps that don't preserve addition. You also, another neat. You also, another important example for this talk is the Le Viere theory of polynomials. So, this is the category whose objects are the natural numbers and where a map from n to m is going to be a tuple of polynomials. And finally, probably the most important example for this talk is the category smooth, whose objects are the Euclidean vector spaces Rn and whose maps are just smooth functions between them. Smooth functions between them. And you'll notice that when that for the specific example of polynomials over the reals, then smooth is a sub-Cartesian left additive, sorry, poly over the reals is a sub-Cartesian left additive category of smooth. Are there any questions on Cartesian left additive categories? Awesome. So now let's go back to our definition. So a Cartesian differential category is a Cartesian left out of category with a differential combinator. So what is the differential combinator? So a differential combinator is a family of functions which I'll write out as this. So it picks up a map from A to B and it spits out a map. b and it spits out a map d of f called the derivative of f of type a times a to b so this is the thing so this is the part that rick was talking about that it's a different form than in differential categories so before giving the axioms let's go over some examples so again the main example of a cartesian differential category is the category smooth so remember that a smooth function So remember that a smooth function from Rn to Rm is in fact a tuple. And then you define the Jacobian of f as just the matrix whose coordinates are the partial derivatives of the small fi's. Then for a smooth function capital F from Rn to Rm, its derivative d of f is just the one from, is just the directional derivative. But if you don't know what that is, essentially what you do is you Essentially, what you do is you take the Jacobian of f evaluated at the first argument x, multiplied by the second coordinate y, and what you end up is a tuple where at each component you get the sum of the partial derivatives of, say, f1, and then in the first coordinate, and then the last, and then at the js coordinate, you get the sum of the partial derivatives of fj. So, for example, if I have the function If I have the function in two variables, x1, x2, x1 cubed times x2, then d of f takes in x1, x2, y1, y2. And so the first part you derive with respect to y1. And then the second part, then you add the derivative with respect to y2. So hopefully this is an example that most of you are familiar with, or at least this is an example of differentiation that most of you are familiar with. Of differentiation that most of you are familiar with. The other two sort of classic standard examples are the category of finite, any category of finite byproducts, where the derivative is super simple. You just pre-compose by the second projection. So for example, for the category of vector spaces, for a Cartesian is a C D C, a Cartesian differential category, where D of F evaluated at X Y is just F evaluated at that. Is just f evaluated at that second argument y. You can also do this for the category, the Le Vier theory of polynomials. That's going to be a Cartesian differential category where the differential is again just taken to take the sum of the partial derivatives of each polynomial component. And you know in particular, once again, that for the reals, poly r is a sub-Cartesian differential. Is a sub-Cartesian differential category of smooth. So before I go over the axioms of a differential combinator, does anyone have any questions about these examples? Cool. So now let's go over the axioms. So to help us with the axioms, I'm going to use the following notation, which is sort of use the following notation which is sort of i'm going to use element notate sort of element notation or sort of a proto term logic so d of f at a b I'm going to write it out as such now now just so I'm clear this dot here is not shouldn't be is not does not mean that I have a multiplication inside the category but it is definitely inspired by matrix multiplication from our from our smooth example in the previous slide now those of you familiar with computer Now, those of you familiar with computer science may think about this and say, ah, well, this sounds like a term logic for my category. And you're completely right. There is a sound and complete term logic for Cartesian differential categories. And in short, anything we can prove using the term logic holds in a Cartesian differential category. And doing proofs in the term logic is super useful. And take it from me, it helps by proving very difficult things. So if you're interested, go check out the original paper. Interested, go check out the original paper by Rick, Robin, and Robert on the term logic. All right, so Cv1 is called additivity of the combinator, which essentially says that the derivative of a sum of function is the sum of its derivative, and that the derivative of zero is just zero. Easy enough. C D2 is called additivity in the second argument, which essentially says that in the essentially says that in the second argument the derivative is preserves addition preserves addition cd3 tells you how what the derivative of identity maps and projection maps are so the ident the derivative of the identity is going to be the second projection while the derivative of the projection of the projections will just be that projection precomposed That projection precomposed by the first projection. CD4 tells you how to derive, tells you that the derivative of a tuple is the tuple of the derivatives. Now, this is often an overlooked axiom when it comes to Cartesian differential categories, but it is super useful in examples. So, for example, back in our smooth example, if you take capital F, then D of capital of F is just the tuple of the D of the little F's. CD5 is the chain rule, which tells, what is the famous chain rule, which tells you what the derivative of a composition of functions is. So if you go through this, we have that the derivative of the composite g in f is equal to the derivative of g evaluated in its first argument at f, and then evaluated at the derivative of f in its second argument. So this should be very familiar. This would be very familiar. And then the last two axioms, CD6 and CD7, are a bit mysterious when you first encounter them, but stay with me. They'll become clear in a few slides. So CD6 and CD7 talk about the second derivative. So if I have F from A to B, I take its derivative, which is A cross A to B, then I can take the derivative of the derivative, which gets me to A cross A cross A. A cross A cross A cross A to B. So four copies of A in the domain. So C D6 is called linearity in the second argument, which says that if I put in zeros in the middle two arguments of D2, I just get back the first derivative. And then symmetry says that I can swap the middle two arguments. And again, if these look sort of weird to you, don't worry. In a few slides, we will justify them and explain what they are. Explain what they are. And that's it. A Cartesian differential category is a Cartesian left additive category with a differential combinator. So with that, before moving on, are there any questions on the CDC axioms? Awesome. If not, let's move on to see what we can do with the Cartesian differential category. So, the first thing that we can do is talk about partial derivatives. So, say that I have a map of type A cross B to C, and I want to differentiate with respect to A while keeping B constant. Well, normally what you do in classical calculus is that you take the total derivative and you input zeros to obtain the partial derivative. And luckily. Partial derivative. And luckily for us, because we're working in a Cartesian left additive category, we have access to zero maps. So that's so we can just replicate that idea in a Cartesian differential category. So what do you do? So you take the derivative of f, which is of type a cross b cross a cross b, and to obtain its partial derivative, which is just a cross b with an extra copy of a, what you do is you're going to introduce a zero. is you're going to introduce a zero in that missing B component. So in the term logic, it looks like this. So here on this side, this is the partial derivative. So I'm only bounding my variable x. And this is equal to the total derivative where I've introduced a zero in that missing b component. I can also do this, of course, for I can also differentiate with respect to b while keeping a constant, where this time instead of putting zero. Where this time, instead of putting zero in the B part, I put a zero in the A part. So the great thing is that from the total derivative, we can easily obtain partial derivatives. And not only that, we can do this for maps whose domains are just A cross, A0 cross, A1 cross A2, da da da, A N. So a consequence of C D 7, the symmetry rule. Of CB7, the symmetry rule, is the famous symmetry of the partial derivatives, which says that if you differentiate with respect to A, then B, it's the same thing as doing differentiating with respect to B, then A. So in the term logic, it's written out as so, as such. So see here, I first did derived y, then derived x. And but in this side, I derived x first, then I derived y. Another important part is that Cd2, which remember was additivity in the second argument, tells us that actually d of f is the sum of its partial derivatives. So I put the calculations here to show you, to give you an example of how to use the term logic. But if you remember, in our example of smooth functions, well, d of f was precisely the sum of the partial derivatives. So we recapture that. So, we recapture that idea in the categorical sense as well. Any questions on partial derivatives? Cool. Another important concept that we can do in a Cartesian differential category is talk about linear maps. So, in a CDC, a map is linear if essentially its derivative. Linear if essentially its derivative is just the map you started with. So, what precisely I mean that d of f is just f post-composed by its second derivative. Sorry, the second projection. So examples. In a category with finite byproducts, every map is linear by definition. Easy enough. In my Le Vier theory of polynomials, a polynomial, capital P, is linear if and only if Is linear if and only if each of the small p's are polynomials of degree one. So if they're of the form some sum constant multiplied by the variable x. In the category of smooth functions, a map is linear in the Cartesian differential sense if and only if it is linear in the classical sense. So you might look at this and go, now wait a second, these are all additive maps. And you're right. In any Cartesian differential category, In any Cartesian differential category, every linear map is always going to be additive. But the converse is not necessarily true. There are examples of Cartesian differential categories whose additive maps are not necessarily linear. But in the above examples, all the additive maps are linear. CD3 tells me, tells us that the identity maps and the projection maps. That the identity maps and the projection maps are linear. We can also talk about what it means to be linear in its second argument. So, which is, so essentially what it means is that it's just going to be, when I take the partial derivative, it's just going to be the map F. So, CD6 tells me that the derivative that for any map F, its derivative is going to be linear in its Derivative is going to be linear in its second argument, where now we have a concept of linearity in a Cartesian differential category. Are there any questions about linearity? There's a question in the chat, JS, from Alexander. What are examples of non-linear additive maps? Well, Robin is going. Well, Robin is going, in the next talk, Robin is going to give an example of a co-universal Cartesian differential category. And in that category, there are examples of additive maps, which are not linear. So, I'll answer that question later on, but I guess there are probably no natural examples that you come across where the Across where the out of the maps and linear maps don't coincide, you actually have to go digging for it. Hopefully, that's good enough. Yeah, we got an okay, thank you. Cool. May I ask a question, sorry? Yeah. Okay, so my question is, is there something it sounds like the dot annotation looks like a metric? metric sorry like a scalar product between between two vectors so my question is if is there is there a relation with a kind of dual operation of vector spaces and we are just forgetting about the duals because we are looking for finite vector spaces something like that The answer is yes. The answer is yes. Not every Cartesian differential category has this notion of duals and finiteness. So, this is why when I mention that you've got to be careful when dealing with the term logic. That it really is there just for intuition and is not necessarily true in your category. Okay, so there is no relation with scholar products. There can be. So, there is a There can be. So, there is a notion. This was studied by Ben McAdam and Jonathan Gallagher of this, of Cartesian differential categories where the linear maps have a tensor product and where you can talk about that category of linear maps being closed and talking about duals in that case. And there is a notion, and then you can recover the differential from scalar multiplication. Okay. So there is a notion, but for this context, for an arbitrary Cartesian differential category, there may not be any notion. Differential category, there may not be any notion of scalar multiplication. Okay, okay, thank you. Thank you very much. You've got one more from the chat, JS, from Mark Hamilton on the previous slide where you talked about differentiating with respect to A and then B, and then B being equal to A. Sorry, B. I can't even read it. Is that just a formulation of equality of mixed partials? Yes. So we're talking about C D, whichever one it is, seven. That is right. That is correct. Correct. Cool. Okay, so other examples. So every model of the differential lambda calculus induces a Cartesian differential category. And conversely, a Cartesian differential category, which is Cartesian closed, such that the evaluation maps are linear in their second argument, gives rise to a model of the differential lambda capitalist. Lambda calculus. Bauer, Johnson, Osborne, Real, and Tebb, also collectively known as Bjort, construct an abelian functor calculus model of a Cartesian differential category. Correct me if I'm wrong, but I think that Brenda is going to talk about this on Wednesday. And then for any Cartesian left additive category, there exists a co-free Cartesian differential category over X. And this is something that Robin. And this is something that Robin is going to talk to you about in the next talk. And I think it was Alexander who asked the question: It is in this category that you have examples of maps that are additive but not linear. And if Robin doesn't answer that question, I'll happily give more details about that in Gather Town or something. What are some applications? So here are just some applications that I thought about very quickly. About very quickly. So, the first one of the things that you can do: Cockett and Crotwell have started to study differential equations in a tangent category. And of course, since every Cartesian differential category is a tangent category, then you can also study differential equations in a C V C. And the cool thing is that you can also sort of use those differential equations to define sort of exponential functions, trigonometric functions, hyperbolics functions, et cetera. There is a notion of integration for Cartesian differential categories. And as promised, in my first slide, I sort of mentioned that people from computer science have picked up Cartesian differential categories. This is because there is a notion called Cartesian reverse differential categories. And this is what people have been using in machine learning algorithms and differentiable programming. And I some this might be a very important thing. I some this might be talked about on Thursday. Don't quote me on that. All right. So going back to my map of differential categories, I now want to talk about the construction that goes from differential categories to Cartesian differential categories. And Rick sort of hinted at this, and this is via the Co-Kleisley construction. Via the Ko-Kleisley construction. So every differential category has a notion of smooth map, which remember, so a smooth map from A to B is going to be interpreted as a co-kleisling map. So this is the, which is, of course, going to be a map bang A to B. We remember that bang is a co-monad. So how do we wrap our head around this intuition? Well, let's. Well, let's consider Rick's example where bang of Rn is the free symmetric algebra over Rn, which, if you want, is just the polynomial, the ring of polynomials over n variables. So pick a polynomial function from Rn to R. Well, then that polynomial can be seen as an element of symmetric algebra over Rn. But of course, in category theory, we don't like elements, we like maps. Well, there's an easy way around that. Every element of a vector space gives you a linear map, the base field to that vector space. In my case, I obtain a linear map from R to the symmetric algebra over Rn. But this is in Vec, which means that in VecOp, I obtain a map from sim of Rn to R. sim of Rn to R, but sim of Rn is just bang of Rn, so I do obtain a map bang of Rn to R. And so this is the idea that you should keep in mind while I'm doing this Coclaisa construction, and hopefully that also answers your question about why Rick's example was in VecOp, while the differential categories deal with comonads and co-cleisles. So consider a different So, consider a differential category X with a co-algemodality bang. So, I have my comonad co-multiplication and co-unit, and I also have my comonoid comultiplication and co-unit. And for this construction, we also have to assume that we have finite products, which actually end up being byproducts, but we don't need to worry about that. So, So let X subscript bang be the Kokleisley category. And to help, I'm going to use a trick by Bluetooth, Cocken and Sealy in one of their papers on Cartesian differential categories and use interpretation brackets. So by that, I mean that if I have a map F from A to B in the Koklisni category, I'm going to wrap it around with these interpretation brackets to get me a map from bang A to B in the base category. So for example, here So for example, here, for example, if the interpretation brackets around the identity gets me the comonad co-unit, so bang of A to A, and then the interpretation brackets around the composition is defined as follows. So if you've never seen Cochleis decomposition, here it is. You first do the comonad comultiplication, then you do bang of F, and then you follow that by G. So how do we make the Cochleisley? So, how do we make the Cochleisley category into a Cartesian differential category? Okay, so the first thing we need to talk about is Cartesian left, the Cartesian left additive structure. So for products, so on objects, it's just going to be the product of the base category. Easy enough. For projections, what you do is you precompose by the comonad co-unit. So you do epsilon, you do bang of A, then pi. Of a and pi followed by the projection of the base. And it's a well-known result that for a co-monad in a category with finite products, the co-klisli has finite products. If you've never done that, it's a fun little exercise to work out. For the added structure, what you do, so this is essentially the same as the base. The sum of co-kleisli maps is just the sum and the base and the zero maps, and the co-kleisli is just the zero maps. And the Gokleisley is just the zero maps in the base. However, and this is super important, even if your category is enriched, is additively enriched down below, so for example, if every map in your base category is additive, that does not mean that up above every map is going to be, in the Koklisde category, every map is going to be additive. That's because co-klisdecomposition doesn't necessarily preserve additive structure. Additive structure. So, in particular, for a comonad on a left additive category, for a comonad on a Cartesian left additive category, the Cochleisley category is going to be a Cartesian left additive category. So, that's the Cartesian left additive structure. And so, what's missing is the differential combinator. So, how do we derive a Coclaisley map? Derive a Coclaisli map. Okay, so at this point, I should mention I am not using the same notation as Rick, and there's a reason for that. In the original paper on differential categories to Rick's point, the deriving transformation was a type A tensor bang of A. And at some point, the convention got swapped. I'm not certain when, I don't know where. Where, since I've started working on it, this has been the convention. So I'm going to stick with that because most recent papers on differential categories use this note, this convention. Okay, so for a co-classly map, bang of A to B, how do I define its derivative? Well, what you do normally is you just precompose by the deriving transformation. The problem is this is not a co-Kleisley map, right? Because a co-kleisley map is of the form bang of. map is of the form bang of something. Bang A tensor A is not of that form. And this is where the and this is where you use the fact that bang of A is a comonoid. So to define the deriving the different to define the derivative of a cochleisley map, what you do is you're going to start at bang A cross A. You're going to use the co-multiplication to copy it. You're going to do the first projection on one side and then the second one. Projection on one side and the second projection on the other side, you're going to bump down the left component bang of A to get you to the bang A tensor A, and then you're going to do the deriving transformation followed by F. And this map is indeed a Kokleisny map, and it's super important to understand that the composition here is in the base category and not in the Kokleisny category. So, the theorem for a differential category with finite products. A differential category with finite products, its co-kleisley category is a Cartesian differential category. The cool thing is that every map, every Co-Kleisley map, which is of the form the Comonac co-unit followed by a map in the base category, ends up being linear in the Cartesian differential sense. So, as an example, if you take the example of VecOP, where bang of V is Of Vec op, where bang of v is the free symmetric algebra over v, then that category is going to be, then the co-klisle of that is going to be a Cartesian differential category. The Levier theory of polynomials is a subcategory of this co-klisli category. There are other examples. So for example, you can check out this paper, Categorical Models for the Simply Typed Resource Calculi. And there are two important models, which are based on the relational model and the finiteness space model. And the finiteness space model. So, before I move on, does anyone have any questions on the Koklisley construction? All right. What if I want to go the other direction? So, there's this paper called Cartesian Differential Storage Categories by Blue Cocken and Seeley. And in there, they say, And there they say it was not obvious how to pass from a Cartesian differential category back to a monodal differential category. So, in that paper, they essentially what they do is that they characterize precisely which Cartesian differential categories are co-kleisling categories of differential categories. It's important to note, of course, that not every Cartesian differential category arises as a co-kleisley category. Those that do are called Cartesian differentials. Called Cartesian differential storage categories. So we made a differential category with finite products. Okay, there's a little caveat, and there's this extra assumption called the ceilie isomorphisms. I put them here, but for now you can sort of sweep that under the rug. So a differential category with finite products and the celebrity isomorphisms, its co-klisla category is a Cartesian differential storage category. Conversely, for a Cartesian differential storage, For a Cartesian differential storage category, its category of linear maps form a differential category with finite products and the CLI isomorphism. Okay, that's all fine and dandy, but again, not every Cartesian differential category is a Koklisley category. Luckily, or as it turns out, so a result, a very recent result by Richard Gardner and myself says it gives the embedding theorem. It gives the embedding theorem, which says that every Cartesian differential category embeds into the Cochlisley of a differential category. And this is Robin, what Robin's going to talk about in the next talk. All right. So once again, if there's one slide to take away from my talk today, it is this map here. Actually, if there's one thing to take away from this entire conference, it's this one slide here about the map of differential categories. About the map of differential categories. Just to wrap up, I sort of looked at the conference schedule and realized that no one was probably going to mention differential restriction categories. So here's a very, very brief word on them. A restriction category was introduced by Lack and Cockett. And essentially, what it is, is a category that comes equipped with a restriction operator. So, which essentially captures the notion of the domain of definition of maps. So, essentially, restriction categories allow you to work with partial functions. Then a differential restriction category is very, very, very naively a Cartesian differential category with a restriction operator such that the differential combinator and the restriction operator are compatible with one another. And if you want to learn more about that, check out the original paper, Differential Restriction. The original paper differential restriction categories. All right, so once again, here's the map that you should take away from my talk. Hope you enjoyed it. Thanks for listening. Merci. And of course, I'll finish on that. Thank you, Jazz. Okay. Okay. Can we? So, first of all, before we open the floor to questions, JS, a little tiny conversation happened in the chat. Robin is not planning on showing the counterexample that you were talking about. So I would like to suggest that we do plan on taking that to Gather Town for anybody who wants to hear about it. And maybe at the end of this session, we'll talk about exactly when those conversations can happen. People might want to take a break. So we'll try to actually schedule times when we know people are going to be there. When we know people are going to be there. Okay, so having said that, can I open the floor to questions? Unless you want to start talking about the construction of a map, which is additive, not linear, or whichever way it's supposed to go. Sure. Okay. Okay. Sure. So an example of a map, an exam, okay. So an example of a CDC, which is actually, actually, can we do something wild? Let me just pull up a paper and see how that goes. So there are, so there, an example. Where'd it go? Yeah. Where to go? Yes. Yes. It's going to work. Give me two seconds. This is great. This is great conference, conferencing. Awesome. I am going to share another. Okay. Share screen. This one. All right, can everyone see this, these slides? Yep. Neat. Okay, here's a very simple example of, here's a very simple differential category. If you start with a category with finite byproducts, you can build a new category whose objects are the same as B, but whose maps are going to be a pair of maps f and g. And the identity is just a pair of identities, and composition is pointwise. So, this has finite byproducts, is also a category of finite byproducts. So, it has a differential combinator in the way that I described it in my other talk, but it also has another differential combinator, which is defined as follows. So, D of F G is just going to be G precomposed. Precomposed by pi one followed by G precomposed by pi one. And it's really important that you note that the copying of g is not a typo. And it turns out that this example is a co-free Cartesian differential category over any category with finite byproducts, with this differential combinator specifically. If you take the pair F0, that map is going to be additive. That map is going to be additive, but not linear. And that is the counterexample. I can't remember who asked that question, but hopefully that makes sense to them. Okay, thank you. Jeff Crutwell has a hand raised. Yeah, just a minor notational thing about the point vector or the vector point, like which order you write the bang in versus whether it's bang A tensor A or A tensor bang A. Tensor A or A tensor bang A. Can you go back to that slide or is that? I guess you're on a different slide now, but maybe it's. Yep. Yep, yep, yep, yep, yep, yep. This one? Yeah, somewhere here. Maybe you want this one? Yeah, I guess. Well, at some point, you were talking about, you know, we switched the notation from. Point, you were talking about, you know, we switched notation from point vector to vector point. One thing I want I remember was that you convinced us to change that notation, was my memory of what happened. But actually, I think there was a good reason for it. I think part of the point was that in a lot of the differential calculus literature, differential geometry literature, if they list things like a pair of a point in the vector, they lift it listed as point first and vector second. So I think that was part of the reason we went to it. And I thought you convinced us of that, and that was the reason we changed the order of notation there, but Change the order of notation there, but oh, so it really did change when I started working on with this with my murder, but it could be because, yeah, some of the earlier papers use one notation, then we switch to different notations, so it's useful to know. Yeah, I think the term logic got you convinced to flip. Right, that's a good point. Yeah, the way it is here with the vector coming second, the dot B, yeah. Further questions for JS? Okay, so hearing none, I think we will. So, hearing none, I think we will stop recording and the question session for now. Sorry, I'm having a little trouble. I'm having a technical problem on my end, which I'll take care of in a moment. Thank you again, JS. We appreciate the talk.