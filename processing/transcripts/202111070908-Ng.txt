In Montreal with Andrew Granville and at Michigan with Hugh Montgomery and with Sound. So you might infer from that background that Nathan's an expert in analytic number theory and you would be absolutely correct. So he's done work in L functions, lots of different things, as well as significant work on the Riemann Zeta function, which is the topic of this talk today, moments of the Riemann Zeta function. Take it away, Nathan. Thanks for the introduction, Mike. I'd like to thank Renata, Ha, and Habiba for inviting. Renata Ha and Habiba for inviting me to speak. And I'd like to thank them for all the organization they've done for this conference because I know it's really a lot of extra work to do the hybrid model conference. Okay, so here's what I'm going to talk about today. I'll just begin with an intro. I'll talk about bounds for zeta, divisor functions, moments of zeta, random matrices, and then approximate functional equations. And then if I have a bit of time, I'll talk about some current research topics. Current research topics. Okay. So that's the plan. And I was told to sort of make this an introductory talk. So for those of you that want to study the zeta function, there's some classic textbooks you can read. So sort of the classic book is Theory of the Riemann Zeta Function by E. C. Titchmarsh. It was revised, I think, in 1986 by D.R. Heath Brown. So if you really want to learn about the Zeta function, this is the first book you would study. Would study. A follow-up book is this book by Iwich called the Riemann Zeta Function. And Ivich wrote another series of lectures more advanced in the Tata Institute, this yellow book. Sadly, Ivich passed away, I think it was this year, and he was one of the leading researchers in zeta function theory over the last 30 or 40 years. Here's several other important books. There's this book by Ramachandra. It's also a Tata lecture series. It's also Tata lecture series notes. And then a sort of modern textbook is this one by Modahashi called The Spectral Theory of the Riemann Zeta Function. So, anyone wants to study zeta function theory, these are some good books to start. Another one I didn't put there on the slides is by Edwards, and that's a good book too. Okay, so the zeta function, it's given by this Dirichlet series, n equals one to infinity, one over n to the s for real s bigger than one. That shows it's holomorphic for real s bigger than one. It's holomorphic for real s bigger than one. It has an analytic continuation to real s bigger than zero, okay, and it's given by this formula. You can prove this by something called abel summation or partial summation. This integral right here is holomorphic for real s bigger than zero. And then you see there's a pole here with residue one at s equals one. Okay, so zeta has a simple pole, residue one at s equals one. one s equals one it has a functional equation relating zeta s to zeta one minus s and it's given by this formula here this functional equation was proven by riemann in his one paper on the zeta function in 1859 this functional equation it shows that the zeta function has trivial zeros at minus two minus four minus six and so on all negative even integers Even integers. It also tells you the zetas, the zeros of the zeta function are symmetric about the line real s equals a half. It also happens that the zeros of the zeta function are symmetric about the real axis. So here's some facts about the zeros of the Riemann zeta function, or I'm going to list some in a moment. The fundamental conjecture in the field is called the Riemann hypothesis, and it says all non-trivial zeros of the zeta function satisfy real s equals a half. Real s equals a half. Now, the trivial zeros, I remind you, are at the negative even integers. So it says all non-real zeros, the zeta functions, satisfy real s equals a half. So what do we know about that? There's a result of Platt and Trudgeon in 2021. They showed all zeros in. So the critical strip is the region between real S, between zero and one. They show all zeros in the critical strip up to height three times ten. Strip up to height three times ten to twelve satisfy the Riemann hypothesis. This was a large-scale calculation. I think it took them quite a long time to do. It was months and months on some supercomputers, and they verified this. What about zero-free regions? Well, we also know zeta has no zeros in the following region. So, say s is sigma plus i t. There's no zeros if sigma is bigger than one minus one over r log t. Minus one over r log t with r is 5.57. That's a result from 2015 of Mossinghoff and Trudgeon. And this built on a method of Habiba Qadiri, who previously proved it with 5.69. And they pushed her method basically to the limit. They got 5.57. This zero-free region goes all the way back to De La Valley Poussin, who got 30.46. And then you can see over the years, various people improved the value. Various people improved the values of R. So Westphal got 17.53 and 38, Stetschkin 9.65 and 75, Kadiri 5.69 in 2004. That was the record for about 10 years, and now the current record is 5.57. We also know there's not a lot of zeros of the zeta function in the critical strip. So we can prove sort of zero density results. The total number of zeros in the critical strip up to height In the critical strip up to height t is t over 2 pi log t, but you can actually show this very few zeros. So, say I want to know how many zeros are there between real part between three quarters and one, and imaginary part up to t. Well, it's basically less than 5.3 t to the two-thirds. That's a joint result with Habiba Qadiri and Alessa Lemli from 2018. Okay, so there's not a lot of, if there's zeros not on the critical line, so away from the critical line, there's very few of them. So, here's a Line, there's very few of them. So here's a picture. This is inside the critical strip. This is one half, and this is one. By platin trudging, we know all the zeros up to some very large number h, where h is three times 10 to 12. All the zeros sit on this blue line here. Okay, so there's no zeros off the half line in this section here. And then above imaginary part above H, you have a zero-free region. So you have this purple region right here. So, you have this purple region right here, and there's no zeros in there. And then there can be zeros in this green section and this section here. If you cut off at a point, say three quarters, then there's not many zeros to the right of this point here. So, here's a picture of some of the people that contributed to those results. So, there's Abiba, Tim. Tim was a postdoc in Lethbridge from 2010 to 2012, and he's now at UNSW in Australia. Alyssa was Alyssa was a master's student in Lethbridge from 2012 to 2014, and that's when we worked on that result on the zero density. Yeah, so there's actually a lot of people coming out of Lethbridge who have worked on the zeros of the zeta function. Elchin spoke about the total number of zeros of the zeta function up to height t yesterday. Okay, so now I want to talk about bounds for the Riemann zeta function. Okay, so the principal object of this talk. The principal object of this talk is about the two kth moments of the zeta function. So, here, k in this talk is mostly going to be a natural number, but you could take k to be any real number. So, you take the absolute value of zeta one half plus i t to the 2k, and you integrate it from 0 to t. That's called the 2kth one with a zeta function. Why do we study them? Well, Hardy Little would introduce them to study the size of absolute value zeta one half plus i t. Zeta one half plus it. So, oops, there's a typo. Um, so forget that one. So, the Lindelof hypothesis says the zeta function on the half-line is less than one plus t to the epsilon for all epsilon bigger than zero. Um, so what I was trying to say in the bullet above, which is typo, it says the Riemann hypothesis implies the Lindelof hypothesis. And the Lindelof hypothesis is this statement: zeta one half plus i t is less than. plus i t is less than t basically t the epsilon for any epsilon positive okay so this is a um an open problem in zeta function theory um and there's applications to this this hypothesis like to gaps between primes and there's other applications um it's known that the lindelof hypothesis is true if and only if the two kth moments are less than t to the one plus epsilon for all natural numbers uh so for all k and n and all epsilon pops All K and N and all epsilon positive. So that's this is sort of why Hardy and Littlewood wanted to study these moments. They wanted to prove the Lindelof hypothesis. And then their idea was, well, maybe it's easier to prove an average bound than a pointwise bound. So what do we know about bounds for zeta on the half line? Well, you can prove from a theorem from complex analysis called the Fragment-Lindelof theorem that zeta one-half plus psi t is less than t the one-quarter. Plus i t is less than t the one-quarter plus epsilon. Okay, so this is a version of the Hadermart three-circle theorem from complex analysis. So it's just to the right of one, you can get a bound of one for the zeta function, and just to the left of zero, you can get a bound of t the half for the zeta function. So the Fragment Lindelof gives you the convexity bound. So it gives you the bound halfway between a half and zero. So it gives you t the one-quarter. And zero, so it gives you t the one-quarter, okay. So that's called. Yes, I'm sorry. When you highlight with your mouse, what we see is actually a blurred space. Okay, I won't highlight anymore. Okay, thanks. All right, so you get t the quarter. Now, any bound better than t the quarter is called t, any bound better than t the quarter is called a subconvex bound. So people try to beat the People tried to beat the convexity bound and prove that zeta one half plus i t is less than t to the theta plus epsilon for some theta. Okay, so over the years, people tried to prove bounds better than one quarter. The first bound was T to the one six by Hardy and Littlewood. Okay, and this was using a method of vowel. And then over the years, people tried to improve the t to the one sixth. So you can look at this list and go down it all the way. Go down it. All the way up till 1986, Bambir and Ivanich got 0.1607. So basically, over 65 years, they didn't beat 0.16. And then in 1993, Huxley got 0.156. So we're very far away from proving the Lindelof hypothesis. Borgain in 2014 got 0.154. So we're very far from Lindelof. Very far from Lindelof, and people still work on this problem. I should say, this list isn't complete at all. There were many other results on the bounds for Zeta. You can look in Titchmarsh's book, and they have lots of references to that, and also Ibich's book. Now, what do we know about bounds for the 2kth moment? In 2013, Adam Harper proved the Riemann hypothesis implies that the two kth moment of zeta is less than t to the log t k squared for. t to the log t k squared for any natural number k so this was a this was a really important result and it built on a on a work of sandorajan who had t to the log t k squared plus epsilon now what about lower bounds um for any real k redziville and sanderajin got a lower bound of t log t to the k squared so um it's the same size as the upper bound on the riemann hypothesis and this built on work of many others in Built on work of many others, including Ramachandra and Heath Bran. So, from these, I should say that the book of Ramachandra I mentioned at the beginning has lots of results on bounds for the moments of the zeta function. So, these two results suggest the two k'th moment is asymptotically ckt to the log t k squared. Okay, so that's the expected size of it. So, that's what I'll talk about in a little while about what's the size of the 2k. Little while about what's the size of the two k-th moments. So now I want to talk about divisor functions and their connection to, say, the kth moment of the zeta function. Okay. All right. So what's the kth divisor function? So dkn, here k is a natural number. It's the number of k tuples m1 to mk that lie in n to the k and whose product is n. Okay, that's the k-th divisor function. Now, how is this related to the k-th? How is this related to the k-th moment zeta function? Well, we have zeta s to the k. You write out k copies of the series, and I'm using a different variable in each series. By absolute convergence, you can rearrange the series like that. And then what you do is you group, you say n equals m1 to mk, and you group terms together. So collect terms n equals m1 to mk. m1 to mk and once i group them the thing multiplying one around to this is this um this counting function the number of m1 to mk that line n k such that m1 to mk is n and that's precisely dkn so that's the connection between the kth divisor function and the kth power of the zeta function okay so it's the dirichlet's series coefficient of zeta s to the k The k all right, so let's just do some examples. What is okay? If k is 2, what is dkn? Well, it's just d of n, it's the number of divisors of n. So I think most people might have seen this. So just as an example, what is d20? Well, d20 is the number of divisors of 20. So I know it's early in the morning, but anybody know what that is? Okay, I'll let you think about it. It's six since 20 has the following divisors. Okay, let's do another example. K equals 3, and that's P prime. Let's work out D3 of P. How many solutions are there to N1, N2, N3 equals P? Does anybody know that? You can put it in chat if you want. Okay. Okay, there it is. It's three. Yeah, three. So you can write as p times one times one is one, or one times p times one, or one times one times one. No, one times one times p is p. So it's three. And okay, let's do something a bit harder. So d3 of p is three. Let's work out dk pj. So how many, what's the kth divisor function of pj? Okay, so you want to count solutions to n1 to n. Count solutions to n1 to nk equals pj. Uh, okay, so what should you do? Well, all these ni's are p to the ei for i equals 1 to k. So just they have to be a prime power, plug them in. So you have p to the e1 plus ek is pj. So you want to set the exponents to be equal. So you want to count solutions to e1 up to ek equals j, where the ei's are bigger than zero. The EIs are bigger than zero. Yeah, so this truly is a homework problem, not like the other two were a bit easier. So, if you want to have a bit of fun, you can try to think about how to solve this. And it works out it's k plus j minus one choose j. So that's a little counting problem. So if you want to have some fun, you can think about that. So that's dkpj. All right. That's the divisor function. Now I want to talk about divisors on average. So how could you evaluate? So, how could you evaluate divisor functions on average? So, let's begin with a sum of n less than x of d of n, okay? So, d2 of n. So that's n less than x of ab equals n, count one. Now, if you rearrange this sum, it's just the number of pairs ab less than x, count one. So, this is the number of lattice points ab and n squared, such that ab lies between. Squared such that AB lies between 1 and x. So, this is what Dirichlet realized: this geometric interpretation of, so he wanted to count the number of n less than x of d of n, then he recognized it was the number of lattice points, ab less than x. And then what you can do is you can draw a picture. And when you draw the picture, you can split that picture into a couple regions or three regions. And then you count the number of points, and you get exactly x log x. And then there's an error turn. log x and then there's an error term of size square root of x and that's what this gamma function so you can count uh n less than x of d of n and you get this answer so it can be done now what about if i want to count dk of n n less than x well you do the same sort of thing you put in the definition of dkn and then you rearrange the summation and it turns out to be the number of lattice points of a1 to ak and nk such that they're products less than x. Such that they're products less than x bigger than one. And by induction or like by a geometric argument, you could get it's equal to this, okay, or asymptotic to this. It can also be done by another method called Perron's formula, but this is sort of the simplest elementary way to do it. Okay, what about something a bit harder? What if instead of I want to sum n less than x of dkn, I want to sum dkn squared? Well, it turns out you can do an elementary argument or like using. Elementary argument, or like using sort of SIB methods, or even something called Perron's formula, you can very easily prove some n less than x of dkn squared is asymptotic to a k prime x log x to the k squared minus one. And then another sum we're interested in is not just summing dkn squared. I want to divide dkn squared by n. And if you use something called partial summation, you can get from the first formula dkn squared. Get from the first formula that dkn squared over n is asymptotic to a k over k squared factorial log x to the k squared. Or I could just do this directly by Perron's formula. So, I mean, the upshot is if I want to sum n less than x of dkn squared, I can do it. And same with dkn squared over n. There's sort of elementary arguments to do it, or you can use something called Perron's formula. And then you get these constants ak prime and ak in front, okay? All right, so um All right, so let's look at something slightly harder. Oops. Let's look at what are additive divisor sums. So additive divisor sums are much harder to evaluate than sum n less than x of dkn squared. So what I want to do is in a summation, I want to put in the first term dkn and then in the second term a dl n plus r. So l is a different integer. It could be equal to k, or maybe it's different. So and then what I do is I shift n by And then, what I do is I shift n by a little bit. I shift it by, say, one unit or two units. So, I sum n less than x of dkn dln plus r, and r is a non-zero integer. So, this is called an additive divisor sum. In the language of probability, this is a correlation sum. And we like to evaluate these two, but they're hard to evaluate. So, why are they hard to evaluate? The basic reason is the prime factorization of n is not directly related. Of n is not directly related to n plus r. In the earlier sum, dkn squared, you can think of it as dkn times dkn. And if you know the prime factorization of n, you also know the factorization of n. So it's easy. Okay, what do we know about these sums? Well, if k and l are both two, so you just put the ordinary divisor function in there. There's a result of Motahashi from 1994 building on early work of Estherman, who was the first to. Early work of Estherman, who was the first to do it in 1930. The sum d22xr, it's x times this expression here, and there's coefficients c0r, c1r, and c2r. And then there's descending powers of log, log squared x, log x, and then log x to the 0. That's what the main term looks like. And then Motahashi proves an error term of x to the two-thirds. And this result is good for all integers r up to. Good for all integers r up to x to the 20 over 27. Now you might ask, what are the coefficients these c0 are? Well, they can be very complicated. In this case, when you have k and L is 2, it's not that complicated. It's just 6 over pi squared times, this is called sigma minus 1 to the r. So it's a divisor sum, d divides r, and you count d to the minus 1. Okay, so that's that's the result of Modahashi. What about K? What about k bigger? So, you can also prove something if k is bigger than 2 and l equals 2. So, there's the following result of Topako-Ghilari in 2018 and Drapeau in 2017. So, dk2xr, it has the same form. There's an x in front. There's these coefficients c0r all the way down to ckr. Oops, I see there's a typo here. Multiplying ck minus 1r, there should be a log x there. There should be a log x there. And there's a power savings error term. So this error term is little of x, and this is good for absolute value of r up to x of 15 over 19. So I've written the result for k between 3 and 15. There's also a result for k bigger than 16, 2, but it would have been too complicated to write down in both cases. And these coefficients C0r to CKR, they're very complicated. It's hard to write down a formula for them. Hard to write down a formula for them. Now, how do you prove these things? The method is called the spectral theory of automorphic forms, and it uses something called Kuznetsov's formula. So that's what's known about these additive divisor sums. There's actually a conjecture about the additive divisor sums, and I'll label it ADK-Sabel. It's due to Vinograda Ivich, Conrig Gonak, and it sort of developed over the years 1989 to 1989. Developed over the years 1989 to 1998 in a series of papers by all these people. Well, Ibich wrote a paper, Vinograd wrote a paper, and Connor Gonick wrote a paper. It says that let epsilon, epsilon prime be two positive numbers. I guess, yeah, epsilon should be less than one. It says that adaptive divisor sum is a main term, and we believe there's an error term of size square root of x, okay? And this formula should be uniform for absolute value of r up to x to one minus epsilon. Up to XL1 minus epsilon. Okay. And it looks, what does the main term look like? There's an XL front. There's these descending powers of log, and the highest power is k plus L minus 2. Then it goes down to the second last one is log x, and the bottom one is log x is zero. And then you have these coefficients in front. Okay. As I said, the coefficients are very complicated. But the first one, C0R, looks basically like this arithmetic function. This arithmetic function. There's an exact formula for them that can be written down. So in 2016, Terry Tao wrote a blog post on C0R, the first coefficient. He gave a formula for them. And one week later, Mark Tom and I put a paper on the archive giving formula for C0R. So he had a probabilistic method and we did two. And they both gave the same answer. It was actually surprising because the methods were different and the formulas were different, but you can actually prove their. Are different, but you can actually prove they're the same. Yeah, so Mark Tom did an NCERC USRA with me in 2008-2009, and that's the project we worked on, or one of them. For the lower order coefficients, nobody's written down a formula for them, although it's probably possible, but very difficult. Okay, so what we expect is that these additive divisor sums are asymptotic to C0R, X, so log X to K plus L minus 2. x so log x to k plus l minus 2 and the leading term looks like this the c0r looks like that now what do we know about this by the previous slides that versions of the attitude divisor conjecture are true if k is bigger than two and l is two that's due to tolpako gilari mudahashi many others okay um if both k and l are larger than two say they're both three nobody's proven anything it's an open problem okay Proven anything. It's an open problem. Okay. However, this partial results towards that advisor conjecture, you can get lower bounds and upper bounds. So the lower bounds due to myself and Mark Tom. The upper bound is due to Kevin Onrial in 2015. And there's also almost all results in R due to Madamaki, Radzivill, and Tao. I should say Kevin Onriel. So Kevin Onrio proved the upper bound. Kevin Onriel was a brilliant number theorist and also analyst. Number theorists and also analysts and additive commentaries, and sadly, he passed away. But now there's a prize in the name of Kevin Unrial for young analytic number theorists in France. So he did some really brilliant work. There's a picture of Kevin when he visited us in Lethbridge. That's my former PhD student, Farzette Arian, who was a very good friend of his. Okay, so when you study the zeta function, you actually need to not You need to study not just additive divisor sums, but smooth additive divisor sums. So, oh, I see, I have another type of here. I want to study a sum of this form. M minus n equals r. So this is this tau k really should be dkm, and this tau l should be dln. And what you attach to it is a smooth function f. And now this smooth function is supported in some box m2m n2n to r. n to n to r and it's smooth. And what we want to do when you study the two kth moments, you want to formulate a conjecture for these smooth sums. And those are the things you really want to work with. Now it's very complicated to write down this additive divisor conjecture for the smooth sums. So I'll just say there is an additive divisor conjecture for the smooth sums, but it's too complicated to state. Versions of this additive divisor conjecture have been proven by Duke Friedlander Ivanis in 1990. Proven by Duke Friedlander Ivanis in 1994 and also by Farzad Arian in 2017. So that was my PhD student in the previous slide. So I just want to point this out. So let's. So I want to talk now about sort of what a Dirichlet polynomial is and its connection to the zeta function or the case moments of the zeta function. Oops. So a Dirichlet polynomial is a truncation of a Dirichlet series. Sorry, can I interrupt? Sorry, can I interrupt? The the screen doesn't the the page yeah oh okay it is it okay now screen perfect thank you sorry uh the i see your files now okay so we see your files okay it's okay so a dirichlet polynomial um is what is it it's a truncation of a dirichlet series so if i take the dirichlet series for z to the k and i just cut it off The k and I just cut it off at the end term, that's called a Dirich-shaped polynomial. Now you would expect, so when you study the zeta function in the critical strip, well, the series doesn't converge, so you have to cut the sum off. So if zeta s to the k is approximately this dksn when s is one half plus i t and n is t to the k. So you can prove something like this using something called the approximate functional equation. Something called the proximate functional equation, which I'll talk about later. So now I want to explain the connection between the 2kth moment and these Dirichlet polynomials. So we have zeta one half plus i t the 2k. Well, that's the same thing as zeta k to the half plus i t squared. So what I can do in this middle line, I can put in the Dirichlet polynomial and I can square it. Okay, so this n here is about t the k by the approximate functional equation. Decay by the approximate functional equation, and then on the second last line, I have the absolute value of z squared. So, absolute value of z squared is z times z bar. So, you just take that sum and you conjugate it, okay? So, the two case moment of the zeta function is approximately this expression here. So, it suggests, so what I could do is I could plug this formula into the two-kth moment of the zeta function. So, this suggests the two-kth moment of the zeta function. So, I'm going to do The zeta function, so I'm going to do a little trick first. Instead of integrating from zero to t, I'll integrate from minus t to t and insert a one-half because this function is even. And then what I'll do is I'll plug in this absolute value of the Dirichlet polynomial squared. So the second last line. So I'll just plug that in. So you see the two-kth moment of the zeta function is approximately this expression here. Okay. So what I want to do to get the two-kth moment is just integrate this. Oops. Okay. Oops. Okay, so the two kth moment of the zeta function is approximately this integral of you have dkm dkn mn over minus it. Now, else what you can do is you can swap summation and integration, and then you're integrating mn to the minus it. Now, if m equals n, that's just 2t. And then it's another little exercise. You integrate that, you get 2 sine t log. You get 2 sine t log mn over log mn. So if I plug that integral in and then swap summations, what you do is you group the terms m equals n, they're called the diagonal terms, and then you group the off-diagonal terms m not equal to n. Once you do that, you get two sums. You get the sum from m equals 1 to n, dkm squared over m. And then you get the second sum in this labeled equation 2 is dkm dkn over. Is dkm dkn over square root of mn. And then you have the sine function, sine t log mn over log mn. Now, the first sum we saw in an earlier slide, we know how to evaluate dkm squared over m. That's something easy to do. And now you look at the second sum and you wonder, well, sine is oscillating, so you would think this is small for lots of m and n. And it turns out it is. But if m is close to n, log mn could be very large, a very small. Could be very large, very small. It could be close to log of one, which is zero. So you're dividing by zero, so it could be very large. So it turns out terms with m equals n plus r and r is small, they're going to contribute to the second sum. And if you do a little bit of partial summation, what you see is to value the second sum, you need to know how to evaluate n less than x dkn dkn plus r, which we talked about earlier. So all evaluations of the two-kth moment of the zeta function with k equals one, two, three, or four. So let's say One, two, three, or four, so the second, fourth, sixth, and eighth moment of the data function, they all go through this sort of technique. Not exactly this, but something like this. And if you know how to evaluate these additive divisor sums very precisely, you can get the moments. So that's basically how the fourth moment is done. And same with the sixth moment. Okay, so I want to talk about moments of zeta a bit. So what's the history of it? Okay, so what do we know about these moments? Well, the second moment was. Well, the second moment was evaluated by Hardy and Littlewood in 1918. They showed it with size t log t. And I can write this as one over one factorial a1 t log t, where a1 is one. Eight years later, Ingham did the fourth moment, and he showed us t over two pi squared log t to the fourth. And I can rewrite this in this weird way, two over four factorial a2 t to the log t to the four. And they actually use the result of. actually used the result of Hardy and Little. He used the result of Hardy and Littlewood on the, it's called the approximate function equation for zeta squared. Now, since then, nothing has been proven. So in 1996 and 98, Connery and Ghosh, they came up with conjectures for the sixth moment and eighth moment. They conjectured the sixth moment was 429 factorial A3T the log T9. And then Conrad Gonick, a two years ago, And then Conry Garnick two years later did data moment and they said it was 24024 of a 16 factorial a4t log t to 16. Okay, so for certain constants a3 and a4. Okay. And that was a conjectural method using something called Dirichlet polynomials. So also in 1998, Keating and Snape, they came out with a conjecture for the two kth moments for all k. And they said they were asymptotic to g k over k. said they were asymptotic to gk over k squared factorial ak t to the log t k squared and what was g k well g k was this number k squared factorial times this product of factorials so j equals zero to k minus one of this thing j factorial j plus k factorial and k a k some infinite product so they they were working at a university of bristol nina snaith was uh john keating's phd student at the time nina by the way is canadian and she i think she did her undergrad at Canadian, and I think she did her undergrad at McMaster, but she's been living in the UK for a long time now. So, this was a big advance at the time, and they used what's called a random matrix model to model the zeta function. So, this is what we know about the zeta function, the two-kth moments, and this is what we think their behavior is. So, we think they're asymptotic to t log tk squared, gk over k squared factorial times a k. And here, gk is this product right here. And so this finite product of factorials. And then ak is this infinite product over the primes. By the way, this k plus j minus one choose j. I don't know if anyone remembers what that was, but that's that's dk to the pj that we saw earlier in that little exercise. Okay, so let's just summarize. Hardy in black, there were two thirds. Summarize. In black, there were two theorems for the second moment and the fourth moment. Harding Littlewood did this. The technique was Dircher polynomials plus approximate functional equations. And then a bunch of people made conjectures for the sixth moment, eighth moment. So I listed all in red are the conjectures. So Connery and Ghosh did the sixth moment. Connery Gonick did the sixth and eighth moment. And they came up with the 42, the 4224024. Now, Keating and Snake, they gave a formula for all K. Nate, they gave a formula for all K, and you might want to wonder: does GK match up with this 42 and 24024? It does, so there's homework. Oops, if you're a bit bored right now, some fun, try to work out G3 and get 42. And you try to get G4, it's 24024. Even harder homework, show that GK is an integer. It actually is. You can check that. I think there's a paper of Economy of Pharma where they show that. And then And then, so when this was done, it was believed people believed this conjecture because there were different methods that gave the same values. So, Conner-Gosh, Conrad Garnik had Dirichlet polynomial plus approximate functional equation methods, and then Keating-Snaith used random matrices. They got the same answer. Okay. Okay. So there are several other methods due to Diocano, Goldfeld, and Hofstein. And then there's another paper of Connery Farmer, Keating, Rubenstein, and Snake. Of Connery Farmer, Keating, Rubenstein, and Snake, they also give the same answer. Okay, all right, so I'm going to move along. I want to talk about the connection to random matrices. So, Keating and Snake, they model the data function with a random matrix model. And I want to say what that is. Okay, so in 1972, Freeman Dyson and Hugh Montgomery, they met at the Institute for Advanced Study in Princeton at tea time, and they were talking, and they realized that statistics of zeros of the zeta function is the same as statistics. Of the zeta function is the same as statistics of eigen angles of random matrices. So that was a huge breakthrough at the time because random matrices were used in the study of like energy levels in nuclear physics. So for energy levels of heavy nuclei, they studied that. And then, so basically, they studied use random matrices to study these energy levels. And they looked at the statistics of them, they computed them, and they realized. Computed them, and they realized they had the same statistics as zeros of the zeta function. So in 1987, Andrew Edlitsko, he did lots of numerical calculations of the zeta function. He verified this numerically that they really seem to be showing the same thing. There's theoretical evidence towards this connection due to Montgomery in 72, Hedgehog, and Rudnick-Sarnak. And then in 1998, Keating and Snaith modeled the zeta function by the character. model the zeta function by the characteristic polynomial z u theta so what i want to do now is i want to explain what what what are these statistics so what you do is you take a large integer n and you take the unitary matrices um a a sorry equals in now for any um so yeah un is a compact group with a hard measure d mu n so this is in vowel's book classical groups a unitary matrix u has eigenvalues on the unit circle eigenvalues on the unit circle say e the i theta one to ei theta n and you can order them from smallest to largest so say theta one up to theta n the average spacing of these numbers is two pi over n okay so n numbers and interval length two pi now you can take the zeros of the zeta function label them as one half plus i gamma n and then label them increasing order say gamma one gamma two up to gamma n gamma n plus one the average spacing of these numbers is two pi Spacing of these numbers is 2 pi over log gamma j, which is asymptotic to 2 pi over log t when gamma j is between big t and 2t. So this follows from the zero counting formula n of t, which Elchin talked about yesterday. Now, those are the zeros of the zeta function, and you have these eigen angles attached to, say, some unitary matrix. Okay, so you just take a, you pick a random matrix, and then you label the eigengles. So this matrix. angles so um this matrix has eigenvalues e the i theta one ethi theta n you just you look at those eigenvalues okay what you want to do is actually you want to normalize these um the spacings between these eigen angles okay so you multiply each of them by n over two pi so on average um and i i call this new number theta j hat on average theta j plus one hat minus theta j hat um it's about one on average. It's about one on average. And you can do the same thing with the zero zeta function. You scale each of them, you multiply them by log gamma j over two pi times gamma j. On average, that difference is one. Okay, now the Dyson-Montgomery observation is that the number, so I want to look at statistics of these eigengles, and I also want to look at statistics of these zero ordinates. Okay, you could count the number of these normalized. Normalize differences that lie in a bot in an interval, say between one and two. How often do they lie between one over two? And you look at that count. And you can do the same thing with the zeros of the zeta function. You look at that count. How many of them lie between one and two? And what they noticed was that basically what Dyson and Montgomery said is that these statistics are the same. If you try to take the limits, these numbers should be the same. So in random matrix theory, they knew that there was some density function p of. There was some density function p of u. So, if I took the limit, this limit exists, and there's some density function. And Montgomery had done a similar statistic called the pair correlation conjecture, and it matched up with what was happening in random matrix theory. So that's what we mean when the statistics are the same. Okay, so you could actually draw pictures of, say, eigen angles around the matrices. So this would be this third column right here, column C. Third column right here, column C. And you could draw a picture of the zeros of the zeta function. That's this fifth column, E. And if you look at the statistics, the pictures, they actually look the same. So this is C and E. Here there's also a Poisson distribution in the first column. The last column is a uniform distribution. So this is from a paper by Higas and Giannoni. And that just pictorially, you see they look the same. Here's what a little Here's what Elitzko did. So, if you start taking the zeros of the zeta function and you look at the normalized differences and you just start putting them in boxes, so how many lie between one zero and a tenth, a tenth and two tenths, and so on, you can make a histogram. And this function right here is that p of u, that's the neighbor spacing for eigengles. And these dots here represent, say, the tops of, say, a histogram of little boxes. Histogram of little boxes of where these integrals fall in. So if you see from this picture, the dots sit right on top of that curve. This is the curve P of U. And that's why people believe the statistics are the same. It's not been proven theoretically, but that's what they believe. Okay, so what I want to talk about now is a model for the zeta function. Okay, so what was the model for the zeta function that Keating and Snaith used? Well, they use essentially characteristic polynomials. Essentially, a characteristic polynomial. So you take a random matrix u and u n, and you look at the determinant of i n minus u e to the minus i theta. So that's basically the characteristic polynomial. And here theta is just some parameter, 0 to 2 pi. So it's well, characteristic polynomial would have x in front of i n, but this is a scaled characteristic polynomial. If you use the formula for the characteristic polynomial, it factors like this: 1 minus e to the minus i theta g. 1 minus e the minus i theta j minus theta. And so this is like a model for the zeta function. It has zeros at theta 1 to theta n, which are the eigen angles. So what Kitty and Snath did is they computed the two kth moment of the characteristic polynomial. And what it is, is you take the absolute value of this function and you integrate with respect to Haar measure and you integrate over the unitary group. And this is their model for the two-kth one of the zeta function. function. Okay. So Keating and Snaith show that this 2kth moment is this product of gamma functions. And then using Sterling's formula, you can show it equals this product, j factorial or j plus k factorial times n to the k squared. Okay. So going back to here, sorry, you have to divide up by one over t here because this is hard measure. Here, because this is hard measure. So, you want the total weight of the measure to be one. So, this is the analogy for one over t i k t is this. Okay, um, so that's the computation of two case moment of the z function. And it suggests, so if you set the average spacing of zeros to eigen angles, so you set n to be log t, it suggests that the two case moment of the zeta function is this number here times log t k squared. And then you have to put the arithmetic factor. And then you have to put the arithmetic factor in front. Okay. So that's how Keating and Snaith computed that. Okay. And that's where their conjecture came from. I think I have five minutes left. How much time do I have left? So I'm looking at the schedule. Yeah, like two minutes, something like that. Two minutes? Okay, all right. Okay, all right, so I'll just skip this section then. All right, so the sixth moment the zeta function, I did some work on this. So if you assume the attitude divisor conjecture, so d3n, d3n plus r is asymptotic to x, the log fourth to x. If it showed an average form, that the divisor conjecture implies I3t is less than t the one plus epsilon. Okay, um, so I showed in 2021 in this paper in discrete analysis that the smooth turnary additive device conjecture implies that the sixth moment is asymptotic to G3 over nine factor A3T to the log t to the nine. And I just remind you, smooth turn reactive divisor conjecture has to do with d3m, d3n. To do with D3M, D3N, FMN, where F is smooth. And so previously, Connery and Gonick proved a heuristic argument for this. So yeah, the main thing here is you have to use smooth additive divisor sums. I also recently did some work on the eighth moment. So the additive divisor 4, 4 conjecture is d4n d4n plus r is asymptotic to x the log 10 dx. And we have to assume error term of And we have to assume an error term of size root x with r all the way up to x1 minus epsilon. So with Chen Li Shen and Peng Ji Wang, we're proven the eighth moment is asymptotic to 24024 over 16 factorial, A4T to log t to 16. And this uses the smooth quaternary additive divisor conjecture. Okay, so this makes rigorous some work of Connery and Gonick. Gonic. And I guess I'll just, yeah, I was going to say a bit more recent work on additive. So I'll just stop there then. Thanks. Yeah, we're going to record it much. Okay, do we have questions? Yes, we are. Yes, Amia.