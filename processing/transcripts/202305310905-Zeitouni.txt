It's a pleasure to welcome me. We'll talk about all the menace and moments. Thank you. It's a pleasure to be here, to be in this meeting. So I'm afraid I'm going to bore a little bit Timo with this, but my experience with Timo is the following. Each time I do something that I think is nice and whatever, then sometime after, And sometime after, FEMO does the same thing, but in completely different methods with bigger generality and much nicer proofs. So maybe this will happen here, you know. That's a challenge. I don't like the topic. So it's really a pleasure. Anyway, I'm going to talk about directed polymers in the weak disorder regime. And so there's a general setup that I'll go over. So the general setup that I'll go over quickly because in this audience I think it doesn't need too much introduction. We have a graph, we have IID weights in time and space, so I would be time, x space. And we have a simple random walk on G, on the graph, which we bias by the weights along the vertices it exists. Okay, so that's the So that's a new law of the walk. The main object of interest will be Zn, and for reasons that will become clearer, I choose to keep the starting point of the work in the notation. Now, this is easy to check. This is a positive martingale, and therefore it converges almost truly to a limit, which we call Almost truly to a limit which we call z infinity of x. It's not very hard to convince yourself that z infinity of x can be zero, there are cases where it will be zero, or can be a non-degenerate random variable. And in fact, in quite general setup, there's a zero, one goal. Either it is degenerate, in which case it doesn't depend on the starting point, or it is some limit, non-degenerate. Some limit, non-degenerate limit. And the fact whether you're in that, in one or the other case, in general, does not depend on x. Now, the continuous analog that I just wrote here and that many people around here know, I wrote it for dimension three. And you can ask this kind of questions for this equation, and there's a very rich homogenization theorem for that. Theory that I'm not going to describe. In what is called the weak disorder regime, that I might have done too quickly, the case in which this Z infinity is non-trivial, non-zero, is what I'm referring to as weak disappointment. So for dimensions 3 and larger, in the continuous setup, there is a generic, first of all, there is a rule. First of all, there is a proof, a general proof, that you have a sharp transition in terms of this parameter beta here, in terms of the strength of the noise. So when the noise is strong enough, then you have strong disorder, Z goes to zero, and otherwise you have weak disorder. Okay, so since I'm not going to talk about So I'm I'm since I'm not going to talk about homogenization today, I'm not insisting on this continuous setup, but this is a point of contact to some of the talks. Okay, now if you have a uniform L2 bound, then of course Z infinity is non-degenerate and under mild conditions it's almost surely positive. And in particular, for Z infinity, And in particular for Zd, for the graph Zd, this happens in dimensions 3 and larger. So there is a range of betas with some beta 2, I'm denoting it, from which you can compute that. That's actually quite easy. It involves, when you compute the second moment of Zn, it involves the intersection local time of a pair of blocks. And for beta small enough, it wins. It wins the tail estimates on the intersection local time because the walk is transient, and therefore you get the double crop. On the other hand, there's another threshold, which is the largest beta for which you have weak disorder. And in general, beta C is smaller than beta 2. And in fact, it kind of goes back now for more than 20 years. More than 20 years, or in dimension 1 and 2, beta C is just zero. So for any positive beta, you have strong disorder. And all that is related to localization and delocalization, etc. I should say that there are graphs in which beta 2 is 0, but beta C is positive. And those Um and those uh in fact for percolation cluster on Z D, beta two is uh zero and in fact beta C is also zero turns out. Yes, as an open problem from last year, Nietzsche. Okay, now I want to focus on Zd and as we showed there is no weak disorder for D equal to 2. D equal to 2. All the talk is going to be in this case D equal to 2. And the title of my talk was Polymales in the Weak Disorder Regime. And now I'm telling you I'm going to focus at dimension two, where there is no weak disorder. So what do I mean by that? Well, that's an observation going back to Bertini Cantrini and very much developed by Carla Vedna, son and Ziguras and other people. And Zigoas and other people will quote it later, but since their work plays a prominent role in what I'm saying, I'm going to abbreviate the initials as CSC. And what they suggested is to modify the parameter beta and to rescale it. That's not very surprising that you should do something like that, but they are They were the first to point this out. So if you take Rn to be the intersection local time, which is essentially the number, the expected number of visits of a random walk to zero, the two dimensions of a random walk to zero, which scales like log n, and you rescale by that quantity, and now you look at zn of x in that form, then some. Then something interesting happens if beta hat is not too large. One thing to note is that you lost the martingale property. So Zn defined this way is not a martingale. You need to do something in order to make it into a marking L. And in fact, for beta hat smaller than 1, then the log of the n. And the log of zn, and now when I don't write the starting point, I just take it to be zero. So the log of the n converges in distribution to a Gaussian random variable with a variance given by that quantity. And that, of course, is true for beta hat smaller than one. As you approach the critical point, things start to happen and then And okay. Now, not only you have that convergence, but in fact, as shown last year, or two years ago, by Conice and Nichols here, in fact, Zn itself converges in all Lp to the exponential of a Gaussian. It's actually a stronger notion of convergence than just what I said. There are results also for the continuum. Our results also for the continuous model, both by these people and also by actually the orb over here is universities, Gu cosm psi. And however, okay, all that is nice and probably known to many of you. My starting point and the entry point to this list of my interest to that is that if you look Is that if you look at the following object, so let's take log of the n, we know it converges to a Gaussian, subtract the mean so as to get rid of this lambda square over 2, but now blow it up. Okay? So now you've got yourself a random variable whose variance goes to infinity. And what was proved by CSC is that it CSC is that in fact this converges to a logarithmically correlated field. Okay, and it goes to a logarithmically correlated field with variance in quotation marks, which is this one. So that's a bit different from the lambda squared that you see up here. So, and of course you can write exactly what kind of log correlated field it is. It's a solution of the non-multiplicative. So, again, the continuous work of Guero, the work of Nakajima and Nakashima that show this. For me, coming from the world of recently coming from the world of low-correlated fields, anytime you see something like that, You see something like that, you're asking yourself what kind of things can you say about this local paper field. And in particular, maybe you want to study thick points, which are points x, where the value is much larger than the typical value. Maybe you want to ask yourself what are the extremes of g n of z, of g n of x as a function of x, let's say in a Let's say in a blown-up lattice or in a lattice of psi square root n. And for that, usually the first step is to understand exponential moments. Now, we already saw, I mentioned earlier, some moments of this thing, and that gives you some exponential moments, but because of the blown up, Of the blown up, because of the blown up by square root Rn, when we're talking about exponential moments, we're really talking about exponential moments with a very large exponent, square root Rn. It's not just exponential moments of log of Zn, which are moments of Zn, but it's actually high moments of Zn, of the order of square root Rn. And those are what you need if you want to do any. Are what you need if you want to do any kind of study of extremes. Okay, so the first result I want to mention is this upper bound with Clemencouse. So this upper bound is telling you that the Q moments of Zn, where Q can be as large as Can be as large as log n, sorry, where q squared can be as large as log n times some constant, really match the Gaussian pace. But I'm emphasizing that the Q you need to take is really of this order if you want to study the log correlated structure. So if you have this condition, I'll say more on that. More on that. Later, then you get really the Gaussian one with the variance that corresponds to the point-wise variance. Now, if Q is slightly smaller, then you don't need any condition on better hat, except for being smaller than one. And in particular, this gives you a uniform integrability for all finite moments. For all finite moments. So, in a sense, this recovers the results of because you have already weak convergence, as soon as you have a good upper bound of exponential moments, you have the convergence in A. Okay. Now, the second result I want to mention is that this is sharp. Namely, for any Q that is not too large, you get the same type of lower bound. Type of lower part. Okay? So we now have at least the first step in doing extremes, which is to get the right behavior of the relevant exponential models. Questions about the statements? Okay, so I have a few remarks on that. So in The first thing is that if you want to do extremes, you need to understand some other exponential moments, not just of the end point, and you need to do some kind of scale decomposition. And here there is a very natural scale decomposition in terms of time because we are looking at the expected value of e into the sum from 1 to 1. sum from 1 to n n I think I call it omega of i x i minus normalization that's the object we are looking at and of course you can replace these by some t1 and t2 and that will give you And that will give you some pieces of the exponential moments. And if you look at it from this perspective, then it's very natural to think where does the local correlated structure come from. Indeed, if I could show that this thing is essentially the product If I could show something like that, then you have a natural state decomposition on the one hand in time. On the other hand, in space, if you put here an X, of course pieces that are before particles start to meet together, starting from different X's, don't see each other. Different axes don't see each other. So you get a very natural independence in space. And that leads you to think that there should be some kind of branching structure behind the storm. So I want to focus on that, but first let me finish the remarks. So one comment is that the upper bound that I mentioned already gives an upper bound of the maximum of zn of x, but not good enough to go to the what Enough to go to what we expected at the time to be the correct answer, which would be the same as if the variables were independent. That's kind of what the law-correlated structure tells you. The leading order should be the same as if the variables are independent. As I try to convince you, this is actually not true. But anyway, because of the condition we had on the Q in the upper bound, we couldn't even get We couldn't even get to that, and we could get on smaller boxes. Now, another remark is that in continuous time, the lower bound is actually easy, relatively easy, and was done by recent work of CSZ based on Gaussian correlation inequality. The word Gaussian here does not The word Gaussian here does not refer to the disorder, but rather to the Brownian motion. So the continuous setup, you have Brownian motion, you're essentially doing finite cuts with respect to a Brownian motion. And there you can apply a version of the Gaussian correlation inequality that deals with convex symmetric sets to get a lower mark. So the one comment, and maybe someone in this audience. And maybe someone in this audience will have a better idea. We don't see how to apply that in the discrete setup. If we could apply it in the discrete setup, no need would be for the work on more. Now, now comes maybe the main message for people who have looked at that from this talk, which is some kind of the difference between the Difference between the variance that we saw between the variance of the log of the n, the pointwise variance, which was lambda squared, and the GMC variance, which is actually lambda. Okay, so by GMC, I mean, I called it GMC. I meant really the low-coverette variables. So this was from So, this was for a long time a bit of a mystery for us. I mean, in principle, there is no reason for these two variances to be the same. Take a low-correlated field and just add to each edge some independent variable. This will be completely washed out when you integrate against smooth continuous functions. But most examples we know of log-correlated fields, this is not the case. Field, this is not the case. The variance of the low-coordinated field you get is the same as the variance of the point-point variance. So what is happening? So from this description I told you here, and in fact from the proof of the rho-bound that I'm going to talk about later, in principle you can justify some version of this equation, of this Wiggle side. Of this wiggle sign. It's not quite right, but it's almost right. You have to introduce buffer zones and stuff like that. But if you do that, you do get a certain product structure, which I'm going to write like that. And which is exactly of the form that you go from Ti to Ti plus one, where the Ti grow essentially diameter or exponential. And in And in that situation, you can check that the z and k themselves are close to 1. And if you bound the error of the variance of what remains, when you choose the ti properly, and properly means essentially in exponential scales, you get a variance that looks like this. That's a computation, but I don't want to. That's a computation that I don't want to show you, but you should accept it. And these z and k's are roughly independent, and what this means is that you have essentially a tree structure. So when do points x and y become correlated? If enough time has elapsed that the random walks can move. So that gives you, in terms of That gives you, in terms of a genealogical structure, something on the points in the plane. So you should imagine this like that. On the other hand, the variance on each of the edges is different, is not constant. That's the main point here. So the variance is what I computed above, and as you can see, it depends on K. As you can see, it depends on k. Right? It's just this function of k. So, a caricature for our model, and I should say that for the moment this is somewhat speculative, so I don't have yet hard results on that, but they will be coming up. No doubt, but I thought that the picture is interesting enough to describe it. So, if you look at the So, if you look at the total variance, what you get is the sum of these variances on each edge. So, if I fix an x and I'm looking at the total variance getting to here, what I'm going to get is this log of 1 over 1 minus beta hat square, which is exactly the variance we have. On the other hand, if you're looking If you're looking about the log correlation, the log correlation, they really come from the bottom of the tree. Sorry, from the top of the tree. And the top of the tree has higher values. So in particular, this is giving you the beta hat square over 1 minus beta hat, which for me for a long time was a mystery. Why you got different variants? So that's the answer. You have different contributions from. You have different contributions from different scales. They are not of the same variance. And therefore, you get this kind of picture. And there is an interesting consequence of that, which again, for me, changes a little bit the perspective. You can ask what is the maximum of the log when you look at x in the lattice. So you take your lattice of psi squared n, you put here on your starting point. You put here on your starting point, and you compute the maximum load of the L of X. So this is a prediction, but let me try to tell you where does the prediction come from. So in fact, a proof of lower bound essentially gives this prediction for the lower bound and for the upper bound as a little bit. The thing is that if you The thing is that if you have the following border of branching on your block, you take weights on the edges and at time t, so at the edge between t and t plus one, you put the variance sigma of t over capital T. Capital T is the depth of your t. Okay, well sigma is some fixed function. We saw Some fixed function. We saw in my previous slide what sigma should be. It should be something like 1 over 1 minus t times something. And you ask, what is the maximum here? So, turns out there's a big difference between sigma constant, sigma increasing, and sigma decreasing. And for sigma decreasing, there's a formula. The formula is exactly this. And the reason is that you get constraints that are affecting the maximum you can achieve throughout the tree. So you get a constraint variational program instead of a variational program with only the last the last layer. So this was all done. So some version of it appears in work of Douvier and Kukukova. Work of Jovier and Kukova, and then Feng and myself, Mayar and myself, and for the case of Benching on the Moore Malin, it was a complete COVID now. We know it's an interesting model because the correction term is not the Bramson. There's a whole story to be told about that. I'm not going to tell you now. But the important thing is that when sigma is decreasing like it is here, the maximum is not determining. Not determining is just not the same as the maximum of I8 points of the same. Yes. This is one V or T. Sorry? This is one dimension. The branching is also 2D. 2D. So the branching, let me. So this is the branching. See? So that's a distance. The distance is 2D. You know, you can embed a tree, you can embed it in one dimension, you can embed it in two dimensions. So this is embedded. Dimensions. So this is embedded in two dimensions. Okay? So you have logarithmic distances are measured as the scale, if you wish. Okay, so that's the story, but that's not yet a theorem, but it's kind of hints on what. No, I should probably stop in five minutes, is that correct? Okay. Okay, so let me tell you a few steps, a few words in the proof. So the starting point, like many works in this business, is to first get rid, so we are trying to compute moments, so it immediately becomes a question about intersection local time. Except that now you have as the intersection local time a q which goes to infinity. Q, which goes to infinity. So it's not intersection local time of finitely many paths, but you have a large number of paths and you need to control that intersection locally. And in the previous works I quoted there are several approaches to this. There's also an operator-theoretic approach that was devised by the people of Wu and Jeremy and Psy that I mentioned. And psi that I mentioned. Somehow we could not quite use, we were inspired by some of these works, but we could not quite use them. So the idea is to do the following. First of all, you can rule out triple intersections. That's not too hard. So you can show that the random walks kind of intersect in pairs. And the picture you should have in mind. You should have in mind, and you can do it even when Q is large, at that scale. When Q is larger, you cannot do it. When Q is larger than this order of square root log N, that's not true. But it's kind of related to that critical. Sorry? It's related to that order critical. Yes, I think so. Although I'm not sure it is not true that the tails Okay, we can show that the tails are not the Gaussian. Are not the Gaussian tails when Q is not there. That's not to just compress everybody and force everybody to stay in the small room. We have the bound for square root log n. I'm not really sure what happens in between in terms of the upper bound. I don't know. The lower bound is all. We get a better lower bound when Q is bound. Okay. So the mental picture you should have in mind is something like that. You have your cue points. I'm going to draw only a few of them. And what happens is that they match in pairs, then they go together for a while. They kind of intersect together. So this picture already appears in earlier works. And then somehow they exchange partners. Somehow they exchange partners. So another guy comes from here, and these guys continue together, etc. So the main part of the proof is to really kind of not try to follow all single intersections here, but rather to follow the points where things split. Okay, when couplings split. So in order to talk about coupling, we want to first get rid of triple intersection. First, get rid of triple intersection, triple or more intersection. So, once you get rid of that, then you really want to do a kind of chaos expansion, so decompose the exponential of the intersection local time in terms of all sorts of transition probabilities and couple intersections. And the crucial point is this kind of picture. point is this kind of picture and the point is that when when points are matching someone from far away there's a big factor coming from the probability density of the big jump of the kind coming from far away to here and therefore uh yeah I'm not going to go over the details but uh a an important part An important part of the proof is to find a recursion scheme in which, it turns out, you have to distinguish between short jumps. I'm not giving a precise definition, but the picture should make it clear. This means that if you look at the times, those times A and B are the events of a match being broken and a match being set. Okay, so that's an example of a match being broken, because this is the last time. Because this is the last time these two guys are intersecting before this guy goes to greener pastures. And here is the beginning of the new patch. So you would decompose it according to that. And it turns out that the difference between when this is relatively short and when this is long, where new pair are really particles that came from far away. Particles that came from far away in the past. So, okay, I think that's all I want to say about this, about this idea. For the lower bound, here is a lower bound. Are you using some expansion of the exponential? So this is an expansion of the exponential. That's exactly what you just rearrange it. It's all powers, right? It's all powers, right? But you gain, okay, I should say, because of this beta n, beta n goes to zero. So you have something that is dumping high powers, api also. And of course, this has to be matched with a combinatorial expansion. If you do it too stupidly, it is not controlling the exponential, the combinatorial explosion. The issue is to control the combinatorial explosion. That's really the issue. So the point is that long, for long jumps, as in this picture, for long jumps, you get a gain from these arms because of the transition probability there. For the short jumps, you don't get that gain because here you did not move much, but you gain in the combinatorial explosion. So the whole game is. So, the whole game is to play on these two things. So, that's about the upper bound. For the lower bound, so this is really, if anyone was in my office in Brikesburg, this is how it looks. That's my blackboard. That's the first time the book was written just post-pandemic. And maybe what I should, the only thing I want to say about it. The only thing I want to say about it is really these are the blocks I was talking about before. So you have this LK and LK plus one, which are certain well-chosen scales. And we are going to look only at intersections in a window inside. And the point is that if we start from here, we are going to do a worst case on initial conditions, forget completely the information about where. The information about where all the work started, but we leave here enough a buffer that the particles can spread out a lot, or at least many of the particles can spread out a lot. Then you do the computation inside this window and then you repeat. And when you do this computation, you get what I told you. So essentially, you get, because we do a worst case. We do a worst case here, we essentially get a product structure, and the variance here is actually quite easy to compute. There are some technical issues in order to make this work, but those are better swept on those. Okay, I'll stop now. Thank you. We have time for a couple of questions. How close to the critical point of control would be relative to the ambition for question? So, what do you mean by critical point? There are two critical points here. One is the beta array. So, we did not try to take... So the lower bound is true for any beta art, if we call any fixed beta art. We did not try to now do... I forgot to mention, of course, there is a very interesting work. Very interesting work on rescaling better hat in the window around the critical issues. This we did not touch at all, but below it it works all the way. And what I should say is that we now have a better upper bound that actually matches what should be the probability. So it's close to this middle, but not exactly the good permit? So so as I said, we we never try to take better head function of n, which is what you are assuming, which is what you are saying. I think which is what you're asking. Not quite. And I suspect that for some, I mean, there is what our proof gives, and I'm sure that you can in there take a better head that goes, a better head that goes near enough to one to get the same result. This I'm sure. Whether it gets you to the critical window, I don't know. That's an interesting question. So just I want to understand the motivation. You're taking this endpoint distribution, question point distribution of the polymer, right? And raise the density to the square root of block power. Why did you do that? So for me, this is a random field. People have proved this local overlapping. So after you did this. So after you did this square of log n multiplication, you got yourself a log correlated field. And I'm for some time now on the search for a general theory of extremes for log core relative fields. So that's for me a good lab analytic. Even if you want to look at the extreme, like the mode of the distribution, right? Yeah, so for example, what is a leading order? What is a log? What is the law, I call the law of large numbers, let alone fluctuations? This will come, right? What is the order of the maximum? So the upper bound on exponential moments give you something, but it doesn't give the correct answer, as I now understand. For a long time I thought it would give the correct answer because all the experience in log correlated films that I've played with, that come up in nature, this is the case. You look at cover time, you look at the Time, you look at the random matrices, you look at almost any natural log correlated field, the extreme, the maximum, the sig points, the multifactor spectrum, all these things would be like in the Gaussian free field. Here I don't believe this is proof. I don't have yet the proof, but I'm sure it is proof. I'm now convinced just from this three picture, that this is simply not proof. And the explanation is simple why this is not proof. It's really that. It's really that you see, the fact that sigma decreases means that the variance in the beginning of the tree is larger. Now, the beginning of the tree here is the end of the pass. It's actually points that are far away from each other. Remember what is the translation between distance on the graph and the tree. So it's low correlation, meaning that points have started far away from each other. So they only have... So they only have a chance to intersect very much towards the end of your time interval. So somehow what happens in the beginning is completely irrelevant. It's not completely irrelevant, but it's less important than what happens at the end of time. And this is really the message I'm trying to convey. I strongly believe this is what happens in these polymers. You just want to understand the maximum of this power rate, the right method is to create this pipe power. Method is to create this pipe power. Yes, yes, that's always you need to get tails, right? It's a large deviation event, so you need to get tails. Getting tails is high voltage. There's no other way. And in fact, you have to split it in pieces. That's also part of the theory, but that's more or less what I explained here. So it's not enough to just take the exponential moments of the end point, it turns out. We have to take exponential moments of pieces. The fact that you have this only two-body interaction in terms of your pictures over there, I guess if you go to even higher moments or you go to beta one or bigger or you go to E equals one, you just take anything. No, just Q. Just take Q. Not a Q, not beta hat. Let's play with beta hat equal to one, because that's smaller than one. For beta hat larger than one, really what you should do is stay good. Push everything. Push everybody to stay in a small ball and they will just naturally intersect as crazy. Actually, but if you take very big data, then you get some combinatorial structure. Very bit cube. Very big cube. And you get some combinatorial structure on how they pair together. And this is also somehow related to the spectrum of the operators. So I don't have a good object possible. You look at that. It must That it must be. Must be because what I know in terms of lower bound is that if you take Q, which is log n, a strategy that beats the bound that you get from here is just to force everybody to stay beat. And that, of course, has to be related to the total ball of the essentially confine them not to a bone of radius one, but to the border of radius that depends on the sheet of m and then just takes it. And the end zen just makes a much more good. I don't know. Maybe we should discuss it. I'm not sure I see the rotation. Yes, but why spoil the Markov property? You have to understand it spoiled the computer, but for the Markov property, where they were is not available. Why would you need more information to track them? They came from far away, at a distance that is more or less a heat kernel distance, so it's not Heat kernel distance, so it's not. It's not, I mean, to do things beyond the heat kernel distance is very expensive, so that's not what I'm really coming from typical distances and just meet because there are so many of them, but some of them would meet. All right, maybe we can continue the discussion in the complicated. That's let's take two minutes and then we will. 