For inviting me. So, this talk is based off of a paper with joint with Max Geering and Christian Weiss. And the title on the archive is Families of Well Approximal Measures. So, if you're interested in looking this up, look up the top title. It exists on archive. So, I do want to do a quick plug. I am from the University of Washington, and Max Gearing is also from the University of Washington, and we will From the University of Washington, and we will be on the job market for fall 2021. So, thank you very much for allowing me to present some of my work. One thing that I got into discrepancy theory, I am actually from the world of translation services, dynamics, and ergodic theory. And actually, talking to Christian in Bonn got us into talking about discrepancy theory. And we also are working. And we also are working with Max Gehring. And Max Gehring's specialty is actually geometric measure theory and understanding measures, which makes him well suited for understanding different measures that aren't nice, like Lebesgue measure. So without further ado, I will start talking about our results. So I'm going to talk about, we have a dimension, one result. So we're going to talk about that as an introduction and kind of an inspiration for how we got into this problem. got into this problem. And then I'm going to pose an open, well, share someone else's open question for when our dimension is bigger than or equal to 2. And then I'll share what is our family of measures and what rates do we have for D bigger than or equal to 2. And we were inspired by this open question from the second section. And then the last thing I'll highlight is kind of some further directions, open questions. There were some times when we were trying to prove things and we decided to add extra. Things and we decided to add extra assumptions because none of us are combinatorial lists. And I know there are combinatorialists here. So if people are interested in combinatorial theory and interest in these types of problems, that could be really interesting. And then I also want to talk about changing metrics instead of star discrepancy, because that is kind of what got me interested in discrepancy in the first place. Okay, so to start, I want to talk about the star discrepancy just so that we're all on the same page. So the star Same page. So the star discrepancy, we're going to just take a difference between two probability measures, and it's given by the supremum over all half open axis parallel boxes in 0, 1 to the D with one vertex at the origin. So this is the star discrepancy. And the theorem I'm going to share, this seems to be old news more folklore. If someone has a specific attribution, that's great. I could not find one. So if lambda. So if lambda one is the Lebesgue measure on CR1, so we're in dimension one, and then we say, well, for all n in the integers, there exists a finite set, x1 through xn, so that the star discrepancy between lambda 1 and the measure, which gives equal weights to each point xi, is at most c over n, where c is independent of n. So, one thing I want to note. So, one thing I want to note is that if we switch to sequences, so instead of a finite set x1 through xn, if we want to have xi from i equals 1 to infinity for our approximation, then our upper bound becomes log n over n. And so there's this kind of general principle that if we have 1 over n for our set, then we add a power, we multiply by a power of log of log. By a power of log of n in the numerator to go to sequences, and this will come up again in higher dimensions. So, this is fairly well known, telling us that this is kind of our order of approximating the Lebesgue. And then we also know that this is an optimal order. So we know that for Lebesgue measure, for any finite set, our discrepancy is bigger than or equal to 1 over 2n. So, the question that I'm interested in saying. The question that I'm interested in saying is: We want to say what happens when we change the measure. So instead of working with Lebesgue measure, what happens if we work with arbitrary burbelled measures? So I'm going to call this renewed. So instead of old news, we're going to call this renewed news. And so this occurs in our paper. So if mu is a normalized Borel measure on 0, 1, and I'm going to have the Lebesgue decomposition on mu. So for those of you who may not remember the Lebesgue decomposition, this is Remember the Lebesgue decomposition. This is the absolutely continuous part. So there exists a function f which is continuous, so that the mu AC is absolutely continuous with respect to Lebesgue. This mu D is going to be the discrete part, so gives weights to point masses. And then this part is the continuous singular part, which does not give weight to point masses, but also doesn't give weight to But also doesn't give weight to Lebesgue sets of positive measure. Okay, so given a normalized Borel measure, we're going to have a Lebesgue decomposition for it. And no matter what our normalized Borel measure is, for every n, there exists a finite set so that the star discrepancy between our measure mu and the measure with equal weights is at most c over n with c independent of n. And you can do this similarly for sequence. And you can do this similarly for sequences. So, what we're really saying here is Lebesgue is, in some sense, the worst or hardest to approximate in dimension one. So, what we're saying here is no matter what probability measure, Borel measure, we have on 0, 1, we can't have discrepancy, which is worse than we have for Lebanon. Which is worse than we have for Lebesgue. And so, this is kind of the core question that we're going to be talking about, and that I'm really interested in: is what happens when you change your measures? The intuition that Lebesgue is the worst is Lebesgue is spread out over the whole interval and doesn't have gaps or anything. So we have something very equally spread out giving weight across the whole interval. So, if we want to approximate it, that should in theory be the hardest to approximate by a discrete set because we have to have things spread out. Because we have to have things spread out very, very well. So we know in dimension one, Lebesgue is the hardest to approximate. And then we were also able to show that as long as we don't have discrete components, so we add an assumption here. So as long as we don't have discrete components, then the star discrepancy is bigger than or equal to 1 over 2n. And so why do we need this discrete component here? Well, the discrete component, if we have the trivial measure, component, if we have the trivial measure, which just puts weight at one half, then our star discrepancy can always be zero. And so not bigger than one over two n. So as long as we don't have discrete components, we know what happens with the measure. So the reason I call this renewed news instead of just a theorem is that this generalized ideas of Waka and Muk from 1972. And they didn't really state this. They have some of the ideas here. They do have an extra assumption. Here. They do have an extra assumption on the absolutely continuous part. They have a Lipschitz continuity assumption, which we don't need. So we did generalize it, but they also just didn't state this because they had different goals in mind. Okay, so this is dimension one. I think for a lot of experts in the field, this is not as surprising, but I still think it's really nice to have a base case where we know exactly what's happening and we have it very clearly stated. Okay, so this leads us to our. Okay, so this leads us to our open question. So the open question was posed by Eisleitner, Billick, and Nikolov in 2017. Sorry if I can't pronounce your last names with my American accent. So for D bigger than or equal to 1, there exists a constant C D so that for all n bigger than or equal to 2 and for any Borel measure, the discrepancy between the Borel measure and giving equal weights to Giving equal weights to our finite approximating set is at most C D log of n to the d minus one half over n. So this is an actually open question. This is the theorem. So this is the theorem in their paper. There we go. Theorem. So this is the theorem in their paper. And one thing I want to notice is so we have this log of n to the d minus one half and if we switch from seven If we switch from sets to sequences, then this would become a d plus one-half. So we have these points, we have this discrepancy, and then the open question posed in the same paper was, does there exist a Gorel measure which is worse than Lebesgue? So this, we notice our exponent here is d minus one half, but for Lebesgue, the power is actually d minus one. Is actually d minus 1. So for up the power for Lebesgue, we want it to be d minus 1. So the question is: does there exist a Borel measure which is somehow worse than Lebesgue measure? So if there exists a Borel measure that's somehow worse than Lebesgue measure, that's really interesting because our intuition is Lebesgue is the kind of most evenly distributed and will be the hardest to approximate. So that would be really interesting. Also, if not, then that shows that the proof structure Then that shows that the proof strategy in this theorem has some gap that doesn't give us the d minus one upper bound, so then we could work on strengthening that. So I think this is a really interesting open question. I was really excited when I read this. So the question is, okay, so D minus one is the upper bound for Lebesgue, and we want to know, are there measures which are worse than Lebesgue? Are there measures, we know there are measures which are strictly better than Lebesgue, and the question is, how do we know what's going on? How do we know what's going on? So, this is the question that inspired us for our project. And then, as a reminder, I just want to note that what we started with was we read this and we said, well, what happens with d equals one? And we said, well, when d equals one, no such mu exists. We know that the Lebesgue is the hardest to approximate. So, now our response to the open question. So, what we did is we The open question. So, what we did is we constructed a family of measures. And so, you know, you got to start somewhere with an open question. We definitely aren't anywhere close to solving an open question, but I think it's just good to have ideas and know what's happening when we are trying to approximate by different measures. So for d bigger than or equal to one, we do have a family of discrete uniform Borel measures on zero, one to the D, where the discrepancy is less than or equal to log of n over n. equal to log of n over n. So one thing to note is this is independent of the dimension d. So we know that we can do, we can definitely have at least better approximations. And the proof that we actually use, so this was mentioned in the paper that there's different proof strategies. And one proof strategy is to use the total variation metric. Total variation metric because the star discrepancy is always smaller than the total variation metric. So we actually use the total variation metric and our sufficient assumptions on the family of measures. So our measures mu, the reason I say this is sufficient is because our theorem is stated actually more generally than this, but this is easier to understand and most connected to this open question. So I just wanted to phrase it this. So I just wanted to phrase it this way so that you can understand. If you're interested in the paper, we do have this statement as well as the actually full general statement of our theorem. But in order to make the connection and have a nice story flow, this I think helps a lot. So these are sufficient assumptions on our family of measures. So let's have mu be the sum of alpha j delta yj. So our alpha j are our weights. And what we're going to do is we're going to say, well, let's assume that our alpha assume that our alpha j's have a sufficient decay rate. And so we have zero is between r and one and our constant c here actually is a function of r and the c sub r is going to be increasing in r. So the idea is as r gets closer and closer to one, then our measure could be closer and closer to a uniform measure, which means it could be Measure, which means it could be closer to Lebesgue, which means it would be even harder and harder to approximate. So, what we're doing is we're saying under sufficient decay rates on our discrete set on the weights of our measure, then we have that we know the discrepancy is at most log of n over n. So, the proof idea for this theorem has two main components. Theorem has two main components. So, the first component is what we're going to do is we're going to approximate our measure μ by measured supported on finite set using the decay rate on the tail. So, once we have a sufficient decay rate, what we're doing is we're saying, well, it suffices to consider measures on finite sets. And then we can explicitly construct x1 through xn, which approximate our given finitely supported measures. Finitely supported measures. So, this is our theorem. I think it's very nice and it is inspired by the open question. And we have a lot more work to do, but I think this is a great first step. And then I just wanted to talk about in my last few minutes, I just wanted to share what these ideas, what got me into this, and what are further directions we could do. So, one thing is, So, one thing is we have sufficient assumptions on the tail of our measure. And because we have these assumptions on the tail is how we can get our approximations to be independent of the dimension D. So, one thing that happened is we said, well, if we don't have these assumptions, our proof strategy started to involve combinatorial methods. And neither of the three of us collaborators are experts in combinatorial methods. In combinatorial methods. So, we do think there is a direction you could go and get improved information on this using combinatorial methods for the larger families of discrete measures. The other thing I want to advertise, which is how I got into an interest in discrepancy theory, is there was a presentation at the University of Washington by Stefan Steinerberger, who By Stefan Steinerberger, who he's actually a professor there now. And so he has a theorem in 2018, and this comes from the Vasserstein metric. So the Vasserstein metric is very popular in optimal transport. And so this is optimal transport is this W2 here is called the Wasser sine 2 distance and it's called the movers metric and it says well if you have mass associated to mass associated to your to your like probability measure like how much what's the cost to move the mass to the other probability distribution so um one interesting application that i really liked and was inspired through dynamical systems is um if alpha is a badly approximate number then if we look at our vasserstein distance between lebesgue and Between Lebesgue and this 1 over n. This is the Kronecker sequence. So we're going to take rotations on the circle, n alpha mod 1. And then this is less than or equal to C alpha log n to the 1 half over n. So in particular, this 1 half, the Wasserstein is strictly smaller than the star discrepancy. So the star discrepancy we have. Discrepancy. Because so the star discrepancy, we have this log n, but for the Foster sign distance, we have actually a log n to the one-half power. So one thing I think is really would be interesting is understanding. So we have this discrepancy metric, but we also have this Basserstein metric, which could be very, has a lot of applications and interest in the study of optimal transport, which is a very hot topic right now. So then one thing I would be So, then one thing I would be interested in understanding is what happens if we try to understand what are optimal sequences for the optimal transport for the Vosserstein metric. How does this relate to the discrepancy? And can we have sequences, does the Vosserstein metric sometimes have better discrepancy bounds than for the discrepancy, star discrepancy? So, these are two questions that I'm very interested in. Yeah. In. Yeah, so that's everything I had to share. Thank you, Samantha. Let's see. Again, if you have a question, please either raise your hand or you can also use the chat. Sorry, let me. So many things to click. Let me bring the participant list. List. Sorry if you mentioned it. Let me start by asking a question while people are thinking. I actually have a few things in mind, but okay, so one question is: is there any sense in which your bounce with this log n over n is the right answer here? Is this like in some sense a tight estimate? We haven't proven that this is a tight estimate. Maybe for this family of some family of measures that includes this family of measures. Yeah, so we do have family of, so we so we have examples of family of measures. I'm trying to remember. I don't want to lie to you. So we do have these, so we do have these so we um what we do in our oh sorry I think I lost it so what we do in our family of measures is we do have examples where when we take this R if this R gets close to zero we prove that our discrepancy we can do strictly better and as R goes to one we show that the discrepancy can't do better but we don't have explicit like I don't think we we don't have like a family where we have explicit like lower bounds on the discrepancy We would have explicit like lower bounds on the discrepancy. We just show that, well, for these families, we can't do better because as r approaches one, we would approach the Lebesgue measure this more uniform, and so then we can't have this log n over n. Was the infinity of Asterstein distance considered in this context? No, so as so as I think. Um, so as I think, uh, Stefan Steinerberger, so this paper where he does the Basser-Sein 2 distance, he just has this one application, and he just states as an open problem, there's a lot more to do here. Um, I do not believe he does have one other paper about approximations with the Vasserstein metric, but I think he again was focusing on the Wasserstein two metric as opposed to the infinity Vasserstein distance. So, I do think that could be studied. I do think that could be studied and interesting. I think I'm asking you guys a question. Yeah. I was just curious. So the results that you were finding were on these discrete measures. Did you find, were there, did very different things happen if you considered measures that had parts that were absolutely continuous with respect to the Lebesgue measure? With respect to the Lebesgue measure, or what was the other one? It was singularly continuous or continuous. So we don't have it. So here's the big problem. And I want to give a shout out to Max Gearing because Christian and I were saying, how bad can the measures be? And Max Geering, who is a measure theorist, was like, they can be really bad. So it was a very fun collaboration, just being like, the measures can't be that bad. And then constructing terrible examples where they really can be that bad. So we don't have this. That's bad. So we don't have this Lebesgue decomposition in dimensions higher than one. Because what happens is, so you can have this like Lebesgue portion in like the dimension two part, but then you can also have like Lebesgue pieces in dimension one parts. So and you can decompose into like canter measures, which are like all over the space. So I think the big issue, and so this is. Issue, and so this is the thing we ran into: is we don't have a nice classification, and because as soon as you're above one dimension, you can put sets in anywhere with Lebesgue measures and lower dimensional measures that aren't just discrete. So you have to deal with two-dimensional, one-dimensional, and then anything in between, which we had no tools for. So, yeah, that's the big. So, yeah, that's the big conundrum. And the reason we ended up sticking to discrete is because things could get really bad. So, we just don't know what to do with that yet. Thanks. Oh, and Stefan Steinerberger. Oh, great. You're here. You know this stuff. Great. So, Cogram proved for WP 2 less than P less than infinity that the same result does hold. Okay. And also for P equals infinity. So. P equals a 50. So, yeah, there has been some work done, but I do think there's still a lot more work that can be. Nicole, ask one quick question also for transportation. Has this also been studied in higher dimensions? And is there anything known? I think that's the paper I was thinking of. Stefan, do you want to give information on that? Yeah, maybe. Right. So, my talk on this was supposed to stop in. My talk on this was supposed to start in 20 minutes, but because of this conference, it's moved back in two hours by two hours. And yes, there's a lot of results and a lot more that are missing. And essentially, what we have is we have Kronecker and we have optimal rates. And what's interesting in Wasserstein and high dimensions is you don't have to lock that you get for discrepancy. But there's some very interesting other problems that sort of start coming into play. Okay, okay. We should just have more talks about this. Interesting. Thanks. Thank you. Okay. Thank you. Okay. Let me sorry, let me just stop the recording. And thank you, Samantha.