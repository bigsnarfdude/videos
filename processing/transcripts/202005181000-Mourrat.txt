We still have a stream of people coming in, and I assume this will continue for a while, but we may as well get started. So, thank you all for joining us. It's nice to see a handful of familiar faces and nice to see a lot of unfamiliar faces and names on the participant list. So before a few words of thanks to the various people and things who made this possible. So first of all the coronavirus which is the main reason and not in other summer schools and a word of thanks for the organizing committee apart which apart from me includes Luigi Adario Berrio, Ed Perkins, Luigi Perkins, Luig Leopopovich and Alex Freiberg, Mattham Morgan and also Sarai Hernandez Torres and Thomas Hughes. And in a way it was Sarai's original idea when we realized we couldn't have the Vancouver Summer School to try to move various activities online. So thanks to all of the So, thanks to all of those. Now, as you can see, this week we have a course by Jean-Christophe Mourat. Next week, there is a course by Gaddy Cosma, and we will continue later on down. So, you can see on the website the list of future courses that we will have. A quick word about logistics. So, we have the chat here in Zoom. Chat here in Zoom, and you can ask questions in chat. So, most of you cannot unmute yourself. But if you ask a question on the chat, we will either unmute you or allow you to, or one of the moderators will ask the question to the speaker. So, please do try to ask whenever anything is unclear. Secondly, we have the Zulip forum, and I see a lot of people are online there as well. So, the Zulip forum. As well. So the Zulip form, I will also be monitoring that during the talk. If there are questions there, they will be flagged for Jean-Christophe. And if there is anything... And the advantage of the Zulipoam is that it remains active also after the meeting. And this can be used also to discuss various questions that arise from the meeting or from problems that might be assigned in the meeting with the meeting. In the with the meeting, so uh, the Zulip form is there for you all all the time. Um, okay, so I think that's a few words about the procedures. So, I will pass you to Luigi now to introduce our speaker and we can get started. So, again, welcome everyone. Thank you, Luigi. Okay, thank you very much. I'm having my own technical difficulties at this end, unfortunately. So, if my signal is weak, I apologize in advance for that. So, I'll just briefly go over the plan. So, we'll have roughly a one-hour lecture, which our plan is to divide in half with a short Divide in half with a short question period and break after half an hour, followed by the second half. We'll then stop the recording and have a second question period, after which participants who would like have the option of joining breakout rooms to introduce themselves to one another in smaller groups and discuss the lecture. We'll ask our lecturer to stay in the main room at that point. So, participants who prefer to stay in the main room and ask him questions directly can do so. him questions directly can do so. Let me also remind you that this lecture is being recorded and streamed to the web and will be made available on the web after the lecture. So if you don't wish to be recorded, it's best to turn your video off and mute yourself. One further point in order to try to stimulate a bit of Try to stimulate a bit of conversation and dialogue, we're going to select some random participants and unmute those participants, or rather give them the power to unmute themselves. So if you discover that you've been made a co-host, that means you've been randomly chosen. And we, in that case, you're welcome to ask questions during the lecture. Having co-host privileges also. Having co-host privileges also means you can do things like kick people out of the meeting accidentally and whatnot. So please be careful if you've been given such privileges. This is a bit of a trial run. We'll see how it works. Hopefully it'll foster more of a dialogue and a bit of a feeling of interactivity in the lecture. Okay, so with that, it's my pleasure to introduce Jean-Christophe Murat from New York University. From New York University. John Christophe received his PhD in 2010 under the supervision of Pierre Mathieu and Alejandro Ramirez. He then spent three years at EPFL in Lausanne, followed by six years as a research scientist with the CNRS, based in first Ernest de Lyon and then Ernest Paris. In 2019, he moved to the Courant Institute of Mathematical Sciences at NYU, where he's currently an associate professor. Associate Professor. So, JC, it's a real pleasure to have you launch OOPS with a mini course of three lectures on rank one matrix estimation and Hamilton Jacobi equations. So I'll now invite you to share your tablet and start your lectures. Thank you. Thank you very much. So let me try to share with my screen. It's a great pleasure to be here with you guys. I'm really I'm really excited to make this experiment of this online summer school. Yeah, I hope this will be a successful experiment. I hope we'll have good conversations about the topic. I changed the title a little bit, but it's the same. It's just that I will speak about my motivations, which are a bit broad, so I slightly rewarded the title, but it's really what was announced. What was announced. Yeah, so thanks to the organizers in particular for making this possible. So I hope that the people who became co-hosts are aware of this. And yeah, let me say one more thing, which is for those who can unmute themselves directly, please feel free to interrupt me because there's no other way for me to know you want to speak right here. You cannot, like, it's hard. You cannot, like, it's hard for me to see if you're raising your hand or something. So, so just, you know, just stop me. That's it. All right. The sound is good. You hear me okay? Yes. Okay. And yeah, I'll try to, so for everyone, also, you can ask questions in the chat, as was said. I'll try to look at it, but also I hope Rigi will help me or other people will help me. Will help me, or other people will help me to see what's going on in the chat if something happens and I forget to look. All right, so let's start. So, yeah, I want to spend a bit of time describing some of my motivations, which are a bit beyond or maybe different from the main focus of the talk. And to give a little story around the motivation, we can imagine that it's like in the Harry Potter series where there are students who arrive at Hogwarts. And you know, when they arrive, they are supposed to be assigned dormitories. And in the story, there are four dormitories, but for simplicity, I'll just assume there are two dormitories. And the students are supposed to put this sorting hat on their head, and then the hat decides where. Decides where each student goes into, you know, into which dormitory each student is supposed to go. So, I'm going to, you know, start from this and make a very simple model of that situation and see where it leads us. So, let's say that we have these students which are so there are capital N students which are labelled from one to N. Okay. Th these are their names if you want. Okay, these are their names if you want. And let's say there are two dorms, two dormitories. And an assignment of students to dorm will be will be encoded by a vector of plus one and minus one. Okay, so for instance, if the vector starts with a plus one, it means that student number one was assigned the dormitory plus one, and otherwise it's the dormitory. Plus one, and otherwise, it's the dormitory minus one. And the magic sorting heart is trying to optimize the quality of the assignment. And in the model I'm describing, we can imagine that there is a parameter that describes the quality of the interaction between students i and j. Capital J ij. Okay, and again, this is a, you know, it's a toy, it's a toy problem, it's a simplified problem. So I'll just assume that these random variables are independent standard Gaussians. And we are good people, so we want to maximize the quality of the interaction. So, by quality of the interaction, I mean, if J and J is positive and very large, it means it's really, you know, individuals I and J really like to be together, and if it's negative and very large, it means they really dislike to be together. And so, what we would like is to maximize The total sum of the interactions in each of the dorms that we are trying to form. So it's the mapping that to each assignment sigma associates the resulting total interaction. So is the sum of j i j indicator of the students i and j are in the same dorm, which here is encoded by the indicator that sigma i is equal to sigma j. is equal to sigma j. And so, you know, one question you can start to ask before even trying to identify the optimal sigma is what is the behavior of the maximum of this function? So what is max over sigma? How does it behave when n becomes very large? This is the first question I want to ask. And to be more consistent with statistical mechanics standard setting, instead of writing this indicator function of sigma is equal to sigma j, which is here, I'm going to write the product of sigma sigma j. If you think about it, it's just a constant shift between the two quantities. So it's okay to... It's okay to to do this change. So I want to understand this as n becomes large. I want to understand the asymptotic behavior. And so for those of you who are familiar with the easing model, the easing model is a bit sort of the same. We have a sum of sigma i, sigma j, except these capital J's, they are all equal to one. And in this case, you know, for the easing model, it's very easy to Model is very easy to figure out what is the best possible configuration. Will we just have every sigma to be equal to plus one or every sigma to be equal to minus one? But here, because the j's are disordered, because some are positive and some are negative, it's not obvious what is the best configuration. If you have three individuals, let's say i, j, and k, and i wants to be with j, j wants to be with k, but k really doesn't want to be with i, there's no way for you to make each There's no way for you to make each pair maximally satisfied, right? So we could say that there are frustrations in this problem. And this is really a signature of what physicists call glasses. So these models are often called spin glasses in the literature. The important point is this aspect of frustrations. In other words, it is difficult to find the optimal confidence. In other words, it is difficult to find the optimal configuration. And if you think of it as trying to do an optimization procedure where you try to adjust each sigma one after the other, then sometimes maybe to find a better configuration, you will have to first make a move that looks detrimental, and then maybe another one, and then ultimately it will improve the thing. But it's difficult to find the optimum. I was looking at the chat. So far, so good. Looking at the chat. So far, so good. People in the chat, you're happy? Yeah, there have been any questions aside from technical issues. Awesome. Okay. Yeah, I suddenly realized that there was a big stream of messages which I had not seen. Yeah, so the maximum is an interesting question, but I want to relax it and also look at the question. Question, yeah. So, so I don't, yeah, so is the JIJ symmetric? I don't assume it to be so, but um, because it's more convenient for analysis, but uh, in fact, it only depends on the symmetric part of this matrix of the j's. So, I don't assume it, but it's not crucial. Thanks for the question. Yeah, so I want to consider a softer version of the maximum also, which for people who have done syscall mechanics before will be very. Mechanics before will be very natural because it's like I want to look at the Gibbs measure associated with this function. So, for instance, I want to look at so okay, I put a normalization here and for now you'll have to believe me that this is a reasonable normalization. So, I put a parameter beta and then this one over square of n and I'm going to sum it over all sigma. sigma and you see that if beta is very large then this sum is essentially carried by by the maximum so if then i take the logarithm then you know in the regime of very large beta this essentially is like the max but then you can pay with beta and it's like a relaxation of your problem issue so i did not explain this but the way i normalized it um we we should expect that what's inside the exponential is of order n. Inside the exponential is of order n. So if I want the thing to stabilize as n becomes very large, I should divide by n. And finally, this quantity is still random, you still depend on these j's, but the fluctuations are not very large. So, you know, you should not worry about this. And just to make it to kill them, I'll take the expectation with respect to the j's. And so now, okay, this is associated with the model of statical mechanics, which was. With the model of statical mechanics, which was introduced by Sherrington and Kirpatrick a long time ago, and it was not related to Harry Potter initially. And it was initially not very clear what the limit is. And ultimately, Parisi in the late 70s proposed an answer for what this becomes when n goes to infinity. And I'm not going to write it because. And I'm not going to write it because actually it's a fairly complicated formula. Okay, oops. Sorry. So, okay, I mean, you know, the main emphasis of the course is not on this model specifically, so I don't want to spend too much time. But still, I want to mention this because for me, this is also an important part of my motivation. Like, there is this formula, which for me is very difficult to. For me, it is very difficult to build an intuition about. And also, I like it very much. I find it mysterious in some sense because the expression on the left side, okay, it took me, it's kind of long to write, but deep down, I find it very simple. You have this function of sum of j, j, sigma, sigma j. It's very simple. And basically, I want to look at the maximum, or maybe some relaxed version of this. Relaxed version of this. You know, if you think about a disordered high-dimensional problem, it's hard to think of something simpler. And yet, the right-hand side is very complicated and reveals a very bizarre structure. And I find it very difficult to understand what's the relation. So I thought it would be interesting to think about it more. But yeah, let's just give some references. So this formula was. So, this formula was proposed by Parisi in the late 70s, and progressively, physicists became more and more convinced it was correct over the course of time. And then Guerra and Talagron, each for one bound, proved it rigorously. Okay, so in fact I like to think about it. I I find it an interesting problem, but I want to stress that um it's already proved. It's already proved. And part of the reason I find it mysterious is: oh, yes, yes, sorry, I forgot to mention the sum inside the exponential is over the i and j between 1 and n. Yes, thanks for the question. Yeah, so I think for me what really revealed the fact that I don't understand this formula is that I wanted to I wanted to think about a slight variation of the problem where instead of having connections between each of these i and j's, I was imagining maybe, you know, what about these things are organized in two layers. So you would have a situation like this. Okay, these are the, I draw circles for the eyes and j's, the eyes, you know, it's like from one to n, but You know, it's like from one to n, but now the maybe it's from one to n over two is the top layer, and the rest at the bottom layer. And then, what you know, what happens if we only have interactions between the two layers like this, but not but not interactions within the layer, okay, no, no interactions like this. And it seems a relatively innocent modification, but But it so happened that I could not understand what I was supposed to write in place of the complicated formula which I did not describe. And so, yeah, I found that was a good sign that I actually should think more about this problem. And so I will stop speaking about this model. So it's called the spin glass model. And in case you want to hear more about it, there is a talk I gave at the one world. The talk I gave at the One World Property Seminar on this. So now that was part of the story of the motivation, but now I'm going to try to move to the problem that I actually advertised in the abstract, which is rank one matrix estimation. Or maybe I should say. Or maybe I should say matrix inference. So the setting is that of a statistical question, if you want. You're trying to observe something, and unfortunately, you only observe a noisy version of it. So we observe. A noisy version. A noisy version of the Rank one matrix. And the question is, is can we recover information about this rank one matrix? So you should imagine it's a very, very large matrix. And so, you know, I don't claim it's a. I don't know if the setting is actually useful in the setting I'm going to describe, but sometimes it's useful to have a concrete instantation in mind when we think about the problem. So if you want to think about some concrete setting, you can think that you are Netflix and you're trying to understand how to make recommendations for your customers. And so you have a bunch of customers. Of customers, and maybe in a simple model of the situation, you could assume that a movie is only, you know, like whether or not a person will like a movie will be captured by a few parameters that describe the movie and a few parameters that describe the customer. For instance, the movie, maybe whether or not it has more action or more introspection, or maybe is it sad or joyful? Maybe there are. Sad or joyful, you know. Maybe there are two or three aspects that you care about, and maybe there are two or three aspects of the customer that describe how it reacts to how they react to the characteristics of the movie. And ultimately, how the person likes the movie is some linear combination of these parameters. So then you will see that I'm trying to build a very big matrix, but which has a very low rank. It's only a few parameters that vary. A few parameters that vary, and so as an extreme simplification of this problem, you can assume it's just a matrix of rank one. So, I'm going to describe this setting with rank one, but it's not difficult to generalize it to higher rank problem. Okay, and maybe another setting at which you can think is if you try to do community detection problems. So, for instance, in the US, For instance, in the US, the situation is the political situation is relatively polarized. So maybe if you want to guess whether or not two people will be friends, there is a binary variable which if you know it, it will indicate, you know, it will influence a lot whether or not these two people will be friends. And so maybe you observe the link of connections and you want to recover these communities. And if you think about it, it's also some sort of instance of a Some sort of instance of you're observing a noisy version of a rank one matrix. So you can imagine that each person has this binary variable attached to them. And then whether or not two people will want to be friends, will be very influenced by the product of each of these variables for each one. Okay, so these are two settings that you can think about. Let's say I will not come back to these more applied aspects later. More applied aspects later, but just to give you motivation. All right, and the common thread between these two problems is that, I mean, the thread that I want to emphasize in this lecture is their relation with certain partial differential equations, which are called Hamilton-Sacropi equations. Uncheckable equations. So, yeah, I don't know if everyone connected to Zulib, but as I said in Zulib, I don't assume that you know these PDs beforehand, right? Like the basically the point of the lecture is to get familiar with how they work and how we can use some techniques that are inside this context to solve this rank-quant matrix estimation problem. Okay, so I think. Problem. Okay, so I find it interesting, this point of view about partial differential equations, because, well, I think, first of all, it allows us, or at least it allows me to better understand the nature of the difficulties that arise. And also, if you come with this intuition, you know, this intuition comes with a lot of tools and techniques and ideas that were devised for studying these equations. These equations. So it's like you, you know, if you say this keyword, basically it comes with a toolbox, right? And then you can see what's in this toolbox. And maybe some of the tools are useful. And maybe it's also, I'm not sure about that, but for the spin glass model, it's possible that in some cases, at least right now, I find it possible that in some cases it's maybe unavoidable to to speak about these equations. Avoidable to speak about these equations. I don't know. All right, so I see some more messages in the chat. Okay, that sounds good. So the rest of the lecture will be divided into two parts. And the first part of the lecture will be on the QEVice model. And the Curievice model is really an extremely simple model. And it can be solved in a solve in a very large in really many many different ways but I want to emphasize a particular way to solve it that uses this intuition with Hamilton-Jacques Cuba equations and and I want to spend really a decent amount of time really doing it well the point of this is that next when we're going to turn to the problem of rank one matrix estimation the proof will be really almost the same Proof will be really almost the same. So, you know, maybe you're not passionate about the Curieby's model because you think it's too simple. But in case you're in this category, please consider that ultimately the proof for the real problem I want to solve in this lecture will be solved using exactly the same technique. So I think it's important that we understand well how to go. So yeah, in the in the so the way I'm going to So, the way I'm going to try to understand the Curievax model is not standard. And if you want to see a more standard way to solve it, I recommend the book of Friedli and Velenik, which I also someone gave a link already in the Zulip chat. So you can have a look there. And maybe it's a good time to take a few questions and then maybe also have a little break, and then I move to this first point. So are there questions on these kind of motivations? So the reference was Friedli and Velenik. I'm typing it. The title of the book is Statistical Mechanics of Lattice Systems. It's available on the internet. time and also it's uh yes thanks for the for the link it's also a good introduction if you want to you know know why why statistical mechanics uh do things in these ways like why do we look at these measures or things like that um so okay i'm not sure exactly how to interpret this question about motivations but i'll try Motivations, but I'll try. So, for instance, I'm not sure if I will have time to fully explain this, but one question we would like to ask, for instance, is can you actually recover meaningful information on your rank one matrix? And this will depend on the strength of the node. But it will depend in a way which is quite striking. If you're already familiar with the Isig model, you know that there is a phase transition. You know that there is a phase transition between this disordered phase when there is like no organization in the magnets and this organized phase where there is a magnetization happening. And for this inference problem, there's also a phase transition of this sort. Sorry, which means that, so in this context, when the signal-to-noise ratio is too weak, you will not. week you will not be able to recover meaningful information on the rank one matrix i will call you the signal okay so you will not be so there will be a regime of signal to noise ratio which in which basically you you cannot recover information on on the signal and then after a threshold suddenly you you start to be able to to recover at least partial information on the signal And if you want to understand that aspect, the study that we're going to do essentially will give that. I don't know if it answered the question. Does that satisfy you, Oliver? Yeah, so Adam asks: Can we fix the number of plus ones and minus ones? I think it's possible, yes. In general, In general, you could even imagine completely changing the reference measure. So, here it's the way I presented it. You could think of this sum over all sigmas. It's a bit like averaging over a Bernoulli measure of plus one, minus one with probability one half each. But you could change this reference measure by something. You could change this reference measure by something else, right? You could, I don't know, for instance, it could be maybe you're not only interested in sigmas that are valued in the set plus one, minus one. Maybe they could be uniformly distributed and you still want to understand some of JJC minus imaging. And yeah, again, okay, so I think you can, I mean, now that I understand this Hamilton-Jacobi equation point of view, I Point of view. I can see how you could also do it with the other point of view as well. But for me, it was useful to understand how to generalize it to arbitrary measures. And okay, so the question you're asking is if we fix the number of plus ones, but this can also be encoded as changing the reference measure, except that in the examples I was giving, there will be product measure on each coordinate. And here you're imagining another type of constraint. Another type of constraint, which is you would want to look at the uniform measure over all configurations that have the same number of plus ones and minus ones. But yeah, so yeah, I gave a very long answer, but long story short, I think you can change this. And the way I would understand this is this amounts to changing the reference measure for our plus one, minus one set. Sorry, I should try to give a shorter answer in the future. Yes, so okay, so there are two questions. The second one is a bit easier, so I'll start with the second one. So someone says that it's not clear what's the connection between the rank one estimation problem and the spin dot problem. It's a bit normal, but we But we'll see the connection very rapidly when we start to study it. It's like when you will want to study the law of our signal conditionally on the observation. And this, we will be able to write it as some sort of Gibbs measure of some form. And the form will be fairly similar to the one. To the one of the spin glass case. It will involve some sum of some noise matrix times C by C my J type. So yes, okay, that's my answer. The parameter beta in my free energy. Yes, so let me show this. So yeah, someone asked what's this parameter beta? So in the statistical mechanics understanding, Mechanics understanding. This could be understood as the inverse temperature. And in more pragmatic terms, it's this parameter that allows you to bridge between, you know, if you set beta equals zero, then there is no complication. You're just summing over this reference measure, like this. I like to think of this as this product of Bernoulli. So this you understand perfectly well. Maybe if you care about the maximum, you really want to insist on. Maximum, you really want to insist on sending this beta to plus infinity, and in between, you know, there is some more flexibility. And yeah, and inside scale physics language, you know, beta very small is a very high temperature, and beta very large is very low temperature. All right, so All right, so I'm kind of. I'm not completely sure now. How much time do I have left? Oh, yeah, time for a three-minute break. Yeah, I think that's Reggie suggesting a three-minute break. I think that's a good idea. So let's take a three-minute break. Let me just comment that on Zulip so we are all new to this system, but if you don't see the stream, you need to subscribe to the stream. So there is a link for to the stream for this course posted both in the general stream to which everyone is subscribed. Stream to which everyone is subscribed. And also, if you scroll up the chat here, there is a link to that. So we'll try to figure out later a way to have everyone subscribe to the stream so that you can see it. And the reference to the book is also there now. All right, so maybe I should restart. Is that fine? Thanks, K. Awesome. Awesome. All right, so now for this first part, as I said, we are going to study this QEVICE model. So let me define it. We want to study the probability measure that to reach configuration sigma. Sigma in this so I write I write this plus plus minus one I mean you know plus one comma minus one to the power of n associates a weight proportional to exponential of of T over N sum of sigma i sigma j so you can think of it as I have set all these j couplings to be equal to one okay and then we will see in a second but the normalization with one over n is reasonable and for some reason I like to change beta and call it t Beta and call it t, which is a little bit confusing because here this little t is really an inverse temperature, not a temperature, but okay, we'll have to deal with that. So, if people are familiar with the standard Ising model, you know, the Ising model, you can think of it on any graph if you want, and this would be the Ising model on the complete graph. And I also want to add an extra term, which is a magnetic field, so it's a one. So it's a one, there's no interaction on this one, it's just going to be a tilt on the sigmas, if you want. It kind of encourages the sigmas to be on a particular position. So I want to study the probability measure, which to e-configuration associates the probability, which is some constant times the factor which is written here. So the notation I will use is with I will use is with this bracket. So if I write f of sigma like this with these brackets, so it should depend on the choice of these parameters t and h. I will often forget to write these indices, but okay. So it's the sum over sigma of f of sigma times the exponential of that thing above, okay, which I don't. that that thing above okay which i i don't rewrite i just write dot dot dot dividing divided by the normalization parameter so the sum of exponential dot dot dot okay so is the definition clear this is a uh not necessarily yeah so so the question is um is there a sign for for t and h and uh no here there can be any yeah think of t as being a positive sorry yeah t is positive but uh or non-negative but h can be positive or negative H can be positive or negative. I also allow for, you can insist that you want the sigmas to be negative if you want. So if h is positive, it will encourage sigmas to be plus one. And if h is negative, it will encourage the sigmas to be minus one. So yeah, so t is positive and h is a read parameter. Thanks for the question. Okay. The the definition of the probability measure is is clear. Definition of the probability measure is clear. So I want to understand: is there an interaction in the first step? So, yeah, so this is, yeah, I have a bad habit of not writing the indices. So, this is the sum of i and j between 1 and n. So, if you want, there are interactions in this thing, because if you want to know, you know, like sigma i is talking to sigma j in the sum, you know, like this quantity inside the exponential depends. Depends on not just on summing what each of the sigma i's is doing, but on the product of them. That's what I call interaction. So, you know, by contrast, this sum over sigma i is not with no interaction. It's just a stupid sum of sigma i's. Basically, I just mean it's a linear sum, basically, while the other one is not a linear sum. All right. All right. So the thing I want to study first, instead of focusing on this measure, is the following quantity. So basically, I want to study the normalization constant in this ratio. And just like I'm going to give it a... There was a question in the chat about the sine of T and H, whether you answered that question. Yeah, T is positive and H is a real. Is positive and H is a real number. Okay. Thanks for, but thanks for, yeah, it's good that you checked because I try to look at the chat, but sometimes it's and another question whether the sum is over ordered or unordered pairs. Yeah, so I sum over all pairs. So that does not answer the question. I mean, yeah, there are n squared terms in the sum. I guess that's the. I think any other way I try to answer it will be, will confuse myself. So yeah, there are n squared terms in the sum. All right, so I want to say this quantity. And perhaps you think Perhaps you think, so you see, it's just the normalization factor in the definition of the measure. And then I expect that this thing in the exponential will be of order n. So I take the log and I divide by n, and I'll just give it a name. And perhaps your first reaction to this is this seems to be missing the point. I don't want to study the normalization constant. I want to study the Constant, I will study the measure. We don't care about the normalization constant. But I think it's not the this is a bit misleading because really what this quantity is doing is that it's looking at the it's it's really the moment generating function of the quantities you care about. You see, it's it's building this so if you think of this as some measure, you know, up to dividing by two to the n, it would be a probability measure, then you're really computing the exponential moment generating function. The exponential moment-generating function of these variables. And if you understand the moment-generating function of these variables, you will understand these quantities well. Okay, so I will answer the question in a second. So, yeah, from a physics point of view, it's natural to have what's inside the exponential be over. What's inside the exponential be over the n, and so, yeah, so I did not explain the normalization. Um, so this will be of order n, right? It's a sum of things which are of order one, and this will be of order n square. So, if I divide by n, it will be of order n again. And so then it fits my request of what's inside the exponential being of order n. And you know, from a physics. From a physics perspective, it's natural. It's like an extensive quantity in the size of the system. But also, you see, if you want to have an influence on, if you want your measure to really affect some tilt on each of the variables individually, you do need something of order n inside. This parameter, for instance, will produce a tilt on each of the variables. And if you were not scaling in that way, it would not produce. were not scaling in that way, it would not produce such a tilt. Okay. All right, so I want to study this function and I promised partial differential equations. So what I want to do is take derivatives. Yeah, in the first example there was one of a scored event because the quantity in front was random, so it was centered. So you have to think harder about what's the actual contribution of the best signal. What's the actual contribution of the best sigmas and stuff? So it was a more subtle scaling. So here it's simpler, it's just the deterministic scaling is called. So let me take derivatives of this quantity and see what happens. So this is a derivative with respect to h. So h is here. I have a logarithm here. So when I differentiate, I have the ratio of the derivative. Ratio of the derivative divided by the function itself. So, first I have to look at the derivative of this thing. And when I differentiate with respect to h, well, there's whatever is in front of h, which will come out of the exponential and be brought here. And you see that it has a ultimately, it will have a form which is similar to what is written here, except that this f of that this f of sigma will be sum of sigma i right so so when I differentiate with respect to h so there is this one over n which stays so one over n and then ratio of derivatives so sum of sigma of sum of sigma i exponential of blah blah blah divided by sum of sigma of exponential of blah blah blah okay so so I also Blah, blah, blah, so I already decided, but you see that it's really, oops, oops. It's really of this form here, right? It's really of this form with this ratio of two things. So this is 1 over n the expected value, you know, the expectation of the sum of sigma. Okay. Okay, and if I do the calculation with the t derivative, then a very similar thing will happen. So I have one over n in front, and then whatever is in front of t, which in this case is this sum of sigma i, sigma j, will show up in the expectation. So there's still a 1 over n in front, and then sum of sigma i, sigma j. sigma j okay does do people agree with that derivation so so yeah oops sorry this is the sum of all i and j between between one and n and because the curl is model is so simple this sum is really just one over n sum of sigma i. sum of sigma i everything squared okay so so that's that's where the you know the simplicity of the model comes really at the forefront is that I can rewrite this in a very simple way just as a function of this sum of sigma i. So again we see that the normalization is probably pretty good because we see that the derivatives of this function are of order one. You see that like this Like this one is between minus one and one, and this one is between zero and one. And moreover, if you're a bit optimistic, you think that actually this t derivative is the square of the h derivative. It looks like it's so. So it's not really true, but it's a good starting point to notice that because what we see when we look at the truth. Because what we see when we look at the difference between the t-derivative and the h derivative squared, what we find is that it's the variance of this quantity 1 over n sum of sigma i. I would call it the mean magnetization. It's the variance of the magnetization. So let me write this. 1 over n sum of sigma high. Sum of sigma i squared minus one over n sum of sigma i squared. Okay, so so I mean we don't know yet how to really make this regros, but you see that what's on the right side is I mean what I've written is regrose, sorry, but what I'm going to say next is more at the level of the intuition. At the level of the intuition. What's on the right side, like this quantity itself is of order one. And what we're writing is the variance of this thing. So, you know, if you believe that there are some stochastic cancellations in your problem, then maybe the variance of this thing will be of lower order. So we start to feel some hope, at least, that when n becomes very large, you know, this function fn almost solves an equation of this form with basically zero on the right-hand side, you know, something kind of small. Hand side, you know, something kind of small as a function, you know, as n becomes very large. So, so the next thing I want to do is try to make this rigorous. Okay, so how can we kind of justify that this thing on the right side is actually sort of small in some sense, and therefore, in the limits, this function fn solves the equation with zero on the right-hand side. So, I need to find a way to To find a way to understand the variance of this sum of sigma i. And before I said that our function fn is the moment generating function of our variables. So in particular, it's the moment generating function of sorry, I mean this one, it's the moment generating function of sum of sigma i, if I look at it as a function of h. So in particular, it should encode the variance of this variable in some way. Of this variable in some way, right? So, if I study fn carefully enough, I should find a way to express this variance in terms of fn. I just need to find the right way to put it out, but it should be there somehow. And, you know, maybe looking at the second derivative of fn will be a good idea. So let us try to do that. So I'm going to. So I'm going to slide up so that we see the H derivative. It's here, right? So I'm looking at this expression, this ratio. And I'm trying to differentiate one more time in H. So I wrote dot dot dot, but inside this dot dot dot, there's an H, right? So when we differentiate in H, the first thing is we will have to differentiate that occurrence of H, and then there's also one somewhere here, right? One somewhere here, right? So we have two terms. So let's focus first on the first one. So this one here, which I have not written, it's again h times sum of sigma i. So if I differentiate again, what will happen is that I'll have an extra sum of sigma i that goes out and shows as a square here, right? And so it's again an expression of the form, maybe I'll of the form one over n. Of the form one over n and then this average of sum of sigma i and then everything squared. Okay, so that's the first part. And then there's the second part with the edge that is somewhat hidden here, right? I need to differentiate this one as well. And in this case, what will happen is that, so when we differentiate this one, we can think of the numerator as being constant, and we're just differentiating this This denominator part, and it will be, you know, the result will be the derivative divided by that quantity squared, that denominator squared. So we will have, you know, this will come to appear twice, and this will be squared at the end, right? That's what happens when we differentiate this term. And so, what it is, is 1 over n average of sum of sigma i, everything squared. Sigma i, everything squared. Okay, there's a minus info. Okay, so there's a bit of a magic. It's actually very directly related to the virus because up to I think I messed up a scaling. Yes, the one over n is outside the square. So, it's really very directly related to the variance because it's, you know, when you compare these two expressions, you see that they are the same, except one of them has a one over n extra. So, what we have shown is that dt F L minus dh squared F L is Is 1 over n times the h derivative. Sorry, I made a mistake again. Here I meant the square of the derivative. And here I mean we differentiate twice. Okay, this is my notation for differentiating twice. And this is the square of the first derivative. This is the square of the first derivative. Okay, and this is a very, very important. This is a very, very important observation because now everything is expressed in terms of fn. So if we want, we can completely forget about our starting point and the study of this probability measure and what's the definition in terms of probabilities. And just think about what is a function fn that satisfies this equation. And just try to understand what happens when... And just try to understand what happens when n becomes large for a function that satisfies this equation. Okay, and it also gives a further credibility to the idea that as n becomes very large on the right-hand side, you know, this right-hand side will vanish because there's this one over n in front. And probably, you know, we're not so sure how to bound the second derivative, but maybe still it will disappear in some way. And yeah, one last observation. And yeah, one last observation I want to do about this function is that also, so you see now why I wanted to call this parameter t is that now I think of this as an evolution equation in t. I think this is, I think of t as time basically. It's like the partial differential, the derivative in t of this function is doing some things. And so it will be useful also for me to understand what happens when t is equal to zero. What is the initial condition, if you want. What is the initial condition if you want? And so, let me write this. So, I rewrite the definition when I set t equals zero. So, it's one over n log of the sum over sigma of exponential of h sum of sigma i. Okay, so it's the sum over i between 1 and n. And you see that now, not things, so I guess maybe it's also one way to understand also why I call this non-interacting because this you see it can be factorized, like this is exponential of a sum over i. And so I can write it as the sum of the product. Okay, maybe I'll write it log sum of sigma product over i of And then you can factorize this expression. So it's a sum of products, etc. And once you have factorized it, you see it's just one over n log. Okay, so maybe it's one over n log of a product of each time the same expression that is repeated, which is exponential h plus exponential minus h everything to the power of n. So in fact, it does. Power of n. So, in fact, it does not depend on n. You see, the power of n cancels with the one over n outside. And so, what's really important is that it's kind of very easy to understand the, I'll call it psi of h because it's kind of funny. It's very easy to understand what happens when t is equal to zero in this problem. Okay, so we understand perfectly. So I could write explicitly what is psi, you know, is log of blah, blah, blah. But this I don't want to stress too much. I just want to emphasize that we understand perfectly what happens at C equals zero. So yes, there's a question in the chat about generalizing to a case with not all to all interactions. So yes, so a big aspect. Yes, so a big aspect of what makes this possible is indeed that the thing is kind of mean field, but there's very little geometry. If you were studying, for instance, the easing model on a lattice, then things would be more difficult. I think you would not find a way to close the equation the way I did. Although, yeah, so there are papers in the literature that use ideas like this to get bounds on some aspects of this. On some aspects of the easing model, for instance, on the lattice. So you cannot close the equations in general, but maybe you can find inequalities, and these help you to understand the model, even though you cannot write closed expressions. But yes, indeed, the fact that here is kind of all-to-all interactions is what really allows me to close the equation, and that's what will allow us to completely understand the problem in the end of the day. Okay, so you see. Okay, so you see now what I want to do is explain what we do with the two things that are squared in red. If you want, we have made a first step where we're studying this probability problem and we derived these two identities and now I claim that from these two identities we should be able to understand what this function fn is doing in the large L limit. In the large L limit. It becomes a completely self-contained problem about a function that satisfies this thing and that has a simple initial condition. And we should understand what happens when n becomes large. And before I move to that and describe this, I want to stress one more thing, which is in some sense the most important connection between this function fn and our understanding or our heuristic understanding. Or our heuristic understanding of the system. And it is that the h-derivative of the function fn is the mean magnetization. Okay, so this I want you to remember just as a way to interpret whatever we find about the function fn later on. Okay, so the derivative will indicate for us what is the mean magnetization of the system. So So yeah, maybe I'll take five more minutes to just describe a few things about this and tomorrow we'll spend more time making things rigorous about this. So let's say it's a second section and it will be some interlude. Okay, so I want that we take a step back and think about what this equation is doing. So we need to think carefully about What it means to be a solution of this equation, dtf minus dhf squared equals zero. And just in the short amount of time, I'm I have left, I want to describe a first naive attempt and why it's not working. So, the perhaps the most naive thing you should first try to do when you think about what should it mean for a function to solve this equation is, let's just look at a C1 function and then ask that it solves the equation pointwise. What's the problem with that? So maybe we look. For C one functions that satisfy the equation everywhere. And now I want to borrow a bit of what maybe you already have as intuition. You already have an intuition about the standard easing model to explain why there will be a problem with that. The problem is the following: those of you who already know about the easy model know that there is a phase transition going on there. So when the inverse temperature, which in our setting is called T, is small, then nothing particularly impressive happens and there is no There is no, when you vary h, the mean magnetization varies smoothly. So maybe, you know, if I if I draw a picture of the function fn of th as a function of h, it will be some some smooth, some smooth thing like this. Maybe this is this is h varying, this is the, you know, then the parabola is the value of the function, and this is for small t. Okay, so for fixed small t, we expect For fixed small t, we expect that it will look like this. But then when t becomes large, we expect that there will be a phase transition. And what does it mean? Well, you could understand this as saying that if h is positive but very, very tiny, still the mean magnitude will be positive and away from zero. So it means in our context that the derivative of fn, even when n becomes very large, is Very large, is positive and away from zero. And instead, if the parameter h is negative, even very, very tiny, then the mean magnetization by symmetry will be negative and away from zero. In other words, we expect that there will be a jump in the derivative of a function. So it's like it will look a bit like the absolute value function near h cos zero. So let me try to draw it. Okay, that's not a great door. You know, I want to emphasize that there will be a corner at the bottom. Oops, and that was supposed to be near h square zero. So this is h, and this is for t large. Okay, so I claim that this should be what the function looks like when t is large. When t is large, when we vary h. And in particular, it is not a c1 function. There is something that goes on here, which is not c1. So, in particular, here, there is no good notion of derivative, and you cannot say, okay, the equation is solved pointwise at this point. So, we'll have to find a better way to express what it means to solve the equation. And this is what we are going to talk about tomorrow. So, yeah, I think now it's a good. So yeah, I think now it's a good time for questions. Okay, so before we go to questions, I'm just going to briefly unmute everyone so we can give Jean-Christafe a big round of applause for a wonderful first OOPS lecture. Thank you so much.