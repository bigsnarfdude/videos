My speaker is Shu Chong Penin and he will be talking about Croxlet and the record. So hi everyone. I'm really happy to be here and today I just want to talk about our recent work called Proximity Attention Point Rendering or Beaver. So we consider a problem setting that's similar to MERF where we're given RGB images of a scene with the corresponding Scene with corresponding from multiple views with their corresponding camera poses. And our goal is trying to reconstruct this. And just for a little bit of background, in case you feel a little bit rusted, so NERF uses a volume metric representation where it assigns for each coordinate in space, assigns with a volume density and a color. And these two values are computed by a network. I split up over there. Network. I explained that over there. And to render a view, they basically need to cast ray from the camera center and aggregate sample points along the ray toward the final color. Good and pixel. And this is a result, I'm pretty sure a lot of you have seen hundreds, if not thousands, of times, but you can see that this produces pretty good results. So today we want to mainly We want to mainly discuss how we can go one step further from just modeling the scene to editing the scene. Because the scene editing ability is crucial for a lot of users, such as artists, designers, or even just for casual users like myself. And you might want to ask, okay, how can we then edit a NERF? Achieve this with a NERF then? Well, if we Then? Well, if we think about it, well, maybe the most simple thing is we can change our model. So suppose our original model will be able to render this Lego doser at this view. Our goal is to want to somehow manipulate the weight theta here such that we'll render the view that's edited. But the problem here is that we don't know actually how to change the network weights because there's no Because there's not like a straightforward geometric interpretation of the model weights. These are just numbers. How can we change them to achieve this desired edge? This doesn't look like a very viable path. So how about this? How about instead we change the model, we fix the model, but we change the input to the model. So the idea here is that we can define a global deformation field, which specifies Deformation field, which specifies which transform the original scene to some edited scene. However, the problem here is that it's very laborious to define what happens to every coordinate in this deformation field. If you can't think about it, if you're trying to change all the volume, there's just an infinite number of points. So definitely changing this naively or managing. Naively or manually doesn't work that well. So, this might lead to another idea: is that okay, maybe we can just try to deform this input space using some explicit geometry proxy. So, in this case, instead of like manually defining a deformation field at all coordinates, we can use some geometry proxy. So, in this case, it could be like a cage. In this case, it could be like a cage. And we can just edit the scene by changing the control points for this geometry proxy. However, the problem here is just that this geometry proxy is extracted and it might not match with the original geometry correctly. So when you're trying to edit this with this proxy, you might introduce errors in your edited results. Might introduce errors in your edited results. And moreover, to do this requires an additional step before you can edit it anything because you have to do this extracting proxy first. So the question we want to naturally comes out is just why don't we just learn this explicit geometry directly? And that's the idea that we want to achieve in this paper, where we're trying to build on a To build on a point cloud-based explicit scene representation. And our goal is to learn this representation and a rendering pipeline jointly from scratch. So to render an image requires shooting array from a camera center through all the pixels on the image point. However, since we're dealing with point clouds, each of the With point clouds, each of the points is inherently infinitesimal. So the ray rarely, in most cases, doesn't intersect with this point cloud exactly at one of these point locations. And in those cases, it's unclear how to get this output color at that pixel. Because this ray is just hitting nothing. It doesn't intersect with any of the points. So a common approach is to turn these points. is to turn these points into spheres, sorry, disks or spheres, and project them onto the image plane. And this is also known as squad-based rustization. So this increases the probability that the ray will intersect with some of the points. So for a given pixel on this image point, this output can be then computed by combining all the spots that intersect with this That intersects with this pixel, with this ray field. So, in this case, it will be EMB as plot A and B. And to allow learning of our same representation, it's common to use a differentiable render. So, a differentiable render allows the rendering output to be differentiable with respect to the same representation. So, in our case, it's a point count. And with this, we can then also. With this, we can then optimize our SIM representation to make the rendered output as close to the ground truth image as possible. However, there's an issue with learning point cloud using this spot-based pair. So let's illustrate it using like an example. So suppose our pixel is no longer here, it's on this bottom left corner. This bottom left corner. And you can see that the contribution from all of the splats drops to zero towards this pixel because none of them intersect. And this also means that the gradient at this pixel with respect to all of these splats also drops to zero. But the problem now is that the loss at this pixel is not zero because you can see that the round truth image has some pixels here, right? So the loss here is not zero. But the But the only way we can reduce this loss is by moving some of the splats over to cover this pixel. But these none of the splats have any gradient, so they're stuck. So there's no way we can reduce the loss in this case. So by the way, these kind of splats are also known as hard boundary splats. So they only contribute towards pixels within a certain radius at this point location. So it seems like the reason behind why the gradient drops to zero is because of the use of this hard boundary spot. Sorry, this hard boundary. So you might think, okay, then how about we just soften this boundary? So instead of having a hard boundary spot, we can have the spot modeled as Gaussian centered at this pixel location. Well, this does make the contribution of the spot at this pixel non-zero. Of the spot at this pixel non-zero, but the contributions are still so small that the gradient is not enough to move these spots significantly towards this pixel to reduce the loss. And if we think about it, this behavior occurs because of the fact that the contribution of each point towards this given pixel is only dependent on the absolute distance between them. Without considering how. without considering how far the other points are to how far the other points are to this pixel. So in this work we propose a different approach where we're trying to model, trying to calculate this contribution by considering not just considering how far this point is to the target pixel, but also how far the other points are to this pixel. And to calculate this distribution, And to calculate this distribution, we then take this absolute distances and then normalize them, such that the contribution from all the spots towards this given pixel sum up to one. And this ensures that for all of these pixels, your contribution, there's always some points that are going to make a large contribution towards this pixel, even if this pixel is far away from all of these points. And this allows us to learn. And this allows us to learn a point cloud from scratch. Also, a small detail here: we no longer use plots here, so each point is free to contribute to all of the pixels. So here's an overview of our method. So like I mentioned before, so we built on a point-based theme representation, where each point is defined by its position, a feature, a view-independent feature vector. A view-independent feature vector and a full ground scroll. So all of these are learnable. And given a ray, we first calculate the displacement from the camera center to all the points that is nearby this ray. And we feed that into a differentiable render that uses a tension mechanism. So the attention mechanism takes these inputs and then tries to select Inputs and then try to select the points, relevant points, most relevant points for this ray and then combine their point features to render the output public. And additionally, we'll also add a latent code input to this renderer to model the differences among different views, such as the exposure. And to avoid this network ignoring this latent code input, we train it using a technique called conditional implicit. Conditional implicit maximum like people estimation, which could talk about this. So, using only RGB images as supervision, our method is able to produce a point cloud that correctly captures this target geometry, this scene geometry. And this is done also from scratch. And here we visualize the clustering of our point feature, and you can see that. Feature. And you can see that the clustering effectively captures the distinctive local texture features at different places. And additionally, our method also produces pretty high-quality dash map and RGB color renderings. So comparing to recent point-based neural rendering methods, our method can best capture the scene geometry. We're using the same number of the same random. The same number of random initialization. So, here all of the method uses a thousand points. And you can see that our method also evenly spreads out the points on the object surface, unlike some of the recent methods that missed some points like on the back and then just put all of the points upon the front. And we evaluate our methods on both synthetic and the real world settings. So, one thing to note is that our method uses significantly. Our method uses a significantly reduced number of points compared to previous methods. All of these are done with a total of 30,000 points, whereas the prior method usually uses somewhere around 50,000, ranging to 100,000, some even uses remote and points. So the color R2B rendering are shown on the top, and the bottom row are just the functions. So here's the type and temples. So here's the time and temples data sets, and you can see that our method is able to learn these fairly intricate geometry, which is captured using this point. And here's some quick numbers. So these four methods are the recent point-based methods. So all of these we run it using the same number of 300,000 points. One thing to note is that these methods all require some. These methods all require some sort of initialization through multi-view, serial, visual haul, whereas our method doesn't require any of these specific initialization. And just as a reference, you can see the performance comparison between our method and NERF. So we also achieved pretty good quality. And here's a Blazion study where we studied the effect of the number of points towards the round. Of the number of points towards the rendering quality. And you can see that our method maintains a fairly competitive level of performance, even when the number of points dropped from 30,000 points all the way to 1,000 points. So in comparison, the prior methods, you can see a significant performance drop when they're using less number of points. So here's the fun part. So we showcase some of the zero shot editing ability of our method. Editing ability of our method. So, all of these are done just by changing the underlying point cloud at the point-based representation without any additional supervision. So, here you can see some of the rigid bending motion of the objects here. You can see the rotation of this status head and the ship over here. And here we demonstrate a non-volume preserving. Demonstrate a non-volume preserving stretching transformation. So, this is typically really hard for the point-based methods. Because, as you can imagine, if you were trying to pull the points apart, it's very likely to create some holes inside them because they were just usually treated as like little tiny spheres. But you can see that our method does preserve the surface continuity, and it's able to stretch out these patterns in a very reasonable way. A very recent little way. And the second application involves some of the object manipulation. So this is just done by copying or removing some of the points. So here you can see that we can just duplicate and then put additional hot dog on the plate. This one is removing some of the materials and duplicating some of the elements. And this third occupation involves texture transfer. So in this example, So in this example, we start by identifying some of the points that correspond to the muscular texture. So you see these yellow points here. And we then find these some of the points that correspond to the catch-up texture, highlighted in red. And what we do is just we simply transfer the feature vectors from this mustard corresponding points to the catch-up corresponding points. And you can see that we can transfer the texture of the mustard onto this. Texture of the mustard onto this petroleum. And last but not least, it's about exposure control. So at test time, we can change the exposure of the rendered image by just varying the latent code input in our map. And that's it. For more results, please visit our project page. And we will put the paper on archive real soon. Thank you. Any minutes or questions? So in point-based processing graph, so in graphics there's a community called point-based graduates. So many of the issues with point is that we have to adjust the summary of the points, right? They introduce insertion division points. Yeah, point-based rendering. Are you familiar with the measure of surface rendering book? Yeah, there's um so they introduce some recognizations where you balance the distribution of the points. Right. Uh did we use that or we do have some uh so okay, yeah, so in these results over here, we do have some uh point adding and pruning strategy. So if you can see um So if you can see, actually this is my laptop is kind of biking, but yeah. So if you can see here, you know, if you don't apply any pruning or adding strategy, you know, sometimes you do get some outliers, even though it's not very, but you do get some of that. And we do have some of the strategies to do this plumbing. And then also, Do this proming, and then also we have some strategy to add some points, which you know, the results you've seen here look much more denser. It's progressively adding the points towards this. Yeah, so in our case, this is mostly done by a combination of detecting whether this point should be on this scene surface, which is the learnable part. Learnable part in this learnable parameter, there's 4.4. We basically use this to prune the points. And then also when adding the points, we do something similar when trying to find the more sparser regions. And then we'll insert some of the points. But the method on its basic form still does try to spread out, tend to spread out the points more evenly. The points more evenly. So, usually, this is not a very complicated process here. So, we just do some really simple padding structure. Depending on the clusters that will come over there, right? You mean the objects? So, when you do editing, that would be dependent on the clusters that are being formed. What do you mean by the clusters? Can you do arbitrary editing? Say, for example, you know. But remove certain things from the shape. So there you go, object shape. Yes. Oh, you mean the texture or this armrest? Maybe remove armrest? Yeah, you simply just remove the quiz. And change the orientation of the armrest and all the things. So you can, whatever you can do in Nigerian with the point cloud, you can just apply it and then just render it. It and then just render it without any finding. All we will say react to point out in the manipulation examples. Oh, so this is very naive. We just kind of like manually define that. For example, the master example. Yeah, I cannot imagine. So is the place we constructed also below the master or like the master, like below the master is all? Well, okay, yeah. So our point clouds tend to be honestly. So our point clouds tend to be on the surface. So once you do move them away, there are some points that are still on the bottom. But if you don't have much training view from the bottom, then the points are more likely to be spread out on the top. For logo, I think it's pretty hard to select a point model that we further. Yeah, well, it's actually not that bad. You can roughly have a If you can you can roughly have like an idea like where these things should be like you know because yeah, it's not like a you know like a trivial process, but it's at the same time it's not undoable. You know, like you can, here we're just doing these blades, so if you actually see it's like mainly these regions. Definitely combining with some other techniques will be easy. Yeah, so like there was this exciting new paper at Sigraph, this 3D Gaussian splatting paper. Platting paper. Have you read it compared to it? Yeah, but. Yes. So I think we're tackling slightly different problems here. I mean, our current method isn't really optimized for really efficient real-time rendering, those kind of things. But I think we put more focus on learning the point-clock representation from scratch. So it's like, how do you better produce stable learning? Like, produce stable learning signals for this point representation such that you can move them more evenly on the surface. So, yes, I think also the, like I said before, like the paradigms, it's kind of different. I think it depends on what you want to do. But they also learn it from scratch. Like, they use MDS initialization mostly to speed up. But in the scenes, like the one you've shown, they're also starting from scratch. Yes, but that still requires you. But yes, but that still requires you to have points everywhere in the volume to start up with. Like, if you have, let's say, if I have a scene here, all the points were initialized in the middle, and then you want to just easily deform it to the whole thing, there will be some challenges. And then here we just want to say that, okay, we don't want the gradient to be like to vanish when the ray or just two. One array, or just too far away from it, right? So, we're just trying to find a way such that it's better, easier to learn these kinds of representations. So, I feel like I think it's a different kind of paradigm in how to render these. One other difference is that they use 250,000 points. They have to do it because otherwise, you cannot cover the actual scene. In this case, I believe I started with a few thousand threads. Yeah, so we can go. Yeah, so we can go even less, like a thousand points. You can still, like, the PSNR won't be that high, but like, you can still get the object. In fact, if you can do as little as 100 points, that was like the very tour example that we tried. Is there a reason or a guarantee that the points tend to be on the surface? I think it's a result from the views that we get. It's more, you know. The views that we get. It's more, you know, like, even though it's not uniform across the thing, but it's somewhat, you know, like a 360 view. That helps, I think, in putting things roughly at the right place. If you have, let's say, if you have views that are just not covering the full scene, like, there might be.