Next speaker is Jen Jing Wang and she'll be talking about spectral bands and the plans on ETS. Thank you for the introduction and I want to thank the organizers for the opportunity to come here and present the work presentation. So in this talk I want to present some research results joint with Fanio Mariano that we study That we study spectral terms and exit times our metric space. And what is more important, what is more interesting for us is the interaction between the two. And so to start with, I will start with some of the classical results in RN. These two objects have been like has a lot Have been like has a long history of study in literature. So let's just warm up in the Rn case. I will start with exit time in Rn. So if we consider domain in Rn and Brownian motion started from a point that is inside the domain and then killed upon its arrival to the boundary, then Chris will record the Of course, we record the exit time by a random variable tau d that is given just like this. And then if we consider the distribution of tau d distribution function, we can consider the probability that tau d is greater than t. And of course this has interpretation, can be interpreted as that the Browning motion survives up to time t. Up to time t. And so we call it survival probability. But if we try to compute this guy, say for the Brownian motion, of course in Rn, we know the transition density for the Brownian motion is Gaussian kernel. Then to compute this guy, we just need to integrate the direct kernel of Brownian motion with respect to space inside time, inside the inside time inside the the the space D. And this connection I think is quite nice because this is the probability that the Brownian motion starts from X and hit point Y that is inside the domain d at time t. Therefore, if we integrate over all the space, that should give us the total probability that it is not killed until That is not killed until 19. And we see that this quantity is closely related to the heat kernel. It therefore contains lots of the information of the domain. So by information, I mean both geometric information and the spectral information. So and in literature, there has been a lot of And in literature, there has been a lot of work discussed in trying to decode this type of disinformation from the survival probability or say exit time in general. And so here I just want to have a brief review of some of the results that are interesting to me. And some of them are related to the the talk, the second half of the talk, when we want to move to metric measure space. Move to metric measure space. But here I arrange these results, probably I put them into three regimes: the small time asymptotic results, large time asymptotic, and then we also consider integration with respect to time. So in the small time regime, what we consider, what people consider is the spatial integral of the survival probability. And this quantity has a very nice. And this quantity has a very nice because this survival probability, if we write it in terms of function u of tx, this is exactly the solution to the Dirica boundary condition, boundary problem with initial condition that you start from the time zero, that you start from temperature one inside. Temperature one inside the domain and temperature zero always zero at the boundary. And therefore, this guy is given by while we were trying to integrate space, this is exactly the temperature. So therefore If now we try to, so this survival probability tells you what is the temperature at time t at location x inside the domain if you do the heat evolution of a domain start from temperature one. And therefore, if we try to integrate this thing with respect to T E L. With respect to X. With respect to X, then you get the total heat, total temperature inside. So that's why we call it the heat content. And then if you consider small time asymptotic expansion of this heat content, there's a natural intuition to say if you were kept temperature one inside domain at time. Inside domain at time zero, and you just wait for a very little bit of time. So, this domain has no time to lose too much temperature. Therefore, what matters is what happens to the boundary. That's why we see correction terms that start to give you information about the domain, like parameter, and also mean curvature, total mean curvature, because correction, higher correction will happen, depending on if the Happen depending on if the boundary is more curved or more flat. So this is natural intuition, but in our end case, this was given by Landenberg Lauda using some probabilistic method. And then there are results also existing in the Riemannian setting and some results in sub-Riemannian. Some results in sub Riemannia setting. So I did it with Jeremy Tyson for the Heisenberg role, and Rissi Rossi also did some, I think they have more general results for still for salary money and situation. And for this part, I haven't seen any result that people try to extend to rougher space like fragments or so. It could be interesting. So it could be interesting to think about. Then there is a large time regime, and I guess this type of result is quite familiar to people, to the audience, that if we consider large time asymptotic for the axiom, then asymptotically it can give us the spectral information. This is a a classical result in Rn that if you That if you take, well, it simply says that your survival probability has exponential decay, which the rate tells is given by the first duration eigenvalue, say if the domain is a boundary domain. And this result lately was extended by Capanini and Marcia Gordina to Canal groups. That is using the sub reminding. I mean, kernel boots equipped with sub-remining metric. And okay, the last regime I want to recall is about the time integral. So this if we integrate this survival probability with respect to time, then of course this is Time, then of course this is give us the mean exit time. And to discuss the mean exit time, there are different literature. People consider, could consider L1 norm in space of this mean exit time. And that is closely related to a physical quantity, that is the so-called torsional rigidity. And in literature, there are people considering the whole sign. There are people considering the whole sequence, so of course, you can consider also the spatial integral of nth moment of the exit time. Then you get the sequence. And in literature, McDonnell-Meyer's result says that actually, if you consider the whole sequence, it determines the whole information of your spectrum. So it determines, by determining, By determining, I mean that if you have two domains who have the same sequence of this TN, then their spectrum sequence will agree with each other. We are now in which setting, so this is what you see. This is still in Rn. This is still in Rn. And also, this sequence determines the heat content sequence and heat content. Heat content sequence is what I mean by heat content sequence is the coefficients that show up in the small time asymptotic of heat content in the first part. And that one, if we recall, that contains all the geometric information. Even higher order terms, they will contain some of the coveratured information of the domain. So all these information are contained in this sequence. In this sequence. And there are also other literature that discuss particular relation between the n-th eigenvalue and the nth moment of the exit time. And there are also people who consider this L-infinity norm of the mean exit time. And this is the part that is more related to our work where we try to generalize. Where we try to generalize things from here to the metric spaces. So if we consider the L infinity norm, of course, that just means that we want the longest survival mean exit time and optimized among the best starting point. Then there's something that we can, okay, these are. That we can okay, these are there exists this kind of isoparametric type of inequality for exit times, which says that among all starting point and among all domains of the same volume, this quantity is maximized by both with starting point at the center. And quantitatively, because this kind is computable, so quantitatively we can just bound this by the We can just bound this by the domain, the volume of the domain. And then on the spectral side, there's Fable-Crown inequality that plays the role of isoparametric type inequality, which says that among all domains of the same volume, then you have your your principal degree eigenvalue is minimized by both of the same volume, which is also computable. Which is also computable, and we see that this is divided below by one over the volume of the domain. So at the end, when we see these two quantities, it's a natural guess thought, natural thoughts that whether these two, if we put the product, if we can get some uniform bounds. So that these two questions about this. Questions about this so-called tug of war between these two quantities. And another, of course, quite natural observation is also the domain monotonicity that suggests this. If we increase the domain in the inclusion sense, then this guy decreases and this guy increases. Okay, so this is a typo. Sorry. Right, so Um, right, so then the question is whether one can come up with a uniform control over all domains for this quantity. And in RN, we are still in RN, classically Benuelos and Carol could give bounds, upper and lower bounds for this product among all simply connected domains in R2. And Georgie Smith generalized for Generalize for all smooth domains, Mandembergero generalizes for all domains with positive spectrum, positive bottom of spectrum. And then the lower bound is still true, but the upper bound they could improve. And also it's nice that they observed that asymptotically this growth like linear. And in particular, the lower bound is sharp. The two is sharp. So one example is given in these two papers that independently and the shape is like a Swiss cheese. You dig a lot of holes. And that gets to the lower bound. But for upper bound, And for upper bound, it's still unknown, actually. It's still an open question and conjecture what is the best shape for the upper line. So now, what people commonly believe that among convex domains, the best shape is triangle, equilateral triangle. It's not a bowl. Usually you would guess a bowl, but it's not. But it's not. And so there have been works that are trying to improve these upper bounds and also generalize. So there is the paper by Volt. He actually obtained some nicer bounds than previous work in Rn and also obtained asymptotically certain expression for the upper bound in terms as an expression in terms of D. In terms of D, it has asymptotic growth as D over 4, and this is actually sharp, just in asymptotic sense, because D over 4 is realized by holes. That's the result. And then in the joint work with Rodrigo Banielos and Daniel Mariano, we did extend this to the Extend this to the product between the peace power of the spectral band, bottom of spectral, and the peace moment of the exit time. And in that work, we can also obtain this lower bound is sharp, the upper bound is again sharp only as simple in large dimensions. And I also want to mention there has been work for this by Wendenberg who studied this type of question for women and manifold with rich lower bound. So, okay, so next this is we're going to move to metric measure space. Space. So, right, after we finish the working, then since Faniel and I, we are more interested in sub-Riemannian geometry, so we just ask the question whether this type of results can be generalized to sub-Riemannian settings, but then after we get our hands on the problem, we realize actually we can go into more general, we can just go directly to metric-metric space with careful assumptions. So, that's why we So that's why we now consider on measure space with local regular Euriclet form. And then of course the process now is the Hung process generated by this Euriclet form. There is the heat semi-group which admits the heat kernel which will play the role of transition density for the HAMP process. And we consider some open domain, open set inside this space, but we cannot necessarily assume that it's bounded. Still, we can make sense of the bottom of spectrum just in this way. And there is still the exit time for the process. And yes, of course, I should have introduced this idea. We introduce this generator for the domain, and then we can define the bottom of the spectrum. So that is the setting, but we just in this work we are not assuming that this this this this durical Laplace has a discrete uh spectrum. We are not uh assuming that uh the Assuming that the headline is continuous. We are trying to be general. Then we have to come up with assumptions, nice enough assumptions to give us a way to find the estimated exit time survival probability. So when in RN everything is nice, and the technique used there, of course. And the technique used there, of course, takes a full advantage of the Dauchen kernel. In this general setting, what we can come up with are these conditions. Of course, we don't use all of them at one time, but the audience is more familiar with this than me. So we have volume here, just this, the volume doubling, reverse volume doubling. In some settings, we would need both of them. And then And then subgauss and upper bound and here I list upper and lower subgaussian middle bonds. So in Euk of course it has everything that is the optimal situation but now to work with to to to to do our estimates in in general metric measure space maybe A general measure space, maybe we don't need everything. So we try to minimize what we want to ask. So we come up with two sets of assumptions. Set one is we do need both volume dubbing and reverse volume dubbing, but we do not require lower subduction bound. And so this set of assumptions could work for lots of Work for lots of flat space, but not for compact spaces. That's why, then, if we try to also consider applying to compact spaces, we realize maybe by removing reverse volume doubling, we will need also the price to pay is the lower stop Gaussian estimate. So, in our results, then we will assume either one A1. Either A1 or A2. And here, let me just list some examples. We come from, our motivation was from the sub-Riemannian geometry. So what we considered at the beginning just flat sub-Riemannian spaces, like a kernel group equipped with kernel curve third resistance and the hard measure. This will satisfy set A1. Set A1. And then some compact spaces like Riemannian sphere or sub-Riemannian manifold with good symmetry and with rich horizontal rich lower bound will satisfy this example will satisfy assumption I took. So this type of class of manipulators. Many codes have been studied quite well understood thanks to the work of Fabrice. So basically, we know that, yeah, this has both upper and lower subgaussian bound. And then we see that actually we can also include Serpinski gasket as our example. That is my connection to metric measure space, I guess, to include some of the fractal examples. Examples. And then, so this is where we started from. But then thanks to Fabrice again, who actually mentioned to us that actually there is a more general condition that has already existed in literature, that is, which can include this fractal-like manifold. So, we tried our result of that. On that setting. So at the end, we realize by replacing the upper UE beta, replace it by this, instead of using the polynomial growth to the work dimension, we can actually just replace it by this parameter function. This kind of conditional. Function. This kind of condition appeared already in literature. We are using Grigoyance notation. And of course, this kind of condition also had a, I think this is more or less similar in the paper by Paulo Basmagui that you have the two pieces. So these are quite close. Yeah, so therefore, of course, if So therefore, of course if f is r to the beta, then we go back to the previous UE beta assumption. But if we want to include some more examples, this can work. That is, the UE app will replace this discussion tile here by using the function three in terms of some optimization with respect to parameter function. And also to handle the lower bound, we will. To handle the lower bound, then we will need the near diagonal lower estimate for the kernel. And so this is the assumptions that we finally stay with. That is again volume doubling, reverse volume dumping, then we consider VF or only volume doubling with UEF and LEF. And that is the example that I that is the example that we would like to we we we want to include at this picture I stole from you your paper I'm not able to make the picture but I like it so I stole it right so these assumptions then work then can include this fractal like manifold that was described in this paper that you replace the edges by a bit of truth and remove a few edges and Remove a few edges and add a little bit of knots, then you see that this diffusion will have different behavior locally and globally. Locally, it behave more like, treat more like manifold, but globally, if you want to diffuse further, it will be the work dimension will be agree with the fragile case. So here it will. So here oops. Here is our going very cool. Okay. So this is our main result. We assume either A1 or A2, then we can give a bound. We can estimate the exit time, the survival probability. Survival probability by some exponential decay that is that the rate is still given in terms of the spectral bound, the bottom of spectrum. And here we have to use, we have to take the swoop outside of a properly exceptional set. That is simply Exceptional set, that is simply because we didn't assume that the heat kernel is continuous. And in order to estimate this, we so in that case, the transition density is not always the same as, almost surely the same as the heat kernel. So that is where we have to remove a little bit of this invisible set that is capacity zero. zero. And then we have this estimate. So, what I want to point out is the exponent deep line here that is given by this, the bigger contiguum amount 2. Of course, this guy, well, in our notation about the notation in Gory Goyens, his notation. So, the alpha here is the fractal dimension or housework dimension. Beta is the wall. Dimension, beta is the walk dimension, therefore, this is the spectral dimension. But we don't know what is this some dimension. So it would be nice to have a nice interpretation of this so that we see that this d prime has some geometric meaning. And why do I mention this is because in literature there has been work by analysts who actually who actually pursue the sharp bounds for this type of quantity. If we look at this quantity from an analytic view, this is simply the infinity norm of this operator. And there has been work seen by analysts who pursue sharp estimates. Of course, not necessarily Longfashion. La fashion, they consider also some shooting operators, or maybe Lafashan in Riemannian settings. And so far, the best one that we have seen have this kind of look, but here the best is n over 4. Instead, what we have is some dimension over 2. So our dimension at least agree with this paper that shows. Paper that shows up the dimension showing this paper, but we wonder if we can really improve to sharper one because n over four for the RN case was a sharper one at this point when n equals to four. So now let's say that. So now let me present some of the applications. Because at the end, we in our introduction, I mentioned the talk of war between the spectral bound and the supreme of mean axi prime. But our main result only estimates the survival probability, but as we can see, it's Can see it's a rather easy consequence from that estimate. So then, as a consequence, we could bound this product from below and above by these two quantities. And so the upper bound depend only on P and these dimension constants. And actually, we also need to use constants appeared in this assumption. In these assumptions, then in the UE, in the Gaussian upper or lower bounds. And a nice consequence of this is that we get the equivalence between the spectrum, bottom of spectrum being positive and the exit time being finite. The exit time being finite. It's like when you ask the question: when can you, if I give you a domain, when do you have the exit time being finite? This type of question, people would come up with maybe if it's bounded domain or whatever, but actually the equivalent condition is that you have positive bottom-up spectrum. And another consequence that is almost immediate Immediate is the asymptotic estimate of the survival probability because we already got the exponential decay and therefore just take log and divide by t. We have the upper bound of this limiting quantity. And if we need if we want the equality, then this we would need that this Dirichlet form to be irreducible. Be irreducible. And to have that, for instance, there is a sufficient condition. If we know that p-kernel is continuous and the positive, strictly positive, then this can be. And okay, and I want to mention this the third application that is Gaussian alcohol characterization. So I'm not expert in this, but this. I'm not expert in this, but it's just we find out that our result has a little bit, can contribute a little towards this kind of characterization. So this, so there are a lot of literature, including many, many of the people in the audience here, that has worked on Worked on factorized upper duction, upper subduction bound, unique beta. And so we started from Grigoy and Boulogne's paper, in which they in this paper they gave a lot of equivalent conditions for characterized UE beta. So among them there is this one, that is to say that That is to say that this guy, the UE beta, is equivalent to exit time isotarometric inequality plus the exit time growth. So therefore, if you combine the two, that is to say the exit time exoparametric inequality holds and is sharp for both. So this is equivalent that can characterize, give us the upper subgaussian bound. So then in the same paper, So then in the same paper they also had other conjectures, but in the same paper they had this conjecture that brought us our interest. Simply because here you have this interpretation of the right-hand side, then the conjecture is can you replace exit time iso parametric inequality by or data about exit time replace it by spectrum. By spectrum. So therefore, exit time is a parametric inequality, maybe we should replace it by Faber Prune spectral as a parametric inequality. And the control of the exit time growth, maybe you can replace it by spectral regularity. That's what showed up in that paper. And I think in their 2015 paper, they also have conjectured that it involves. That involves capacity, and all that. Markov has some nice work towards that conjecture. They are all related. So, here we have a little bit contribution to this that we give partial result is to say that, okay, actually to go forward is not that difficult, but to go backward is more of a problem. We are not able to go backward directly. To go backward directly, but we need some extra information that is to control the optimal starting time. That is to say that if we need this assumption that the best to or say if you don't start from the center of the ball, if you start from somewhere else, there are still Start from somewhere else, they are still comparable, so we need that. And the connection is really quite simple because we just need it, it's already proved in Grigoy and Hu and Lau's paper that UE can be characterized by FK beta plus E beta. So really what we are doing here is use our result to make connection between lambda beta and E beta. However, our our we could our if you remember our result is to give a connection between lambda beta and the supreme of the exit time so therefore there's still a little gap that is to control the the starting time the start sorry the starting position so that is that is where of course our question is can we simply Can we simply remove this? Is this removable, right? If so, then we have a nice answer to the conjecture. Okay, so I prepared uh a few more slides that is about another application of this uh survival probability estimate. Fiber is com it's a Is a little bit different story that is application to this hotspot conjecture or hotspots question, which says that if you have a rod, right, and then you let with insulated ends and you let the temperature involve, and then you ask where is the hottest and the coldest part will locate. This is kind of a famous uh conjecture that uh uh conjecture that uh uh that uh that that raised uh by Rauch uh and uh Kaul, Banuelos and Berzi. So these works actually rephrase his original conjecture. So the conjecture says that it has to, because if we think about the equation that hold hot and the cold temperature neutralize each other, then the extreme motion will move apart. The extreme motion of the move apart. So then, if you want to say it in a nice way, more rigorous way, this is to conjecture on the first the eigenfunction associated to the first positive eigenvalue of the Neumann Lafashan. Because if you think about the spectral decomposition at the end of the maximum, Spectral decomposition at the end, the maximum extremal should agree more or less to the extremal of the first eigenfunction. So, if you want to rephrase that conjecture, then it is to say the extreme of this phi2, which is that eigenfunction, should always appear to the boundary. And then of course this conjecture is known to be false in general, but it holds for some examples, actually many examples in literature. And it's still widely believed that it could hold arbitrary convex domains. So actually out of time, I just want to say that lately there is the probabilistic approach to this. Probabilistic approach to this question: How wrong can Hausman's trajectory be? So, actually, some work can show that you can give an upper bound. This constant, if it's one, then that means trajectory is true. But if it's wrong, it's more than one, it's wrong, but it cannot be too wrong. That's the so-called hotspot constant. So, that is where we want to give a nice estimate to these constants. Estimate to these constants, and basically, there are some probabilistic approaches to this where actually you do need the survival probability to play an important role in this type of estimates. And so in this work, we just, because we have things ready for measure-measure space. However, a problem is that we don't know too much about. Is that we don't know too much about Neumann Lafarge of general space. So we stayed with Riemannian manifolds. And if on any domain that we can make comparison between the Neumann eigenvalue, first Neumann eigenvalue and the Erique eigenvalue, then we can have a good estimate or some estimate to these hot sparks constants. And the example is the hemisphere in S N or S Q. I will stop here. Thank you very much. So you had these two several slides earlier. Several slides earlier, the sets of assumptions A one and A two. If I remember correctly, the common to both was the volume doubling and the upper heat kernel estimate. But then you could either have the lower volume estimate or the lower heat kernel estimate. And one question is: is there an and you gave several examples to find all the other set of assumptions satisfied? Is there a some some what would is there a some kind some uh what what would is there a some kind of a premium case that you would like to like to understand if you where you don't have either the volume doubling or the or the lower heat kernel estimate. What is the first example that kind of goes outside this code that you can handle right now? Without like remove something from either set of the assumptions. Yeah, right. Right, I I I'm saying if suppose you suppose you only only have the have the intersection of A one and A two, which is the volume double in and the upper heat kernel bound. Is there an interesting situation where you have these assumptions but don't have A1 or A2? I don't know. I don't know any examples. We never think about that. I don't know. It's a good question, but I don't know any examples for example. So there should be some examples, for example, if you look at spaces with different dimensions. Same with the effect home. This does not satisfy all the limit, but you still have the low boundaries different form. In fact, it has two sided sharp bounds. In fact, there's two sides sharp out. I think a similar phenomenon happens, mainly for which was to just follow up on this example. Can you go to that page of the examples of the market? Yeah, this is yeah so for the for the CRP gasket right those conditions they won't a too much If it's a compact one are satisfied and so my question is for some many manifold you have um how about A1? Is A1 How about A1? Is A1 condition satisfied on separate mega manifolds? And this condition? Well, what what is better is uh if it's a compact manifold then I doubt you have reverse volume doubling. With both volume doubling and reverse volume doubling, you need volume doubling you can always uh up to certain tasks that's not to go all the all the radius, right? Audio radius, right? Um yeah um so so would your question be like what my question is doing with this condition, whether A1 if you had A1 for some range of radio? Uncompact one volume doubling spans the reverse, I doubt. Yeah. But that's almost too constant. That's what I was asking. If the volume W holds up to say constant, um Say constant and then the indicated kernel holds for time one time. But you want to, I mean, don't talk about exit type. What exactly the question does. So you if you have reverse volume doubling for some range, are you asking? Are you asking very much? Okay, so my question is: I'm interested in discovering many manifold. What kind of condition you have is attack even up to finite range of time that's not to go to all the time. I suggest to discuss in other departments, maybe first of all, because time is changing. So my comment, well, let me have some comment on the data chart. So, in the earlier part, you are mentioning some heat point data that is in the multi-process of different things. And in the setting of Sericina set, actually, there is a paper which is due to Blaine Harp in 2008, probably 8.8 plus 46 for Series 1. For some is also. Yeah, there is such a paper. Oh, okay, okay, cool. Yeah, I apologize. Oh, okay, yeah, I apologize. I am not reading so much of literacy, so yeah. Concerning this balance between the explanation and the principal aggregate value, these are for same groups related to local UniFlatboard. And what if the UniFlatborg were not local? Like for one local? You take the subordinate remote hand? That's a very good question. That's what we want to consider right after this. Okay, yeah. There is a possibility. We still need to look into it. Okay, so let's send this to me. And we have a coffee break and we begin at hot. 