Her hand up, but I assume that's just yeah. Sorry, just want to make sure I didn't have you didn't see something uh you had to pay attention to here. Um, great, yeah, um, thank you, Joe, for the uh introduction and um and thank you and everyone for organizing this really cool workshop, um, even if it's uh tempered by being there in person, which is uh should have been even cooler. Um, really enjoyed uh everyone's talks. Um, look forward to talking more. To talking more. Right, so I'm excited to present some of our work here that we started with Blake's Lab and the OpenScope program. And we've been working on extending the results that we have in our preprint that you can look for here if you're interested in checking it out. And I want to mention before anything else that Colleen, an excellent grad student and really cool person in Blake's lab, really worked on very much of this analysis as well. And we have three co-authors, co-first authors on this paper, and I've learned quite a lot from her. I've learned quite a lot from her. My background is actually in some of you know closer to some of the other talks here in applied mathematics and modeling. And so I've learned quite a bit on this project and I've learned quite a bit from her. Cool. Okay, so I'm glad that Blake already gave a talk this morning about predictive hierarchical networks. So I don't have to take too long going into this. Take too long going into this, and indeed, I want to go through a lot of this fairly quickly so I can get to the problem that I'm really interested in your feedback on that we've been trying to tackle. But the project in general, like Joel and Blake had mentioned during his talk, was to look for evidence in actual mouse brain as to whether predictive hierarchical models might be instantiated. models might be instantiated might be instantiated there and in particular to see whether there's evidence that that they see unexpected that they can detect unexpected events and whether that drives learning going forward as is predicted by this these this class of models and so in general like I mentioned the idea is that The idea is that you receive that higher areas and cortex send predictions down to lower areas, and then that's compared to the incoming stimuli that those lower areas are receiving at the same time. And then some signal is then passed back forward to the higher areas. And many of these, that signal is a subtraction. So it could be a prediction error, for example. And that can drive, that can potentially drive learning. That can potentially drive learning in those lower areas and update the weights in the system. Now, for our first sort of go at this, as Joel and Blake were mentioning, they're looking at expanding this into a new project going forward where we can image multiple areas at the same time. But just to start with, we looked at just a just Just primary visual cortex in the mouse brain. And as a proxy for top-down signals, we looked at the apical dendrites, which tend to receive their inputs from higher areas like Hannah had described to us the other day. And then we also looked at the somas in the L2, 3, and L5 pyramidal cell layers and looked at the somas as they tend to reflect To reflect bottom-up activity. Okay, and then to sort of test whether what we see is consistent with these predictive models or not, we looked in particular at several general consequence of predictive hierarchical models and wanted to see whether what we see is consistent with that or perhaps we would obtain evidence that falsifies that. Of course, you can't prove it. Of course, you can't prove it. Just check those two things instead. And so we looked for whether there are distinct responses to expected and unexpected stimuli to begin with. Also, whether they changed with experience over time. And then finally, because of the hierarchical structure where you receive predictions from the top down and incoming signals from bottom up, then we expect those signals to evolve differently. Signals to evolve differently in time. And so the dynamics in this sort of project is perhaps a little less so in the sort of millisecond and second and minute range, and more so from day to day, as we see that's the time scale in which we tend to observe these changes, perhaps due to consolidation. So the experimental setup is Setup is fairly simple. We just have a mouse looking at a screen. We present them with, for example, with two sets of stimuli. One is these expected sequences of Gabor patches, where we then replace one of the frames with an unexpected one about 8% of the time. And we image in the places that I've already described. And we can see some examples of those at L5. And we can see some examples of those, the L5 dendrites and L5 somata here, for example. So after habituating them to the rig, we habituated them to just the expected sequences, ABCD in the case of the Gabors, and then imaged them for three days where we then did that replacement that I described about 8% of the time with the unexpected frame. And then, as I mentioned, we imaged them. And then, as I mentioned, we imaged them over days. And then, importantly, we were able because of the high quality pipeline of the Allen, able to in fact match not only SOMAs, which is cool enough over days, but in fact, even dendritic segments from one day to the next. And so that's an important part that we'll come back to as part of our extension to see how those individual responses evolve over days. The other stimulus that we habituate. The other stimulus that we habituated them to and tested against was a visual flow stimulus. And I'll show both of these in a second, where we had consistent flow for most of the times punctuated by unexpected inconsistent flow. So let's take a look at those. So here are the actual Gabor patches. When the light turns red, which the mouse cannot see, that indicates that it is the unexpected sequences that are being presented. There we go. So it might be fairly subtle to see. It's actually pretty impressive to me that we do indeed find indications that the mouse is able to observe these and potentially learn from them. Here's the visual flow stimulus. It goes in one direction, either left or right, it doesn't matter. We do both and we randomize them. Then we have inconsistent flow at various points. At various points. Okay, and so here's a quick example of what the actual activity looks like, a little bit sped up in all the four different layers. And we were able to segment these through both the Allen and the extract algorithm provided by Mark Schmitter's lab. Okay, and then I do want to sort of breeze through these. I do want to sort of breeze through these quickly. Again, you can check the preprint for more details, but we did indeed see distinct responses to expected and unexpected stimuli. Here are individual neurons averaged over the entire presentation of the stimulus, which is half an hour per day. You can see strong positive and negative selectivity to these expected and unexpected signals in both the dendrites here and the somata. In both the dendrites here and the somata. And then to measure those, we used something that co-mentioned the other day. It's a riff-off of that, of the D prime sensitivity metric. We call it the Unexpected Event Selectivity Index, USI. We take the mean of the unexpected times, subtract the mean of the expected times, and divide by the average of the variances. And so the only difference here with the D prime is we have it signed. D prime is we have it signed so we can look at positive and negative selectivities. Okay, and what about actual dynamics, if you will? Slow-scale dynamics, perhaps. Do they change over days? Indeed, they do. So here now going down in days, we see averages over all of the ROIs, and we see small change, bigger change, bigger change. Similarly with the somata, except it seems to decrease, and we can see that over here. So the dendrites. So, the dendrites on average tend to increase their selectivity, their USI. The somata tend to decrease. And so, we see that indeed they evolve over days and they evolve differently in these proxies for the bottom-up and top-down representations. Great. And just a quick mention as well, this is at least consistent with a reduction in a prediction error. A prediction error signal, the dendrites are not. And so that already suggests that a simple prediction error model may not be sufficient to explain the activity in the dendrites at the very least. Great, and now we want to look at individual ROIs. So we saw that at a population level, they evolve over time, but what about the individual ROI level? Do those evolve systematically? Do those evolve systematically over time, suggesting that again, perhaps this USI serves as a signal to drive changes in the individual ROIs? Okay, and so these spaghetti plots are the USIs for each tract ROI. Again, we see this tendency to increase for the dendrites and decrease for the somata. But to check for whether there was this systematic change, we wanted to look at the correlations in each. In each, so how what the USI is for one day versus the next for each individual ROI. And so we just plot the USI day one versus USI day two and take the correlations. And here's what we get. So these are the tracked ROI correlations across days. Here, the left column is session one, which is here versus session two. Session one, which is here, versus session two, which is the middle column in both of these. And then we did shuffle tests to see whether those correlations were significant or not. Okay, and you can see that for most of these, they are not. So these, in fact, these for the Somata, the ones that look potentially to be consistent with an error-driven signal, we don't see significant correlations. And in part, that may just be because they all go towards. In part, that may just be because they all go towards zero, and so you wouldn't expect a significant correlation in that case because it's just zero times day one. Now, if there weren't so much noise, perhaps we would be able to disentangle that. In the dendrites, we do see statistically significant correlations, and we see this edge case over here as well, which I think may be, I can't remember if this is a plotting error, may actually be beyond the gray. We'll come back to that in any case. That in any case, but we'll keep an eye on this one as well. But here we see that these correlations are strong and negative. This one is positive. And then for the bricks, we have only positive, statistically significant positive correlations. Now, you may wonder why aren't there stars indicating statistical significance for these positive correlations for the BRICS? Well, that's because. Well, that's because we're not sure that in fact they can't be more trivially explained by stationarity. So here we see this is these are the plots for the Gaboras I should mention. I should have put for the bricks as well. My apologies. In any case, a positive statistically significant correlation would also result. Would also result if, for example, day two is equal to exactly day one. Now you might say, well, in that case, the correlation should be stronger. Well, noise can reduce the effect of that correlation. However, for the negative correlations, we don't have that same concern. Essentially, noise in the system will always bias correlations towards zero. And so if they're statistically significant and negative, then we don't have to worry about stationarity. Chances are they're probably. Stationarity, chances are they're probably actually more negative than what they appear to be, than how we measured them. Now, having said all that, we want to return back to these positive ones, and this is where we've been working to extend our results. So in our preprint, we simply said, well, we can't say too much more because of this problematic noise effects. We could, for example, take the linear regression coefficient and make sure it's not one, but again, noise will reduce that. Noise will reduce that. And so we've been working on a method to try to improve an estimate of what that correlation coefficient would be in the presence of noise. Okay, and the basic idea is normal regression supposes that your variable x is just it. It's fixed. You know what it is. For example, perhaps a time. And you know from one day to the next what the time label is or one minute to the next. Label is, or one minute to the next. There's not much noise associated with that. However, in our case, of course, we're correlating the USIs from one day to the next, and these indeed can be noisy. And so instead, a better way to approach it is to suppose that indeed we have noise in both of these variables. So we have some underlying sort of true distribution, if you will. So what the actual neuron's USI is, for example. And then we have. And then we have some noise laid on top of that. That can be, you know, as it as the animal moves around, then that affects the signal, in addition to all sorts of other stuff that will tend to disrupt from the underlying actual selectivity, for example. And then similarly for Y. And so the regression problem that we want to solve now is whether these underlying Whether these underlying distributions are indeed linearly related. I should mention that for all of this, I'm assuming, since we're looking at systematic changes, we're not so concerned about whether there's a global signal that's affecting the USIs, that throughout all of this, we'll just assume that we've mean subtracted these. And so we're not concerned about the bias term in the regression at the moment. The moment. Cool. Okay, so how do we solve this problem? Well, turns out, well, okay, so we do the same thing that you normally would. So you minimize the L2 norm of your two distributions, E and then this alpha D, and you want to find alpha that best fits that. And to do that, then the alpha is going to be the covariance of D and E divided by the variance of D. Divided by the variance of d. And now you can solve for these directly from these equations over here. And so, what you get is that it's the covariance of your measured variables x and y minus some term here that we'll go into. And similarly with the variance minus some residual term involving the noise terms. And so, in particular, the epsilon term turns out to be. Term turns out to be not very important. And so we have covariance terms involving all the noise. So the X noise, the delta noise, and all of these. Now we can calculate it, and it's no problem, but we don't have to worry about that too much. And now for the mu term, the mu term, however, is quite important because it involves the variance of the noise. I apologize, this should be. I apologize, this should be delta here. And this should be D. My apologies. We're looking at X here. Okay, and so this, so the mu indeed involves the variance of the noise, and that can be, of course, substantial. This again turns out to not be very important, but we can measure it all at once in any case. So, how can we find these terms? Well, remember that we're taking the x and y in our case is the Taking the X and Y in our case is the USI that involves the activity over the entire session. And so here we'll assume a slightly simpler. We can take different metrics, of course, but suppose that in this case, instead, we just take the average over some underlying stochastic process where now these again are driven by the same distributions D and E with different noise terms. And in general, these Different noise terms, and in general, these will be much noisier depending on how many trials you've averaged over. Okay, and so here's an example of such a process where I've sort of separated it just for ease of view. And so you can, so each of these horizontal lines. Oh, just as a time check, Joel, how until do I have? You have it at least till 10 past, because we started 10 minutes late. Because we started 10 minutes late. Okay, great. Thank you. So each of these vertical lines now, these vertical lines are the D. This is X. So this is sample one. So in our case, it would be day one. And over here, we have day two. And I should mention that in the real data, we found no evidence of plasticity. We can do this with dynamics as well, but here we'll just stick with stationarity and basically two different time scales. And basically, two different time scales where you have stationarity on one day or in one sample, and you have repeat measurements here. And then you can have non-stationarity from one sample to the next, from one day to the next, for example. And that's what we have been measuring in our project, the correlations from one day to the next. Okay, so these vertical lines are the D. Over here, these vertical lines are the E. So where the wiggly lines Where the wiggly lines are due to the noise delta and xi. And then these dots at the end are the average over the actual noise, essentially. And you can see that in general, even just with 30 trials, it tends to be fairly close, but we'll see the wiggle around that can, even that small wiggle can have a large effect in the correlate in the measured correlations. Okay, so. Okay, so since we have this underlying process, then we thought, well, maybe we can use that to estimate the variance of the noise that we're trying to remove the effects of in order to get a better estimate of the actual linear regression coefficient. Okay, and so to do that, one way we can do, we just use the data itself. Way we can do, we just use the data itself, and we can sample over time, resample using bootstrapping over time, and so over the actual trials. And so we have different samples that result, bootstrap one, two, three, four, et cetera. Each one of them now will result in a different X, large X, and that is the large X are these dots at the end. So you have a distribution now. So, you have a distribution now of large X bootstrapped samples. Okay, well, from that, that will give us now an estimate of the variance of our noise. Okay, so how so? Well, if you take the variance of the X's across each row, so involving each individual neuron, for example, then you get the variance of you get the variance of the underlying d distribution plus oops plus the variance of the of the uh of the noise it should be variance not just mu um but this variance of the d well we uh you know that's just uh simply static we know it and that's zero and so we're just left with again this should be the variance variance of this term and so yeah sorry i just dropped variance everywhere here um in any case um so the variance of mu will So, the variance of μ will then be approximately the average of all of these individual row variances. And so, that's how we approximate it. Where again, this was a couple of errors there. Original term were the D's instead of the E's. Okay, so apologies for those typos. Hopefully, that underlying idea is clear, though. That's basically all we have to do now, and we just plug it into our original. And we just plug it into our original equations that we saw in order to now estimate the alphas and see whether we can rule out stationarity now, for example. It turns out to be a bit more general than that, in fact, as well, fortunately. Okay, so here's a little toy example. So these are the same, this is the same one that we were seeing in the other slides, but this is the actual data from which that was generated. It's not pretty to look at. You can see the means are clustered all around zero. see the means are clustered all around zero okay and then the actual here's uh the actual uh measured um uh correlation and uh and coefficients um so i i guess i didn't mention um we can of course do the same process for correlation but in that case we also have to measure the uh the um variance of e in that case and so we do a similar uh process for that and so throughout here um basically And so, throughout here, basically, now we can have estimators for both the correlation coefficient and the linear regression coefficient alpha. And so, we'll look at both of those now in a number of examples, and then finally in the real data. So here, this first column here is looking at the raw values that we would measure from the averages of this activity, these blue dots here, the average of the rest of the same thing as here. And you can see there's, you know, it's a pretty good. And you can see there's you know it's a pretty good uh correlation, it's around 0.5. Um, the alpha is also around 0.5, but here we actually have access to what the real alpha and correlations are, and we can compare those. And so, in particular, sorry, so for the alpha coefficient, in fact, this was generated. This was generated from a stochastic process where it did not change from one day to the next, from one sample to the next. And so the real alpha is in fact one. The alpha distribution, again, remember that it produces around 0.5, so it's around here. And here's the confidence interval, the distribution, I should say, generated by just bootstrapping over the ROIs. And so that gives you a population estimate for the real. A population estimate for the real alpha, which again is one here. And so it's quite far away. And you would rule out stationarity in this case if you were to do this naively, but of course it is indeed stationary. And so in our case, when we do the steps that we just outlined, instead we get an alpha estimate of around 0.8. So that's pretty close. But importantly, now our prediction interval when we bootstrap over ROIs again. We bootstrap over ROIs again to get an estimate of the population. Alpha in fact does include one. Now it goes up to 1.05. Now that's maybe not super impressive. We'll come back to that in just a moment. But up here, we can also see how we improve our correlation metric as well. Okay, yes, I apologize for this. So our raw correlation metric was 0.5. If we look at the actual correlation between the underlying D and E variables, which remember are sampled from some distribution, then we get around 0.9. And our estimate is around 0.824. Cool. Here's a second example with the same coefficient. So again, stationarity. In this case, we get a much In this case, we get a much better estimate. Again, remember it's one, and our confidence interval is centered exactly around one. So it can be quite good. And we'll come back to that in a moment, or towards the end, I should say, insofar as what sort of confidence can we have in our method. Okay, all right. And so here are a couple of examples where I look more also at More also at how our distribution compares, how our generated distributions compare to what we would get just looking at the D and E variables since we have access to those here again. So the way to read these plots real quick is the blue is the large X. It's been sorted based on its values. So we have 200 elements. These are the values. So these would be, for example, the USI values. It's been sorted. So it's easy to look at. It's been sorted so it's easy to look at because this way now, with y plotting y on top of that using the same sort, we can see how it both follows it, but there's a lot of noise on top of it. If y were exactly equal to x, then it would be exactly that. And we can look also at our underlying distributions, the d and e, where e is equal to alpha d, and we've independently taken alpha from a normal distribution as well. For all of these, we've just used normal. As well. For all of these, we've just used normal distributions for the simulations for the moment, just to keep things simple. And in that case, you get a much closer approximation to what X was or to what D was in this case. So D and alpha D. And again, if alpha were homogeneous, then it would be an even closer match. It would be exactly that, of course. Great. Okay. And so. Okay, and so we can look to see how our metric now how our estimates compare to the initial estimates in the middle column and to the underlying estimates provided by D and E or D and alpha D in the rightmost column. And so again, we have access to the data. And so in all of these, the dark vertical lines represent the true values, the true value over here. Value over here. And then the dashed colored lines represent the estimated values for each distribution. The blue distributions in both are our estimated distributions. So the true alpha here is one. Our alpha estimate is close to one, and it includes one within there. In fact, it is at the 50 at the At the 50th percentile. Sorry, this must be an error. Whereas the average distribution does not at all. That is at the zeroth percentile. So similarly to what we saw before. And now we can also see how it compares to the underlying D and E distributions. And basically, they perform very similarly. They perform very similarly. The estimate is again around 51st percentile to the true population, and the D and E is at the 56th percentile. So they both do a good job of estimating the underlying coefficient of one. And then similarly for the correlation coefficient, the Pearson correlation coefficient. Okay, and let's see. So this is just an example now where alpha is not one, it's 0.5. And just to see that we can indeed rule out stationarity in this case as well. And so here we want to rule out one from our distribution, and indeed we do so. And indeed, we do so. And so it's at the zeroth percentile. And looking at the real distributions, we do again a similarly good job. And again, our Pearson correlation coefficient does a good job of mirroring the underlying D and E. Of course, I should say, you know, the no-free lunch theorem means that we have to lose something in the process. And you can see, indeed, our distributions in general are fatter than Are fatter than the real underlying distribution generated by bootstrapping the D and E. And so our precision, of course, will be lower as a result. But we are, in general, it turns out that this is an unbiased estimate. It'll sometimes lie to the left, sometimes to the right of the true value, whereas the raw distributions are biased and will always lie towards zero. Toward zero. Okay. So let's go back to the back to the data now and recall that we had these correlations that we measured initially. And now we can revisit and ask whether indeed can we say something about whether these are due to stationarity or due to actual underlying systematic changes with those. Underlying systematic changes with those ROIs. And we'll revisit this edge case again as well. Okay, and so we'll start with the bricks, with the visual flow. We'll start on day one, the L23 Somata, and then we'll go to day two. And then finally, we'll go back to the good course. Okay, so that's what we see over here. And great. And so we have two rows here. So this is retunded. Two rows here, so this is uh pretended, but um uh up here uh we have in the uh purple our estimate, and in the gray the estimate that you would get just by looking at the alpha of the x and y rather than the d and e. And over here, same thing, but for the correlation coefficient, the r. Okay, and you can see uh you can see a couple things so our um importantly our Importantly, our alpha value is indeed right around one, and the percentile is around 70%. So, we would say, if this were a confidence interval, if we have confidence in that, and we could say there's about a 70% chance that this arose actually, these correlations over here arose in fact from stationary distribution that day two is equal to day one. Whereas if we were to use that measure from x and y, Use that measure from X and Y, we would not say that. It is being the zeroth percentile. Okay, and then we can also ask, well, can we say something more even about, you know, are these still statistically significant? And to do that, then we generate null distributions. And that's what we have in the bottom rows here. So we have null distributions here centered around zero for both of these. These are the For both of these, these are generated from the estimates in both cases. And indeed, the alpha estimate p-value is zero. Yep, just, you know, so up to, of course, the limits of the number of bootstraps. So I think this is about a thousand bootstraps. So let's say less than 0.001. And then similarly for the Pearson value over here. Now, it's not too surprising, these lay above the distribution. Lay above the distributions in both of those cases as well. The story is about the same for day two to three for the L23 Somada for the bricks. Again, we are right around one. And so finally, we can look at the Gabors. And in this case, the first one I want to actually point our attention to is this bottom right. Our attention to is this bottom right. And so remember, this is an edge case, but with our estimated correlation value and the estimated permutation distribution, we actually get a very strongly significant result, again, less than 0.001. Now, I should mention that all of these were computed also with Bonferroni corrections, but this will survive. So, but this will survive any such multiple comparison corrections as well. But that will be the next step to give the actual Bonferoni corrected value. Might need a few more permutations for that. Okay, and so fine. So we can now say, indeed, perhaps not an edge case, indeed, this does look statistically significant. Is it stationary or not? And indeed, now we can look up here and say, no, it is not stationary at all. In fact, we see that the We see that the estimated and the measured distributions very much overlap in this case. And that shouldn't be too surprising. And why is that? Well, with the bricks, there was a lot of noise because they're moving all over the place rather than getting stationary images flashed at you. Plus, we were able to use a lot more Gabor data in order to generate these. So the averages actually do help denoise quite a bit. And so the improvements that we Quite a bit. And so the improvements that we get are indeed less than we would get otherwise. But even with that, we still get improvements enough in edge cases to be able to potentially disambiguate those scenarios. And so indeed, so now we would contend that we can add this, add some stars to this and say we do have statistically significant correlations in the dendrites. In the dendrites that are positive and non-stationary from day two to day three, indicating further systematic changes to the ROIs that may be driven by these USI signals. Whereas for the BRICS, now adding to the evidence that we already have in the preprint, there seems to be less such changes and learning occurring in the visual flow. And both of these, we would contend, we can. We would contend we can rule out as being, in fact, due to stationarity and not evolving in time. And so we've been talking about prediction intervals. Just to wrap this up then, can we say something a little bit more? And I should say this is preliminary, need to dig into this a little bit more. But here's what we can do is, oh, and I left one slide out. Left one slide out. So, what we can do is ask: what is the percentile for the alpha estimate for the actual population value again that we have access to in these simulations, not in the real data. And so we can just run another loop and ask what is the alpha percentile for the population level for the alphas for both the estimated distributions. Estimated distributions and the actual DNE distributions. Remember, those are sampled from a Gaussian distribution, from Gaussian distributions. And here, that's what we have over here. So if these were perfect, so these are the percentiles. And if this were perfect, then you would expect this to be exactly a horizontal line at one. Horizontal line at one so that each percentile occurred that percentage of the time. So, what we have here is that the real alpha is in the 95% prediction interval. So just go down to 0.05 and integrate the remainder of this. And it is, in fact, in there 93.1% of the time. Yes. And whereas if we do it for the average, or I should say the raw alpha value that you get from measuring the actual X and Y, then it is not in that prediction interval almost at all, only 3% of the time. And so it looks like. So it looks like I did this for the Pearson as no. So this is, I apologize. I've been working on finishing this up over the last few days. Like I said, this is a bit preliminary. So I apologize. This label is wrong. And so, in fact, this is for the alpha estimate. And this is the alpha that arises. Is the alpha that arises from the D and E. And so, what is cool is that even though this isn't perfect, it's quite similar between these. And so, basically, our estimator is doing about as well as it possibly could based on the underlying D and E distributions. Okay, and then this is just to show that if you do want to get a 95% confidence in. To get a 95% confidence interval, well, you might just have to just increase your percentage a bit. And so we're working on this a bit more. But basically, this is to say that the confidence intervals, the prediction intervals that we have do indeed seem to do a very good job of approximating real confidence intervals that we could say with confidence we can rule out stationarity if it's not within the prediction interval that we have. Okay. Okay, and so that's about it. So we saw that, we saw evidence that visual cortex may instantiate predictive hierarchical model. And then we looked at individual neuronal element selectivity and saw that in different cases, like we saw before, even if they were negatively correlated. And now we extended to the positive correlation case that indeed the elements may drop. Indeed, the elements may drive changes to their selectivities over multiple days. And then we developed this estimator, and this is what I'm quite interested in your feedback on. I just mentioned again, this is a bit out of my element, been scrambling to catch up with a lot of stuff throughout all of this. And from what we can tell, I have not been able to find this method out there before. If it is, then great, we'll just cite it. We'll just cite it. But if you all have thoughts on that, that would be great. And it's certainly quite generic and can be used in many different situations. We've looked at R and R squared values quite a bit over the last few days. And as long as you have, as long as your variables are taken from some metric on some other underlying variables, like repeated measurements, for example, then you should be able to apply this and it might be able to give you a better. It might be able to give you a better estimate of what your regression actually is. And then I only showed the simple bivariate case, but it also extends directly to multivariate nonlinear. And we'll be looking at that as well. Okay. Cool. Thanks for your time. Sorry, it's a couple over. Thanks, Jay. We got time for maybe one question if it's not too long before we're supposed to wrap up. Before we're supposed to wrap up this session, any questions for Dr. J? All right, I will take that as a no, which is fair enough. We're also running a bit long. So let's wrap up here. So let's wrap up here. And reminder: there's a informal gathering in Gathertown now for the next 45 minutes. If you want to go there, find your friends and meet up and chat a bit. I'm encouraged to do that. And then we'll meet back here in this same Zoom room tomorrow morning for the continuation of our workshop. And thanks to all the speakers. It's been a fun day. Cheers. 