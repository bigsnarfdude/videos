So this is, since there's a stable here, so this is joint work with Masha and with Laurence Olavkost at Cornell. And some of you I know have seen a version of this talk before, or maybe an earlier version of this sort of, I guess, once it's more than one paper, you call it a program instead of a project. But so maybe the difference between what you may have seen before and this one is that now there's not an S in front of the U there. In front of the U there. So I'll tell you that this is an extension of some previous work and some of the reasons why that extension was important. I think for this talk, and also because of the audience, and because some of you know a little bit about this project, I'd like on this talk to maybe dig a little bit more into the details than on some things, because a lot of the techniques in this group are sort of very, very hands-on geometry compared to some of the more high-level things that show up elsewhere. Some of the more high-level things that show up elsewhere. I think there may be sort of interest to see how these things work kind of a low level. So a brief outline. I'll tell you what we mean when we say util front doubling and why we care about it, some of the significant consequences of it. How we proved it for SU2, the special unitary group in a project and paper published a few years back, how that we've How that we've now extended it to YouTube, so one dimension higher. And in fact, some other groups came along for the ride. And then a little bit at the end about what's next and where we hope to go from here. Hearing any ideas that anyone else has covered. So, a bit of the setup. Throughout this talk, G is a lead group connected in unimodular. E will be the identity of it. Little G will denote our left-invariant Romanian metric on this group. And let's just emphasize that, unlike in many other situations, the metric will not be fixed throughout this talk. The whole point of this talk is that we want a lot of metric to vary and study what happens or what changes or what doesn't change because. Mentioning that whenever you have a left-invariant Riemannian metric on a group, if you restrict metric on a group. If you restrict it to the identity, if you think of the tangent space of the identity of your group as the Lie algebra, this metric induces an inner product on the Lie algebra. And conversely, given an inner product on a Lie algebra, you translate it around to get a metric. So these things are basically the same object. You can view it either. And then everywhere in this talk, we talk about anything like Riemannian distance, Riemannian volume, a ball, the gradient, the Laplacian, all of these are with respect. All of these are with respect to whatever metric G we happen to be thinking about. Typically, that'll be with respect to some varying metric, some arbitrary metric. Also let, this isn't standard notation, but frac L of G we're going to use to denote the set of all left-in-variable Monty eventual expressions, or equivalently the set of all possible inner products on this direction. So, what we call. So, what we mean by volume doubling, this is a very familiar concept to, I think, many people have worked in geometry, but just to repeat the definition, we would say that a particular group equipped with a particular Riemannian metric, whose volume doubling the constant D sub-Wittlergen just emphasized the dependence on the metric. If whenever you double the volume, double the radius of a ball, that its Riemannian volume increases by a most effective in a bit. Backward independent of the radius. So, sort of overall the rate back. Just to emphasize that I've put the ball centered at the identity here, but of course, because the metric is left invariant, I think centered at any other point in the mid-change. Just to get rid of one bit of dependence on the metric, we're on a unimodular group. The Riemannian volume is left invariant because the metric is, and therefore the Riemannian volume is just some. Volume is just some constant multiple of the hard measure, which is unique up to scale. So I might as well, since it's on both sizes in the quality, I might as well replace ball g by just any fixed normalization of the hard measure, which is in fact biomedical for the reason before. So just in future, I may write mu of V G, or mu of B and mu of B on both sides, or mu is just any your favorite multiple of the harper. Okay. Okay, so doubling is always an interesting property, right? This shows up throughout the, you know, in so many places in geometry, stochastic analysis, metric measure spaces in particular. This is a very heavily studied property. In the context of a Lie group, it's particularly interesting. And the starting point for that is the following: that if you have a Lie group that's doubling, or a particular metric on Lead group that's doubling. Then you get a Poincaré inequality of the following form. It says that you look at the L2 difference between L2 distance between the function and its average over a ball. You can bound that by the L2 over the gradient. And that the constant you get, here's the important part. The constant you get is not just any old constant, it's actually the doubling constant, or times of two. So doubling actually implies a quark array in the context of all the methods. Let me just Let me just emphasize here. I haven't, I've dropped the subscript G's to clean up the notation a little bit, but the ball, the volume, the gradient, all of those are with respect to the metric. So this is great. I mean, doubling is great by itself, but doubling plus Poincaré, if you have studied in this area a little bit, that's when you're really in business. So once you have doubling a pancreas, Once you have doubling a Poincaré, then the doors sort of open to sort of what you might call a constellation of other functional inequalities. So I'll just list a few on this slide. We go through them in more detail in our paper from 2018 on SU2. But for instance, if you have doubling and Poincaré in some generality, then you get, for example, a parabolic carnect of the coral, saying that a positive solution of the heat equation. Of the heat equation, that the ratio between the soup and the inf over a slightly earlier time and a slightly later time is controlled by a constant. And in general, the constant may depend on the metric, but I'll say a little bit more about that. Once you have a pair of Carnik inequality, you can get things like key kernel bounds. I believe we have an upper bound here, but you can also get lower bounds, time-derivative bounds. This particular, this one you'd get. In this particular, this one you get a sharp constant in the exponent. Of course, there's a multiplicative constant out front, and there's a power here on this polynomial factor. In general, depends on the metric, as does this constant out front. When your group is compact, there's a few more things you can say. You can estimate the spectral gap in terms of the diameter of the group with, again, a constant that in general depends on the measure. It in general depends on the metric. The diameter being with respect to the metric we're talking about. This is sort of interesting because there's a universal lower bound for homogeneous spaces that looks exactly the same as this, but where the constant is the universal constant, I think, pi-squared over. That's a famous result of Peter Lee back in the 80s. So essentially, you get the upper bound that matches the lower bound. Here it looks a little bit odd to say that there's a constant, depending on the metric, whenever. Constant depending on the metric when everything depends on the metric. But I'll say more about that in a second. The spectral gap, roughly, you expect to tell you about the rate of convergence of, say, the Brownian motion. So as Todd pointed out, the defining feature of this conference seems to be that everyone has to like Brownie motion, so I need to say at least one thing about it in this talk. So you can think of the heat kernel, the difference between the heat kernel and the The heat kernel and the one over the volume, the constantly converges to as a measure of how close the Brownian motion is to its equilibrium distribution. So the exponent here is the spectral gap. That's not surprising. But in general, you expect to see a constant out front. That constant may in general depend on the metric. But the thing that I want to emphasize through all of this is that all of these are proved using doubling of quantum. All of these are proved using doubling and pointer. And if you, and these come from various places around the literature, but if you look through them carefully, you find that actually the fact that they follow from doubling of Point-Marie is not only qualitative, but quantitative. But the constants that appear in all these things, I said they depend on the metric, and they do, but they only depend on it through the doubling of Point-Marie constants. And therefore, in this 4 in this setting, those are the same constant, mod a factor of 2, which nobody cares about. So that says that everything on the previous two slides, all the constants that appeared to depend on g, actually only depend on the doubling constant. What is really special about the lead group of case that there's only the doubling constant, not two independent doubling and Poincaré constants? Really the Really the homogeneity. The fact that there's it gets used in, I guess, the combination of the invariance of everything, basically. Everything can be shifted to any point and not have any. So there's no dependence on base points. But I mean, that's sort of a non- I mean, that's sort of a non-answer because that's almost the definition of a lead group again. But essentially, this, you know, the very strong structure of this homogeneity, this what lets this all work. I don't know if there's a really fifthy way to summarize the key idea. It's not long. No, the general derivation of Poincare from doubling. In a generic setting, though. Other questions for now? So, therefore, this suggests that we're very interested in what the doubling constant actually is, and maybe in particular how it depends on the metric. And maybe the best possible And maybe the best possible case would be that in some sense it doesn't depend at all. So, suppose, hypothetically, that for some group we were able to show that there was actually a uniform upper bound on all the doubling constants over all the left-hand variant modeling interests. So now this doubling constant would be a function only of the group rather than of the metric. So, for that, we've coined the phrase uniformly doubling for maybe obvious reasons. And then that would mean that everything on the foregoing slides, all these functional equalities, all the C sub G's now don't depend on little g at all. They only depend on big G. They only depend on the group you're working on. So if that property is true of your group, then you have these functional inequalities where the constant doesn't depend on which metric you choose. So that's about, that makes them, you can think of it as highly robust. You could adjust the metric in any way you want. You could adjust the metric in any way you like, and that constant effectively would not change at all. I'm not saying the best constant wouldn't change, but that there is a constant that would work at all. As Sasha alluded to in the talk yesterday, it's also the case that essentially because sub-Riemannian metrics on a lead group are limits of the Riemannian ones, that if you have a That if you have a uniform bound on the doubling constants of all the Riemannian metrics, you also get one double sub-Riemannian metrics. It covers the Riemannian and sub-Riemannian geometries that are adapted to the best culture. Do you have this uniform upper bound, but I have no lower bound? No positive lower bound? Could the optimal ones be as small as you want, perhaps? The lower bound is trivially at least two to the dimension. Because on a very small scale, it looks like Euclid. Because on a very small scale it looks like Euclidean space. Yeah. I don't know if there's, it might be possible to lower bound away from that, but I'm not sure. Okay, so that was a nice hypothesis. Is it amacuous? So for which groups are uniformly doubling? Okay, well trivially, abelianly groups, right? How Rn is doubling. Rn is doubling. The doubling constant is 2 to the n. Essentially, because there is only one left-invariant Ramanian metric on Rn of Polisometry. So whatever metric you're using, you double the radius of a ball, and the volume increases by exactly a factor of two of the n. The same is true for tori by a relatively simple argument. So abelian new groups are not very interesting here, but they at least they satisfy it, but that's. If they satisfy it, then that's of no real interest. So, for non-trivial examples, one thing that you might think of trying from sort of a more classical geometry perspective, that won't work. So, one of the ways that you obtain doubling in sort of classical harmonic geometry is the Arici curvature lower bound. So, the Bishop-Gromov comparison theorem tells you. Theorem tells you how you can, what you gives you a bound essentially for the doubling constant in terms of Ricci curvature lower bound. So you might think, okay, well, then maybe we should try to find a uniform Ricci curvature lower bound over all the Lithuanian mind metrics. Well, that won't work because typically there is no such bound. You can, it's, I don't know if, I'm not sure if it's true on literally every group, but typically you are able to, without much difficulty, construct. To, without much difficulty, construct left-invariant metrics for which the Ricci curvature in at least some directions is arbitrarily. So, you're not going to find a uniform Ricci lower bound. That's not going to work. But there are, there is, as of 2018, we have found one example. So, we were able to prove that this is the case for the special unitary group SCU2. So, that's the 2x2 complex unitary matrices of determinant 1. We were able to show that this. One, you were able to show that this is uniformly doubling, so that there's a uniform upper bound on all the doubling constants for all the left and right. And I'll tell you a little bit about that result today, but I'll also tell you about six years later, another step. We are now able to show this for the unitary group U2. So that's all the two bytes unitary matrices with no restriction on the determinant. Restriction on the determinant. In fact, we get a little bit more based on the way we prove this. We actually get, you can characterize it as all the Lie groups whose universal cover is a product of SU2 with Rn. So that's the case for U2. It's a quotient of SU2 cross R. Yes? Can I ask a question? What about SU2 cross SU2? Is that we don't know. Not so obvious, right? Not so obvious, right? That's my next slide. Okay. As to why it's not so obvious. Because, yeah, it's. It doesn't help with products. It doesn't, sadly. I'll mention a rather admittedly bold conjecture that we posed in the 2018 paper that I think we're still standing by. There's not really any reason to think that any group wouldn't have this property. That any group wouldn't have this property. So it's reasonable, we think, to conjecture that actually every compact connectivity group may have this uniform doubling property. Despite, so obviously it appears that we're extrapolating from two examples to everything. But there are heuristic reasons to think that this is plausible. Very roughly because when you, what are the limits of Riemannian metrics on a lead group? On a lead group. Well, if you ascend one direction to infinity, you may get a suburmanium metric after descaling. If you ascend one direction to zero, you may get a homogeneous space after descaling. Compact subramanian manifolds are doubling. Several metrics on groups are doubling. Compact homogeneous spaces are doubling. And so essentially, there isn't any way to approach. Any way to approach a metric, a non-doubling metric from using Ramanian metrics. And so that's a suggestion that maybe there's not actually a way to make the doubling constants blow up. That is not a proof because there is no good sense that we know of to be sure that the doubling const varies continuously with these levels. Other than trivial ones where you adopt a topology that requires the doubling content. Where you adopt a topology that requires the doubling constant to be controlled, but then you can't show that all the limits are counted for. So that's at least a heuristic reason that we tend to believe this conjecture, but admittedly the rigorous evidence we have for it is certainly, we'd certainly like to have more evidence for that. Okay. So, as a maybe just to say a little bit about this issue here, I think Brian's question is addressed on the following slide. So one thing to know, one property that does preserve this uniform, one operation that does preserve this uniform doubling property is quotients. So if you have a group that has this uniform doubling property, That has this uniform doubling property. And if you mod out by a closed normal subgroup to get a quotient group H, then that group is also going to be uniformly doubling. And somewhat sloppily here, but the doubling constant of H will be no more than, say, the square of the doubling constant of the group upstairs. So I cite that as folklore. This seems to be something that people were generally aware of. That people were generally aware of, but we never saw it explicitly written down. So we do have a proof written up in this forthcoming paper. Essentially, the idea of it is that given a metric downstairs, you can lift it to a metric H in such a way that the quotient map is a submersion. That in particular means that it maps falls to balls. And then there's a rather clever, and I don't, maybe not as well known as it might be, an argument, a lemma in a 1973 paper. In a 1973 paper of Ibarsch, that's really just Fubini's theorem used in a clever way, where you look at a carefully chosen subset of the product of these two groups, and by essentially estimating its volume by slicing it horizontally versus vertically, you're able to get a lower bound on one side, an upper bound on the other side that, when rearranged, tell you the ratio between the balls of 2R and R down. Between the balls of 2R and R downstairs, is at most the ratio of the balls of, say, 3R and R upstairs. And if this 3 were a 4, then the number over here would be exactly the double constant squared. So the power here probably should be 3 halves or something. Or sorry, is it 3 halves or is it log 2 of 3? Not in my head on the spot, but something a little less than 2, but 2 is certainly going to happen. So, doubling does pass to quotients. That's nice. And so, therefore, if you wanted to prove that U2 is uniformly doubling, it would suffice to show that SU2 cross R is uniformly doubling, or more generally, say SU2 cross R. So as Brian was hinting, that sounds like that would be trivial because we already showed SU2 is doubling. Rn is trivially doubling, uniformly doubling, and direct product is about. And direct product is about the simplest operation on each you can imagine, right? We're letting the two factors commute with each other. The problem is that we're taking, essentially we're taking the product in the category of groups, but not in the product of homonymic manifolds. So what you do get, what is trivial and what you might have in mind if you're thinking that this is, if I was to look at the product of, I was to equip each one with a metric. One with a metric and look at the product as a product of the Ramanian manifolds. So, equipping the product group with the metric that made their two tangent spaces orthogonal, that would certainly be double with something like the, at worst, like the product of the squares of the constants, something like that. So, what is trivial is that if you look at all those metrics which decompose in this way, which make those Decompose in this way, which make those two tangent space corfolical, there's certainly a uniform bound on their doubling constants if you have uniform doubling on the two. But that's not enough, because that does not account for all the left-invariant metrics on this product group. In particular, the two tangent spaces certainly not only need not be orthogonal, they may be arbitrarily, sort of acutely angled between them. And the trivial bound that you get, That you get would blow up as that angle goes to zoom. It's certainly reasonable to believe that, in general, a product of uniformly doubling groups is doubling as far as certainly I think so, haven't, I won't hold my co-authors back because I haven't discussed it with them. Yes. No kind of ergodic decomposition argument would help at all either. What do you mean by that? So if you have a measure of maybe. If you have a measure, maybe you can write it as an integral over some nicer measure, like where the measures in the composition vary. I think we're trying to. I would imagine so. I do. Oh, good. Okay, so something else that doesn't work. Okay. So, right. So, right, so that's certainly, I think, reasonable to believe, but there's no proof of that that we know of. That would be a nice thing to prove, but so can I ask another probably dumb question? So, because they're left and grand, we're just really talking about an inner product like the algebra. So, there's a smallest angle, right? It's not like the, so you still think of the angle going to zero. So, you still think of the angle going to zero, and I don't see how it can. So, for each metric on the product, there is an angle between the two factors. Yeah. We are considering all metrics, and so the unifimum of the. Each one has a smallest angle. Yes, right, like there's no uniform smallest. Yep, exactly. Okay, so I want to say a little bit about, I want to start by outlining how we proved this theorem for SU2. This theorem for SU2. And I want to sort of give a proof in hindsight. So, in the process of studying U2, or this next step, we were able to, and sort of maybe one of the reasons why it's been such a long project is we kept finding more and more ways to streamline it. And so, in sort of them rewriting the paper every time. So, what's here is not the argument that's in our. What's here is not the argument that's in our 2018 paper. Sort of hindsight were able to simplify it quite a bit. And so, this, I think, maybe captures the essential ideas a little bit more directly and gets you more directly to what looks like a proof. But the starting point, certainly all along, has been the idea of what are called Miller bases. So, we have to be studying all the possible metrics on SU2. SQ2, and it'd be nice to parametrize that space in some way. And then, so what we're able to do is once parametrizing that, then what we do is we actually compute estimates for the volume. We say the volume is bounded above and below by some much simpler-looking function and the bounded between two constants times that function. And that simpler function, trivially, you can plug into our function trivially you can plug in 2R and R and divide and see that the ratio is no more than 8 or something. And so essentially we literally determine what the volume is for all these, up to constants, for all these metrics in terms of this parametrization. And so that parametrization is based on what we call the Milner basis because Have called a Milner basis because it goes back to the work of Milner in the 70s. And it says that once you have given any metric, you can select a basis for it, an orthogonal basis for it, that has these very nice, pretty Lie bracket relations. So we're thinking here of the metric as an inner product on the Lie algebra. So we take the basis for the Lie algebra, orthogonal for that metric, and such you have this. And such, you have this relation where the lead bracket of any two is the third and the sixth fashion. So, if it's orthogonal, well, I didn't say orthonormal. So each of them has some norm, some non-unity norm with respect to that inner product. But if you know what those three values are, call them A1, A2, and A3, those actually determine the metric of. 283, those actually determine the metric up to isometric isomorphism. So we now have a parameterization of all this, all these metrics as a three-trainer now. You can think, if you like, of this AI as these U1s, you can think of them as left-in-variant vector fields on the group, if you like. And then these constants A1, A2, A3 are telling you essentially the cost this metric imposes on moving infinitesimally in those three different directions. In those three different directions, away from the value. So, a larger value means a higher cost, and therefore means the volume is going to be smaller in that direction. So, I'll start with saying a little bit about how volume-lower bounds work. So, we have a metric with these three parameters. The way that we The way that we estimate the volume of a ball from below is to try to identify sort of a lot of group elements that are inside it in such a way that we can bound the volume of that set. I'll mention that one thing we explicitly tried to avoid was trying to actually explicitly compute the ball or try to precisely characterize all the points of the ball, perhaps by computing geodesics. Largely because we have in mind that we expect this to be true for more groups. We expect this to be true for more groups, and we'd like methods that are as robust as possible. Certainly, what's here does still rely on the algebraic structure of this group in very significant ways, but for us, trying to go to geodesics was just maybe, A, seem too much like hard work. And B, you know, sort of would take us a little too far into the very fine structure of this group that we wanted to try to be more. This group that we wanted to try to be more general than that. So, a question you could ask is: if you look at elements of the form exponentials of, say, the first basis vector, u1. So, elements of the form e to the sum scalar x1 times u1. For which values of that scalar do you have an element of that ball? Well, trivially, if you just look at the path that follows that. That follows that vector field, the flow of that vector field. Since that vector field has a constant norm, that's going to have constant speed A1. And so therefore you have a path from of length less than R to any such point as long as X is less than R over A1. Let me just point out that the metric is not bi-invariant, so this path is not geodesic. If it were bi-invariant, it would be, but But it's likely not the shortest path, as we'll see in a second, but it's certainly a path to that point. But the other thing you can do, that's not the only way, that's not necessarily always the best way to reach a given point. Here we thought, here we're inspired by Severmanni geometry. Another way to produce an element of the form e to the something times u1 is essentially via the Baker-Campbell Housework Think. Essentially, via the Baker Campbell Housework Thinkin formula, try to write it as a commutator of elements that are exponentials of the other two vectors, u2 and u3. Because of the fact, oh, I didn't write it there, because of the fact that the bracket of u2 with u3 is u1, the Baker-Campbell-Hausdorff formula, Campbell-Hausdorff Deken formula, the leading term, there's an expansion for what goes in the X1 here, the leading term of it. For what goes in the X1 here. The leading term of it is SSP1. So that suggests that if you want to get something of this form, you could produce it by a product of elements of this form, and maybe in such a way that you were able to bound the distance to it more precisely. And this would be the way you would do it in subvermontian geometry, right? If, for example, U1 were not in a horizontal space, you would have no choice, then this path would be. You know, this then this path wouldn't be admissible at all. That wouldn't just correspond in sense as A1 being infinity. And so you would sort of have to instead produce this element, the other container. And so even though we're in modian geometry where that's not necessary, it may still be better, depending on the relative values of these parameters. So if you do that, what do you get to say? Well, if each of these four elements were in the ball of radius r. were in the Volov radius r over 4, because of left invariance, that would put their product in the Volov radius R. That would be the case if the coefficient on U2 were less than R over, say, 4A2, whatever the length of U2 vector was. Similarly, for the U3 coefficient. And so that would tell you that you would get this element to be in that ball as long as the scalar is less than something like r squared divided by the other two. Divided by the other two. And so combining those two bounds, you get a bunch of elements of the ball for x1s that are smaller than the larger of the two ones. And as you can maybe, as you can sort of see, which of these two wins will depend on the relative values of the radius and the different parameters. So there's certainly cases where this is better. And so that improves the estimate. So that improves the estimate that we've got. Similarly, for the other two, just permute the indices around, everything is cyclic. Okay, I lied somewhat because it's an approximately equals here. There's a remainder term. In our original paper, we spent quite a bit of effort on controlling that remainder term. We're now able to bypass that. If instead of just writing this as a product of four factors, there's a somewhat longer formula that we were able to derive. We were able to derive where you're, if instead of four factors related by seven, then you can actually do this in such a way that there is no remainder, or that the remainder term only involves more than you once. And so that there are no, in some sense, cross-terms in this. And so that essentially lets you write this thing exactly as a product of U2 and U3 exponentials, and therefore not have a remainder. But I'll say that everything here, and especially this somewhat more complicated formula, depends very heavily on its brag relations working on. So we really are still fairly tied to the structure of SU2 via this nice brag relations that we have on the node basis. Yes. Can you tell us what are those parameters A12 say geometrically? A12 state geometrically. It's like one of them, the size of the circles and the hop vibration or something. No, they're just the. Oh, I see. They are, yeah, so if you. What did you say, Pasha? I think this one is the line. Oh, right, right, right, in that sense, yes. For SU2, right, A1 turns out to. turns out to yeah with the length of the longest close shortest close GFS I thought was the shortest part image shortest I think A2 turns out to be the diameter and I think the product of all three of them is the volume so you can extract them from more sort of directly geometric Directly GMS. K1 is the smallest. Right, I didn't say that. In a lot of what we do, you can assume without loss of generative automatic that they're going to program. So I should say the smallest one is the length of the shortest, lowest geodesic. Yeah. So it's quite explicit. So you send A1 to 0, then you collapse to the 2 sphere. Exactly. Exactly. Yes. And again, our. And again, our 2018 paper makes heavy use of that. We don't need it quite as much in this. Okay. So as far as how you finish the lower bounds from there, well, so now it's possible to write elements in general as a product of three of these things, to use the so-called coordinates of the second kind. This is the so-called coordinates of the second kind on this group. And so, since I now know which elements of each of these forms is in a ball, I can tell you for which values of x1, x2, and x3 I know for sure that this object is in a ball of radius 3r. And so I've shown that this coordinate map phi maps to a certain box inside that ball. Obviously, I compute the volume of a box in R3. That's just multiplication. If you chop off these values at a certain point, you keep the thing from wrapping around itself so it stays injective, and you can bound the Jacobian determinant from below. And so that's how you prove it. You now have a, you bound the volume of a ball below by the volume of a box in the coordinate chart. And we know exactly the dimensions of that box. So that gives you an estimate. That gives you an estimate or something like that. Again, this can be simplified a little bit if you assume without loss of generality that the AIs are in a certain order. It's the smallest, medium, largest. It looks a little more symmetric, this will be. Okay, as far as upper bounds, very briefly, the idea is to, instead of putting a box inside a ball, to put the ball inside a box. So given an element of the ball, So, given an element of the ball, you try to show, you try to say, if I were to write it in coordinates, what could I say about the three coordinates? It's easy to bound the Jacobian from above, that's even easier, so that would give you a bound, an upper bound on the volume. So, that can be done if you fix a path from at least to that point. From the least to that point, write it in the coordinates. And then, without going into too much detail, but essentially, as showed up in Marty's talk, I believe, pull everything back to the identity look, which is the so-called Mara-Cartan form. I want to go through this a little bit faster. I want to talk about some more things. But essentially, what you end up with is a system of ordinary differential equations for these three coordinate functions driven by the coordinates of the Marvel. The coordinates of the Mara-Cartan form of the original path, which, if you have parametrized it at constant speed, you can bound each of those things. So, with a little bit of simple a priori estimates, you're able to say that essentially the coordinates of the endpoint of that path could be at most some number in terms of the parameters, and those bounds turn out to exactly match the lower bounds. And so, that gives us upper and lower bounds on the Okay, I really do want to say a little bit about how we extend this. So, when we try to extend this to SU2 cross Rn, now our Lie algebra has this additional Rn suband, you could still find a molar basis for the SU2 part. That would not be as productive in achieving this goal of getting a basis with nice bracket relations, because when you try to extend it orthogonally, Orthogonally, the vectors that you get may not be in the center, and so they may have non-trivial relations with the E1, E2, and E3, and that makes things messy. So what we do instead is we sort of tilt everything a little bit. We still want things to be orthogonal to this Rn element, this Rn submn, which is the center. And so we're able to find a different basis that's actually related to a Milner basis. A Millennium basis. They're still orthogonal to each other, orthogonal to the center, and are written as something that's in Millennium basis plus something that's in the center. And that means that when you bracket two of them, the terms in the center vanish because they're in the center, the brackets just in row. And so you get back to the U, which has, and we know what those bracket relations are with each other because we can struggle. Also, these vectors in the Rn component, we have a bit of a lifting argument that says you can actually make those orthonormal for the metric you're working with. The cost of just having one scalar here. So there end up now being four parameters of the metric. And we're no longer considering all possible metrics, but what's the case is that every metric is a quotient of one of this form. We already know the double constant quotients. Double cases to quotients. So, in this case, for lower bounds, we have to do a bit more work because if you want to know which elements, so now sort of the V1 and F1 go together. You have a direction in the SU2 sum and you have a direction in the center that are sort of coupled together. Okay, well, as before, since you know the length of this vector, you You know that things of this form are in your ball. You also know that you can say something similar about the vector in the center. You can take advantage of brackets as before, since a bracket of v2, v3 is now u1, you can get things of the form e to the t u1, and u1 is a linear combination of u1. So, however, that's now not all, so we don't get to now stop. Now, not all, so we don't get to now stop at two-fold brackets. We have to iterate one more step. Because you could also have a three-fold bracket and get a bound that looks like this. And it turns out in this setting now, it's not clear which of these wins over the others. There's no, you can't. In previous cases, you could still try this graph, but it turns out not to be any better than the other recipes. In this case, sometimes it is. It is. So, what you end up with is several different ways of getting elements that are exponentials in this sort of two-dimensional, of this two-dimensional subspace of the Lie algebra. Essentially, you can go sort of either vertically, horizontally, or diagonally. And so, by being able to go by known amounts in each of those directions, you end up sweeping out a shape that looks kind of like this. It's, you know, this is how. It's, you know, this is how far you can get by going this far horizontally plus this far vertically plus this far horizontally plus this far diagonally sweep out anything in this hexagonal shape, which also you can think of as being the projection of a rectangular box. Actually, since SU2 is compact and you go around it, you eventually come back to the same place. Eventually, come back to the same place. One of these directions is actually periodic. And so the shape that you end up working with in the coordinate chart is not actually this hexagon, but the result of, oops, the result of wrapping it around a cylinder, modding it out by making it periodic. And so what you end up having to do to take advantage of the estimating volume upstairs in the coordinate chart is you now have to estimate the volume of one of these shapes. And because they can overlap, of one of these shapes. And because they can overlap themselves, that becomes a little bit not certainly still elementary, right? A little bit um a little bit more work. Okay, so in my last few seconds, if we have to compute those areas, that ends up giving us lower bounds. The technique for upper bounds ends up being similar to this idea of looking at the more carton forms of getting headed. A system of ODEs, and you end up being able to say that an element of the ball can be written as, again, something, you know, the image under this coordinate chart is something in a set that looks similar to the one. Okay, I have one last slide. Just some future directions. So, certainly, we would like more examples of uniformly doubling groups. We had one, we now have, in some sense, two, I mean, with some extras that came along with the rise. I mean, with some extras that came along with the ride. So, anything that's a quotient of SU2 costs R is the sound that we thought. Obviously, more would be good, preferably lots more. Something like SU2 cross SU2, as you suggested, would certainly be nice. Things like SUN, lots of other things to try next. Since this idea of having a nice basis was so essential to our arguments so far, it would be interesting. Essential to our arguments so far, it would be interesting to investigate when that happens. So, what kind of Lie algebras have, you know, is it the case that you can always simply diagonalize a metric in terms of a basis whose graph relations are not too complicated, whatever, whatever. Certainly, we have no way of making that precise yet, but something like that. And maybe just going back to the context. And maybe just going back to the consequences, the sorts of inequalities we wrote down are classically often proved using Ricci bounds, but it's very interesting in this case that you can prove them directly from doubling and pompé instead or just from doubling in the case of a group. And so maybe this could inspire sort of further motivation to think of doubling as being the fundamental property, to think of even in Romanian geometry. Property, even in Ramanian geometry, more so than she mounts, perhaps when possible. And just generally to have more general and robust techniques, things like be able to use products or semi-direct products or other ways to be able to build our, to sort of be able to continue building on what we have to get more examples and expand further the classrooms that we're talking about. Okay, that is all I have. Thank you very much. That was all I had. Thank you very much. Questions for Ed? I have a couple of questions. The first question is, is there a general result of the form that if the universal cover is doubling, then the group is This one. Oh, okay, I see. And the other one is: do we know of a Lie group that's not uniform to Dublin? No. Or we wouldn't have made the conjecture. But compact, and your conjecture was for the compact. Oh, I said I swear I thought SL2R? I mean, there's one to the I mean, there's ones that are not doubling at all, right? Yeah, but we also know something about important ones. Yeah. Compact was good because we also have more analytical consequences. Right. Like while. In general, it could be non-compact, but do we know of something that's less non-compact? Yeah, yeah, so like I'm remembering right, I think SL2R has exponential volume, but R has exponential volume growth. So it's not none of the contributions. Yeah. Yes. Do you have some heuristic evidence why you believe that this is true or not? More or less what I said, that the limits of Vermontian metrics are things that are themselves doubling. That you can't make the geometry converge to something that's not double. So essentially, the only way to, if you had a family of, if this wasn't uniformly doubling, you would be able to find a family of metrics for which the doubling constant blew up to infinity, a sequence, let's say. It would have to be the case that that sequence, you know, that along that sequence, the That along that sequence, the geometry blew up in some way that couldn't be fixed by rescaling. It could converge neither to a quotient nor to a separate monogenometric. So since we don't know of any, we can't think of any good way for that to happen, that's maybe some heuristic. Does your problem reduce of left invariant sub-Riemannian structures which are all normal with respect to the Riemannian metric? If you could show that those were all normal with respect to the ones. About submarine structures, but I guess we could always take the theme, you know, the very vector fields, make up the supermodel structure, and branch it with respect to the remote. Oh, I see. Just introduce non-degeneracy into the choice of the I think that's the same thing. I think that gives more remote metrics. I think it goes about the same problem with classifying. Problem with classifying all different metrics, like you don't have minimal type based, because then you would have to have some kind of exclusive description of all possible submind structures, and that would include different generating sets and things like that. And it really probably is the same as finding newer-like bases for your code. Take SUM or SOM and come up with different generators. I mean, from under type. I think at least in this setting, I think it's actually true that any Romania metric could be realized as a Romania metric extended for compact groups. Isn't there some typical? compact groups, isn't there some typical maybe up to scaling. You can always take like Cartanzon algebra as a as the as the generating step so you can do whatever always like a step to sub alignment but I guess that it doesn't but it doesn't mean that these are all because you have some dispute things and also it doesn't it still doesn't give you all possible left Doesn't give you all the possible left and merit metrics. It gives you a huge clause, but not all. I have a question. So the theorem of Miller says that there is a three-dimensional family of left-handed magnetrix on SU2, upper to right-sound matrix. So how fast does this dimension go for SUN? This is a very specific three-dimensional result. So this came, this is, the paper is really about the classification of three-dimensional Bie algebra. Essentially, all three-dimensional Lie algebras have a basis with something like this structure. I think the negative signs for some of the other, if I remember correctly, were zeros or something like that. So. So that's sort of the issue is there's we don't really know any analog of this for any other hints maybe but nothing nothing nearly so elegant. So that's sort of one of the things we'd like to investigate is what what analogues of this might there be even if not as nice for estima, for example. For estimate, for example. I have one final comment about Brian's question on whether the doubling implies point graph or a general demand mindful. Dan says that a simple concrete example is connected some of two copies of R2. So borderline between the two copies of R2 somehow doesn't allow for point grain equality to be true. The doubling is still true. There are no further questions from fantastic. I have an announcement. Anyone who is here who is a student of Leonard Gross or a student of a student of Leonard Gross, or if there's any further dissent to that, who would like to be, it's not, of course, required, any picture. I'm going to take a picture. Right now, right here. Just a second. But yeah, you can have a minute to talk. But if you're willing to do anyone who's willing to do that, we can take a picture of the setup for lunch. People appreciate that. We have a coffee break right now.