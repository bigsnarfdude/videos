  Oh, as you should say, because whenever you give a talk, you have to either say that what you're doing is relevant to Alzheimer's and neurogenesis diseases, as Thomas did. And I'm going to say, well, this could be relevant to COVID-19, which is pretty trendy at the moment, because it's been shown in certain references that cells that spread COVID-19 actually extend for the public. Actually, extend philipovial-like protrusions, which are thought again to make direct contact with other cells and spread the virus. And that's something I am actually thinking about at the moment. Okay, so there seem to be two distinct mechanisms currently. One in which, and this tends to occur in invertebrates, where you find that your cycling makes a direct intermediate time contact with another cell. Time contact with another cell, and along that fixed contact, action, these are actin-rich shelling filaments, myosin transports morphogen or the cognate receptor along the cytin to make contact with another cell. There's still a debate whether it's a direct contact, as many time magnitudes tend to be, or whether it's analogous to synaptic contact. And they do have differences, and we've modelled both. Interestingly, it Interestingly, in vertebrates such as zebrafish, it seems to be a different mechanism in which rather than having these intermediate semi-permanent contacts transporting actively via motors, in search and capture what tends to happen is the morphogenal receptors are at the tip of the cytonube, which then executes a random search process until it makes contact with the target cell and then delivers it. Cell and then delivers its burst of morphogenal receptors and then retracts. I'm going to talk about the latter one today because it's some really cool natane. And so what I want to do is talk about a model I developed initially with Kim Jong Kim, my graduate student, who's now postdoc, science postdoc at UPenn, and then some subsequent work where I want to try and show you the sort of problems. And show you the sort of probability theory that's needed to understand the build-up of a morphogen gradient in tissue that's based on this search and capture of sitemins, like target cells. So to make things simple, let's consider a two- or three-dimensional target tissue with a bunch of cells distributed in the tissue and for simplicity, just a single source cell. And of course, if the source cells are independent of each other, one could scale this up and instead of. One could scale this up and consider many source cells, but let's just focus on one. And let's suppose that there's just one cytonine. Again, there could be multiple cytosine. And each time the cytonine nucleates from the source cell, it moves in a particular direction with a certain probability PI. And if it goes in the direction PI, of course there's some um it doesn't have to be a precise direction, there's a range of directions in which it will capture or be captured by the I th target cell. By the ith target cell. So you have a bunch of probabilities for each target cell that the source cell nucleates in the correct direction to reach the ith target cell. Now we're also going to assume, this will be important for later, that there's also probabilities in the dash-dashed curve line that it misses all target cells. So it fails to find a target cell. And this seems to occur analogously with in terms of In terms of mitosis, where you have microtubule searching for kinetic cores of chromosome pairs, where again there's an analogous search and capture process where you want there to be a probability that the filament or microtube will stop searching and retract because it's missing. And so it's a stochastic search process that is needed because there's a property of faith. So that's the basic picture. And I'm just going to talk about one of the simplest To talk about one of the simplest search and capture models, you can make this much more fancy, but just to kind of get the mathematics, the ideas going, I'm going to focus on that. So, let's imagine that the size of the immune creates in a particular direction with certain probability, it moves with a speed V plus, but then there's a Poisson rate of retraction and returning to the source cell speed V minus. And let R be the rate at which you get a retraction. So, that's the basic stochastic. So that's the basic stochastic search process. And it's actually an example of what's known as this stochastic resetting process. So here's a semantic diagram of what's going on. So you have a source cell. It sends out a sighting with speed V plus, and if it's going in the right direction, it hits a given target cell, which is say a distance L. However, there's a certain probability, because it doesn't know a priori that it's going to reach your target cell, that it resets and retracts back to the original. And it retracts back to the origin with speed v minus. At the origin, there's some sort of refractory period before it starts extending again, if you like, at nucleation time. And so we also build that into the model. Now, for the moment, I'm just going to look at a single target cell. So I have a probability P that the cytonine goes in the correct range of directions to hit the target cell, and a probability minus P that it goes in some other direction and doesn't deliver any morphogen. On the right-hand side, you see the corresponding See the corresponding equations which come up a lot in also motor-driven transport models. This is more analogous to the doctrine-legler models for microtubule growth and catastrophes. So P plus is the probability of the cytonine has length X and is in the growing phase, this P V plus, but remember it can shrink to the retraction phase, switch to the retraction phase at some rate R, so that's the R P plus term. rate R, so that's the RP plus term. Then P minus is the probability that it's just going back to the origin, the speed B minus. And finally, once it reaches the origin and shrinks to zero, there's a waiting time where it has zero length before it starts growing again, given by the D capital P naught equation. And then we have a programme of boundary conditions, an absorbing boundary condition, if it hits the target in the right direction, and then various initial conditions. Just remind you. The reception plate, and beta is the mediation plate. Okay, so what we'd like to do is  And conditional first Aschersteins. And again, I'm trying to show that these methods, which have been using a lot of different contexts, and also Sean Lawley and others have been using, is a very valuable way of setting up and understanding search processes, which have a complex. Search processes which have a complexity. And why is there a complexity here? Because in order to calculate the expected time from each the target, you have to take into account, keep track of how many times the sizing shrinks factors in. So in order to do that, you see the picture at the bottom, I show a growth and resetting phase, and let's assume that it resets at least once, and the time for it to reset and return to the origin I'll call curly S. Then there's an utilation time. Then there's a nutilation time, the refractory time, and then after that, it does more stuff. And it could reset 10 times more, no more times, etc., etc. What I'm going to do, I'm going to set up an integral or a Newell equation based on the idea of once it's reset to the origin, as far as the future statistics are concerned, there's no memory. And so the statistical or stochastic process, once it's returned to the origin, after one Once it's returned to the origin after one resetting, is identical to the original statistics. And that's where in your theory comes. Let T be the first passage time to eventually be absorbed by the target, independent of the number of resettings, and let S be the first passage time if there is a one resetting and return to the origin. And let R be the total time it takes to reach the target after one reset. Target after one reset and define these various objects. Given these definitions, and using the fact that even so, generally speaking, a Markov process is one where if you look at two previous times, you can ignore the earlier time because you just have memory of the most recent time. A strong Markov property is one where the times that you're kind of going back to are themselves stopping times, random times, rather than determines. Rather than determines, but you can then apply all the same sort of theory. So, I want to kind of go through this because, again, I think it's not something that a lot of people have seen, but it's a very powerful method. So, let omega be the set of all events in which the particle is eventually absorbed by the target, and let gamma be the subset of events in which there's at least one reset. And therefore, omega backslash gamma is the set of events which the particle finds the target without any reset. The target without any set. And the basic idea is you can decompose the total mean-first passage time into these two complementary sets, which I've written down in the middle of the page. So the first one, which is the mean-first passage time, given that there's no resetting, is easy to calculate because the probability of no resetting up to time t is just e to the minus rt and then I use a survival property without resetting, which in the case of this deterministic Which, in the case of this deterministic growth, is just the Heaviside function L over V plus minus T. So, from that, I can work out the Laplace transform of the survival probability as an explicit simple expression from which I can generate statistics if there is no reset. However, I also have to analyze the possibility that it does reset at least once. And so now I have to calculate that. And again, because of versus time, I can't go through every single step. But basically, what you could do is you could decompose to Basically what you can do, you can decompose T, given there is one resetting, into this picture I show here, an S plus a tau plus then the remainder R. So that's if there is at least one resetting. And you can calculate each of these terms, I won't go through details, but you can calculate each of these terms sequentially. And the main point in terms of renewal theory is the statistics of R is identical to statistics of T because of this strong Markov property. Strong Markov problem. So, given all that, and you calculate everything, you can end up with, and combining the various results, you end up with an implicit equation for the mean first passage time, which involves the Laplace transform of the survival property if there's no reset, which is easy to calculate. And rearranging this equation, you end up with an explicit formula for the mean first-passis time that depends on the statistics without resetting and then various other terms. This is a very powerful method. So, this is a very powerful method of breaking up events into those with no resetting and those with at least one resetting and writing down a renewal equation that we rearrange and you can get an explicit answer where all you need to calculate is what happens if there's no resetting, which is a much easier problem to solve. And you can work out the formula explicitly for this model. Okay. Now, that was for the case where you definitely find the target. One of the major interests of these sort of Major interest of these sort of renewal and resetting processes is that you can optimize a search process when there's a probability of failure. So I'm not going to go through the calculations, you can extend the calculations in this case, but what I just want you to notice that the schematic at the top shows you what's going on. There's some probability P, I start going towards the target, but there's a possibility I shrink back to the image. There's also a probability 1 minus P that I go in the wrong direction and then eventually Wrong direction and then eventually return to the origin without finding the child. So I've got to now, if you like, condition on all these different events and calculate. But the main observation is that the mean first passes time is now minimized, there's a minimized varying resetting rate. So you can actually optimise the search process, i.e. minimize the mean first passage time by varying a resetting rate. And this is a general result of these sorts of resetting processes. Processes. Okay, so that's what I wanted to say about this. You can extend this method also to multiple targets. Now you have to label each target and you have to then calculate the probability that you find the jth target and given you find the jthart target, what's the mean first bash time, the conditional mean first bash time to find that target. But all the techniques I've talked about make sense to this problem. And there's a paper in Siam, I think, came out. Paper in Siam, I think, came out this year where you can find all the details. And also, I can put this talk on the website after this meeting, after this session. Okay, so you can calculate everything, but what I want to do is go to the next step because what I'm trying to do is kind of give you these conceptual frameworks for understanding these things. And one of the major problems I've been interested in for a while now is multiple search and capture events. In Thomas's talk and many other talks, very interesting talks, people tend to focus on, if you like, the searcher, whether it's a vessel being delivered to a dendritic spine or what have you, and it's a searcher-centric picture. However, now I'll take the viewpoint of the dendritic spine or a target cell in my talk and ask, well, from the target's viewpoint, it's not just interested in receiving one vesicle or one morphogen burst. Or one morphed universe, it's going to accumulate resources over time. So it's a very different story where one search and capture event is, as I talked about, but how do you now imagine multiple search and capture events delivering a succession of bursts to the target cell? And at the same time, this resource is going to be used up, degraded, if you like. So what we like to do is have a mathematical framework to study the long-time steady. To study the long-time steady-state accumulation of resources in competing target cells. And the basic message here is that you can use Qing theory to study. And I've looked at this in lots of different contexts, including axonal transform, so forth, so you can apply to synapses. But let me focus on this case. I won't give all the details. Again, I want to just kind of show the mathematical mapping to the problem. So in Curial theory, you have a bunch of arriving customs. Theory you have a bunch of arriving customers that arrive according to some statistic, usually non-Markodian. They form a queue, and then there's a server that sequentially serves the waiting customs. And once they've been served, they exit. So what's the analog in the morphogen problem? Well, the site me searching for the target is the customer. Once it reaches the target, it delivers a burst. It did it as a burst. That's the arrival of the customer. And then, degradation of a burst is being served. Now, because these guys can degrade independently, there's no sort of maximal capacity. You're not restricted to three servers, and once the three servers are busy, you can't degrade. So, from a mathematical viewpoint, this is a queuing problem with an infinite number of servers. Once you make that mapping, you can take the statistics. You can take the statistics I talked about in the first part of this talk for a single searching capture, that's the statistics of the formation of the Q, and then combine it with the Markovian exponential degradation rate, and you can fit it into what's called a Gm infinity Q. And once you've done that, then there's a lot of mathematical machining you can use to study the accumulation of morphogen of these cells. And I'm not going to go through the mathematics of that, but I'm just pointing out here. But I'm just pointing out here that the, as I said, that it's a GM and Funti queue, and the statistics of these single search and capture events determine and the degradation determine the statistical quantities you need in the Turing field. So you can map it into there. All I'm going to do is quote results. So first, for example, what is the steady state number of resource packets in the case type? And this is quite intuitive. It's actually also an example of distance law. It's actually also an example of Little's law in Euring theory. What it says is that the mean number of resources in steady state at the K target is the spree probability, the probability that it's the K target that captures the cytonine, divided by the degradation rate times the mean first patch time I calculated in the first part of the talk, plus a new refractory period in that it takes a finite amount of time to drive your cup. So that's basically the formula for the Basically, a formula for the mean, which is quite intuitive. But the really nice thing about queuing theory is it can also allow you to calculate the variance, the size of fluctuations in the resources, which is very important. Particularly if you have, for example, in synaxes, you don't want fluctuations to be too large to have reliable computing and things like that. So getting a handle on the variance in the amount of resources is very important. Okay, so let me just give you a result for at least I can maybe cover the second topic. I can maybe cover the second topic. So, what I did, I considered a configuration of target cells in the layer, worked out the geometry of sightings moving in different directions to determine the probabilities of finding a particular target, and then calculated everything in order to determine the distribution of resources across this array of targets. And loving the whole, otherwise it would be pointless given this talk in the context of morphogenesis, you get an orphan ring. We could calculate the variance and so forth. And also interesting, the total amount of resources across the whole array can again be optimized by varying the resetting. And so this, so the two purposes of this part of the talk is number one to show these really cool mathematical techniques out there to study a target-centric aspect of search processes, of the accumulation of resources, renewal theory when you have resetting events. Theory when you have resetting events, and if we put it all together, you can actually look at the mean invariance of the formation of morphology gradients of these systems. Okay, so that's the first topic. I'm not going to have time to go through the second one. So, how much time we've got I have until 15 over or 20 over the hour. Okay, go ahead. Just shout at me. Just insult me or something. At me, just insult me or something when you're going to kick. So, let me just talk about the second topic, and that is intracellular protein concentrations. So, I'm going to focus today because some time constraints on brain information rather than non-classical Turing mechanisms. That could be another talk for another day. So, one of the important points to point out in terms of intracellular protein concentration gradients is the sort of Concentration gradients is that sort of the time and length scales are such that this sort of degradation, using up of resources I talked about in the first part of the talk, don't apply. So in general, what happens is that rather than having a source of diffusing proteins and then an analog of degradation, what you have instead is a modification of the protein as it goes away from a source. And so one of the best known certain intracellular and a lot of people In studying intracellular, and a lot of people in the audience have worked on these sorts of problems, is the phosphorylation state of the protein. So, for example, there can be activating enzymes in the cell membrane that phosphorylate proteins, which then diffuse inside the cell, and as they diffuse, they're deactivated by deactivating enzymes, phosphases. And this is thought to be a major mechanism in cell phylerization. In cell pylorization, cell division, and so forth. And this is intracellular rather than the last topic, which is extracellular. Now, although the biophysics is different, the mathematics of this sort of process is typically the same. Rather than the degradation of the protein concentration as it's evolving, it's a switch to a different protein state. So if you're looking at the concentration of phosphorylated protein, then K would represent. Protein, then K would represent the rate of deactivation. But the mathematics is the same, different scales. However, there's a very interesting mechanism that seems to occur in C. elegans one-cell zygotes prior to cell division involving polarization. And what I want to do, I'll just tell you briefly about the experiments, because this is one of these frustrating situations. I mean, those who know me know I tend to not work from experimental data, but just kind of Experimental data, but just kind of hallucinate and come up with ideas, and a lot of them are completely irrelevant to anything. But occasionally, they're actually relevant. And with Sean Lawley and Patrick Murphy, my age student, we came up with a mechanism for setting up a protein grading involving switching diffusions. And we submitted it at this red letters, and it got rejected because it said there's no experimental evidence for this. Which really pissed me off, I must say. And then, lo and behold, six months later, an experiment came out. Months later, an experiment came out with exactly this mechanism. And so I think there's a tendency these days to too quickly jump on speculative ideas because who knows what's going to happen. Biology always surprises us. Anyway, so what's the mechanism here? Well, I don't want to go bogging down with lots of different chemical names, but basically part one undergoes a kind of classical intracellular polarization. But part one then affects through buffering the rate of diffusion. The rate of diffusion of another chemical, MEX56, and MEX56 itself affects the diffusivity of another chemical, Pi1. And on the left-hand figure, you see that what basically happens is that MEX5O6 diffuses slowly on the anterior side and quickly on the posterior side, and vice versa for pi1. Now, if you're diffusing more slowly, you're going to have a high concentration. So this difference in diffusivity. A difference in diffusivities between different regions is a mechanism for setting up a concentration. And I'll go through the math in a minute. On the right-hand side, it just shows that if you create mutants that block some of these chemicals, then you don't get this differentiation in speed of fusion, and you get no polarization, no silver. So, this was a really interesting set of experiments. And I just want to show you, along the Ember One length, they actually measured the relative concentration. They actually measured the relative concentration of the pi1 and x5 anterior posterior axis. And they also looked at, so basically what happened was, because of buffering and so forth, that there were two, at least two diffusion states of each of these chemicals, and there was a switching between the chemicals. And the switching rate varied as you went along the body axis. So you would switch more quickly into the faster fusion state on one side and switch. State on one side and switch more quickly to the slow diffusion state on the other side. And it was this mechanism that set up this concentration rate. Pretty cool. So, in the last 2.5 minutes or whatever, I want to tell you what it is mathematically. And this came before the experiment. So, anyway. So, let's now imagine that we have a concentration of proteins that have two diffusion states. Diffusion states. And this could be due to buffering or so forth, binding to a substrate, you name it. And let's consider the hybrid Vener process where x, again I'm dealing with one dimension, so we can do this, of course, in high dimensions. So dx is the position of one of the molecules, and it undergoes a Vena process where the diffusion coefficient depends on this randomly switching discrete variable mt. And we'll assume. We want to assume that the switching rates between these two states themselves depend on where the molecule is due to the effects of other chemicals in the substrate. So what you can do, you can write down at the population level the mean concentrations of proteins that are in the slow diffusing state and proteins in the fast diffusing state. And again, it looks like this kind of hybrid type. Kind of hybrid type of model, reaction diffusion model. You can plug in numbers and so forth, and you can actually solve these equations. And what I'm going to do now is just briefly talk about something many of us in the audience, virtual and real, have done over the years, and they'll do a quasi-steady state approximation. So let's imagine we decompose the concentration. So I should say one thing. The extivents of beta and alpha is prescribed by the substrate. Is prescribed by the substrate, just as an experimental system. So now I do a decomposition where if there was very fast switching and it didn't move, then you rapidly converge to the X-dependent stationary distribution of the Markov chain, the two-state Markov chain. But because you're moving, there's a correction to that process of order epsilon. Where epsilon here determines the rate of switching. And so you can go through the standard. And so you can go through the standard analysis, and what you find is that to leading order, you get an inhomogeneous diffusion process. Ignore the epsilon term, to leading order, dc by dt equals d2 by dx squared, an x-dependent diffusivity coming from the fact that the alphas and the betas are x-dependent. And I've written down the form of the dx at the bottom. And the punchline is, if you now apply that to the given system, To the given system and work out what the say concentration is, you end up lo and behold and reproducing the experimental data by appropriate choices alphabet. So of course the figure we produced was the mechanic we try and came up with a mechanism and then when this paper came out then with Patrick we went back to it and matched it to the experimental data. And matched it to the experimental data. And so, with that, I kind of think I will stop. I'll compromise. It's not quite 11.20, it's not quite 11.50, and I'll stop there. Yeah, boundary lanes, whatever. Okay, and I won't do extra patent information because I'll be solved. All right, I'll stop there. Thank you so much for the next staff. Let's give it a hint. Okay, that was great. So, we will start for the virtual session. So we will start for the virtual sessions with questions from the virtual participants. Feel free to unmute or raise your hand. I've got a question. This is Evan Rowe. That was a nice talk. I enjoyed it. I'm wondering if you thought about, if there's a lot of interest in the context of morphaging gradients, in what makes morphogening robust with respect to variation in the size of domains and so on. So I wonder whether if you think about the mechanism that you're If you think about the mechanism that you're describing involving cytoniums, whether interesting properties emerge. It's a good question, and yes, in the process of doing this work, we did study things like robustness. So that's an interesting question, actually, because what we found in a lot of these cycling-based mechanisms is in order to get robustness, you have to have inefficiency, which is quite strange. What do I mean by that? What do I mean by that? So in one of the models we found was that you had to fail quite a lot to set up a robust gradient. And so there's some very interesting questions about how do you juggle these different costs. And a related question is why use site teams, why not just use diffusion? And I think, and I know my ex-students working on this, and I think in one of you, I don't think there's much difference in terms of which is better in terms of setting up a gradient. In terms of setting up a gradient. But I think it's when you go into these high-dimensional tissues, you waste a lot of stuff producing chemicals that diffuse in a high-dimensional space. Whereas there's a possibility of the sightings being much more directed and much more controlled. So I think that sighting-based morphogens, at least on the length scales where they apply, could be more effective in higher dimensions. But there is this kind of playoff between setting up a robust gradient setting up a robust gradient and at the same time being rather inefficient. And you get similar things in terms of diffusion gradients where you want fast binding at one end and slow binding the other. So there's a lot of interplay between these things. But yeah, good question. Keith Kramer has a question. Yeah. I know you had to rush through that slide. Could you just say a word where you did the fitting to the experiment, how much freedom you have? To the experiment, how much freedom you had in choosing alpha and beta? Oh, so what we did, we actually took the they fitted alpha and beta and we took their fitted data. Okay. Yeah. Yeah. So we have no freedom because we only use one data set. All right, so maybe I'll ask you something before we switch to in-person questions. I was wondering for your side and your. I was wondering for your cytonine stochastic model. It showed the retraction state was going all the way back to the source cell. Is that necessary or the case or can it make a microtube shrink a little and then grow again? Yeah, yeah, that's a good question. In fact, in the original version that we developed, that's what we did. But then I got interested in reset problems, and then I thought, well, this is actually so he could do that, but of course, but he. So yeah you could do that but of course the analysis of the first patch time is a lot more complicated. So to get intuitive insights I went to a simpler model but the more realistic one is that you're going to tend and shrink and grow just like in a catastrophe's microchemical. Makes sense. Thank you. All right. Jay, can we take questions from you all? Do we have the audio? Yeah, yeah. I have a question. I think Con has a question. Yeah. Should I stop? Should I stop or okay? You go first. Okay, uh I'm gonna go back. Uh so uh can can you hear me by the way? I'm not sure. Can you hear us? I can hear you. Fairly well. Fairly well yeah. Yeah, I just want to say that I have found the the mapping of the the self with the the queuing model current system. And uh what I was wondering, so if your queuing model uh it's a key I infinite uh queue, right? So you you assume some personal route would drive all the time and for some rival time and some exponential self-time. And I was just wondering if just something about the parameters of the biomedical, you know, biological parameters that makes this nothing valid? I mean, do you need some assumptions on the biomedical process to nothing? No, this is a very broad framework. So what I do assume to simplify. What I did assume, for simplicity, was I assumed that each cytonym or searcher is identical independent. So I take the statistics of each searcher to be. But there is an interesting twist on this. And in fact, for axonal transport, I think a more realistic model is rather than having a bunch of independent cytones growing and shrinking and nucleating, I think in the axonal case, a more reasonable idea would be you insert a motor cargo complex. Motor cargo complex, say periodically, or according to some waiting time density. And then it turns out the statistics and the analysis is very different. But it's still the same mathematical framework. So one of the really powerful things about Kewing's theory is that you can deal with non-Markovian processes. And so I think it's a very powerful method. And I have to say I'm surprised that it hasn't been taken up more in the transport community because I think it's kind of just like polarization. It's just like colonization in this country where everyone is favoring the searches. What about the poor target people? So it's the same thing. Jay, do you have a question? Yeah. So the connection between your spatially varying diffusion and your steady state concentration profile, that depends an awful lot on the micro scale model. The micro scale model of whatever is leading to the spatially varying diffusion, right? So the epsilon terms that you get, is it right to assume that maybe they're not insignificant, but even if they weren't, would they not contribute to the spatially varying diffusion profile because of the form that the spatially That the spatially varying diffusion would take in the resulting Fokker-Clanck equation? So, if I understand what you say that one should try and have look at the actual process underlying the slowing down of the diffusion state and build that into the model. Is that what you're saying? Yeah, through the quasi-steady state, you get like a fixed... Like a fixed law in your resulting equation, and that's not going to influence at all the spatial profile at steady state, right? Right, right. Yeah, I think so we were, so the honest answer to your question is that Patrick, who was working on this, graduated and went to Rice. And so I had given him a new project working. I had given them a new project where to actually get into the nitty-gritty. Because as I showed you on this hierarchy here, the slow-fast diffusion is a hierarchical process where pi1 activates or deactivates the diffusivity of the mechs, and then mex itself activates the diffusivity of pi1. So there's a whole interesting class of processes underlying the uh determining the diffusivities and that should be built into the model, I can believe you'd agree. Should be built into the model, I completely agree. But it hasn't been done yet. Well, you were just using the leading order average diffusion, right? You just neglected that. We did that because we didn't include the more detailed biophysics of how this is happening. Yeah, yeah, okay, cool. Yeah, but you're right, yeah. That's the next step. Great. All right, let's think I'll wrap it up again. Alright, and I think we're ready to switch to the next talk of the session. You might need to stop sharing, Paul.