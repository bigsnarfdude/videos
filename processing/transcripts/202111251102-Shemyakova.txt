To the topic, um, without uh further ado. So, in this talk, um, it's going to be two topics kind of together. So, one, uh, well, you'll see, I mean, because it's this work is in, um, you know, like lays in the two areas, pretty much. So, one area is a cluster algebras, and another one is supergeometry. So, why they are together here? So, why they are together here? Because construction of supercluster algebras is now, I mean, like a new hot topic. So, many different groups working towards this notion. And there are several different directions how you could approach this problem. And basically, you have to create some key example. So, there are some certain key examples where cluster algebras appear and Appear and people consider super versions of this example. And in our approach, this is joint work with my UK collaborator Ted Worner. So in our approach, we consider super analog of glucose embedding and we, basing on that, on the results which we obtain, we propose our first, you know, our own version of supercluster out, which has like simple, very simple mnemonic rules, which is, you know, very good. Anyway, I'll go. Very good. Anyway, I'm going to explain, of course, everything I know that you may not know what it is. So I'm going to explain everything. So, okay, so let me say the following. So first I'll state, I will state main results. Then I would explain very quickly what's a classical, not explain, set notations for classical pre-embedding. So I'll quickly tell you what supergeometry. I'll quickly tell you what supergeometry, and you will see that for political embedding, you need exterior, exterior, you need the exterior powers of your vector space. So I'll explain why exterior powers cannot be computed in a simple, naive way, how you would expect it. And I'll demonstrate everything in very simple examples. And then I'll show you how instead you can construct the super exterior powers. And then, actually, the last two is the actual results. So, I will show you how we construct superpleting and we obtain super plug correlations. And it's actually important. So, these super plug correlations, these are the ones which allows us to actually construct cluster algebra. So, prototype for cluster structures, supercluster structures. In supercluster structure, we don't construct. In supercluster structure, we don't construct in all cases. We construct in just a particular case of two zero planes in NM space, but the PLUCO embedding and PLUCO relations superversions are obtained in all cases. Okay, so that's a bit delayed. Okay, so maybe a couple of remarks again about the general structure of the talk. So what was problematic. What was problematic for construction of the classical super of the superanalog of classical Pluka embedding? The problem, one of the problems was actually construction of the target sprays. And in fact, in the book of Manin, of Yuri Manin, years ago it was said that such construction superversion of Pluk embedding is in fact impossible. And so he is, of course, right, but the thing is. Of course, right, but the thing is that, well, instead of projective space, we consider the weight projective space. So that's how we, you know, we're able to construct it. So, okay, so like, okay, so that's maybe I'm not going to show the main results. I don't have so much time. So, okay, let me just go straight away. So, what's the motivation for the problem? So, the first of all, it's nice to construct, you know, a superanalogue of the fundamental notion, right? The fundamental notion, right? So it's interesting for the field. But secondly, there's a, as I said, that's a construction of supercluster algebras. That's a really, really hot area right now. And very prominent and famous people in the field of cluster algebras actually attacking these problems. And papers on this topic appear like, as we say in Russia, mushrooms after the rain. So they appear like a lot. And so just, yeah, so very. Just yeah, so very, very fast, but very primitive. So it's a very, very hot field. Okay. So what is the classical Fluke embedding? So it's okay, you probably all know so that we have R-dimensional subspaces. So what's Grassmann manifold R in N, we can see the R-dimensional subspaces in R. And what is, you know, what is the Purkin betting? That's how you realize your Grassmannian as a projective subspace. Grasmanian is a projective sub-variety. So, what you do practically, so you have, I don't know, no, you don't see a pointer. So, you have, okay, so you have, so this is, so this is element from here, right? So, you have some plane, and you say, okay, so that's a plane generated by, so it's, it's our plane, right? R plane. So, let's say it's generated by our vectors, and we map this r vectors. And we map these our vectors into the vet products of these vectors. Now, every you know, plane, well, we'll call it plane subspace can be, of course, generated by different bases, right? But you know that if you can see the vetch product, then just the determinant of matrix of change of variables will appear here. So, this map will be well defined up to a factor, and so this map is well defined, and uh, okay. Uh okay, uh so uh now, okay, what else uh is happening? So, uh uh okay, so we're mapping my uh planes, so my planes from here, we are mapping into much larger space, right? And so the question, how, uh, so in the coordinates in the space, so the space obviously have uh well, if you introduce if v have the basis if e1 e. E1 En, if your vector space have the basis E1 EN, right? Lambda, Lambda, so exterior power of V, going to have a basis what? E1, Veg, E2, I mean E2, E1, Veg3, and so on. Yeah, so it's going to be a base. So, yeah, and if we have lambda 2, right? And so now the question is how we can characterize. How we can characterize the image of so planes, our planes from here mapped into some space, right? How we can describe it, how we know that elements from here will be coming from some plane. And flu correlations, these are exactly relations for the coordinate, for the coordinate of the exactly relations for the coordinates of the Relations for the coordinates of the um here, such that guarantee that your element comes from here. So, I will see, you'll see much better in the examples. So, okay, so let's go. So, I'll try to do some fancy stuff, but it didn't really work that well. So, let me try to erase it. So, okay, so here let's see some examples. So, it's an example just to So, as an example, just so you understand what I'm talking about. Okay, so you have, but of course, you know this. So, you have a basis here, right? So, if we have a two-dimensional plane in four-dimensional space, so if it's plane two-dimensional, generalized by two vectors, so we can see the vegg product, right? So, these are, right, these are coordinates in here, and these are coordinates at the corresponding basis factors. So, for example, Basis factors. So, for example, this coordinate is a coordinate at e3, e4, and so on. So, where this one is a basis factor. Okay, so one of the standard time notations is going to be, so we put a matrix, so we expand u1 and u2 in terms of this basis, right? And we put coordinates, components of this vectors, as the rows. So, here you see 1, 0, and 0, 1. Well, of course, because you have a non-digital. Of course, because you have a non-degenerate, so I mean, L is a two-dimensional plane, so you should have some non-degenerate minor here, right? So, we assume, okay, here for this, for example, the first two rows make your non-degenerate, so you multiply by the inverse of this matrix, so you get it normalized. Okay, so as a result, okay, so what we can do, you could consider, so you could literally consider, so you represent, you have u, right, u1, a vector u1 represented as. Vector u1 represented as with this component in terms of this basis, vector u2 represented with this component in terms of this basis. So you can literally, like, you know, like in school, like your students do, can expand this, right? And have some expressions in terms of A3, A4, B3, and B4, right? So you expand in this kind of basis. On the other hand, you know that coefficient and this has to be like we have a standard notation. So as a result, when you compare, equate this. Result: When you compare equators, you have these relations. So, from the first relations, right, you just establish, well, A3, A4, and so on. But you're also going to have extra relations. And this extra relations are going to be exactly Kluker relations, which are, I mean, in this case, going to be just here, right? And when you go back, you pay back for this normalization, right? Because if you multiply, all minus will be a multiplied by T12. So then, respectively. So then, respectively, go from here to here, you have to divide all these guys by T12. So, some algebra gives you this classical plug correlations. Those who knows recognize. So, okay, so let's go next. Okay. Oh, wait. Well, next. Okay. So it doesn't have a race I is clear. Have a race, I is clear, okay. Okay, supergress mining. So now you realize that I need supergress mining, right? To realize this whole thing. So, okay, so supergress mining. So you have just you just have a vector space, okay? So which have two subspaces, one v0, elements of v0 we call even, and one v1, elements of which we called odd. We don't consider, we only consider homogeneous elements, we don't consider them mixed, right? Although just Mixed, right? Although, just because mixed problems with mixed parities can be always split into two for homogeneous. So we don't need to consider. Anyway, so the coefficients, so at the moment, this vector space, right, so the coefficients come from the constants, so constants. But if you want some non-trivial coefficients coming from, which can be also even or odd, they come from this commutative super opera. Okay, so I'll. Okay, so I don't have here too much time to say, but it's a result, right? Because the basis is integrated, and the vectors which generated my RS is degraded. You have this block matrix, which kind of understandable, right? So, okay. So, you could have, so you also have Grassmannian supermanifold, so you can compute dimension in a natural way. Okay, so what's the problem? So, now the question is how to construct. So, now the question is: how to construct exterior power of your vector space, of your super vector space? Well, it turns out that there are two possibilities. So, first, there is naive possibility is that you have your vector space. You can see that there's an algebra corresponding to the state, to this vector space, and you quotient out by anti-commutative conditions. But your anti-commutative conditions going to be, well, okay. Well, okay, um, even yeah, so so up to sign, right? So it's going to be sign. Remember, with supercase, you're going to have possibility of a sign. So, for example, odd with odd elements, if you can see the anti-commutative condition, they already anti-commute. So, anti-commute with anti-commute, you get plus. Okay, so yeah. But you'll better see it. Uh, so on this on this example, on this example, you'll better see why see why um why this natural construction which you say well what if if the if they diff if this whole stuff is just different by science why we can just do the natural construction well let me show you crucial example which maybe explain to you kind of some things about supergeometry so if suppose we consider one two planes right in nm space so it's a very simple example you understand guaranteed and so you have here a plane okay and let's say the plane is generated by one Plane is generated by one even right and two odd vectors exactly here, one, two plane. So then we say that my map, right, if it's going to be analog of superbrooker, right, going to be just mapped into the vetch product, defined in this way, right, in standard algebraic way. So then, of course, the very same plane, right, is generated by these vectors, right? I just took linear combination of this and this, right, and put it there. So it's going to be the very same. There, so it's going to be the very same plane. So, theoretically, right, if I just consider this, right, it should be this up to a factor. However, it's not happening. Why it doesn't happen? Well, if you just expand, right, so you see, okay, this is the term, which is the same as this one up to, which so this term and this sum, they're the same up to a factor. However, we have one, this extra term. And let's talk about this. And let's talk about this term. So, look at this part. So, this and this, right? So, in a normal classical situation, this term will never exist. And why it never exists? Yes, it would never exist because you have here, you know, square, veg square of U2 hat and U2 hat. But in our situation, U2 hat is odd, U2 hat is odd. U2 hat is odd, and square of odd is, I mean, as I said, squares are going to become u in a normal sense. So you can have squares, you can have cube, and this term will not go away. Okay. So because normally you have just only even terms, when you do anti-computerity, it means that square is going to be equals to zero. Okay, so that's what the problem is here. That in fact, here in super case, you have these additional terms, and this is actually. Have these additional terms, and this is actually known as the absence of top power problem. So, in super charm, so okay, so here uh we have two uh okay, so it turns out that actually um okay, so so this is a place where our work splits into two directions. So, one, so we started to consider two different cases, and let me explain why. Uh, well, if you look at this example, hopefully, you understand uh what is um Or understand what is, you would sort of see that this phenomenon, which I just explained, happens not always, but it will not happen if I would forbid L to be depending on odd, to be to be generated by odd vectors. And so, in other words, if you consider what is known in supergeometry as completely even planes, so planes can be allowed. Planes so planes can be allowed only to generate it by the even vectors. I would never have this situation, I would always have, if you can see the change of basis, I will have something which product going to be just proportional, and then the map is going to be well defined. So that, yeah, and so that, and therefore, okay, for completely even plane, which we will consider so-called algebraic is, which will be relatively easy to consider, but for the But for the non-completely minimal planes, you will get much harder consideration, unfortunately. So, okay, so let me see back. Okay, so let me first explain how the much harder consideration works, just in a few words. Okay, so we have a special construction. So you to for this construction, so it was constructed years ago, and it was and there the exterior RS. The exterior RS exterior powers, right? So, Rx exterior powers, they are constructed in the following way. So, you just have to recall, right, that exterior powers, they also can be understood, right, as a multilinear functions on covectors, right? So, exterior powers of vectors can be understood as multilinear functions of co-vectors, as one-to-one corresponds. And then you say, and so that's definition which you see, it sort of goes towards that direction. So, we say that, so they define RS. So they define Rs multivector as a function of R even and S odd vectors, right, which satisfies the following property. Again, so this is just a change, linear change of linear change of variables. So determined, by the way, it's a super analog, it's a super analog of determinant. Berizing is a super analog of determinant. So this property, I'll just have to rush through. So only thing I want to say here is that we have, so this function's f. So, this functions f, which now, well, now lambda r s, that's a collection of our functions f, define this, and they are defined very abstractly, of course. And we, they have property of multilinearity only in even argument, only in even covectors. So, only here. Okay. So, we have this construction, right? It's very abstract. So, let's see. And we don't, in fact, it's an open problem to describe, to give explanation. It's an open problem to describe to give explicit description of the space. It's an infinite-dimensional space. So, now, how are we going to work with this? Because we need to have explicit, actually explicit formulas. Well, in fact, we have an excellent example. It turns out we have an excellent example of such covectors. And such covectors, well, sorry, excellent example of such multi-vectors. And this is And this is this proposed construction will play a role of simple multi-vectors. So, in other words, you know, simple multi-vectors, so you know, multi-vectors could be like splash and so on, right? But simple multivectors, which is completely represented the vetched products of vectors. And so this one, right, this is going to be a function which we think of as you have so many even vectors veg with so many odd vectors. Vectors and we just write in them like, and that will be a function which will be representing this, and it will be acting on so many characters. So, the formula we say, okay, going to be the following. So, it's a Brazilian of the following thing. So, you take u's, right, components of u's and write them in the rows. You take components of p's, right? If you have wanted matrix form and write them into the columns, you multiply, you compute Berizian, which I don't tell you how to compute, and then How to compute, and then you get this, but in fact, you get explicit warnings, okay? So, uh, now, um, okay, so okay, so we have, so what to say, we have a complete, so coming back to algebraic case now, so we finished with exterior power, so now coming back to algebraic case. Algebraic cases are completely even of completely even planes. So, in the case of completely even planes, right, we have, okay, so We have okay, so I don't have time, but okay, we classify everything, we have all results, we have we prove it's embedding, and we prove that this will be embedding, and we proved that we have super pluky relations, and I mean we obtained superfluous relations, and we have here the form. So, here, okay, the relations which we actually have for some particular, so this is just example, some examples of two planes in four one space. We have them in general case. In general, case. And they are not just random relations. For example, there is a work of Cervantes Fiarezi Eliado, which is of physicists, work of physicists, and then they consider this particular example, just one example. They just consider this example for their own purpose, and they have exactly the same relations. So now, okay, so we have relations of these types. Now, so how many means they have like seven? I think I don't hear anybody. Yes, you're correct. You were not hearing me. So you have like 20 minutes left if you want. Oh, how many minutes I have? 20? Yeah, so you have like plenty of time. You have 20 minutes if you want. You have 20 minutes if you want. I have plenty of material. And I'll don't worry, I'll try to be clear. So I'm not going to overlan. So, okay. So let me go, you know, where to show you. Let me show you this. If you want, I'll show you this later, the specific computation. I have lots of specific examples so that you could understand everything. So let me just first explain what happening in the case of. In the case of superpluker, so in the general case, so we have this. I showed you how we can start in algebraic case. So we basically, what we did, we have your, I have a plane, right? And I just map it into the watch product, defined again, take a tensor algebra, factor out by adopted anti-computer language, adjusted anti-computer language up to with additional sign. Now, okay. Um okay, so now uh our first definition, our first definition for the for known for general case. So, general case is when you have R S in N M, right? So S is not equals to zero. So now you have like a general case. So the first, our first idea was, remember that map example, which I said to you is going to be computed bare, this, you, remember, you know, this one, it's a particular example. Yeah, so something I was. Example, yeah, so something I was trying to rush and I forgot to tell you that, no, I told you, but still, it's a particular, so this, right, so this matrix, right, was a particular example of functions. So again, once more, again, to give you the general picture, right, for this one, lambda rs, right, they constructed by Warren of Zorich, and these are effectively these functions f, right, schematically speaking. Okay, and then this is very, very abstract notion. This is a very, very abstract notion, right? But we have a particular example. We have an example, right? An example of such functions f is this construction where, well, if you have u1, veg, ur, veg, and here you started to write down with the heads. So hats are my notation for odd, as you already know, odd vectors. So this, if you consider functions, correspond to this, which is acting on Which is acting on characters, on characters, that's going to be computing using this Berkele formula. Okay, that's a particular example. And so, our first inclination was, okay, so we found this example of a function of Berkele, right? So, this is analog of simple multi-vector. So, why don't our map going to work in the following way? So, we have a plane, right, which is generated by so many odd, even, and so many odd. Even and so many odd vectors, and we just map it into this wedge product. Why not? Defined using this formula. Well, it turns out that maybe it was a good idea, but it had to be adjusted quite a lot. So first of all, let me explain you a specific example. So here's a specific example where you have, okay, so we have an ambient space of dimension to two, and you have a plane of dimension. And you have a plane of dimension one, one. So, again, so let's explain one even, one odd vector. So, you expand them, as you could see very explicitly, in terms of basis, in terms of basis of your vector space. And here, just to just keep you in the loop, so that you have to keep, yeah, so because of the rules, right, you have to pick a parity. So, for example, this is supposed to be right, even vector. This is supposed to be right, even vector, right? So, if E is even, right, E1 is even, then the coefficient at E1 has to be even. Reason, because, I mean, very roughly speaking, so even times even equals to even, odd times odd equals to even, and odd times even equals to odd. Okay, and using this kind of arithmetic. And using this kind of arithmetic, you know that X has to be even, Z has to be even, psi has to be odd, right? Because total of this one has to be even, and we know this one is already odd, and so on. And in principle, so I take standard notation from physics. So Greek letter stands for odd, and Latin stands for even. Okay, so we have this expression, right, for u and u1. Again, similarly. U1 again, similarly, somehow similarly we normalize, right, but now just in separate places. So, and then we have corresponding matrix U, which is made of coefficients of this U1 and U1 hat. Okay, so now, okay, so here what's happening. So, we have, okay, so we have this, so this one, right? So, we test this particular example. So, we know that my plane L, right, my plane L, we. L, right, my plane L, we originally map it into this wedge product. And just for, you know, for the beautification reason, so this wedge product, right, which we define as, we denote as this. Okay, just notation. Okay, so then we know how the switch product acts on how it acts on covectus, right? And so the question is. Cavectors, right? And so the question is: if it's embedding, it has to be one-to-one-on-one on the image. So we should be able to restore my plane from the knowledge how my function f, right? How this function acts on the characters. And it turns out that we cannot do it. So we can do it, but we figure out how to fix it so we can do it. And so let me explain. So here, actually, you see how we're explicitly, so I can compute everything explicitly. Explicitly, so I can compute everything explicitly. So, here you see matrix U, as I promised, as a rows. Here's my covectus, right? Coordinates of my covet, components of my covectus are written as a columns. So, Berizinian, I have no time to tell what it is, but you see it's a rational function, by the way. So, as opposed to determinant, right? Determinant is a polynomial in terms of entries, matrix entries, and Berizinian is a rational function. So, it's a quite a different, so yes, it's a So, yes, it's a very non-trivial analog of Berizinian and very non-trivial analog of determinant. So, to restore, so what do we want? So, if we want to restore plane L, right, we effectively want to restore X eta, psi, and Y, right? And so to restore X, right, after some algebra, very simple, you can see that you need just plug E1 and E2 hat. Plug E1 and E2 hat this covectors into this function, and then you can restore. Secondly, you can restore Y in the very similar fashion. Now, the problem comes when you try to restore psi. So when try to restock psi. And you could see, right? So to restock psi, you actually, unfortunately, need to plug in so odd vector into even spot. Spot. Remember that my functions f is only defined this way, so that you put even first and then so even covectors as a first argument and first some arguments and so on. And then you put, so the problem, you put wrong parity covector in the even spot, so to say. So can we fix it? Yes, we can, because we know that my function f, remember I stress it for you, in the even arguments, it's multilinear. And therefore, And therefore, we can extend bilinearity and allow to drop their odd covectors. Okay, so that's fixed. But it turns out that it's not enough. Because if you go and further, because remember we have one extra term to restore, eta. So after some basic algebra, you would see that you actually need to plug in even covector into the odd splot. Into the odd slot, into the odd slot. You would say, okay, why don't you just do the same? But what happens is you cannot, because actually, remember I mentioned that these functions f have this property that they actually non-linear in the odd covectors. And in fact, even more, for example, they have homogeneous, they are homogeneous. They are homogeneous of degree negative one. Okay, so they're not linear, and so you cannot extend anything by linearity. Okay, therefore, we can see that, well, we figure out how to fix it. And so we consider, I don't want to go too much in details, but we consider this parallel construction, right, which called Berryzinian star. So this is Berizinian star. It's an idea due to Bergwell Robinson years ago. And so that's just a way to consider. Just a way to consider so it's a Berizinian of pair, it defines Berrysinian of parity inverse matrix, and it's defined is somehow, I don't know, some analog of transposition in linear algebra. You can think of this by sort of a peritary version. But anyway, so it's too long to speak about this. But what's happened is it's sort of convenient to consider Berizinian style and Berizinian because they have sort of symmetric, I don't know how to say symmetric properties. And so when Berryzinian Properties. And so when Berryzinian is multilinear in odd characters, sorry, in even spots, Berryzinian star going to be multilinear in odd spots. And so as a result, remember that my definition, my original definition of this example of function f, remembered, was bare of, right? Remember, u written as a rows and p matrix P written as a columns. So what I'm going to do, I'm just going to be everywhere there. To do, I'm just going to be everywhere there, also at star. So I'm going to consider them both. I'm going to consider bears, right? And then I consider separately bear star. And this is pretty much it for this construction because that's just basically, you have to believe me that we proved that it works in general, right? So instead of mapping, so my map pluk is going to be map. So pluk is the one which just you shown in example C in examples is corresponding to bear. And this one. Corresponding to Bear, and this one when you take Bear star. Okay, and so you have a weighted projective variant. So, in this situation, we proved, so that's of course, so that is indeed embedding. In other words, L can be uniquely reconstructed from such a function. And super relation, superplus relations, we also obtain them, but yeah, so its formulas are too large. Now, let me turn to the second part of my talk. So, of course, this whole stuff, right? So, I just, we wanted to consider because, yeah, so we were interested to try to also solve this, you know, at least approach to this interesting problem. So, what is cluster algebras? What is supercluster? What is cluster algebra first? So, that's it's a relatively recent notion. And why they are interesting, you probably must. interesting you probably you must every each of every one of you must heard heard about it so they are interesting because they appear in very diverse areas very very different areas of mathematics so they anywhere from Teichmüller's theory to say well I was surprised that using cluster algebra you can prove that this sequence of numbers right which technically should take rational values in fact take integer values only and this was conjectured Values only, and this was conjectured, some was conjectured for a long time, but it was proved using cluster algebra. So, but what think were these questions types of questions? Where are some other, and then Tech Müller theory? So, what I mean is it's like very, very diverse area. And so, this is just example, there are much more, and people just see them sporting out in physics. And so that's that's a very, so that's why they become so popular. Now, what is cluster algebras by themselves? So, very, very briefly. So very, very briefly speaking, and that is geometric, I'm talking about very particular type of cluster algebra, those called geometric types, but nevertheless, they're very famous. So, but anyway, so you suppose, just imagine you have a field, right? Some field, and then inside field, you have a commutative ring with, of course, with some special properties. For example, let's put a field, you can consider just rational functions, right? Functions, right? Or, for example, if we take, say, the case n equals to 2, right? So let's say that you have a field of rational functions just in two variables, x1 and x2. And inside, you can see the commutative ring generated by this rational functions. Okay. So that's an interplay between generating by, you know, you know, by polynomials and generating, allowing to take rational functions, right? So yeah, so that's a commutative ring inside F. So that's a commutative ring inside of M. Now, what is this cluster algebra happening? You divide these generators into clusters. And these clusters intersecting. So neighboring clusters intersect. So they have some intersect. They intersect only by one element, always. So clusters themselves can be longer, but changes only one element changes all the time. Changes all the time, okay? Only one. And so, what you see, and the question is: how does this element change? And the rule, how this element change, the rule is the follows. So, this is the same, right? But the rule is encoded either by a matrix or by, you know, equivalently by a quiver. And what's interesting, this quiver just explained you how to compute this, how to compute this guy in terms of x1. To compute this guy in terms of x1 and x2. And what is interesting that this quiva or matrix, which encodes the rule how you're going to change or mutate, as they say, this cluster into another cluster, mutates itself each time. So rule changes each time. So in other words, you start here. So as they say, right, you start here with some cluster. Start here with some cluster, initial cluster, for example, anyone, you can be initial cluster, and you have a quiver associated with this cluster. Then you use up this quiver to obtain this element, and then you also mutate a quiver to get another queer. Okay. And there's certainly lots of, so this approximately works like this. So now what's so wonderful about them? Well, there are certainly nice, wonderful properties. For example, well, For example, well, someone could actually call Laurent phenomenon. And Laurent phenomenon, okay, so Laurent phenomenon tells you the following. That you can express the elements. So, of course, in this particular case, I gave you a very, very simple case here. But in principle, it could be very, very complicated when it's not clear, right? And so many sequences. And so that every element. Sequences. And so that every element of the cluster, well, for example, every element of arbitrary cluster, for example, this can be expressed as a Laurent polynomial of elements of another cluster. Right. So any element from other cluster can be expressed in Laurent. So when you have such a trivial, maybe it's more obvious, but in more difficult situations, it's not obvious. But in more difficult situations, it's not always. So they have this very beautiful properties, sort of amazing property of different types. Here, I actually on the next slide, which will appear in a second, I think it's a bit delayed, you will see the queer mutations rules, but I don't want to stop on this because you don't need to know how it exactly works. So here is, again, again, I don't want to because I either have to go into details. Because I either have to go into details, but this is approximately how it works, right? So you have a quira, right? And then you put this is one way of writing it, you put rational functions in the vertices, and then each mutation just changes them. Okay, but now for Grassmannian, okay, so it's it's okay, so it turns out that cluster algebra, so Grassmannian, homogeneous coordinate ring for Grassmannians, that's apparently a classical, turns out to be a classical example for the finite. Classical example for the finite type cluster algebras. And what's happening is, in fact, they are parametrized. So, what's happening? They are parametrized by triangulations, by different triangulations in the following way. So, okay, so here, if you consider this one, right, this triangulation corresponding to the cluster one, so you just count diagonals. So, it's proper diagonal. So, it's T13 and T14. T13 and T14, right? So here is the plug correlation which I showed you before in classical situation. So this one, how you're going to mutate. So what you're going to, so yeah, and let me explain mutation. Mutation is about classical anthropology when you describe all different triangulations. So what happens, right? So you take quadrilateral, right? And if there is one diagonal mutation, if you just erased one, erase this diagonal and instead draw this one. Diagonal and instead draw this one that's just a flip called a flip. And so mutations are done by flips. And okay, and instead of this cluster, we have one, two, four. So this one and one remain the same. So you see some, at least some how this approximately works. Okay, they probably have like three minutes left, right? Let me show what we have. So it spoke approximately like this, right? And this relations tells you. And this relations tell you, so in principle, if I want to show you the full picture at every vertex, I have to write rational functions. Okay, and these rational functions I will be computing using this Pluke correlation. So now what we have done. So that's what's, of course, known. So this is classical result, how pictures look, how this triangulation, you know, how Grassmannian looks for the Grassmannian 2.6. So it's very classical. So in our situation, let me show you. In our situation, let me show you a more difficult example. Actually, yes, let me show φ2. So, what happens if we have two zeros, so planes completely generated by two even vectors in a 5-1 ambient space? So, it's still super case, right? Because you have one odd basis vector in the ambient space, right? But the planes are generated only by if. So, in principle, let me first. So, in principle, let me first show you the exchange graph. We're going to have in this situation the following picture, right? And so, what we are proposing, so let me just maybe do it on this picture. We're proposing the following thing. So, it's again, it's proposal cluster structure. At the moment, people who know this are wondering, you know, what trying to make sense of what we're proposing. But actually, the mnemonic rule, let me just formulate it, it's very simple. So, we're proposing our mutation. So, we're proposing our mutations going to have. So, first of all, what is our clusters? So, our clusters now, mnemonically, can be also described by the same triangulations, but they're going to be described by what known as decorated triangulations. So, in other words, you have triangulation, and in addition, you have dots. So, the dots can be only endpoints of the proper diagonal. And so, for example, this one corresponds to the cluster containing. To the cluster containing even elements, this one and odd element theta one and theta four. Remember, Greeks are for odd. So these are odd. Okay, and so we have two types of mutations. One type of mutation is called odd mutation and where the odd cluster variables, so this guy is called cluster variables, just jumps, they just flip. So four was sitting here, right? Four was sitting here, right? And it just jumped into three here. And one of our relations governs this jump. So we have relations which tells you how to write corresponding rational function here. And then we have another type of mutation. So this one odd. And then we have another type of mutation, which called even, which is happening here. And mnemonically, you can understand this way. So you have here quadrilateral, right? This one. Right, this one, uh, quadrilateral, this one, and you have a proper diagonal. And the setting for mutation is when you have it's like a dumbbell. So if you have a diagonal and also the two marked point, if so, you do a flip and then this dumbbell mutates all together. So all three elements mutate together. So in this situation, right, we have one, three, t1, three mutates together with ceta1 and ceta3. Take together with theta one and theta three into here. And so on. So, in principle, what we have, the classical mutate exchange graph, as they say, basically how this mutation goes, that you see the periodic. In classical situations, they just have five. So, for pentagon, they have five different triangulations. So, we have decorated, so we have 10 different ones. And so, that's how we have a graph. So, in principle, So, in principle, just to finish up, wait, we have, right, so this is my crazy picture of what I do for 2, 0, and 6, 1. And what I did, I took a picture for G2 in 6, 1. So in G2 in 6, so classical situation, in just a little bit. So just insert some members, but it's get much, much more complicated structure. And right, and so. Great, and so it's almost the end. And so, well, okay, so I just wanted to show you that we have other relations, relations say for one, one plane. So that's, I just put them particular way. One, one, for example, sub-example, one, one planes in two, two space. We're not completely even case, but planes also allow to be generated by odd ones. So, oh, I don't know what happened. Ah, okay. So, Zoom is very smart. Zoom is very smart. Okay, and so that's for example when we use these three difficult constructions, which I was explaining to you. We may have such relations, for example, which are not any more linear in thetas, in dues, and so on. But here, we don't even have any conjecture on how to construct cluster structures corresponding to this. So, and yeah. And I think it's my time. And I think it's my time to stop here. Thank you very much for listening. Any questions for Ekaterina? Well, it's not. Let's thank the speaker one last time. Thank you very much. Thank you. And then we're done for today.                   