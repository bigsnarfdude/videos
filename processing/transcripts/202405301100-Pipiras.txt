Being here, you know, great, great place, great talks. You know, I was ready to say it was great meeting you all, but you know, after previous talk, you know, and kind of everybody except maybe Braxton, because the thing he did with his game at the end of the talk, I mean, that's not nice. Not nice. Nice. Not nice to everybody else, you know. So, I mean, so, but anyway. So, as you can see, I'm going to be speaking about multivariate, possibly high-dimensional time series models for multiple subjects. When I was asked to present something in this conference, I think what I picked up from the title was high-dimensional data. So, this is sort of one. So, this is sort of one type of high-dimensional data I'm thinking about. Maybe at a few times, I'm going to draw connections to networks. I mean, certainly one can, there are lots of places here one can think about networks, and I'll try to indicate those places as we go along. It's going to be a high-level talk, just gonna sort of tell you what we're doing, what I find interesting. What I find interesting, you know, perhaps what the challenges are without really getting into details. So, no theorems? There are going to be any theorems here. I could write some theorems, but there will be no theorems here. I know the crowd is a bit of a diverse here, you know, but again, given it's going to be a high-level talk, hopefully, you know, you guys are going to take something. You guys are going to take something out of it. Certainly, people like G, I think, and Todd, I think you're probably going to see a lot of common themes too. Certainly, things you've been thinking about through your career. I have little doubt. For others, I think some of this material is going to be probably relatively new. A number of my former students were have been involved in pushing this along. In pushing this along, Zach. Zach was actually a PhD student in our neuroscience, psychology neuroscience department. And after that, he stayed as a postdoc with me. So this is sort of when we started looking at some of these issues. He's currently a faculty at Pennsylvania State University. And he's sort of the one that's grounding very much this work in applications to. Applications to certainly psychometrics, but maybe a little bit less so to neuroimaging, neuroscience. Chris is his PhD student, who's been recently involved in some of the aspects of this effort. Yangoon and Marie, I said, the other two PhD students I worked with. With Yangoon, we started looking at the Started looking at other kinds of models. You're going to see them in a moment. And with Marie, we're going to think more of the theoretical aspects of some of these models. So I'm going to just skip, you know, just go to the last slide. This is what I want to get at the end of the talk. I think it's an interesting direction with still much to be done. I'm going to be discussing two classes of models. Two classes of models. I'm going to refer to them as multivar. So that just means I'm looking at vector regressive models. You see those defined in a moment. It's a sort of a canonical time series model for multivariate data. And multi-just refers is going to be looking at these models across multiple subjects. And there's going to be stuff another. And there's going to be sort of another canonical model, factor model. So, in time series context, one sort of calls maybe them dynamic factor models, but there's going to be factor models. Again, looking across multiple subjects. I think there are a number of interesting issues around heterogeneity, sort of scale, and sort of the approaches that one takes. I think there are many applications areas. I'll give you a few examples. I'll give you a few examples of what we apply these models to, but I mean, I think there are many more instances that one can apply these models to. That's the goal. I do not have too many slides. So starting a bit with a sort of general setting. So we're thinking of we're considering time series data. So consisting of d variables. So for each time t, one is looking at d variables. So from one to d. So if it's a sort of typical multivariate if this large sort of high-dimensional time series data. Series data and one is looking at across different subjects. So a subject here is indicated by small k and that goes all the way to capital K. I'm going to be focusing on models that, or even data that look stationarish. In other words, things do not change as you move along in time. We're thinking of potentially large d no, small, smallish t, d could be larger than t if we went. We're also thinking, looking at this data across stuff, thinking that case objects are independent. So, as far as models are concerned, typically kind of data we're dealing with sort of things are independent across objects. But obviously, naturally, one is looking at that for each subject across subjects, we're looking at the same variables. So a component series, so there was the component series, measure the same thing across different subjects. There are many applications where this kind of data arise. These kinds of data arise. Again, thinking about subjects as being K-people, I think you mentioned that a lot of this is grounded in psychometrics. So there's a lot of psychometrics type data, for example, where one would be measuring emotional states of a person across time. So, in other words, there's a bunch of emotions. Are you sad? Are you happy? Are you angry? Happy, are you angry? And every day, an individual would have to indicate how she or he associates with that emotion on some scale. That's sort of a typical data that people consider in psychometrics. Another term that people use is diary data. So in other words, like on the phone or something, you can measure your physical activity and a bunch of other variables collected over time. Uh I think fMRI was mentioned. A number of instances. Yeah. Yeah, I don't know if exactly that's washing. Certainly the sample size does not have to be Does not have to be independent of this subject. So I, you know, it's to simplify placed capital T, but I mean, whatever we do, it could have been T of K. But time is not important. I think for the kind of things we're thinking, it's not important. I think it may start being important if you start looking perhaps at what I got factor models. We'll talk about factor models. There are some factors that potentially influence all the people or something, all the subjects. Then you kind of potentially, let's say, if a factor is something representing what happens over time, so maybe day of the week or something, or something like that. That might become important that people are measured exactly at the same time. So here, that's not what we're thinking about. So, again, fMRI data. fMRI data. So maybe not at the pixel level, but at ROI level. So maybe, you know, typically, I know 200, 300 regions of interest, you know, where one is measuring stuff bold activity in the brain. Again, doing this across different subjects. This, you know, one can map stuff otherwise to the same ones across different people. Some of the examples. Some of the examples, I'm mentioning this because some of the examples I'm going to show you are going to concern this particular data. You know, in economics, you can look, subjects do not necessarily have to be people, could be countries. And let's say for each country, you have sort of a bunch of macro variables or something. The independence assumption is a bit harder, I think, to make. Countries are related, but this is sort of another typical data, time series data across different subjects. Data across different subjects that people consider. So, the kind of questions with this data that we're interested in is, is there anything in the dynamics, you know, as one is looking at this data across time, that's common to all, I don't know, maybe most or you know, many subjects, but maybe what are individuals and maybe some other. What individuals and maybe some other features? I do not necessarily know if I can make a case why this is an interesting problem, but certainly this is the kind of problem that has been interesting of people working sort of in applications. I mean, I think, how is this sort of translate to whatever people do in the real applications? It's a bit harder case to make, I think, for me, but for example, the Thing for me, but you know, for example, the emotional stage data people sort of certainly thinking about interventions to do, you know, affect emotions. And you may want to maybe start with the kind of interventions that apply to the whole population. So you'd be looking at something that's common to all subjects rather than sort of individuals. Or maybe sort of if you think of sort of maybe personalized medicine or something, you can start looking at individual individual effects. Individual effects. Sure, anytime. At the moment, I think it's going to look like unsupervised. You know, with any of the time series data, you know, one of the questions is prediction, you know. Is prediction, you know. So you can start also asking, you know, I mean, if you want to do prediction, you know, is it worthwhile to start thinking about fitting these models across different subjects to be helping with the prediction? We looked at some of these questions. That can happen. So I'm not claiming here that we're the first ones looking at this thing. We're the first ones looking at this thing? Definitely not. I mean, you know, people have been thinking about these questions for a long, long time. You know, I'll put here just a few references just in the time series context. So, for example, there are this sort of multi-level model, specifically in the time series, sort of multi-level vector to regressive model. In psychometrics, there's sort of a very popular Gimme approach. Group, iterative, multiple model estimation. Model estimation. There are related matrix autoregressive models, simultaneous component analysis that you're going to see me coming back to in a moment. You know, sort of a bunch of other ones. And in the non-time series context, there's even more. And maybe I should say the obvious thing is that, you know, in a way, one is also kind of the most basic statistical framework. You know, when you have a population, you know, and you're picking capital K here, you know, a typical notation would be. A typical notation would be maybe small n or big n, but just instead of having one single point, like you would have statistics 101, you have a high-dimensional series associated with that, you know. I know here in the audience, some people for this type of data would not necessarily take the perspective of a time series analysis. You know, some people would look at functional data. So, in other words, you would look at whatever data you're observing over time and look at that as one. Observing over time and look at that as one curve and sort of try to apply some of the functional analysis tools to analyze this kind of data. So here I'm sort of taking more a time series perspective. Yeah. They're all basically elaborate. Is there stuff out there that you did that would probably not be? I think I'm gonna be just uh you know assuming linear type dynamics definitely. I mean I think you can there's a way to consider some of the non-linear models but that's not what I've seen people doing so far. Okay, so again, what I'm going to get into is discussing these contributions, look at us multi- or multi-subject vector regressive models and multi-multiple subject sort of factor models. And these are going to have sort of common plus individual plus subgroup features, or maybe combinations of those. And maybe sometimes I will say. Maybe sometimes I will say I'm not really going to get into details of saying how does this compare to any of these previous approaches. So I'm going to admit that. You can hopefully, I'm not offending people here who have done related work and not included here, but again, there's a lot of missing references. Okay, so starting with multiple subject vector to regressive models. Vector-to-regressive models. And just for simplicity, I'm going to just look at vector-to-regressive models of order one. And these ideally basic models. So one postulates for the kth subject that data follows this vector regressive model of order one. So it's really sort of basic linear model, if you want. Again, this is a DJ. Want again, this is a d-dimensional vector column vector consisting of the d variables. Again, this is the same vector but at the previous time, right? And then this d by d matrix. So, you know, if d is large, potentially seriously large matrix, plus the error terms. So the white noise type, white noise series with mean zero and With mean zero and covariance matrix sigma k, which can depend on the subject. So, in this multiple subject, the multivar models postulate that these transition matrices are decomposed into two components. Again, this is a D by D matrix. It's being decomposed into a D by D matrix, which is common to all. A D by D matrix, which is common to all individuals, all subjects, and another matrix, D by D matrix, which now can depend on the individual. And furthermore, especially for large ID, the common and individual matrices assume to be sparse. So you'll see some sort of penalized estimation in a moment. Do you have any quickly? So I think whatever I'm discussing could in principle be done on the covariance matrix and people have done some versions of that on the covariance matrices, but at least here we're sort of focusing on the dynamics part of the model. So we're just looking at transition matrices phi k. Yeah. Definitely. We'll get to that. Definitely. We'll get to that definitely. But the question is, there's certainly identifiability issues around this model, and we'll come back to that. You said we do not sigma here? Yeah, yeah. At the moment, it doesn't figure in the objective function. The norms that you see here are so the L two norm, the L one norm. Component-wise L1? Yeah, component-wise L1. I think I said I'm going to just mention, I'm not going to get into it, but there's certainly a lot of places, for example, for this model and the next model, and where people look at these models through a network perspective. So, a network perspective, there's certainly a lot of people. So, network perspective, there's suddenly a lot of work. I'm not going to do it here, where one thinks of the transition matrix, the D by D matrix, think of it as adjacency matrix of the network. That's probably not surprising. There's been a lot of work. So, this is one way to make connection to networks if you want. And you can also see another sort of natural connection is that you'll see me in a moment. Because when he's doing it across people, you can also start sort of thinking, I mean, Thinking, I mean, it's the, you know, whatever model I'm fitting, this model flows across subjects now, you know, and so that they relate subjects in that way. So there's another underlying network there if you want to go that route. I think the way I'm writing here, this really, you know, just requiring those to be sparse. One can try to put other penalties than low rank, then the sparse, the sparse penalties. People have done that too. It's becoming nice. I feel like you almost giving my talk, you know? That's cool. Thank you. So I think most of you are already looking at, you know, as far as the way one is estimating this model, you know, here's sort of the usual L2 loss, and one is adding this L1. Adding this L1 penalties to get this common and individual matrices that look sparse. It's sigma k? Yeah, at the moment it doesn't figure in the equation. Yeah, I mean, you know, it's a bit of a separate issue that even if I just estimate this model. Even if I just estimate this model sparsely, just only phi k, you know, typically it's done without including sigma k in the optimization function. And if you do not, it's a bit of a maybe strange that if you're dealing with a stationary model, and if you do not panelize whether you include it in optimization or not, it actually doesn't matter. It's sort of a general result of some sort in statistics. If anybody wants, I can point you. So it doesn't really matter. The moment you start considering penalized version, You start considering penalized version, it starts to matter. But you can try to incorporate into Odel 2 law, sort of maybe add another penalty only. That's doable, but I'm not writing it here. I don't know if you've people seen any insight. I mean, what's the insight? What exactly one is doing here? I think probably the simplest insight that one can give just. That one can give just by looking at this one is that what happens is that if you fix whatever total effect you estimated, so suppose I estimated whatever my optimization gives for the sum of the two, that's the total effect. What happens is that depending on the choice of these penalty parameters, the population version, the gamma zero version, you can think of it as some sort of weighted media. Of weighted median of the elements of the matrix across the subjects, or maybe zero, depending on the choice of the penalty parameters. So, you know, one is doing something reasonable and maybe a little bit more in an automated way, sort of trying to automated an adaptive way to trying to take medians of some sort in a sort of intelligent way, potentially also getting sparsity out of it. I'm not writing. We did spend quite some time looking at the adaptive versions. In other words, when these coefficients, penalty parameters is the more data, more, I don't know what's the best ways to phrase it, but I think most of you probably know what that means. And again, this adaptive version sort of looks a bit different from sort of the kind of adaptive. From sort of the kind of adaptive penalty parameters you see in sort of regular last. I think we tried various versions of optimization. I think the latest we use is one of the fista versions. And much of this is implemented in this R package multivar. There are a lot of related approaches, certainly in non-time series regressions context. Series regression context and all kinds of various names. So, shed lasso does something like that. Multitask learning that's not a type of dirty statistical models. Stephen some other ones. I guess you probably guess why somebody called some of these models dirty statistical models. I suspect, say it again. I suspect it goes back to G-Scom. These models certainly have identifiability issues. So you're kind of doing something a bit dirty, because by identifiability, I can always add and subtract an element of a matrix. So hold on a second. So what exactly is one? Second, so what exactly is one estimating here in the first place? So I'll mention there's been some theoretical work, some of it I do not necessarily understand well yet, but there's certainly something we've been thinking about. I mean, putting these identifiability issues aside, I mean, it's, you know, the moment you try it in practice, it gives interesting stuff, you know. So I'll just give you just a few illustrations. I'll just give you just a few illustrations. So you start also getting a bit of a feel what exactly is going on. So for this illustration, simple one, there are five subjects, let's say the only plots for two subjects are displayed. One is considering 10 variables. So transition matrices are 10 by 10. So these are 10 variables, same 10 variables. Variables 100 time points, so capital T is 100. And in this simulated example, one assumes that there are among those, so 10 by 10, so 100 matrix coefficients. So sometimes, you know, at least in psychometrics, they call them paths. So there are 100 paths, let's say 100 effects. So in the simulation study, you want 10 common paths. 10 common paths. So these are the ones indicated here: 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. These are the 10 common ones. And then one adds 10 individual paths. So this is for individual 1 and for individual 2. So it's the same 10 as for this common matrix. So like these two, you see the same ones, but 10 additional ones are added here and here. And here. And in this simple example, if you run this multivar, it's the optimization that I described. That's the estimated common effects. So this is the estimated total effects. I mean, and you know, it's not doing a bad job. So certainly there's sort of interesting things going on as far as just seeing how these things work in practice. How these things work in practice. I don't remember. Obviously, that's important. This is a bit of a simplified example, you know, in the sense that, for example, when it's assuming that these ten paths are shared by all the individuals. All the individuals. That's a bit of a stretch, probably, in most of the applications. But you start seeing that there's something interesting with this approach if you start breaking down and getting more into heterogeneous situations. Now, what do I mean by heterogeneous situation? I don't know exactly if there is a definition what heterogeneity means, but here's one other example. I'll give you sort of maybe at least one way to start thinking about it, where we see that this approach also. We see that this approach also works pretty well. So, in this example, there's an example which I'm calling a null heterogeneity. So, this is the example where one is just taking, so there are still 10 by 10 transition matrices. And in the no heterogeneity situation, when it's just taking 20 non-zero paths, so these are the 21s indicated here. And so, this is the first. And so, this is the first number indicated here. And the second one just said how many of those 20, what's the proportion of the population is going to be sharing those 20? And this just one means that all the individuals are going to be sharing those 20. So, individual one, individual two, individual three are sharing exactly the same paths. Okay, so just no heterogeneity in that sense. And then one is sort of trying to push this to something may look a bit more heterogeneous. May look a bit more heterogeneous. So, for example, this is the low heterogeneity situation. So, 20 paths out of 100 are shared by everybody. 10 out of 100 are shared by two-thirds of the population. And then five out of a hundred are shared by one-third of the population. So, this is the picture. So, for example, this path probably shared by everybody, this one, this one. By everybody, this one, this one, this one looks like it's shared by everybody, at least by those three individuals. Like this one is not shared by everybody, so it's one of those two. It appears here, does not appear here, but it appears here. So if this is the low heterogeneity, and pushing this envelope to the high heterogeneity, 20 out of 100 shared by one-third, 10 out of 100 by two-thirds, and maybe five by And maybe five by everybody else. It's the visually represented here. I'm not showing you really any sort of performance plots or anything, but we see that, again, if you apply this multivar, the adaptive version, it generally outperforms competitors, you know, as far as, for example, path recovery, the kind of errors you're making, and so on and so forth. That's one thing they care about. Yeah. So hopefully, it gives you a little bit of an idea of what's going on here. I'm going to say a few words of the kind of things that we've been doing or thinking about in this direction. In this direction. One of these, we certainly something see that if we run some of these models on more heterogeneous situations, is that if you have a path which is not shared by everybody, but just by a part of the population. So, for example, like in this low heterogeneity situation, when I'm suspecting that this part here, maybe it's one of those two-thirds. So, it appears here. So it appears here, it does not appear here, and it appears here. Suppose it's one of the two-thirds shared by the population. I mean, what would typically happen if you run this multivar is that this path is going to be estimated as common to everybody. And then the individual effect for this individual corresponding to that path is going to just try to compensate it because I don't really have it. Have it. It's kind of probably not too surprising, you know. If you really wanted to maybe capture the situations a little bit more explicitly, you know, for example, you can do something that looks like a fuse penalty. So, in other words, you penalize these to be sparsely, but also for the sum of the tube. That's maybe not to say. And there are different versions of this fuse penalty that one can use if one wants to. Use penalty that one can use if one wants to. That's sort of one direction we've been thinking. The other direction, again, maybe adding a little bit more heterogeneity into the model is the one where one is not just having common and individual effects, but some sort of subgroup or community effects. If anybody's interested, there's a recent paper of ours: how to do it, even not necessarily. To do it, even not necessarily knowing the community structure to start with. I think there are interesting questions to think about as far as cross-validation goes. I think at the moment we're just using block, the most basic block cross-validation, where we're doing the same thing across subjects, but you know, various versions where you sort of train on one subject, trying to validate on the other, sort of, and things like that potentially would be. Sort of and things like that potentially would be interesting too. This non-identifiability, as far as theory is concerned, again is an issue. There's been theoretical work of trying to prove things in this even non-identifiable setting, but I don't know, I d I don't wanna maybe insulting you, it hasn't quite yet passed my BS filter what exactly is going on there, you know. Filter what exactly is going on there, you know, whether that's useful or not necessarily useful. Certainly, as far as theory is concerned, one interesting aspect, none of the theory at the moment reflects anything related to heterogeneity, which I think is sort of the more interesting aspect of trying to use this model on populations. Yeah. Yeah, definitely. Yeah, definitely. Definitely. I just raised as a question and what we're doing at the moment, but there's certainly different possibilities to try. Okay. As I said, I think there are a number of areas one could try to aspire, see if there one suspects the sort of dynamic component to the data. Dynamic component to the data one can try. Just as an illustration, not really just to make a case of how great it works in application or give it, but just so you sense a little bit what kind of things one gets in the first place. This is from the original multivar paper that we wrote. So across 11 weeks, daily responses, so the total number in time is 11 times 7. So 77 time points for each individual. Time points for each individual. 16 participants, so that's capital K. And there are 20 variables. So 10 positive affect, again, sort of 10 positive emotions and 10 negative affects, negative emotions. What you see here is sort of a common estimated matrix. These are These are not only that you cannot read, but it's not like I can read on the screen. This is glad. I can't really read it out. So these, I think, positive affects. These are negative affects. Sad, guilt. I can't read the other ones. But scared. Maybe not surprising, the sort of positive, reinforced positive, negative. Reinforce positive, negative, reinforced negative. You know, that's maybe not too surprising. And again, what you get in addition, sort of, are the individual models. I mean, I maybe didn't really say, but I think one nice feature about these models, if it's not obvious, especially compared maybe to some of the alternatives, is that you get models, certainly at the individual level, that potentially can look qualitatively different, which is a nice feature. You know, which is a nice feature. So you get the population model, but as far as the individual ones, you sort of get something at the potential, it can look quite different across the individuals. That's one nice feature about these models. I don't know if there are any questions about this model, or any comments as far as this model goes. Again, I don't have those numbers here. Yeah, I think it may not directly answer your question, but you know, again, it's not, I think it goes from minus one to one. Goes from minus one to one, you know. You know, obviously, your, you know, maybe not surprising, your values of the transition matrix, sort of they're typically restricted to sort of range minus one to one. So you get stability in the model. But, you know, when I look at this one, it tells me, you know, this some of the darkish blue that when it's estimating pretty large coefficients, which tells me that, you know, it's probably explaining quite a bit of the data. There's a lot of dependence in the data that when it's explaining as far as when it's That when it's explaining, as far as when it's far the dynamics is concerned. Yeah. You don't do anything predicting like the growth of the course it's kind of nonsense to predict that over 1905 to the growth of you know you're just going to start being hilariously happy all the time. I don't know. Maybe, I don't know. Again, I don't know. I mean, I mean, I'm hearing that one is estimating. Certainly, I think it's. You know, one is estimating certainly, I think, a stationary model. You know, the fact that the coefficients are large here, again, it means that we're certainly going to spend a lot of time being happy or unhappy, you know. But other than that, I don't know. Yeah, I guess we're going to have to go to the next one. When you say penalty function, I think it means penalty parameter. Let's see if I'm following what you're saying. You think about things like that, like you know, adding penalty to this one or penalty and add it to. Penalty to this one or penalty another term to this one. Yeah, I think again, these penalties, additional penalties, additional terms sort of serve different purpose, you know. Like in other words, this is again this is the one that you know when you're trying to identify the situations that you know one element is the individual. One element is the individual is trying exactly to compensate for the common because I do not have a path present at that at that coefficient. Implementation of this, I mean, at the moment, I don't think we're at the place where we are even happy with the way optimization works, I think, you know. But if that's what you're asking, if you do not have this term, typically, you know, you would get something where the sum is potentially not necessarily zero, but close to zero. And this would be the term that would force it to be equal to zero. To be equal to zero. I'll try to force it to be equal to zero. Anything else? Then you're sort of thinking everybody. Yeah, I think. Yeah, I think I hear what you're saying. I think what one is trying to do with this model is kind of also get the right sparse solutions, which I think you can't, I mean, it's a bit, you know, I think once you try to get sparse solutions sort of, you know, maybe shrinking towards the mean, and then you wanted that to be potentially zero, I think you may kind of end up with a situation like that. Try to do something along these lines. Definitely. Definitely. Yeah, and people have been doing some of that, not in the time series context, but in the regression context. Yeah. Definitely. I mean, yeah, you can do all kinds of other penalties. No, no, no, this uh this is not uh there's no treatment here. There's no treatment here. One is just individual. There's no treatment here. One is just individuals. I mean, the data itself, I think, is much more complicated than the one. I think there were already sort of groups of individuals having some neurological disease in the first place, and divided into groups or something. So I'm ignoring a lot. But otherwise, other than that, no, everybody was just responding to like a questionnaire with, you know, how do you identify with those 20 motions daily? No, we don't really do that. I mean, whether if you panelize where you necessarily estimate a stationary model, it's a bit of anything an open question, but I think typically you do. I still have fifteen more, ten minutes or something. We're not going to play games, so I'll tell you just briefly the other kind of model with the same type of data that we've been experimenting with. Maybe it's a little bit more preliminary, but on that front, we've also been looking and have some sort of more theoretical results. And this is this is. Results. And this is this multiple subject or multi-factor model, dynamic factor model, if you want. And what one does in this model, again, one sort of similarly assumes that there are here sort of common or joint components, so to all individuals in the population subjects, potentially subgroup or individual components. So one is thinking of as data consisting of those three components. Three components plus noise. And not only that, one is furthermore assuming sort of a factor of low-rank structure on those individual, on those three components. More precisely, this low-rank structure, factor structure is sort of restricted in the form of loading matrices. People call loading matrices. Matrices, people call loading matrices, and factor, factor series or factor scores. Now, what makes this common component is that the loadings are the same across all the subjects. So, the series themselves depends on K. I mean, something has to depend on K, obviously, because I'm collecting data independently across individuals. So, what's common across So what's common across individuals is the loading matrix. So again, for R is how many factors one has, or how much of the dimension reduction one is getting. So D is large, R is small. You can think of R is being just one, if you want. So it would be just one long vector or one long matrix with just a few columns. Let's say with a subgroup or community, this matrix is allowed to depend on the community. To depend on the community that the individual is in. Okay, to give a little bit more heterogeneity. And again, in this one, something which is completely dependent on the individual. Certainly, some assumptions need to be placed for sort of identifiability of these different components. Maybe I don't think that's surprising, and I'm not going to get into what those conditions are. What are those conditions likewise? There are different assumptions that one makes on the factor series as the series in time. Like, for example, for FTK, one is a, you know, they can be arbitrary. So if I'm dealing with stationary data, it would be some arbitrarily stationary across time. What I'm writing here, this is one other popular formulation. That's probably the one that's mostly. That's probably the one that's mostly used in applications called SCAPF2 or Parafact2, whatever that means. This is the one where if one looks at the covariance matrix of these factors, without the C case, that we just say it's common across all the subjects, so that does not depend on K. But one is allowing, when it's putting diagonal matrices on the two sides. Matrix is on the two sides of the covariance matrix phi. So diagonal matrix just says that if I have this matrix to be diagonal, that if I look across the individuals, that the variance of my factors can vary across the individuals. That's what it means. You'll see me once I mentioned maybe some theoretical results. You can also move this CK on the lambda k. On the lambda k. So, in other words, F T K have the covariance matrix of phi, but if you look at the loadings across different subjects, they wouldn't be quite the same. They wouldn't be quite the same, but across the individuals, they would be lambda times ck. Maybe one other thing I'll mention is that this is written. I'll mention is that this is written as for one individual k. This is written as the combination, the summation of three components which look similar, but I could also just put them all together sort of in this form, where I would similarly have the loading matrix of dimension columns R plus R S plus R K, right? And similarly, you know, with all the other factors. So, you know, as far as factors. So, you know, as far as factor models are concerned, it just means that there is looking into individual K, there is this part of my large factor model that is common across all the individuals. And similarly, then maybe for communities and sort of individuals. This is a bit of a different animal. If you're sensing then, this vector. You know, if you're sensing then this vector regressive model, so for example, these factor series, I mean, in principle, the way the model is written, they do not have to be even dependent over time. So in principle, there's no dynamic part altogether. They can be dependent. If the factor scores, for example, follow a lower dimensional vector-to-regressive model, you would immediately get some sort of dynamic part. And then there is a connection to the previous models. But in principle, you don't even have necessarily to have a dynamic. Necessarily to have the dynamic part to the model. This kind of seems like it could take a lot of time to read. Do we know what properties of time series they have to be disposable like? Again, I don't know if that answers your question, but you do need identifiability on the components or on these components so you can sort of write it uniquely. I mean, it's not the only one, but a typical one is that this loading matrix is sort of orthogonal in a suitable sense. So, you know, for example, the typical situation is that the population one, common one, load. Population one common one loads on one set of variables, the individuals load on the other set of variables. That's sort of a typical situation that would be identifiable. How does one estimate a model like that in the first place? So uh just wrote down sort of for high level. High-level typical steps ones take. This way of trying to get sort of common individual, et cetera, effects. People have been doing this sort of things in this literature on data integration. I don't know if you heard this word. This is a bit of a trying to, I think, take this data integration into. Take this data integration into different directions. The typical data integration is where you have a, you know, you're collecting data, potentially loss of variables like gene expression data. That's sort of one technology across individuals. But you also have other technologies, you know, like protein expression or something. And then you're sort of trying to integrate this thing. And so this similarly, you know, would become one component to the two techniques where data has been collected and maybe sort of some of the individual ones. Maybe sort of some of the individual ones. So, what one does, what we do is that we're using some of these methods to sort of get these components in the first place. And if you're interested, I can talk offline a bit how this is done. Many of these methods have this sort of rank selection incorporated, you know, because how you decide on this, you want to get just the joint component, how you decide on the number of. The joint component, how we decide on the number of factors to use across all the individuals. That's sort of a hard problem. If you know R, it's sort of not that difficult estimation problem, but how you decide on that number is difficult. You know, once one identifies those components, getting these factor forms, you know, one can use like this simultaneous components analysis. Components analysis. If you think there's dependence over time, once you identify these factors, of course, you can look at a time series model if you want. Let me just give you a little bit of a simple example so you see a little bit what's going on. This is a simple application of one of these data integration methods to the time series data. There are just three subjects, one, two, three. And typically, with this all these data. And typically, with this all these data integration methods, you will see blocks of data. Again, this is not a typical picture. You would see typical data integration. This would be the subjects, this would be the variables for different types of data collected. Here, blocks are being formed slightly differently, where different blocks correspond to different subjects. And potentially, the number of blocks, depending on your subjects, can be large. Typical data integration. Typical data integration, the number of blocks is small. So, here, number of blocks like, you know, can be 100 subjects or something. These are the variables. So, here one is considering 10 variables, one, two, three, blah, blah, blah, blah, blah. The data is at 10 time points, 100 time points. So, variables, time points. This is the observed block data for subject one. And similarly, all the way for subject three. This is LTL simulation. This is a little simulation illustration. What the way the model was built is that the first four variables had the same loading for all three subjects. And then there were individuals loadings. Individual one, just loading on variables five and six, for individual two, loading on variables seven and eight. And similarly for the last one, this is the data, sort of this is the output of this data integration. Output of this data integration application methods. And you kind of can see that it's Lisa is doing exactly sort of the way the model is built. This is the common component where you sort of see it's recovering exactly the way the first four variables vary. And then the individual ones. Again, variables five, six mostly for that individuals, seven, eight for the second individual, and so on and so forth. Once you fit these models, you can do all kinds of, you know, as far as application is concerned, but I'm running a little bit out of time, so I'm going to skip the data application. I'm just going to tell you sort of the other kind of things that we've been doing. Again, data integration, you know, if you're in statistics, it's a large area. Lots of people are working. There's not too many people, I think, doing things as fast as time series data. Think doing things as far as time series data is concerned. As I joke, you think if I Google integration and time series, the links that pop up are related to integrated time series. You know, just when you take a time series and you sort of integrate, you know, like take partials and sort of what things are. So, as far as theory is concerned, I think it's quite open, certainly in the time series setting, just to give an example of the kind of things that we did recently. Of things that we did recently is we sort of developed a formal statistical test for really basic problem, the one where if I look at loading matrices corresponding to cased individual and I have a factor model, can I actually be testing that sort of they have this commonly shared structure? So there is a common component corresponding to all the individuals, potentially can transform. Potentially, again, transformer and lower dimensional matrix, which depends on the individual. So, this is sort of one little inroad that would look as far as theory is concerned. Again, developing a formal test in high dimensions, testing whether the model that one is fitting, it's even simpler than I presented. So, this component is not present, this component is not, is assuming a model like this with this structure. Is that even a reasonable assumption to start with? Even a reasonable assumption to start with. What does heterogeneity mean for this model? It's also an interesting question, and people have been thinking about certainly for a smaller number of data blocks, not when you have too many. What I mean by heterogeneity is that, again, if I think of this as being just one big factor model, when I'm writing this lambda, If when I'm writing this lambda, it means that this is something that's shared across all individuals. But again, similar with my multivar, I can start asking for another thing, and what if it's just shared by half of the population? This is sort of referred to as a partially shared structures. You know, how do you go about discovering those? I think people have a pretty good idea how to do it. If you have a small number of blocks, but if you have a large number of blocks, like for many individuals, like we do here, it's not quite clear yet. I'll stop here. So, kind of, it's still got there. Thank you. Yeah. This one? Yeah. Even if this type of type of week button. As I said, you do not you I guess I'll say two things. One is that again, if you, for example, impose a model across time for factor, that sort of becomes more like a time series model. But you do not have to, those factors can be independent across. Have to, those factors can be independent across time. Independent across time, that's perfectly fine. I mean, this is that fits in this framework. In the same way, it will be all independent. Yeah, maybe sort of the second thing I wanted to say is that, you know, because of this independence thing, is that a lot of things that we're doing, so for example, you know, when I said, you know, developing a test for just checking whether loading matrices sort of share a structure or something, you know, we writing it maybe more in a time series context where things can be independent, but again, this is the kind of things that extend to other settings as well. Things that extend to other settings as well. So, I mean, this is a fairly general setting with the one that we're just focusing sort of more data that's collected over time. Yeah. Yeah, yeah, I think it's a bit of a difference that goes back to Zach's question. I think that's those equations are typically non-linear, I think. In other words, here it's only linear setting, you know. Maybe again complications come from the fact that potentially high-dimensional, so lots of variables. I just wondering if this one I don't know anything about I know that literature exists. I know a little bit about it. Exist and a little bit about it, but the intersection of the two is not that great at the moment. Um is it possible to pick up a group of then you have some kind of a larger less spark estimate of the basis. Then you do another or operate different try to figure out the common part, individual part. So they this way. But uh are you going to get the same for first what you show is tuple stuff? Then you open my discomfort. Then we do one stop open up, another stop open up. Try to figure out the pattern we need that path for there are maybe some something we could do to keep. But if you like each kind of algorithm, they I assume maybe it's a like innovative algorithm. They need some initial value. So you missing those kind of things that help you better with the initial value. Okay, and then you you know start to come in with the algorithm. So what Yeah, I think the very basic original, I don't think from the optimization standpoint, it's not a difficult one, I think. No, we tried, I think, coordinates. I don't think it's a very difficult one. Well, but there are good like talks about splitting schemes for doing those by like there are good tools for solving side would be that's problem side one thing the best one okay well another way is to basically have more cost Okay, you'll do your things. Thank you.