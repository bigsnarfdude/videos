          So, I'll be giving a mini-course over the next four days, which I've been titling The Summer Squares Paradigm in Statistics. And in this course, what I'm going to try to do is, I'm going to try to illustrate this sum of squares technique for getting algorithms for estimation problems. So there's this, I guess maybe you inferred the catchphrase proofs to algorithms. So basically what this is, is it's a way to take a proof of identifiability and to automatically turn it into an algorithm as long as the proof is simple enough. So we start with a goal, which is to estimate some quantity from data. And rather than giving a formal definition of what I mean, I'll just give an example problem that we'll use as an example. So here's one example, robust mean estimation. Okay, so here what we're given is we're given a bunch of samples from a high-dimensional distribution over Rd. And these samples may be corrupted, adversarially corrupted. So an epsilon fraction of the sample will be corrupted. So we observe, I guess, so there are unknown samples. Cycled independently from some distribution D. Okay, and we observe some other samples, V1 through Vm, with the guarantee that for one minus epsilon fraction of the samples, Simples v dot is equal to v y. Okay, so some adversary can do whatever it wants with the remaining epsilon samples. Okay, and the goal is to estimate the mean, which I'll call mu. Okay, so this will be our problem for today. And okay, so the first question that you would ask is, for which settings is it even possible to estimate the mean? I mean, it's not generically possible. And okay, and then what we have in order to say that we can do this is a proof of identifiability. Let me make an official division of the board here. Okay, so under the following, so here's a claim. Okay, if the covariance matrix The covariance matrix of the distribution is bounded by identity. And the number of samples is at least the dimension times poly log d. Okay, here I won't be so specific about the exact SM colors that you need. That you need. Then, with high probability, we can find some estimate mu hat which satisfies that the difference between mu hat and mu is of most epsilon. Okay, so we have error that doesn't depend on the dimension. Sorry, squared is of most epsilon. So we have. Epsilon. So we have an area that doesn't depend on the dimension, it only depends on the faction and adversarial corruption. Now, okay, I guess at this point it will be good to say something. So all of the problems that I will be discussing in this mini-course have been studied, you know, for a very long time. And I am going to play to my strengths and I'm not going to make almost any bibliographic remarks until maybe the end of the talk, just in because I want to emphasize that technique. Because I want to emphasize the technique over everything. For this problem, Gautam was among the first authors to give a dimension-independent bound on the error. So at least when the author is in the room, I'll try to acknowledge it. Okay, and actually, something even more specific is true here. So here's like a refinement of the claim. So any subset Of the indices, which satisfies the following conditions. So defining mu bar to be the average of the samples in S. Of the samples in S. And, oh, okay, the size of S should be 1 minus epsilon. Okay, so any subset of 1 minus epsilon m coordinates which satisfies that its covariance is bounded, the empirical covariance is bounded. The identity has empirical mean, which is close to mean. So, okay, so the proof of identifiability, I'm not going to give it yet, but supposing that I prove this automatically, I have. Supposing that I prove this automatically, I have an inefficient algorithm for finding an estimator, right? I just look at every subset of 1 minus epsilon of the coordinates, and I'm guaranteed that the empirical mean of one of them will be close to the true mean. Okay, then I guess maybe I have to discuss some way to test this, but okay, you could do it. But we're not happy with this, right? Because this will take exponential time in the number of samples, like m choose epsilon n. M choose epsilon m is a running time that we don't want to accept. It's again squared, right? Sorry, oh yeah, again squared. For these estimates, is it supposed to be normalized by 1 over m or 1 over the size of the set? I guess it's 1 over the size of the set, but because the size of the set is 1 x epsilon, there's some epsilon factor that I'm. So it never guarantee that such a subset exists. Yeah, in particular, if you take the subset of uncracked examples, The subset of uncorrupted samples. This is a great point. Yeah, I should have said this. If you take the set of uncorrupted samples, it will satisfy this. With high probability. With high probability. Yeah, yeah. This with high probability carries down to here. Okay, so the really cool thing is that about the proofs to algorithms framework is that if you're able to take this proof of identifiability and turn it into a simple proof of identifiability, where here simple means sum of squares. Simple means sum of squares proof of identifiability of low degree, which I will define in a moment, then automatically you can get the polynomial time algorithm for the summation problem. Okay, so you just took your proof of identifiability, you made sure that it was kind of simple and boring enough to be recognizable by a powerful but yet structured computer program. And then automatically, you got an operating. Automatically, you got an algorithm running in polynomial time. So, yeah, this is really cool, and this framework has been applied in the last decade, I guess, to a number of problems and has really raised the bar for polynomial time algorithm estimation. So, yeah, so the goal of this course is going to be to give you a taste of what this technique is like. So, Sneet, how do you turn this claim into an actual You turn this claim into an actual estimator because it could be that the only subset which satisfies this is the one with the uncorrupted samples. So how do I turn this claim into an actualist? Right, so what you're going to do is you're going to run a sum of squares semi-definite program. And we'll be an optimization over some variables. And it will turn out that if you look at the And it will turn out that if you look at the value of some of those variables, that will give you some proxy for mu bar. Yeah. Okay, so none of this should be clear yet. And I guess there were a couple of options for me now. So one thing I could do is I could show you this proof of identifiability, but I'm not going to do that. Instead, I'm first going to introduce what it means to be a sum of squares proof. I'll tell you about the sum of squares proofs to algorithms framework. Sum of squares proofs to algorithms framework, and then I'll give you the proof of identifiability in the end using this, you know, and at the same time you can verify for yourself that it's a sign-force proof. Sorry, I guess the answer I was looking for is that you just need to find a set where the empirical covariance is bounded, and then the mean of that is good. Exactly. Exactly. Yeah, yeah, sorry. Okay, yeah, by the way, questions are really great, so please. Are really great, so please ask. I guess also, it's possible there's some, you know, like people are more or less familiar with these notions. All right, great. So now I will say what a sum of squares proof is. Okay, first we'll start with a sum of squares inequality. Inequality. So here's a definition. So if p of x is a polynomial with real coefficients. And real coefficients and variable in n dimensions. Then p of x greater than or equal to zero is a degree k sum of squares inequality. If we can write PX as a sum of squares of polynomials, where the degree of s is equal to for all s in the sum. Okay, so so So so it's a polynomial inequality that can be certified, can be proven by demonstrating that the polynomial is equal to a sum of squares. Okay, over the reals, squares are always non-negative. So writing p of x equals to a sum of squares of polynomials is a proof that it's non-negative. And we're quantifying the complexity of the proof. In this case, using Proof, in this case, using the degree of the polynomial. Okay, so a lower degree sum of squares proof is considered a simpler sum of squares proof. All right, so yeah, and if I can show that, yeah, so right, so you say that, yeah, and like, just like a trivial, I guess, observation is that, you know, if I have qx less than or equal to x. Q of x less than or equal to p of x, we say this is degree k S of s if the difference px minus qx can be written as a sum of squares. Okay, so does this because I guess this is just a simple idea. I guess this is just a simple definition. Hopefully, it makes sense. Is my font big enough? No? Do you want to move closer, Peter? I'll try to write bigger also, but... Okay, so when does a sum of squares proof of non-negativity Of non-negativity exists. So in the case when n is 1, so it's a univariate polynomial, if the polynomial is non-negative, a sum of squares proof always exists. It's not too difficult to show this, even though I'm not going to. But when n is larger, when the number of variables is larger, this is not necessarily the case. And sometimes there are not, so even when n is 2, you know, there's this Motzkin polynomial, which is non-negative. Polynomial which is non-negative always, but is not a sum of squares. And Hilbert, in his 17th problem, asked whether or not you can always take a non-negative polynomial and show that it is a sum of squares of rational functions. And this ended up being true. Okay, so that's a theorem, and it's one of the triumphs of algebra in the 18th or 19th century, I guess. So, okay. So, okay, but but uh but for us you know we're we're actually concerned with when polynomial uh uh proofs of non-negativity exist, not rational functions. And so I just guess also it's important to note that in the settings that we typically care about, there always is a proof of non-negativity, which is a sum of squares proof, but the degree may be very large. So for example, if If we're working over the constraints that xi squared equals xi, so Boolean variables, degree and SOS proof always exists. Okay, similarly, if your variable is bounded like over the sphere or something, then also some Something, then also some of the squared proofs exist, but the bound on the degree depends on more things. But so for our purposes, mostly some of the crisp proofs will exist, of a very large degree. And yes. So now I guess, okay, this definition is a little bit opaque, so I was going to give a couple of examples, which will be useful for us later, of inequalities, which are sum of squares inequalities. So a useful statement. Which are sum of squares inequalities. So, useful statements which have sum of squares proofs. Okay, so example one is Cauchy-Schwartz. Okay, so right, the Cauchy-Schwartz inequality states that x dot y at most Half x squared, plus half, y squared. Okay, and what I'm alternatively claiming is that the difference can be written as a sum of squares. And indeed, this is this is exactly. And indeed, this is exactly, I mean, it factors to a square. So I can completely square. Okay, so this equality is a sum of squares proof of the Cauchy-Schwarz inequality, this polynomial factorization. And if you want other forms of the Cauchy-Schwarz inequality, you can get it as well with the usual, you know, in the usual way that you prove the Cauchy-Schwarz inequality. Usual way that you prove the Cauchy-Schwartz inequality, you can prove sum of squares Cauchy-Schwartz inequalities. I won't show you because it gets a little bit dull, but trust me that you can do it. Okay, we're good? Here's another example. So if M is a real embedded matrix, embedded matrix and symmetric and m's maximum eigenvalue is at most lambda then x transpose mx is at most lambda times the norm of x squared. Okay, so this also has a sum of squares proof statement. So this one So this one is is a it's a little bit more normal than the the first, but it's also quite easy. So here's the proof. So let's take, let's notice that what we want to prove is that zero is bounded by lambda amount of x squared minus x transpose mx. Okay, this is the same as x. the same as X transpose lambda identity minus M. Just some simple manipulation. Okay, but now this matrix, because I have promised you that M's maximum eigenvalue is in most lambda, this matrix is positive semi-definite. So in particular, what I can do is it can be written as in its eigen decomposition as a sum of sigma i Let's call them yi transpose, okay, where all of the sigmas are non-negative. So then I can just re-express this whole thing using linearity as a sum of squares where the coefficients in the sum in the in the square. Where the coefficients in the sum and the in the squares are just the eigenvectors. So that's that's the proof. Okay, so this was a degree two, some square street, I guess. This is also a degree two, some square should be cool. All right, now I didn't, I mean, so you can see that even though this is kind of maybe at first a natural definition, many of the Definition: many of the arguments that you think of as basic arguments can be recast as sum of squares arguments. Is there some sort of canonical example of what can't be a sum of squares equality? I mean, of low degree. Yeah, low degree, let's say. I guess if you, yeah, I mean, I guess if you, like, a union-bound type of argument. Like a union-bound type of argument, typically you couldn't cast it as a low-degree sum of squares proof. Right? Somehow that requires some kind of analysis. Yeah, I guess, okay, but right. So arguments that are maybe a little bit more analytic often can't be cast as some of Squares arguments. But sometimes you can find a workaround anyway. Yeah, I mean, it's. Anyway, yeah, I think it's a great question. I mean, the cool thing is that every time that you discover a new kind of technique or like a new type of inequality that can be recast as sum of squares inequality, then it will lead to a bunch of new algorithms because you can capture a bunch of new proofs of identifiability as sum of squares proof. So this is sort of like the industry that I'm kind of part of. You know, you try to understand what can and can't be cast as a sum of squares proof. Can't be cast as a sum of squares proof. And then, once you find new arguments that can be, then you get new algorithms. Sure. For example, I guess like I was thinking, like, SQ, for example, can't do Gaussian elimination, so to speak. And that's the canonical thing I'm going to point to. It seems like there's nothing that you folks can't take down at. No, Gaussian elimination definitely is known to require a degree. So that also is hard to do. So, right, so if you have, I guess, Gaussian elimination over F2, right, so you're in this setting. F2, right? So you're in this setting. So there are sum of squares proofs of degree n, but also it's known that you need sum of squares proofs of at least degree n over 2, let's say, in order to find solutions to systems of linear equations in n variables. I see. So it's also kind of a hard example in some of the things. It's a hard example of it. Yeah, it's a good point. Okay. Alright, so okay, so this is this was my Okay, so this was my definition of sum of squares. And now I wanted to say how it might be applied to solve this robust mean estimation problem. How much time do I have, by the way? I can't see a lot so perfect. Okay. All right, so do we remember the robust mean estimation problem? So I'll erase this. Okay, so how could we possibly use some of squares proofs in algorithms? Okay, so the idea is the following. So I'll give you a black box primitive, and then I'll show you how to use it in parallel to capture this robust mean estimation problem. Robust mean estimation problem. Okay, so suppose that I have a polynomial system. I'll call it S. So it's just a system of polynomial equations. So let's just write down a concrete polynomial system, which we'll try to kind of Concrete polynomial system, which will try to capture the constraints on the set S, just so that we have something in mind. So, for example, to capture this set of constraints, what we can do is we can introduce a bunch of variables, w1 through wm. And these are supposed to take Boolean values. Take Boolean values. So we constrain the Wi squared equals Wi. And these variables are supposed to indicate membership inside the set S. Okay, so we'll require that the sum of Wi is 1 minus epsilon M. And we'll also require that Wi times Zi minus Vi equals zero for all. Okay, so here I introduced these variables Zi without saying it first, so let's do it. So Z1. First, so let's do it. So Z1 through Zm are going to take values in Rd. These are also variables that we introduce. Okay, so what's the idea of what Zi should be? So Zi should be equal to UI, ideally. That's always a feasible solution for this problem. And here this is saying that whenever a variable is not in the set, we require it to, or whenever Require it to, or whenever a variable isn't a set, it has to match the sample. All right, and then we can also define mu bar to be the sum of the zi variables. Okay, and we can finally add this constraint in. So we require that sum over i to the i minus u bar. Minus bar have bounded covariance. And in order to capture bounded covariance in a sum of squares way, we'll introduce a new variable. So b is going to be some matrix in a matrix value variable in R D A D. And we'll require that this matrix is equal to identity minus P V transpose. minus B V transpose. Okay, so B V transpose is positive semi-definite. Alright, so in this way I've encoded the constraints that this subset S has to satisfy in a polynomial system. Do you want the Wi to be non-negative? Yeah, so I have this constraint that Wi squared is equal to Wi, and that automatically gives me non-negativity because W squared is. W squared. No, no, no, it's really good, yeah. It's very non-optomy. Yeah, it's hard to, like, when you're up close with things, it's like you forget what is clear and what isn't clear. So please ask questions. This is like a very common thing in some sense in SOS things, right? Like you have the square wi squared equals wi. I see this like almost in everything, right? Yeah, yeah. Well, whenever you want, I guess like uh in in any setting where you want to solve for Boolean value variables, this is a constraint that encodes Booleanity naturally. Encodes Booleanity naturally over the reals. Yeah. Okay, so I guess our variables here are the W's, the Z's, and the matrix V. Okay? And what we want is we would like someone to hand us a solution to this system of polynomial equations. If we had a solution to the system of polynomial equations, then our mu hat variable would just be an estimator for the mean. The mean. And we would be happy. So, what we will get is we won't get solutions to systems of polynomial equations. I mean, in general, it's NP-hard to solve solutions of equations, of polynomial equations, so we won't be able to solve this, but instead we will have the following thing, which is kind of like a pseudo-solution to this polynomial system. Okay, so for any polynomial system S, there exists There exists at time n times n to the order k algorithm which produces a linear operator which will use this notation form. Which we'll use this notation for, and we'll call the pseudo-expectation operator. And what it will do is it will take monomials of degree ethnos k in the variables and send them to real numbers. And it will satisfy the following three properties. Okay, so the first is the scaling property. It's very boring. You just want that it maps one to one. Okay, the second quality that Quality that it will satisfy is that it will be non-negative on low-degree squares. So the pseudo-expectation of any polynomial squared is non-negative if the degree of s is at most q over 2. Right, and then bilinearity, this extends to sums of squares as well. Uh okay, and then the final property that it will satisfy will be specific to this system of equations here that we kind of fed to the black box. So can you see if I write down here? I guess you can kind of see. Okay, so we'll satisfy that the value of any of the polynomials in the system times any other polynomial is zero as long as the Is zero as long as the degree of Pi times C is at the most K. Okay, so this is something that would be true of an ex so okay. So what are we thinking of this as? We're thinking of this as sort of like a relaxation of the expectation of a distribution over solutions to the polynomial system. system. Right, so there could be many solutions to this polynomial system. Even if I could hand you the actual expectation operator for distribution over such polynomials, okay, there would be still some work to do. But I'm not even promising you that. I'm promising you something that's weaker than a moment oracle for such a distribution, but it does have to satisfy these constraints. So it has to be non-negative on squares, and it has to be zero. Squares and it has to be zero on your system. Sorry, what is C of X? C of X is any polynomial. Sorry, yeah, it's any polynomial. So you can multiply any polynomial by P as long as the degree isn't too large that you get zero. Right, this is something that is naturally satisfied by the polynomial, right? Once you have zero, you get zero. And this is And this is any for any polynomial inputs is that for any system s of inputs, I missed uh yeah yeah. So for any for any uh system s of inputs, there's a an algorithm running in time, you know, number of variables times number of constraints to the something like the degree, which can give you back such an operator. Okay, and I guess I wasn't planning to cover how you can get this algorithm. I mean, it's okay, it's it's a nice thing, but there's like a limited amount of time today. But the algorithm is based on semi-definite programming. But the algorithm is based on semi-definite programming. Also, oh, one more thing, I guess. So, here I've completely neglected issues of bit complexity of the solutions. So, you can have some situations where the solution bit complexity is really large, and then maybe your running time should also depend on the bit complexity of the solution. But I'm ignoring this because in most situations that you would naturally encounter, this doesn't happen. This doesn't happen. And there's, you know, there's some like really nice paper by Prasad Rigovendra and Ben Weitz that just knocks out a bunch of situations where you would never expect to see these big complexity issues. Since I'm being recorded, I have to do this disclaimer. Okay, so just to just to make things now concrete, like I Concrete, like I just to take one more step. So now if we can give a degree K SOS proof. Okay, so let's prove that mu bar minus mu squared is at most order epsilon. Then it must be the case that the pseudo expectation of mu bar minus mu Minus mu squared is at most epsilon. Okay, so this is exactly the proofs to algorithms thing, right? If we can give a sum of squares proof of this fact, then what we can do is we can just read off the value of this quantity, pseudo expectation of mu bar, right? This is an expression in variables from our system. Expression and variables from our system. We can just apply the operator that we got out. And we are guaranteed that this holds. I need to prove this. This is just the claim. The proof isn't hard, but okay. So here's the proof. So I know So I know, since I have a sum of squares proof of this fact of degree of most k, because my operator has to satisfy this constraint, I know that the pseudo-expectation of some term that's order epsilon minus mu bar minus mu squared has to be non-negative. Has to be non-negative. Okay, now I have, I use linearity and also the scaling property one to determine that this means that I can take some order of epsilon term and upper bound the pseudo expectation. Okay, so this is this is just, you know. This is just, you know, linearity, property one, to make this the student expectation do nothing here, and then the non-negativity of squares to say that this inequality is being affected. Okay, now there's one more step. So also, it's the case that the pseudo-expectation of mu bar minus mu squared is upper bounding This your expectation of mu bar minus mu squared. So this is also a sum of squares inequality. Okay, in the same way that the variance being non-negative is a sum of squares inequality, that's, I mean, right, like the way that you prove that the variance is non-negative is just by factoring it as a square, so that's automatically a sum of squares inequality. So you get this as well. Okay, and then that's it. Alright, so what we've done is we've taken our whole task and reduced it to proving that this inequality holds using a degree k sum of squares proof, where k is as small as possible. This will give us a time end to the k algorithm. It's artistic in how you derive the first inequality, put the pseudo-expectation there in the first place. The pseudo-expectation there in the first place? This one? Yeah, yeah, that property. Okay, okay. So suppose we can give a degree k sum of squares proof that this holds, right? Then what this means is that I can take this order of epsilon term minus this and write it as a degree k sum of squares. So really what we have is that this is equal to the pseudo expectation, which is a linearity of sum of squares. Sum of squares. And this has to be non-negative because of two. Got it. That makes sense. Yeah. Nice. Yeah. Okay. And can you explain also the last one? How would you get that? This one? Yeah, probably. Yeah, yeah, yeah. So, okay. So, right, so if I want to, so the pseudo-expectation of A minus pseudo expectation expectation a. A here is a stand-in for a single coordinate. Squared has to be non-negative. Right, this is a square. Okay, but then I just do the same thing that I, you know, so this is the same as the pseudo expectation of A squared minus two pseudo expectation of A squared plus pseudo expectation of A. Squared. Okay, and then I get a cancellation. What is degree k? Because we assume that this pseudo expectation is degree k. This is uh. Oh yeah, yeah, okay, okay, good, good point, good point. Okay, so here I'm treating the pseudo expectation of A. Here, I'm treating the pseudo expectation of A as a scalar, right? So this isn't a variable in my program anymore. This is just some number. And A itself would be degree at most K, I guess. Yeah. Oh yeah, that's a good point. Here maybe I should have said that, you know, assume that the degree of PI is at most K for all I. Maybe this is actually implied already. Okay, anyway, it doesn't. Already, but okay, anyway, it doesn't make sense to get a pseudo-expectation operator where you can't even evaluate it on your polynomials. Okay. So it's like this x to the less than the k units. Yeah, yeah. Polynomials in n variables of degree level. Exactly. Yeah. Cool. So recapping where we're at. I guess we've seen how to set up a polynomial system which is supposed to kind of mirror our proof of identifiability or to capture the constraints on our set. And then given that we have this black box algorithm that will return to us such a pseudo-expectation operator, we can see that if we can give a sum of squares proof of a statement such as this, Proof of a statement such as this, automatically we'll get an algorithm for estimating the quantity that we're interested in. So the thing that I still owe you is an XOS proof of this inequality, a low-degree XOS proof of this inequality. And I guess I'll spend the next 15 minutes doing that. Right, like, I think it's very unfortunate that, okay, so now I That, okay, so now what's going to happen is I'm going to prove this to you, and not to like sell this short or anything, but it's going to be a little bit boring, right? It's going to be like a boring sequence of inequalities, you know. It'll be mostly like Cauchy Schwartz and this thing. So like how exciting could it get? Well, I mean, in some sense, the fact that the proof is so mechanical is the reason why we're able to get an algorithm for it. So the proofs themselves are not. So the proofs themselves are never like, I mean, rarely in this situation is the proof itself very enlightening. It feels kind of mechanical and dull. I guess it's important to do it once because, you know, otherwise it feels like I'm just selling snake oil or something. But the next 15 minutes might be a little bit dry. So I'm sorry in advance. Can I ask a quick question? Yeah. Okay, I don't have much intuition for what that operator is. Yeah. Is it possible to describe what it looks like in the Is it possible to describe what it looks like in the robust main estimation case? Yeah, yeah, yeah, yeah, okay, okay, good point. Okay, so in the robust main estimation case, we have a bunch of program variables, right? So these are the program variables, and these are program variables, and I guess this is a program variable. Okay, and also mu is a program variable, but I mean it's a function of other program variables. And basically what this is, is the pseudo-expectation operator. I can take any Operator. I can take any function in the z's, the w's, you know, b and mu bar, and it equals some number, as long as this function, you know, for all f of degree with most k. With most k. Right, so like we're just running a computer program, and it's spitting out a function, you know, e tilde, and you can evaluate e tilde on any polynomial in your program variables. Right, so what is really happening? Like, under the hood, I'm going to solve a giant semi-definite program. It will have a variable for every monomial in these program variables of degree of most k. Programmed variables of degree minus k. Okay, and I could just read it like off of the solution. Yeah. Does that make sense? I think so. Yeah. Good. Yeah, yeah, I know, yeah. It's a lot of notation to absorb on the first, uh, on the first time. Okay. One thing I find weird is that you don't, like, in expectation use like square brackets around the quantity, but if you do this in some square, then you'd be a psycho or something. Is that what you're going to do? No, no, you'd definitely not be a psycho. I just don't have, I'm not like, in real time, I don't have enough dexterity to do the difference between the different brackets. Okay, I know you do that, and Sam also leaves out the square brackets, so I just want to make if that's like the norm or. No, no, no, no, no. Yeah, it's just sloppiness. Yeah. Okay, alright, so here, so here's the Okay, alright, so here's the promised proof of this. Okay. Okay, so here we've assumed that m is large enough so that if I have the uncorrupted samples, the ui's, remember the ui's were the uncorrupted samples, that effectively I have that the true Have that the true mean is really close to the empirical mean. Okay, so this is only approximately true, but let's take it as basically an equality because I don't want to have to carry our approximations around. So, okay, so let's take the quantity that we're interested in bounding. Okay, now I'm going to expand this, this term, as a sum over the samples. So this is like 1 over m, sum i from 1 to m z i minus u i. Alright, so all I've done is I've substituted in the definition of mu bar, which is the average of the Zi's. Okay, and I've taken mu and I've written it instead as the empirical mean of the UIs. So nothing exciting happened yet. Now is the first step where something's actually going to happen. So I can write this. As a sum over two types of terms. So the first term will be terms which Which in the way that we're thinking about these variables means that the sample is not inside S and Vi is not equal to UI. Okay, so it's uncorrupted and not in S. Sorry. Yes, I think that's right. Okay, and then Okay, and then uh we have to add back this uh confused by the the very stuff. I I thought you were writing a sum over the subset of right at the top now, right? Mu approximately one over I thought that sum was over the subset of uncorrupted variables. Over the top set of uncorrupted variables. Oh. Because the others can be uncharacter. Yeah, yeah, yeah. Good, good, good. Okay, let's just clarify this. So the U1 through UN were the uncorrupted. Oh, that's the Vs, I think. Yeah, and the V1 through VN or the observation. Yeah. U is for uncorrupted. Yeah. Okay. All right, great. Now, here, this term, actually, we have as a constraint in our system that In our system, that it has to be zero. Because we said that wi times zi minus vi is always zero. Right, so this is saying that if an element is inside your set, then zi has to equal vi. So this is zero as a constraint in our summer square system. So now all we have to do is bound the other term. And where is that bound going to come from? Well, so that bound should come from the fact that here we know that only an epsilon fraction of the samples are either corrupted or not included inside S. Right, and this term, we're going to use our bound on the covariance to bound. Okay, so that's just. Okay, so that's just letting you know where we're headed. But what we're gonna do is we're going to apply Cauchy Schwartz. Okay, so squaring everything so that we can apply Cauchy Schwartz so this is announced.  Cauchy Schwartz, and as you recall, Cauchy Schwartz is itself a sum of squares inequality of low degree. So this is a totally legitimate step. Excellent. Okay, now what we'll do is we'll bound each of these terms separately using a sum of squares proof. A product of two squares, or two sum of squares is itself a sum of squares, so this is fine. A sum of squares, so this is fine. This will still respect the rules that we've set out for ourselves. Okay, so first let's do this term. So, right. We have five minutes. I don't want to make things too tedious, but okay, but let's just do it. Okay, so 1 minus wi times. times this indicator squared. The claim is that this is just equal to 1 minus wi indicator ui equals vi, and that this itself is a proof that's captured by our system here. You can just mechanically do it out for yourself and figure that out. Okay, so I'm not gonna spend our time doing this, just trust me. Okay, so then this term is just Okay, so then this term is just 1 over m sum i equals 1 to m 1 minus w i betor ui vi. And then this itself is at most 2 epsilon. Why is that? Because 1 minus epsilon of the wi's have to be 1, right? I mean, we had that as a constraint for ourselves over here, right? And then maybe sometimes the And then maybe sometimes the indicator that ui equals vi is zero for some of those. So we'll get most epsilon fraction additional there. Okay, so this whole term is bounded by epsilon with a low degree sum of squares proof. Nothing that we did here really required anything more than just simplifying the degree we already have. I there's just one thing that I'm confused by. Why do we have the zi at? Or why why not if I knew bow to be one around the sum of w i di? Yeah, you can do that. This just makes the sum of this like math nicer. I mean, I think, yeah, that's definitely possible to do, and then some of these steps would be a little bit more ugly. Yeah, there's there's in general there are many ways that you can specify similar equivalent systems of equations, and it's not like obvious that they're equivalent, but you can do it. Obvious if they're equivalent, but you can do it that way too if you want to. Okay. It was an aesthetic choice. Okay, great. So now this term, what we're going to do is we're going to bound this term using the bound on the covariance matrix. Yeah, and then it's done. So this term is going, okay, and then it will be done. Should I just do it? Or we have like two, three minutes. It's okay. It's okay. Alright, if you really want to see it, I'll show you. Alright, if you really want to see it, I'll show you in the coffee break. This is kind of similar to the example two you did, right? Yeah, exactly. It will rely on the example two because it will use the fact that there's a matrix of bounded norm and then, you know, some variable, some vector like quadratic form with it. Yeah, exactly. Exercise as well. Okay, so that's it. That completes this proof. Completes this proof and automatically by giving a. Okay, so notice that I proved to you this, basically. So I proved this statement, but I did it in this kind of annoying way, using only low degrees, homosquares, inequalities. But given that I did this automatically, we get an algorithm running in polynomial time for this problem. Okay, so, and this is not the first polynomial time algorithm for this problem. Like, as I was saying, Gautam has a previous algorithm, but I think this analysis is more, it's actually simple. This analysis is more, it's actually simpler than that one. Once you have absorbed this sum of squares paradigm, it's actually very clean. And yeah, and so my plan for the remaining couple of lectures was to do one of a couple of things. Maybe I can get your input on it later. But so there are a number of problems for which applying this similar paradigm has dramatically increased the Increase the power of polynomial time algorithms that we know. So, like some examples are like clustering for mixtures of Gaussians, let's say, robust regression, you know, heavy-tailed mean estimation. Tensor decomposition. So I was planning to spend the tomorrow's lecture talking about clustering and robust regression at a high enough level of detail where you wouldn't have to see anything like detailed like this, but you would sort of understand how you would apply the framework there. Understand how you would apply the framework there. And I was going to do tensor decomposition on Wednesday. And then I was also going to talk about a little bit about lower bounds. So how our understanding of computational complexity of problems in statistics has evolved in the context of understanding some of squares programs better. But if there's some specific But if there's some specific, you know, some of Squares algorithms that you would like me specifically to cover that I didn't say I would, please just email me. Everything is casual enough that it can probably happen if you want to see it. So yeah, that's it. So here the K is just 4 or something, but it's just 4. Yeah, yeah, K is 4. I think if you want, maybe if you want this particular I think if you want, maybe if you want this particular form of the Cauchy-Schwarz inequality, then it increases the degree to six or something. And then so you get n times d to some. Yeah, yeah, yeah. Yeah, yeah, some like polynomials. Capital N here is the number of polynomial constraints. Okay, so in this example we have this constraint, we have these m constraints, we have this constraint, we have these m constraints. There's this, which is like d squared constraints, I guess. So it's like, yeah, but it's, you know, something like m times d squared or m plus d squared, yeah. And okay, and this is like a very naive, I mean, in some situations you can actually improve on this. So here I'm stating this like very generically, but often you can do better in specific settings. I have a stupid question. Why do you call so the only non-mechanical step was The only non-mechanical step was this proof of identifiability. I mean this claim any subset S which satisfies this. Yeah. Like once you have this everything else close. Why do you call it proof of identifiability? Like or where does it come from for you? Well I guess we're trying to okay so I guess proof of identifiability we're identifying the mean, right? No I mean I mean it's not okay like for example I needed that this was large enough For example, I needed that this was large enough because I need that the empirical estimate of the mean and the covariance will concentrate, right? It's not, I think it's okay to call it proof of identifiability. Yeah, I was just curious. Yeah. 