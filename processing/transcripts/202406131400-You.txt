Theoretical studies as well towards the end of the presentation. So let's get started. Okay, so I think at this point, we all acknowledge that we have really achieved great success with these artificial neural networks and with numerous success stories. And I think everyone has a long list of successful applications. But here, I just want to But here, I just want to get us reminded on how we were able to achieve this success. And then, well, there are many reasons, but arguably, one of the most important reasons is that we are now able to train models that are very large. And just to give a sense of how large the model we're training now, so if we talk about PPT-3, which is already a model from maybe two years ago, it contains already 100-something billion parameters. And if you talk about the modern, like And if you talk about the modern, like the latest larger models, then they are even larger. And immediately associated with the large number of parameters is that we are basically paying a very high cost for training such networks. And here I found a number online which claims that in order to train this GPS rate, it really took around 1000 megabyte hours. And I'm going to put that number into perspective. So that's a huge amount of energy. And efficiency. And efficiency is just one aspect. And then there's some other aspects that people usually talk about, like lack of robustness, reliability, and groundedness, and all those kind of challenges. And hence the question is, is that all that we can do with this model? Or is that issues or challenges with efficiency and robustness kind of the fundamental challenge with neural networks? And perhaps we know the answer is no, because. We know the answer is no because we know where these neural networks were coming from, right? We know that those things were really motivated from biological networks. And for biological networks, they are also very large. So here I'm listing the number like 100 billion neurons. There is no intention to equate the number of neurons with the parameters, but just to give you a sense that biological network can also be very large. But at the same time, the surprising thing is that biological... The surprising thing is that biological networks are much more efficient. So, there is some estimate to show that, like, the amount of energy that we consume through our lifetime is around 20 megabytes. And that contains both training and then a lot of inference. So it's really much smaller than the amount of energy that we're spending to train very large artificial networks. And finally, we all know that biological networks doesn't really have the robustness issue. Hopefully, you can agree. Hopefully, we can agree on that. Like, we are reliable and more robust compared to these artificial networks. And hence, the natural question for us is: why is that the case? Why is the biological network so much more effective than the artificial neural network? And I guess there could be multiple answers, but there is this one tentative answer that is also not new, that is basically arguing kind of the differences in the way that we. The differences in the way that we do the computation in artificial and biological networks. And on the left is kind of a basic unit for doing training or inference of these artificial networks, which is kind of the TPU units. And if you look at it, what it's doing, so the main module for doing the computation is this so-called pencil core. And if you look into it, like what is having there, the mostly is this matrix. There, the mostly is this matrix multiplication units, which are designed to carry out this large-scale matrix multiplication. So, even if you have sparse matrix, it's going to still do the dense matrix multiplication for the inputs. Whereas, if we consider on the other hand, the biological networks, there has been a lot of studies to show that neuron really fares, not always, but really like. But really, like very sparsely across time and also across, I guess, across neurons. And hence, the argument is that because the neurons are varying very sparsely, then it could be very efficient, like less computation cost. But the question is, is that really the right answer? And in fact, before asking whether that's the right answer, I think one question that people have perhaps not really looked into or lost. Parts not really looked into or not intensively is that is that really true that in artificial networks we are doing all these kind of dense computations. And that basically comes to the kind of the main message of this talk is that that is really not true. So what we did here is that we were looking into transformers, which is, I assume everyone here knows about. And if you look at the activations, then we found that it's really doing sparse computation with very high sparsely. Sparse computation with very high sparsity level, like very sparse. And so I made an animation to illustrate the idea, but let's first take a look at the left to set up the stage. So basically, on the left, we have the transformer where, you know, given the input, you first feed that into the self-attention layer, and then you feed that into the, and then you get the, you know, the embeddings illustrated in the blue rectangles. And then you feed those embeddings. And then you feed those embeddings into this so-called feed-forward network, which is basically just a multi-layer perception with two layers. And then these red things are the first layer weights, and then these yellow things are the second layer weight. And with an activation function, which we assume to be RALU in between. And hence, basically, the analogy to biological network here is that each pair of this yellow and the red box represents one neuron, right? One neuron, right? And then we can ask the question of: given any input, is that neuron activated or not? And here, what we were doing is to say, okay, let's take a pre-trained model. And in this case, it's a T5 model from several years ago. And then let's take an arbitrary sentence from our training repository, like which is here. And then what we do is that we feed the entire sentence one token at a time to the transformer, right? And what's going to happen is that for And what's going to happen is that for each of the input, let's say this her, you're going to get one embedding. And then for this embedding, you're going to get an activation vector to represent which of all those neurons are activated. And then our main message is that that activation is false. And then that is basically what is showing on the right here. So I'll just explain a little bit. So what's happening here is that each dot here in this plot corresponds to one. Plot corresponds to one neuron in like one layer of the transformer. And then blue means that that neuron is faring for this particular input that is being highlighted. So, and I can tell you that there's neurons all over the place, like in this kind of in the unit circle of this plot, but for any given input and any given time, only a small set of the neurons are activated. And just to give you a quantitative sense of And just to give you a quantitative sense of how many, so for T5 large, which is the model we're showing here, there is only around 1% of all those neurons that are being activated for each of the input. So this came to us as kind of a surprising observation because, you know, at the time I was given this product of kind of improving the efficiency of large language models. And then I was saying, okay, so since I'm familiar with sparsity, let's not, why not? Let's try to info some sparsity in the activity. Let's try to info some sparsity in the activations. Then I started to add a top K constraint on the activation to activate 50%. I found it's quality neutral. And then I started to reduce that number from 50 to 20 to 10%. And then I found that it doesn't really affect the quality of the model. And hence, so that's basically how I kind of discovered this phenomenon. And at the end, it turns out that for some of these models, it's actually at 1% non-zeros. Okay, so that's going to be. Okay, so that's going to be the main message or the kind of the entire presentation is going to about this phenomenon. So before I move forward, any questions? I'm just curious that the phenomena that this is true, I think, had been observed earlier for specific. For specific applications. And in particular, the architectures that have been used was in the first layer, I throw away some percentage and I keep throwing away more and more to only keep the top K attention coefficients from layer to layer. And so, what I'm curious about is this is something you did 1% in one layer, 1% in the entire network, or did you do it in a hierarchical In a hierarchical way across layers. And is there any difference? Yes, so I'm not aware of the work you were talking about, but it seems to be about attention rather than feed forward, right? And here in this talk, we're going to be talking about the feedforward network or the MOP layer, which is kind of less studied until recently. And so maybe then. So maybe then clarify what you're saying. So are you when you say 1%, you're keeping 1% of what precisely? 1% of the neurons here that are being activated. So among all these neurons, only 1% is activated. But you keep those whose attention coefficients are the highest, the top K? Or what is the, in other words, how do you select? Okay. So, Okay, so maybe I didn't explain this correctly. So, but this is kind of an observation without any intervention. So, this is to say, take any model that you have already trained, then look at the activation, like the sparse TLT activation in there. Then you're going to see that it's very sparse. Okay, so what I was talking was about network architectures that actually keep only the top K from layer to layer. I see, I see. And they're trained. I see, I see. And they're trained already with the top KAS. I see, yeah. Yeah, thanks. So I think that's kind of the surprising part of this, right? Don't do anything with, just take any model that you can download from the internet, like any transformer model, and then look at the activation. Then it's going to be sparse. And I'm going to show you my evidence of that. Question? Yes. Yes. Yes, I'm gonna talk about that in the next slide. Can you hear me, Chana? Yes. Hey, yeah, it's Ben Hapley. Yeah, I was just curious. So here you're showing transformers. How specific is this to transformers versus just MLPs in general? If you were to just take a large pre-trained MLP or something and do the same thing? Something and do the same thing. Do you have a sense for how sparse that would be? That's a good question. So, we do have some results in the appendix of the paper to show that in MLP as well, you're going to get a lot of sparsity. It seems to be only the, I think it's the conflict seems to be the only problem. So, confidence don't really have this strong level of sparsity. But MLP does, pure MLP does, and also MLP in transformers does. MLP in transformers does. Yeah, that had been observed before as well. In particular, for instance, Keremias has some papers on exploiting the sparsity of the activations of MLPs to analyze robustness because the calculation of the Lipschitz constant is much more tight when you know that it's far. Yes. Thanks. Yeah, and I'm going to talk about some empirical evidence to show that it's indeed the case for practice even. But it's indeed the case for practice, even like sparsity leads to robustness. Okay. So then one question that I actually often get is this question that is to say, okay, so since you're using ReLU as activation, then what's so surprising about this? You have ReLU and then you cut half of the interest to zero, right? Then you get sparse key. But the argument here is that, sure, with ReLU, you already get some sparse key, but. You already get some sparsity, but if you just have a randomly initialized network, then you are really at 50% sparsity level. Half are activated and half are not. But what we are talking about here is really this very extreme sparsity level with 1% or 3%, depends on the model, of the neurons. And what is being shown here is basically the training curve. So we did the training of the T5 model just to observe how sparsity evolves with the training. So the x-axis. evolves with the training. So the x-axis is the training steps and then y-axis is the percentage of non-zeros in log scale. So and then different colors or different curves just correspond to different layers in the network. So what we observe is that at the beginning of training, indeed everything is 50% because of the randomness of the initialization. And then as long as you start training, very quickly every layer almost becomes sparse. And stays to be roughly sparse throughout the training process. Yeah, there is. So that's going to be on the next slide. So thanks for the question. Okay, so is that clear? Like where, because I'm going to talk about this sparse D phenomenon's route, so is it clear where sparse D is merging? So, is it clear where sparsely is merging? It's basically very simple, right? It's just in the intermediate activation of the MLP layer. Okay, so let's get to the outline. So, the first thing that I'm going to do is to convince you that this phenomenon is prevalent, right? I'm not going to do some theory picking to show you one fancy case where it emerges and then in practice doesn't. So, I'm going to show you or flood you with a lot of the experiments that we did to show. Experiments that we did to show that there is really sparsity everywhere. And after that, I'm gonna go into the kind of engineering section to explain to you at the high level how we were able to obtain some efficiency gain with disparsity and also talk about what's going to be the challenge in that. Like it's not really like translating from flux reduction to wartime reduction is very challenging. And then there is the third part, which is And then there is the third part, which is to say, okay, so aside from the engineering part of efficiency, why should I care about sparsity? So I listed a few reasons why you should care about scarcity, which is to say sparsity would really help to improve all of these things. And I'm going to show you some empirical results about those. And we'll talk about theory at the end. Okay, so the first thing I want to do is. Okay, so the first thing I want to do is to convince you that this is a very prevalent phenomenon. And at the beginning, we were experimenting with this T5 model, which is NLP model. And the immediate thing that I did was to try that on a vision model and see if there's any difference. And hence, I did the experiment on VIT as well. And then in this figure, what's happening is that the, so this is about a trained model. And the x-axis is the index of the layer from the zeroth layers to the 11th layer. Zeros layers to the 11th layer. And then y-axis, again, is the percentage of non-zeros. And we're going to see a lot of pictures like that. And there's three curves: one for VIT and two for T5, because T5 has both encoder and decoder. But the conclusion here is that all of them are quite sparse, right? So again, y-axis is in log scale. So we can see that most of the layers are below 10%. And here we can also see some patterns like Sam's question. Python, like Sam's question, basically in the first few layers, it tends to be denser, and last few layers it tends to be denser. And in middle layers, is where most sparsity occurs. And then this also, I guess, answers somehow Rene's question, like sparsity is really emerging in all of the layers, not just some terrific layer. And then we can do a lot of other studies, like ask a lot of other questions, like, do you get? questions like do you get sparsely both on training data and eval data do you uh okay so let's first look at the figure on top right so basically we're what we're doing here is to evaluate the sparsity on training both training and eval data and we see that they align very well so both of them are very sparse and then the third question is what about data scale so here we train vit on two data sets one is the vanilla image net and then there is the larger version net and then there is the larger version of image net and then they also align very well so the training scale training data scale doesn't really affect it and then the last question is how about let's try some other configurations like vit base large field and then we did all of the experiments and then in all of the cases varsity emerges so it really occurs uh in all the cases that we tested and what's more And what's more, if we really believe in scaling law, or at least if we believe that large model is at least necessary, right, for realizing artificial intelligence, then let's look at how sparsity changes or sparsity level changes with the size of the model. So here on the left, what we did is a controlled experiment where we take the T5 and then fix everything, but only change or vary the depth of the model and see and then. And see, and then train all of those models and see how far the evolves. And then, so here, deeper color means that we are training deeper or larger models. So, the conclusion is that larger models are faster. And on the right, we can also do the experiment with the width, which here means the number of neurons in the MLP layer. And then we observe something similar. Like if you go to a very wide network, then it's going to be even faster. Network, then it's going to be even sparse. So, sparsity is really likely to be future-proof if you study this layer index. It just means that given a network, let's say eight-layer network, then it's going to go from the first layer, which is the input layer, up to the eighth layer, which is the output layer. Okay, so the final one is the first one. The final, not the final, but one question that I really get is: does the emergence of FARST means that you get a lot of dead neurons so that you can safely prune a lot of the neurons that are never activated? So, we look into that as well and basically to tell people that there is no dead neurons. All the neurons are activated for one input or another one. So, and on the right-hand side is the empirical evidence where Evidence where so basically what we are doing is to compute the activation for all of the 3,000-something neurons and then sort the neurons according to their percentage of activation on a given data set. And the conclusion here is that some of the neurons, like this part, are densely activated, not dense, but very often activated, maybe 50% for some of the neurons. 50% for some of the neurons. But by and large, even the least activated neuron here is activated around 0.2% of the time. So there is really no dead neurons, at least for this model. Okay, so then the second thing, which is basically coming back to my initial motivation of studying this phenomenon, is that we really want to see whether we can gain efficiency with this process. And I hope. And I hope it's clear at least in terms of blob count why sparsity can be helpful, right? But just to make sure we're on the same page, so this is just the MLP layer where what's happening is that you have the input of the MLP and then you're going to multiply the input with each of the column of the first layer MLP weight. And that gives you this very long high-dimensional embedding vector that is sparse, as I talked about. And then in the second layer, you are multiplied. And then in the second layer, you are multiplying this vector each entry with the corresponding column of the second layer weight, and then sum them up to get the out. So everybody knows about that. And then because as the side activation is sparse, right, this middle thing is not odd and only, let's say, maybe three of all the neurons are activated. Then what this means immediately is that we can drastically reduce the computation associated with the second layer. We don't really need to look at We don't really need to look at all the weights that are not associated with the activated neurons, right? We only look at very few, like three columns in there. So that's really give us a huge room for fluff reduction in the second layer of the MLP. And just to give a sense of what's the like kind of the headroom for the overall reduction in the transformer, so usually the amount of flops in MLP and in the tension are roughly Tension are roughly on the same scale. So, that really suggests that overall, we can still gain, in principle, a lot of the flux reductions. So, that's kind of the theoretical picture, or like in principle, the amount of efficiency we can get. And that's basically how we justify this effort. We show, okay, so you see, we can obtain 70% reduction in flops, but the practical picture is different. Different. And this is what happened in practice. So we were, at the time we published this first paper, we were doing this experiment where we take this T5 model again, and then we were doing a controlled experiment where we want to see how the sparsity level represented by the K in the X axis affect how much speed up you can get. So the Y axis is the relative speed up. Relative speed up. Hence, positive means better. And then the message here is really not that good, right? If you look at the T5 large model, then the speed up is really negative, showing that you don't really get speed up. Even if you have very sparse activation, even if you can get to 32 k equals to 32, you're not going to be able to gain any efficiency gain. And it's only when you go with a fairly large model, let's say 11 billion model, that you're able to gain some efficiency. To gain some efficiency, right? And here I can tell you KX256 is about the point where the natural sparsity, like the natural sparsity of this model is around at this K2256, which means that for this model, we can hope to gain around maybe 8% wall time reduction. So there's a huge difference between what you can really get and what in what And what instruct, what's like, okay, so there's a huge difference between what you can reduce in terms of wall time versus what you can reduce in terms of flux. It was not clear to me. Could you repeat how the knowledge that the activations are sparse is used with the goal of reducing computation? Yes, it's illustrated here, right? So basically, you can obtain huge flux reduction. Reduction because in the first layer, you don't really need to do the dense computation, right? You only need to care about those that are highlighted in right here. Do you have any sense why you're so far away from the theoretical reduction? Is this to have to do with like GPU architectures? Because here you're scale and accumulate, which maybe GPUs aren't well handled for exactly so that. Exactly. So that is the challenge we have been suffering from. Like the modern hardwares, like TPUs and GPUs that we typically use, they're really like kind of designed for doing dense matrix multiplication, like the MXEO that I was talking about. But in terms of doing sparse computation, they can do that, but they really do it in the most stupid way. Regardless of whether there's zeros, they still do the multiplication in any case. And hence, And hence, it's really hard to gain efficiency, like wartime reduction from this blocks reduction. And so that was the result in 2022. And since the problem was so important, so we got some more engineers to work on this problem. So then we were able to make some progress. Then we were able to make some progress. And then there is a new result that we just posted online this year. And so there's going to be a lot of the implementation details. But here, I guess I'm just going to talk about the high-level idea of what are the kind of the main challenges in really realizing a lot of the efficiency gains. So the first challenge here is exactly like what Ben was asking, right? I think Ben was saying. uh then was saying like the challenge is really how can you translate from flux reduction to wartime reduction because of the all the hardware support and then the second challenge is that if you recall the picture that i was showing previously so we can hope to gain flux reduction in the second mlp layer sure but what about the first layer so again there's going to be nothing fancy uh no too much mice here but i'll just uh talk about some quick ideas of how About some quick ideas of how, like from an engineering perspective, how we solve this problem. And for the first problem, what we were doing here is to resort to so-called structure sparsity, right? Broadly, structure sparsity. And the high-level idea is that structure sparsity is much better supported on hardware for whatever reason. And here is just to illustrate the idea where, so this is the activation pattern you would get, where x-axis is representing neurons. Is representing neurons. Sorry, the basically each column represents a neuron, and each row is representing one data point. So basically, different data points activate different neurons, right? And then it's going to be a very unstructured sparsity pattern. And what we did here is to say, okay, so let's first separate out a set of the neurons. And then we treat them as the kind of the common path neurons, which are neurons that are ideally always activated for whatever input you have. Activated for whatever input you have. And then for the rest of the neurons, we are going to group them into several small groups. And so this is basically the group sparsity concept that everybody here knows about. And the goal here is that we want to enforce the activation to have this python. And then once we get this pattern, we are going to be able to get some more efficiency gain, because this is more hardware-friendly. We're friendly. Okay. And then, so I know perhaps you are bored with all this engineering stuff, but just one slide about this. How about the second challenge, the first layer of the MLP? So here the idea is also very simple. All we need to do is to have a way to know the activation Python a priori, right? So here, if we know this first D Python, we can accelerate the second layer, but we cannot accelerate. The second layer, but we cannot accelerate the first layer because you have to first do the first layer computation in order to get that spurs deep pattern. And hence the idea here is, okay, let's find a cheap way of predicting which are the activated neurons. And there's multiple ways of doing that. And at the end, what we found to be most effective is this quantization method, where we just take this matrix and then obtain a quantized version for our intake quantization so that the computation. So, that the computation of the input with this context matrix is going to be much cheaper. And then, from there, you can readily obtain the sparsity support. And then, with that support, you can accelerate both the first and the second layer. Okay, so then some results. We were not, okay, so I guess we were not able to obtain very surprising numbers, but we are still making a lot. Surprising numbers, but we are still making a lot of progress. So, this is the result to show basically how our method can obtain kind of similar quality compared to the baseline, whereas being faster compared to the baseline. And we are faster by around like 16%, which is not huge, but I'd say still decent. And just to remind people, like, well, we are getting 16% speed up here, but that is really not the theoretical speed up, right? So I think the theoretical speed up with the FFN layer itself is around 2020%. Okay. So, next point. So, before going to the Next point. So, before going to the theory, I do want to talk about some other benefits of sparsity. And for this part, just to set the right expectation, so I'm not going to, I'm listing a lot of terms like generalization, robustness, calibration, explainability, and whatnot. I'm not going to talk about any theory about those, but just to give you some more empirical evidence that first E, like even in the context of practical transformers, first T do seem to have. First T do seem to have a close correlation with all of those concepts that are key to machine learning. Okay, so first of all, just to set the stage, I think we are all familiar with most of those concepts, but just to make sure that we are on the same page. So, first bullet point is about lipo noise. So, we all know that there is this famous work from already. already a long time ago, I'm showing that deep networks are really able to overfit to any training label that you give it, even if it is random. And then this really suggests that if you, in a more practical setup, if you have some label noise in your training data, then your model is going to overfit to those label noise, right? It's not going to be able to detect those. And hence, you're going to be overfitting to those label noise. So label noise is one highly studied problem. And then the second problem we're considering. And then the second problem we're considering is input corruption, right? So there was this data set called ImageNet Type-C, where they were saying, okay, so people at the time people were working on the adversarial robustness problem. But what these guys show is that even without considering adversarial corruptions, let's just consider some even kind of practical corruptions and see how the network works. And then they found that even for those kind of corruptions, the networks, the quality of networks degrades a lot. The quality of networks degrades a lot. And then there is finally the problem with the calibration, which basically means that your deep models tend to be overly confident, right? If it is predicting that there's a picture of bird with probability 0.8, then in practice, it's maybe only correct 60% of the time, not 80% of the time. So that's the issue with the calibration of the model. And I'm going to show you. And I'm going to show you that sparsity seems to be able to not solve, but at least alleviate all of those issues. Okay, so what we're going to do here is to verify this idea that faulty level is going to be associated with the level of robustness or level of calibration. And for that purpose, we need to do this controlled experiment where we so Where we, so what's shown here is just a regular feedback network with two layers. And in order to have a means to control the scarcity level, we insert a top case thresholding layer, which is nothing but to say that we are gonna compute, we're gonna sort this vector and then keep only the top k entries of those. And by varying k, the value of k, we can control the sparsity level and then observe how robustness and calibration varies with the sparsity level. The faulty level. Okay, so the first result is about label noise. So, what we did here is to take the ImageNet data set and then take the VIT architecture. And then we corrupt certain amounts, like in this case, 80% of the labels in the training set of the ImageNet, and then train a model, train a EIT on this model, right? And then there's four curves for time. And then there's four curves. So for time being, let's focus on the baseline, which is the blue. And then this solid blue curve is the evaluation curve with the baseline. And what you can see here is that the evaluation accuracy continues to improve at the beginning, which is good. But towards the end of the training, the accuracy begins to reduce, right? Which is showing a sign of overfeeding to the label noise. If you train with clean labels, Train with clean labels, there's not going to be this dip here at the end. And here, the surprising information is that if we add a top 128 to the MLP layer to the activation vectors, then what we're going to end up with is this green solid curve where we saw that the quality on the evolve side continues to improve all the way until the end. And then there is no overfitting in this case. Okay, and then here. Okay, and then here I didn't show, but of course you can vary the sparsity level and vary the percentage of noise that you add to the data. But we saw this as a consistent pattern. Like adding some sort of sparsity helps you to improve the robustness to the label noise. And the second point is about input perturbation. So here again, the goal is to say, let's just train a VIT. just train a VIT on ImageNet as you basically take a pre-trained VIT and then instead of evaluating the VIT on clean images, let's evaluate on these kind of images that are corrupted by some Gaussian noise. And in case you are familiar with this data set, this is ImageNet Type-C data set. For this Gaussian noise type, it comes with five degrees of corruption. Basically, five means that it's heavily corrupt. Basically, five means that it's heavily corrupted by the noise. And so, different bars here correspond to baseline and VIT with varying levels of sparsity. And the message here is that the green bar, which corresponds to top 64, always beats the baseline by a fairly large margin, regardless of the severity level you have. Like if it is not very severe, then the gain is small, but if it is very If it is very, if you add really a lot of noise, then you really see a lot of the quality gain. What is the x-axis? Is this an adversarial perturbation or is it just Gaussian noise? This is just Gaussian noise. This is basically the x-axis. This is basically the variance of the Gaussian noise. What about L1 or L infinity attack? That's a good question, that we didn't evaluate. That we didn't evaluate. So, we were mostly focusing on these three types of corruptions: Gaussian noise, short noise, impulse noise. And for all of the cases, we observed that top 64 seems to be a very good number. And for calibration, when you say five, that's five out of 255 for the amount of noise. I think there is no, okay, so this number. Uh there is no okay, so these numbers doesn't represent uh the variance of the uh noise, but it's just uh signaling the the level of the the noise. I think they pick some numbers. I'm not sure what those numbers are. Yeah, but that's exactly the question, is to know how much noise is relative to the image intensities. Yeah, that's a good question. So yeah, I don't know the amount of noise they were adding there. Need to check their code. Okay. Okay, so the next question is about calibration, right? Basically, calibration is a matter of whether the model is going to be, or like what I was writing here, to what degree the model's output confidence agrees with its prediction accuracy, right? And there is a standard metric for evaluating the calibration of the model called the expected calibration error. And basically, the smaller number means that the calibration is better. Calibration inspector. And on the right, I was throwing again some empirical result on ImageNet with VIT. And what's happening is, okay, so x-axis is the training steps and then y-axis the calibration error. And here again, what we see is if you have a smaller k, like k equals 32, then your model is going to be much more calibrated than the baseline. Okay. So, and one last thing is this work that is actually not our work, but a paper by Anthropic, but I think it's quite relevant. So I'm posting it here. So, what they were trying to do here is that they were discovering some phenomenon, like if you take the MLP of a train transformer, then they Of a train transformer. Then they found that certain neurons have certain degree of interpretability. So there is some association with the neuron, with the certain concepts in the training data. But the issue is that not all of the neurons are interpretable. Only a small percentage of the neurons are interpretable. And then this blue curve is basically the baseline, which is the amount of neurons that are interpretable in a vanilla transformer. And then what I'm gonna talk about. And then what I'm gonna talk about here is their follow-up work in 2022, where they say, okay, how about let's introduce some sparsity and see if that improves interpretability of the neurons. And then it turns out to be the case. And the way they introduce sparsity is to say, let's go with the softmax as the activation instead of the real SD activation. And because softmax is more peaky, so it's going to generate some more sparse activation vectors. And then the refining is shown in this. And then the refining is shown in this red curve, which is to show: okay, if you add some sparse key, then you improve the percentage of neurons that are going to be interpretable. It's not clear to me how interpretability is measured. Yeah, I guess they have some. Yeah, okay, that's a good question. So, I'm not exactly sure about the exact way that they measure the interpretability, but I guess it's. measure the interpretability, but I guess it's somehow empirical to see whether each neuron is corresponding to one concept. Okay, so I hope that's sufficient evidence. So that's the end of this part. So I think that's sufficient evidence to show you that if you hopefully if you care about any of those problems, generalization, robustness, or whatever, then this phenomenon of sparsity activation of transformers could be of interest. Of transformers could be of interest. And in the last part, I want to talk about the theory, which is perhaps the most interesting part to this menu. And okay, so let's get started. So the first question that we came up with, like when we found this phenomenon is what theory should we do? Like, usually the way that people do theory in deep learning is to say, let's come up with some, you know, Uh, you know, model network like over-progress model or deep linear network or whatever. And then let's consider some assumptions on the training data or something separable that we have seen or low-dimensional or like any kind of structure of the data. And then you can prove something about the solution and the algorithm. But the question we are having is really how should we pick the architecture and how should we pick the a thumb. The assumptions on the training data. And the way we look at it is that it should really depend on whether we have a good intuition of what's really contributing to the emergence of sparsity. And hence, we went to this route of studying this problem. So we came up with some hypotheses to explain why sparsity emerged. And then before we come up with a model for directional analysis. So the first assumption that we came up with is That we came up with is that sparse T must have been immersed from some properties with natural data, right? I think this is perhaps to us is not a fancy argument because we all know that there are some intrinsic structures, like a low-dimensional structure or whatnot, at least some sort of compact structures in the data. And hence, perhaps what this transformer does is to magically find some of these low-dimensional structures or compact representations. dimensional structures or compact representations so that they can represent them using very sparse vectors. And it turns out that's not true. And what we did here is to design an experiment where we say, okay, so if it is truly with the structure of the images, why not let's just train the model on some purely unstructured images, right? So here what we did is to design it. So here what we did is to design a data set where the images are purely random noises and then train a VIT on such data and then see if the sparse emerges. And then the result is thrown on the right hand side where the blue is the curve of the vanilla VIT which we have seen before. And then this curve with the crosses are the specific level with this model trained on random images. And what we observe here is that it's still. Reserve here is that it's still sparse and still quite sparse, which is suggesting that perhaps we shouldn't be really assuming anything from the data, right? We don't really need any kind of separability or multimodal structure to observe this for speed. And then we were kind of thinking, okay, so if this number of data, then how come that supposed to emerge? And then the second hypothesis we have is Hypothesis we have is to say, okay, perhaps it's simply because the network is so over-parametric, and hence, for each input, you don't really need all the parameters to fit that input, right? Hence, ultimately, the reason is about over-parametrization. And turns out that that's not true either. And the way we did the experiment is to say, okay, let's design this training data set that contains infinite amount of training data in the way that whenever we draw a batch of data, we just we draw a batch of data, we just sample a new batch of random images as the input to the model. And hence for whatever architecture you choose, it's just going to be the model is going to be under-parametrized. And in this case, the result is shown again on the right. And what we observe here is that perhaps for some layers, it becomes slightly denser, maybe 20% for these three layers. But by and large, for most of the layers, it's still very sparse. Of the layers, it's still very sparse. So it still appears that it has nothing to do with overparameterization. Is that the question? No. But we were training the model like using the same number of iterations as the like basically we want to the comparison to be fair. So we use the same number of epochs, training epochs for training both models. Training epochs or training both models, like the baseline and our model. No. Okay. So this is showing perhaps, okay, we shouldn't be assuming anything about the overparametrization of the model either, right? We explain this firstly. And hence, if it is not about the data, not about the over-parametrization, then what could it be? And then we have a called. Colleague, this post-author here, who is a physicist, and then he was looking at this, and then he's saying, Okay, so it's obviously from like this intuition, like if you reduce the activation of the neurons, then you're going to be reducing the training loss. Does that make sense? So, I mean, at the time when he was saying that, that doesn't make sense to me, at least I didn't see why this could be the case. Why this could be the case? Yeah, question? It means that if you reduce the magnitude of the activated neuron, then it's going to decrease the loss. Okay, so he was saying that it's so obvious, and then I was saying it's not obvious. So then he came up with a proof to show that that's indeed the case. So then I got convinced. Convinced, and here is how we did the theory. So, the model that we are analyzing is something like this. So, the focus here is that we're going to want to compute the gradient of the loss with respect to the activations. And for that purpose, we are ignoring all the previous layers, the previous attention and the feed forward networks. And then we focus on the last feed-forward network where the input is. Where the input is the embedding, and then you product into a very high-dimensional embedding. And then we take that as the subject of study, that activation vector, pre-activation vector. And then that pre-activation vector is fed through RELU and then the second layer weight to produce your output. And then you have the last compute difference between the output and the label Y. And then what we want to study here, sorry for that, is to compute the is to compute the gradient of the loss with respect to this reactivation vector. And this is our result here. Okay, so consider a network that is shown on the right, right? And the statement is as follows. So take any data point, X, Y, Trinian data point, and consider any entry in the pre-activation that is positive. And hence, that represents an activated neural. Then the main result is to say if you compute the gradient of the loss with respect to that particular entry, and then you take the expectation with respect to the randomness of the V initialization, then that thing is positive. And what this result means is that if you decrease the value of that activated neuron, then it's going to decrease. And then it's going to decrease the loss. So that's the definition of gridding. Okay. And the key assumption here is that the V is random. So we have to, so in some sense, this only applies to perhaps the initialization of the initialization state of the network training. And this actually quite aligns with. Sorry, that was not clear. Okay. So the expectation is taken. So the expectation is taken, I think you said, with respect to the randomness of the initialization. Right. Of the, yes. And so why then training is going to decrease the loss? This simply says that the yeah, I mean, in reality, you begin at a fixed set of weights. Yes. And you follow gradient descent. And all operate in the sense with respect to the weights. Yes. And this expectation is taken with respect to the weights. So I think there is more steps to solve the statement. Definitely. So this is not a perfect theory. So this is like taking derivative not with respect to the weights, but really with respect to the activation, the one entry of the activated neuron. Hence, it doesn't directly tell you how the weight is going to evolve. How the weight is gonna evolve. But our hope is that at least this provides some justification for why the model weight update tends to decrease the value of those activations. Because if the weight is updated so that they decrease the activation value, then it's going to decrease the training loss. Okay, and then Okay, and then just to remind you of this figure, which is to show that this theorem, the statement, seems to be quite aligned with what we observe. At the beginning of the training, you get a lot of the gradient pointing towards surprising those activations, so you quickly get a lot of sparsity. And then thereafter, you break this assumption that V is random, and hence you no longer have that momentum, and hence it's no longer becoming faster. No longer becoming fast-up. So it stays the same. Okay. That we don't. That we don't. So this, yeah, this result is purely about the initialization, at the initialization point. Yeah. Yeah, the loss should be decreasing throughout the training. Though, you know, at the initial phase, it decreases very rapidly, and towards the end, it's flattens out. So I'm finding it hard to believe the statement, but I will say there is truth behind that. If you did not have a red. not have a rem then the map f would simply be v times the activation and so the derivative with respect to the activation would just be v and in that case there is no expectation to be taken and so whether this is possible or not would just depend on what d uh so yes my statement is not correct in general because there is a value and that's impacting Really well that's the only thing that's a little bit there is also the loss I guess things gonna be clearer if I present the proof which I do have a sketch. I mean the proof is not too fancy but still I find it to be non-trivial so I didn't see how my colleague was able to I didn't see how my colleague was able to see this intuition as obvious. There's gonna be a lot of notations, but I'll just say at a very high level to explain the idea. And just to set the stage, so what we want to do here is to compute this thing, right? The expectation of the loss with respect to the activated neuron. And the formulation of the network is like this. And then the first step is exactly like what Renee said: the chain rule, right? You first take the activation. You first take the activation with respect to gradient with respect to basically take the gradient of the loss and then take the gradient with respect to the activation. The second part is easier because you really just get VI star where this Vi star is just the I column of the V matrix. And the first part is more complicated because of the soft max, but still you can do that. So ultimately, it's going to be something like this. And then this thing is what you want to take expectation over. Want to take expectation over? Okay, so there's two challenges associated with taking expectations with this thing. The first thing is that, okay, if there is no exponential, then at least the numerator is easy to deal with, right? All it involves is the covariance between this Vi and Vi star. But because this Vi is really hiding inside exponential, that really makes life much harder. So that's the first challenge. The second challenge is that Vi not only occurs That VI not only appears on the numerator, but also in the denominator. And when you take the expectation, you cannot really take the expectation separately for the two parts, right? You need to deal with those cases. And hence, what's the idea? The idea is that we want to, we're going to do a decoupling of the entries. So here is just a rewriting of the numerator. And there is nothing. So this is the numerator. And the first step is nothing fancy. We just write it. Fancy, we just write it separately, like entry-wise. So, this following basically the definition of inner product. And the key point here is that once we write this as this, we want to extract all the terms related to VI star comma m together. So among all the exponentials there, we extract the term that is containing this VI star m to be together with this term. And then hence this term. And then, hence, this term does not contain this vi-star, m anymore. Okay, you have to do that. And then, if you plug that in, then you get this, you get this thing where this C, we are just introducing this notion, notation to represent this thing that is unrelated to VI star, m. So that's how you deal with the numerator. And then you still need to deal with the denominator. And here, the tricky point here is that the The tricky point here is that you can also decompose this into the summation of something in a similar fashion, but here we must pay attention to the fact that this thing is inside of the summation over M. And hence, when we are decomposing this term, we have to decompose this term with respect to m. So there is some details that I'm not going to talk about into detail. But by and large, the information is still that we are decomposing this. is still that we are decomposing this thing into this long summation where everything related to vi star comma m is here and everything else does not really depend on this v i star comma m and then plugging this in you get this formulation at the bottom where we just introduce more notations like this c2 represent this term and c3 represent the last term here Here. Okay, so now we obtain this thing. And here we are ready to take the expectation. So we can just look at this, right? So now if we want to take the expectation of this thing, then the first thing is easy. You just swap expectation with the summation so you can put it inside. And then the difficulty with like in like dealing with the term in the parentheses, which contains a lot of terms. But the idea is that we're going to decompose the That we're going to decompose the expectation into two parts. The first part is that we first take the expectation only with respect to this EI star, m and then we take the expectation with respect to everything else. And that's what happened here. So basically the truth is in this line. This is just a copying of the previous equation. So here I decide, so this thing in the parenthesis, I first take the expectation over v i star m and then take I star m and then takes expectation over everything else. And the important observation here is that if you look at the innermost expectation, then this thing is positive. That's not trivial, but we can kind of get it because everything here is exponential, the C's are all positive, and hence you can prove that this thing is positive. And then because you are taking an expectation with like of a positive thing, then you get something positive. And then this term is zero. And hence, this finishes the proof. And hence this finishes the proof. Like the expectation is positive. Question? Yeah, that's a good question. Yeah, perhaps even more difficult. How do you see that that's positive? So you get the exponentials positive, but isn't this just like the sine of Vi star? Sine of Vi star. Which one? VI star, which one? The uh okay, so the technical assumption here we want to make is that this thing shouldn't be always zero, right? So that we can make the assumption. And as long as this VI star, m is not always zero. Then, I mean, it's perhaps not trivial. So we do have a proof in the paper to show that rigorously this thing is positive. But just the intuition is that everything, I guess it's not really. I guess it's not really. But isn't V just a random vector? Yeah, I take it back. So maybe it's not so obvious, but we do have a Lima to deal with this quantity in here and to prove that it's positive. Okay, since I'm over time and luckily, I only have one slide left. So I think the title of the talk was How many Flops? The talk was how many flops is the token worth? So, here I want to just briefly answer that question. So, it's easy to compute the nominal number of flops per token for generating each token in a transformer. And this is the formulation that everybody, if we sit down, then we can write this down. And I hope the notations here are self-explanatory, like number of layers, the context length, dimension of the model, and the vocabulary size. But the key message I want to say here is that that is only the nominal number of flops. And in practice, the number of flops could be much lower. And the focus of this talk has been on this feed forward network. We were showing that you can reduce this to perhaps 1%, this flops number. But the argument I want to make that I didn't get to talk to and I actually didn't get to work on yet is that for these two parts, naturally, like, or in principle, they should. like or in principle they should also have a very strong level sorry a very strong level of sparsity because the first part is nothing but just the attention where you have the soft max so it's very geek you're not going to activate all the key value pairs in history so that part is going to be false and the last part is the decoding layer where you basically do the vocabulary lookup and hence you know in producing each token you don't really need to look up the entire vocabulary right so basically you only add Right, so basically, you only activate a few of the relevant tokens in the vocabulary. So, all of those three components should be able to make very small. Hence, I think it's really a question to be studied, perhaps more on the empirical side, like what is really the real number of blobs that is needed inside a transformer. And once we have an answer to that, maybe we can be better at bridging the gap between these kind of artificial networks and the biological networks. And the pathological networks that are more efficient. So that's all I have. Thanks. Any questions for John? So I was wondering, you mentioned something about generization and you have experimental evidence, but do you have a Experimental evidence, but do you have a theoretical understanding of how generization happens in this case? I mean, perhaps the obvious answer is if you focus parts, we have to understand the question. So, yeah, you do have some evidence of empirical linearization results, but I was wondering on the theory side, I mean, perhaps the obvious answer is that if you I mean, perhaps the obvious answer is that if if your models end up being sparse, you can maybe use a standard optimization theory in that case, but I I'm not sure if that's good enough. So what's what's your take on that? So that we are not sure. So I hope I said the right expectation that this work is mostly about empirical study to show that sparsity can really help in practice. And in terms of theory, I guess there's already some work on the robustness side, like René has mentioned, but I guess for general But I guess for generalization, perhaps there's something on the theory side that could be done to rigorously establish how sparsely level correlates with generalization. I guess related to that, should I just think of like the top K sparsity as some kind of regularization? Seems like it's doing a lot of top K. Of top K as some sort of loggerization, right? And it's kind of the correct inductive bias for the model so that you can really reduce the overfeeding, right? Yeah. I was curious, how much does the actual, you know, so I guess you had some experiments on the structure of the data, but if we were to do something simple, Data, but if we were to do something simple, you know, I guess, like, does the direction of the data really matter? Like, or the direction of the embedding coming into the layer matter? Like, if you were to just say that, you know, say that you just look at like what the average norm is of the input to a layer. And then if you were to just take, you know, the average value of the weights, like essentially, if you were to just match the magnitudes and then see what's the probability of being over the bias of the neurons, right? Of the neurons, right? If you were to just pick random directions for the weights, would you get roughly the same sparsity level? Does that question make sense? Are you referring to this theorem? No, not this theorem. It's essentially just like how much is the direction of the weights actually matching sort of the input statistics to the layer versus are the bias levels just sort of set to a point where you know if you were to just take random. Where you know, if you were to just take random dot products between vectors with similar magnitudes as the weights and the inputs to the neurons, would they be above the bias at whatever 1% or whatever the sparsity level is? Does that make sense? So you're talking about if we take a already trained transformer, like because if at the random at the random initialization, things the neurons are random, right, in the first layer. Layer. And hence, after the training, the neurons have to be somewhat aligned to some specific directions so that for most of the input, it can be sparse. Well, but it's not, yeah, really what I'm saying is like basically take trained biases, right? Because like when you initialize, right, the bias is random too. But essentially, if you just take whatever the bias is after training, or whatever the magnitude is of your weights and the inputs, right? Like if you were to sample random vectors. Like, if you were to sample random vectors with similar magnitudes, essentially, but keep the bias the same, you know, you go over that bias at roughly the same level. That's a good question. So, one thing that I didn't mention is that, so I guess you were talking about the bias in the maybe the first layer weight of the feed forward, right? So, one thing I didn't mention is that that bias doesn't matter. Even if you don't have the bias, you still get a lot of sparsity. Okay. And in fact, And in fact, one of the, I forgot which one it is, like VIP and T5, one of them does have the bias and one of them doesn't, but both have strong sparsity. Thank you for the very interesting talk. So, you mentioned earlier that you can use your optimizer. This might be a stupid question, but is it the case that there are architectures that are optimized for sparse matrix multiplications? Or CPUs better than that? Better than this architectures? Yeah, so this is a hardware question that I don't really have the expertise to answer. But I guess there is some benefit of dense computation in the sense that it's more efficient. It's more efficient in the sense that, you know, in terms of doing a lot of floating point operations. So it's much more efficient. If you are to do sparse computation, even though overall your required number of blobs is lower, but still the computation is going to be slower because it's just harder to do sparse computation. Yeah. That's an excellent question. So we did want to study the effect of the optimizer, but for transformers, unfortunately, it only works with atom. But I think we included some results on continents, like because for continues, Like, because for conf nets, you can train with I3D or IDEM. And I thought the conclusion, I need to double-check, but I thought the conclusion is that the optimizer doesn't really matter. So it may affect a little bit of the scorpity level, but regardless of the optimizer you use, as long as it's screwed in based, I guess it's going to be having scarcity because of this erratic result. All right, I guess we are 15 minutes late, so let's take a break. Thank you very much. Thanks, Renee. I was just going to ask for all the experiments you did, how many megawatts did you spend?