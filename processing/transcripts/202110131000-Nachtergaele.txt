Thank you, Shen and Martin, for organizing this workshop. It's been really great so far. I've enjoyed it. All right, so I'm going to talk a little bit about spectral gaps and stability and discuss a family of models and examples where we have some recent results. Recent results. So there are basically two results I'm going to mention. And the first is with Bob Simpson and Amanda Young. And then the other one is with Daniel Ilchi and Jakob Bjurenberg and Peter MÃ¼hlberg, who is a student of Daniel Ilchi and contributed a lot also. As usual, the junior collaborators do a lot of work. Now, Now, I hear some feedback noise. I don't know where it comes from, where it's generated from. You guys bother with it? No. So I had to select a few things to talk about. And I know this is a specialized audience, mostly, but there's also a couple of students. And I thought I should at least make it possible for them to. Possible for them to fit what they hear in the context of what they know. So I will talk about that a little bit. So that's this first topic, locality, visa locality, and talk about ground states and the GNS Hamiltonian, which Yoshiko, for instance, talked about all the time. And so I think I should probably say that that's the framework I'm working in. Then, if this is about stability, I should say what stability is. And of course, it's a question with many answers, but I will give you the answers that pertain to the two new results that I want to present. Okay, so I'm going to go very quickly over this because I know that almost everybody is very familiar with it. We work on some letters, maybe. work on some on some letters maybe it's z new maybe it's some delone set um embedded in r new or something like that and we have our quantum spin system i could do exactly sort of the same discussion on fermion systems and there would be really very few changes and there would be some interesting things happening but the basic structure is the same and and we have these local observables that involve These local observables that involve only a finite number of spins. And then, both for physical and mathematical reasons, we want to take limits and obtain the nice seaster algebra of quasi-local observables, which is obtained by completing the local observables. And then we define the physical system we talk about ideally by Heisenberg dynamics. By Heisenberg dynamics, or the generator of that dynamics, which in this case is given by an interaction, which in principle gives you all the terms in the Hamiltonian. They are self-adjoint local observables. And formally, then the generator of the Heisenberg dynamics is, of course, the commutator of the Hamiltonian. So I sum all these terms, and of course, These terms, and but of course, that's a divergent sum. But if I sum commutators with local observables a, then, um, for instance, for a finite range of direction, I have, of course, only a finite number of non-zero terms. And so this would be a finite sum. And generally, I would have to assume some decay of this interaction. And I will talk about that a little bit as we go. So if everything is fine, then there will be Eisenberg. Everything is fine, then there will be a Heisenberg dynamics that is a solution to the Heisenberg equation. And note, I put the I here explicitly in the exponential, so the generator is just a commutator with the Hamiltonian. And if you compare with Yoshiko, there will always be I's there, which then I don't have to write. Okay, so as I mentioned, this certainly exists on the local observables if. The local observables have a fast decaying interaction, and maybe some not so local ones. But generally, certainly the domain of this derivation, this unbounded operator, will not be all of the quasi-local observables. And so it's useful often to quantify that a little bit. And that is where Lieb Ramsenbaum's come in, sort of in the technical part of proofs. The technical part of proofs, but I won't actually talk about that. So let me just quickly show, maybe for some of the students who haven't really seen this, how you measure sort of locality. So the algebra itself is obtained by completion. So any sequence that converges has a limit in it, if I start with local observables. So any A in this algebra is the limit of the sequence. A sequence of a's that are supported in a finite volume lambda n. And now I can also go the other way. If I have such an A, I can actually construct local approximations by taking the partial trace and the complement of some finite volume lambda. So this is what these conditional expectations or projections pi lambda mean. I mean. So since these are finite-dimensional spins, this is well defined. I can take just a normalized trace on the factors outside some finite volume lambda. And for instance, for simplicity, I can say maybe I'm going to construct a sequence that is supported on balls of some center x and radius n and let n go to infinity and construct an approximating sequence this way. And so this always works. And But the speed of convergence is kind of arbitrary. You're guaranteed it converges, but you're not guaranteed any particular rate. So there is some, this is one way to sort of quantify what the rate is. You say there's some function, but upper bounds the difference between these local approximations and the limit. And I can also turn this around. I can sort of fix a sequence, maybe balls, and fix an F some positive function. Some positive function that decreases to zero and tends to infinity, and then look at the quasi-local observables that are approximated at that rate or better. Okay, so another way to measure this in many cases, if you want to know a certain A, is it in AF for a certain F, is to look whether you have something like this. You can say something about. Have something like this. You can say something about the largest commutator you can get of the observable A, which is in A lambda, with a B of norm one that is supported in the complement. And that gives you exactly up to a factor of two upper and lower bounds. You can reverse this if you want. If you know the Librarian sometimes, you can have the same inequality between that and the decision of the approximations. And so, in many cases, it's really not a restriction to assume that the interaction terms you have are supported on balls. Sometimes this can change the decay, but that's sort of the really technical condition. So, in what we do setups, we usually have that's sufficient. And so, I can assume that I only have interaction terms supported on labeled by balls, BXN, and there's some constant and a function. There's some constant and a function f that goes to zero that says how this interaction decays. There's other ways of expressing decay of interaction which are sometimes useful and you can go back and forth between them with some elementary estimates. So what we were saying then that if you have an A that is local in a certain way, specified by a function g, maybe it's exponential, maybe it's a power law, and then I have a delta. And then I have a delta that's generated by an interaction at a certain decay. I'll be able to say this certain decay for the result, the action of the derivation of this observable. And we already know that if you have finite Reese interactions, you can take characteristic functions and everything stays local, but generically, this is not the case. And a family of functions that we use a lot are stretched exponentials. Exponentials. Exponentials also, power laws, also, but stress exponentials are very convenient to sort of naturally arise, and if you can avoid power laws, something somewhere general. So, this is a class of functions that we will use a lot. Okay, so if you have such decay, then for instance, it's often convenient to sum all the interactions that. All the interactions that sort of are centered at some point x and call that hx. And then we have our derivation. And you see, people work with that kind of structure often. So even though in physics, the important models you pick up are very often finite range in what we're doing. And for instance, in Yoshiko's talk, the range by manipulations. By manipulations that you do, you need to consider interactions that are infiltration of a certain decay. What is a question, Hunter? Yeah, a question on the upper, in the formula, in the top formula. What is a norm with the subscript little f? Well, it's meant to be a constant. It's not a norm, the way it's written. We can make norms if we want to make norms. All right. If you want to make norms, all right, but then what you use in estimates is that there's a constant that does this, right? So it doesn't really matter that it's a norm. Okay. Ground states. This is the inequality that, for instance, Yoshiko flashed on her slides yesterday for infinite systems. For finite systems, Infinite systems. For finite systems, this is equivalent to saying that you have a density matrix supported by eigenvectors of the smallest eigenvalue of the Hamiltonian. But if you have an infant system, you want to express this with the derivation, which is defined on this nice domain that I said. But it expresses the same idea that any other vector that you create, any other state you create by perturbing omega will have higher energy. Will have higher energy. And so, omega is a ground statement. It satisfies the inequality for sufficiently many A. You have, in fact, you don't have to check it for all A in the domain. In practice, you do this for a core, maybe local observables or some that have the decay properties that are natural in your context. So then we want to talk about a spectral gap. Want to talk about a spectral gap. So, this is actually a feature of a spectrum, and by this, I imply that there is a Hamiltonian we are looking at, and that Hamiltonian for infinite systems is going to be an unbounded operator because there is no bound to the possible energy of the system. And then the standard setup is that we look at the GNS representation of one of these ground states, for instance. One of these ground states, for instance. What it gives you is a description in terms of a Hilbert space and a representation of the algebra of observables. And so especially these faithful representations are as good as the algebra itself. And so you can represent this state, this ground state of that infinite system, for instance, as an expectation in a vector omega, capital Omega that represents the state. And the only thing that's happened is that. And the only thing that's happened is that now I'm representing my original observables on that Hilbert space. And if you have a ground state, then there's going to be a time-invariant state. And you could have learned from Yashiko's talk that that means that there is going to be an implementing unitary. Maybe it went by quickly. But the time evolution is actually of the form conjugation of a unitary in that case. A unitary in that case, and it's a strong if the tau t is generated by decaying interaction, as we explained, and it actually is strongly continuous. And then this group of unitaries will also be strongly continuous. Then by Stone's theorem, you know, it's of this form. It's generated by a Hamiltonian. And if you have a ground state, you can show that this self-joint operator is actually non-negative and that omega is an eigenvector of eigenvalue zero. Very often, we study situations where we have a unique ground statement. It's certainly not the only thing that is of interest, but for instance, if that's the case, then it will automatically follow that in this irreducible representation generated by omega, which is necessarily a pure state, this eigenvalue zero is a simple eigenvalue. So it's only the only eigenvector is capital omega, the eigenvalue zero. Okay, so that's ground states, and we got a Hamiltonian. And this Hamiltonian is associated with the ground state. If it's a unique ground state, there's of course no discussion what the Hamiltonian of the system in this ground state representation would be. In general, it could be different ones. So let's take the situation as we often have, where omega is a simple. Omega is a simple zero is a simple eigenvector, and omega is the eigenvector, the only eigenvector. So then, what do we mean by gap? Well, there's some positive constant, and you can ask whether the spectrum of the Hamiltonian, which starts at zero and zero is in it, but then above that, there is if there is an interval that doesn't contain any spectrum, so the intersection of this open interval is empty with the spectrum of the Hamiltonian, and that's exactly the same. Hamiltonian, and that's exactly the same thing as saying that there's a stronger inequality than just a ground state inequality, which says that this is greater than zero, it says it's bounded below by gamma times omega b star a for all a that have zero average. So you can also express it differently. This is an expression again of the variation principle. So obviously, not all models will have a gap. The spectrum starts at zero, but it could be continuous spectrum. Spectrum starts at zero, but it could be continuous spectrum, it could be the entire positive real line. But in many interesting cases, and exactly topological insulators, we expect that there actually will be a gap above the ground state. So when that is the case, we say the ground state is gap, and then the gap itself would be the supo of gammas for which you have that. So it really gives you the distance to the first non-zero energies. And if you do this, And if you do this now for an infinite system like one defined on Xenu, then when we talk about the build gap, that's the gap we're talking about, the gap of that GNS Hamiltonian. You can use this setup also to study edge states in a standard setting. For instance, if you take a half space and you can see the spectrum of edge states that may occur near the boundary of your system. What's the matter? I mean, you can change it to politics if you want. I mean, you can change the topology if you want, things like that. Okay, so that's ground states. Stability. What is stability? I was thinking about that, what I should say about that. And believe it or not, this is really true. Last week, I was in line at Pete's coffee here on campus, and I see this t-shirt in front of me. And it says, What do you mean by stability? Okay, so I immediately realized I need to answer this question. I did not talk to the person. I did not talk to the person to find out how far they had gotten with answering this question. So, but here we go. So, what is stability? I didn't make the job. I couldn't believe my eyes. Somebody was asking this question. Okay, here we go. Stability of the bulb gap. Our first result will be about that. So, what does that mean? So, again, here I have written it sort of without assuming. sort of without assuming that omega a is vanishing. So that's again the variational principle. Suppose you have a model and they have these hxes and they decay in a certain way. We have a nice generator and we have a gap gamma zero, some positive number gamma zero. But at least we know that there is a lower bound of the gap. Maybe we don't know the exact number. It's not important. But suppose we have a positive lower bound. Suppose we have a positive overbound for it, and then we ask: if I now change these interaction terms by some other term, one of those that the same type, that decay in a certain way, and I put a real constant in front that I will have to make small because I'm talking about perturbations, right? And I'm going to have to assume that this decays in a certain way. Stability is not. Stability is not a general thing. It's not like you can change a Hamiltonian into any other Hamiltonian. No matter how you do it, you will still have a gap. That's certainly not true because gaps close. But we say that the gap of the model is stable under some class of perturbations, maybe specified by this function g, if for every small value, positive number, gamma, in between Number gamma in between this lower bound for the gap that you have and zero, there is a value of this perturbation parameter s0 of gamma so that the gap of the perturbed system is lower bounded by this smaller lower bound for all s less than this parameter. So I think it sounds pretty reasonable that that is stability. And so as long as we don't So, as long as we don't know that if you do this nicely, and if you have a phi like this that decays and you say linearly, or it could depend on a parameter in a smooth way other than linear. But we don't know in general that the gap is continuous. And so that's why it's sort of more natural to define it this way. And we will see an explicit result that tells you. An explicit result that tells you how that looks, how this S0 of gamma looks in general, where we have results. This is not a question that has been answered in general. We don't have techniques that tell you what to do to find or to determine whether you have stability or not and to define these lower bounds that in general has not been solved. Let me have some results. Let me talk about one that we proved recently. This is a story that has been going on for more than a decade. Well, actually, for many decades, if you go back to the sort of the earlier models before anyone knew about topological order and things like that, this question already came up because spectral gap has important. Has important consequences for the statistical mechanics of a model. So, this has always been an important quality. So, but around 2010, people started to be really interested in this question for systems with the topological order, possibly. So, here is a recent version that adds something to what we know about it. So, I'm going to It so I'm going to have to restrict myself for the unperturbed model to models with finite range interactions that are frustration-free. So that means that the ground state minimizes these terms that I'm writing here individually. I can normalize them that they are positive. And so the ground state energy then is zero, and it's zero expectation for all of these terms. Expectation for all of these terms. So obviously, that's a restriction, but that's where the general theorems live for the moment. And then I'm going to perturb in the way I described before. So I said finite range, so there is some number R so that these HXs are supported in a ball of radius R. And I'm going to also assume that they are uniformly bound. This is less essential. This is less essential, but we use that in sort of the computation of our estimates on some gamma, say maybe if the learning certain are new. And then I'm going to make assumption that is sort of better than what was done before, but maybe still doesn't satisfy you entirely because it's an assumption on the gap. Entirely, because it's an assumption on the gap of finite volume Hamiltonians. So that's why I introduced here this lambda to define finite volume Hamiltonians. So if I take the unperturbed model, so the s is equal to zero, so it's only about this part, and I define the finite volume Hamiltonian stuff. It's finite range, so there's some freedom how to do this. But anyway, it's a finite number of terms, and I can do this for a sequence of finite volumes, say balls. Say balls, and I'm assuming that of these the gap is not too small. Well, it will definitely be positive because it's just a finite-dimensional matrix, and so you have zero eigenvalues that generate the ground state, but then there may be there are positive eigenvalues, and there may be positive eigenvalues that go to zero as the volume increases. And so I have to assume that they don't vanish too fast, not faster than a power law. Not faster than a power law. This could be tweaked a little bit, but it's generally in the cases where we know that such states occur that in finite volume see that the gap closes as you increase the volume. That's generally what you see. So, but in general, I have to assume it. I mean, definitely you can construct models where this vanishes much faster. So, then I'm going to assume my upper model. I'm going to assume my upper multiple has a bulk gap, right? So that's my GNS Hamiltonian. It just has a gap that is gamma zero, it is bounded below by gamma zero. And I'm going to assume then that I'm going to perturb with interaction terms that in terms of the radius of their support decay as a stretched exponential. I mean strength. It can be any strength exponential. It's also not optimal. It's not the only thing you can do, but. It's a broad class for which things work. And then finally, there is a condition called LTQO, which says that the ground states that you see don't really depend on boundary conditions very much. That's one way to put it. And in fact, the simplest condition, which is sort of totally in sync, I think, with In sync, I think, with the things we heard yesterday. If you look at the projection P lambda, which projects on the ground states of the finite volume Hamiltonian, the unperturbed Hamiltonian. And then you consider balls. So you take an observable that is supported in a ball of radius K, and then you're going to put it in between the ground state projection on a ball of radius M, and M is like. A ball of radius m and m is larger than k. So then, um, if you would write this in a basis, maybe in the basis of eigenstate of the Hamiltonian, you would have many terms if there are many ground states, and you would have diagonal and of diagonal matrix elements. But what this says is that, in fact, you have essentially only diagonal matrix elements, and they're all of the same size. And if you think about it for a second, the only thing that is numbered then can be. Number then can be is the expectation of the observable A in the infant volume ground state. You can put something else with a small correction, you get a different estimate, but eventually that's the memory you have to put there so that you can do this for all m and k. And this becomes small as m becomes large. And I'm going to have to assume that it becomes small sufficiently large. And from some examples, you kind of expect that. You kind of expect that maybe you should allow some factor that depends on the support of the observable A. And even if that's not really what's happening, it's sort of interesting to put it in because if you have to prove the property, you may not be able to prove it without a factor like that. So it's better to show that even if you assume it's there, actually, it does not make much of a difference, it slightly changes. Changes this condition that we have on this decay, which is often exponential, but it doesn't need to be. So, this function has to decay fast enough so that this moment here is finite. So, some sufficiently fast fall, a power fall of would be okay for this condition to be satisfied. Okay, these are the conditions. This is the class of models we are talking about. Is that more? Is that more or less clear? Okay, but then we have a theorem. But before I show you that theorem, I want to emphasize once more that we did not assume that there is a gap for the finite boom Hamiltonians. And you may think, well, maybe I can tweak the boundary conditions, and especially maybe if I take the periodic boundary conditions, I don't have to worry about these edge modes. I won't see them. And so I don't need this condition. But there are situations. I don't need this condition, but there are situations where that's not so obvious how you would need that. And this is, for instance, if you have a quasi-crystal. So, here is a Penrose styling, and people have defined models on that. And in fact, quasi-crystals occur, it can be constructed, they actually even occur in nature. And here is an example that somebody studied. It's called Amman-Binker tiling, and maybe it's a little bit too small to see exactly what it is, but it's something with 10-volts in. Is with some something with tenfold symmetry, I think. And this is not so easy to look at large enough volumes so you can clearly distinguish edge states from bulk states, but that was what Terry Loring did in this paper here for a particular model that they were interested in. And then what he plotted here is sort of the energy density of certain states at certain values of the energy, which are too small for you to read, maybe, but it are. Are too small for you to read, maybe, but are very close to zero. Like, you know, 10 to the minus two or something like that. And as you vary this value, you see a transition to these edge states at very low energies. And so you wouldn't be able to assume that a Hamiltonian like this has a uniform gap. At least that's what this numerical evidence suggests. And it's also not clear how to get rid of them. Maybe there's some term that you can. Rid of them. Maybe there's some term that you can add in some boundary condition that will do it, but that's not obvious. So that's why we didn't want to assume that there was a uniform gap for finite volume Hamiltonians. Nevertheless, we have this stability of the bull gap. So if you go to the infinite system with the perturbed dynamics and you look at its ground state, you can still prove it has a gap if the perturbation is not too strong. And so that's what this theorem says, very simple. So if these conditions that I Very simple. So, if these conditions that I mentioned are satisfied, you will have stability in the sense I said that for every positive number between zero and the gap, there will be a constant so that if you look at the perturbed model and its GNS Hamiltonian, and S is less than this constant, you will actually find that the gap of the GNS Hamiltonian is bounded below by gamma. And so this proves stability. Uh, stability and use the strategy that developed by Bravi Hastings and Mikolakis in 2010. In fact, a series of papers or some generalizations, in particular by Michelakis and Zolak in 2013. And so we apply this to the genes Hamiltonian, which poses some technical difficulties, but it can be done. And you, in fact, find this kind of estimate with a beta that is kind of explicit in the data. Kind of explicit in the data. There are ugly formulas that I don't want to show, but if you wanted to, for a particular model, find a number, you could find a number. You have to sum up some series and see what it says. Okay, so that is one result about stability of the spectral gap, the bell gap in this case. Is everybody happy then? Smile if you're happy. Martin is happy. Very good. Happy, Martin is happy. Very good, okay. So now, but you see, so I want to switch to another class of models, and I have various motivations for this. But in particular, in this previous result that is beautiful, we had to assume that the upper third model was finite range, and that's okay. They often are. But it's frustration-free. And that is for the moment a restriction. And there is different ideas of maybe how you could. And there are different ideas of maybe how you could bypass it, but we don't have a general approach at the moment. So I'm going to study a particular spin chain that has been studied in the literature a lot and has some nice features. And I'm going to ask the stability question there in some restricted sense. So let me just introduce this model here. It's defined as It's defined as a sum over nearest projections, nearest neighbor terms, their projections, and they're rank one projections on a maximally entangled state. So I choose some basis of n-dimensional spins. So alpha goes, runs from one to n. I have an orthonormal basis. And I make this maximally entangled state of two neighboring spins. And so the Hamiltonian is the sum of the negative of the projections on these states on all nearest neighbor pairs. Okay. Okay. So that's a simple definition. Translation model of n-dimensional spins. It's clear that this is a simple calculation. You want to check. This is a vector that's invariant on the simultaneous application of orthogonal transformations on orthogonal with respect to these bases, right? So this is going to be an ON invariant model. Cohen and variant model. It's also obvious that it's not frustration-free because if it were, that would mean that you could find a state that has eigenvalue one for each of these Q's, so that you have minus one, the minimum eigenvalue, for each of these terms in the interaction, which would mean that you have a state that is maximally entangled for every nearest neighbor pair. And that is something you can do if you're maximally entangled with one neighbor. Maximum entangled with one neighbor, you can be entangled at the same time with your other neighbor. So it's not just station-free in a pretty dramatic sense. But it's a model that people like and have studied for a long, long time. In fact, for n equal two, it is really equivalent to the Heisenberg anti-ferromagnetic. So there are some models that are equivalent by unitary transformation. I'm not going to talk about that. I wrote the simplest form of a magnet. I wrote the simplest form of a maximum entangled state there, but obviously, for instance, you could put the singlet state there for a spin-one-half system. It's a simple unitary transformation to go from one to the other, and that would give you the Heisenberg interferometer, which is beta-unsat-solvable, and we know that it has Gaplar spectrum. So, that's one we are not going to talk about anymore today. But for n greater than equal tree, there are two periodic grand states. Two periodic ground states. So, the way that this model deals with frustration in the case n at least three is by having a preference to entangle with one neighbor more than another. So the even sides entangle with the right neighbor to a larger degree than with the left neighbor, and this leads to a periodic structure. And so, then you get because it's translation parameter, you get two periodic ground states. Two periodic ground states. And here is, you know, how that looks. So there is an omega plus and an omega minus. So E n is the average value, the average expectation of minus Q. So that's the energy density, right? The energy density has a periodic pattern. It's corrected by this minus one to the x delta n, but delta n is some positive number. Depends on n. And so the two patterns are distinguished here by this plus and minus sign. And this problem also has been studied, like for n equal tree, it was studied a long time ago, around 1990, I believe. You can do essentially also an exact solution. It's not beta answers, it's temporary leap algebra. And many works followed after that. But it's only recently that Eisenman, Dominic Copin, and Warsley proved that this is in fact the situation for the ground states for all values of n greater or equal to three. That's a paper that appeared last year and I'll concurry. Okay, so well that so far there's nothing about stability here. So far, there's nothing about stability here. Let's talk a little bit about stability, actually, a small bit of stability. But I think it was sort of interesting to do again because this is a class of models now with a parameter that does appear in the literature. And you can think of it at least in some regime as a perturbation of the one I just mentioned. And what I'm doing, I'm going to add the swap operator as an additional nearest neighbor interaction. As an additional nearest neighbor interaction. So this is just the operator that changes, interchanges the states of two nearest neighbors, right? So in the basis, it is this. And then now I write a Hamiltonian. I put coupling constants in front of these. So previously, I had u equals zero. I didn't have this. And I had v equal to minus one. But I can take arbitrary real numbers, and it's a nice translation value nearest neighbor model. And in fact, if you think about it for a second, this is the ONX. The ON extension of the well-known spin one bilinear bi-quadratic chain. So it is the most general ON symmetric nearest neighbor interaction if you fix the representation of ON. So you can use different representations to study even more models, but let's fix one. So that's the family of models. So I think of it as, I'm going to say, It as I'm going to say, I'm interested in the neighborhood of the case v equals minus one, which is what I just discussed, and has the periodic. I know it has the periodic states, I know it has a gap. And I want to add a little bit of the swap operator. So this u may have to be small. Well, we can't even prove that in general. We can prove it for large n. All right. So the theorem, the main theorem here, the P Here, that appeared recently is that for large enough n, so n greater than n zero, that is the dimensional dispense, and small enough u, so absolute value u less than some u zero, we still have two pure, two periodic states with a gap above them. And in principle, you can also give estimates, but it's kind of complicated. The estimates are not any good. Are not any good. And so, but it's nice to know that this dimerized phase for this model has at least a range of stability for certain perturbations that are important to consider. And we have something that is absolutely not frustration-free. And so, you can imagine the approach to proving this is quite different. And so, I'll tell you a little bit about that. And what we use to study this model is To study his model is a graphical representation of the ground state. In fact, it starts with the graphical representation of the Gibbs states, but we are in a nice situation. We actually can prove, we did this a couple of years ago with Daniel Ulce in another paper. If you have an even length of a chain, then in fact, for the finite chain, you have unique Rank state. And so this will still be true if u is small. If u is small, and that means that you can write the projection on this unique ground state directly. I mean, otherwise, it would be a higher-ranked projection, so but it's particularly simple here. You can take the Gibbs state and take beta to infinity, the temperature to zero. And since I'm going to take it to infinity, I write two beta here, it doesn't really matter, but I can write sort of symmetric formulas. It's nicer than I draw pictures, it's otherwise irrelevant, all right? Is irrelevant. So the expectation, the grand state then also is limit beta to infinity of the Gibbs expectations. And now, so I'm going to work with this to derive a representation of the ground set. You can think of it as a representation in terms of some basis, but quickly you sort of abandon this picture and you start to think in different concepts. And so one way to get there is to. So, one way to get there is to do this: write this exponential as a high power of one minus the exponent. I've sort of separated things this way. So, this is what Adorak told us not to do. It's very bad. You should use Chebyshev polynomials, but okay. But we have another goal here. So, this is the as the benefit of just a monomial. I can work out the monomial, right? So, this thing to a large power. To a large power, and I have to take the power, I have to take the power to infinity. And since this all converges, I can also take beta integer. I can make sure this is always an integer, right? And then I can expand this in many, many, many terms. So each time I take one term, so when I expand this, I take either the identity operator, or I take some coefficient times the T operator on some nearest neighbor pair, or I take the Q operator. Or I take the Q operator. And so I have a ton of products of such operators. And then I want to compute, for instance, to compute the normalization, I would have to compute the trace and sum it all up. All right. Well, these are kind of simple operators. So I could say, well, I need to compute the matrix elements to compute the trace, diagonal matrix elements, and see what they are. But in fact, here, But in fact, here you see that these are these matrix elements are all just products of Kronecker deltas, and they are kind of different products of Kronecker deltas. Well, there's a coefficient one over n here, but that's simple to include. So what does the T operator do? It swaps. So it means that for the non-zero matrix element one to occur, you know, alpha has to be equal to beta prime, alpha has to be equal to beta prime, and beta has to be equal to alpha prime, right? And I'm going to depict that. And I'm going to depict that. So this is sort of a alpha beta, alpha prime, beta prime by crossing lines. So the lines are going to represent the chronicle deltas, where there's a line that basis label cannot change. That's what it says. So the Q does it is something different. It is a zank one projection. It says that for the matrix element to be non-zero, it's going to be one over n. I have to have. have alpha equal to beta and alpha prime equal to beta prime. So that I represent again by lines, but now in this way. So these lines say that these basis labels have to be the same for the matrix element to be non-zero. I then at the operator I don't have to explain, you know, nothing changes. So alpha has to be equal to alpha prime and beta has to be equal to beta prime. And so in this product, now I have if you could think of it as a Now, I have if you could think of it as a product of the homogenous number of chronic deltas, but I can also try this draw this picture. So I can think of the action of the operator goes like this, right? From alpha beta to alpha prime beta prime. So here it goes. And maybe I should put some labels. So I have, so I can have my alpha and alpha one, alpha two, alpha three here. And I have alpha one prime, alpha two prime, alpha three prime here. of a two prime of a three prime here. I could fix them to calculate the general matrix element or I could put them equal so I have a diagonal matrix element that I need to calculate the trace. And then these operators I write all these pictures. So here I wrote a Q right so I go from here I go in this direction. So from the left I so first I had a Q operator and then I have another Q operator and another one and so on. Eventually here I had the T operator. Eventually, here I had the t operator and another t operator here. In the meantime, I had some q operators and so on. I write those, and then the non-zero matrix elements are the ones that are consistent with this idea that the lines represent where the basis label is constant, and that's exactly the constraint. And then the magnitude of the matrix element is also very simple, it's all ones, except for each q, I have a one over n. One over n so I can do this and then I have to take the limit n to infinity. And if I do that and I want to my pictures not to blow up, I can scale everything down so that this really goes from minus beta to beta, if you want, or from beta from zero to two beta, if you like. So it's kind of convenient to think of imaginary time beta equals zero to be in the middle. And I can. To be in the middle, and I can go from minus beta to beta. But in fact, I have n beta possible factors here, and so I do a scaling limit. And what you get is that these can appear in all random places, and that the so this is the Lebesgue measure here, and it's corrected by powers of what you have. So, I have, for instance, every time, so this is the number, the number of q's, I will have to have a one over n for every. I will have to have a one over n for every Q. So that's this factor, n to the minus the number of Q's. Then there are internal loops, you see. For instance, this red loop, this line closes on itself. And so the condition to have a non-zero matrix element is that the basis label is constant on this loop, but there are n possible values. And so for every closed loop, I have n choices of basis label to do. Have n choices of basis label to do independently, so that's why there's a factor of n to the number of loops, and then the t, it's not really the t I have, I'm going to have u t, u times t. And in fact, I'm going to put v equal to minus one right away. So I'm going to have u to the power the number of t operators. And so I get this measure and up to the normalization. Up to the normalization. So, when I do the scaling, there's some exponential factors actually that come out. I compute the trace, and then here they are, as just an integral of this measure on all possible placements of Q and T operators. And each contribution is determined just by the number of loops, the number of Q's, and the number of T's. I know this is a little bit quick, but in a talk, it's sort of the best I can do. Best I can do. Okay, so that was sort of partition function, but I can also calculate correlation. So if you want to do that, I have to put my observable A there in the middle. So that means I put it here somewhere. So suppose I take a matrix unit, the rank one operator, it goes from alpha to alpha prime. Well, if I put one, then it's going to require that. It's going to require that the label jumps here from alpha to alpha prime at this point. And since it has to be constant on the rest of the loop, this, of course, is not going to work. It will have zero. But if I put two of them, I can get non-trivial correlations. Because if I want to have something non-zero, so I introduce this combination because they are actually the standard generators of O n. They are the analogs of spin matrices. they are the analogs of spin matrices we don't have to you don't have to take this you can just take one term if you want you got a simple formula so if i take the product of two i put i put them somewhere at side x and side y then there can be a non-zero result but it requires that these two jumps occur on the same loop so i jump from alpha to alpha prime and then i jump back from alpha prime to alpha um and now And now, in the interest of time, I won't explain this in detail. The way these loops close, there's a sort of two ways. When a loop closes, it can close like that. If I start here, it can go up and then come down and come back there. That's one way. But there's another way. It can also be that it closes, but it comes from the other direction, say from here to there, it goes up, but then actually it comes from the downside when it goes there. The downside is going to go there. And if you think about it, this gives a sign difference. This is, for instance, an explicit formula of this correlation function. Just as an example. So this picture with all these lines actually encodes the structure of the ground state in great detail because I can put arbitrary matrix units and give you a formula of what integral do you have to compute. So, one thing, I mean, if all these parameters were positive. If all these parameters were positive, if the measure was positive, this would be a probability question. But now, since u is a real number and it is in there, it's in general a signed measure. So you have to be a little careful. You cannot reason. It's still true that these integrals give you the right answer, but they are not probabilities. Okay, so the colour. The correlations are carried by these loops, so the non-zero expectations you get if you put observables in two places, it could actually even be in space-time. It's only those configurations of lines that connect them that can possibly contribute to a correlation. And so we're interested in long-range correlation, so we have to look what happens with long loops. And it's sort of interesting to first note. And it's sort of interesting to first note that since we are also going to take beta to infinity, this kind of situation here, or even like the blue one, where a loop winds all the way around, sort of disappears, doesn't contribute in the limit beta going to infinity. We have to prove that, of course. And then we separate long loops from short loops. So I call short loops loops like those that involve only two neighbors. Loops like those that involve only two neighboring sides, and what they do primarily, you're going to have the Q operated there, and it will actually force these two neighboring spins to be in the maximally entangled state. But then you have longer loops which do something different. So like the red one that involves more sides and it's going to mix things up. And certainly it will no longer be possible for this Nielsen pair to be the maximally antenna stable. And so then. And so then we have to do an analysis. And what we want to show is that these short loops really dominate. Because if you have only short loops and you look at the typical configuration, there have lots of them that appear with a lot of weight in the integral, they will start with a maximally entangled pair on the left, and then there will be one here, and there will be one here. And that will be the unique ground state for that particular finite chain. Okay. But if I go from L to L plus one, But if I go from L to L plus one, I will get a different one because then I will want to dimerize, I want to entangle the two spins that are at the boundary there. And then all the rest is noise, there's perturbation on top of that. That's what you have to prove. And these long loops play a crucial role. And you can see this. So here is a picture where I don't have the U, but U equals zero. I don't have crosses. But what these long loops do, they are contours between the two possible grounds. Contours between the two possible ground states. So, even in the ground state of one finite volume with predicted boundary conditions, I will have some contribution, some coefficients of these long loops. And inside the long loop, the favorite dimerization pattern is shifted by one. So, they are the contours that sort of give the basic fluctuations between the two dimerized states. Okay. Okay, you can actually study the U equals zero model by just using these contours and doing a parasite. But that doesn't work once we put in the crosses, becomes a big mess. So every Q and T operator has some effect on the loop. So in one way or another, so it may cross lines, and that may mean that they are then connected, or maybe they're not connected. You can look at it, there's no way to predict exactly. You can look at it. There's no way to predict exactly what happens locally. You have to look at the whole picture to see what they do. And therefore, we put them all, sorry, we put them all together. Everything that has these double bars and crosses connecting long loops, we put together in clusters and we do a cluster expansion for the clusters that you obtain in this way. Okay, that's quickly said. There's a little bit of work to do, but there is a variant of Kotetsky-Preiska theory. Variant of Kotatsky-Price criterion for this kind of expansion and convergence for large n and small u. And that is why we have this kind of theorem. So in fact, so one thing you wonder indeed is correlations and you can do space-time correlations. So it's not surprising you also get the gap. So I put one of these elementary matrix units at X and it's Y, but actually the one at Y are shifting time. But actually, the one at Y is shift in time. And what you can prove is that you have exponential decay of correlations in space-time using this expansion. And everything follows from there, essentially. Okay, so my time is really up. I could discuss a little bit what that says. Maybe we will go to the phase diagram of this land model. So we've been talking about the neighborhood here of this point. So this is. Of this point. So, this is the u and v parameter I talked about, the coefficients of the t and the q operator. So, this is v equal to minus one. And then u small is around that point. And overall, scaling doesn't matter. So, these phase diagrams are usually drawn on a circle because you multiply by a positive constant, that doesn't really change anything about the ground states, right? And so, what we proved is. So, what we proved is that the diameterization you have for all values of n greater than 3 is stable. And it's not just in this point that is in some sense an exactly solo point. It's true in an interval around it. And you have dimerized grand states there, and you have a spectral gap for these two states. And this is a particular phase that, you know, it's not the only thing that's happening in this whole. It's not the only thing that's happening in this O model. If u becomes larger, so in particular, if u is equal to n minus 2 over 2n. It's a little small. I hope you can read it. So there is a point where something happens. You get a Gappus point. It's again exactly solvable. Rashitikin did that. And then you move to another phase. To another phase that actually contains a frustration-free model. And in the case any context, it's AKL teaching. But it's a generalization of the AKL t-chain. And it shows actually different behavior depending on the parity of n. So for AKLT, N equal to 3, there has a unique ground state and a gap. But for even N, you get two matrix product states that are periodic. That are periodic and also capped. And by stability results, we proved with Bob Sims and Amanda Young also in another paper. They apply to this frustration-free case, and they show that this dimerized phase is also stable. You have dimerization, two ground states, and a spectral gap around that point in some interval. There's many other things to say about this phase diagram, but this is not really relevant here, and we are running out of time. Really relevant here, and we're running out of time. So let me wrap up. So we proved a little bit of stability for the ON chains, but in fact, I believe they are just as stable as these other cap models. And so, and there is more that I would know how to do, but general stability, I'm not entirely sure what the good approach is for that. And so that's something to think about. A good general Think about a good general method for non-frustration remodels. It would be good to have, and there's different things, different ways you can think of how you would do that. But until actually there is a result, we don't really know what would be as. And maybe we also will need a more general formulation of LTQ, which isn't really, you think about it, doesn't need frustration freeness. And that's probably less of a problem. That's not conceptually much easier to deal with. Easy to deal with, but if you do that, maybe we can get a more general stability theory. I mean, the only thing you cannot expect is to prove that all gaps are stable. So, and there's also counterexamples, and that's not the case. All right, I'll stop here. Thank you very much.