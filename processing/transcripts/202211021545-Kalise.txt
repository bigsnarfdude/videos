Thank you. Well, good afternoon, everyone. Yeah, I'm very happy to give this talk here because of such an illustrious audience. No, because honestly, no, truly, whenever, no, I'm not joking. I have to joke about myself that whenever I give this talk, I never get any questions. But I feel, I don't know if it's too easy, too complicated, or whatever, but I think that, yeah, maybe. I think that, yeah, maybe this is the right audience for somebody to ask. You explain that no, I would like to think that, but I'm afraid that this is quite the opposite. So let's see. Because this is really a bit convoluted. It's a three-headed monster about things that have to do with control theory, a little bit of machine learning, but nothing to be taken too seriously, and collective dynamics. So I will have at the end some slides about how can we use what I'm showing for kinetic equations. Use what I'm showing for kinetic equations. It also applies for type of non-local transport equations in general, but most of the dog will be at the microscopic level. But there is a connection. So first, some motivation. I mean, what can I say about collective dynamics that you haven't mentioned already? It's that, for instance, here we're interested in problems where there is really a control component. So it's just not. A control component, so it's just not a simulation of the forward dynamics. Somehow it's related to Sue's talk yesterday in the sense that, well, this is not a data-driven discovery problem or something like that, but we do have collective dynamics that are forced by an external signal. In this case, I'm thinking about human, sorry, robot animal behavior. So you could have a set of animal agents that, of course, are uncontrolled, but you try to force them through the action of a robotic agent. Of a robotic agent. So that introduces another level of complexity in the sense that it's not just a simulation of the forward dynamics, but it's the tuning of an external control signal. Actually, this is something related to this. This nice picture is from some work that we're doing with Jose and some colleagues in Swanson in collective behavior. And actually, I always show these simulations because they are very nice. This is really real, it's a real data set. Data set of a shepherd dog acting with a shepherd, a flock of sheep. So this is, yeah, you could think about, for instance, the simulations of the fish that were showing yesterday, but here the difference is that you really have a shepherd dog. Yeah, you would like to think about it as an external signal. And you might wonder questions that are traditionally labeled in optimal control as inverse optimal control. So you might want to discover. Control. So you might want to discover not the parameters of the dynamics, but you would like to discover what is the cost that the shepherd dog is optimizing. And of course, already this is computationally a bit more demanding because you will have a multi-level optimization problem where there is an optimization problem of learning whatever you want to learn, but then constrained to the solution of an optimal control problem for a fixed set of parameters. So that also sets the idea that I will also be interested in. Idea that I will also be interested in building surrogates, so building through machine learning some objects that will allow me to perform many times a simulation or some optimization on top of it. Okay, so this is, yeah, this is this slide is certainly not for you. It's for my control colleagues that I yeah, because no, seriously, because sometimes my feeling is that you talk to the control community and nowadays they're a bit skeptical that there are still interesting non-linear. There are still interesting non-linear control problems to be dealt with. There are many people who are sometimes focused on net worse, large-scale problems, and stochasticity, but I'm always trying to make the guess that still we do have people from collective dynamics, they do have a class of interesting control problems that have a nice structure, either at the microcopic or as a mean field control problem, because they exhibit this sort of behavior. You would have disordered stage, you will have meals, you will have flux, and you might want to think. You will have flocks, and you might want to think about transitions between these regimes and how to design control signals that can enforce the systems to switch to one behavior to another. And well, and the third component of this is what I want to do in terms of controllers. I'm always working with these large-scale particle systems, but then these are meant to be very, very high-dimensional. And then I would like to incorporate data here to do. Would like to incorporate data here to do a design of controllers of external signals. And as I always say, there should be my wish list as a control designer would be that whatever I design is robust with respect to external perturbations. You can think about wind acting over a set of drones. You would like to keep them on a formation. Then real-time computability. So you will see a lot of expensive computations. And I'm okay with that as long as, at the end of the day, I'm able to give a control signal that does not require. Signal that does not require to run an optimization online, it's merely an evaluation of a proxy or an evaluation of some object that is spend a lot of time training offline. And sparsity, of course, but that's just one characteristic that you will see later. So convex optimization, L1 norms, trying to obtain objects of very, very reduced complexity. In the world of control, and if you think about data-driven control and data-driven optimization, in general, there is always a trade-off between efficiency. Trade-off between efficiency and how automated is what I'm proposing. In the sense that people working in control theory with no interest in data would know very fancy controllers for very specific set of equations, dynamics, where they know a lot of physics and they have decay rates if you were to plug on the right-hand side a certain control signal. That's what I call adult design. Of course, this is highly efficient in terms of data in the sense that they don't use any data other than knowing what is the current state. Other than knowing what is the current state of the system. And then you, that's the only thing that you need. On the other hand, this is not automated, no? You will change a little bit the dynamics and you will have to start from scratch. And that might be an opportunity to write yet another paper on some modification of the dynamics, but it's really not automated. On the other hand, you have very, very general frameworks like deep reinforced learning that they, in fact, they are very automated. You just pick a generic algorithm of reinforced learning and it will apply for a generic knowledge of It will apply for a generic knowledge of your Markov decision process and so on. It wouldn't be that specific on the dynamics that you want to control, but on the other hand, it's highly inefficient in terms of data, really using billions, millions of samples to train a model. And one thing that you will see a little bit later on is how through the use of control theory and some things that we know about dynamic optimization, we can move to a more equilibrium state between a trade-off on data efficiency and automation. Automation. Yeah, you cannot read that. No, this is just if you are curious about this hype of deep learning and so on, maybe it's good to read this paper because is somebody having a bad day ranting about how inaccurate are machine learning models compared to traditional discretization techniques? Which is something that I generally agree if you were to solve low-dimensional problems. The things that what you're Of low-dimensional problems. The things that what you are going to see now later is some very, very high-dimensional problem, not in the sense of the mean field. I can always take the mean field, but the internal state of the system, of the dynamics of each agent is going to be also very high-dimensional. So, in a certain sense, the problem has a double curves of dimensionality. You have many agents, and each agent is already very, very high-dimensional. But okay, we will get to that. So, okay, that's the generic part on control. The generic part on control. So we'll have a non-linear dynamical system. You can think about your favorite collective dynamics model here acting freely with a certain non-linearity. And for simplicity, I will just keep it control-affine here. So it appears linearly in the control. And we want to do stabilization to a certain region. You can think about stabilization to the origin or to any other stable state of the system. Usually you do this with a quadratic penalization of the state, a quadratic. Penalization of the state, a quadratic penalization of the control, and you take an infinite horizon. Usually, here, the infinite horizon is the one that gives you symptomic stability towards your equilibrium. Of course, one may always be tempted to treat this just a discretize this and run a very large-scale optimization problem. Of course, here, the time frame here is not very helpful for that. A different technique that we always use for these class of problems is dynamic programming. So, you value function of your control problem. Of your control problem, which is the optimal cost associated to solving the optimal trajectory from a given initial condition x, and instead, since the 50s, we know that by Bellman that this value function satisfies a partial differential equation, a non-linear partial differential equation. For this particular problem, you have this non-linear Hamilton-Jacobi first-order Hamilton-Jacobi equation. If you were to add noise here, of course, what you get is a second-order equation. Equation. This is a different derivation of the dynamic programming principle. But in general, the logic here is that you cast your control problem and you always have the chance if you have a dynamic programming principle behind to derive a PDE for the value function. Once you solve this partial differential equation, you will obtain the control in feedback form. So it will only depend on the current state of the system and some very low-dimensional minimization to be solved along the way, because it will only depend on It will only depend on the gradient of V, which is something that you have already computed, assuming that you have solved this PD. You solve this PD, you start the gradient somewhere, and then as you move along a trajectory, you just recompute this finite dimensional minimization. This is just a point-wise minimization on the current state of the system X that tells you what is the optimal action that you have to undertake in that point. Then you move forward in time. And that is robust because if you move forward in time and somebody kicks you out from your takes you out from your trajectory, you are still somewhere in the state space and you can always do this recomputation and obtain what is the current optimal feedback control given the current state of the system. So these feedback laws are very, that's feedback and that's robustness is the ability to just depend on the current state of the system and compensate as you are perturbed. Of course, the big if here, the big elephant in the room here is the computation of this partial differential equation. This is difficult because it's a non-linear PD, but on top of it, Is a non-linear PD, but on top of it, this is written in the dimension of the state space of the system. So you have 40 sheep here, like I have, and in two dimensions, a second-order model, we're already talking about 100 dimensions for this partial differential equation. Of course, the price, then what you will obtain if you really think about the dog as a control, is that the dog now will be able to find the optimal action just by sensing what is the current state of the ship, and then it will. State of the ship. And then it will know what is the action if you set a goal, for instance, trying to steer the ship to a certain location in space. The ship, they are perturbed, they move somewhere else. It doesn't matter as long as the dog is still able to sense the current position, the dog will be able to solve this minimization and find what is the optimal action. Okay. Well, there is, this is really, this is an. This is an, I wouldn't say an open problem, but this is something that people have been looking for for the last 50 years, trying to really scale up the solution of the Hamilton Gerko equation. The curse of dimensionality, of course, means in this case that if you were to have a PD of this type, you cannot really just take finite differences or finite elements and tensorize your high-dimensional grid because you wouldn't even have memory to store that many degrees of freedom. But there is a myriad of works all very recent, like say from 2017. Say from 2017 and so on, that people have been using now instead of grid-based schemes, deep neural networks of machine learning tools in very different flavors to solve very high-dimensional Hamilton-Jacob equations. And the state of the art is more or less between 100 and 200 dimensions for these problems, if architectures are chosen properly. It's not just about machine learning, just for a bit of just some advertising. Just some advertisement, one of the things that we do, for instance, is to use tensor decomposition. So we still carry on with the agreed structure for the value function, but we look for low-rank decompositions of that. And that also allows to scale up to 200 dimensions. Unfortunately, these are not as fashionable as the neural networks, so we don't get so much good press about our tensor. You compositions work, but the truth is, if you look at the papers, we get the same scales that you can do with deep learning. And we do have a code. With the deep learning, and we do have a goal hope to prove really rigorous convergence estimates for these schemes. One idea here that may or may not be related to this talk, but I find very interesting, it's that around 2016-17, I mean, I think it's an interesting idea from the PDE viewpoint. There was a paper by Joshua and Dalbon where they say, you know, for this very, very particular Hamilton-Jacob equation, you can recognize the Hamilton-Jacob, just a quadratic gray, and you're not a quadratic. Gray, and then a quadratic nonlinearity on the gray. For this very particular Hamilton-Jacob equation, if you have a convex initial condition, you have the Laxcop formula. So, you actually have a representation formula for the solution of this PD that goes through convex optimization. So if you want to know at a certain point of the space-time cylinder, the solution of this Hamilton-Jagov equation, you just have to solve a finite-dimensional optimization problem where this is the convex conjugate of the initial condition, and this is just a convex optimization problem. And this is just a convex optimization problem, which has a very beautiful structure, very nice structure for just casting your favorite convex optimization solver and really do this in real time. So although this is not exactly machine learning, it really sets the stage and saying, you know, maybe there are ways to navigate around the solution of the PDE through representation formulas and use this as a synthetic data generator to just generate samples of the solution. And then you can try, for instance, with supervised learning, you can. For instance, with supervised learning, you can train a model for V. And the only thing that you will need to do is to collect many independent samples of X and solve one zillion times this convex optimization problem, which believe me is something that you can do very, very quickly. I'm talking like one microsecond, one millisecond to solve this optimization problem in high dimension because it's just a convex and nice optimization problem. Of course, the catch here and what drives a lot of research in terms of the construction of In terms of the construction of data-driven schemes for the Hamilton-Yago equation, is that this Hamilton-Yago equation is sort of useless. It's a beautiful one, but it's too simple for to have any meaning in control-oriented, say, Hamilton-Yago equations. So they belong to the same family, but this object here is radically different than this one. It's not the same thing. So I don't have such a nice representation. So, I don't have such a nice representation formula for the Hamilton Degree equation arising from optimal control. Or do I? That's the question. I do have. I do have. It's just a bit more convoluted. It's something that we know also around the same dates. One interesting thing about the development of model control theory is that really was some sort of Cold War product in the US. There was Bellman working on dynamic programming. And in the Soviet Union, you have all the school around Pontragen working on. School around Pontragen working on optimality conditions for a given initial condition, not globally. But there is an equivalence between Hamilton-Jacob equations and the optimal trajectory steeming from here and the optimal trajectory that steams from solving Pontriagi's maximum principle, which is just you give me an initial condition and then it becomes a two-point boundary value problem for finding the optimal trajectory departing from that initial condition. That initial condition. It's philosophically different because here I'm really solving a very difficult PD, but I'm obtaining the solution for the whole state space. Here I'm doing something simple: no PDEs, just ODE involves in a two-point boundary value problem, but that only gives me the optimal solution for that initial condition. So for a given initial condition. So it's not very useful if you want to do real-time control and it wouldn't have any robustness. Any perturbation along this optimal trajectory will lose any optimality certificate, say. And here, well, this is a contentious issue in optimal control theory because if you pick any optimal control book, they will tell you how to go from here to here, but you should not go from here to here because these are necessary optimality conditions. These ones are sufficient. So, it's like this will give you a local optimizer, this will give you the global optimal solution. So, we need some additional structural assumptions on the control. Structural assumptions on the control problem to be able to claim that actually Pontiagian principle in under certain conditions is also sufficient. Okay, but if in that sense, if you have a smooth dynamics and you have the quadratic cost, so there is a certain convexity on the cost, not on the dynamics, you can actually claim that the characteristics of the Hamiltonian CoEPD would correspond to the solution of still. Would correspond to the solution of this two-point boundary value problem. And then, for a given initial condition, if you solve the two-point boundary value problem, the V associated to that initial condition would be nothing but to evaluate the optimal cost of the control problem along the optimal trajectory. So it's a bit silly. It's a rough convoluted way to generate V, but that would be the optimal solution. Solve the optimal control problem through Pontraigen and then evaluate your cost along the optimal trajectory. Along the optimal trajectory, and that will give you the optimal cost, the value function along that trajectory. One thing that, and this somehow is the first link to what I was showing in this graph about balancing and doing data-driven control, but with a bit of more mathematical flavor, is that from also from control theory books, you know that once you solve this two-point boundary value problem, the gradient of V comes for. The gradient of V comes for free because it's the joint variable of the Pontrain system. So now it's getting more interesting because if I'm thinking now about learning through sampling, I'm not only recovering samples of V, I will also recover samples of gradient of V. So that's sort of what we call gradient augmented regression. But that's it. So one thing that I will do is very simply is just to sample different initial conditions and build a data set based on these two. Built a data set based on these two relations here, and then I can train a model for V without having to solve the Hamilton-Jacob equation, just sampling two boundary value problem to all these systems. So it sample initial conditions. At that point, we did it uniformly. Now we realize after many vapors, different people realize that we have to pay attention and there should be better ways to generate this data, to generate the samples that you want to use. For each one, That you want to use for each one, and this is embarrassingly parallelizable because this is completely unrelated. For each initial condition, you can parallelize the solution of the Pontraien solution of the Pontragin system using a gradient method or a gradient or a Newton method, whatever optimization method you want to use to deal with the single optimal control problem. That's your data set now that becomes a data set for the value function and its gradient. And then what I will show you here first, the first What I will show you here first, the first idea that we had was to do linear regression on this. Not even deep neural networks, we just say, okay, I have these samples of the function and its gradient, and I'm just going to train a linear model for this. Linear in the coefficients, of course, my basis here is going to be polynomial. You can take orthogonal polynomials or you can take kernels, your favorite object of approximation here, but you will have a linear combination of that. And this regression is a grain augmented one. Is a gradient augmented one. So we train this model based on samples of V and gradient of V. Just a very short discretion here, of course, there is a part here where you have to choose a model. In our case, we just take just a monomial basis in high dimensions. And in order to scale into high dimensions, we truncate this base on what is called a hyperbolic cross. So if you want to generate a high-dimensional basis, for instance, For instance, in two dimensions, you will have something in one day, you will have x, x squared, x cubed, and so on. These are your monomials. Now you go in two dimensions, and how do you do this? Well, if you do direct insularizations, you will do x1, x2, x1, x2, x1 square, x2 squared, but you will get things that are a bit too much, like x1 squared, x2 squared, for instance, if you do direct sensorization. You do direct sensorization. So, a first question is: how do I generate this in order to not blow up the cardinality of this set? And one thing is truncate by total degree. So, for instance, you say my total degree is two, and this one goes away. That allows you to go from exponential cardinality to a cardinality that depends combinatorially on the degree and the dimension. And we go one step further and we really trim this. I think you can, yeah, it's fine. You can turn it off. Of I um we go one step further and we use what is called hyperbolic cross-approximation. So, my rule is not to truncate according to the sum of the exponents, but according to the product of the exponents. And this gives a much stronger specification of the of the coefficients, of the monomials of the basis. So it's a full tensorization will have this plot here with the degrees just you will have a just a cube. The total degree will just be a plane, and you further three. Plane, and you further trim this with this rule of the product of the exponents, and of course, this is much, much has a reduced cardinality. You don't have exact formula for the cardinality, but you have upper bounds. What you can notice from here is that these bases will give more provenance to the very high-dimensional, very steep terms in one dimension. So, you can have x to the eight, but you'd not you are not allowed to have, I don't know, x to the three times x to the four, because that product is just. To the four, because that product just goes too much. So, of course, there is a problem there. You would really want to classify to understand if the value function that you want to approximate is really well represented in a basis that is so steep along just single dimensions. But anyway, that's another string. Okay. And then, yeah, I showed this slide to my optimizations too, is because really, this is really. Optimizations too is because really this is really linear regression. It's just a matrix assembly. Now I have my samples of the function, I have my samples of the gradient, I have my model here, my polynomials evaluated in the sampling points, and this is just really the linearly squares matrix assembly based on the data of the samples of the function and its gradient. And that will be the linear least squares. If I just really have a polynomial, a linear model for the parameters here that I want. For the parameters here that I want to learn from my value function. And then there is the third thing here: I really want to add sparsity. So I'm not going to do the linearly squares regression, but I'm going to do also some sparsity here. I'm going to do a Lasso regression. I'm going to add a L1 norm penalty here that is non-differentiable, but still preserves convexity, because this will give me sparsity on the theta. Sparsity on the theta means that this sum here, sorry. means that this sum here, sorry, that this sum here, although I might write it with Q components, they will have many zero components. So I would expect that at the end, my representation of the value function will even have even more reduced complexity here than just Q components. And that's useful if then really you want to go to a real-time calculation to have a feedback load that it has a much more reduced complexity. And anyway, this is a nice regression problem, quadratic term plus here. Quadratic term plus here a one-norm penalty. Okay. So, what happens here at the end of the day, for instance, you have this nothing related to agent-based models. I will show you that in one second. Here I have just a two-dimensional problem, no linear dynamics. The control enters linearly. I want to do stabilization to the origin. I want to find the control signal that brings any initial condition to zero. And for that, that's my quadratic norm of the state squared. Norm of the state square, norm of the control square. So, what I'm doing, roughly speaking, is generating a lot of samples of V. This is V. This is the value function of the control problem. And for each one of these points, I'm solving the Pontrain conditions and calling this two-point boundary value problem. For each one of these points, of course, in reality, you don't want to have that many points. And this is just like the idea is to save the samples. There's no point to have, but you get divided. So I sample for each one of these points, I have. For each one of these points, I have to solve the two-point boundary value problem. This now becomes my data set. There is additionally to this, there is also the information of the gradient of each one of these points. I cast my regression problem, for instance, with 40 samples and without using the gradient, just to show you what happens. If you have 40 samples, you do linearly squares without sparsity and without gradient information. With 40 samples, you get this plot here that somehow it's okay around the origin, but you see this has this. Around the origin, but you see, this has these spurious variations that I want to raise. People, at this point, for any other reasons, if you find something like that, people will just, the natural, the zero solution to this will be, you know, I'm going to blow up the number of samples. I need more samples for my regression to improve the accuracy here. And then you go and you, instead of taking 40 samples, you take 120 samples and you recover the right shape. The right shape. That, of course, there's nothing wrong with that, but it's a very expensive idea, especially because here, each one of these samples requires the call to a two-point boundary value problem. And this is very inefficient. Instead, I can keep the number of samples, but now I use the fact that I have knowledge of the gradient. So now my 40 samples at the end, they become 120 samples because I have a two-dimensional gradient. Because I have a two-dimensional gray that brings two extra data per point. And then I also cast the right optimization framework. So to the linear squares regression, I give it a bit of more robustness through sparsity. So I put an L1 penalty here. And now I get with 40 samples, I get something that beats this one in even in H1 norm here. And moreover, this one, because of the sparsity, now if you go back and you look at the coefficients, you will realize that although you had 40. You will realize that although you had 42 coefficients, the expansion is only using 19. So it's more efficient, it's more accurate, and you require a fewer samples. I always make this point because, you know, five years ago, we saw a paper, and we saw a few papers where they were not using this. And say from a control viewpoint, it was very puzzling because we know that the adjoint variable comes for free, that the adjoint is the gradient of the value function. That the joint is the gradient of the value function. And if you were able to solve the optimal control problem, it was just one extra line of code to save that output and record information of the gradient. So yeah, actually, you can also think about higher dimensionally, higher order information, like Haitian information as well. But yeah, at the end, knowing from optimal control theory that the adjoint of the solver is related to the gradient of the value function really allows you to do things in a more To do things in a more parsimonious way. Now, of course, we want to push this to high-dimensional problems, to agent-based. So we're playing around with the Google's main model here for consensus in two dimensions. Now I have 20 agents in two dimensions, a second-order model. So now I'm having 80 dimensions for solving a law to control these 20 agents under any initial condition. Initial condition. I take polynomial degree up to degree 4 for the expansion of the value function, but now you have a polynomial of up to degree 4 in 80 dimensions. So that's a lot of falls there in the basis. Actually, the cardinality of that, when you do hyperbolic cross, now you have 3,000, almost 3,500 terms in that expansion. So it starts scaling quite fast. The uncontrolled state, of course, you know it, depending on the initial. State, of course, you know it depending on the initial condition, if this may or may not go to consensus, you might get a divergence initial state. I'm just showing you one single sample. What you will do if you solve the Pontrain conditions for a given single initial condition, you will obtain an optimal trajectory that goes to consensus. So just to make sure that my solver is working properly. And then here you see, for instance, the linearly squares result if you don't use gray information and you use 2400 samples. 2400 samples. This is not good. It's not giving you stabilization to consistently. It looks like it's trying and going to get it, but it's nowhere close to the optimal solution. And really, it's 2,400 samples of this 80-dimensional optimal control problem, which can take a while just to generate this data. Instead, if I use the gradient augmented regression with the L1 norm, I really have 70. I can already get something very acceptable with 70 dimensions. Very acceptable with 70 dimensions with really fewer samples than agents. I'm not under sampling because I also have the gray end, and the gray end now is very high-dimensional. The gray end is a, it scales with the dimensions. That's what thing. One thing that we realized is that with the gray end-based framework, as the problem increases in dimension, I don't have to blow up the number of samples because I'm always recording an object that scales as high as the dimension. Okay. And then, of course, this is, you shouldn't be surprised that this. You shouldn't be surprised that this is not exactly the optimal solution because now I'm really validating. So, this is not the data that I used to train. It's just I'm looking at the feedback load at the optimal control at other initial conditions that I didn't use to train the model. So, I will have certain error. So, it's not going to perfectly state, but as I increase the number of, I increase the cardinality of my basis or the number of samples, this should be fixed as well. Okay. Yeah, just just read it. Yeah, just to reiterate on the idea, but I think it's clear. This plot here is without gradient, this is with gradient. You can see that to reach the same level of accuracy, you can save a few orders of magnitudes in respect to the samples. And of course, you also recover a certain sparsity. And we see this. And this is, I'm very proud of this because it's just linear regression. I did it. It's really a linear regression with polynomials, linear coefficients, Lasso regression. Linear coefficients, lasso regression, and that's it. Of course, but if you want to make a living on this, you need to use the neural network. So that's the last part of the story. Yeah. Of course, you may not want to use linear regression or polynomials, and you might want to use deep neural networks, but you can replicate exactly the same idea because the data generation, which is the difficult part, it's there already. So it's the same. It's there already, so it's the same idea of using samples, solving point training, getting the value function, getting the gradient of the value function. What it makes it more interesting to this loss function is I'm not just matching the date of v, but I'm also matching the gradient v. The literature on gradient augmented say learning is not as vast as just the classic just output and L2 loss here on the data. People have been looking at these for many flavors. For instance, you might want to. For many flavors, for instance, you might want to learn directly the optimal control law. I'm always in everything that I'm explaining here, I'm always recovering the optimal control law as a byproduct of recovering the value function, just as some generalized gradient of the value function. But you might want to forget about all this and just construct directly a neural network for the optimal feedback law without going through the gradient of the value function. But you use deep neural networks, feed-forward neural networks. feed-forward neural networks, it works, it works well. But we have discovered actually that we can do a bit better in terms of accuracy and say the dynamic behavior of the system if, for instance, we not just use feed forward neural networks, but use more sophisticated architectures like recurrent neural networks. So it does make a difference. Of course, well, this is equivalent. Well, this is equivalent at a certain point to a feed-for-world neural network, but the approximation theory side of this is not as nice as if you were just have a fully connected feed-forward neural network. But anyway, having this sort of recurrent structure, it also plays a role, it improves you, it gives you a bit more accurate solutions in certain problems. Okay. So here, the last part here is: okay, what else can we do with that? So now we have a way to construct this very high-dimensional. We have a way to construct this very high-dimensional feedback loss, but of course, that wouldn't be enough if you really are thinking about controlling one million agents or whatever, a very, very high-dimensional state space. So first thing, of course, that comes to your head is the blessing of dimensionality, you know, mean field modeling. You just take n going to infinity, and now you get, you can write again all these nice control problems in terms of what we call mean field optimal control. Of course, you would have here a non-local transport equation, but you will have a cost here. But you will have a cost here. You can do the same business of the Pontregin of deriving, say, non-local optimality conditions, first-order optimality conditions for the mean field control problem. Okay, that's something that we have done in the past, and you might want to, for instance, be interested in the numerical approximation of this when the dimension here D is already very high. We do have machinery how to solve this in two or three dimensions, the non-local TD here and the non-local adjoint equation as well. Equation as well. That's doable, but one problem here is what can we do if this is very high here? Why D is very high? Of course, I can always tell you about the financial markets and you can think that the agents are just their state by via portfolio or something like that. Something more concrete, for instance, is that very interesting papers by Rostov and van der Edigen in 2019, fairly recent, where they are thinking about mean field models of training deep neural networks. So, really, Training deep neural networks. So, really now you would be thinking that the state of the agent is a vector where each entry would be the one parameter of a neural network. But believe me that this, of course, if this is true, this leads to very, very high-dimensional kinetic equations or mean-field equations, even if it's just forward. I'm not trying to control these PDEs, I'm just trying to learn the evolution of this model. The evolution of this multi-agent system, but they will be naturally very high-dimensional because the state of the agent now will be the parameters of a deep neural network. It's like what you have been doing on consensus-based optimization. If you really want to use this sort of framework to optimize the parameters of a neural network, naturally, these agent-based models are very high-dimensional. So, and of course, here, somewhere in between the Somewhere in between the microscopic world and this non-local transport equation lies a kinetic PE that for sure you know 100 better times than me. So I'm a bit ashamed to talk about that. I go very quickly here. I'm running out of time. No, no, no, you know this better than I. So let's not enter that conversation. I just want to say that here, you will have to sample many times this binary interaction. And now, Interaction. And now, in a control framework, this will be a control, a two-agent control problem, which is something that I can solve easily if the, of course, the agents here are low-dimensional. But still, if the agents are high-dimensional, even if these are just two agents, this is still very expensive. And even if I can tell you what is the Hamilton-Jago equation here that you would need to solve to find the solution to this, this would be difficult. So, of course, if you were to do a forward simulation. Course, if you were to do a forward simulation of this, you will require many samples of this. In the most navy implementation, that would mean that every dynamic you want to sample this collision, you will have to solve this optimal control problem. Of course, you don't want to do that because that will take forever. And that's probably the first words by Diago Moal, Via and Lorenzo Paeschi, where they were concluding sort of optimal, instantaneous optimal controls that they do have a closed form, because you cannot afford to be solving the optimization problem every time that you need to sample this. Every time that you need to sample this interaction. But of course, based on what I was explaining before, it wouldn't be a problem here to just write this generic binary system, solve this very high control problem, do it in a feedback form, do it with a neural network, and then just provide a proxy of this control. So to do a forward simulation without having to sell the optimal control problem every time that you sample these agents, you just evaluate the feedback load that I just trained in my offline phase. In my offline phase. So you will have your collisions here, your binary interactions that would feed a Monte Carlo simulator of the Boltzmann equation. And then I will do just what I explained in the first part, supervised learning, just to train a model here of the feedback law that should be used. At that point, we also got a bit carried away with this and say, okay, if we can just build a neural network for this. Just build a neural network for this. Why don't you just build a neural network for the whole right-hand side of the interaction? And a neural network just predicts for the post-interaction state, just assuming that, of course, every time you will solve the optimal control problem. And it's really a minor modification of what we did. I won't discuss this link because it really comes at the end of the talk. It's really, if you want to think about that, you're welcome to think about it. And if you have something interesting, please. And if you have something interesting, please tell me because everything that I'm going to do is optimal at the binary level. I don't have any optimality with respect to the optimal control of the kinetic equation or to the link to the solution of the optimal control problem at the mean field level. We do know that this is a very good approximation. We have done this in the past, but we don't have any theoretical guarantee that by doing this sampling at the binary level, you get something that is really the solution to this under certain conditions. Under certain conditions, it shouldn't be because what I'm doing at the microscopic level is really that I'm just solving control problems with two agents here, and I'm just doing a massive sampling of this. You shouldn't expect the same behavior as having really the optimal strategy for the full large-scale system. To show you some tables here, relevant, for instance, the sort of accuracy that we get with feed-forward neural networks, because usually people would complain that using feed-forward neural networks. Complain that using feed for one neural networks give you an accuracy of, say, 1% or like between 1% and 10%. Using regular neural networks, we're really now down to 0.301%. So it's a bit more accurate. I would say it's dramatically accurate, but still it's much better here. So if you do Google's mail models or just more potentials for the binary interactions. And this other plot here, this other table here is sort of CPU savings. If you were to fetch in a classical four. were to fetch in a classical forward simulation, say 10 to the 4 Montegalo samplings of these binary interactions, you will have say 30 dimensional agents in a second order model in two dimensions, or really again talking about 120 dimensions for the binary system. 120 dimensions for the second order binary systems, you will have 10 to the four samples and the evaluation of that is really a matter of sec. Of that is really a matter of seconds here. It really makes a difference though. Here, two lines is really I'm showing you the neural network for the whole right of the collision. The last two neural networks here are just the neural networks for the optimal control. And you see, sometimes you can even get more accurate solutions just by just wrapping everything, the whole right-hand side, in just a neural network. And of course, this is the last row here. And of course, this is the last row. Here is what you would have to deal with with these 10 to the 4 samples in these very high-dimensional problems. If you were not using any of these and really solve the optimal control problem every time you call, it's really not tractable. It really takes forever. And probably these 10 to 4 samples will just do one time step of the forward simulation. So it's not good. Yeah, these are of results that we have. So we have this very, this class is a consensus problem here with the Problem here with the attraction-repulsion dynamics. So, this is just the initial state of the particles. Without control, they will just go to sort of scattered distribution or the marginals of the three-dimensional velocities. And with the control, you can see if you look at the order of the axis here, there is a consensus on how the particles are concentrating in a single point. But this is three-dimensional second-order model. So, yeah, this is already about 12. This is already about 12, what? Yeah, 12, yeah, 12 dimensions for the binary system. Yeah, I think that's my message. We do have control theory to solve these problems. I don't know why you might be interested in this. I guess I have the explanation of the beginning that maybe you are interested in controlling these large-scale problems or solving inverse optimal control problems. Optimal control problems, but in principle, you would have this controlled Minfield equation for which you can train a neural network to learn the collision operator, or you can train the option of that. And these are naturally high-dimensional models. And yeah, artificial neural networks will give you certain accuracy. Machine learning in general would allow you to scale these to high-dimensional problems in a more or less efficient way. Okay, so these are some references in case you are interested, but yeah, otherwise you can just ask me. It but yeah, otherwise, you can just ask me and happy to explain. Thanks. Thanks for asking. Very short, specific question: What kind of numerics did you use for the mean field option control? This one. For this one, for this one, you use a semi-agrangian scheme, and it's just a forward-backward iteration of that with a bit of damping. So it's like a reduced gradient for the optimization. So we don't solve this simultaneously. We really go of the control, a semi-Lagrangian skin here to the forward simulation, semi-Lagrangian back to this adjoint simulation, and using the optimality condition to compute the reduced gradient of the control. No, no, this is just for two dimensions. This is in this paper. That's the thing. We haven't done the solution of the high-dimensional mean field control problem. We just have done the simulation of the Boltzmann forward Boltzmann equation with a control term here. And what I'm telling you is that from in this paper, we did compare this. And actually, we use this sort of kinetic framework to initialize the solver here. And it works very well. So this will give you a feedback that is a very good. You have a feedback that is a very good initialization of the optimal control that you will obtain here. But we don't have any guarantee because this is really, this is one problem. This other problem is something different. No, it's not. And then more general comments, because you say, why wouldn't we invest this? I think we had in the beginning this figure the hard talk models where you have like a classical control theory and then at the other end the machine learning, purely data development. Machine learning or purely data development, you are somehow in the middle. And I think that another qualification between this is that this is amenable maybe to some theory, you know. I mean, for some things you can prove it. So this is a great, great thing. Yeah, no, no, this is extremely... No, for instance, here, this is very actually. I mean, and it's not our idea, but this goes back to admit interpolation, learning a function based on v and gradient of v, and there are very nice and recent approximation theory papers that you have better. Vapors that you have best approximation type results if you pick a certain basis and you cast this in the proper optimization framework. Yeah, it is machine learning, then I think there's a big plus for all those. Yes, I like to think so. Thanks. During this talk, so I also have a question regarding this part. So, when you mentioned, so do you actually assume that this V data kind of automate exact fast, I mean fast representation influenced theory, or it works generally well for, you know, because you use polynomial like for smooth functions. So, what is the question that I try to do? So we will assume that it has exact sparse representation for this kind of problem. No, there are two things. One is smoothness, the other sparsity. No, I'm not assuming any sparsity on the representation. I'm just creating these big bases and I'm letting the optimization just to find the this is what you will obtain when you do L1 type regression. So you don't. So you don't now smoothness. How could I ensure I choose with what? Sorry? You gave me a problem that with Dax and how could you choose the current application available? Okay. Yes, no, yeah, no, I think it's a very good question because I'm cheating a lot here because the original Hamilton Nicole equation is one. And of course, before. One and of course, before me doing these fireworks, there is a lot of big theory on viscosity solutions and what is the proper framework to understand this. You will have control concerns and the natural value. Probably there are more papers on viscosity solutions than classical solutions of the Hamilton Jago equation because it's not a natural framework for this. So I'm always working under the assumption, for instance, that things are very smooth, that then from this, the value of the variable. That then, from this, the value function inherits some smoothness that makes this amenable to polynomial approximation. Okay, but this is not the, there are things that I cannot, I can do and others that I cannot do. For instance, here, if you do a proper selection, instead of taking a U square, you take, for instance, an arc tangent and you can play around. If you want constraints on U, you can impose it through penalties. And this is true. I mean, I really mean it. I mean, I really mean it. There are some penalties that will allow you to enforce a box constraint. What will happen is, as the constraint becomes more stringent, somehow the value functions start losing smoothness. How do you compensate for that? Well, with a bit of lag and stability, you just increase your polynomial degree of approximation. One beautiful thing that I know here, although, is that, for instance, in a linear quadratic case, dynamics are linear, cost is quadratic. We know from control theory. We know from control theory that the solution of this is an algebraic equation. So, actually, the value function, what will tell you at the control theory book is that the value function itself, v of x, is x transpose pi x. So you will have exact recovery if you take polynomials of degree two, if you take monomials of degree two. And then there are other control theory vapors that say, okay, if the dynamics are non-linear, but you can still make a PowerShell expansion, you can just put high-order terms here. Just put high-order terms here, so this makes sense. Here, yeah, yeah. No, I didn't play with the lambda. What I used to play with was this. This W here. This is a weighted L1 norm. And that's the thing. It goes back to Verton Common. There are some approximation theory papers that will tell you exactly how to control these weights. And these weights depend on the bounds that you can establish on the basis functions that you select here. Yeah. Yeah. Yeah. I don't know if this thing like uh basically like opinion what is the classic. Patients have overcome the curse of packaging. Yeah, no, I think that dimension-wise, I do have a limitation here with the polynomials because still the cardinality will increase. Yes, no, I don't think so, to be honest, because the thing, the sparsity you will get after you do the calculation. You don't assume sparsity, you do your calculation, and then you say, I was lucky this object was well represented with the sparse representation, because you get a lot of zero. Maybe you can use this in an incremental way, say, okay, this is what they learn with this. Yeah, probably is related to other things of machine learning on how now you have your model, now you have spare room. Now you have spare room because you have a lot of sparse, a lot of sparsity, and then you can use this to afford bringing more elements to your basis. And maybe you can do this in a progressive way, but I haven't tried this. But yeah, but the neural network will give you cases curves of dimensionality-free architectures. Yeah, that's probably if you really at some point related to high-dimensional problems, you have on this. You on this polynomial framework, I'm afraid, yeah. Yeah, okay, okay. No, no, and we may um slogan.