A year ago, I think now, where we're going to take a more sort of large-scale network approach to synchronizability. And we're going to look at connectomes, both toy models and actual connectomes from data, and determine actually a metric, which is going to be the second largest eigenvalue of all things, that will be an indicator for synchronizability. So, synchronizability, what is it? Well, there's a mathematical It? Well, there's a mathematical definition, but you can loosely think of it as essentially the ability for a system to synchronize onto one solution. And you can collect a bunch of imaging methods, record, estimate structural connectomics, and the idea behind it is: can you use structural connectivity to actually assess a brain network's ability to synchronize? Again, this is going to be synchronizability. Now, parts of this talk are going to be. Now, parts of this talk are going to be a bit more mathematical because I figured I didn't actually expect Majeed and Arthur to be here, so that's a pleasant surprise. I forgive you for your sins. It's okay. And does this idea of synchronizability translate at all to the prevalence of EpolfSk? So what is it mathematically? Mathematically, it's the stability, really, of synchronized solutions. Really, of synchronized solutions. If you have a system with some connectivity, is the synchronized solution stable? And what are the kind of indicators of the underlying connectivity that promotes stability of the synchronized solution and that inhibits the synchronized solution? And for the mathematically literate, it's local asymptotic schedule. So, for certain types of coupling profiles, this actually ends up being somewhat mathematically tractable. And you can use what's called a mask. And you can use what's called the master stability function. This was originally developed by Grant Carroll back in the 90s. And what Sue Ann and I did is we extended it to a couple of alternate scenarios that are more realistic for real connect domes, and we actually applied it to connect domes data. So I'll describe qualitatively how this works, where essentially you generate an image, you plot your eigenvalues from your connectome on that image, and if any of them lie in this one region, then your synchronous solution loses them. Your synchronous solution loses the point. And from there, you can easily derive a metric, which is going to be the second largest eigenvector you get, as it turns out. My talk won't be so long, so I won't interfere too much with your lunch. Thank you. Yeah, you're welcome. I'm considerate. So, you know, essentially, with the master stability function approach, you need to know the eigenvalues of connection matrix. You need to know something about the nodes as well, the node dynamics, but we As well, the node dynamics, but we introduced a kind of general model, a Wilson-Cowan model, to model the dynamics of nodes, which we'll introduce over the next sort of slides. But if you know the eigenvalues, you can figure out fairly quickly whether or not your solution is, your synchronous solution to the system is stable. And the dynamical nature of the synchronized solution ends up being irrelevant. It can be chaotic, oscillatory, doesn't really matter. So it's a very general approach to actually assessing. Approach to actually assess whether or not a trajectory is stable and if an entire network will synchronize on that trajectory. Alright, so again, mathematically, so what we need is with a master stability function approach is the way it works is the signs of the Lyapunov exponents, so they determine the local stability of a trajectory, they are actually directly determined from the eigenvalues. Determined from the eigenvalues of a connectivity matrix, which, you know, for nodes and equilibria, for dynamical systems, that's common. For trajectories, that's totally uncommon. This happens because when a system synchronizes on the one solution, you introduce a lot of symmetry into the system at that point, and then the math just works out, it turns out. I know that's not a nice explanation, but it's probably the best one for the multifaceted nature of this audience. Multifaceted nature of this audience. This is true for what are called diffusively coupled weight matrices, and this was in the original formulation by Berker and Curl. And these are weight matrices where the row sum is zero. Now, that means your nodes can excite or inhibit each other, so it's not so linked to structural connectivity, and it's very unclear whether or not a real connector would be diffusively coupled. This is one of the cases where, oh, I need this to make the math work out, so let's just assume this is true. Out, so let's just assume this is true in general. So, what we found is an alternate condition you can use to replace this that might be more biologically realistic. If any eigenvalue leads to a positively eponymous exponent, then the synchronized solution is not stable. All right, and again, this is something that Sue Ann and I looked at. We just published it last year, in the middle of the pandemic, in Siam Dynamical Systems. We published a follow-up paper where we looked at We published a follow-up paper where we looked at systems with delay differential equations and delay coupling as well. It turns out you can extend this even when there's kind of a communication time lag between nodes. All right. So here's how we model one node. So you can think of this as like one little patch of brain. We've got an excitatory population, an inhibitory population, and a homeostatic weight. And this is what's called the Wilson-Caman system, and we've got ENIPA. So the way the weights work. So, the way the weights work are: again, so we've got these excitatory nubutory populations. The excitatory nodes connect basically globally. So, one brain region can excite another brain region over long distances. Inhibition, on the other hand, is local. So, a node only inhibits itself, and there's also a homeostatic weight which is trying to control the level of excitation in this thing to actually prevent it from doing these runaway effects. Prevent it from doing these runaway effects. And this comes from kind of a Wilson-Cowan model, it's a bit old, but the homeostatic term is something that actually my other old supervisor, Claudia Bell, helped develop. So we've got this local inhibitory term, and it's also homeostatically modified. So that's just basically the node dynamics. It's a three-dimensional dynamical system, standard Wilson-Callan stuff. It's again, it's very handy to model kind of a population of neurons. It's one of the classical. Population of neurons. It's one of the classical approaches for modeling populations. Actually, Wilson and Cam derived kind of these original equations in the 70s. So it's fairly. Well, again, local inhibition is homeostatic. So every excitatory node tries to reach the set point, P, just to maintain kind of the level of excitation in the circuit to prevent things from blowing up. So what's the condition that we looked at that was not diffusive? Was not diffusive? Well, we kind of thought about how a neural circuit would work, and instead of diffusive coupling, it's basically the sum of the weights going to any particular nodes. It's a constant. So you can think of a node as, all right, I'm going to receive so much excitation. There's only so many connections that can be made onto me. And we can assume that a lot of them are fairly similar. And we essentially just get a constant row sum for the connection matrix, which means that this is also a large spiking value, which means Which means we can actually use this in the master stability function approach. So the way it works is I'm going to generate an image, and I can plot my eigenvalues on that image. And I can scale out this WE so that all the eigenvalues are in between negative 1, negative 1, real and imaginary. And turns out there's going to be a zone where if an eigenvalue lands in the zone, the synchronous solution loses story. loses stable. And if an eigenvalue is outside of the zone, synchronous solution, if all the eigenvalues are outside of the zone, the synchronous solution is locally asymptotically stable. Now one eigenvalue always ends up being there. That's the dynamically neutral one when you're basically the trajectory that runs the eigenvalue that runs parallel to the trajectory, essentially, if you want to think about it that way. So there's always going to be one there, but that one is neutral. You can ignore it. It just comes from the fact that we have one simple Just comes from the fact that we have one synthodized solution. If another one comes in here, though, so this is why it's the second largest I can LED, that's when the synchronous system loses stability. All right, and you can figure out what the synchronized solution is going to be. It just ends up being essentially like one Wilson-Howen node coupled to itself with this constant excitation term. So, this is actually going to be what the system synchronizes to. And because it's a 3D system, you could actually. And because it's a 3D system, you could have synchronized chaos, synchronized limit cycles, canards, a bunch of other conditions. So here's a simple simulation just to show that it goes to the self-coupled node. So we've got a random, Erdogani randomly coupled network and a single self-coupled node. And they both go to essentially what's going to be the same chaotic attractor. And in this case, you've got n nodes randomly coupled, synchronized onto the same attractor, right? On the same tracker, right? It's just one low-dimensional chaotic system. By the way, there was some studies done, I think, in the 80s and 90s by Elaine Destech, where he actually mapped out the dynamics of REM, non-REM, sleep, and he computed the Lyapunov exponents numerically, where REM is essentially very noise-like, you know, many positive Lyapunov exponents. Slow REM is something more like the slow dimensional. Sorry, non-REM, slow wave. Alright, so we ran a Alright, so we ran a bunch of simulations as one does to verify that an analytical technique works. We had the self-coupled nodes, synchronized, reciprocally coupled nodes, synchronized, rings, small world, random, lattices, larger lattices. Finally, we start seeing desynchronized solutions. So we've lost stability and all the nodes go to a bunch of different stuff. That's what you're seeing with the colors. It's like some crazy shit is happening there. And the larger ring. And the larger rings, and weak coupling. Weak coupling is kind of the obvious one, right? Where every node is acting somewhat independently. So we can break the synchronization, but we need some condition on the eigenvalues. What does that condition end up being? Well, here's what the master stability function actually looks like. It is a pain to compute numerically because you're essentially computing a, you're computing what is a single Lyapunov exponent over. Over a grid, and then plotting the eigenvalues on this grid, and that'll determine actually what the sign of the eponymous exponent is. So each grid point is actually quite hard, but you get some nice-looking images. And so we have a zone of stability here, distinguished by this green line. So if the eigenvalues are here, we're stable, and if the eigenvalues are past this green line, it's unstable, right? As we increase the excitatory coupling, this image changes, and we end up with essentially. And we end up with essentially like stability everywhere. So that's the master stability function for three different values, the discrete values of the excitatory coupling constants. And what we can do is start looking at those individual solutions that we generated. So we've got rings for larger rings. You can see, so the eigenvalues are in blue, and you can see it does a cross at like n equals 8, n equals 9, n equals 10. So at n equals 8, all the eigenvalues, except Equals 8, all the eigenvalues except for the largest, which is the neutral one, are on the stable side of the master stability function. Synchronous solution. Then we get a crossing onto the unstable side. What happens is kind of cool, because this directly relates to Jorn's talk. So they're synchronized for a bit, and then they lose stability, and what happens is they get a little traveling wave, essentially, of activity. And so instability again, synchronized for a period of time, and then Time and then they're never like fully synchronized. That's what lets this happen, right? They go very close to the synchronous solution, but it's got an unstable eigenspace. And that eventually they'll get kicked out and they'll do this sort of transient asynchronous activity. Yeah. Checked it for the lattice as well, where again we can kind of cross into the unsynchronized solution, you just have to figure out. Into the unsynchronized solution, you just have to figure out how big of a lattice you need, how many root, what root n has to be. At 16, we cross the master stability function, and we end up with a single positive sparse. Actually, yeah, well, it's going to be more than one. So basically, with these toy models, this approach works. So we can directly apply it to the connectome data. Now, for the connectome data, Connect dome data. Now, for the connectome data, we can already guess what we need here. And you can analytically prove the second largest eigenvalue is going to be the critically determined factor. So, we don't necessarily have the node dynamics correct. We don't necessarily have an idea about how to scale up the connectivity strength, but we know that the second largest eigenvalue is in general bad if it's large for synchronizability. The second largest eigenvalue is, oh, sorry, yeah, yeah. Is, oh, sorry, yeah, yeah. If it's large, you destabilize the synchronized solution. Large and positive real part. The magnitude. It depends on the master stability function because it has to cross the line, right? So if the second large steigen value is small, then synchronizability is large, and that could be bad for uploads. Sorry, the reason why I'm mixing these things up is because someone else has another synchronizability. Someone else has another synchronizability metric where they flip the way it goes. So if it's small, it's bad for synchronize, but like, we'll get into that. So essentially, it's just, it's where it crosses. And from the simulations we have, we generally have some zone on this side. So it has to be large to have a chance of crossing the side. Exactly. Yeah. But it could be large and have a negative real part of the. Yeah, it's the positive real part. Yeah, it it's the positive real part has to actually cross into this, essentially. Yeah. So again, the other reason is with connectomes, all the eigenvalues are typically real because it's a very symmetric matrix. So we can just generalize and say it's the largest. All right. So we used a large age-range connectome data set. So this was a subject of 196 people with 180. People with 188 nodes, which correspond to a bunch of different brain regions, and they were ages from 6 to 85. And we actually took a look at this. We computed the second largest eigenvalue after normalizing the row sum so that they all had the same constant row sum of 1. And then we computed the second largest eigenvalue and we checked, okay, how is it going to change with age? And the reason being is other authors have actually looked at synchronizability in connectomes, but they've assumed diffusive coupling and. But they've assumed diffusive coupling and have developed actually alternate metrics for stability. So these eigenvalues are actually global properties. They're not easily related to the graph statistics, at least not for an arbitrary graph. You just need a matrix, compute its eigenvalue. So we can't kind of infer necessarily that much in terms of the relation to any type of graph, just some type of graph. So just compute the eigenvalues essentially is what that means. So an alternate candidate to measure synchronized. Alternate candidate to measure synchronizability that was proposed by Danielle Bassett's group is the inverse spread of the eigenvalues. And so you take the eigenvalues, you compute the mean, you compute this guy. I think that normalizes one of the terms essentially that they need. And then you compute basically this ferro sum. They end up being not equivalent metrics. We checked, we came up with some trivial analytical cases where our second largest eigenvalue metric was different from the eigenvalue spread. Spread. So, what they did was they checked it with essentially a data set of children to young adults. This is again from that Tang paper from Dean Le Bassett's group. And so this is controllability. We can kind of ignore that. That's another metric you can check for a graph or for a connectome. For synchronizability, they start off with a high synchronizability index that goes down with H. So from age 8 to age 22, they become more synchronizability. 22, they become more synchronizable. Alright, so basically, it decreases, sorry, they become less synchronizable from age 8 to 22. This is again because my metric actually goes down with increasing synchronizability. So synchronizability decreases with age until young adult. So you start off being more synchronizable, and as you go into young adulthood, you become less. Is there a link to epilepsy? Is there a link to epilepsy? I'll show you after I discuss my metric and show how these two results might come together when dealing with actual people. So with us, we took kind of this connectome data. We've got the DTI-derived structural connectome. We L1 normalize it, and we can compute our synchrony metric. And so we can age sort all of the subjects. You can see a nice kind of linear correlation for the second largest eigenvalue. So we actually have an increase of synchronizability. An increase of synchronizability with age that's fairly significant. And we actually operate, we're about 33% stronger as a synchronizability metric than the classical metric of diffusive coupling. So basically for our result, and even with the old metric, we have an increase in synchronizability with age, but going into old age. So from young age to young adulthood, Age to like young adulthood, you are decreasing synchronizability, and then here with this larger data set, you increase in synchronizability as you enter into old age. So, we think the link to epilepsy is actually kind of this U-shaped prevalence of epilepsy, where you're likely to have seizures at a young age and then likely again at an older age. And we think these two results are related. Our synchronizability metric ends up being a little bit better. Ends up being a little bit better in terms of actually having a stronger effect. But you end up with basically a again, sorry about this because our metrics are going in different ways. So you end up with decreasing synchronizability until young adulthood. And then you start increasing synchronizability again with age. Again, metrics are going in different directions just because of how it works. So if you fitted a straight You fitted a straight line to your question. But if you did some sort of curved regression, you might not be able to. Yeah, we would end up with a better result. You might end up with it being a little flatter than you can answer. Yeah, exactly. And you can see with theirs, they flatten out. So we didn't end up actually using a curve to fit it. Just eyeballing that would be plausible. It would be plausible, yeah. But you can use a general linear model to fit a curve. A general linear model within a curve, and then yeah, you would likely end up with that, and you would likely end up with this U-shaped prevalence in epilepsy. So, essentially, what we do need, though, to test this is we need data in non-healthy patients to link these synchronizability metrics to epilepsy and then like a healthy group control. Because otherwise, this is just all correlational, right? Like, these subjects were just normal people who had their white matter tracks measured. People who had their white matter tracks measured. Alright, so DTI-derived connectonomes, which we've gotten from a couple of data sets, they may yield some information about the brain's readiness to synchronize via these eigenvalue metrics. It's possible that the decrease in synchronizability in young age and the increase in old, so that U-shaped curve, does reflect these underlying changes to white matter trends. But again, this is all sort of correlational. Sort of correlational. We don't actually know if this is the case. You can compute different metrics that use the eigenvalues rather than use any kind of quantity about the graph. And we actually need connectome data, I think, from epilepsy patients and healthy controls, to see if these eigenvalue metrics are actually predictive or useful. Because otherwise, we're just essentially looking at healthy subjects and trying to infer and correlate this U-shaped curve to this other U-shaped curve. This U-shaped curve to this other U-shaped curve that we're getting, which is not necessarily what we want to do. All right, so with that, I have finished on time so that you guys can have a nice lunch, even for those of us who haven't paid for it yet. I have a half an hour worth of questions. All right, thanks a lot. So, can we go backwards?