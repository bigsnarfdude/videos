Yeah. That's good, because I think that would be distracting. Yes. All right. Welcome everyone. I hope you had a really great lunch and a little bit of a break enjoying these beautiful views. So our next speaker is Dr. Sarah Lotsbeach. She is an assistant professor in the Department of Statistical Sciences at Wake Forest University. Previously, she completed her PhD in the Department of Biostatistics at Vanderbilt University and a postdoctoral fellowship in the Department of Biostatistics at Elms. For an abiostatistic at UNC Chapel Hill. Her research is motivated by challenges in designing and analyzing studies with error-prone observational data, particularly in international HIV cohorts and the EHR, and in statistical modeling from data with such a covariance with applications to Huntington's program. A fresh academic, Dr. Laspeach is the 2023 recipient of the David P. Buyer Early Career Award from ASA Biometric Section. Please welcome our speaker. Please welcome our speakers. Thank you. So, thank you for letting me talk to you today. Today, I changed my title. This one's just a little more specific than what's on the program. And I know it has extrapolation in it, and extrapolation is kind of a dirty word in statistics. I teach Introstat just last week. I was talking about how extrapolation is bad. And I'm not here to say that it's not, but sometimes it can be useful. And this is one of those times where, at least in our case, extrapolation is kind of helpful. Is kind of helpful. And so I'm going to talk today about how extrapolation before imputation can help us reduce bias when we're imputing heavily censored covariates. And so if you'd prefer to follow along on the slides in front of you, this QR code will take you to my website. You can click right there and download your slides from Dropbox, just so that everybody has that option. So the motivation for this work comes from Huntington's disease. And if you're not familiar, it's a fairly rare estimate. I read a rough estimate of about 9 in 100,000 people. And 100,000 people are inflicted with Huntington's disease in North America, and it is so named for this handsome fellow, George Huntington. He did not have the disease, he is made popular for basically giving it a name and studying it more deeply. So George Huntington was like a third degree Long Island physician, and in his community, there were these whole families of people who were inflicted with Huntington's disease. But at the time, people didn't know what that was. That was just, oh, that's that family that has involuntary muscle movements, and it looks like they like they're losing. Like they're losing a little bit of control. And so he helped to really formalize that idea. And so, to put it a little more specifically, Huntington's disease leads to impairment in a couple different areas of life. It leads to motor impairments, like he talks about kind of involuntary spasms and things, cognitive impairment, which is similar to dementia, and also psychiatric impairments. And so it is genetically linked, and it is genetically linked in a way that is said to be fully penetrant, which means that people with at least 36 mutations on this particular Huntington Mutations on this particular Huntington gene are sadly guaranteed to develop their disease in their lifetime unless something else happens. And so, while this is sad for the patient, it is kind of unique and interesting for us as data people, because since we have this genetic link, we can use genetic testing to assemble ongoing cohorts of people who are at risk but not yet diagnosed. And we can use these prospective cohorts to get kind of a fuller picture of the trajectory of this disease. And so, while it's been, I mean, at least since 1870. While it's been, I mean, at least since 1872 when this picture was made in George Huntington's, we've known about the disease, there haven't yet been any successful treatments. The goal of which is thus far been to slow the progression of symptoms because the symptoms are said to be kind of irreversible. So you're trying to just slow it down as best you can. So similar to any other new treatment, clinical trials are going to be a critical part of proposing new treatments. But these are, of course, expensive, which leads to limitations in the numbers of people we can. And the numbers of people we can recruit, for example. So, the statistical piece of this that I'll be focusing on is if the typical trajectory of Huntington's disease looks something like this. You have symptoms that are maybe not that bad and relatively flat for a period of time. You then suddenly get much worse, at which point your doctor would make a clinical diagnosis of Huntington's disease, and then you might actually level off again at a heightened level of impairment. And so, if I have maybe funds for a two-year clinical trial, where do I want to recruit? Where do I want to recruit people along this trajectory to see if I can slow progression? Anyone? Yes, we hear. Right, in the middle. I want to see if I'm slowing progression that's actually progressing. It's really hard to see if my treatment is working if I'm comparing a flat line to a minusculely flatter line. And so the statistical piece of this is I'm trying to basically model the trajectory of the disease as a function of time to clinical diagnosis, but not everyone has yet. Diagnosis, but not everyone has yet been diagnosed, so that's our sense of covariate. This is because I was supposed to say it's kind of a Goldilocks problem. So we don't want to recruit people too long before diagnosis or too long after. We want people here in the middle at that steep change. And so my objective to reiterate is modeling how Huntington's disease symptoms progress as a function of clinical diagnosis, or the time to clinical diagnosis, which is a censored provariant. And now I will put a minimal amount of notation on it, which will be easy going forward. Which will be easy going forward. I have some outcome, Y, which is a continuous measure of the severity of Huntington's disease symptoms. I also have time to clinical diagnosis X, which is going to be censored. And I have some additional observed covariate C. This could be something like age, this could be the number of mutations that the person has, just traditional other covariates. And so now X is, to be particular, randomly right-censored. Anyone who has not yet been diagnosed in my study is being right-censored by their disease-free follow-up. By their disease-free follow-up time. So that means in these prospective cohorts, you have a mix of diagnosed and undiagnosed people. And so instead of observing X, like we wish, we observe a placeholder W, which is either your censoring or your disease diagnosis time, and we observe an event indicator. So the methods. I came into my postdoc with Tanya, having spent years on a dissertation on measurement error and missing data. And so as I started to read about all the methods for censored covariates, I was kind of For censored covariates, I was kind of excited to see infutation because it's a very common approach to the problems I was already used to. It felt like an approachable way to kind of dip my toe in. And so the thing is, that's kind of interesting, and actually Roland said this earlier, sensored data aren't missing data. You have this unique kind of partial information that's available. And so imputation specifically for censored covariates wants to take advantage of that information. And so if I, I guess a common choice would be to replace a censored X with a conditional me. Replace a sensored X with a conditional mean. And usually you're conditioning on some other pre-observed information. And so I have a couple of examples here of common options to impute this bright sensor covariate. And I've ordered them one to four from the least to the most information being incorporated. So this top option, I would argue, is treating sensor data as missing. I'm ignoring the sensor value altogether and just saying, what is your clinical diagnosis as a function of maybe your age and the number of mutations that you have? I'm ignoring the fact that you've been disease-free. I'm ignoring the fact that you've been disease-free for however many years. The second option to me looks like how I would impute an error-prone covariant. I'm imputing your disease time from your censoring time and from additional covariates. This has more information than the first option, but it's missing the inequality piece that is critical to censoring. We don't just know that, like, we know your diagnosis time given that you were disease-free for 10 years. Your being disease-free for 10 years tells me that your value of x that I impute needs. Value of x that I impute needs to be at least as big as the 10 years we've already observed you. So that's why the third and fourth options, with this condition on my censored or my imputed value being bigger than their censored value, are to me kind of the most natural choices. And so options three and four really only differ in whether or not you include the outcome in the mutation model. This is sometimes warranted. I'm going to focus today on the third option, and I should note that both of these were proven to be. That both of these were proven to be consistent by someone in this room under proper specification of the imputation model in normal linear regression, which is my setting. So, in particular, I'm going to focus on imputing x from a district, assuming that it has to be bigger than the sensor value and some additional covariates. And I think probably in large part due to ease of implementation and maybe just approachability, conditional mean imputation, which is this is called, has been adopted for a lot of different types of sensoring and for a lot of different. Of different types of sensoring and for a lot of different types of outcome policies. So that's a couple of things. And so, in the case that I'm looking at this imputed value, under the assumption of non-informative censoring, which means I assume that conditioning on your covariates, the sensoring variable c and the true variable x, are independent of each other. This is the equation that you get for that conditional mean with which you're imputing. If you took survival recently, or if you do this a lot, you'll notice that that is equal to your sensor value. Notice that that is equal to your sensor value plus your mean residual life at that time. And so, again, I came into this looking for a new postdoc project. I thought that's a fairly nice approachable equation. And I thought, well, what are maybe a couple of ways I could add to this literature? What can I do here? And to me, there are two key pieces for me to potentially intervene. The first is I have to estimate that survival function. And right now, what's really commonly done is you use a semi-parametric Cox model, and then you go from the hazard to the survival function using Breslau's estimator. Function using Breslau's estimator. I thought, you know what? I'm not assuming a distribution, but I would love to not assume proportional hazards. Maybe I'll dabble in that. But what I found was that kind of no matter how well I was estimating the survival, I was having issues with this kind of persistent bias, which is when I realized that this integral is also something that has to get approximated. And so I was trying to maybe make changes in one, but in doing so, I realized that maybe changes were needed, at least in my setting, in option number two as well. So right now, Number two as well. So right now, this integral is being approximated with the trapezoidal rule. And I'm going to basically elaborate on both of these, but that's the anecdote. That's how we got here. So right now, the survival function is estimated. I could think of it in like three stages. I fit a Cox proportional Hazlitz model. I get the log hazard ratios. I can then use them to get Breslow's estimator of the baseline survival function. And then having gotten the baseline survival function, I have, putting them together with the log hazard ratios, a semi-parametric. A semi-parametric estimator of the survival. This is probably largely review, but for completeness, there it is. So now once you have this survival function, which might look something like this in the toy example, your next challenge becomes approximating that integral that we saw in the conditional mean. And so what's done right now is using the trapezoidal rule and basically defining your trapezoids, which in this case are rectangles, so I guess it's the rectangular rule, on the observed values. So in this example, I'm showing you the eight. Example: I'm showing you the eight observed values of my covariate, and in this case, they were all observed, and so I can use the trapezoidal rule to basically just add up the area under each little segment. And so, this will work pretty well. In fact, it's computation, very fast, very simple. And implemented in the existing software, if you really want it. And so, the thing is that this formula is approximating a slightly different integral than the one I needed. It's approximating the integral from the It's approximating the integral from the first to the last observed covariant value. And that can be fine when your survival function estimate actually does hit zero at the end. By the time you've hit the end, the observed w's that you have are actually doing a pretty good job of representing the x's that you wish you had. Unfortunately, that was not my lived experience. So if instead your data cut off and you have like 20% of people or something still undiagnosed at the end of the study, Undiagnosed at the end of the study. I now have that full black line, and then there's this whole red piece of it that I just don't even know about. I don't know the data. Breslow's estimator is defined as far as the observed data, and then things get a little iffy. So I could still use the trapezoidal rule, but I'm missing this piece. And so I'm then going to underestimate that integral, and in doing so, I'm going to miscalculate my conditional means and cause some issues downstream. And so the easiest way for me to point it out is that this is a much bigger issue when the largest observed values in your data. When the largest observed values in your data are censored, then we'll not censored. And so I thought, okay, I've gotten kind of to the source of it. It doesn't matter how well I estimate the survival. Maybe I need to tinker with this integral. And so the first thing that I really wanted to do was I wanted to be able to integrate not just to the max of the observed data, so the trapezoidal rule is meant for like proper integrals from constant to constant. I needed to do an improper integral. Unfortunately, there was already code in R that did this for me. So the integrate function comes in base R. So the integrate function comes in base R, which I love, doesn't rely on any additional packages. It's a nice, sustainable software choice. And in terms of you as a user using integrate, it's just as easy to use with an infinite upper bound as a finite one. But then the problem became, well, okay, now my integral can go to infinity, but Breslo's estimator doesn't, really, as defined. And so we have adaptive quadrature, which is our choice. It's just what's available in software. But now I needed a way that I could take. But now I needed a way that I could take Breslau's estimator from the max of my observed data reasonably to infinity, which is where the extrapolation comes in. And so this idea of having to extrapolate from a non-parametric, or I might call it like an unstructured survival estimate, is not unique to me. It is in fact a very common problem also in traditional survival analysis with a censored outcome. Basically, the data support the curve to this point, and then it's a big question mark. And so I was. Mark. And so I was able to kind of borrow methods from the traditional survival space that I wanted to see how well they might work for our purposes. So the four that I looked at were: first off, carry forward. I believe this was Gil's suggestion. You assume that the survival at the last time just carries forward indefinitely. It's equivalent to, in my case, assuming that people who weren't diagnosed at their last visit will never be diagnosed, which is counterintuitive. The next option at the other extreme is Efron's suggestion of immediate drop-off. I assume that everyone who Drop-off. I assume that everyone who was undiagnosed would have been diagnosed like the next time I saw them, or right after I finished seeing them. This is a little severe, and it also, in my view, really isn't extrapolating at all. You're just forcing the survival function down to zero so that you don't have to extrapolate. So I wanted to note that if I do this with adaptive quadrature, it's basically just equivalent to doing the existing approaches with the trapezoidal rule, which I'm going to call non-extrapolated conditional meaning condition. Conditional meaning condition. So then the more appealing methods for me are these kind of parametric extensions. Yeah. Does the carry forward extension act? No, it diverges. So I don't actually consider it. I put it up there for completeness and in case somebody was like, well, why didn't you use Gale Suggestion? If it's okay, I'll answer that in like two seconds. No, it's fine. Okay. So we have these two kind of extreme options and then these parametric options fall, at least in this example. Options fall, at least in this example, somewhere in the middle. So I can think that, all right, well, my data were supported up until this point, and then you basically like splice together a parametric exponential distribution at that point, and you carry out the rest of the curve. This was a suggestion by Brown, Colliner, and Hughes, I believe, in like an Air Force technical report a while ago. And then there's another parametric extension, which at least for me, I didn't feel like was discussed as much in my survival classes. This was a Mushberger and Klein paper, not the Klein Mushberger book. Not the kind of mushroomer book. It's the same idea. It's a Weibull distribution that you have constrained to tie in where Breslo's estimator leaves off. I like it because it's more flexible. What's interesting is that when it was proposed in the 80s, they were like, yeah, it works pretty well, but it's like not really worth the added computational strain. And at first, I was like, yeah. And then I realized that it's been almost 50 years. And that what was computationally intensive in the 80s is like not a problem anymore, which made this suddenly a very appealing option. And so getting back to Mary Claire's question. Getting back to Mary Clare's question, all of these could be a valid choice if I was just interested in extrapolating from the survival curve. But because I'm interesting in extrapolating and then integrating under it, that is why carry forward is in divergent integral as we go to infinity. So I can't really even look at that one. Immediate drop-off, you can do, but like I said, it's equivalent to really not extrapolating at all. And the exponential and the weibull both kind of were promising, and so we wanted to see how they worked in speed. And I will show in a minute, but the Weibull extension. In a minute, but the Weibull extension worked pretty well, and interestingly, it worked well whether or not the data were really weibal. Yes, is it an exponential distribution teas? Yes. Yeah. That's why I thought it was kind of interesting. I think the exponential one is for some reason very complicated. And the Y bowl is just like that, but more flexible because you can change the shape parameter in addition to the scale. So yeah, I like it because the Y bowl will always work for us, but then it'll put our vice versa. Yes? Yes. And both exponential and high level don't lead to problems where you can't integrate, right? Right. You can still integrate out of them. Good question. All right. So I have some, did you have a question to make? For the shape parameter, do you just try to like create a, it's like the optimal thing is to create like a kink in it? So to do the Weibull. That's actually interesting because I had to implement that based on how the paper was written in the 80s. So basically I set it up. I'd have to show you a little bit, but take I'd have to show you a little bit, but take the log likelihood of a libel, and then you have to constrain it because you know that the probability at this point has to be exactly equal to some fixed value. So I use that to set up a constraint and then rework the whole log likelihood to be in terms of just one of the parameters. Then you can do it that way. I'd be happy to show you. I don't have to slide about it, though. It's a good question. Yes. Could you just do the brick or fitting of the part that we have and then decide that this is the Decide that this is the parametric load that you're going to be using for the extension? Oh, that's an interesting choice. So, just like the whole thing, and then you continue it. We have data, and main is going to be and then we continue data. So, at least it seems that the pay will represent what was before. Right, it represents what's before rather than just representing length at that one point. That's a good suggestion. I haven't looked at it. I have looked at just a fully parametric approach where I don't have to do this. Approach where I don't have to do this, but that's an interesting choice. Cool. So I have some simulations. It's a fairly simple setting, but it does mimic the setting you see in real data. So I have a linear regression model with a continuous outcome y, and I have a censored x as well as an uncensored binary covariance. And so the way that they're generated, just for completeness, I have my binary z, it's balanced 50-50. I generate x in two different ways. They're both vibal. It's just that I wanted one version where x depends on z and one. One version where x depends on z, and one version where x is independent on z. Importantly, both of these settings were chosen because they do not violate proportional hazards. So then, once I have them, I use them to construct my outcome, and then I generate censored versions of the variables. And I did try my best to base these settings on existing papers because I was trying to kind of compare our ability to improve there. So, I have two sensoring settings. I have about 20% for light sensoring, I have about 80% for heavy. I think you could argue this is more like ethics. For heavy. I think you could argue this is more like extra heavy, but that felt more dramatic for the slot. So, most of my discussion, I'm going to focus on this parameter. It's the beta, it's the one that goes with the sensor covariate x. Arguably, it's the one that's going to be the most impacted by the sensoring in X moving. And the true value is 0.5. So, just a quick map to the simulation studies to reiterate some of this. I simulate, you could call it the true versions of all the variables. Then I have to decide: do I censor it or not? I have to decide: do I censor it or not? If I don't censor it, then that would be a full cohort analysis. I would have true Y, X, and Z on everyone, and I use that just like a gold standard. If I do censor X, I then have a choice. I can delete them, like Roland said, and I can do a complete case analysis. Sometimes I might be unbiased. Most of the times I'm going to be inefficient, so I'm not going to explore this option today. I'm going to instead explore, well, instead of deleting them, what am I trying to do for them? And I'm going to look at both ways of imputation. I'm going to look at the non- I'm going to look at the non-extrapolated conditional mean imputation, which is the existing approach, and I'll compare it to our extrapolated conditional mean imputation, which is what we're proposing. So the first thing I needed to decide for us is, well, okay, you've decided to extrapolate, but how are you going to extrapolate? And this is where I introduced four methods. I can't include carry forward because it makes the integral diverge, so I only included three here. The first in the light blue is the immediate drop-off. The second is the exponential extension. The third is the while. As a reminder, Third is the Weibull. As a reminder, X is truly a Weibull variable. So, a couple of things to note. To be honest, this is a light sensoring setting in a small and a large sample, left and right. They're all doing a pretty good job here. There's some little bit of residual bias in the small sample on the left, but it all seems to get better on the right. And we do see a pretty dramatic reduction in damage as you go left to right. The differences are much more pronounced when I go into the heavy. More pronounced when I go into the heavy sensoring setting. So, this is where, if I were to assume, like Efrem said, that everybody was diagnosed the second after I stopped looking, we start to see that bias appear. So, and it can persist it, whether you're in a small sample or a large sample. Over here, we're unfortunately very confident about a number that is kind of far from the truth. So, we do see that not extrapolating is going to cause a bit of a problem here. Then the second option is the exponential, like earlier special case, but it is technically misspecified here because my Misspecified here because my X was a Y. So it is offering improvement over the immediate drop-off, but it's not quite as close to the truth as the true Libel extension would be. So in this case, under heavy censoring, the Weibel kind of pulls ahead of the pack. It sees the lowest bias, and interestingly, it actually offered kind of lower empirical variability, it seemed like. I also want to take this opportunity to point out that this middle one, even though it's misspecified, is still. It's misspecified, is still offering some improvement over not trying to extrapolate at all. Yes? I might have missed this, but so both the exponential and y bowl extensions have some parameters. How are you? They're both chosen under the constraint that they have to tie in where Breslo's estimator leaves off. So, in the case of the exponential, it's like an analytical, it's really nice, it's very clean. Under the Weibull, it's more of like a coding problem. I had to just set up a weird lug like. Problem, I had to just set up a weird log like and put the thing gets estimated. Good questions. So, with this in mind, we adopted the Weibull for the remainder of our simulations. And so, this is where we can actually kind of compare the bias. So, the first table is under light sensoring, about 20%. And this is bias, absolute is in front, relative is in parentheses. And then relative efficiency for the two imputation approaches to the full cohort analysis with its standard error in parentheses. Analysis with its standard error in parentheses. And so there's a couple of things to note here. Both actually saw pretty low bias in the light censoring setting. In light censoring, my guess is probably this like uncensored max value is maybe less of a problem. So we actually do pretty comparably. We do see a bit of a bias emerge when we go into larger samples, which is kind of interesting. My guess is, again, that that has to be like something is getting cut off slightly differently when you now have more value. Off slightly differently when you now have more values to consider. And finally, an unexpected thing, but even though, so we kept our bias low, so extrapolating did help keep the bias low in the large sample setting. So we're still looking at about 3%. And it also offered kind of some efficiency gains, which is a nice surprise. So under light sensoring, nothing is all that alarming. Under heavy sensoring, unfortunately, is where we really start to see this profound problem. So if I choose not to extrapolate in this heavy sensor, So if I choose not to extrapolate in this heavy sensoring setting, in small samples I'm looking at like 200% bias. And in large samples, it does reduce, I mean, quite a lot, but it's still looking at more than 100% bias. So cutting off that tail is really costing me quite a lot in this heavy-sensoring setting. Now, unfortunately, it doesn't totally solve the problem, but extrapolating does actually reduce the amount of bias by quite a lot. So I don't love 44%, but I do love it a little more than I did 200. And so we do see. Did 200. And so we do see that in this case it is helping, and it is helping more in a larger sample. I didn't include it because it would have just been more tables. All of the tables I showed you were when x depended on z. In additional simulations where x doesn't depend on z, it's really very similar. So we still see like up to 149% bias if we don't extrapolate when we're estimating that beta for censored X. And if we do extrapolate, then I'm looking at no worse than 33% in the same setting. 33% in the same settings. Yes. So then I talked a lot about beta, but we also had two other parameters. I had an intercept alpha, and I had a parameter gamma that's on my additional covariate. And so what's kind of interesting here is when x was dependent on z, so which was the first setting I showed you, then that bias from the beta parameter for non-extrapolated conditional mean imputation actually kind of. Extrapolated conditional mean imputation actually kind of spills over into our estimates of the others. So we see up to 58% bias on Z, even though Z was uncensored, and we also see some amount of bias in the intercept. In all the same settings, extrapolated CMI remained really unbiased for these parameters. It was within 2%, but it actually recovered more efficiency. So it recovered more than half of the efficiency of the full cohort, which is pretty good. Interest, I guess it's not that surprising. I guess it's not that surprising, but if instead X is independent of Z, then this is not a problem. And then the bias that we see with not extrapolated conditional meaning position is really isolated to just the parameter on the sensored covariant. And the same is true for us. And so finally, I have some results in the real data. PredictHD is a prospective study of people genetically identified to be at risk for Huntington's disease. So they had at least a certain number of mutations on that particular Huntington gene. Specifically, I have a sample of about Specifically, I have a sample of about 1,100 subjects who have at least 30 centimeters mutations, so they are considered to be guaranteed to get the disease, but they were not yet diagnosed at study entry. This is a prospective study of the pre-manifest stage, but it's people who will develop it someday but don't have it yet. And finally, they needed to undergo all relevant testing at study entry. So there's testing in cognitive, motor, and functional testing. And so, as a reminder of kind of my objective here, we want to see. Here. We want to see successful treatments for Huntington's disease, and if we want to see successful treatments, we need to be able to test them. So, clinical trials are critical, but like we said, they're expensive. And so, that brings some real-world constraints. So, if I am looking at a fixed period of time and a fixed number of people I can have, then as the statistician, kind of the one thing I can maybe control is how we prioritize subjects for recruitment. And so, recruiting from existing studies, so if I take what we already know about all these people in predict HD and I use that to decide who And I use that to decide whom to recruit into my trial. That's an important first step. Definitely more so than if I just set up and I'm starting with everybody from scratch and I don't know that much about them. So specifically, my outcome model, if you'd like, is this model of Huntington's disease progression. I know it's a big equation, I'm going to break it down. The first thing I'm going to look at is a response. It's the symptom severity at the end of the period. This acronym, C-U-H-D-R-S, stands for the Composite Unified Huntington's Disease. Disease, Clinton's disease, rating scale, thank you. What's important in my mind, the composite piece, is because it pulls from scores in all areas of impairment. So it's not just cognitive, it's not just functional, it's not just motor, it's all of them together. And by design, the CUHDRS will decrease as the person approaches a diagnosis or as their disease is worsening. So my outcome is CUHDRS at the end of a period. I'm looking at how it's going to change from start to end. So then, of course, I'm going to So, then, of course, I'm going to also have to include their COHDRS at the start. This is not a big deal. We have this. But I also need to really include in the model the person's proximity to diagnosis or their time to clinical diagnosis. So, this is important because, like we saw in that little trajectory, people who are closer to diagnosis are expected to have their symptoms change more intensely and more quickly. And this is our censored covariate because this is a cohort of people who were not all diagnosed. People who were not all diagnosed. And then I'm going to control for two fully observed covariates: age at study entry and the number of gene mutations. I also have two interactions in the model, both of which I can explain. The first is interaction between time to diagnosis and CUHDRS at the start. This is to allow for that dynamic that I'm talking about. This is to allow for people who are nearer to diagnosis to have a different change in their symptoms relative to people who are farther away. And then I also have an interaction between age and the number of mutations. This is based on a fairly The number of mutations. This is based on a fairly common metric in the literature called the CAG age product where the ch is one. Yes? So you're taking the difference between the CMH and that in the starter frame. Is that to kind of remove the non-linear component that you saw in the graph earlier? Is that kind of what you're trying to do there? I wasn't thinking of it that way. I was, well, we definitely met. Yeah, we could have just done it as a function of time. Yeah, we could have just done it as a function of time. I don't know. I think I was thinking of it more as, like, what can I then make an interpretable end result of the analysis? And for me, saying these people had the biggest expected change was a little more straightforward. I don't know. This is just the way I thought of, oh, we could probably do that too. Although, consistency, I don't think, can be guaranteed. Good question. I don't have the answer. Oh, sorry. Is the component score a piece of the diagnostic criteria? A piece of a diagnostic criteria? This one? Yeah, do they use that score? Did it make a diagnosis? I think they use components of it. So the diagnosis is actually made by a clinician. There's this Huntington's disease rating scale, and they have basically different levels of confidence that a person's disease is at a certain stage. So it relates to like, I am 90 to 100% certain this person has entered this full-blown kind of stage of the disease. But I would imagine that all of the observations that lead The all the observations that lead to that decision, many of them should be present in the score. So, the available data include the outcome and the covariates, which are all fully observed on my 1100 people. And then the covariate timed to diagnosis is censored for 78%, which is why my simulation setting is so extreme. That's actually kind of what we're dealing with. And so, I also have the CUHDRS, so there's symptom severity from the first and the last visits. And I use the first and last visits as the two types. The first and last visits as the two time points to fit the model. So now the right sensored values: so anyone who was undiagnosed at their last follow-up visit is going to be replaced with their conditional mean in the two methods, two methods that were discussed, given that the diagnosis must be after their last disease-free follow-up time, and that model also includes their age and their number of mutations again. So we have some additional covariates in there. So now, what do I plan to do with this model? So my ultimate goal was to connect this model to some. Goal was to connect this model to some sort of recruitment strategy that might be useful. And so we're going to compare these disease progression models and how we can then kind of use them to maybe rank patients in order of, or in terms of priority by most changing symptoms expected to least change in symptoms expected. And so to refine the strategy a bit more, this is hypothetical, but suppose this is a real enough logistical constraint that I can recruit 200 at-risk subjects from Predict HD. Subjects from Predict HD. I will recruit them at their last study visit, and my trial is expected to last for two years. So I want the 200 people, in theory, whose symptoms are going to change the most dramatically in the next two years while I'm watching them in my trial. That's kind of where I was coming from it. And so the common strategies in Huntington's disease studies right now are stratified random sampling. So you take some kind of measure of like disease severity, so something like I said, that CAGH product, and you create strata of low or high-risk groups. Of low or high-risk groups. Maybe my hesitation with this is: once I've created these groups, I've lost any amount of granularity within the group. So the person with the highest CAP score in the entire study has the same priority for recruitment as somebody who barely counted as high risk. Maybe that's okay, but maybe we could look at it in a little more detail. And so we propose what we call a rank-based recruitment strategy. It's basically extreme tail sampling. I'm only in one tail. I'm only in one tail. Maybe I should say that. But it proceeds in two steps. So, they both are going to use my model. The first thing I'm going to do is I'm going to use the symptom models to predict the person's symptom changes during the trial. I'm going to start in Galesis and predict where you'll be in two years at the end and just take the difference. Once I have that, I'm going to rank the whole study and I'm going to take the 200 people with the largest predicted drops in CHDRS. And I have a couple of fun pictures for this. So consider. So, consider this one person we'll use as an example to talk about priority. This person's symptoms were already seen to increase as their COHDRS went from a 15.9 to a 13.3 between their first and last visits. So I'll call this an observed pre-trial symptom change of negative 2.6. So then I can start at that 13.3 and predict out where I think their symptoms might be in two years at the end of my trial. So I'll call that CUHDRS hat end, and then you use. TUHDRS hat end, and then you use that to get the predicted symptom change. I take the prediction minus where they would have started at trial recruitment. And so by design, this delta hat being less than zero indicates that their symptoms are going to worsen. And the larger, the farther that is from zero, the more severe. So we can also use this kind of priority to build little trajectories for people. And I'm going to compare them between the two models. So this person, like I said, was observed. So solid line, this is observed data. So, solid line, this is observed data to decrease from their first to last visit from 15.9 to 13.3. So, then using the model, I predict out that this person would have a CUHDRS of 10.8 at trial end, and that might lead me to say that they have predicted change of, what, 2.5. Then if we instead use the non-extrapolated CMI model, all it varies in is how I chose to do the imputed values, this is going to predict not quite as severe a change. In fact, one full score higher. Change. In fact, one full score higher. So this is just the different predicted value from the model imputed using the other hybrid. So then, if I'm actually trying to take this not just from describing their trajectory, but kind of operationalizing it to the whole trial, then I have their symptom change before the trial. I use it to get their predicted symptom change during the trial between the two models. I can then compute and compare this for everyone, and then I rank them. So in this particular case, So, in this particular case, the extrapolated CMI model would have ranked this person 43rd. 43rd out of 200 would have made them a pretty high priority for a trial. But the same person using the non-extrapolated model would have ranked 201st and would have just missed the mark and probably not been a high priority for a trial of 200 people. So, I applied this approach to the 732 people in Fred APHD who were still undiagnosed at their last visit. So, every point here in the plot is a person. The x-axis is Person. The x-axis is the rank based on the extrapolated CMI model. The y-axis is the rank based on the non-extrapolated CMI model. If they agreed, of course, everybody would follow along the line. And so once I have everybody's rank, then the non-extrapolated or the 200 lowest ranked people or highest ranked people for the first model. They would all be recruited. The low. The low red is recruited by the non-extrapolated CMI, and the purple area is people that they agreed upon. So, this specifically happened. We're 158 subjects, so 158 out of 732. We agreed on 22% of people to recruit. We also agreed on 490 people to not recruit, so all these people in this white area. And then all of these people in these kind of offshoots are people that we disagree on. That happened in 11%. So, 84 people. 11%. So, 84 people where one model or the other would have prioritized them differently. And in practice, this is an important consideration because accidentally giving a spot in a trial to someone who maybe didn't need it takes away a spot from somebody who could have benefited more or maybe been more informative to us. Just to wrap up, initial imputation is appealing. Perfect, last line. Initial mean imputation is appealing. Imputation in itself is, you know, ubiquitous. We've seen a lot with missing data. I like it personally because if you're working with a collaborator and you're Personally, because if you're working with a collaborator and you're already having to sell them on the fact that their data aren't missing, they're censored, it's different. But you can tell them that maybe we could handle it in a similar way. I think that could be really nice. So I'm a big fan, but I found that in my setting under heavy censoring, the existing kind of non-extrapolated approaches could be pretty biased. So what we've done so far is we found that we were able to reduce this bias by extrapolating from Breslau's estimator with a parametric extension, even when that parametric extension wasn't exactly quite right, and we could reduce the bias. Reduce the bias. What we're still working on are some more robust or efficient imputations. I'm now circling back to what I wanted to do by changing the survival function and seeing what we can do there. But I also think it kind of begs an interesting question of maybe how can we pick the best survival estimator for this case? If 80% of my data are censored and I'm then extrapolating an enormous amount from the survival curve, maybe it's not worth the trade-off of not assuming a distribution and picking something more structured. It might be useful here. If you'd like to learn any more about this work, it's available in a pre-profit. More about this work. It's available in a preprint. Simulation scripts are all on GitHub, and there's also an R package that imputes both our imputation procedure as well as the existing imputation procedures. It's called Impute Censored. I'd also like to acknowledge Mark Dosak, advisor and mentor at County Garcia for DhD, for allowing us to use their data. Of course, the funders and also the clusters without whom the simulations wouldn't have been possible. I have slides of references. I've highlighted a couple of names of people who I know are here with us in the room. I thought that was kind of cool. This room that that was kind of cool. So, the first paper is the Weibull extension. These are three existing inputation approaches, a couple of more, and then the proof about the consistency. And with that, I thank you for your time, and I welcome any questions if I can. Yes? So, did you do any kind of follow-up simulation to see how much power was reduced when you were selecting the wrong energy? I didn't look at power. That's interesting. I should look at that. I don't know. It's a good question. Ingrid? Have you thought about non-parametric extrapolation? Maybe I'm just not familiar enough with that. I came at this from like, how can I repurpose things that were already common in survival? But I would love to chat with you more about that. I just don't know about them, maybe. I can talk about it. Yeah, that'd be great. I can talk about it. Yeah, that'd be great. Thank you, Girl. Whenever we hear a question. I'm wondering whether you consider IBW IBL approach in a previous sneaker? Well, I would think if the model is correctly specified for the both of us, imputation should be more efficient, right? So I think that's maybe the main trade-off. So in a world where I could perfectly extract. So, in a world where I could perfectly extrapolate maybe, then I would expect this to have that efficiency gain. Whereas, that is generally how I think of the trade-off, at least. So, imputation is going to have all of the data deliberately included, versus IPW is relying on those weights to pull in the ancillary information about the uncensored people. That's my expectation. I haven't looked at it though. Great presentation. So I guess in the decision-making process of like identifying those individuals that would be better to select for like a clinical trial, I think the methodology here focuses more on the point estimate, but you're not really using any of the estimators. For example, if one of them works to be like 10 times more efficient, that's great, but you're still going to be having the same one. Have you thought about ways about how you could incorporate that efficiency? Incorporate the efficiency thing from the different solutions? Yeah, I am, I haven't. It is a good point because the simulations focus largely on inference, and then the data example is kind of more about prediction. But I like that it shows both pieces. I could imagine that if you put some sort of like interval around the prediction, or you could use the standard errors that you get from them to build some measure of uncertainty around that delta hat, CUHDRS, that maybe people would be more comfortable with. That maybe people would be more comfortable with. I think that's fair. I'm generally skeptical when presented with only an estimate and no measure of uncertainty. So you could see it there. And with the argument then being like, this person is expected to change here to here, and it should be a narrower interval with the extrapolated than the non-extrapolated. By how much? Depends. But that might be one way to do it. Have you done any real-world validation where, since predict HV is longitudinal, like the Predict HP is longitudinal, like built a model on one time period and then tested your predictions of the progression against the next time period's data? I didn't. That's a good idea, though. So like take, like I took people who run censored at the end of the last data poll, but pretend that I pulled data maybe halfway through and do the same procedure I'm doing, but actually has some ability to validate. I have not. Yeah, I think it'd be interesting to just kind of be like the real world kind of stuff. I have two questions, but I want to say first, like, of the presentation, especially the illustrations, I think you can encourage you to have us mister the different parts. When mine was dry, I saw I was thinking specifically. But I remember you wrote a paper where you said president integration can be problematic. So, are you changing your mind on that? Mind or not? So, this is the same paper. I think I missed the mark on what it was that I was doing that was actually helpful, if I'm being completely honest. So, it was not the trapezoidal rulepiece, because the trapezoidal, it's getting past when the data end, basically. So, if you were, I did try, like, if I extrapolate, you could still try to use the trapezoidal rule, but I don't really know where, like, it doesn't know where to end if you try to let it go to infinity. Where to end if you try to let it go to infinity? So I could see how it's still part of the problem, but the bigger part, I think, is extending the curve to just kind of integrate under it more completely. And I was also wondering, because you sort of build a new part of starting apps. Is there a way to start from the beginning, like probably the teacher that goes from the beginning or something or in the beginning? A variable model that follows what's already known so that it gives you its formulation of the end. Yeah. I think it's possible. I think that was kind of Lupe's suggestion too, and I think it's a good one. I don't, I would have to think more, because right now it's like a single constraint, and it's simple enough. So I would imagine you then need to constrain it to like a line that's kind of the top of the data if we're looking at the graph, right? Because you then want it to be: I have some step function. Have some step function that drops off here, then right now all I'm saying is, well, you have to agree there. So you're saying it have to agree there, and also I would think kind of have to agree up here. I don't know how to implement it off the top of my head, but I agree. It feels like it would do a more complete job because right now, this piece is only worried about the tail, and it's kind of ignoring everything that came before it. Yeah. I think that by doing that, Mr. Shield. I think that by doing that, it's assuming that the time follows the distribution from the beginning. Otherwise, if you put a different parametric assumption, it's assuming that something has happened at that time that has made this procedure to change. So I would be into having, it doesn't have to be web, although we would be very flexible. That could be normal. And it doesn't have to be that difficult because you just adjust. That's difficult because we just adjust this. What I don't know is whether the variability in the data, I mean, when you are doing this, you are not going to take into account the variability of the data that you have recruited so far. So if you adjust and you are going to sort of use the parameters alpha and beta that agree with what you have recruited so far, then this variability has to be taken into account. And this will make things a little more complicated. But I think it's very much Think it's really much just for the sake of time. Um, we have quite a long break, so you can definitely ask some more questions. Let's thank our speaker one more time. Close off the break, and then we'll come back. Oh no, thank you. Yeah, I I think I like misunderstood what one is until I already fixed it. And then I was pleased, it's not like nothing. I was thinking about this. I was thinking about teaching off the battery job, I would never find any other sort of. And I said, Oh, maybe something better. And I would say that's a good idea. Right, because there's like cool stuff. And so I just like but we won't see that. But we won't see that. I just can't remember which one. And I say, okay, maybe loud, but don't have to have a friend, and so is that. And sometimes I'm quite tricky. It's really episode. We sat down and we went line by line. Line by line. Yep, so there's four choices about that. Oh, I think it's so good. I mean, that's good. It does get like, that's how I feel. It's my next one. And then the ones that are in the middle of the middle is like first. I think it's a simulation part of the details. I'm just trying to show something. Something and I think one of the things that you want to do is like value violence. But so many different countries. I don't know if you're going to be able to make a margin, but I guess in my personal job. I think I was thinking it because right now I'm basically fitting an MLE to this. So all I have to do is fit it to this train. So it's still just transitions. 