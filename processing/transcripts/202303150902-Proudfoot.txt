So, Jacob gave a list of graded vector spaces that you can associate with a matriarch, and then polynomials, which are just the Planck array polynomials of these vector spaces. And they were sort of the running examples in his talk. And I really want to do the same thing. I'm also going to start with five graded vector spaces and five polynomials, and four of them will be the same as Jacob's. One of them will be different. So, um, so let me start with a different one. So, So let me start with a different one. So the thing that I want to call the Poincaré polynomial of a matrix, all right, pi sub f of t. So it's the one grade polynomial of the Early solvent. And this polynomial, you know, up to a change of coordinates, this is just a calculation. Change of coordinates, this is just a characteristic polynomial. So this is the characteristic polynomial of m, but you have to evaluate it on minus t inverse and then multiply by minus t to the ninth of m. So if some of you are more familiar with the characteristic polynomial, this is just the same thing, except you get rid of the signs and turn it around right about it. That's one of the polynomials I want to talk about. The others are all ones that. The others are all ones that Jacob said. So there's the child polynomial. I won't use the same notation he used. So I'm going to write underline h sub m of t, and it's the Paul Gray polynomial for the chow ring. We'll use the same notation we use for the chow ring, but not for its polynomial. There's an augmented version of this. It's the same thing without the underlines. Hmm, so HM of T is the quadratic polynomial for the augmented value. And the last two, by the way, I'm To, by the way, I'm going to use, give you the same warning that Jacob used. You don't really need to know these structures. So I'm going to put all these examples here. And if you know what they are, great. If you don't, you can sort of take it as a black box. I think that if any one of these five examples resonates with you, then you can follow the talk. So I want these writing. Examples. So this is called the Z polynomial. It's written Zm of t. And it's the Poincaré polynomial for this thing called the intersection cohomology of the matrix. In the realizable case, this is the intersection cohomology of the Schubert variety that Jacob defined. Otherwise, it's just something you can define abstractly. And the last one is the Kajan-Lucing polynomial. A listing polynomial. I'll just write KL polynomial. But I am going to write P. Jacob said he didn't want to write P. And that's closely related thing where we put a sub-empty set on the bottom. Yep. Did you say that the Z polynomial is also a Poincare polynomial? Yeah, so it is the Poincaré polynomial for the intersection cohomology. Polynomial for the intersection cohomology of a matrix. So all these things are Poincaré polynomials in the sense that I have not just a polynomial, I have a graded vector space, and the coefficients of the polynomial are the dimensions of the graded pieces of the vector spaces. So that's true in all five examples. And Jacob's talk was really about the polynomials themselves. That's great. I want to talk about the vector spaces. The vector spaces. So, in all of these cases, the vector space is something that's canonically associated with a matroid. And, in particular, if a matroid has symmetries, by symmetries of a matroid, I mean permutations of the ground set that preserve independence, that preserve the set of bases. If a matrix has symmetries, then those symmetries act on these graded vector spaces. So, if I have a matrix and I have some symmetry in the matrix, I can ask not just what this I can ask not just what this Poincare polynomial is, I can ask what the Ehrlich-Solomon algebra is as a graded representation of the symmetry group of the matrix. So it's like I have this polynomial, and as I've written it, the coefficients of the polynomial are natural numbers, but really you can think of the coefficients of that polynomial as representations of the symmetry group of the matrix. Where if you take dimensions, you get the natural numbers, but these representations carry more information. Carry more information. So, what this talk is going to be about, you know, I'm going to state some results and conjectures about the actual polynomials, the non-equivariant things, but then I also want to state some analogous results or more often conjectures about the equivariant setting, about the graded vector spaces themselves regarded as symmetry groups of the matrix. So, for the first half of the talk, I'm just going to talk about a bunch of conjectures just to sort of motivate and explain. Just to sort of motivate and explain what kind of questions I'm interested in. And then the second half of the talk, I'll talk about the notion of valuativity in the equivariant context, which maybe gives some tools for actually thinking about these polynomials and variant variant spaces. Any questions? Yeah? Okay, so let me start with the first one. So, the first non-equivariant theorem, an equivariant conjecture. So, this is maybe the most famous one, the sort of flagship theorem. So, the theorem is the theorem of Adipasito, Hein Ka. The names have been written enough that I feel like I can abbreviate them now. So, they proved that five demerits. Yeah, okay. They proved that this is logic. What that means, explicitly, they show that if you take the dimension of OSI of them squared, then that's greater than or equal to the dimension of OSI minus 1 times the dimension of OS I plus 1. Right, this is the coefficient of t to the i of the polynomial. This is the coefficient of t to the i minus 1, and this is the coefficient of t to the i plus 1. And this is the coefficient of t to the i plus 1. So, what's the equivariant analog of this conjecture? So, this is due to my former student, Katie Gedian, myself, and Ben Young. Well, it's just the same thing, except now we talk about tensor products of representations. So, OSI of M tensor squared should contain as a sub-representation OSI. subrepresentation Osi minus 1 tensor OSI plus 1. If you take dimensions you get exactly this statement. Or equivalently if you have the trivial group acting then you have exactly this statement. But if you have a non-trivial group of symmetries of the matrix then this is an interesting conjecture. And let me point out the category of representations of a finite group is semi-simple. So I could have said this includes as a sub-representation This includes as a sub-representation here, where I could have said there's a subjection from here to here, or I could have said every irreducible representation of my finite group appears at least as many times here as it does here. They're all sort of equivalent. They're all equivalent ways of saying that this guy on the left is bigger than this guy on the right. Yeah, I feel like you can say. Yeah, if you like, you could say I can decompose into pieces indexed by irreducible representations of my symmetry groups. And each one of those pieces, this thing should be bigger than this. So this is very far from known. I mean, even in the realizable case, it's not at all known. But we have some partial results that can maybe motivate it a little bit. So, proposition: the first part of this is due to the same authors, Gabian, myself. Authors, Debian, myself, and Dan Young, it holds for uniform matroids. So if M is the uniform matroid of rank D on n elements, then there's an action of Sn. Sn permutes the elements and it preserves the notion of independence. So these are some interesting representations of Sn. That's true. And more generally, And more generally, it sort of follows immediately that it's true for having HR. I won't go into too much detail about why that's true, but let me just say you can reduce from, you can think about the reduced Erling-Solomon algebra rather than the Erling-Soloman algebra, and the reduced Ehrlich-Salman algebra, and the reduced Orlich-Salman algebra of a paving matroid looks the same as for the uniform matroid, except that you make the last piece a little bit smaller. And making the last piece a little bit smaller can never break bottom compatibility. So that's one partial result. And second, suppose that m is the brain matriarch. So suppose I write m of kn. So this is the matriarch associated with the complete graph, kn. With the complete graph, Kn, or you can say the matrix associated with the type An oxider arrangement. So, this again has an action of the symmetric group Sn. And the Early Solomon algebra, in this case, is the tomology of the configuration space of n points in C. Of n distinct label points in C. That's what the Euler-Sullivan algebra is, and the action of Sn is just permuting the labels of these end points. Labels at these endpoints. Then, I guess the theorem of Mathern, Viata, myself, and Ramos is that the conjecture holds for i less than or equal to 7, like all n. So it doesn't matter how big. So it doesn't matter how big n is, you take OS7 tensor OS7, it is bigger than OS6 tensor OS8 as a representation of Sn. And I'll point out that this is infinitely many statements, right? So for each of the, we only checked it for finitely many i, but for infinitely many n. We do that using the theory of representation stability. So with represent what's that? In this context, yeah. In this context, yeah. So, using the theory of representation stability, you can show that if you want to check this conjecture for a particular i, then if you can check it for enough n, it'll be true for all n. And enough n means n less than or equal to 6i plus 2. So we checked it for n up to 44. So that gets to 6i plus 2 when i is 7. And that check took about a month running 24 hours a day on. Running 24 hours a day on a fast computer in Bond. So, yeah, that's that was uh that was some serious work. Not by us by the computer. On computer, one I think I was using one, yeah. They told me I was one of those. All right, so that's all I want to say about the first polynomial, the Mercury polynomial, but again, I'm trying. But again, I'm trying to give you a flavor of what questions I want to ask about these equivariant polynomials, these gradient vector spaces. Now let me move on to talking about the Chow polynomial and the augmented Chow moment. So let me start with a pair of conjectures that appeared in Jacob's talk. So the first one, this is due to Ferroni and Venjin Schroeder. So this is a question about that one. So are we always a bit wrong with representations developing this? Or does it matter? Does it depend on the particular? Right, I mean in general the symmetry group of a matroid will not be a symmetric group. So this is a particular class of matroids for which the symmetry groups are symmetric groups. There's one for every n, and the symmetry group of the n coordinates s n. And the symmetry group of the n coordinate is Sn, and you can prove that this Euler-Solomon algebra stabilizes as n increases. But no, this is something purposeful to fill it down. Yeah, yeah, so the application of representation stability is something specific about Freight Nature. Okay, so the conjecture, our point of the conjecture is that the child polynomial is real rooted. Factors over R until A or turns. The second conjecture is this thing about the augmented child polynomial. And I learned from Jacob yesterday that this is due to Matt Stevens. Again, these are non-equivariant conjectures, so I'll give the equivariant analogs in a second. And the third, this is due to George Maser and myself, it sort of generalizes these two conjectures. It says that. It says that the child polynomial interlaces the augmented child polynomial. So what that means is, remember, these are both supposed to be real-rooted. If m has ranked 5, then this polynomial has degree 4 and this polynomial has degree 5. So the degree of the augmented chow polynomial is the same as the rank. Degree of the chow polynomial is one smaller. And it's saying that the roots sort of look like this. That the roots sort of look like this. Here will be the five roots for the bigger polynomial, this one, and here will be the four roots for the smaller polynomial, this one, and they'll alternate that up. That's the conjecture. What's that? Is there a mutational or variable? This conjecture is based on experimental evidence. But let me say something about how this conjecture might be approached. In some sense, Might be approached. In some sense, it's easier to prove that two things are real-rooted and interlacing than it is to prove that any one individual thing is real-rooted. Let me say what I mean by that. So let me define a matrix BM. This is, if you know the theory of Besoucient, it's the Besoucient of this pair of polynomials. So what is it? It's the matrix. Matrix of coefficients of a certain two-variable polynomial. And that polynomial, you take hm of x, hm, h underline m of y minus hm of y, h underline m of x. So now we have a polynomial that's anti-symmetric in x and y. And then I divide by x minus y. And then I divide by x minus y. So now I have a polynomial that's symmetric in x and y. And I take its matrix of coefficients. So the ij entry of the matrix is the coefficient of x to the i, y to the j. I get a symmetric matrix, a symmetric real matrix, and that's called the Besouchian. And sort of a basic theorem is that 1, 2, and 3 together are equivalent to the statement that this matrix is positive definite. If you take two polynomials, one of them having degree one bigger than the other, and you build a matrix like that, that matrix will be positive definite if and only if both polynomials are real-rooted and the roots interlinks. And this is nice, because I mean, this is something you could imagine proving geometrically, right? I mean, I don't have a serious proposal for this, but you can imagine like saying, oh, this may. Like saying, oh, this matrix happens to be the matrix that's the Hodge-Riemann form on the primitive part of the homology of this variety. Hodge theory will occasionally tell you that things are positive delta. So this, I mean, again, I don't have a real approach, but this at least gives you an idea of how one might try to prove such a theory. So here's an example. M is Boolean of rank 3. It's already interesting in the Boolean case. Boolean case. Then, and this matrix is the following: 3, 6, 1, 6, 2, 2, 4, 1, 4, 1. If these numbers ring any bells to anyone associated with the Boolean matrix of Rec 3, if you have a guess as to where these numbers are coming from, please tell me. What's that? Eulerian numbers. Yeah, well, that's right. So in the Boolean case, Yeah, well, that's right. So, in the Boolean case, this is an Eulerian polynomial, and this is some version of that. Okay, so let me now formulate the equivariant version of this conjecture. So now let me consider the matrix. Of the matrix, I'll write it sort of not as the polynomials, but as the graded vector spaces. So V of CH of M underlines CH of M. So what do I mean by this? These are graded vector spaces, but I want you to think of them as polynomials where the coefficients of the polynomials are representations. And then I'm going to do this, and I'm going to get a matrix where the coefficients of the matrix. Where the coefficients of the matrix are virtual representations. There's some minus signs here, so I might not have an honest representation. I might have a virtual representation. So example, in the same example, I'll write it out. So I get, it turns out that all nine entries really are honest representations. Here you get a representation. Representation. Representations of the Boolean matrix, the symmetry group possessed three. Representations, irreducible representations, are indexed by partitions of three. So here you got V21 plus V3. Here you get two copies of V21 plus two copies of V3. Here you just got V3. You should be checking if anyone knows how to. Be checking if anyone knows how to compute the dimension of an irreducible representation of Sn, you should be checking that the dimensions of these guys match up with these numbers. Sorry, am I meant to think that the entries are somehow polynomials of the coefficients that you should? Yeah, that's right. So here the entries of the Bezou matrix are polynomials in the coefficients of the polynomials themselves. In fact, they're quadratic polynomials in the coefficients. And here I'm going to get the same quadratic. And here I'm going to get the same quadratic polynomials in the coefficients. Well, I guess we'll just make the representations simple. The coefficients, these things you should think of as polynomials whose coefficients are representations of the symmetry group of m. I mean, really what these are are graded vector spaces. So you should think of it as a polynomial where the coefficient of t to the i is the ith piece of this graded vector space. So should I think of this as a So should I think of this as a new definition of a position or am I new to finding from the original position? Right, this is a definition of the Bayesian of, yeah, sorry, I'm defining something here. So I'm thinking of a polynomial whose, I mean the Bazouian can be defined where the coefficients of the polynomial is in any ring you want. And I want to do it where the coefficients of the polynomial is in the ring of virtual representations of this. So I'll just, this is symmetric, so I won't. This is symmetric, so I won't bother to write down the lower half. But two copies of V111, seven copies of V21, and six copies of the trivial representation, V3. This is Z21 plus three copies of the trivial representation. So I had two copies, and this is just the trivial representation. So, this is this, this is this, and this is this. So, here's the Bezou matrix. So, the conjecture, due to George Maser and myself, is, well, I'll write it first sort of vaguely as this guy interlaces, the showering interlaces the augmented showering. What do we mean by that? By that? Well, we mean that this matrix is positive definite. What does it mean for a matrix of representations to be positive definite? We mean that all principal minors are honest representations. I couldn't read the indices, I tried to compute. Oh, did I get something wrong? I'm hoping it doesn't end up in the middle, so in the very middle. Okay, so this is 2 plus 14 plus 6 is 22. Oh, okay. What's the first one, one? 111. Represent it. So, um, I mean, in general, a matrix, you know, a matrix of real numbers is positive definite if and only if all of its principal minors are positive. So I just want to say here that all the principal minors are positive. And what does positive mean? A priori, they're virtual representations of a group. I want them to be non-zero, honest representations. Yeah, sure. And it seems to be true. Yeah. Yeah. Did I miss? Does it mean anything for the augmented child to interlace child, or is that the definition? That's the definition, okay. That's the definition. And let me point out, I mean, again, we've done some experimental calculations. So we've done it for uniform matrix, which have actions of Sn, and for braided matrix, which have actions of Sn for small values of the parameters. And interestingly, it seems not only are all principal minors positive, but all minors are positive. But all minors are positive. So these, in both the non-equivariant setting and the equivariant setting, these appear to be totally positive matrices. I don't know what that means on the level of polynomials. This thing being positive definite means that the polynomials interlace. This thing being totally positive, that's a stronger condition. I don't know what to make of that, but at least experimentally, it's a speech. Do you have any examples of representations of like SN that satisfies? Aspect that satisfies this definition, that don't come from this conjecture. I mean, any classical like let me say something a little bit negative now. So the Z polynomial and the KL polynomial are also conjecturally real-rooted. So conjecture, this is This is partly due to Gedian, myself, and Young, and partly due to Yuan Shu, myself, and Young. First of all, Pm of T and Zm of T are both real rooted. And second, if E is an element of the ground set, so call E is the ground set. So C of the contraction should weakly interlace. So interlacing means that there are strict inequalities of the roots. Weak interlacing means the roots can coincide. Interlaces ZN of T. Yeah, so these are non-equivariant conjectures, and we have some pretty robust experimental evidence for this. Experimental evidence for this. And the wording, and something similar for PMFP. Weakly interlacing means that the matrix is positive semi-definite and that the GCD is real-rooted. The GCD. Like the GCD of the polynomials. So weak interlacing and strong interlacing are the same if the polynomials have no common factors. But if they share a root, then you have to check that the matrix is positive semi-definite. And you need to check that the GCD of the two polynomials is 0 to the root. Slightly stronger than just checking positive definite s. Positive semi-definite Yes. So okay, there's a similar statement for PMOT. It's a little more complicated. I won't write it, but. It's a little more complicated. I won't write it, but these are the real root in this statements. And warning: so the equivariant analogs of these are false. So if you take, I think, the uniform matrix of rank 9 on 10 elements, you can compute the Bezou matrix for Z is M and Z is the contraction and check the positive definite statement. And it's just not true at the main. So until about a year ago, I would have said that like any I would have said that, like, any non-equivariant statement of this form that has, you know, like if the non-equivariant statement is true, then the equivariant statement is always going to be true. So, philosophically, I'm very, very optimistic. And it kind of sort of shattered my world when I found that there's a natural equivalent analog of this statement that doesn't hold. With respect to the symmetry of M. Well, I mean, symmetries of M that preserve E. Of M that preserve E. So that would be the natural equivariant statement. So I'm not making the equivariant statement because it isn't true, but that would be the candidate statement. So I haven't defined what it means for something to be equivariantly real-rooted by itself. I've only defined what it means for two equivariant things to interlace each other. What about the basic of the taring band and the taring of M Deli? Towering of M and the towering of them delete I. Like, I think the real interlacing statement is actually true. Okay, that's not a statement I've heard before, so I haven't challenged it. He said the chow ring of M and the chow ring of the deletion of M should interlace. Now these are these things have the same degree. So interlacing is a little bit funny. Little bit funny. It's not one thing inside the other. Yep. The beta element, the symmetry or contract one, the symmetry, you break the symmetries. Right, so this is the equivariant statement, which I am not making, is with respect to any symmetries of M that preserve E. Have you tried the truncation? I have not tried checking, interlacing with the truncation. No. And if you have this property for some group, you have it also for all the subgroups, but it's not. For all the subgroups, yes. That's it. Okay. Maybe the conjecture is n-contract rat. Maybe the conjecture is n-contract. Okay. All right. So my talk was called equivariant slash categorical invariance. So this was supposed to be an introduction to the equivariant part. Let me tell you the categories I want. Let me tell you the categories that I want to come in. So, I want to talk about matrons with weak maps. People have said this a few times in the last couple of days, but let me write it again. I'm not sure how clearly the definition has appeared on the board. So, let m and m time be matrix on e. So the weak map. From m to m prime is a permutation from e to itself, bijection from e to itself, such that the free image of a basis for m prime is a basis for E for M. So if you're seeing this for the first time, you should imagine that the is just the identity map. So it's saying that every basis for m prime is also a basis for m. So things are more likely to be independent in m than in m prime. But if you write it this way, where you allow the underlying dot to be not the identity knot, then symmetries of a single then symmetries of a single matri are also sort of entitled in the same language. So if m equals m prime, then phi is just some bijection, some permutation of the ground set that takes bases to bases. So it encompasses the notion I've already been talking about. So maybe I didn't even have to say on E, so I could have said maybe M is a matrix on E. Maybe m is a matrix at E and M prime is a matrix at E prime, and then phi is a bijection from E to E prime. Maybe that would have been nicer because I can now say, let math be the category of matrix with weak knots. Objects in my category are natural and Category of matrix and morphisms in my category of weak maps. Maybe for technical reasons I want to say of the same length. You'll see this will come in later. I don't really want to consider maps between matrix of different lengths. Ah, okay, that's right. I didn't say independent, I said basis. So that's actually I said basis, so that's actually a plot. The pre-image of a basis is a basis, then bases have the same size. Thank you. Okay, so I have category of matrix of retrain with meat maps. And I want to think about functors from this category to graded vector spaces. So the main example is I have the Ehrlich-Solomon algebra. So this is a functor from the category mat to actually the category of graded Q algebra. Graded Q algebras. So it takes a matri M to its Arlexalman algebra. And if I have a weak map, sort of phi from M to M prime, I get a map from the Ehrlich-Soloman algebra at M to the Ehrlich-Solomon algebra at M prime. Okay, I haven't defined the Ehrlich-Soloman algebra, but the Ehrlich-Soloman algebra, the generators are indexed by elements of the ground set. By elements of the ground cell. So generator index by E is just going to go to the generator indexed by theta v. And the relations, well, I have a relation for every dependent set. But the point is, if I have a weak map from m to m prime, then m prime has more dependent sets than m. So I have these generators, and the ideal of relations for m prime is basically the ideal of relations for m plus some extra relations. For m plus some extra relations, because there's more dependency. So, in fact, this is always going to be a surjection. Well, it's sort of less obvious that the other invariants I talked about are functorial this way, but they sort of are. So, majority of the factors are functional. When George Naser worked it out for chow rings, and augmented chow rings. So there exist functors underline CH and CH These are not going to be functorial as rings, but they'll be functorial as vector spaces. These will be not ring maps, graded cube vector spaces. Such that for all weak maps pointing to m prime, we get subjective maps from the chau ring of m to the charing, so the augmented chow ring of m to the augmented chow ring of m prime, and same charges. If I didn't say surjective, this would be a trivial statement. A trivial statement, right? Because I could just say, well, you know, I mean, I take M to its chow ring, and if a weak map is an isomorphism, then I take the action of the symmetry group. That's obviously true. And if my weak map isn't an isomorphism, if F prime is strictly smaller, I can just take the zero. That would be a perfectly good functor. It wouldn't be very interesting. And in particular, I wouldn't get suggestions here. And now I do get suggestions. Remember, June said something yesterday about monitoring. Remember, June said something yesterday about monotonicity. So, this proves that the child polynomial and augmented child polynomial are monotonic. So, if I, with respect to weak maps. So, if m maps weakly to m prime, that means that this vector space is bigger than this vector space, which means that coefficient-wise, the chow polynomial of m is at least as big as the chow polynomial of m prime. Sorry, this is chow, and this is augmented chow. Okay, so that's kind of nice. One question I have is whether, I mean, these maps, to define these maps, you can use the fact that both of these rings have natural bases indexed by chains of flats. And you can just define the maps on bases. Given a chain of flats of m, you can send it in a natural way to a chain of flats of m prime or maybe to zero, and you can show that every chain gets hit this way. So it's a purely combinatory. This way. So it's a purely combinatorial construction. So I wonder if there's some geometric meaning to these maps with some specializing cycles and a degeneration. As Matt has pointed out before, some weak maps are sort of interpretable in terms of geometry, and some are not. So I guess what I'm saying is if M is a realizable matroid and M prime is a realizable matroid, and this particular weak map can be interpreted in some geometric way, then can you interpret the induced map on Chao rings and augmented Chao rings? An augmented character is a nice genetic. Is that a question? That's a question. Okay. Let me now talk about the Z polynomial and the KL polynomial. Let me start non-equivariantly. So conjecture. Well, I'm not sure who to attribute this to. I think a lot of people have asked this question. Maybe I'm going to write George Naser because I know he came up with this on his own, but apologies if other people have asked the same question for me. So if m to m prime is a weak map, this is just going to be monotonicity for the z polynomial and K alpha. So then Zm of t minus Zm prime of t and Pm of t Pm of t minus p on the prime of t have non-negative coefficients. It's the same statement I made here, non-equivariantly. The size of the coefficients goes down, can only go down under weak house. It was not a constant. This conjecture sort of suggests that we should try to do something similar here. And we should try to come up with functors, one being ih. One being IH and the other being IH sub empty set, you know, the graded vector spaces that gave me the Z polynomial and the P polynomial. And we should come up with surjective functions. I don't know how to do that. So that would be sort of the best way to prove this conjecture. I can do something weaker that does not help with this conjecture, but I'm going to write it anyway because it's useful for the next part of myself. So there exists subjective functors IH plus and minus and IH sub empty set plus and minus. So they both go from the category of matroids with weak maps to the category of graded vector spaces. And what's the relationship between And what's the relationship between these and the graded vector spaces I care about? Ih of m, this thing whose conqueror polynomial is the z polynomial, is equal as a virtual representation to ih plus of m minus ih minus of m. And similarly for the ih of m. So empty set, I think for the KL column, I'll be. empty set, the thing for the KO polynomial, is equal to ih plus empty sat of n minus ih minus empty sat of n. Right? Yeah? Subjective functor means that big maps transform to subjective. Yeah, that's right. So I have a subjective functor that gives me this, and a subjective function that gives me this, and the difference gives me as a representation. So what this says implicitly is. So, what this says implicitly is that this representation is actually a sub-representation of this one. And when I remove it, so that the complementary thing is isomorphic to this one. And same over here. This is totally useless for proving monotonicity, right? Because the fact that this is surjective means when I have a weak map, this thing gets smaller, but this thing also gets smaller, and that has a negative sign in front of it. So, what I need is that this thing gets smaller. So, what I need is that this thing gets smaller by at least as much as this thing gets smaller under weak mouse. So, it doesn't really help for that conjecture at all, but it'll help for this next thing I wanted to talk about. I didn't say, so if I, so non-equivariantly, if I want to compute, let's say, the Z polynomial, I can express it as an alternating sum of counting chains of flats. So, for example, well, let's say the KL polynomial. So for example, well let's say the KL polynomial, the degree one coefficient of the KL polynomial is the number of co-rank one flats minus the number of rank one flats. And then when you get to higher coefficients, it's similar except now instead of counting single flats, you sometimes count chains of flats. So this is going to be sort of all the things that appear with positive signs, and this is going to be all the things that appear with negative signs. So is this some kind of verification of top heaviness? Is that right? That's sort of only the linear term of the KL polynomial categorifies top heaviness in rank one and co-rank one. So the rest of the terms, it's more complicated. Okay, so finally I want to talk about valutivity. That's also something we've heard a little bit about in this workshop. So I'll try to maybe not give a precise definition, but draw the picture again. Precise definition, but draw the picture again. But theorem, all the polynomials that I've been talking about in the non-equivalent case are valuable. So pi and underlying h and h and z and p are valued. Again, I'll draw the picture to remind you what that means in a minute, but let me just give the credits. So this is due to SPIRE because it's a specialization of the polynomial. Because it's a specialization of the polynomial. These two, I believe, are due to Ferroni and Schroeder. This one is due to, okay, I'll abbreviate their names because it's been written before, but Ferroni, Mathern, Stevens, and Becky. Those four authors appeared in Jacob's talk. And this one is due to Ardeala Institutions. Okay, so let me draw the same picture that others have been drawing to remind you what this means. So this is about what happens when I have a matroid polytope which decomposes into smaller matroid polytopes. So the canonical example is the octahedron, which is the matrio polytope for U24. Which decomposes into the top pyramid plus the bottom pyramid glued along the square. So let me call the whole thing M, let me call the matrix associated with the top pyramid M1, with the bottom M2, and with the square in the middle M12. So in this particular example, it says the Point-Gray polynomial of M should be the Point-Go. M should be the Poincaré polynomial of M1 plus the Poincaré polynomial of M2 minus the piece that they're glued along. And you can, I mean, the same thing is true for the other polynomials, and well, for larger decompositions involving more pieces, then this is a bigger alternating sum, but similar idea. But similar idea. So, the theorem due to my student Daniata, myself, and Lorenzo Becki is that the functor O is valuable. So I haven't told you what that means, but let me tell you what it means in this example. It means that I get an exact sequence. 0 goes to OS of m. goes to OS of M goes to OS of M1 plus OS of M2 goes to OS of M12 just to 0. You see, if you take Poincaré polynomials, that says that the Poincaré polynomial for this plus the Poincaré polynomial for this equals the Poincaré polynomial for this plus the Poincaré polynomial for this, and that's exactly this equation. And the maps in this sequence come from In this sequence, come from the category. So, remember, a weak map, these are always going to be with the identity as the underlying weak map. So, for a weak map, the bases for M prime are supposed to be contained in the bases for M. That means that the polytope for M prime is contained in the polytope for M. So M has the biggest polytope, M1 and M2 have smaller polytopes, and that induces maps here. induces maps here. And M12 has the smallest polytropic that induces maps here. And then I have to put some signs to make it actually exact. I'll come back to that in a minute. But that's the general idea. So one nice thing about this is that this whole notion of valuativity, I mean, I don't know about the rest of you, but I've always found it kind of mysterious. So unclear to me why Faunkrand polynomials satisfy this kind of relation. And I feel like when you get a And I feel like when you get a canonical exact sequence like this, it makes the whole relation a little bit less mysterious. The other thing that I like about this that I want to use it for is, you know, these are now representations, I mean, these are not just vector spaces, but these are representations of any group that acts by symmetries on the matroid. I mean, sorry, not just the matroid, but on the decomposition. So as an example, let me consider an S2 action, where it's S2 action where it's going to be sort of rotation around this line in R3. So it's going to take the square to itself, flipping over this line. It's going to rotate the octahedron to itself. If I do this 180 degree rotation, it's going to take the top polytope to the bottom one, and vice versa. So this then is going to be an exact sequence in the category of representations of S2 there. And it's going to allow me to compute these things equivariantly. Me to compute these things equivariantly. There's a little problem here, which is it has to do with the signs. So, how do I actually get the signs? I'm supposed to choose orientations of all the polytopes, and then it's of the boundary map, so sometimes I get signs of one and sometimes minus one. And I can't actually do this in an equivariant way. The problem is that this action, it's fine on the polytope for M. It preserves orientation, but it actually reverses orientation for the polytope. It actually reverses orientation for the polytoplinum 1,2. So I can't do this consistently. And you can see the problem. If I try to look at just OS0, it's the degree 0 part, you know, it's just a copy of Q for each thing. This thing would be the trivial representation of S2. This thing would also be the trivial representation of S2. But these two things are swapped by S2, so this would be the regular representation, and that just doesn't work. The way you correct it. The way you correct it is that you tensor all these things by a one-dimensional regular representation that encodes how the group is acting on the orientations. So we call that the determinant. So we tensor this with the determinant representation for m. In this case, this thing acts preserving orientations on the matrix polytope for m. So this is just trivial. And determinant here, and determinant here, and determinant here. So tensor dead. So tethered debt, tets your debt, tensor debt. Here, it's acting by signs because it reverses orientation. So this turns into the sine representation. And indeed, regular is trivial plus sine. So everything works like that. Well, let me now state that. Yeah. So, I mean it's not quite done, so maybe let me call it an almost error. This is the same group. Make this a matrix just, yeah, so could you also type oriented matrix to get to all sorts of stuff? Does this make any sense? I don't know. Maybe we can talk about that. I guess I haven't done that. So the Algo's theorem is that all the other things are valued at two. So CH underline, CH, IH plus minus, and IH plus minus, empty set are all valid. The missing part has to do with this theorem that Benjamin quoted yesterday of Ardela and Sanchez. So they have this nice non-infravariant, this convolution theorem. It is convolution theorem that allows you to take two valued functors and put them together and get a new valued functor. So there is, I believe, a categorical version of this convolution theorem. And we've proved like 90% of it, but we haven't quite finished the proof. So that's what's missing. So now all these things, the functors are defined using bases that involve chains of flats. And chains of flats, counting chains of flats is a valuable thing to do because of this convolution theorem. To do because of this convolution theorem. And similarly, on a functorial level, it's going to be a valuable thing to do because of the categorical analog of this convolution theorem, which we haven't quite finished proved. And remember, when I first introduced this IH plus minus and IH empty set plus minus, it was a little bit lame because it didn't help us with monotonicity. But now we can prove that they're valuative. And that means if we want to compute ih, we just take ih. Ih. We just take IH plus and IH minus and take the difference. So if you think of valuativity as a good tool for computing equivariant functors, it's totally fine to break it up like this. You want to compute IH, you just compute IH plus and IH minus. So as a corollary, Benjamin was talking yesterday about how if you have split matrix, split matroids are matrix where you can obtain their polytope by starting with the polytropic. Polytope by starting with the polytope for the uniform nature and then chopping off a bunch of pieces that don't interact with each other. You know, a corner here, a corner there, something here, something there. And he was saying that if you want to, because these polynomials are valued, you can get very nice, explicit formulas for these polynomials for split matroids by just saying, well, you know, if I understand the piece that I'm chopping off and I understand how that meets the polytope, then sort of I. Meets the polytope, then sort of I know exactly what I'm losing when I do that chop, and then I can do this nice formula. And exactly the same thing makes sense equivarially. So, for example, we can come up with nice formulas for the Ehrlich-Solomon algebra and the Kajan-Loustic polynomial of arbitrary paving matroids and I think also split matroids using this theorem and this theorem. And I'm not saying it for all five things. Saying it for all five things just because those pieces, those cuspital things, the lambda and whatever that Benjamin wrote down, we can come up with very nice calculations of the Ehrlich-Solomon algebra and Kaji-Luc polynomials for those pieces to make everything explicit, completely explicit, when we're trying to compute Ehrlich-Solomon or KL. Whereas for these other things, like I don't actually know how to write down the Chao polynomial of these. Of these cuspital matroids in some nice equivariant way. But if I understood those things, then I could understand the polynomials of the split matroids. Sorry, maybe this was said yesterday, but is there nice intuition or examples of what matroids are split? So, what I understand is there's a notion of paving matroids, which means so if you have a matroid of Means, so if you have a matrix of rank five, a paving matrix means every set of size up to four is independent. And then size five, some of them are independent, some of them are not. So split matroids include all paving matroids, and then some more stuff. I don't really have good intuition for the more stuff, but it's bigger than paving matroids. Okay, so let me say one more thing. I want to start by quoting this really nice theorem of Erd, Larson, and Top. So they want to understand what is the universal value of invariant. So they fix ground set E and rank R and they define this thing val E R. So you should think of this as a universal valued invariant for matrix of rank R. Invariant for matroids of rank R on E. So this is, by definition, it's formal linear combinations of matrix on E of rank R modulo value of error. If E is a four-element set and R is 2, one of the relations will be M equals M1 plus M2 minus M12. Generalizing that. And what they proved, this is for Larson, they proved that this thing is isomorphic to H lower 2r of XE, where this is the stellhedral. XE, or this is the stellar filter. So you can take a matrix of rank R on E and construct a homology class of degree 2R on the storage variety. That's a valued thing to do. And every valuative invariant factor is naturally confirmed. So, what's the categorical analog of this theorem? I don't know, but let me at least. I don't know, but let me at least start it. It's going to get a little technical. So first of all, I want to look at matroids on the ground set E of rank R with weak maps. This is a category. And I want to just restrict my attention to where the underlying weak map is the right. Attention to where the underlying root map is the identity. I'm going to put a plus here, which means take an additive envelope. So now objects can be sums of natroids, and maps can be sums of maps. And now I want to take the category of bounded complexes up to homotopy. This is some triangulated category. And I want to mod out by the triangulated. So mod out by the triangulated category generated by valuative complexes. So for example, in my category, there will be a complex of the form M goes to M1 plus M2 goes to M12, where these maps are the weak maps. And I'm going to have some signs here because I actually have linear. Some signs here because I actually have linear combinations of maps. And in my new category, I'm going to set complexes like this to zero. So this is sort of the universal triangulated value to the variable. I was talking about functors to graded vector spaces, which is an abelian category, and I wanted to get exact sequences. You can abstract. Sequences. You can abstract this a little bit and try to get, you know, send a functor to a triangulated category and you want to get something that's equivalent to zero. A little technical, but not so bad. And this is somehow the universal one. I mean, just by definition, the same way that this is the universal value that invariant in the non-categorical sense. So I mean, this should be equivalent as a category to something. And the group need group of this triangulated category is canonically this. Category is canonically this group. So whatever goes here, it should be some category whose Groten-D group is this. Well, there should be a triangulated category whose Groton D group is this. So my feeling is it should be something like the derived category of sheaves on the stellahedral toroid variety, except this category is filtered by dimension of support, so it should be. Filtered by dimension of support, so it should be like: you know, sheaves that are supported in dimension R modulo the category of sheaves that are supported in dimension less than R. Something like that. And the equivalence, if M is realizable, it should take M to the structure sheet of the augmented wonderful variety corresponding to the realization of M. Something like that. I mean, I haven't thought through this at all, but that would be. I haven't thought through this at all, but that would be the kind of answer I'm looking for. That would be the natural guess of a category, geometrically defined category whose graphic group is like, so you're shaking your head at that. Well, the structure sheaf of the augmented wonderful variety, it's like not combinatorial. So it doesn't quite work. All right. So this is not... I mean, I'm a very optimistic guy, and I'm willing to conjecture almost anything, but this doesn't even elevate to the level of a conjecture, but it's a question. Level of a conjecture, but it's a question. So, what should go here to make this true? And I'll stop there. Any questions for you? So, in in this paper with Marius Hatches, where we were talking about that convolution property of valuations, our interest was in this. Our interest was in this half-monoidal matrix of having a kind of the set of sorts of quotients. Do you do you expect there's also a categorical version of this? Yeah, so we I mean we have we have a monoid where you have you know maps of categories from I mean you have this for every E, and then you have a product and coproduct of categories. And essentially we're trying to prove that this subcategory forms a Hopf ideal. Forms a Hops ideal in that categorical Hopf marmite. Maybe a question for Matt, but the thing Matt said, you don't get a well-defined element of KP, if you get a well-defined element of odd valued, we know that. I doubt it, but I think that's a good idea. Yeah, is the Ehrlich-Tyral algebra valuative? Like, it seems like it would get rid of the problem of having the tensor with the. So the Ehrlich-Terrow algebra is not an invariant of a matrix. It's an invariant of a hyperplane arrangement. And it actually depends on the choice of hyperplane arrangement realizing the matrix. Yeah. So thinking about matrix with with good symmetry. So so another another one is a matrix of of any finite root system. And these things have very nice matrix fans tab by graphics which deeply incentives. Is that an example that you looked at past the type the yeah I mean of course type A is what I was talking about. I mean certainly I believe that everything should work in those examples. I haven't checked them. Those examples. I haven't checked them. You know, it's very easy to do computations involving representations of SN in Sage because you just work with symmetric functions, then everything is totally built in. So like, you know, I've done calculations of anything I can think of that's a representation of Sn. In theory, if I were much better at Sage, I could probably do it in other types, but I haven't. So Mitroid fans have more symmetries than the actual Mitroid. Like the braid arrangement has a fan. And the Orlock-Solomon algebra is an invariant also of the fan, which is the matrix. That's an interesting question. I've definitely thought about the fact that graphical matrix often have more symmetry than the graph, which is not the same as what you're saying, but it's somehow related. But no, I haven't thought about that. No, I haven't thought about that. And I guess what you're saying is that maybe my category of matrix should have even more morphisms than I wrote down. You know, that I should have not just symmetries of F, but it somehow should be a category of fans instead of a category of matrix. That's an interesting idea and something I haven't thought about at all. We also have one more question as well. To anything that you thought about, but we're wondering if you can realize as a K-theory, then with matrix with strong maps form something which is called a proto-exact category, generalization of exact category. So I was wondering if your category of matrix with weak maps also forms a proto-exact category, because as formerly it's more diecom rather than just spinning the auto-case, you're in this case. I have no idea, but I'd love to hear more about that. To hear more about it. I think the rest of the questions can be a break, and let's thank Nick. I've never computed that experience. I haven't lived in the paper. Right, okay. Some of the patches. So yeah, imagine that. You know how big one ideas computation says volume is. So, I guess my question to you is: do you have the notion of how to expand the symmetry group of the major? Can you expand the notion of sort of include symmetries, but also generate some of the majority of the materials? I don't know if you believe in the option math, but like trips. Well, I know another one. That is one yeah. What gift? That is equivalent to it so I can interpret that as an equivaler toilet. A tensor touch with a line toilet blind. This is basically an intersection plan of the star and frequency. So, you want a category like a US type of product. So we worked pretty hard to draw a song algorithm that worked. Do you think there's an action of easy? I know I've got the substitute that are in the next one. So really for a trade subdivision to the exact two numbers. I mean, this is how we're So, I think it's a very good idea. So we're starting to get the K range of the wonderful variety or the K range of the what is K? Oh, okay, yeah, okay. Well, I mean, I guess I would say that it's. Well, I mean, I guess I would say that it's not going to be a marketing coverage and therefore what I was going to say was that this would be this correspondence for the level of optical linear spaces, which is reduced to correspondence for non-tracking content because of Chris. Just that you have this. This you have this exceptionalized support, this is it. What might you do?