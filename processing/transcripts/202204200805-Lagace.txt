Organizers of this conference, and to Bercier for hosting us. So, today I'm going to talk about, as just said, an abstract almost 30 page transform. This is joint work with Sage Morozov, Leon Parnowski, Bano Firsch, and Roman Scharberg. And so, the main object of our talk today will Today will be either things like a Schr√∂dinger operator H, which will be minus Laplacian plus some potential B here acting in a priori L2 of Rd. So this is continuum, this is multi-dimensional. Multidimensional stuff, so a bit different from the ideas that were talked about in the last two days. And it will I will also talk about the hype operator, I guess D, which will be D naught plus again some particular. Plus, again, some perturbation B in L2 of Rd Cm. But you should really see only these two examples as model operators, because in fact, I could want to put any other power here. I could look at other systems that are unitarily. Are unitarially equivalent to some perturbation of the diagonal operator in this way. These need not be potentials, they could be super differential. So these are really the basic models, but we'll be looking at let the methods apply more generally. And here, I want this B to be almost periodic. Periodic, which means that B here is equal to a sum over theta in some frequency set theta, b theta e to the i theta i theta x. So b is multiplication by by the multiplication. is multiplication by by uh this, where theta is uh a generic countable set. And here, by generic, I don't want to get into the details of exactly what the conditions that we might need in order. So, in order to define the operator, we don't need any condition, but to obtain some of the But to obtain some of the results we're talking about, we need essentially some sort of Diophantine type conditions on the frequencies here. So the goal is to obtain either our goal here is to obtain is to obtain asymptotics for the integrated density of states, which is n of an interval i or some operator h, which will be the limit as L goes. As L goes to infinity of N I H L divided by 2L to the D, where this here is a counting function for For eigenvalues of H on minus L L to the D with, say, periodic boundary conditions. And eigenvalues of h in the interval i, and our aim is to get asymptotics for this. Our aim is to get asymptotics for this thing as say i is equal to some interval 0 to lambda. Lambda goes to infinity. Now, in the case of the Schrodinger operator, of course, you would want to take this to be minus infinity lambda, but this at the end of the day would change only the coefficients, some of the coefficients if your goal is to get a full-scale expansion. But in the case of operational, But in the case of operators that are not semi-bounded, of course you would want to send this lambda to plus or minus infinity. And the goal, so I guess it's number one, assuming that B is in fact. Is in fact periodic. So this just means that this data here is a lattice, instead of being just some countable set, show that the spectrum contains a A frame say lambda naught to infinity, which is known as the Belle-Sommerfeld property. So the study of either of those questions is somewhat classical and And a lot has been said, at least in the case of Schrodinger operators or operators just acting on functions in the last 15 years by combinations of Danovsky, Stellenberg, Molozov, and Sasha Sovolev as well, in various situations and various contexts. Not use technology and have sheets of papers instead. And it turned out in the last 15 years that there was a reduction that was crucial in the study of those operators, and it was to perform some. To perform some form of gauge transformation, h prime is equal to the exponential of i sine, say h exponential of minus i psi, where psi is also some pseudo-differential operator. And the idea is that you want, so we want this H prime to be as close to diagonal as Possible. Where here, what we mean by diagonal for absolute differential operator is that its symbol depends only on the phase variable C. Because for operators like just a free Laplacian or anything whose symbol depends only on the phase variable, both of these questions are easy or even difficult to solve. And And so, if we can, at least in an approximate sense, write operators in this way, we might have a chance to solve these questions. So, what I aim at today is to describe this step and what are the obstacles and obstructions that you will arrive at. At while performing it in the setting, in an appropriate setting, and see how this can be generalized in a way to make very transparent what these obstructions are. Now, I want to make clear that I'm really talking about one of the specific steps that you then want to use as a black box to reduce your operator to something that you know how to deal with, but there's presumably Presumably, and usually, then other steps to describe what you want to achieve those things. So, I'm describing essentially a common first step to all of those problems. So, before I go on, are there any questions now? I have a question. Do you display the expi psi? What is that? Is that an operator? What is that? Is that an operator? Yeah, yeah. I mean, the idea is that you could write this in some sort of a functional calculus sense, the exponential of ipsi as a Taylor series in the operator ipsi, ip psi is an operator itself. If psi is self-adjoined, this will be unitary, and this is essentially conjugating your age by uh by initially or. So is the psi here the convict of some sort of like projection operator? Sort of like projection operator? This psi will be no, it's not really some form of projection operator. It will be a. So basically what I'm going to describe today, in a way, is how to construct this psi, what sort of properties we can get out of this, and how properties of those psi will tell us how well we can achieve this reduction. This reduction, because as you might guess, it is not possible in general to get a formidiagonal here, except in very specific situations. So you first work at a single level, and then we want that. Yeah, okay. I mean, actually, this is going to be what I'm going to talk about right now. So the classical viewpoint is that. Point is that I guess this was one, now two. I'm gonna talk about something called Mysukovich space. So the classical viewpoint is that these Schrodinger operators, what is the Hack operator, can be viewed as differential, pseudo-differential operator with some. Differential or pseudo-differential operator with some symbol that describe their action through the Fourier transform. So describe the action of H in L two as a A to the differential operator. And now the problem with that is that whatever sort of eigenfunctions you might want to think you have is a priori not in L2. And if you want to do any sort of algebraic work, it's good to have actual eigenfunctions that you can put your hands on. And There is a setting that was shown by Schugen in the 70s, in the late 70s, to be equivalent for the study of spectral properties of almost periodic operators, where you can actually put your hands on actual eigenvectors and eigenfunctions. And eigenfunctions. So on so let say E be the set E to the I X C with C in R D and you can equip that with a That with an inner product between f and g that will be the limit as l goes to infinity of 1 over 2l to the d integral on minus l to the d f bar and call the space The space B2 of Rd that's the clover, the completion of this set E here under that inner product. Now, and where this will form an orthogonal. This will form an orthonormal set inside that space, and this is called here busy coverage space. Now, this space is structurally definitely more complicated than the usual L2 space in the sense that it's a non-separable Dilbert space. The word space. And while this might a priori sound a bit daunting, the advantage that you have here is that it is not a problem to have intervals of pure point spectrum, for example, because it's not a problem to have an uncountable set of orthonormal elements. And also the action of an operator like H. Of an operator like H here is pretty easy to describe on this. So say you can simply describe this on the basis elements e to the i x psi. This will be simply equal to psi squared plus sum over theta b theta e to the i x x psi plus theta. So you can actually describe your operators in a purely algebraic, and even you should really interpret this in a matricial way, with these being the diagonal elements and this being the off-diagonal elements a priori, where your matrix is RD by Rd white. And And so, in this space, it's really easy to describe the operators. You don't really have to go through this Fourier transform formulation. You don't have to, I guess, work as hard once you have it. And the hard work comes from this theorem. This theorem of Schubin in 787 if H is an elliptic almost periodic pseudo-differential operator by almost periodic The differential operator by almost periodic, I just mean here that the symbol is an almost periodic function of the space variable. Then its spectrum as an operator as an operator on As an operator on L2 of Rd and B2 of Rd coincide. So for instance, for answering questions like, does an operator satisfy the Betty-Sommerfeld property, whether you look at an operator on this base or on this base, it doesn't matter. It's the same set. If you want to look at properties of the density of states, you also have a formula that tells you that the number, the density of state in some interval for the operator H is equal to this almost periodic average. Almost periodic average in the of the projector on the interval i of h evaluated at xx dx. So you can actually compute from the eigenfunctions directly that you will have in this business of space spectral properties like this. Space, spectral properties like this density of space here directly from the operator. So, this is in some sense, at least heuristically, the reason why this is the right setting for looking at these operators. And Yes. So from that formula, it seems like this operation like it describes an invariance of space found by a for theta goes over the lattice. If theta is a lattice, that is correct, and this is something that we will. But it is still a countable set, so it's lattice to countable set. It's a graphic set. I mean, I guess that indeed the environment subspaces can be that provid. You still need to not only run over this frequency set, but over any iteration of this. Is frequency set theta. What I'm saying is that if you apply the vector to E to Pi zero I mean here I define it on the basis elements and then essentially you can what I'm saying is the action of on one basis element is a combination of to basic element but by theta. So Theta. So you probably spread to countably many beta seven. So it probably means that there is a block of greater crater spreads into uncountably many blocks, but each block acts in separable spaces. It's blocks of interactive correct, but I'm not I mean, I haven't used this fact if it's true in my description, but it seems a reasonable. Alright, now one thing I want to mention is that once you can write operators in this way, this gives Operators in this way, this gives you also a model for more general setting. There's no reason why here you have to look at functions on RD with Rd acting by translation on itself and acting by translations on those characters of translation. So One idea that you can have is that also this here generalizes as a construction to operators acting Acting defined via some group G acting on an index set C for some. For some basis of a Hilbert space. And of course, in this situation, just like Siegfried mentioned in his talk yesterday, if you want to extract spectral properties of an operator directly from a representation like this, you might want this group to be. You might want this group to be amenable. You might want this index set to actually be a metric space, maybe locally compact. But in terms of description of the operator itself, this is not really needed. And as long as you have some form of distance function on C from which you can have a scaling like that, you can actually form an algebra of operators that will essentially behave like super differential operators, even in some. Curiggers, even in some sense, slightly better behaved because you can directly relate their symbols to their norms, whereas it's a bit more complicated than that for pseudo-group special operators. Where this will act, you get to have traces, define spectral properties, as long as the group is amenable and everything. And it's a good idea to think about operators in this setting. In this setting. One of the actual good things to come out of this representation here, I mean, there's presumably many of them, is that you can represent operators that are systems. So systems here can be viewed as scalar operators. With this C, which will be Rd times some finite set here 1 to n here, where you will have your translation on R D as usual, and you will represent via a permutation group the action of matrices here. Here. And so we have all the non-commutativity put on the permutation of these indices here to represent the matrix action. So Are there any comments before I carry on? Alright. So in this setting. You're going to contribute now for the most general setting. I mean, essentially now, whether you're in this setting or the Bizykovich space setting, it's essentially the same. It's a matter of whether I will write plus or this in my notation. In my notation, but you can really think of either one that you would like in this way. So, all right. So, the idea, as I was saying Is to write this operator as some form of conjugation by unitary operators in this form. And my goal is to be able to find some psi so that I can kill the theta contribution. kill the theta contribution from the potential B. And if you write this here, you can actually write it in this form. Here, well you will have that h prime will be equal to a sum over k written or equal to zero of add our k of h with psi where add zero is the identity add sorry add zero of h sine is h add a Add h psi is the i times the commutator of h psi and add k of h psi is add of add k minus one of h n psi p size and I And in this form here, actually, this is usually just when these are matrices, this is just a formula that you can prove by a data series and all that. For unbounded operators in general, it's somewhat you need to define the right function spaces for this to be true. Well, here, essentially on the space. The space of pseudo-differential operator that you define on either this abstract basic coverage space or on this space, this converges in the appropriate symbol norms for the order that this is. So, this formula is something that converges in an appropriate symbol space in the necessary way under some self-adjointness condition or some properties. I have a question. Oh. Can I interrupt the new? Yes. So when you say. So, when you say symbol, do you mean element of capital C? What I mean by symbol is that I will write a function so that P of an operator. An operator will be defined by some symbol that will be a sum over theta of say B theta of C e to the I theta dot that will act, say, by the group action that this theta has on C or you can Or you can do this as acting on characters if you view this as in this way, and you can define simple norms in some sort of standard way using the scaling that you have in XC for those. I don't want to get into the details of how these symbol norms are defined, but they behave like the usual symbol norms of two differential operators. Okay, thank you. And now, so this thing will look like say the Laplacian part plus B plus I times some conjugator. times some conjugator of minus Laplacian with C plus something that will be a pre-order lower order but your goal is to write your C in such a way that this B here is killed now doing that in the classical setting The classical setting requires a bit of finesse, but here, in this case, there is some obvious candidate, which will be C theta C theta of C should be equal to the theta divided by Theta divided by C plus theta squared minus C squared, or maybe with an I here. And if this is actually true differential, this will mean only C, it doesn't really matter here. But as usual, we have in this place here the problem of small divisors. So at the end of At the end of the day, we will not be able to completely remove how do you get this right class part? Well, this here? I mean, this is, if you write the symbol of this plus this, and this, you get that the symbol of I minus Laplacian C will be. C will be equal to C plus theta squared minus, so the C squared times C theta C. So this is the symbol at theta X C of that thing here. And this comes just from some matrix manipulation at the end of the day. That's really what is the advantage here. Here. And then if you want to kill up B, you have to have psi theta of Xc being equal to this. Yeah. Yeah, yeah. Exactly. So psi will be, normally you would want to choose it like this. And the idea is that if, say, this B is a potential, this would be of order zero a priori. This, if you are somewhere where there's no small divisors, this will be of order minus two. So this C, you would hope, would be of order. minus 2 if you take a commutator here with well this cancels with b but later terms a commutator of an operator of my of order minus 2 with an o with the later terms that you have here which should be of lower order will reduce order again and that at the end you would be with something that is in some sense smaller than what you started with so in So, in reality we have to take c theta of xc to be, say, I b theta of xc divided by c plus theta squared minus xc squared times times some characteristic function theta of x where chi theta of xi is equal to zero if psi plus theta squared minus psi squared Is small. And how small precisely will change what is exactly the order of this psi theta here. You want this to still be an operator of either zero or negative order in order to get improvement in those. That's the thing here. Once you're in the Bizikovic space, Once you're in the Bizikovich space, these here can be actually indicator functions. If you want to get, sometimes if you want to get precise estimates in these things, you can actually ask them to be more, to be smoother and get estimates that will come from how they decay. But in the first few times that these techniques were applied, everybody worked in a smooth setting and took very good care of ensuring really good. Very good care of ensuring really good properties for those functions, and this was quite painful. But it turns out that for many of the at least soft properties that we could want, often just indicator functions here. And this is this is one of the advantages of working in this setting. Now Now um what will be left after that? It will be h prime which will be equal to minus Laplacian plus some here that I will call the resonant part of B, which is the part that I could not kill with this, plus something Of ideally A lower order than B. And in again the real world, a lot of the complications come from controlling exactly what happened. Controlling exactly what happens with this resonant part. So, once you're done with this black box, you know that everything that you will have to study will be from this resonant part and from consecutive applications of this same technique, where everything here that is of lower order, you will get to again reapply the same thing. This will stay here again. There will be a resonant part of something of lower order, so on and so forth. This is something that you can recursively. So for this is something that you can recursively construct. Now, I want to discuss a bit the something of ideally lower order than B. And this comes with, in a way, if I want to make this of lower order, it's how small, how good of a control can I get on this here? And this will come from the notion of either what Of either what we call a weak or a strong gauge transform in this situation. And this comes from the properties of the commutator here. So when you look at operators on like the usual absolute differential operators on scalar symbols, this commutator here, what you get out of that is an operator of the order that you get from the multiplication, but because You get the order that you get from the multiplication, but because the symbols at principal order commute, you actually get a reduction in order where whatever happens is below in the algebra. So when you have properties like this, this allows you to actually be more liberal with how well you control the small divisor. Small divisors because you don't need C to be of as low order of an operator. The communicator of C and P will be of the order of C times the order of P, say minus 1, or if you get even smaller improvements due to some algebraic fact, minus delta or something, but you still got an improvement. You could even maybe even only require that C if computator gives you order. If the computator gives you order improvement by one, that means that a priori would be only C to give you, it could be of positive order even. And this would not be a problem. So this is when the algebraic structure can help you. If not, if you only have to rely on the fact that this is like a multiplication and gives you no real improvement due to the computator, Improvement due to the commutator, then you need to make sure that you kill off a larger zone so that this is genuinely an operator of a negative order, so that the improvements you get come from the fact that size itself is an overdue that has some smoothing properties. So I will I will just write down here these two situations. So strong gauge transform. Here is improvement from the computator and the week. And the weak gauge transform no improvements we need psi to be small and to be small. And you really, even You really, even though all the formalism that happens when you want to compute those things is exactly the same thing in either cases, you really want to view them as two morally different things. The weak gauge transform is really just a good bookkeeping system for perturbation theory. Because you're really using that the perturbations end up being smaller in and of themselves. Smaller in and of themselves. Whereas the strong gauge transform reuses the algebraic structure of whatever problem you have at hand. And an example I could give for that is if you have some operator H. Some operator H, which is equal to H naught plus B. H naught is a diagonal matrix in L2 and B is turbulent. Applying this algorithm with psi being this here will be just j minus k that you will have in the denominator here, will give you that h prime equals to exponential of i of psi h exponential of. control of minus i psi with psi defined through this iteration gives you something that is h prime is diagonal plus zero so and this uses the fact that turplitz matrices commute between each other and in this Each other in this computation, you have no resonant part, you have no nothing that remains out of this computation. So, this is an extreme case where there's a lot of structure where you have full permutation of the lower order part because the PC that you will define through this way will be turbulent as well. But really, using this strong gauge transform method is about Is about the algebraic properties of the set that you're given to work with and the operator that you're given to work with at the beginning. Now, there is an advantage to the fact that it's really the same bookkeeping that you will use for both methods. The conditions on which you get improvement in the commutator, sometimes they're from Sometimes they're from everywhere in the problem, like with scalar operators. But sometimes you can only get them once, but the application of the gauge transform itself kills those conditions. However, sometimes you just need one step of improvement that comes from the computator, and then the perturbation B here becomes of low enough order itself that you're able to start with one or a finite number of steps of strong gauge transform where you're able. Of strong gauge transform, where you're able to have some improvement through some commutator estimates, and then keep on going with many more steps of this weak gauge transform where the properties that you need are invariant under applying the gauge transform itself. So that's the advantage of having those two things together. And I want to finish by mentioning one application to the case of systems. So in the case of systems, basically what prevents you, so you don't have communication between the principal symbols, so you can't a priori You can't a priori get use a strong H transform. But if your principal symbol has the form, say A A1 up to A E or A M C for an exceeded t alpha. Xc for X C to T alpha like this. So this is your principal simple like this, some diagonal operator like that. Well, if all of these are different, you will actually be able, when you use these computator estimates, in here it will be something that will look like And these will have uh have i uh j here, k here, and this will be j k. This is a matrix as well. And so if these are different, you're actually able to kill off things. If these you have blocks where you have the same values in front of your c to the alpha, what you need, say if I write the simple example of the 3D hack, where Where you have something like this here, you need that in the perturbation here to have two blocks of zeros, you want your perturbations to be off diagonal with respect to those blocks. These will be able to be killed off. So, as long as you know that your perturbation has some sort of structure where Structure where everything lives away from this, you can actually apply the stronger transform at least once and get some improvement. Later on, you can use, if you have had enough improvement because the perturbation was of low order enough, you might be able to use the weak gate transform later on. But in this formalism, this tells us exactly when you can do this. When you can do this, and if you do this for Dirac operators, for example, this tells us that potentials that are built upon elements of the Clifford algebra from which you build the Dirac operator itself will be able to be killed up at least to one order, and we will be able to get for these Dyke operators at least some uh limited asymptotic expansion for the density of states depending on the order of the perturbation. My wheel and here the microphone. I will and the cure can be. So you mentioned at the beginning a goal you want to achieve for the integrated density of states. So I mean I just mentioned it now, but I will write it down instead. So my question was what you can actually do. So yeah. Do. So, yeah, yeah. So if D is a D-dimensional de-hydroperator, I get that and so N of of I will write n of here 0 to lambda plus or minus lambda for d2 plus b will be equal to lambda times a sum from j equals zero to infinity of c. Uh C J lambda minus j plus C prime j lambda minus j log lambda. Of course the first one here, C prime zero is zero, and where these depend on B and this is for any pseudo-differential perturbation B of lower order than one. Uh fantastic. Over the one. Doesn't it depend on the sign of the lamb? I mean, C0 would be negative. If you do the same thing for D plus B, where D here is greater or equal than 3. I can do that, but there is But there is here instead, this is some k is the same thing, but with some finite number of steps here. So I get plus or minus lambda some j equals root sum k cj lambda minus j plus c j prime lambda minus j like lambda, where in this k here This k here depends on the order of B and this B needs to be built so with some specific elements in the Clifford Clifford algebra so it's not for any pseudo differential. So it's not for any pseudo-differential B, it's for B that respects some properties of this. How come that you jump from the infinite by that? How come that you jump from the infinite sum to the final sum? Basically, I'm not able to necessarily perform the gauge transform enough times to get the perturbation small enough compared to the size of the microphone. Yes, yes, yes, yes, yes, yes. Yeah, in in this case, of course, I say equal when I should say this here, and this is, of course, plus k co of lambda. Yeah, yeah, yeah. 2 is better than 3. Yeah, yeah, yeah, 2 is absolutely better than 3. Yes, you have this big O here, which is very important. Are coefficients expressible in terms of B? So this is a yes, yes, yes, yes, yes, they are. Because in this case, we reduce things to a direct sum of scalar operators plus some error. And for these scalar operators, it's shown that you can get these from heat environments associated with the operator that's the work of UTEC and Boe Television. Some years back. May I ask a question online? If nobody else is asking? Yeah? Audio speaking, stop it. You can hear me. Yeah, I see you properly. I don't know. Could you tell me, could you tell us what lambda is again? Because I'm a bit confused that for different dimensions you still have lambda to power one right inside. This actually should be squared and this should be. Yes, yes. And thanks. And then one more thing. I must have missed that when you were talking about this. When you were talking about the general setup for the case was for the very beginning, what conditions on B did you assume? Particular. Did you assume that the Did you assume that the constant component of B is zero? I mean, I did not assume that the constant component of B is zero, but in a way, when the constant component of B is not zero, you put it in the H naught instead, or it will remain in the resonant part. And then, once you come to the asymptotic system, that's what I meant, because I remember there was a formula: minus delta must be. Delta plus B R. Yeah, yeah. I mean, so there should be also maybe constant component there. I mean, in this writing, I was viewing this constant component as part of the resonant part, at least for the algebraic manipulations. But once you get to compute things, actually compute the density of states and things like that, of course you separate the contributions from each of these problems. Okay. Okay. Uh can you also work with the let's say all hosting material? No, they I mean at the end of the day you don't need unitarities to to get uh environments or spectral properties like this. Yeah, yeah, yeah. Yeah. And you know, in a way, effectively, that's what we do because we end up computing the symbol of psi up to finite order and we shut things at the end. So in an effective manner, they are obviously tests. Well, thank you very much. So we proceed to the next one. 