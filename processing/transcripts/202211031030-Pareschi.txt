Lorenzo Parisi, and he will speak from the University of Ferrara, Italy. And he will speak about consensus-driven stochastic particle system for global optimization. Now it's your turn. Okay, so thanks a lot for the presentation. And for I would like to thank, of course, the organizers for the kind invitation. It's really a pity that I wasn't able. A pity that I wasn't able to attend in person. It would have been, of course, a fantastic opportunity, not only from a scientific viewpoint, but also to see the DÃ­a de los Muertos celebration, which I'm pretty sure you had the opportunity to see. So, nevertheless, what I will, so I have a short time for my presentation, so my plan is to introduce you a little bit to the topic of To the topic of consensus-driven stochastic particle system, and with some recent results that I've done together with some collaborators here, you see a long list of collaborators. Even though what I will essentially present today, the new part is jointly authored with Jacqueline Borghi, which is a PhD student of. Which is a PhD student of mine, jointly with the University of Aachen and with Michael Herti. Okay, so the motivation is essentially high-dimensional optimization, which is a popular task nowadays, especially because of its impact on machine learning. In fact, the essence of many machine learning algorithms. Of many machine learning algorithms, essentially, to build some optimization model, which depends on a lot of parameters, and we have essentially to find some good approximation of the global minimum of the problem. The point is that typically the structure of the problem is such that the training process where this optimization Process where this optimization process took place is typically a non-convex problem with a large amount of parameters, so it's a high-dimensional and non-convex. And typically, for this reason, the problem is quite hard. The most popular way to tackle this kind of problem is certainly represented by gradient descent, in particular stochastic gradient descent method, and the various variants which have been. Various variants which have been built up. Of course, these methods are highly competitive. There are, however, still some limitation, especially when, of course, if you have that the objective function is non-differentiable or you have situation where you can have the so-called explosion or vanishing of gradients in some kind of neural networks, and it's also quite difficult. Quite difficult to have a result of global convergence in a situation where you have non-convex problem. One of the for this reason, at least in some community, meteoristic become highly popular. Metauristic essentially doesn't make use of any gradient information, and these algorithms are based. And these algorithms are based on some kind of metauristic structure in the, let's call it this sort of artificial dynamic, which often is inspired by some nature analogy. There are many, many different algorithms. The origin traces back to the beginning of the 60s, but then according to the different community, the different approaches. The different community, the different approaches. There are many algorithms which have been devised, which use the notion, I mean, essentially some kind of iterative procedure together with some stochasticity in order somehow to improve a certain state of a given initial guess or multiple initial guesses if we consider this equation with the population of particles. This algorithm becomes extremely popular, and especially recently, you see just here an idea of how the bibliometric data have increased. Where this is done is a recent analysis on artificial intelligence review, where they were essentially spotting at the use of the word metauristic in the various articles. And despite this tremendous empirical success, there is still a lack of a satisfying mathematical theory, so a robust mathematical foundation, and so the convergence to the global minimum. Let's define some annotation. So if we consider an optimization problem characterized by objective function f. Objective function f, our goal is somehow to compute x star as the minimum of our function, and we will typically assume that this function is non-convex, high-dimensional, and possibly non-smooth. This idea of building up a particle dynamic, stochastic particle dynamic, which essentially Essentially, it is able to compute an approximation of our global minimum. It essentially belongs to different field. For example, scalable Bayesian sampling method like stochastic gradient Markov chain Monte Carlo or Stein variational gradient descent, but you have also many other methods. Many other methods of this type. One popular meteoristic algorithm which has this structure is the particle swarm optimization, which is inspired by really the dynamic of a swarm. And more recently, which is the kind of methodistic I'm going to focus, in the community, it has appeared that, let's say, sort of a variant of particles form optimization, which is a simplified dynamic, which is referred. Dynamic, which is referred to as consensus-based optimization, which has then been developed with various generalizations by using different acronyms as the one that you can find here. But we are going to focus essentially on CBO, so consensus-based optimization. So, we focus now on this last class of gradient-free stochastic particle optimization because they have, as you will see, a simpler structure. A simpler structure which is more suitable for a theoretical, a kind of theoretical analysis. So, the idea of this particle dynamic, you see here two examples of two-dimensional functions, is that the particle interact and then they reach, because of some rules, a consensus towards essentially the global minimum of the function. You see on the right, the See on the right, the ROST region function, they initially seem to agree on the wrong spot. Here, in both cases, the minimum is in the position 0, 0, but then they are able to recover somehow and to go to find consensus toward the correct. So consensus-based optimization, as I was saying, they are based on the evolution of n particles accordingly to a set of first-order system, first-order stochastic difference. System, first-order stochastic differential equations. And so we have the particle xi, we have a set of n particles, and essentially they evolve accordingly to a combination of two dynamics, an alignment dynamic and an exploration dynamic. So the alignment dynamic essentially drives all particles towards a, let's say, a consensus point, which is Consensus point, which is the way particles somehow estimate the global minimum of our function. I will be more specific in a few seconds. And you have also an exploration process, which essentially is proportional somehow to some distance of the particle with respect to the estimated global global. Estimated global minimum. In this way, whenever the particles are all they all agree and reach consensus, even the exploration process will vanish. You can consider different kind of dynamic. For example, if you consider really the distance in the exploration process, then you have an isotropic dynamic. But then you can also consider a different kind of dynamic where you keep. You keep essentially, you act differently accordingly to the particular direction of the dynamic, and then you have anisotropic exploration, which typically performs better in practice. But one of the crucial aspects is the way particles estimate essentially their global minimum, which is somehow a regularization through essentially the Laplace principle. Through essentially the Laplace principle of the argument of the set of particles. So instead of pointing toward the best discrete position obtained by the particle, they point toward this value here, this x-bar alpha, which depends on a parameter alpha, which is essentially a weighted average of the particle position where the weight Position where the weight concentrates, where the function f has its minimum. Of course, if alpha is very large, then this concentration effect is stronger. And in fact, by the Laplace principle, essentially, you have that for very large values of alpha, this quantity is the estimated position is essentially. Position is essentially nothing else than the argument of the function evaluation with respect to the particle position. So they're going to point toward the particle in the best position for alpha, very large. But of course, if alpha, on the other hand, if you take alpha equals zero, this is just the average of the position of the particles without any role played by the target function. Target function. You have lambda and sigma, which are the two parameters of the dynamic together with alpha that we have already explained the rule. Of course, lambda measures the strength of the alignment, whereas sigma measures the strength of the exploration process. And you see here a list of references of people that have worked on that. Let me here just mention the first paper, the paper by Pinot of Sen Martin, which is the paper where this approach. Paper where this approach has been introduced, essentially. Now, the idea is to study the dynamic of this system, of this set of particles asymptotically for large values of n, and so then to apply mid-field techniques to essentially prove the convergence of the particle dynamics towards the global minimum. So without Without entering too much into the technical detail, formally we can, whenever n is sufficiently large, we consider the empirical measure associated to the particle position and then the corresponding measure associated to the position. The position towards this particle point. So, whenever n tends to infinity, then formally we analyze the behavior of the particle distribution rho, which characterized essentially the particle density in position x time t. And this is the kind of dynamic we get in terms of Pd, which basically is a non-linear Fokker-Planck equation. Non-linear Fokker-Planck equation, this is the case where you have isotropic exploration. So you see that the diffusion process depends really on the distance between the position and the estimated minimum. In the anisotropic case, you just replace this diffusion term by the term you can see here. The anisotropic dynamic has been introduced in particular by Carillo, Jean, Li, and Thu in 2020. 2020. Okay, so just a sketch of the convergence idea. The idea of the convergence, following essentially the framework introduced by Fornazier, Clock and Ripple, is to minimize this energy function functional, which essentially sorry, which essentially is one half of the One half of the Basterstein two distance between rho and the direct delta concentrated in the global minimum of the function. So how particle concentrates on the global minimum. So by proving essentially the exponential decay of this functional, one is essentially is able to show consensus formation of the particle at the same time convergence toward the global minimum with respect. Toward the global minimum with respect to this Baselstrain distance. The idea is that you look at the evolution of this quantity, which follows a rule which is similar to the evolution of the variance of the particles. And then you, of course, this term here already gives you the exponential decay, but then you have to bound the second term. And you can... What you can compute is that you can bound this quantity again by a term which depends on your original functional, plus essentially the L2 distance between the estimated minimum and the global minimum, which is the quantity that you have to control. But at this level, what you can use, you can essentially consider, apply the Laplace principle. The Laplace principle, which essentially tells you that essentially, as soon as you have some mass around the global minimum, then whenever alpha goes to infinity, provided that your function f is locally leap-shift-continuous and satisfies the coercivity condition which is reported here, then what happens is that by increasing alpha. Is that by increasing alpha, then this quantity, the L2 distance between the estimated minimum and the real global minimum, goes to zero. And so increasing alpha, essentially, you can control this last term here. And this essentially permits, this is just a sketch, of course, permits to prove the convergence toward the global mean. Sorry. Sorry. Then one important remark that shows you the difference between this functional essentially and the variance is that the variance, generally speaking, is always smaller than dysfunctional. However, whereas this However, whereas this concentration toward the global minimum decreases really exponential to zero, the behavior of the variance is different. So we can easily build up examples where the variance of this particle system increases initially, whereas the energy functional still decays exponentially. This is one of the reasons why typically it's more difficult to try to take More difficult to try to take on the problem by first essentially proving that the variance goes to zero and then showing that the point where the particle concentrate is the global mean. From the algorithmic viewpoint, then you can build up the scheme just by using an Euler-Maruyama descriptivation of your system of stochastic differential equation. So this is essentially. Equation. So, this is essentially the scheme that you practically implement. And then you have several computational, let's say, tricks or improvement that you can apply. For example, you can use batches to evaluate this global minimum so that essentially particles are separated into groups, and instead of pointing all toward the same value, they point at every time towards slightly different values, which depends on. Different values, which depends on the batch. This is going to improve a little bit the ability of particles to explore, and essentially, this may be quite important for particularly challenging functions to optimize. Another strategy would be to have a particle reduction. So basically, as soon as the variance decrease, then you can essentially reduce the number of particles in time. And this is going to, of course, saving a lot. Going to, of course, saving a lot in terms of computational time, or you can even use adaptive parameters. So, for example, you can decrease the noise in the system as soon as the particle tends to reach consensus, again, with the code to speed up the dynamic. And you can even increase alpha in time to achieve higher precision during the computation. Of course, Tailoring all these parameters to a specific problem is typically rather delicate. So one has to try to stay on the typically to try to find a good set of parameter which is able to solve pretty well, a large set of problems. This is just an example. I don't know if I have time, probably no, to a machine learning problem. A machine learning problem. I'm just skipping this because I don't have time. I wanted just to say you a few words on how to generalize this idea to multi-objective optimization. So multi-objective optimization is a classic situation which is of great relevance in many applications. So typically you want to minimize not only one function, but you want to minimize more functions. You want to minimize more functions, but the two functions may be in contrast one with each other. So minimizing one thing is going to increase the other and the opposite. In these cases, there is the notion of Pareto optimality, where an objective essentially cannot be improved without necessarily degrading another one. And another point is that what is important is to have a diverse set of optimality solutions. Set of optimality solution because typically you're interested to have a whole set which characterizes in the best possible way the scenario of strategy that you can apply. So, formally, we consider the problem now of minimizing m function, g1 gm, and we And we typically have the notion of a weekly Pareto optimal and Pareto optimal that you can see here clearly in the case M equals 2. So in the case M equal 2, you see this distribution of values and you see here on the bottom left a class clearly a position which is non-optimal. Then in the second here you see a position which is strongly A position which is strongly optimal since there is no other in both directions, there is no other point which is doing better than this. This is an example: the third case, you see a position which is weakly optimal since it's going to be optimal only in this particular case with respect to one of the two functions. And as a result, what we want to build up is the so-called Pareto front that you see here, where you consider all points. Where you consider all points, which, of course, this is a rough approximation, then you want to do this at a continuous level. So the Pareto front will be represented by a curve exactly as here. So you have your search space, you get your set of optimal points, which correspond in the image space of what is called the Paripofront. And the goal that one typically has in multi-objective optimization. We have in multi-objective optimization is not only to approximate the Pareto front, but since you have a discrete set of solutions, you want your solution to be uniformly distributed along the Pareto front, not in the search space, but in the image space. So the technique we adapted to take this problem is a classical scalarization technique, which basically Which basically we correspond to pass from a multi-objective problem to many single objective sub-problem. So basically what you do, you introduce a function which g of x double v which depends on a vector of parameter double v which is which they are where these weights double v are on the on the unitary on the unitary. On the unitary simplex. And then typically, you want to minimize this function g for the various possible value of double v of the parameter. So what you will, the result you use essentially is that you get your point is weakly optimal if the point solves your sub-optimal problem, which is the kind of Suboptimal problem, which is the kind of problem we have here, for some value of the parameter w. What we do, of course, there are different ways to do this here. What we have done here is essentially to assign to every particle a different sub-problem. So, which means that the number of particles is the same as the number of sub-problem that we have. So, basically, the number of So basically, the number of you have one sub-problem essentially for any vector of parameters double BE. Then the dynamic is the same as before. Now the main difference is that the estimate of, so the point toward which the particle point depends on the particle itself, because it depends on the particular set of weights of the particle. Of the particle. So, this weight are a characteristic of the particle. Of course, one can build up a slightly different dynamic where instead of having one particle for one problem, you have subset of particles for each problem, but this doesn't change particularly the discussion. Again, we can pass to the mean field and we get now this mean field equation, which is similar to the previous. Field equation, which is similar to the previous one, because you have the only thing is that you have this dependence from the vector of parameter w. It's possible also to extend the previous result, the convergence result to this setting, but what I wanted to tell you in finishing my talk is that one of the important things is how do you choose this parameter W? So you see that typically So, you see that typically, if the parameters are uniform in the search space, then what happens is that in the image space on the Pareto front, they typically are concentrated, so they are not uniformly distributed. On the other hand, if you want to have parameters uniformly distributed in the image space, then these parameters are not uniformly distributed, of course, in the search space. So, what we have done here is to consider an additional An additional dynamic in the space of parameter by a short-range repulsion dynamic, which essentially the goal of this short-range repulsion is to distribute the point along the Pareto front as uniform as possible. And this is the kind of dynamic we have because We have because of the previous, okay. Of course, this was the short-range impulsion that the particle acts with some power S. And this is the kind of dynamic that we will consider, which is you can clearly see it in the case n equal two, where the dynamic is easier because you always have only two directions the possibility to move along along. To move along the throat. Then you get the dynamic here, and I can show you some results. One important thing is that if you consider the convergence of the dynamic or the wall dynamic, where you also have the mean field part, which corresponds to the Lasso dynamic characterized by the repulsive effect. Then, what you discover essentially is that in order to have convergence, the two processes should have different time scales. So, essentially, the adaptivity in the space of parameter has to be slower compared to the convergence in the search space. Here you see some result. These are classical test cases, so you don't. Test cases: so you don't even formulate the test case, you just plot the resulting Pareto front, and then we just look at how particles approximate the Pareto front. This is a classical Lame test case for gamma equals 0.25. And you see here the result in the first case, there is no repulsive dynamic. In this case, you have a risk-type repulsivity, which is this one, a Newtonian, which is with this potential, and then a Morse. This potential, and then a Morse, which is with this potential. And as you can see, the Morse dynamic, which this is not a stationary state, but is approaching a much better behavior compared to the original dynamic. And these are more difficult results where the Pareto front may even be discontinuous or it has multiple jumps. Jumps, and you see how the introduction of the repulsive dynamic is able to give a much better description of the front that we have to approximate. And this is a 2D example where you see on the left, no dynamics, and on the right, the more. On the right, the Morse dynamic cover is standard simplex in the case M equal to. Okay, so what we have seen essentially is that these processes are pretty interesting, these stochastic optimization particles, because particles can share information and can distribute the work, and so you can tailor somehow the optimization process to your particular problem. Particular, your particular problem. Some future research direction, probably one of the most interesting is to try to see if one can use similar mathematical techniques, by which I mean mean field type techniques, to analyze other meteoristic methods. Let me say that here the most difficult part is to take the metauristic dynamic, transform the metauristic dynamic in some kind of differential model, and then to try to analyze the. To try to analyze the resulting differential model. Okay, thanks for the attention. Yeah, thanks. Thanks for the very interesting talk. So now, because I'm running out of time, so we need to go to the next talk. So you have questions, you can message Dr. Parinski by emails or also some messages. Okay? Sorry that we don't have time for the questions. So, the next speaker, Thomas, could you actually share your screen?