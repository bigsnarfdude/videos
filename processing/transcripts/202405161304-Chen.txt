I just came here to learn new methods because we always say statisticians lead ecommeticians and ecompetitians lead other economists. So that's why I'm here. I try to learn some new techniques. But as Ylan did not make it, I think maybe it's a good idea to present one of my working papers. It's a joint work with Ylan Chen, Stevenson Von Beer from Rogers, and Wong Cheng from Rogers. The title is Time Varying. The title is Time-Varying Matrix Factor Models. Actually, here, a few minutes ago, I still worked on my slides, so there might be some types somewhere, but hopefully, I can present the broad idea and then show you some interesting results. So, first, I'm going to give you some motivation. Actually, that's part of the reason why I am interested in this type of model. So, in economics, we have So in economics, we have a lot of examples of matrix value data. And this up here, this one is for macro data across different countries. So you can see naturally we have this matrix answers. And this one is for international trade. So for the browser, we have export and then we have import. And then we have in court. And for the trade data, the diagonal, we just put zeros there. There are more examples. The very first example is why I got into this literature, the risk spillover effect. In economics, when we talk about risk spillover, usually we just do VAR and then variance decomposition, try to figure out spillover. Try to figure out a spillover effect. Because this kind of observed network, we don't observe the network among different investment banks. Say we have a lot of investment banks. And then something happened to JP Morgan. It will be transformed to other investment banks. But we don't observe these things directly, and then in economics and literature. And then, in economics literature, for example, Frank Hiebo developed some technique to figure out this feel-over effect. But I think using a new technique in matrix or cancer model, we can do this more naturally, taking advantage of the data pattern. So, that's another example. And then we also have hatch bump performance. Have hedge fund performance evaluation at each time point. The hedge fund returns and the characteristics are observed. And we also use the same idea in portfolio selection. I think Yeni has a paper with Wofuzhou kind of using this idea for portfolio selection. And so we also try to work on this dimension a little bit. We call it as cancer portfolio. Try to use this cancer regression to select. It to select portfolios. So, the idea is when we try to construct this, we try to take care of and consider the connection between different individual stocks. And the characteristic of not is all stock, but other related stock will also be helpful in determining the portfolio selection. So that's another example. And then the third example will be input-output analysis among economic sectors and industry. And then because this is an input-output table, so naturally we have matrix values can use. And yeah, I guess I wanted to motivate matrix vector model and more. Matrix spectrum model and more. We already saw a lot of examples. And in this paper, in particular, we try to work with metric spectrum model with time varying loadings. So the idea is in economics, especially if we have long-time horizon data, people care about structure and stability. And the kind of default method is to use certain types of rolling regression, either in the factor. Either in the vector setting or in the regression setting, we always use like five-year rolling or ten-year rolling. And then we think maybe there is a fairly natural way to that data tell us what kind of kind varying pattern we have. That's why we try to consider this kind varying matrix pattern. So basically, here the loadings can be varying over time, and we don't specify the functional form. Specify the functional form. So basically, how the loadings will change over time, it's not specified. We only require that these are some smooth function of time. And we try to use kernel method to estimate the time-varying curve. So, basically, in this project, we try to put all these different dimensions of literature together. First, one, of course, this is not a First, one, of course, this is not a complete list. I just put some here, and forgive me if I forget your important work. So, here I have vector model in the vector, traditional vectors as vector by spectral model. And then we have matrix and tensor regression. The third one is what I was talking about: non-parametric time-varying parameter model. There, the regression coefficients are modeled as some. Are modeled as some known function of time, and then we just try to combine this literature in this project. So for the model, we consider matrix vector models. YTA is our observed matrix value time series. And in our example, we use our import-export data. And here And here F as a common factor matrix is a latent factor. And different from existing literature, we model the loadings RT and the CT as a function of time. There's a time varying low loading matches or bottom loading matches and the ETS or noise matches. So here basically we have low random Low ramp structure, and this is basically a special case of the example of Tucker vector. So we have model ambiguity. We cannot only identify up to this rotation matrix. However, the linear spaces are uniquely defined, and their projection matrices are also unique. So here we can identify. So, here we can identify this linear spaces, and using our non-parametric method, we can also guarantee it these spaces are changing over time, mostly. So, that's the setup. And then for the time-varying part, we model the loading matrices as the non-schochastic function of spot t over capital T. We can modify this a bit, we can add some Moist there. Can add some risk there because some economists don't like this idea that the loading is just some deterministic function, but we can do some minor modification there. And here, they are just some smooth function of small t over x to t. So this is like the inferior some products. So we can revise that. That's what they mentioned. And sometimes people, especially not. Sometimes people, especially economics, people might specify them as some function of observed state variables. For example, this could be some function of important economic indicator or some other observed variables. And to take our import export example, here the observed matrix as a square matrix is unnecessary symmetric. Not necessarily symmetric. Okay, Q2. And the entries represent the volume trade flow from country I to country J at time T. And we can identify K times R latent factors and we view that as export heart and import hearts. I presented this one, related one in our economics workshop. Economics workshop. My trade colleagues are not too happy with this result. They suggest maybe we try to incorporate a gravity model in our framework. Definitely to do that, we just can add the collar iteration, a more like a regression type, and then we use this part as kind of like an interactive fixed effect in our user economic setting. Economic user economic setting is doable. But in our current setting, we just try to identify the latent latent factors as well as loadings. And actually, I like this interpretation, I think interpretation, the sample set of interpretation from Professor John's previous work who I said. I like this a lot, but I still need to work hard to convince. Work hard to convince my trade colleagues. They think it's not very natural. They think they know what kind of network they have among different countries and they have some other frame job interests in mind. For example, like elasticity, we have this trade ward and they want to see how the tariff change affects the trading model. So we can, so what I So, what I told them is we can add that part, the tariff change, as our cobalt, and then this better part as our kind of replacement of their original fixed event, because we'll do that. But in this project, I was here, I use this framework. So, the entry of R, that's the row loading, can be Loading can be explained as the export contribution from country I to hub L at time T. And other hand, the FT can be viewed as hubs. The hubs can trade among themselves and the hubs can trade with other countries. So these are reflected in the factor matrix FT. So that's our info. So that's our illustrative example. In our empirical work, we also use this data set. And actually, I promise usually what people do is they consider a factor model. They just stack all this data, factorize that. And then this will be by and so on's framework that's vector factor model and Phi T ask. 5TR loading there, but when we step that, we must learn the natural data matrix structure. And on the other hand, we have to estimate a lot of amount of parameters. Especially here, we have time-varying amount of parameters. So, actually, we have to estimate a lot. So, in our setting, we just model that as time-varying loadings, and we impose smoothness conditions. Incurse smoothness condition so that when T and S are close to each other, the indexes are close to each other, the loadings will be quite close to each other. So we just leverage the smoothness for estimation. And because of that, the y square s can be approximated by RTFSDT, transcript errone. So we can use this local version. Use this local version, local PCA to estimate loadings at each time t. So we define MRT and sample end log in this way, K as the kernel function, and we use the boundary modified kernel function because here we are always in the web. And we have by using kernel extension, we might have some. And this is just a local version of second moment. So for the estimation, here we can identify the space and we can guarantee the linear space are smooth. They are changing over time smoothly. So here we use RT tilde as the square repeated times. The square root P times the k eigenvectors of n R T L in the same order the usual way and S R Ta as for linear space by the columns of Rt delta. So this will be close to the true S R of T and from smoothness S R T will be also close to S R T minus 1. So that guarantees So that guarantees that indeed we can get smooth the linear space will be smooth, but we don't have guaranteed that the estimated loadings will be smooth over time. And that's the reason why we have additional section on the smoothness procedure because here just from this space we get a loading and then there's no guarantee that. And then there's no guarantee that RT hat and RT hat were being very false to each other. We might have some jump there, but for the space, we can guarantee the smoothness. And in terms of estimation, that's quite straightforward. This group I don't need to target more about that. This is just a local version of PCA. And then we can back out the vector as well as the signal part. Part. So we have some asymptotic results, that's their consistency result. So you won't be surprised at this rate. So compared to the user constant loading factor model, we have this additional term which comes from the bias. H is the bad ways H four, and then this part we have T times P of H. Times P of H. We also have the psychotic normality and the rate as affected. Here we don't have the reduced variability, we don't have virtual QT, we have QTH because we use the local information. Basically, we use the local neighborhood to do estimation, and then we can estimate this loading at each time point. Point. And the next bias turn and that's related to the second moment. That's why we have this ugly form. And we also have the results for the psychotic normality we can derive for this optimal bandwidth. It gives us some guidance for the bandwidth selection, and then that's the consistency result of the factor as well as the signal. vector as well as the signal. So, alternatively, the measure we can vectorize that, but because here we estimate too many anions in this loading vectors, and the convergence rate is much slower than our Matrix spectrum model, temporary Matrix spectrum model. So, we also, since we have We also, since we have this time varying framework, we also try to extend the eigenratial test to our framework and we call it generalized eigen ratio-based estimator. So here, the number of fibers are not changing over time because we use the smoothness condition in our framework, we cannot command date. Common date the time-varying number of factors. We cannot allow for that. But in some other contexts, for example, like Professor James' previous work in threshold model, Markov region switching, maybe we can do that. But here, K and R, K are stirred above column factor, and then the other one is the low factor. And the other way is for low factor. They are not changing over time. So we just have K and R, we show consistency of our selection method. And I don't think I have enough time to go over the simulation. It's quite a lot of simulation, basically to show that estimation results are pretty good. And we also compare our result with a vectorized method. And not surprisingly, we found our We found our results much better, and then we also check the consistency of our selection method and we can identify the correct number of factors pretty well. And so, let me spend some time talking about our empirical application. So, here we try to story the network of international trade flows. And in the economics literature, usually people just use global vector autoregressive. So, this is a VAR, but now we have multiple countries try to put all them together. They don't try to use the natural data structure. And here we use the monetary import and export volume. import and export volume to fill in our time marrying model. We select the data driven method, select four and four, and ask the number of dimensions. So that's the results we first got the original result from our time variant estimation. So that's the latent export loading for the first part. So here as you can see, this one kind of dominated One kind of dominated by China before 2000 is dominated by Japan. So it's quite intuitive. We know with China's entry to WTO, China's export contribution dominates. And that's why we can see this change. The second one, the second export hub is dominated by the United States, and you can guess on the Phase and again, it's not too surprising. As you can see here, throughout the time period, we see very dark color. But after about 2008, it becomes a little bit lighter, likely because after the financial crisis, and some other countries also have such a contribution. So, for the first two, Two loadings they are quite smooth over time, but you can see for the third one we have this kind of break here. It's not our smooth over time. And so that's what I mentioned before. We don't have guaranteed that our estimated loadings will be smooth over time. We can only guarantee the space will be smooth over time. Smooth over time, and that's why we need to use additional smoothing procedure. And that's the fourth one, and so this one is more obvious. You can see it's kind of jumping around, we have kind of a break. So because of that, we consider additional procedure. So that's the import part. The story is similar for the first one: United States dominate, and this one, China has China has one more important contribution, and the last two kind of jumping around. So, here we try to take care of this issue. The loadings may not be continuous over time and some reasons why this might happen. The item matters might change size, as relatively easy to handle. We can just check whether. we can just check whether this estimated 1RT I hat transpose RT minus 1 I hat will be greater than 0 now that we have fixed I that's very easy to handle but for some others we need additional procedure is for example eigenvalues to a switch order order when eigenvalues are the same the corresponding vector The same, the corresponding vectors are not unique, or as they are quite close to each other, we might also have the order switch. And that's why we consider an additional procedure to first to detect this kind of change point, and then we do additional smoothing. And the third reason is we might have the changing dimensions, number of significant values or factors might be changing over time, but in this Change over time, but in this framework, it's hard to accommodate this. So we just try to take care of the diagonal switch. We use this non-converging detection technique. So the idea is kind of try to detect the jump point, and then we use this one-sided kernel for detection. So you are familiar with change points. Change point literature and use these two sides: one side of kernel file wires try to estimate from the left, and the other one tries to estimate from the right and try to see the distance or the difference. And then you can identify the jump point. And we just follow this idea. And here we define the one-sided kernel on this way. So one is from the left, and the other one is from the right. One is from the right. So you can see one is from t to t plus t times h, and the other one is from t minus h to t minus one. Before we use two-sided, so when we do estimation, the summation is from t minus th to t plus th. But now we try to do detection. We use one-sided kernel in this way, and then we can identify the Defy the eigenvectors. Now we have two, one is for left, and the other is further right. So we use IT hat plus and I t hat minus to represent this tool. And we define our strategic in this way. Basically, we just try to measure the distance after swapping the eyes and eye per swap colors. If this distance is large, then this gives us. Explore, then this gives us some information about the potential jump, and then we try to use this statistic to identify the correlation bridges. And the procedure has like an outlier detection procedure, try to use that to find the jump point and then try to smooth that. So that's the basic idea of our smoothing procedure. Smoothing procedure, and you might wonder why I try to do that. So, first, of course, you will see the picture. You might ask this natural question: why it's not smooth over time, and for try to present the better figures, try to have a better interpretation. We are thinking of this procedure. On the other hand, for this detection, this detection itself might also give us some. Itself might also give us some information on potential this kind of jump point. So that's part of the reason why we try to do that. So that's the figure where we actually did a lot of simulation with this musing procedure, but I just present the result with the empirical application. So here we have the passive statistics. The test statistic of RT, and we have the point I fly upper quota. And then this one, so here as you can see, around this 2010, this sign of switching, and then for the fourth one, we find two correlations. find two correlation bridges and then based on that we try to do additional smoothing so that's the smoothing additional smoothing procedure and after we apply our smoothing procedure that's our final estimation results the first two halves and then for the third and Then for the third and the fourth one, you can see now the figures grow much better, and so they are actually on this. You can see changing over time. Mostly we don't see a jumping up and down all the time. So that's the result after we apply this non-parametric detection surface technique. So it's pretty So that's pretty much what I have. So basically on this paper we try to consider this kind varying factor model and we derive some theoretical results and we try to apply that to our trade data. And so we also encourage this smoothing procedure, try to handle Procedure, try to handle the come back to switching problem and try to handle this order switch. But in our setting, we only allow the two adjacent change. We just have t minus one to t and we try to detect the jump point and then to further smoothing. And some vocational expansion here. Extension. Here we didn't present the assumptions, but actually we just assume our factors are strong. We can relax that, we can extend our work to allow for weak factors as doable. And in this project, we just have static factor model. In economics, sometimes people are also interested in dynamic. Also, interest in dynamic vector. So, in the sense that we have not just Ft, we might have the lag of Ft, which also contributes to the dynamics of Y observed Y. That's more tedious, but it's also doable. And people might also wonder the number of factors might also change over time. Also, change over time. In this framework, we are not able to accommodate that, but we can use other modeling strategy to model the change of numbers. And then the last one, our specification task, economics, when I presented this work, always got the comments that here we impose this. This kind of chronic product structure. And then, when we have data, how can we tell whether this is a reasonable structure for particular data we have? So, this is more like a specification test because the vectorized version can be viewed as unrestricted. And then, what we have here, the matrix factor model, where sprung product structure can be viewed as the restricted. Be viewed as restricted. So we can come up with some path to check which one fits our data better. And I think the work Professor Jing spoke about the goodness of fit test could be applied here. I tried to receive a little bit as possible, but we haven't done that. So these are some extensions we are. Extensions we are thinking about. I'm quite interested in this matrix and factor models. I think there are a lot of interesting applications in economics. Also, economists are not aware of these new techniques and we still use very traditional like a panel type of model. You know, you just add additional index there, but we don't try to. We don't try to make full use of this data structure. And I think there are a lot of interesting work there, and I believe that there are a lot of interesting opportunity there. But that's all for my talk. Questions? Any questions from the audience? For the topic, I think the egg vector switch. This an eigenvector switching problem does happen frequently, analyzing this time series of the data, the matrix data. I have a paper, it's a data analysis paper, analyzing the co-citation network of statistics. In that paper, we proposed another approach for resolving this problem that we just use a common projection matrix. But yet all the matrix state of all the time to the common space, at least empirically, that will result in the problem part. I'm not sure if that will be helpful. We're not aware of that paper, but we know there's a recent biometric paper who work, and I didn't put that slides there. We didn't use their procedure directly, of course, actually we tried to get not. Twice not that stuff because there's not something happening there, but yeah, we don't retain your paper, we are not aware of it. Yeah, I think we use those copy to project the matrix, then we often get using the plots of the exit vector. Yeah, see over X-Bact is a projection of the matrix of a certain logic. I have another question. So, sorry. The problem here is not just the eigenvalue are changing, but eigen space, the loading space is also changing. I don't know if you can project the common projection. Oh, I see. Well, these are changing over time. My problem, I really needed to find the eigenvector. So if I have a projection matrix with the eigenvector, the eigen space is properly small, and then I can better the follow-up competence. That's why I don't need to really get. Need to really get the eigen space for at every touch. I just need to find the common low-dimensional space. Sorry. I have another question. Is it related to the situation, the number of factors changing over time? But it could be common like although the number of factors are not changed, but probably at the between certain Between certain times, we have five number of vectors, and then after the event, although we still have the same number of factors, but while the old factor kick out, we bring in a new factor. We kick out an old vector. And in this situation, probably the eigenvectors are no longer smooth because we kick out that old vector. Because we kick out that old factor bringing new factors. Yeah, um. Exactly. We do have that problem. Yeah. So the originally non-significant dimension certainly became significant because there's the fourth and the fifths the eigenvalue switched. So you do see the last. See the last dimension, the smallest and the loading does change. Here I only present the first four, but actually I'll also have five from the first six I think before. I don't know if Henry can we do that because I think in this framework we cannot allow We cannot allow that we do that because otherwise, this is like one zero economy. But whether it's significant or not is also up to signature, right? Yeah, inferior definitely happens. I don't know how to accommodate that in the theoretical empirically it is still a smooth work, but in practice because we can't I mean in theoretical So I think it seems like a break. But yeah, WT can have switch, but I don't think one zero point. That's a small question. I think in your theory, small t cannot be, cannot equal big capital T, right? In your theory, what do we have a theory here? Small for equal T from one for capital T. I agree small cannot equal capital T. I'm sorry, small cannot be equal to. I'm so small, cannot be equal to your period. For each T from one through to capital T then small T I believe it's more T cannot equal capital T if it's a more T equal to capital T then your deal will become nonsense. Oh this one we use tau for each tau this is a standardized one right this is from zero to one we don't do this from infield as in Harmony we use tau which is a ratio of multi over Use cow, which is a relation of small t over we have to t. So that's why we have the smooth function of cow. No, no, I'm not focusing on. I'm not focusing your theory. If you compare your theory, for each T from one to capital T. Yeah, true. Yeah, actually, here just as a notation, we actually should put it as tau. This everything is in tau. Do you need T capital to be much larger than PQ or not? T will be much larger than P. So we have the condition we need us. Condition: We need this one: t times, so there is a certain ratial restriction of t, q, and t. So simple, instead of taking symbol t equal to capital T, then right-hand side is R over capital T. Yeah, so that's what I said. This one should be viewed as the notation should be defined as R T I as R over capital T. Yeah, this is in field, or maybe this is not a very good notation. Maybe it's not a very good notation. We just use tau. Yeah, but that's just tau. We have small tier over time. That become the process, basically. Yes, yes. So yeah, we have some condition of t h and t to guarantee certain weight. Yes, so yes, t is but you know like the ratio that was as well as this ratio because we have t h times this. So P and Q could be