60% of the total emissions are anthropogenic, and wetlands represent the largest source, natural source of methane. One of the puzzling aspects of methane is that we saw the atmospheric concentration stabilize between 2000. And the cause of this stabilization and this subsequent increase in the growth rate is still uncertain. As what we mentioned yesterday, we have isotopic information that's telling us. Isotopic information that's telling us that methane is getting isotopically light over this time period. And so there's been a fair bit of inverse modeling work that's been done to try to estimate the various emissions of methane and the trends in those emissions over the last 10 years to try to tease out if we're seeing changes in the biogenic source of methane from wetlands that could contribute to the changing isotopic concentration. Or if it could be, as some people have proposed, due to the fact that there's been an increase in The fact that there's been an increase in fossil fuel and a decrease in biomass burning, fossil fuel would be more isotopically light than biomass burning emissions. So there's still a fair bit of uncertainty as to what the various sources are that contributed to this pattern that you see here. And as I said, there's just a tremendous focus on trying to quantify the trends in these emissions. Now, as Dadman mentioned yesterday, the Bayesian approach is one of the most widely used approaches for inverse modeling of For inverse modeling of emissions of trace gases. And in this approach, you start with some prior estimate of the fluxes. And this is for CO2, and I thought I was going to give a more general talk, but the same thing applies for methane. So you start with some prior guess as to what you think the fluxes are with some uncertainty, and you pass those fluxes through an absurd model, which you assume is perfect. So it's a perfect mapping of the fluxes into the concentration space. So you have your state. So, you have your state representation of the emissions, and then you compare that to the observations, given their uncertainty. You then try to get a new estimate of your fluxes that represents a combination of your prior information and the information from the accumulation system. So if you sue Gaussian error statistics in this Braysian framework, that optimal estimate can be obtained by minimizing your cost function, which is the cost function you received in many of the talks yesterday. In many of the talks yesterday, where we were looking at the differences between the observations, y, and our model representation of the observation. So, our model here, which is GSK, evolves our state X forward in time given our model parameters P, which are the emissions that we specify. And then, of course, we have our prior constraint in our analysis. So, the optimal estimate of the state that minimizes this cost function and maximize our posterior PDF, given that I'm looking at Gaussian statistics. Given that we're looking at Gaussian statistics, is given by this instruction. Now, over the last 20 years, one approach that's really, I think, emerged as a very useful tool for estimating emissions, especially for gases like CO, methane, CO2, is the 40-bar scheme. And in the FOX estimation problem for 40-bar, it's a bit different from the way the 40-bar device is used in the bar filter prediction, but the idea. By our follow-up prediction, but the idea here is you're optimizing some initial model state as well as the model parameters in this case, the equations of the fluxes. And yesterday, Hendrik talked a fair bit about the need to optimize both the state and model parameters in the concept of the version problem. Now, unfortunately, in our community, often the initial state is not included as part of this optimization. And Hendrik talked a little bit about some of the challenges. Hendrik talked a little bit about some of the challenges involved in not doing that. But often we just optimize for the fluxes, so the control variable here is just the model parameter. And that requires people taking different approaches, sometimes very ad hoc approaches, to try to get a suitable initial state. And it's important to try to get a decent state because if you have any discrepancies in that initial state, or you have discrepancies in the evolution of the state from x0 to From x0 to your observation time, those discrepancies can be detected onto estimated fluxes, which will pose a problem. And yesterday, Brad made a point that if you think your model evolves your state perfectly, you're fooling yourself and made the case that, well, we really shouldn't be looking at flux, doing flux estimated fluxes. But I think there is a way around that, and that is to use weak constraint data for our inverse analyses. And here, Analyses. And here I'm following that framework from Tremol√©'s 2006 paper. And the idea is that as your model evolves to stay forward in time, from time step i minus 1 to time step i, you will have these model errors, these beta terms, that will manifest within the model. And so you can try to account for that in the context of the optimization problem. So you have this extra term here in your cost function that accounts for the model error. That accounts for the model error. And the Q here is the model error covariance. And in this case, this is full weak constraint 4D var. Your control variable here is a full 4D state vector. Now, depending on your particular application, this may be expensive to do. And so one approach that is recommended is to solve for the initial state and these model forcings at each time. Model forcings at each time step. And so your control vector is the initial state x0 plus the forcings that are added at every time step. And this gives you a fair bit of flexibility. You can solve for the forcing at every time step, or you can solve for the forcing over some number of intervals in your assimilation window. So in the extreme case, where you can solve for one value of eta over your whole assimilation window, that's effectively getting a bias. And though that's effectively getting a bias correction through model trajectory over that simulation, the schematic here from Premalit shows a case where you have three intervals in which you're optimizing your ADA over the simulation window, even though you're adding the forcing every time steps. So the fourth time steps over each of the intervals over which you estimate your model forcing. And this is a way, you know, in this case, you're assuming perfect temporal correlation in your model error forcing over this sub-interval here. In this sub-interval here. But the idea here is that you have a fair bit of flexibility in terms of how you specify these etas for your particular application. But the main point being that it's a way of getting at model errors. And what really excited us about this reconstrained 4D bar approach was this paper from Berber from 1989, in which he said a variational assimilation technique is presented which continuously Is presented which continuously adjusts model solution by introducing a correction term to the model equations. The technique is essentially a modification of the agile technique. This variational continuous assimilation technique optimizes the correction to the model equations rather than the initial conditions as is done in the agile technique. And what's really key is this sentence here at the end that says as a byproduct of this variational continuous Variational continuous assimilation technique, an empirical correction for the models, systematic errors is produced. So, this is what we were really motivated by: this idea that we could use this approach to get some handle on the systematic errors in our model and hopefully improve our flux equations. We know that the model is not perfect. There's a lot of work that has been done suggesting that there are clear transfer errors in the GSCAM model, and hopefully, by the end of the year, GSCAM model, and hopefully, by the end of the talk, this will convince you that those errors are there, and we have a good way of mitigating some of those errors. Okay, so for the results that I'm going to show you, we used the GSTM agile model in which we implemented this weak constraint 4D bar. And I'm going to show you results in which we optimized the methane state and emissions at two different model resolutions at 4 by 5 degrees and 2 by 2.5 degrees. And you'll see why we And you'll see why we focused on these two different model resolutions shortly. And the model was evaluated against independent observations from the TCON network, so it's the total carbon column observing network, and also the Atmospheric Chemistry Experiment Fury Transform Spectrometer, ACENTS, which is a Canadian satellite that flies on the SISAT instrument from ACE, looks at solar occultation of the limb. That solar occultation of the limb, so you can get very good vertical resolution throughout the stratosphere and the epitroposphere. And you can take advantage of that profile information at methane to try and evaluate the model. So what are the key assumptions that are going into this that might help you interpret what you're going to see? Here, as you can see from my expression in the cost function, we did not optimize the initial state. We're optimizing the emissions as well as these corrections. As well as these corrections to the state. Instead, we ran a strong constraint 40-bar inversion to estimate methane emissions and use those optimized emissions to get an initial condition that was consistent with the observations. But more importantly, we wanted to make sure that we have exactly the same emissions in both assimilations at the two different model resolutions. But we could have optimized the initial emissions. Optimize the initial conditions as well. Our specification of Q is incredibly primitive. We assume it is diagonal because when we started this, we had no idea as to what the structure of Q could be. So the most conservative thing is to assume a diagonal Q. We specify an error of 50 ppb, so you'll take the square root of that to get your... So, you'll take the square root of that to get your variance, and we played around with different values of Q from anywhere from 0.5 ppb all the way up to 2 ppm and look at the resulting cost function to get an estimate of what we thought would be the optimal cheese for the TUs. So it is quite true, but hopefully you'll see that for this particular case, it is suitable for giving us essentially a first-order direction to the model biases that we know on the right. The forcing terms, these eta terms, we assume were constant over a three-day period. So they're applied at every model time step. For the 4x5 model, that's every 30 minutes. And for the 2x2 and a half degree model, that is every 15 minutes. But we keep the forcing terms constant over a three-day interval. The motivation for that is that we absolutely do not have enough information from our satellite observations to To constrain the state at every mobile timestamp. So we're simulating GOSAT retrievals here over land from February through May of 2010, and GOSAT just does not provide enough information for us to optimize these forcing terms at every time step. The GOSAT repeat cycle for the orbit is about three days, and so this seems to be a reasonable Seems to be a reasonable time scale, and also we're really interested in looking at synoptic scales, so errors in synoptic scales for all of our work, for the work we do here with methane, CO, as well as CO2. And so often we'll assume these forcing terms are constant on time scales anywhere between three days and sort of give us a mean bias correction on these synoptic time scales. For the prior For the prior fluxes, we assumed a uniform emission error of 50%, and the observation error covariance matrix R is based on the retrieval uncertainty from the ghost sat retrievals, but we then inflated them to match this ghost sat scatter against the TCON observations. And the idea here for with all of these, we try to make sure that our estimates of Q and the inflation on R And the inflation on R ensured that we get a decent chi-spread statistic and get reduced chi-spreaders close to one. Alright, so what do the results look like? So on the left here, I'm showing you the monthly mean methane column abundances, and these are the differences between the model and build set. So the top panel is for March, April, and May. It's a bit small. It's a bit small. I'm not sure why it's been projected so small. But these differences here are minus to plus 20 ppp column. So this is the prior model without any assimilation, but in the NWP world you may call your forecast. And you can see there's a fair bit of bias structure. It's plotted this way so that it's easy for you to see that the model is biased high against those at the high latitudes or Eurasia. You see a negative bias in subtropics at a high bias. In subtropics and a high bias over central equatorial Africa, and of course over East Africa. When we simulate the NOSAT data to optimize both the state and the emissions, this is the residual that we get. So this is the difference between the posterior model after reconstraint and DOSA. So you can see these differences are significantly reduced. And then on the right, the right column is the same picture, but when we estimate only the fluxes. We estimate only the fluxes drop and straightforward. So we do reduce some of the biases. So you can see these discrepancies here in what might be the methane emissions being reduced in the strong constraint. But you can see that there's a fair bit of residual structure. There's still high positive bias in the optimized model in the high latitudes, or they're in aneurysia, and that sort of persists. Now, if you look at that structure, you can see that it. If you look at that structure, you can see that it is larger in March than in April, and that sort of should give you a hint that this probably has nothing to do with emissions. And what we've identified is that this is due to the polar vortex. The model is 4 by 5, and at 4 by 5, you can't capture the isolation, so speaking of vortex. In winter, the vortex has very strong descent, which brings down air from the upper stratosphere and mesosphere that has very low methane concentrations. And because the model is not capturing that methane, And because the model is not capturing an isolation of the vortex, we have excessive methane in the pi latitude compared to the altitude. And this strong constraint simulation cannot correct for latitude. And you'll see that more clearly here when we compare the model against independent data. So, this is the model compared to ACE-FTS profile information in the stratosphere, average from February through May. So, this is a zonal mean. Through day, so this is a zonal mean comparison in stratosphere. And you can see that the model is bias high against PCFS data in the extropical stratosphere and is biased low in the tropical stratosphere. 4x5. And we've known from the stratospheric community that at 4x5, it's very hard to capture the vertical gradients across the triplet velocity. And so this perhaps is not surprising. When we do this reconstraint assimilation, Do this reconstrained assimilation, we can correct for these biases. And the plot on the right here is showing the posterior field after we opticalize the state and the emissions minus the prior field. And you can see the reconstraint is reducing this positive bias that we have here in the SPSF theta and increasing and correcting this negative bias that we have in the products. This is the same picture, but when we optimize only the fluxes, and you can see that there's no change. And you can see that there's no change relative to the prior to the stratosphere. And that's not surprising because you're optimizing just the surface fluxes, and there's no way that change in the surface fluxes can lead to a change in the extropolar stratosphere, given the fact that the median age of air at this location is about five years. So that's the mean transit time from the troposphere to that position in the extra tropical or stratosphere. So in the context of a four-month inversion, there's no way you can make a correction. There's no way you can make a correction to the state at that altitude than magnitude. So, what does that bias in the stratosphere really look like in the column? And can we relate that back to the prior biases that I showed you? So, this is the same picture. The left and center column is the same picture that I showed you before, but now I've added this column on the right. And this column is the projection of that structure. And this column is the projection of that stratospheric bias on a closed column. And we calculate that by optimizing, running the reconstraint to optimize the emissions and the state, but only in the troposphere, and leave the stratosphere untouched. And then we subtract from that our reconstraint analysis where we optimize the troposphere as well as the stratosphere. And so you're seeing that stratospheric bias in the prior that's not adjusted in this particular tropospheric home we can see. And you can see his bias. And you can see its bias high, the high latitudes, low in the tropics. And this latitudinal structure matches really nicely the latitudinal structure that you see in the population. So if you don't try to correct for this bias, this will get projected onto your fluxes and lead to discrepancies in the analysis. And what we found in this analysis is that if you take the 4x5 degree model, you run it. Degree model, you run it with a strong constraint forward ER without making any attempts, any ad hoc attempts to correct for the biases, you actually end up, you can degrade the quality of the model analysis, the model symmetric analysis. And in particular, it's at 4 by 5 that these biases are showing up. So here you're seeing this is the zonal mean distribution of methane. Distribution of methane. The colors, contours give you the concentrations in PPV, and you can see the isotropic isotropes shown here as these thin black lines and the tropical masses here. On the right, this is showing the total column differences between the model when we run it 2 by 2.5 degrees and 4 by 5. And you can see that at 4 by 5, the columns are actually higher than at 2 by 2.5 degrees. And that's that stratospheric bias that I showed you. Bias that I showed you. So you see this really clear latitudinal bias between the two resolutions, even though we're running the model with exactly the same emissions at 4x5 and 2x2.5 degrees. So at 2 by 2.5 degrees, you have less smearing of information across the tropopause, and so you get less smaller bias in the extra-tropical lower stratosphere. If you look in the troposphere, this is now showing the distribution of the methane differences between the two resolutions. Differences between the two resolutions. I should mention this is for March of 2010. So it's zoomed mean difference. If you look in the high latitudes of the extra tropics of the northern hemisphere, you see that the 4x5 model has less methane in the free troposphere than the 2x2.5 degree model, but it has more methane in the boundary. So this is consistent with some prior work that we did, and others hard. And others, Harbridge suggesting that a vertical transport is weakened as you go to coarser resolution. And it notes that 4x5, too much methane is being trapped in the boundary layer, and not enough is being exported into the pre-troposphere. And there's also some evidence of some discrepancies in particle transport here. So we can now look at the forcing terms in the troposphere. So this is these eta terms that we apply at every time step and average now over Have an average now over this March through May time period. So it's the mean correction to the model state. This is the lat-long plot in the upper troposphere interpolated to 300 hectopascals. This is at the surface. You can see the strong downward correction of those high biases that I showed you earlier in the talk over Central Africa, over East Asia. And that extends into the pre-troposphere over East Asia and Africa, but you see down. East Asia and Africa, but you see downwind over Asia, we have an increase in methane over the ocean. And similarly, for North America, we see a decrease over the continent and increase in wind. And it's important to keep in mind that we're only assimilating GOSAT data over land. We have no ocean data. So these corrections to the state is due to the effects of transport acting on the information that's being assimilated from open continents. If you look at the cross-section at min latitudes, at 30%. Cross-section at min latitudes at 34 degrees north. This is showing the reduction here in the column of the continent. This is Asia, altitude, longitude, and then you can see this increased downwind of the continent. Over the equator, this is the same thing. This is a cross-section across the equator. This is Africa here. You can see the reduction here. And what looks like increased methane in this sort of shallow convective outflow of the continent of Africa. Flow off the continental Africa. So, this to us looks as though the reconstraint is correcting for what we think is new export of methane from the continental nation regions to the free troposphere. And of course, if we do look at the forcing terms at 2 by 2.5 degrees versus 4 by 5, you actually see that the forcing terms are smaller at 2 by 2.5 degrees than at 4 by 5. And that's consistent with our understanding that the vertical transfer at 4 by 5 is actually weaker than at 2 by 2.5 degrees. Weaker than the 2x2 and a half months. I'm going to end in two slides. So if you compare the model now against the TCOD data, this is looking at a number of TCOD stations in the high latitudes and subtropics, and then we have a couple in the southern hemisphere in Australia and New Zealand. This is the mean difference in the prior, the strong constraint 40 bar and our weak constraint 40 bar, the standard deviation and the correlation. And you see that the And you see that the reconstraint does reduce the bias relative to the TCON data compared to the prior and compared to the strong constraint. In some cases, with the strong constraint, the correlation actually gets worse, but with the weak constraint, we do improve the correlation everywhere. And what we found, as I mentioned, is that if you run the strong constraint at 4 by 5, the posterior model does more poorly against the decon data than if you just take the 2 by 2.5 degree model and run that. By two and a half degree model and run that without doing any simpler function. So running it at four by five, doing the inversion without accounting for the biases, degrades the quality of the model simulation. So just to summarize, previous work had identified that we have resolution-dependent biases in the model, and at 4x5, that the vertical transfer is weakened. And with our weak constraint 40-bar scheme, we're able to detect and partly reduce those biases. Now, going forward, we need a proper Going forward, we need a proper specification of Q so that we can better distinguish between signals that we see from the surface emissions and other types of model errors. But just going back to Derber's paper, our results, we think, suggest that this reconstrained approach is a valuable tool for diagnosing systematic errors when you're doing these flux inversions. And going back to Brad's point, if you run these inversions and you assume your model transport is perfect, That assume your model transport is perfect, you are deceiving yourself, and you have to question what it is that you're getting in the end if you don't try to correct responses. So, thank you very much, and I've got about five minutes over time.