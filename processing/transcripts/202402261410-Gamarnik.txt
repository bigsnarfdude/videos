Something happened. It's in progress. Okay. All right. No, I turned it on the first. I think if it goes over, maybe it stops. Not over by 15. She said it, but she said it's maybe. She's good. She's a 15-minute grace for me. Good. Take your thanks for the invitation. I'm going to talk about sparse renewed mean field models and back, but given that that's a half-hour talk, we might not make it back. It's a one-way trip, not a two-way trip. It's a You trip. It's a well, let's see. First, I have to depart. So let's depart. It's a joint work with Lengla Steborova and her student, Jasmine Andy Berry Young Fellow. This talk is going to be more about perhaps a method than concrete than results, and a bit of a surprise. First, the only two models Two models. I'm going to consider these dense and sparse random graphs. Probably don't need any introduction. Dense graphs and sparse graphs. You know, I feel like tenning those is like explaining how to buckle up in the airplane. Everybody knows that, but you know, I'm not under sort of the airline obligation to do that. So, you know, but if this is something I haven't seen, you know, raise your question. As it turned out, it has been documented. It turned out that it has been documented very, very well in the recent years. And often, if you want to solve some kind of a problem, optimization problem, not necessarily on sparse random graphs, it is easier to look at the dense graph and then somehow translate your result from dense graphs to sparse graphs. And this method, which is in the heart of it as the Lindeberg's interpolation method, has worked remarkably well. Remarkably well. It's just Niederberg originally invented this method to prove central limit theorem, but it really goes above and beyond a central limit theorem to the extent that you can say a lot of things about sparse graphs. And I'll mention one of the highlights of those things. Now, perhaps surprisingly, and this is what this talk is about at a high level, is that the opposite is also true in the sense that for certain more problems, That was certain more problems that I will describe and introduce, specifically the so-called friendly bisections of random graphs, AKA, metastable states in stat physics model, and those turned out to be more or less the same thing, even though coming from different community. It is easier to analyze them in spark graphs and then use Inlindeberg's method to translate to dense graphs rather than directly analyzing them. Rather than directly analyzing them for density. That was a surprising thing for me. I don't know what a friendly bisection is. I'll explain. I'll explain. Right. So the only thing we need to know at this stage is G and half a G and D only. That's all we need to know. Everything else to be explained. I wasn't sure what was included in the buffalo enough, like because I'm not sure myself. All right. All right. And if this was if this was kind of one curious case of going, you know, sparse being more tractable than the dense, that would be fine. But actually, there are more examples. I will not go into those examples, but some examples in the analyzing quantum algorithm turned out to be easier to analyze than sparse random graphs and then translate it back to the dense graph rather than analyzing them directly from the dense graphs. But that's a different work. I'm not going to talk about that. Right. And also, Right, and also I want to discuss. If I had time, I would discuss some work in progress. That one is more canonical, that's too sparse, but I'll skip that. All right, so two models I have introduced. If you're from the physics community, then some of you are, if it's more familiar to think instead of dense random graphs, think about spin glass models, like two spin glass models. Turns out that everything that I'm going to describe in the problem. Going to describe the problem is equivalent to that one as well. And I'll sort of freely go between the two. Right. Then sparse graphs, right. So as it turns out, and I'll give you one example, as I said, optimization on the sparse graphs can be related to the optimization on dense graphs. In the sparse graphs, you have two parameters, size of the graph, average degree. If you take the double limit, you get the two things are equivalent. You get the two things are equivalent. And one of the first examples of this is this very nice paper by Demo Matanari and Sen. And then it's a follow-up work by Sen alone, who has basically realized that they're really using interpolation method. Who showed that if you look at the MaxCut graph on the sparse graph, you can relate it to the Max Cut problem on the dense graph. Problem on a dense graph in the following sets. So, max cut, this is just a formalization that you probably all have seen. That you need to split the vertices, two groups, max mark the cut, blah, blah, blah, call it the max cut of the graph. In the spin glass model, it's relatable to another object, which is this quadratic form on quadratic form, given this by the coupling on the medium. This is by the coupling in the matrices. This is for that physics friends. And if you, for the max cut, first you need to center it. You need to subtract the average number of edges that average cut cuts. That's the right thing to do. In the sparse random graphs, it turns out that the right scaling you need to do is normalized by the number of nodes and normalized by the square root of the degree. And then, if you take the double limit, it turns out that. You take the double limit, it turns out that this is the same thing as looking at the dense graph, subtracting the average cut that you get from the typical cut, and normalizing by n to the 3 half. And those two things, lo and behold, converge to the same limit, and that's a celebrated constant by Parigi, scaled by 2, because the way you couldn't really think about this is that Think about this is that the max cut value is preserved under subsampling. Because I can construct G and D over N by just like sampling G and one half at like some UD over N era. Right, although it's not necessarily preserved sample by sample. I'm not sure about that. The value is being preserved. Not in solutions. It's not good that you know because you know graphs with spectral gap you know admit uh sparsification. You know, a bit sparsification? Yeah, but I'm not sure it will work all the way to the custom degree. I see sparsification might actually be a log n, but the connection to sparsification is something to be explored. I don't think it's sort of the fully explored. I think there should be something there. Anyway, because it's root end and the denominator in there, it's like they can get better max cuts in a relative sense when the graph is sparser, right? Person, right? So there's Rotten, perhaps. Yeah, I'm not sure. So the scaling here is a little funny because you take and the right deep. Maybe I missed it. Alright, so sparsification to be explored. Now I had the whole bunch of slides to right, when did I start which and when should I end? So I started 2.15, should I organizers help me to keep the track of time? Help me to keep the track of time. Yeah, quarter quarter till. Quarter to. No, no, no, sorry. What did you start? What did you say? Yeah, so 242. 242. Okay, good, good. Right, so I have the whole bunch of slides here, which is which gives a sketch of the idea, but then I have to do some random sub-sembling of my slides. Which is a bit unfortunate because. Which is a bit unfortunate because I think this idea of using Linderberg interpolation and sparse to dance, dance to sparse is really beautiful. And I found it useful over and over again. I was hoping to share it with you, but I don't have time. So I will do that up to a certain extent and then switch to the friendly bisection, which is the focus of this talk. Now, the idea is. Now, the idea is that, ignore the slides, just listen to me. This slides is for myself, not for you. So, the idea, the fruitful idea, is that not to think about graphs as graphs, but think of them as a bunch of random variables. And so, in one case, it's a bunch of Bernoulli random variable probability half. In another case, it's a very biased Bernoulli random variable. Of course, I'm not telling you anything new. We knew that already. But what's useful is also. But what's useful is also to do the careful centering and rescaling of those variables. So for example, for the dense graph, it turns out to be useful to scale them by one over them. And that way, and also recenter. So that way you get mean zero variance one on n random variable with some kind of a third moment controlled by this quantity. And then you can relate your max cut to the Can relate your max cut to the imbalance of balance of the cut in some way, which is connected to the energy of the spring loss model in some way. So, this is you can reformulate your original Max cut after rescaling in some other object that looks like it's fine. And that's the quantity that we know that converges to Parisi const. For the sparse graph, what you do is similar. You rescale them in a way so that it makes mean zero. It makes mean zero, variance one. So here you have to do it slightly differently, of course. So for every edge, you replace it by this quantity, and for every non-edge, you replace it by a different quantity, so that the mean value is still zero. Variance is one of the n, and then you have some kind of a growth rate on the third model. Okay, and so the goal is to make in both. So the goal is to make in both models mean 0, variance 1, random variance. That's it. And then what you do is you have the matching of the moments. And what you do to prove this theorem that I have just restated is to first you soften the optimization max maximization problem because maximization is a discrete object. You replace it by the partition function for sufficiently high inverse temperature beta. Inverse temperature beta. So get an upper lower bound on that. And work with the partition function, and you want to show that the partition function two models coincide up to appropriate rate. And that you do by Lindeberg's method, where you replace variable one by one, do the Taylor expansion of one variable at a time, and first and second moment match, because that's what we have done. That's the way what we have done, and the third moments you have some bots in both cases. And if you collect the things correctly, I have the whole bunch of slides, I promised, I'm skipping those. And when you put things together, you get some kind of collected total error. You have an expression that depends on the relaxation inverse temperature parameter beta. It also depends. It also depends on the degree. And it turns out that there is a sweet spot here where you take just the right temperature. It shouldn't be too high, it shouldn't be too low, it should be just right. It should be growing, but not growing too fast, so that when you compute the total error, you have the error that goes to zero as d goes, and goes to infinity, and then d goes to infinity. So, yes. What is the property of max cut that you used to do? The property of max cut. The property of MaxCut that we use here is that it is expressed. That's actually a question might be deeper than you would think. I'm trying not to be offensive. So the property that MaxCard that we use here is it can be written as a quadratic form. But for example, I try to use this method for doing some similar thing on triangle count and doing some kind of And so and and do some kind of analysis theory version of the triangles, and then it seems to break down. So, this this of course it's not just about MaxCard, but some nice way of writing your objective function in terms of the variables is helpful. Maybe I didn't answer here. Well, ultimately, you're bounding some Taylor in terms of the remainder and some Taylor. That's right, that's right. So, if you've seen it as a computer scientist, you've seen the instantiation of similar ideas in the Of similar ideas in the invariance of the logical polynomials and things like that. And that's a kind of similar idea. So you need to be able to control Taylor expansion. Okay, good. All right, so now, and the method is very robust and it can be used even in the quantum setting. Good. So now I want to talk about a model that this method has been useful. That this method has been useful, but surprisingly, it was easier to work with the sparse models as opposed to dense models. And this has to do with the so-called friendly partitions for the combinatorialists, brain more metastability for the physicists. Turned out that those things are really two facets, two names for the same thing. So let me define the model. No relation. No relation. That's a good question. Yeah, well, I wanted to cite. Yeah, this the uh Bray Moore is 1981. I don't know if you were after. I should know more about the physics side of the story, but I can figure it out. Okay. Not just me. So, what is so? Let me use. I have to admit, I prepared this slide for Cambinatori's audience for the first time when I present it. So, it's a little slanted to Cambinatori's drafts kind of language. What is a friendly bisection? It's a very simple idea. So, we say that splitting the notes of the graph into two groups is friendly. If, simply put, the number of nodes for every node, the number The number of nodes for every node, the number of nodes in the same group is larger than the number of nodes in the other group. That's it. So you split it into half-half. And if every node in part A has more neighbors in A than B, and for every node in B has more neighbors in B than in A, we call this bisection friend. Right, and this. Right, and and this can be related to the uh local maxi can be related to this local maximums in the spin glass model. But I will not directly relate it to that. But in a spin glass model, it basically says that it's a configuration, spin configurations, where every spin flip will result in lowering the energy if your goal is to maximize the energy. Or increasing the energy if your goal is to minimize the energy. Goal is to minimize the energy. Those two things are relatable. Let's stick with the friendly partition. Okay, any questions about what the friendly partition is? Let me, okay, what do we know about that? And what's the genesis? Well, as I said, it comes from two worlds. One, Brey Mura already mentioned 81 paper, paper of 81, and what specifically they asked, I'll talk about that in a short while. But back In a short while. But back in 1988, Azalta and Friday, who is a combinatorial, put forward the following conjecture. Sorry, I did, David. Sorry, it just took me a while to format this question. But like, okay, so you're sort of asking to have a local optimum for min dissection? And but it's not exactly a local optimum for min dissection because you're allowing me this little O and no's that aren't if they're a local opt, so why is that? Aren't it throwable up? So, why is that? Like, could I instead have asked for this to hold literally for every node, or is that something that doesn't exist in these graphs? Or why is uh yes, good question. I'll explain. Because the answers are in a couple of slides. Turns out that it's just this one is more trackable and it's also a reasonable thing. You ask that the power. Reasonable thing, you ask that the partition is friendly for almost all nodes, not all nodes. Why are we asking this relaxation to be cheap? Okay, good. All right, so conjecture by Furedi states a very simple thing. It says that there should, do they exist by sections of the random dense graphs which are friendly? Now, one thing. One thing to realize is that we insist on being a bisection. If it was just a cut, you can take the max cut, and that's friendly by definition, because there's no swapping that will... So if something is not friendly, you swap the node and then it becomes increases the cut value. But because it's a bisection, that makes it non-trivial. So this was a conjecture by Furedi, and it wasn't. Ready, and it was resolved in 21 by this group of authors and in an asymptotic way, in a sense that they managed to construct constructively, algorithmically, a friendly partition which is asymptotically friendly partition. And that's an algorithm, yes? I'm sorry, I'm saying something. I mean, it's a greedy algorithm which flips things. Oh, but it wouldn't necessarily be a bisection. That's the problem. That's the problem. It's not bisection. That's the problem. It's not bisection. Good. Yeah, to me, it's also sort of insisting on being bisection kind of sounds funny, but it turned out to be a useful way for some other reason. This was removed. This asymptotic property was removed later in the more recent paper after this work. In fact, in the remarkable tour de force, these two. To the force, these two guys manage to get rid of asymptotic properties and actually sort of fully 40 conjunction. Okay, independently and somewhat earlier, the number of expected number of friendly bisections in uh was computed in this paper. Um and that's actually not hard expectation of the number of friendly bisections is is fairly straightforward computers. Is fairly straightforward thing to do. You analyze the Gaussian jointly. This was done in the Gaussian model, not the G and Hawk, but it's besides the point. An expectation is not hard to do, so you would grow, but we know the expectation might not reveal the truth because of the second moment phenomena, so we need to really see what's going on. That paper, though, as I said, it's actually algorithmic. So they actually construct the partition, the bisection. The partition, the bisection that is asymptotically friendly. Unbeknownst to myself, it turned out that we solved this problem back in 18 for the sparse random graph. So we just didn't even know. For us, the bisection and the local optimality was local maximality was a trick to try to improve the max cut value in sparse random graphs. But by the way, it turned out that we proved that trendly bisections exist in sparse random graphs. Exist in sparse random graphs. There is a certain technicality here with the sparse random graphs because of the presence of the isolated notes and the notion of friendliness. There's some technicalities here, but certain asymptotics, let me not go sort of, let me not dwell on that. Right, and continuing on the background, and this is how we sort of started this work, Lenka. This work, Lenka and her students have looked at the friendly bisections of several models, including random deregular graphs, and analyze it using Stat Physics method. And I'll come back to this paper, and deregularity comes up in some interesting way. All right, so right. So, first, let me further, maybe at the risk of annoying you with the notations and With the notations and changes of the definition a little more. Let me introduce the Relux is a version of the friendly bisections, which is kind of a strong friendly bisection, where we insist not only of one quantity being too larger than the other quantity, but we also want the gap to be sufficiently large. As it turns out, it's not too hard to see, but it takes a little bit of work, that in the dense random graph, the In the dense random graph, the right scaling is that the gap should be of the order square root n. So we normalize this by t square root n and we insist that it's at least h or some constant h. Turns out it's the right scaling. Again, so for some maybe this is obvious, others might be mysterious. Let me not go into that. So and we say again, bisection is asymptotically age-friendly. Age-friendly. This is true for all nodes or almost all nodes. And then the question is: do these age-friendly bisections exist? And one of the interesting things is that, as I'm going to come to, in short, is that once you insist on H being positive, the algorithmic proof that those four authors prove. That those four authors proposed to begin with breaks down and it seems like this problem is algorithmically might be hard. Even though when h is equal to zero, it's apparently tractable because they provided the algorithm. So I think I'm jumping ahead with this a little bit, so let me get to the point. As it turns out, the H-Frank As it turns out, the H-friendly bisections exhibit, they do exist, but they exist only up until a certain critical value that we've computed is 0.175. So they do exist when h is below this number, and they do not exist when h is larger than that. Okay, so it's not that result, which is interesting. Also, I'll Also, we also were interested in algorithmic tractability of that. It turns out that the model exhibits overlap gap property when h is at least some constant h, which we also compute. And I'll explain overlap gap property later if I have time. Probably will not. But basically, it's some kind of indirect evidence that the algorithms should not exist. That the algorithms should not exist where h is at least this number. But going back to the whole sort of premise of this talk is the following. A tempting approach and most natural approach would be to use the second moment net, which you just compute the number of age-friendly bisexuals. The first moment, it's easy. Bisection, the first moment, it's easy to do, second moment. Turns out the second moment is a nightmare in dense random graphs. But it turned out to be easier in sparse graphs. And that's a bizarre thing, and that's still something which I don't fully understand. So is the point one seven five where the first amendment argument? Yeah, so this threshold for one side of this threshold is just the first moment argument. The other side of this threshold is need to do the second moment argument, but what we do instead, we do the first and second moment argument on the sparse graph and then translate. And the reason is that I will just draw a couple of circles here just to explain. Alright, so you have a partition and you Partition and you friendliness means, okay, I have more nodes here than neighbors here than neighbors here. In a dense graph, okay, this is binomial, this is binomial. You can compute the likelihood that one binomial is larger than the other, of course. But the point is that is that correlated with the same property for some quantum? And there is a correlation. It is of the order one of an n, it's small. 1 over n. It's small, but it's there. It turns out that if you drop this correlation, you get the wrong result. You cannot ignore this correlation. This 1 over n correlation between independence properties of two nodes is important. You cannot afford to draw. But when you do the same thing in the sparse models, you can draw. And it does not affect the result. And this is something that I still don't fully understand. Well, part of the explanation. Well, part of the explanation is that in the sparse random graphs, you have a configuration model, and this node, the number of connections here and connections here, is Poisson distributed. And it is, so it's Poisson distributed, and across different nodes, it's practically independent. So the only disturbance to independence is the potential presence of the edge here, but somehow in the sparse random graphs, this does not spoil your Not spoil your arguments. And it doesn't at that end spirit. And yes. So you're saying, you're not claiming that, like, because you're working with the configuration model, you don't have to deal with this. You're saying you're not working with the configuration model, you're making some kind of approximation to it, but you don't have to take into account the neighbor probabilities of the. No, no, we we do work in the configuration model as part graphs. So then why isn't it the case that the configuration model already takes care of the probability that that edge is there? I can sort of Yeah, yeah, it it it does. It's just it's just there's no configuration models in the dense graphs. That's why the difficulty was to work with the dense graphs. Okay, but it's not like you're dropping the effect of this edge, it's just that it's easier to deal with. Yes, yes, yeah, yeah. We're not dropping the edge. Yeah, yes. It's just somehow the impact of this potential edge is just impact potential correlation there is less impactful in sparse graph than in dense graphs. I don't understand why you're saying this, because if if I'm working with a configuration model, Because if I'm working with a configuration model, I'm completely addressing the potential impact of that edge. Like, a different way to say it means that it's easier to work with, to deal with the effect of that edge in the configuration model than in the dense graph. But it's not that it's not impactful, in the sparse graph. It is impactful. Because the Poisson, which is the Boltzmann-to-bins model with the Poisson approximation, is still an approximation. But somehow, when you do this approximation and then pretend these kind of indegree-out degrees are independent across different modes, this approximation turns out to be good enough. Yeah, maybe you might even understand it better than I. I don't have full understanding of what's going on. But somehow this worked in the past, in this parcel, and seems it's workable in the dense graph. It's just a nightmare to work with the second moment. It's just turned out to be night. out tonight. Okay, so I and then you have to pass it from the sparse to dense graph, but this is done by Lindeberg's interpolation method. There are some caveats here, additional caveats that we had to deal with that didn't show up in other simpler implementation of the Lindenberg because we deal with the indicator functions and so on, but let me not bother you. Functions and so on, but let me not bother you with that. I have to wrap up the talk. Ah, so just two very quick things I will mention. One is about the overlap care property. I mentioned that it is there, apparently above some threshold H. But Lenke and her co-authors showed a very nice argument showing that this explains why in sparse regular graphs, Regular graphs, you have the overlap gap property for every positive H. And this is based on some kind of expansion argument. So it's heavily based on the expansion property of the red and deregular graphs. So in all likelihood, there should be OGP for all strictly positive H. We are working on trying to complete this because with random regular graph interpolation, it's trickier to work with, so to be seen. But at least there is an evidence. But at least there is an evidence that this problem is algorithmically tractable when H is zero and algorithmically non-tractable when H is strictly positive. That should be the case, but to be confirmed. So there's no algorithm right now between restricted positive H, but below the HP. No. No, there's no algorithm. We don't anticipate that it would be I mean, it would be interesting to resolve it one way or the other. With one or the other. But we don't have a proof of 4GP either. For H, only if H is large enough. Moment method breaks down. And the other thing that I wanted to mention is that is the following. This is now more of a physics perspective. So Bray Moore also, apparently, these guys studied lotonaximum, well, they studied loton maximums of the Batonian, essentially, sort of, which translates into friendliness, obviously. But in addition to that, they wanted to see. And in addition to that, they wanted to see at what energy friendly partitions exist. What it means is that, okay, if I achieve the friendly partition bisection from my graph, what can I say about the number of edges going across? For example, could it be that the balance partition says that globally this looks like a random cut? It turns out the answer is no. If the friendliness of the partition immediately in the language of physicists, friendliness of the Friendliness of the partition immediately implies the energy should be higher and on the same scale as a ground state. You cannot have the same energy as the random partition. That has to be high. They effectively proved that using the first moment method and showed that the constant that highness constant is this constant. So global upper movement is of this scale, and you should have the energy at least that much. You should have the energy at least that much for it even to hope to be appearing. As it turns out, so we've proved that using the second moment method from the sparse graph and translating, that that's actually a correct rational. So the C star is not just a bound on the energy at which the friendly partitions exist, it's actually a right bound. Okay, I think I've tortured you enough with my random sub-sample of slides. Thank you. So, I mean, there is a configuration model in Denz case that just has a ton of self-educated multiple edges. Or equivalently, it's a model of weighted random graph where everybody has weighted edges of some distributed weights, right? It's a weighted graph. Weights, roughly speaking, of weight one-half. So, does Lindenberg's method let us directly compare that model to the traditional GN one-half in a way that would preserve these costs? Boom. It's an interesting idea. Yeah, I have potentially. So, you're saying that don't don't bother with sparse learning graphs. Do the dense graphs, but just include the loops so that you have this kind of in the Have this kind of dependence. But you do the configuration model, that will include the loops and some kind of. Let's ignore the loops. But yeah, I mean, there'll be plenty of parallel edges in the resulting multi-graph. So, yeah, so essentially it's a weighted graph where each weight is Poisson with mean half into million. But I'm just wondering, like, if that But I'm just wondering like if that matches the again this moment matching arguments. Yeah, I think that's entirely possible and it would be probably even cleaner. Because with sparsity there's an extra headache with the WN symptotics and so on. That could well be the case. David, in this result, if the cut was random, what would you expect for this energy value? Is it like order of n? Like what was the? Is it like order of n? Like, what's the gap on it? Yes, it would be order of n. So basically, the point here is that it's like square root n factor larger. Like, that's how optimal is. Yes. Yes, yes, that's true. Cool, let's put some questions here and take the rest offline.