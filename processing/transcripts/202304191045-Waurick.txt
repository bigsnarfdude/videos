Okay, welcome back. Our second speaker today is Markus Fraurich from Freiburg. And Markus will speak on evolutionary equations with maximal regularity. Please. Thank you very much, Lucy. And thank you much, all the organizers of this wonderful meeting here for giving me the opportunity to speak. And the topic I'm going to speak about is evolutionary equations. Evolutionary equation for maximum regularity, so don't worry about it, everything will be explained. It is about a subject I was being aware of when I was reading the archive newsletter. And once in a while, some result pops up, and you think, oh, that's interesting, but I can contribute nothing to it. But I'm following the story. And in the 2014-ish, In the 2014-ish regions, there were several papers that were conserved with maximum regularity, and they were of differing complexity, but not in my ballpark, so to say. And what was improvement there, improvement there, and it was kind of the relief that I felt when I followed this story. And to start off with, what is To start off with, what is maximal regularity? So, what made me intrigued into this problem? I will tell next before I want to say about my plan. And the plan is going to be telling what maximal regularity is, then saying something about evolutionary equations. And eventually, I'm going to talk about the actual title of the talk. And since this is a meeting on the French Fars-Brack equations in the Meeting on the franchise for architectures and operation cancels. Well, I need to somehow fit to the overall schedule of this meeting. And what you're going to see is that the DAE perspective to evolutionary equations makes maximum regularity from our perspective work. So it is rather a feature than a drawback. So something that is nice. So, something that is nice that differential algebraic equations have. And I will also have the opportunity to say something about holomorphic operator pensors, which will come in this evolutionary equations bit. But first things first, we're going to talk about non-autonomous maximum regularity, famous Lyons problem. So, what is the situation? A situation is we have Situation is we have Hilbert spaces B and H, B density divi densely embedded into H, and this then also in turn is densely embedded into B prime. As usual, we give us a final time t. And that's quinear bounded quasi-coercive measurable form. Don't read these assumptions too much. These assumptions are just meant to be there so that everything works. So that everything works. So don't worry about it so much. This quadratic form now generates an operator from L V, from V to B prime. This is basically Ries representation theorem. And then the starting problem or the start of discussion is Lyon's theorem from the 1960s saying that if you take an F in If you take an F in, yes, we should not read the assumptions, but what is one and a half degree? So I'm okay. I will use it for more. All right, so. Alright, so the equation I'm going to look at in this on the slide is u prime b of dots u equals f. So f is depending on two variables, t and some spatial variable, which is hidden in this Hilbert space b and h. And you also depending on two variables morally, and this is a parabolic equation. So the easiest example you should have in mind. Have a nice is dt u minus divergence A t depending on t and x gradient u equals f and your form in this case d of t is the integral over some omega Some omega in open in Rn and you take A T X red u V red U red V that you integrate along and this is for all U V in V and typically if you want to ask for 30 pounds For derivative boundary conditions, this is just H01 of omega. If you want to ask for longer boundary conditions, you would have the whole of H1. And if you have mixed boundary conditions, you ask for something in between, a closed subspace of H1. And as I said, we all prove that there exists. There exists a unique solution to this problem in the space h10t with values in v prime. So that is in the h01 example. This is h minus 1. And Lion's problem now says, well, if I have a slightly more regular f, so if there's not, if this doesn't take values in h minus 1, but in h, so that the underlying Hilbert space is 2 of omega. Language-based 2 of omega, can I then deduce that U actually is an H1 basin 2 of omega? So can I make the regularity of U improve if I ask more for F? And why is this an interesting or relevant question? But if this conductivity here was dependent on U or gradient of U, so some non-linear equation, you would be interested in the solution operator. The solution operator mapping into some more regular space in order to make some compactness arguments work so that you can maybe apply Schauler's theorem or something like that and then infer some fixed point for this non-linear fixed point mapping. Okay, so problem is interesting, problem is important. So what does the confused? So your operator V acts from V to V dashed. Yes. You see V acting in U. So here should lie in V, right? Yes, it does. It is also in a 20T in plus V. I suppress this. I see. Yeah. So then this is a sum in a tube of L C prime, and then it could be in F and P prime, but if I ask something more, But if I ask something more, then it's something more. And why is this maximum regularity? Well, I'll come to this in the next slide, where I present some results of this. Well, just in a nutshell, maximum regularity is this, right? You have a form B, and this B generates some operator family, graphic B, and if you ask for regular F, then U is also more regular. Also, or regular. Then there are some conditions people looked at. Say, for instance, the alpha regularity of calligraphic B. Or and this is something operator theorists will like. So this is the square root property, and the letter K stands for Carto. So this is related to Carto's square root problem. You could ask. Problem: You could ask yourself if you put your calligraphic B as an operator in this middle Hilbert space H, what is the domain of the square root? And somehow if there wouldn't be any coefficient at all, in the easiest case, it's clear that the square root, the domain of the square root is B, but if your coefficients are somewhat different, it's different. Different, it's different, and it can be very hard to prove that this square root property actually holds. And the emphasis also that is uniformly in T okay, so these are some conditions. And that was what I wanted to say a minute ago. Maximal regularity trivially is equivalent, so these B's shouldn't be B's, but rather straight Bs. Straight B's because I consider this operator sum in L2 of 0T with values in H. And maximum regularity says if I take the right-hand side F in L2 with values in H, my U is in the domain of this operator and of that operator. Because this is the H realization, and this is if I have U in H1, then H1, then time derivative of H1 is near two. So that's nice. So this operator is actually already close. This is an equivalent, more or less equivalent formulation of maximum regularity. So what are some results of this now? Well, Lyons also already in 1960 showed that if B is C1, then maximum irregularity holds. And And then dominant deal in 2014. So it is a very large gap between those, right? So the problem formulation was in the 1960s. And in 2014, Dominique Deal showed that there's a non-symmetric B, particular one, not satisfying Cartoon squared property, such that we don't have maximum regularity. And then, and this is in inverted comas, there's also a positive result. And this is a positive result by Ekiana Spina and Demati Wabas in 2010. But when preparing the slides, I wasn't really aware of this result. So I I knew a more general one, slightly more general, by Haag and Wahabas in twenty fifteen, and they asked for piecewise C alpha. Ask for basewise C alpha for alpha greater than one half. But anyway, C alpha alpha greater one half is fine. And then you could ask whether this is somewhat sharp. And in a way, yes. FACLAB proved in 2016 that there is even a counter example for symmetric B, so counter-squared property is satisfied, with C1 half regular. With C12 regulators. Okay, so you could say, well, this is the end of the story, but we discussed all these Holder continuous situations and let's be done with it. But somehow asking for Holder continuity is not, I don't think, is the right space to look at. Because this is, we were asking only for distributional derivatives, so for sophonof regularity rather. So the That something works for Hulver continuous functions is fine, but it's not addressing the question right. So it's not the correct framework. So I would be confused if that would be the end of the story, and in fact it's not. So we have some other further positive results. So put this here. B is in a homogeneous solar scale, so I call this. So I call this condition h alpha and I just write that the double integral of calligraphic V squared over T minus S one plus two alpha dt ds is finite and there's another Finite. And there's another condition which is asking for the form to be of divergence form. So B of t equals minus diff A of t gradient and I suppress the X dependent, so like this. And there's also a V of O condition. And that's Almost all conditions I go to use, so they will always be four. So the supremum over 0t and double integral over e and then you have this a similar a similar fraction as before. As before, but with the coefficients occurring in the divergence form situation rather than the operators. And given these conditions, you could look at positive results. Well, first off, C one half is embedded in H alpha for alpha less than one half. Okay, so this is a fact. And this means that we cannot expect And we cannot expect maximum regularity for alpha less than one half. Okay. And so Dia and Sana in 2018 showed that if you have alpha bigger than one half, maximar regularity holds. So this already identifies somewhat alpha being one half exactly as somewhat a critical scale. Scale. And also, Oshea and Egot proved that if the thing was of divergence form and you have this BMO condition, I'm telling in a minute why this is called BMO, this implies maximum regularity. So if you just look at this roughly, if other was one half here, so this is two, and this is also two, so these conditions might be somewhat. Somewhat the same, or implying one of the other, or something like that. That's totally not the case. And then eventually, Achar and Muhabas showed if it's piecewise H half and a C beta condition, beta strictly positive, and there's a more precise one later, and the Carto square root property, this implies maximum regularity. Okay, plenty of conditions. I'm going to compare these now. I'm going to compare these now. So we look at this equation and then I mention that H alpha is embedded in this C alpha minus a half thing, and that BMO actually is equivalent to the half derivative of A is in bounded mean oscillation. And now you can look at these three conditions and try to join them and look which one implies. look which which one implies which. So h alpha for alpha being bigger than one half certainly implies h1 half and then h alpha since this is alpha is bigger than one half it is it implies c beta for some beta positive. So this meditation holds but this is somewhat okay But this is somewhat okay. And also, if we just think of divergence form, then this BMO condition says that the half derivative is a bonamine oscillation. But if the half derivative is a bonamine oscillation, it's particularly in a tube. So you get that this BMO thingy also implies piecewise on Half. And now, as I said, you might think that this H alpha condition and this BMO condition are somehow, this implies this one, but that's not true. So, neither DZ nor AE imply one another. And in all this, the Carto square property is still there. So, there's something, there's no So, there's nice, good theorems, but there's no order in those theorems to use. So, we wouldn't really know in which box which is fitting. And so this was the story. And in 2017-ish or something, I was asked to provide a talk about maximal regularity at some conference. I didn't have really. I didn't have really a result on maximum regularity. There was something on the autonomous case, but well, and I thought, I mean, I can prepare something. So I was preparing basically an overview over these things. And during that conference, some talks weren't as nice as you would hope talks were. So I was always trying to, okay, so maybe we can get a handle on this non-autonomous bit. So it was computing and making mistakes all the time and thinking. Making mistakes all the time, and you think, oh no, I got it, and I didn't, and yeah. And how I tackled this, or how we tackled this eventually, I'm going to tell you in a minute. For now, I jump completely subjects and talk about evolutionary equations. We have had yesterday in Kristner's talk, the observation that problems with the wave equation and with the heat equation. With the wave equation and with the heat equation, are somewhat very difficult to deal with in a unified manner. So there was problems, yeah, this is working here, this is working there, but it is difficult to connect these. And evolutionary equations is a notion that tries to connect these notions. So, presumably, completely opposite phenomena in Phenomena in mathematical physics or that on the mathematical treatment of these things can be thrown into one huge ball and can be treated within one kind of theory. So in a nutshell, evolutionary equations introduced by Picard form a framework for PDEs, time-dependent PDEs most prominently, such that you can formulate this. You can formulate this, as I said, in a unified way. And also, the equations you're looking at are comparable to one another, which is kind of nice. And I highlighted this PAE thing because of the workshop, maximum reality because of my talk. But there's also other stuff we did in evolutionary equations and where this uniform or this unified way of looking at partial differential equations helps a lot. And what it also does. And what it also does, since the criteria for solvability for evolutionary equations are very elementary and very at the heart of the solution theory, what you experience is that almost always the condition you can find in literature can be shown to be stronger than conditions you come up with in evolutionary equations. So it's really a good way to have a It's really a good way to have a proxy whether a problem can be solved at all. If you can solve it with evolutionary equations, it is most likely it can be solved also with other techniques, but not the other way around. And if you're interested in that, so yeah, so there's a book and it's open access, so it's just one click away. And so if you have some students, they can read through it and tell you what's in it. Yeah, so we produced that on a we tried desperately tried to produce it on the graduate student level so that it is accessible for many people. Yeah, so if you browse the internet, just go there. Anyway, one of the cornerstones of evolutionary equations is the evolutionary bit. So we need to realize somehow the time derivative. And this is a not so A not so uncommon construction, really, but it is dealt with in a very operator-theoretic viewpoint, and that is why it works so much and works so well. So, this L2, so H is that generic Hilbert generic Hilbert space, and L2 is the expansionally weighted Hilbert space on L2 of R. And if you try to remember how, well, one way of proving Piccolimita's theorem, you also introduced. Proving Piccolimita's theorem, you also introduce exponential weight in the continuous functions as a means to deal with arbitrarily large Lipschitz norms for your right-hand side. So this is basically the same thing, but in L2. And then the time derivative is just the distribution of time derivative in this funny space. And it turns out that dt is normal. And it turns out that dt is normal, you can explicitly write down the inverse, and the inverse is bounded by 1 over rho, which helps a lot. So if rho is then tending to be bigger and bigger, bigger, bigger, this norm gets smaller and smaller and smaller. So if you were so just, I mean, just a rough idea, if you looked at an OGE, And this right-hand side is Lipschitz. And you just invert with dt, and you see that the operator norm is arbitrarily small as long as you increase rho, then you see that the contraction is almost already there. So this helps. And also the real part of dt is equal to rho, which is relevant also in a minute. DT being normal, you can DT being normal, you can find abstractly a spectrum representation, but it is easier to go for something explicit. So we introduced the Fourier transform, as usual, and a mapping that gets away the exponential weight. So this is not so important. The important bit is this one. So if you compose these two unitaries, you get a spectrum. You get a spectral theorem for DT. And this is the same thing as you might have encountered already in the first PDE course, but without the row. And the only thing that the exponential weighting function does is shifting this multiplication operator per row. But you see then also, since this is unitary equivalent, that the inverse of this guy is bounded by one over rope. Is bounded by one of a row. And also that the real part is rho. So you see everything in this picture. And well, it's a functional calculus, so coronary is that you can apply functions to it. And, well, raising it to a power alpha, for instance, for alpha and zero one, or you can take every well continuous function even, but think of just holomorphic functions and then you arrive at uh Holomorphic functions, and then you arrive at the holomorphic operator pencil. You can also do it operator valued, and I'm sure that zeros of these things have some meaning, but this is a different story. So anywho, you want to solve differential equations eventually. And one of the results for the monotonous situation is this one. So you have abstractly two operators m and n, and a skew salvage on an operator. And a skewer self-age on operator A. And I gave a talk about this theorem a couple of years back in the Armstrad. And people confuse this operator A with their favorite generator of the C0 semi-group, because this is Q-Silver transfer, it's a generator of C0 semi-group. And their favorite generator is Laplacian. Is Laplacian. In applications, this is almost never the Laplacian. So if you want to take anything away from this talk, this A is not the Laplacian. The results are trivial in the Western Laplace. Not claiming that the results are very difficult, but they would even be more trivial in Western Laplace. Not Laplace. Not. Okay. Lots. Okay, so this is the equation we want to look at. And we want to come up with conditions that make this thing solvable as an operator in L2 with this funny weight and values in this generic Hilda space. And the only condition that you have, that you need, is this real plot condition, and that your leading order term is somewhat compatible with dt. So that is. With dt. So that is in applications that your m is regular, but different. But what we're going to see in a minute, this m will even be one. So then... Even just those conditions are satisfied, then this operator I was referring to is continuous invertible. It's something more also, but I'm not going to tell this. And the main example we are focusing on is the heat equation. example we are focusing on is the heat equation. So heat equations like this u prime minus divergence coefficient gradient and then you take q to be the flux and then you just read through those those two lines so dt plus divergence q equals f that's this this equation and well you don't differentiate q what you have in But you have inverted this AT and throw this gradient bit to the other side, so you get the second equation AT inverse times Q plus gradient U equals 0. Okay, and well, as I said now, given suitable boundary conditions, this guy is Q self-adjoint. This is our M, so it doesn't. So, it doesn't really depend, it doesn't even depend on t. So, this is the commutator condition is trivially satisfied. And this guy is our n. And for n, so if I look into this condition here, I need to satisfy the strict positivity here and down there. And so, you ask for strict positivity of AT inverse, which in the setting I was presenting at the very I was presenting at the very beginning for divergence form operators is also satisfied. So this theorem tells you that you can solve this non-autonomous problem in L2 to the expense that you need to have the closure pattern. So this is not maximum regularity because we have a closure bubble. Okay. Yes. Yes. Yes, the initial condition is implicitly formulated as put to minus infinity because of the row. Yes, because of the shifted, well, so it's homogeneous at minus infinity. But if you wanted to do also initial value problems, you could do that. But then you would introduce, well, you could do it in several ways, but you introduce one way is to introduce a distribution of it. Use a distribution of right-hand side so that zero dots at zero, for instance, and then you can also solve it. And you can solve it with the same theorem, so for the extrapolation of these things. Would you also introduce an affine idea domain of the operator? Or which operator? In the user operator, to include initial conditions. So you put you put the initial condition into the domain. Condition into the domain, but if it's not critical, then it's not a space anymore. Yeah, that might be okay. The referreration I was suggesting is very elementary, so I don't immediately see why it is needed to work with that, but I don't know. I mean, maybe there's something in it. So, most possibly it can be done with this. Okay, so that's the heat equation. Okay, so that's the heat equation. Oh, yeah, sorry. Just apply to a transport equation, if that wasn't for just one more component in zero on the boundary, then the A operator would be like a direction operator, great dot grade. Yeah, that you well this the easiest case for transport equation is um Is that's the wrong transport equation can be dealt with like that? Easy, right? So the very beginning. I know that there's not a variable velocity yet, right? So this is a transport equation, but it's not an equation, I think. This. And if you did it in higher dimensions, in only one direction, it's transporting to Only one direction is transporting to, then it involves a reformulation of the problem. So we did that, but it's not as direct as you would want it to be. I mean, what you look at is if it was constant velocity, it's not very difficult because then you could establish this as a screen cycle to operate. If it wasn't, then it's delicate. Actually, I'm thinking like for higher dimension you have. Actually, I'm thinking like for higher dimension, you assume we lost it at that point, and on the boundary, you just assume all components zero. So, basically, get the mass conserved, and you don't have to impose any boundary condition for the scalar. Yeah. Yeah, yeah. Yeah, I might. The problem is that it really depends on how the velocity is depending on the underlying variables. So, if it was only So if it was only in spatial direction, that's fine. If it was also in time directions, it's not as telemetry anymore. So I don't know that. Okay. So okay, so heat equation. And other examples, of course, wave equation. Same substitution for the flux and we substitute here We substitute here v to be u prime, and you say, Okay, this is wave equation now. So, the difference between the wave equation and the heat equation is basically that there is a zero, there's an additional term which is not multiplied by dt. So, this moves to the end part in the heat equation situation. And in the wave equation, it's like that. And we also treated some control problems and saw that. That you can look at this also in the wave equation situation, then this whole matrix is with control and observation. Also, there's additional lines and columns, and then you can show that even in this case, the real part condition is satisfied, and then everything works for the well post-less bit. And also, my favorite example, of course, is Maxwell's equations. This Maxwell's equations, which I wrote a bit awkwardly here with interchanging H and E for matters relevant in a minute. And now we have epsilon sigma. And one of the usual simplifications of Maxwell's equations is the so-called adhycurrent approximation, where epsilon is zero. So you just formally put epsilon to zero, and then you have this. To zero, and then you have this equation. But then it might be that you have epsilon zero only on certain regions, because this is the approximation people deal with in metals, for instance, where the conductivity is very high and the permittivity is very small. So you have epsilon being zero in certain regions and others, sigma is zero, but epsilon is there. And these mixed type problems can be formulated within evolutionary. Formulated within evolutionary equations is no problem. Okay, so we have now seen a view on evolutionary equations. We've seen Lyon's problem of maximum regularity. So what is now the one perspective from evolutionary equations to maximum regularity? Well, classically, people have looked at these things, given the speed equation, and then they The seed equation, and then they introduced forms, and they went with this problem. Okay. In evolutionary equations, we somehow know, and this is, well, we not somehow know, this is the heat equation, but we cannot expect maximal regularity if this A of T inverse would be popping up here, because the wave equation doesn't have maximal regularity. So the generalization. So, the generalization in evolutionary equations is like this. We have actual operators C and C star acting in suitable Hilbert spaces being edged of one another so that this guy at the end here is QSO. And the leading operator is some M, and this guy is some low-order perturbation that is. Law order perturbation that is necessary in order to make this problem well-posed because this N11 needs to be there in order to mimic this time-dependent coefficient. Okay, now if you look at this problem, you might think, well, what are good conditions in evolutionary equations to have maximum regularity? And your aim in maximum regularity is you want to have F in L2 implying Q in H1. And let me for a moment do the same trick that we did before. So we want to have U and H1. So why not substituting here V and then this becomes DT inverse. Inverse and then I'll have the equation in terms of V and Q so that there's one here, zero and Q is this C star minus C zero zero. Oh no, the uh scorrect. 1000 plus. And then top you need to differentiate Q so you get dt AT inverse down there F and zero. Okay, so this is a So, this is a possible reformulation of the heat equation, which is not very common, but still you could do that. And then you need to check whether the Bulposis condition is satisfied, and if A of T was nice, that is, since this is now on the M par, so if this commuted with the T accordingly and you have this positive witness condition satisfied, you can solve this guy. So you get F and L2 implying you should. So you get F and L two implying your solution also in L two in uh above so this V is in L two but V in L two meaning that your U is in H1 that's nice but you ask too much of A so it needs to be now commuting with B T so you ask for A ellipsis and we already know that A ellipsis is it's nice but That ellipses is nice, but still not at the heart of the recent results. So, what you do instead is you take something in between, you substitute V to be dt one half, and then you get this problem. And now you have F and L two B. meaning v also l2 implying u and h one half so this is a thing we saw but h1 half is still very far away from h1 but somehow this h1 half perspective showed us that we need to impose something for it for in the h1 half case and this is what happens now uh yes we can solve this blah blah blah uh Blah blah blah. In the theorem of Sasha-Mostov and Mein. So, the first thing that you assume is to SS before, you assume that your problem can be solved for them too. Otherwise, there wouldn't be any maximum regularity to talk about. And if you now assume in addition, basically the same conditions that we asked for in L2, but now in H1 half. So I just formally replaced every L2 by H1 half. Replaced every L2 by H1 half. So then, so this is WP, this is TW, we have situation at F in G, so G is normally zero, but you could also ask for G in H1. This implies Q in H1. This is a maximal regularity theorem for non-autonomous evolutionary equations. And as I said, this is. And as I said, this is at the heart of basically being able to do something because we learned previously that kind of solution H1 half is somewhat necessary to deal with these things. This is solution anyway. So you would think, well, this needs to be the theorem. Okay. And because the second condition, this positive condition, is rather hard. Rather hard to establish without any further assumptions. We also consider the corollary where we ask M to be just independent of time and we ask this commutative condition. And as I suggest, WP plus and TW plus are stronger than WP and TW. So these things that my widow These things by one another, and this is the theorem. So, what you have is basically since this C tilde is related to C, but just think of this as this commutator with DT a half is infinitesimally bounded with respect to dt a half. So, this is this, and n needs to be a bounded operator and h1 half. Okay, so that's something. Okay, so that's something uh workable, okay. And then you get maximum regularity. And the key things, key remarks here: the proof of vector elementary uses Young's inequality, so the Peter Paul thingy, 2ab is less than a squared plus b squared, Gauchy Schwartz, and the density argument. And then if you If you don't like so much the condition n in h1/2, this is the same. But that's elementary, but the reason I'm showing this is that this doesn't, only this condition doesn't apply under this word computator. But it's really something more. And it obviously generalizes the autonomous version of our previous result in final ads. With Feider and Sasha. And the autonomous version is also contained in the book. And you see that it's really elementary, so you don't need to be an expert in evolutionary equations to understand the proof. And it's applicable to Maxwell's equations where there's no square property available. Okay, so this is just remarks, but it already comes down to I mean to those previous conditions. We need to now know what uh the relations between those two um those three were. To it, those three were. And so I brought up the sketch from before. So this is everything, all those implications, and blah blah blah. And now I introduced our TW plus and TW. And as I said, TW plus implies TW. And now we're going to check DC and AE. And first off, First off, this HRR thing is implying this infinitesimal part. So this is nice. And this BMO thing is characterizing the boundedness of the commutator. So this is epsilon equals zero. So this This as I said, and this is epsilon equals zero. But now, one obviously wonders what is this guy, this dark above, having to do with this down there. And well, first and foremost, I don't understand, not at all, how one conveniently could put the square root property in the evolutionary equations frame. Property in the evolutionary equations framework? I don't know. Just a comment on this: it is an annos paper in 2005 that the matter squared property holds for measurable coefficients for omega equals Rn. And it has been generalized for Lipschitz domains also, but as a just rough rule of thumb, you can say the square property for these kind of problems. Kind of problems is saying something about the regularity or the form of omega. In particular, you cannot really expect this to be okay for curl. So I don't know, right? So this card-square property is something that is a bit mysterious here. And finally, I come to some conjectures, as I was asked. So the C beta prime or inverted commas. Or inverted Komas condition is this guy, which I don't understand. I mean, I understand that C beta implies this for beta positive, but this is also an integral, but there's only one integral, and there's supremum, so it mixes two things that shouldn't be mixed in a way. And Auschwitz and Egard in 2019 they conjectured that C1 half plus the diversity form is not in. half plus the divers form is not enough for maximum regularity which I kind of which which from this what we just have I think is true because somehow h one half is the threshold and c one half is to to to view a regularity to have something and now my conjectures here they're a bit tug and cheek obviously I hope that our maximum regularity theorem is the most general. Is the most general for the situation of evolutionary equations. So that I would expect our conditions to apply the ones by Mohabbas and Ahare. But I put the conjecture down to that not being the case because either our theorem is the best or my conjecture is true. And so these are the two conjectures, really. So if one could show this one, then we are lost at once because then the half-loo-based condition wouldn't really. I mean, if this is not implying n being abundantly operate, then we're lost anyway. Anyway, but if one can show that, then it's still not clear whether the TW plus condition or TW condition can be established. Okay. And the summary. So we looked at maximum regularity for these kind of problems for evolutionary equations. And what I emphasize, this is a differential algebraic equation. Right? I have no nothing here. No, no nothing here. And maximal irregularity is really a phenomenon for differential algebraic equations. If there wasn't a zero, it's wrong. I've counter examples for wave equation triply being constructed. So really, this nice property is rooted in a defect of your leading order time coefficient. And with these words, I'd like to close and thank you very much for your attention. I'd like to close and thank you very much for your attention.