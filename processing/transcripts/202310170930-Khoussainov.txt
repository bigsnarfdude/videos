So, I'll be talking about automatic structures. Let's see. Okay, okay, so that's one. Problems, I'll give definitions, examples, decidability theorem, some of the key research directions. Some of the key research directions, open problems, and our contributions. Okay. I want to say just a minute. Oh, it's very strange. Okay, just a minute. I don't understand what's happening. So I want to go back to it. So it's do I need to Do I need to oh, okay, so this one, right? Okay, so Imong HR 33 undergraduate students, quite bad students, and so this is joint work with them. So the automatic structures have been planned for almost thirty years. Almost 30 years. There are many, many questions we've been solved, but there's still many interesting questions remaining open, and I'll say a few words about them as well. And so in this lecture, so I'm going to answer two long expanding open questions. They have been open since basically 30 years since we started doing automatic structures. And we're also going to provide exotic examples. So I'll explain what exotic means as well. What exotic means as well. So, I'll start with the finite optometer. So, most of you know, but still, I'd like to go through a little bit. So, finite optometer, it consists of finite set of states. So, it's number one, initial states, and the transition table. Transition basically says when the machine is in state Q and road assembled from the alphabet A, it goes to state P, and that transition. State P and that transition can be deterministic or non-deterministic, and it can actually finite state final or accepting states. So that's the definition. So what Atom does, it takes input a string and processes the string starting from the initial state. So and the way you can view the automaton is a direct graph. So vertices are states, edges are transitions. So if you have Are transition. So if you have an edge from P to Q, and if it is there by sigma, and then you say it's a sigma transition. That means sigma at state Q, at state fielding automata can go into Q. So when input has a string now, the automata produces, by having the string, the automata produces a path, and the label of the path was the string, and you can be sealed path. String and there can be several paths, right? So if automata is non-deterministic, there could be several paths. And you say that automata accepts the string if there is a path loaded by W such that the path starts from some initial state and ends in some accepting state. So you can collect all the strings accepted by the machine, and this is called finite automatically. So this language is called finite automatically visible language or RF. And I also say very Are often also say irregular language. So now because I want my finite automata to recognize relations, so I want to explain what it means for automata to recognize a pair of strings. So for instance, you have string X and Y, and now think of the automaton. Now, think of the automaton as two head automatons. So the first head reads X, and the second head writes Y. And the finite automator is finitely many states. And so and these two heads move basically synchronously along the X and Y. So basically you have X and Y, you have Have you represented this xy as this string? So on top of the string is x, so 0, 0, 1, 1, 1. And these boxes are put because the length of the string of x is smaller than the length of the string of y. So you just put an extra symbol. And on the bottom, you put y. So an informer is basically moving from left to right simultaneously, right? And once it finishes reading the whole input, we look at money except the L. We look at the accepting final state, last state, if it was accepting, then the automatic accepts the pair xy. So that's a definition of what it means for automata to recognize pair. So now a binary emulation, so all in a relation of a set of L strings is finite atomic recognizable, again, regular evolves a trivial finite sequence and atomic. Synchronous electron that recognizes you. So, and I'll give you a bunch of examples just to give a little bit of feeling. So, let's look at this successor relation. So, there are basically this string goes into zero, zero goes into two zeros, n zeros going to n plus one zeros, and this is the successor relation, right? And the appropriate. Right, and the autometer that recognizes that successor relation is this: so initial state, and if it's two two once the automata has two bits and it's reading zero zone zero zero zero zero and statement is zero and as long as soon as the first string gots shorter than gets shorter than the second string more counter goes into this state and and verifies that there's only one zero left. So One zero left. So this binary relation, successor relation is recognized by a finite parameter. So similarly, so on this letter, so if you look into this relation, it's basically a lesser equal relation. Again, two-state autometer recognizes lesser equal relation on binary strings, both from zeros. So only again is the initial state, automaton is moving zero zero. State automaton is moving zero, zero, this should be also accepting state at the same time. So if it finishes moving m zero, then it accepts. So this should be accepting state. Otherwise, if the life of the first ring is shorter than the other one, it goes into the accepting state and stays in the parallel and accepts it. So a little bit more interesting example is you represent basic integers. Basically, introduce natural numbers in binary. And so you want to recognize all the tuples x, y, z such that x plus y is equal to z. And it's a two-state automatic does the job. So there's the first state and there's the second state. So if you look at the transitions, basically, this is normal carry state on input 0, 0 output is 0, 1, 0 output is 1, 0, 1 output is 1, and you don't have a carry here. You don't have a colour here, but if I have one, one, you remember that you have a car, you change your state to here and just loop here until you zero zero, you lose your car and go into the state. So that's two state automata does the job. So it's a nice basic sort of way to represent operations. So the plus operation is infinite operation. Infinite infinite operation, and you can represent it by just two-state automata. Okay, lexical graphical order, lexical graphical order. So, this is also you can represent it by a finite automata. So, I'm not going to go through it. And here's the definition. Structure is automatic if the domain of the structure is finite automatically recognizable and all the relations are finite automatically recognizable. So, that domain is. That domain is a regular relation over some alphabet, say binary. And all these are relations on the set of on the domain, and you will find a theta for each of these that represent the structure. So the focus of this talk is going to be these very simple, I mean, algebraically, very simple structures. And then the first structure is the. I'll so the first structure is the successor structure with a unary predicate on it. So, so double omega s u u is a unary predicate. The second structure is omega and E S. E S is basically edge relation on so undirected edge relation. S is the directed edge. E sub s is undirected edge. So you put E sub S, you put an edge between X and Y if the difference is the difference. The difference is they differ by one. So it's undirected graph or path, sometimes infinite path. And the second structure is a less or equal relation together with the unary predicate. And these are called word structures. So typically these are called wood structures. And the reason I'm interested in these structures is that, first of all, they're very simple. And second of all, I mean, like, even I mean, like, even omega s is extremely rich structure, in a sense, monadic second-order theory of the structure can code many, many things. So, if you're if you're doing practically any algorithm, more or less, okay, elementary algorithm, so the running time algorithm whose running time is elementary, you're actually inside omega s. That's what's happening in monadic second-order theory of omega s. So, um. So, and from computer science viewpoint, so there's a lot of study of this omega s structure, just success structure on its own. Okay, so why do I call them word structures? Because so the unary predicate calls a binary string, right? So, what you have is you have a word structure and you have u, so that. And you have u, so that's a unary predicate. If at position i belongs to u, you put one at position y, otherwise, you put zero, you produce infinite binary string. So each of these structures called a binary string, and two, and that binary string is isomorphism invariant. So two word structures are isomorphic if and only if the binary string, the word u1 and word u2, are the same syntactically. Are the same, syntactically the same. Okay, so now I want to just, since we're interested in the structures, I want to give precise definition of automaticity. So say you have the word structure, successor with the unary predicate. Instead of successor, it can be underverted age or it can be a less or equal relation. And automatic representation of the structure is simply what we need to do is we need to represent the domain omega with a The domain omega with a regular relation, B, a regular set. You need to represent S binary relation with a regular binary relation, and you need to represent U with a union relation, all finite automatically recognizable, such that the given structure is isomorphic to this triple D, S sub D, and U D. Okay, so that's here example. Here are examples. I already give them the examples. These use zero star. So the domain is all zeros. So empty set, empty string, zero, two zeros, and so on. Successor is, I already explained, zero n to zero n plus one, giving it as obviously finite automatically recognizable. And this is just all the strings of length three, or multiples of three. So use of. And so, U is also finite automatically recognizable. This is automatic spectrum. So, this example is another example. So, domain is zero star, one star. This is a so zero star, one star, you think of it as a three, right? So, have so this is the zero star, and you have one star. So, you can think of it like. Star. So you can think of it like three. So that's zero star, one star. Now, what the successor is doing is basically it starts from the empty string. And I think it goes here and it goes to the next level. And then goes here, goes to the next level. So it's basically running levels by levels from left to right. And so on. So this is the, and this is automatic. Ah, thank you. So And one of the key points that basically initiated the whole area of automatic structures is this theorem. It says if the input is finite at an automatic structure, so because automatic structures are presented by finite automata, there are only finitely many automata. You have that as an input. And also a statement the first of the logic statement. And then you can decide whether A satisfies. And then you can decide whether A satisfies the structure. So this theorem implies that, so there is a stronger version of this theorem, so I didn't state it here. It basically says there is an algorithm that given a finite automata presentation of this structure and given a formula which depends on n variables, outputs finite automata that recognizes all puppets that satisfies the formula. Okay, so it's a strong sort of decidability theorem. Sort of decidability theorem. And as a colleague, you're going to get lots of decision examples of structures whose theories are decidable. A standard again example is abelian groups, finitely generated abelian groups, or the Pursburg arithmetic, for instance. So if you want to do the decide that the theory of the Pursberg arithmetic is decidable, you need to do elimination of quantifiers, and it requires some work, but here you get almost for free. Get almost for free. That decidable theorem. So there are many, many, many questions, directions in the area, but here I would like to basically just say a few words about three directions. One is complexity theoretic questions. So that is how efficient is the algorithm in this theorem? Just a minute. So it's basically if you look at the algorithm that solves At the algorithm that solves that decides the theory of the structure, the algorithm is non-electary. Okay, so is it a proper bound? Non-elementary? Can the theorem be extended to other logics? So that's, I call them complexity theoretic and interpretability type of questions. The isomorphism problem. This is like very, very standard question. So input is automata that represents rough structures. Output is yes if they represent isomorphism. If they represent isomorphic structures, otherwise no. And constructing exotic examples. Now, so what is an exotic example? Other structures that are counterintuitive. So I'll talk about that a little bit as well. So let's go to complexity theoretic questions. And then I'll go to isomorphism problem, and then I'll talk about three. So here's the answer to the questions, right? If we look at the first structure, so it's a binary. structure so it's a binary it's a binary tree the first structure is binary tree you have two mineral operations the left successor and the right successor and the predicate it say it it says two strings are in equal relation if their length are equal in the prefix relation so this is automatic structure and the complexity is non-elementary so um so that's uh basically upper boundaries so this structure gives you the upper bound gives you the upper bound. So the also for automatic structures, so checking whether n quant alternation quantify a sentence which starts with existentiation, so it's express complete. So the natural numbers with plus is two double exponentially complete automatic structures with bounded degrees. So you have a structure. So, you have a structure with a graph where every vertex has a degree at most D, where B is fixed. So, it's double exponentially complete. Automatic structures, if you restrict yourself to only unary alphabet, then the theory with any unary automatic structures is in P polynomial. So, Kushke and Laura. Kushke and Laurie provided examples of automatic structures where the theory is decided by an exact n exponentiations of the power of two. So generally, so there are lots of lots of questions here. And like, so it's interesting to find out the structures which lie in different levels of this elementary hierarchy, so the exponentiations and so on and so forth. So here I'm So, here I mentioned this result. This is a very nice result. It says it's a characterization of automatic structures in terms of interpretability. So, a structure has finite automatic representation if and only if it is interpretable in the binary tree that I just described. So, binary tree, left and right successor, equal length predicate, and prefix relation. So, this is a very nice theorem. Very nice theorem, but like in terms of it sort of answers fully the question about what are the automatic structures in terms of interpretability, but it doesn't answer lots of other questions, for instance, isomorphism problem. So this is, in a sense, like any computable structure is definable in the arithmetic using this any definable existentially definable, but so what? So if I give you a computable structure, what can you say about the structure? Structure, what can you say about the structure? Right? So, for instance, isomorphism problem. It's very similar situation here. Like, what can you, okay, this is great, but what can we say about structures themselves? What are the invariants of the structures? So, the isomorphism problem. So, now I talked about complexity now, isomorphism problem. So, input is two automatic structures, and I want to know whether they are isomorphic or not. So, and here's the state of the art. So, if So, if automatic structures are ordinals, and by the way, you can find out whether automata that represents a structure is ordinal or not. So, it's effective process. If a structure is ordinal, then isomorphism problem is decidable. If the structure is Boolean algebra, then isomorphism problem is decidable. Again, given automatic presentation, you can effectively decide whether the presentation is a presentation of Boolean algebra. Presentation is a presentation of Boolean algebra or not. Of Marky Kuran's relation, which was long, I mean, open question for about 20 years. So it was solved for about maybe 10 years ago. It's surprising in some ways. Automatic constellations is Pi 01 complete. Locally finite automatic graphs, Pi 03 complete, lasomorphism problem. Automatic trees of height n. So Pi 02 and minus 3 complete. 3 complete, but generally the problem is as hard as for computable structures. Isomorphism problem for thematic structures is sigma 1, 1 complete. And these are unknown. We don't know wood structures. We don't know anything. We didn't know anything about wood structures. So automatic wood structures, automatic successor structure. I'll give you an automata and ask you whether it is isomorphic to successor structure. So that has been a long. That has been a long standing open question. So, the first two questions, I mean, people worked on it. Andre Nice worked on it, Frank Stefan worked on it, Balani worked on it, Sankoff worked on it, myself, almost all my PhD students. So they sort of know the problem and try to solve it. And here's the interesting: automatic finitely generated group. So, this is a very interesting thing here is that. thing here is that there is a proper a group is fine automatic a finitely generated group is automatic if and only if there is a description it is uh above and by finite so there is a description but it doesn't imply so that you can solve the isomorphism problem we don't know how to use it like for automatic ordinals there is a description ordinal is automatic if you know Description organizes automatic if and only if it is less than omega to omega, right? And from that description, you can actually extract the counter normal form and because of that you can solve the isomorphism problem for automatic ordinals. But here for automatic, finitely generated groups, you've got a description, but you don't know. So what you're getting. So that's here's another interesting example: finitely generated above in group. Interesting example, finitely generated above groups. If I promise you that a given automata presents a finitely generated group, so then what is it? What does it look like? We don't know. So that's interesting things. So now I talk about, so this is the complexity, isomorphism problem. Now I want to say a few words about natural automatic presentations. So what is an exotic presentation? What is an exotic presentation? Exotic presentation is one which is not natural. What is a natural presentation? I can give you examples. I cannot give you a definition, but so we can try basically approximate what it means to be natural. So linear order is obtained through length, lexicographical order. It's natural. Length, lexicographical, very natural relation. Prefix order is natural. Lexicographical is natural. So plus on integers, the digit twice. On integers, the digit wise, it's natural. So, bruine, these and or negations, these are all natural. So, all the structures, if you can get them from these relations, I call them natural. Okay, so this is a natural example. It's a very simple. So, the example I gave you, so that you have a tree, so you have an infinite path to the left, and so countably many infinite isolated paths to the right. Isolated paths to the right, and you have lexicographical order and success relation. This is natural. So these are all examples. So the binary tree that I talked about, this is natural. So it's also a natural presentation. So to try to basically give a proper definition, sort of try to understand what natural means, there is a sort of parallel to So, the parallel to computable structure theory: you say a relation, so you're given an automatic structure, you're given a relation, and you say that the relation is intrinsically regular if it is regular in all automatic presentations of A. So, in a sense, I would call intrinsically regular relations natural relations in some ways, right? So, an exotic automatic structure is one that makes some natural relations be. Basically, not regular. So, that's basically what the exotic means. So, let me give an example. So, if you look at natural numbers with less or equal, so the set of positions at even, the set of integers at natural numbers at even positions is not definable in the less or equal order. It's not definable. But in all presentations, it can be proved that that set. It can be proved that that set, the set of all even positions, is regular. So the set of even positions is intrinsically regular. So this is a, so basically, that, the set of all even positions, is a natural regular relation there. So any infinite path in any automatic tree, this is also interesting. So if you take an infinite path in any automatic tree with finitely many infinite paths, then any infinite path is regular. In fact, if you take Regular. In fact, if you take any finitely branching automatic tree where you view the tree as a partial order, that's important. View the tree as a partial order. Click any automatic tree as a partial order. Then the set of all positions in the tree that belong to infinite path, it's an intrinsically regular relation. So that's cool, cool examples, basically. Now, Basically now, here are examples which are very, very counterintuitive. So, this is an automatic presentation of the successor in which less or equal is not automatic. So, in all examples I gave you, so when you represent the successor, the less or equal becomes automatic. But there is an example where less or equal is not automatic. There is the second example, which is also very interesting. Also, very interesting. So, you take integers with a successor relation, and the cut is, you pick up any point, anything on the right is a set. So, it's a cut, right? There is an automatic presentation of the integers with a successor where no cut is automatic. No cut is regular. Okay, so the cut is any of these sets. So, there is an example of an automatic graph with exactly two components. So, components of a graph I know I So, component of a graph, I know, I assume you know what it is, such that the graph is automatic, but none of the components is regular. So, it's also an unnatural example. Here's another example. These are all proved by a bunch of people, including Andre Nis, for instance. The last one is Andre Nis. There is an automatic presentation of the group Z integer squares, Z squared, such that no proper one-dimensional subgroup is. No proper one-dimensional subgroup is automatic. So this is an exotic example. And here's five. So here's if you can prove it, then it will be exotic example. We don't know if there is an automatic presentation of the integers with plus where less or equal is not regular. All automatic presentations of the integers with plus is such that they make less or equal regular. They make less or equal regular. So is it always the case? We don't know. So that would be a cool example as well. If we can, anyone can answer that. So now, so this is basically more or less I talked about automatic structures, what we're doing there. And now statement of the problems. So the focus, so to solve the isomorphism problem, the focus has been to solve isomorphism problem for word structures. Morphism problem for word structures omega SU and solve the asomorphism problem for the successor structure. So and build exotic examples of these word structures of these type of input structures. And I'm going to answer all these questions and give lots of examples. So how much time do I have? Sorry. 30? Okay. 20. Okay. So Okay, so um so let's go to okay. So this is the again the state of the art isomorphism problem and automatic word structures. Again, it's when I say word, it's successor. There's automatic less or equal word structure and there's automatic successor structure. There's a big difference between them. And here automatic word structure, by automatic word structure, I mean automatic structure. I mean, automatic structure where you have a successor with a unary predicate. And automatic successor structure, we also solve this. So this is, again, as I mentioned at the start, long-standing open question. And somehow we were able. And the solution is not hard. So it's just so let's. So no algorithm exists that given an automatic structure DS decides if the structure is isomorphic. Decides if the structure is isomorphic to the successor structure. In fact, the problem is pi zero to complete. So the proof is it's basically if you're given a Turing machine and if you want to know and given a Turing machine, you're given two. Given the Turing machine, given two configurations of the Turing machine, you want to know whether there is a path from one from the first configuration to the second. It's undecidable. It's a problem, it's a halting problem. So the intuition is you want to code that one into the successor structure. But if you look at the Turing machines, it's quite complex. It's not easy to do it because you want to be isomorphic to the successor structure somehow. And the idea here is to play with the configuration space of the Turing machine and make things nice. And that's what I'm going to explain. What does it mean to make things nice? So you're given a Turing machine and you look at the configurations. And the set of configurations is regular. So it's finite automatically recognizable. And the relation, there is the instantaneous move from one. Stantonous move from one configuration to another configuration. So you move from C1 configuration C1 to C2 if one of the machine instructions of the machine, of the Turing machine, tells you, allows you to go from C1 into C2 in one step. So Turing machines, every instantaneous move is very local, so it's finite automatically recognizable, and therefore this instantaneous move relation, which is directed graph, makes the whole Directed graph makes the whole configuration space a directed graph, automatic directed graph. So the graph configuration space with the instantaneous moved relation, which is a directed edge relation, is automatic. So now if you look at the successor structure, it's sort of like a line, right? It's zero, goes to one, one goes to two, and so on and so forth. So what you want to do is, at least you want to try your configuration spaces. Try your configuration spaces of the Turing machine to look like successor. But there are lots of sort of configurations which can be not reachable from other configurations. You need to basically get rid of them. So, and the first approximation is basically to get into that goal is to make the machine reversible. So, reversible means that the, so because now configuration space. So, because now configuration space is a graph, directed graph, vertices are configurations, there's in degree and out degree of the graph of the vertices. So, and that's called time machine reversible if in degree and out degree of every configuration is at most one. So, there's only one outgoing edge, and there's only one ingoing edge, right? And so, now observation is now let's look at the structure of the Let's look at the structure of the reversible Turing machines, right? If you look at reversible Turing machines, if you look at the configurations of the reversible Turing machines, then it consists of chains. What type of chains can you have? You can have Z, you can have omega, you can have omega star, and you can have finite. So, any configuration space of reversible Turing machine consists of unions of chains of different types. Okay, so this is easy. Okay, so this is easy, but it's nowhere close to successor yet, right? And in fact, it's a oh, okay, so it's just a minute. Okay, now so now, even with these chains, now you want to basically classify certain chains, right? You take a chain in the configuration space, you say it's legal if it starts with a configuration, with the legal configuration. Legal configuration is the one. Legal configuration is the one where you have initial state and legal input. A legal input is a binary, say your alphabet original alphabet is binary. So anything which is reachable from the legal input, so that's your legal chain, right? So we call configuration legal if it belongs to legal chain. It is now, it is known that any Turing machine is equivalent to reversible Turing machine, which is nice. So I'll give you, there is an effective process that a given Turing machine. Process that a given Turing machine produces an reversible Turing machine equivalent to it in terms of input-output behavior. Okay, so from now on, I'm going to assume Turing machines are reversible. So, but we're nowhere close to the successor yet, right? So, because again, there are many, many chains. And here is a basic lemma. So, this is a combinatorial lemma, which is interesting. There exists a transformer. Is interesting. There exists a transformation. Input is a Turing machine. And you can output a Turing machine such that the following is true. So, transformation is equivalent to original one. So, M is equivalent to the transformed one. So, transform holds on all configurations if and only if M is total. So, know that if your machine is reverse. Note that if your machine is reversible and it's equivalent to M, even if M holds at everywhere, it doesn't mean that the reversible one halts everywhere. Because there are illegal configurations, potentially. So this is important on all configurations. So it's reversible. And here's the key point. So the configuration space of M2 consists of finite chains only, if and only if M is total. Tropical means is. Is it halts on any legal input? So basically, what this means is that the given Turing machine, if it halts on every input, then you can simulate it with a reversible Turing machine such that all of its chains are finite. Okay, so there is no infinite chain, which is now we're close, we're in good shape now, in a sense, in the following sense. sense in the following sense now take a take a reversible Turing machine where all chains are finite you can regular you can recognize all the bottoms of the chains it's a regular relation you can recognize all the tops of the chains it's also regular relations and now we're in a great shape so here's what I'm going to do I list all the chains I list all the chains. I'll take machine, Turing machine, reverse it in a way that the dilemma provides, and I list all the chains where the bottoms are length lexicographical order. Here's my first chain as C01, second chain is C02, third chain is C03, and so on. But I also, to these chains, I put their copies. Right? So in the first chain, So, in the first chain, I go along the Turing machine up. When my configuration stops on the top, I start doing the reverse and go down. So, these are a reverse means to every symbol, just put a reverse symbol and so on. So, now what I've done is when I went to the top, I went from that top of the chain, went to the right. Went to the right, went down, and came up to the bottom of the copy of the first chain. And because it's linked lexicographical order, from here I can go to the second chain, bottom element of the second chain, go up and so on. Now, if my machine is total, then all the chains are finite, and this process goes through all configurations, and this is a successor. I have a successor relation. Relation. If my machine is not total, so I don't get, so it's either this chain somewhere stops and doesn't do anything, or I get infinite chain, right? And I'm not omega. So this is basically what I've done is, so M is total if and only if the structure is isomorphic to omega S. And the index sets of total two initial. Total, total, total Turing machines is what is it? So we're done. This is nice. And now it allows us to actually, with a little bit more work, not when I say a little, but with a little bit non-trivial work. So no algorithm exists that given two automatic wood structures decides if these structures are isomorphic. This problem is pi zero one complete given that you know that the inputs are wood structures. You know that the inputs are word structures. Okay. So. And the proof is so from now on, I always assume my machines are reversible and they're nice. And if machine is total, then it consists of the configuration space consists of finite chains only. So that's my assumption, right? Now, what I want to do is, so because I have the unique. So, because I hold the unary predicate, I want my unary predicate to count. So, the distance between one instance of an element in the unary predicate to the next instance of the unary predicate, I want that distance to code the running times of my machine. That's what I want to do, basically. So, and now let M be a total to your machine. The machine is total because it's total, the configuration space consists of five. Configuration space consists of finite chains only, and you take input and you count how many times the time it takes for the machine to stop. And no algorithm exists that given two total machines, two machines can tell us whether they have the same running time. Okay, so it requires work, but it's it's um You cannot use here a rust type of theorem. So, because the running time of the machines, even if the machines behave similarly, if even the machines are equal, the running primes in terms of input-output behavior, the running primes is not invariant. They could be different. So, you cannot use in this lemma the rice theorem. So, you need to do a little bit of work. That's what I mean. Right, so but it's not surprising. So, now what I want to do, given two Turing machines M1 and M2, I want to construct two structures which depends on M1 and M2, such that the running times are the same if and only if these two structures are isomorphic. So, basically, this structure A and 1M2 and AM2M1, so they're sort of so if the machine, the running times of the machine. Per machine, the running times of the machines are equal, these structures become become isomorphic. So that's what I want to do. Okay. And so I need to remember what I did here. Let's see. So if you understand this, the first fragment, then it should be, we should be okay. So now, here's what I'm going to do. Okay, so this is the bottom red new. The bottom red means I put it into the unit predicate. So this is chain. So this is chain, L I zero. It's the first chain. And bottom of that element, I put into my unary predicate. And I go up. I simulate my Turing machine. I simulate my Turing machine. And then, if you remember that picture. So, if you remember this picture, so this I can't see 01 with you, usually, Predica. I went up, I went down, I went to the reverse one, and I went down, so I went to the bottom of the reverse one here, but then I switched to the second Turing machine. So, that's what I'm doing. What I'm doing. So I went to the reverse one of the first of the first chain. Once I completed the reverse one, I went, I'm going to go into the second machine, right? And I call the bottom of the first chain of the second machine. And I go there. And then basically, what happens is, so suppose. Since so suppose here the running time of the first machine on the on the first input is 10. So that means I made 10 computations up. I went to the copy of it, 11, and then I went down. I did 21 computations, 10 up, switched to the reverse, and 10 down, 21. And then I went to the first chain of the second machine. So it took me the distance. Machine. So it took me the distance between two girls is 21. So if the second Turing machine halts in 11 steps, if I do the same process, so I go 11 steps here plus one plus 11 steps down. 11 plus 11 is 22 plus 1. It's going to be 23 steps. So if isomorphic, the distance between the colour nodes are equal. Otherwise, this distance, I destroy the distance. Distance, I destroy the distance. So, and that's what happens. And this is non-trivial. Okay, so I have how much? Two minutes. So now, these structures, so why are these two results surprising? So I have always been thinking that the problems for these, the isomorphism problem for these structures is decidable. And the reason was that, first of all, they're very simple. Second of all, we have finite automata. Simple. Second before we have finite automata. But let's look at the Scott sentence of omega s. Let's look at the description of omega, the successor. The description is, you say my function is injection. There's only one element without pre-image, which is zero. And then you say everything is reachable. This is a Scott sentence that describes my structure up to isomorphism. Now, if you look at the ordinal, If you look at the ordinal omega squared, it's automatic. The Scott sentence that describes the ordinal is much more complex than this. You need to say that there are infinitely many limit points, and between any two limit points, the distances are, I mean, relative distances are finite. That's what you need to say. So for omega squared, the isomorphism problem is decidable, even if the description is much more complex. But here, this description of successor is. This description of successor is very simple, but the isomorphism problem is not decidable. So, this was the reason for me and for many others, I guess, many, many people, thinking that the structure is the isomorphism problem should be decidable. But it turns out it's counterintuitive. That's why this is surprising. The same is for the word structures. And I don't have time for exotic examples. I can talk about that privately. Now, so in a privately, no problem. So I'll finish. So this is cool stuff, but anyway, thank you.