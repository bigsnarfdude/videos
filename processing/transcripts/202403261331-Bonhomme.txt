This is a joint paper with Martin Weidner and Tim Christensen, and this is about parameter heterogeneity and how to use some distribution and robustness. We'll see which way for that purpose. So our basic model is a moment restriction product. So we consider a parameter theta that is defined to a set of conditional moments restrictions. So to fix ideas, you can think that if a function So to fix ideas, you can think that the function h here is y, simply, and you're specifying the mean of y as being a known function of x and an unknown parameter of theta. In practice, we may be interested in theta, or we may be interested in another parameter kappa, that would be our target parameter. k here is a known function, and kappa is what we care about. This is an extremely common setup in statistics, especially in economics. Many, many papers are relying on this type of setup. And what we want to do here is to allow for the possibility. Here is to allow for the possibility that theta, the parameter, is actually heterogeneous in the population. That's our main goal in the paper. So, how do we do that? Well, we write this theta parameter as the combination of two sub-parameters. Eta will always be constant. It's never heterogeneous. But beta will allow beta to be heterogeneous. Okay, so we're going to denote a draw from the beta distribution as B. And we say that B given X follows a distribution. And we say that B given X follows the distribution GX. So GX is the distribution of the random coefficient B. And we're not going to restrict this distribution, GX. That will be kind of the point of the paper, apart from the fact that Gx will belong to a neighborhood of a point mass at a particular beta parameter. Okay, so we're going to fix a beta parameter. We'll see what we do with beta. Whether we fix beta exante or whether we optimize with respect to beta as well. Or whether we optimize with respect to beta as well. We'll see there are different versions of the approach. But we're going to see this corresponds to homogeneous when my beta is constant. And we're allowing our distribution, heterogeneous distribution, to belong to the neighborhood of that Dirac measure, delta delta. What is the neighbourhood? It's defined by this moment, sobon moment of B with respect to the result distribution of B, integrated with respect to B and with respect to X. To B and with respect to X. Notice that the distribution of B given X depends on X. That's pretty important here. So we're allowing for correlated heterogeneity. Heterogeneity being correlated with the covariates. This is the expected squared Basserstein distance in L2 lower than a given epsilon square. Okay, so this is a sort of a Baserstein ball, a bit weird because it's expected with respect to X. A bit weird because it's expected with respect to x. We argue this is a very interpretable quantity because this is just a kind of a subon moment of b, which we can interpret easily in applications, and that's very useful to kind of have an idea of what epsilon could be, a reasonable epsilon could be. So what do we want to do in this context? We want to do three things. We want to bound the parameter of interest kappa in the presence of heterogeneity. So what are the implications of our model if the parameter Applications of our model if the parameter is indeed heterogeneous. That's the first goal. And the second and third goals is to provide estimators and inference methods, confidence intervals for kappa in the presence of heterogeneity. So that's the goal of the paper. And for that, we've used two approaches. One is a local approach, where the size of the research sample tends to zero with the sample size. We're going to see why we end up using a lot this type of local approach. A lot, this type of local approach, for tractability reasons. The other approach, the global approach, where epsilon, which might be more familiar to you, where epsilon, the size of the board is fixed as the sample size increases. So your previous line is the F? Ah, yes. Here, no, no, X is a draw from a distribution, but it's actually non-restricted here. But you accept X. But you define a ball that will get epsilon and take your expectations. With this expectation, Yeah. With this expectation, treat x as a base. No, no, no, no, it's an expectation actually only what's wet. Yeah, only what's average. Exactly. We average all the square distance with respect to x. Yeah, that's right. Okay. So we have two acquisitions in the paper, so one that is not today, but the one I'm going to focus on is a dynamic regression, dinner regression, a vector auto regression, to look at the effect of monetary shock on different macroeconomic aggregates when we have time varying parameters. When we have time-varying parameters. That will be our source of heterogeneity, parameter of heterogeneity in that setup. Okay, two examples to fix ideas: linear regression. Suppose you have a linear regression with an eta parameter, remember it takes always constant, and a beta parameter, which can be heterogeneous. And suppose we're interested in the mean of beta, the mean of B, right? Some kind of average margin effect. Seven example, logistic regression. Suppose your U is independent of X, it's logistic, and we're interested in some uh average partial effect. Some average partial effect, like the expected margin, you know, expected first derivative of the probability that y is equal to 1 given x. Okay? And take the derivative with respect to x to t. Okay, these two objects could be of interest, but of course, you can imagine the framework is much more general than that. Okay, how does that relate to the literature? Well, there is a lot of literature on allowing for heterogeneity in the model and estimating a complicated model with heterogeneity. There are lots of methods like that in statistics and in economics. Statistics or in economics, we're not doing that here. We are never estimating a general model under heterogeneity, we're estimating a model under homogeneity, and we're trying to adjust the predictions of the model and our inference for the presence of heterogeneity. There is a literature on omitted heterogeneity, what can be the consequences of omitted heterogeneity in practice. I want to flag two literatures that in economics are now very, very important. One is the literature on causal inference. One is the literature on causal inference, a local average Fitmont effect. The debates are all about Fitmont effect heterogeneity. That is omitted. The other one is difference in difference. A lot of literature is worked on that. Omitted heterogeneity in the Friedmont effect. What we propose here is methods for estimation and occurrence in this capacity. The third literature, of course, is robustness and sensitivity. That's really the approach we follow, with the difference that we focus on a particular question, which is to allow for heterogeneity. That's our question. That's our question. Okay? But it's very related, for example, our local approach is very related to Riga and other approaches are more related to what to what we saw yesterday or to what Huger did, for example. Okay, so let's start with the first task, which is to bound the parameter kappa of interest in the presence of heterogeneity. So we first define a bound on the parameter kappa. We define it first for fixed value theta. So suppose we fixed Value theta. So suppose we fix the center of the research temple. What is the set that kappa can take? The lower bound will be the infimum on the kappa parameter. Notice this is an expectation with respect to x and to b. So there is an implicit integral with respect to the distribution gx here. And the g belongs to that Vasar single. And in addition, we had, of course, the model restriction that the mean of H given X is equal to the mean of M given. Given x is equal to the mean of n given x. Again, there is an hidden integral here with respect to gx because of this heterogeneity. So that's the definition of the goal for fixed theta. We can define the bound for fixed theta. We can define a lower bound irrespective of theta that will hold for all thetas. Just in with respect to theta of this object. Turns out that this lower bound and the corresponding upper bound, they correspond to the identified set of Îº. And in practice, this bound. And in practice, these bounds can be our object of interest to try to quantify the implications of a model in the presence of the hypogenetics. How can we compute that? That's where the DRO connection comes in. So we can represent the computation of these bounds as a DRO problem. We need suitable regularity conditions that are not listening here. But under those conditions, we can write the lower bound, for example, as this. The lower bound, for example, has this rather complicated looking uh uh expression. It's like a uh a mean, max, mean uh problem, right? But what is I think very intuitive here, we have two large two multipliers, right? We have the lambda that corresponds to the fact that we improved the model restriction, conditional mean, and we have theta that corresponds to the vasso-chain ball restriction. And if we wanted to confuse this bounds that hold for all thetas, we will need to solve an outer minimization problem. An outer minimization problem, in addition. So, as we can see, this is a rather daunting problem, and doing estimation and inference in this context is difficult. Hence, our use of a local approach. Okay, that's really this local small epsilon approximation will really be useful to kind of simplify your loss the problem at the cost of, of course, an approximation, a minorization. So, lambda prime, this uh this does it depends on at this point depends on the problem. Yeah, yeah, it it does because inside the expectation. Yeah, absolutely. Same thing for the setup. Because each element inside this neighborhood, they are specific that are defined for each x-value separately. But the constraint, of course, is across axes. That's where we see the constraint. Okay, so here is a very simple example. The simpler we can simple we can simplest we can imagine for regression with a scalar regressor. The intercept is constant, but the slope can be heterogeneous. But the slope can be heterogeneous. We are interested in the average slope. Under a particular restriction, it turns out that the mean and the max, the fixed theta bounds for the mean and the max, they coincide and are equal to this expression. Just to give you a hint of an intuition about that, suppose that this was indeed that we did not have a constant, that eta was not there, actually this would boil down to the expectation y given x, which turns out to be an. Given x, which turns out to be an unbiased, under suitable assumption of x, of course, an unbiased estimator for the average token net model. Of course, with this constant, people get that, and that's the expression that the power will give us. So here is one slide, a small slide that turns out to be important in practice. The bounds that we have defined so far, they account for two things. For the presence of heterogeneity in the parameter, and for the fact that the conditional mean can be misspecified. Conditional mean can be misspecified. In practice, we might think that we can check that, or we can assume, that our conditional mean is quite specified and still want to account for the implication of heterogeneity in our quantity of interest, in our counterfactual analysis, for example, in economics. And you could do that by imposing this extra constraint. That this condition is equal to this as particular eta beta m. Particular theta. Turns out the bounds in that case are very similar to the previous. Bounds in that case are very similar to the previous one, just there's one slight twist in the expression, in the Lagrangian expression. As an example, in a linear regression, what you get in that case is that the lower and the upper bound, they coincide and they are equal to beta in that case. Okay, so this average slope turns out to be equal to beta. It's not a very interesting case. Here is a slide where which where now things uh get uh uh interesting and will be actually turned out to be more useful for practice, because that's really Useful for practice, because that's really what will be a bridge for us to be able to do estimation on the field, to bring in the sample into the problem. And that's this local approximation. So suppose for now that the radius of the Bassox channel is standing to zero. And can we approximate the lower and the upper bounds? And the answer is yes. And here is the approximation. The approximation, this is the quantity of interest under the homogeneous model. Under the homogeneous model. These are the bounds at a particular theta value. We fix the theta value here. So this is the prediction you would make if you believed in the homogeneous model. But then you add or subtract, depending on whether it's an upper or lower bound, this quantity here. This quantity that depends on the derivative of your quantity of interest and this M matrix, which I'll tell you to be just a projection matrix. What is really important for this quantity Really important for this quantity is that it is available in close form. There is no optimization involved here. So, the great thing about this local approximation is that it transforms a very complicated variational problem into something that is very simple to write down and then to use for estimation and to produce confidence intervals that would be valid in an asymptotic where epsilon tends to zero at a suitable rate when the sample size increases. When the sample size increases. So that's kind of the tractability payoff of this local approximation. And in particular, any estimate such that this quantity is equal to zero, we can say it is locally robust in the presence of epigenetics. Now, does locale robustness mean robustness? The answer is no. This locale robustness, of course, is robustness with respect to a tiny amount of heterogeneity. Tiny amount of heterogeneity. More specifically, an infinity is in high amount of heterogeneity. You can imagine there are examples of functionals. For example, the mean of the square of it. Learning about the variance in this problem is very hard, for example. Or a quantity that is interesting for economists, which is the so-called social surplus in discrete choice models. These quantities turn out to be locally robust, because the quantity of the previous slide is equal to zero. It doesn't mean that by using a That by using a modulus model, you get it right. Of course, you might still make mistakes. But these mistakes will be small as long as the integration is not too large. That's the promise of this linear approximation for small etc. Okay, so we see a kind of a complementary approach between the global approach and the local approach to robust etc. Okay, so now that I've explained the first part, let me turn to the second part: estimation. Turn to the second part, estimation on inference. So, for estimation, we're trying to estimate this care value. So, we're going to first define a class of estimators that comes from a method of known GMM type of theory. So, we will consider a class of what are called one-step estimators. We take the mean of k at the premier estimator theta hat, and then we add an adjustment. An adjustment. It turns out many, many method of moment estimators can be written of that form for a suitable C. And we're going to choose the best C, or we're going to account for the fact that we have such a C that defines a given estimator, and given that estimator, we will be able to make inference. So here the primary estimator, Fita Hat, is root n consistent, but we don't need more than that. Need more than that, because we're going to impose what we call an orthogonality constraint, a standard constraint and method of moment estimation, which is that the weight satisfy this orthodoxy condition. This sort of condition, what it gives us, is that the asymptotic distribution of kappa hat on the correct specification is insensitive, it doesn't depend on first order, on the choice of the preliminary estimator. So, this is a constraint that we impose on the class of estimators, right? They are written like this: for a C, that satisfies this. For a C that satisfies this sort of condition. So with this, we can actually define the worst case bias of the estimator. And that will be a type of construction that will be very similar to the bounds that we had before. It's a different problem, but it's kind of a similar construction. What is the worst case bias? Well, you can see this expression here is a supremum of a whole distribution of this difference, which is basically at the fifth. Of this difference, which is basically at the fixed theta, the expectation of the estimator minus the target parameter, subject to the fact the model restrictions are satisfied, and the distribution belongs to the research table. So the same two restrictions. So this expression is a little bit hard to compute. We can also characterize it as we did before. But we can also, instead of using And also, instead of using a global DRO representation, we use a small epsilon approximation matrix. And what do we get? And that's where we get something that starts to be really useful for practice. We get an explicit expression for the worst case bias for our C. So we're in a context where our GMM, our method of moment estimators, are quite general, depending on this weight function C. And the worst case bias is proportional to epsilon. If epsilon is equal to zero, there is no bias. But the constant here, this The constant here, this square root, is a variable in closed form. Look, it depends on this Jacobians here and C. There's a lot of infusion built in that formula, of course, that we can see in different expressions. So what can we do here with this? Well, the fact that the bias depends on C in this way suggests two things. One thing is we could try to account for the presence of a bias of our estimator when we Of our estimator when we con when we construct confidence intervals, measures of statistical uncertainty for the kappa. The second thing is maybe we can choose the C such that the bias is small. What are good estimators that minimize the bias or minimize the mean squared error in some way? And that's exactly what we do here. So, for the confidence intervals, it turns out the LLV is very simple, as long as we assume that epsilon turns to zero at the same rate as one there is. At the same rate as one by the square root of the sample size. So, in that case, the squared bias and the variance have the same order of magnitude. So, the variance at this super simple expression, a sympathetic variance, and the confidence interval, what is that? Well, we're going to take our estimator, we're going to add 1.96 times the standard error, so that one should be without any problem, but we're going to add also the bytes. We're going to add also the bias. We're going to add the bias on both sides to enlarge the confidence in the error. Now, this confidence in the error, the claim is that it accounts not only for something uncertain, but also for modeling specification. Because we're not accounting for heterogeneity in our model. We're just accounting for heterogeneity in the definition of the worst-case bias. So may I just say setting epsilon in this way makes the terms work out, but why in particular should we think epsilon should behave as one? We think epsilon should behave as one spherical time. Yeah, yeah, I agree. That's, you know, so, okay. So that's the cost, right? The cost is we are assuming that this epsilon tends to zero at that particular way. The payoff is that we do get confidence intervals and inference methods that are simple. Is there any sort of prescription for how one should set this step so well? Yeah, yeah, so we're okay. I'm going to mention that. Maybe I can mention it right now. So it turns out here the FC Dun. Now, so it turns out here the epsilon is really, we can think of it as the sequence moment of the epsilon squared would be kind of the sequence moment of the city. So, you could think of different ways of thinking about that. One would be maybe I have estimated my model on subsamples, and I start getting different values for that. I compute the variance across this sub-sample of my parameter, that gives me some sort of a lower bound. You can actually try to get something that is more formal than this by really computing a lower bound given your data. Another possibility we Given your data. Another possibility, which is what we use in the application, I might have time to show, which is if somebody has estimated a model with heterogeneity before, like a random coefficient model, you could actually use that as a focal value to think about how epsilon, you know, what epsilon is. The fact that it turns to zero exactly at that rate, it's a little bit more of a delicate question, right? But that's what we need here for the students. Exactly, exactly. Exactly. So the alternative here, which we are very sympathetic towards, is the global alternative. The bottleneck is the influence part. It's very, very hard. So that's why we think that for now, for practice, it seems like a reasonable recommendation. Okay, now what can we do to choose a C that minimizes the mean square, the notion of risk? We look at worst case mean square error that is in this local approach, which is the sum of the variance of the square by s. And here is the And here is the result. The result is very simple: the variance is quadratic in C, the bias, the worst case bias is quadratic in C. Oh, remember, we need to minimize that subject to the orthogonality constraint. Orthogonal constraint is linear in C. So it's quadratic subject to linear. This is really a simple problem that can be solved in close form. Actually, very generally, let me give you two examples. One is regression. So here is what happens to worst case bias in the regression model. Bias in the regression model. We are interested in the average local. This is worse scale bias. Suppose there is no constant to give you an intuition. You would pick the C equals one over X. It turns out that it sets the bias to zero in that way, if you can do it. But with the constraints, you cannot do that. Because you have to satisfy the automated constraint. That's the expression of the bias. And this is the expression of the minimum mean squared over C. And when you look at it, this looks like a rich rate. When you look at it, this looks like a ridge-regularized version of that one over x. If you divide by x squared, which have a kind of a kind of some kind of ridge penalty here, that depends on what, on the variance, for x. Actually, when epsilon is equal to zero, you coincide with the efficient estimator, which is DLS in this model. And it depends also on epsilon times l, right? It's epsilon times to infinity, you get back to this one over x type of thing. Okay? So that was a linear model, that's very simple, but this is non-linear model. Model that's very simple, but this is non-linear model, this is logic, and this is actually the same thing. So, or the logic model, you just expressions are much more complicated, okay? They are not that tool on that thing. The point is there is no optimization. It's a closed-form expression. There are some matrix inverses, but these are lower-dimensional matrices. Okay, so it's a closed-form expression for the best C. Close-form expression for the bias and for your confidence intervals. That's the payoff. So, the cost is this fiction that the epsilon tells you. This fiction that the epsilon transfer at that rate. And so, and perhaps the quality of the approximation is not that good. Okay, so we can also impose correct specification of the model, as I mentioned before. It turns out the bias doesn't depend on the choose the efficient estimator in that case. That's just a footnote, in a way. Something that is more interesting would be the two things I want to talk about here is: one remark is: we'll allow the B to. Will allow the B to depend on X, right? Gx depends on X. It turns out that it's really important to do that. All of the action in the effect of heterogeneity locally is played by the correlation between B and X. If we impose that B was independent of X, which sometimes people do in applications, locally when there is a tiny amount of heterogeneity, that effect is supernormal. The first order effect, when we introduce a bit of heterogeneity, comes from the correlation between heterogeneity and the client, not from the dispersion. Not from the dispersion of the etiology conditional correct. That's kind of one point. Of course, globally, that's not true, but look at this. And then the other thing is what I was mentioning before about the choice of this. Our recommendation in the end is to proceed in the form of sonic theory analysis. It's important for the sense of what Texan can be. We think for this particular problem, it's actually relatively easy to get a sense of what the dispersion of this coefficient can be. Okay, I have just a couple of minutes, so let me show you the application. This is a dynamic linear system that is very common in macroeconomics, where you would look at unemployment, interest rate on inflation, that's the y here, it's a vector, how it depends on the lags. And you want to know when a shock here, which with a particular rotation matrix, is interpreted as a monetary policy shock, how it propagates over time, how it affects inflation. How it affects inflation and how it affects interest rates and all the prime monetary. And there is a lot of interest in the literature in letting the coefficients of that system vary over time themselves. Why? Because they think maybe polytaric policy rules have changed over time. Maybe there have been structural breaks, structural change, and so on. So what can you do to allow for time-varying parameters? The first thing you can do is you can estimate the model with time-varying parameters. Estimate the model with time-bound parameters. But this is hard. First, you have to make parametric assumptions. And you have to be sure that those are reasonable. People use Gaussian, you know, typically independent of the initial conditions. The second thing is you have to estimate the model. Now, it's a Bagesian model. It's no longer a simple regression. It's much harder. So our method completely circumvents that. We just estimate the model by EOS, and then we adjust the predictions of the model, the confidence intervals. Of the model, the confidence intervals, using our bias calculations. And here we take data from a premium chain, and here is what we get, just to give a sense. The black line here would be the confidence interval of this impulse response function, so the dynamic response of a monetary policy shock, on inflation. Over time, those are quarters. And you see the response is actually quite, you know, the confidence interval is actually quite quiet in that case. Well, our method unsurprisingly makes the confidence interval well. Unsurprisingly, it makes the confidence interval wider. When we look at what happens to an employment now, when you look at the standard constant coefficients, not time varying coefficient confidence interval, it's tighter, especially here we can actually rule out that the effect is equal to zero. Well, when we allow for our uh confi you know, heterogeneity robust confidence intervals, we find something that is still not very wide, but a bit wider and unfortunately we don't conf that we have non-zero effectiveness. Okay. That we have non-zero effectivity. Okay, just a simple illustration to tell us that this is a very simple way to assess the potential effect of heterogeneity in this type of settings. The colours mean you're looking at each parameter. Yeah, that's right. That's right. The bigger one is when we look at all of them at the same time. They will all be heterogeneous. Beautiful timing, thank you. We have time for questions. So you assume that the center of the ball is the back. And to what extent you can generalize alternate? Yeah, I mean that's kind of the point of the paper is to allow for to look at this particular neighborhood. Yeah, so we haven't looked at that in We haven't looked at that. In principle, there is no reason why that should not be generalizable to other distributions. That would depend on some parameters. And then, how we treat these parameters in a local versus non-local approach would differ. That's right. And we could think of our baseline might be that we have a parametric model, for example, and we are looking at an unrestricted model that lies within the vassal chain goal of that particular parametric model. If you think we could have developed more scales and have Would be the worst case parameter or distribution that leads you to yeah, so in the local approach is not so nice actually. Yeah, yeah, it's kind of it's really yeah, it's it's really uh it yeah uh in we haven't done calculations, we should do that in the global approach to uh to kind of get a sense yeah what will be the project. I think it's a point mass actually. Yeah, so it's thank you for the very interesting talk. So it's the methodology applicable to study the violation of strong ignorability assumptions in causal inference, saying that, you know, even conditional on the observable compoundings, there are still some unobservable compoundings causing the, you know, um heterogeneous in the you know conditional treatment effect. That you're not conditional treatment. Is that applicable to that? Yeah, subject to the particular conditional moment restriction setup that we have here. Yeah, that's right. So, in principle, these types of sensitivity analysis approach, we can use them very generally, you know, defining the neighborhoods differently. Here, we need to think about how we would define the neighborhoods in that particular case. That no, I don't think that would be exactly the same problem, but very similar uh conceptual. Uh was it you? Bell rang. Thank you again.