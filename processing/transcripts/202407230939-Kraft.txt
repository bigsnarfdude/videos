A lot of the things that we've been talking about over the last day and a half. I'm going to maybe get in the weeds on a couple specific examples, things that we're working on in the division of cancer-reported neural networks. Some of this might be quite simplistic for a few of you, but I hope for others it'll make some of the broad concepts we've been talking about a little more concrete. So, clearly, there are benefits to cross-study collaboration. We will need very large sample sizes. Will need very large sample sizes in order to study small effects, rare exposures. We also need large, diverse samples in order to assess heterogeneity across all kinds of different factors, whether that's genetic ancestry, demographic factors, socioeconomic status, race and ethnicity, whether that's actually clinical contexts, like how care is being delivered. All those things are the important sources of heterogeneity that we want to understand, and any single study. Understand and any single study isn't going to be able to address that. So we need cost of collaboration. And finally, to the extent that previous studies have made their data available, it makes new research much faster and allows many more people to try out new and innovative ideas. So we have lots of legacy cohorts that have been around for 40 or 70 years in some cases. Seven years in some cases of under the math right, which loss of longitudinal follow-up and repeated questionnaires. So, of course, you only get 40 years of follow-up by following people for 40 years, so you can take advantage of that now. But there are challenges, and I'm just highlighting three that I'll be talking about today, and there are many more which we've been discussing. So, one is effective governance and data custodianship. So, we want to make sure that we're sharing data. Make sure that we're sharing data broadly but consistent with patient consent and ethically. Data operability, we talked about yesterday, people collect information allegedly on the same variable, but in very different formats. So harmonizing that to enable analyses is a challenge. And finally, every study is a little different. Every study has its unique characteristics that we need to keep in mind. And there are some challenges. And there are some challenges with the cross-sectional biobanks on two EHRs, especially on how people are sampled and the kinds of measurements we have on phenotypes and exposures. So I'll talk about how we're trying to address some of those. So again, so if people aren't familiar with the FAIR principles, so FAIR, I always have to remind myself, stands for findable, acceptable, interop, sorry, accessible, interoperable, and usable. And usable, and a lot of my thinking in this area has been influenced by this paper developed by folks at DCEG before I joined. So this is actually one of the things that intrigued me in taking the position as director of the Transdivisional Research Program in DCDG. And there's sort of two There's sort of two degrees of fairness scales that Matson, Carson Plosis, and others talk about in this paper that I think are relevant for our thoughts this week. The first is, I'm going to say, sort of on the infrastructure scale, going from what a lot of legacy cohorts have been doing for years and years and years, which is keeping the data on site. Data dictionaries are on site, code is on site. It was within my research lifetime that if you wanted to. Research lifetime: that if you wanted to collaborate on some of these studies, you had to physically come and sit at this one computer in the unair-conditioned room to run your analyses. And even today, there are data sets which, for regulatory reasons, you can't even do that. It's like you email their programmers, they will run the analysis and send you back the tables after they've vetted the tables to make sure that there's no write-in file intervention. We were identifying the mention. So that's on one end. And the other end, you have cloud-enabled services for data, storage, governance, and computation, which we're hoping to go to. I'm going to have two examples of that in my talk. And then the other scale is in terms of maybe the informatics, let's call it, going from things like PDFs and Word docs for data documentation all the way through JSON formats and linked data formats. So it just computationally makes managing the data on. makes managing the data and understanding like computers can understand the data much better than if it's on somebody's handwritten notes. So I'm going to talk about these three challenges and the first one I'm going to talk about is effective governance and data custodianship. And the key thing here is that human subjects data require safeguards and that requires trust and trust often requires people. So the example I want to use to highlight these challenges is the breast cancer risk. These challenges is the breast cancer risk prediction project. So, this is an intramural-extramural collaboration bringing together 20 cohorts to try and build a better mousetrap and then evaluate how this Suptenats breast cancer model works across different contexts. So, these are most, not all, of the studies that are involved. The details, the individual names really don't matter. It's just here are 20 of them. Just there are 20 of them. I'm not going to talk about this, but relevant to Nelange's last talk, you can see those gray and white columns are available of different types of data. So questionnaire risk factors, hormone levels, mammographic density, polygenic risk scores, and then data on tumor subtypes. So not all studies have all those information, all those data types, but some of them do. So we're really hoping to leverage some of the So we're really hoping to leverage some of the some of the methods that Melanji and others are developing to be able to make use of all this data rather than just subs subsetting to complete case analyses. But for this talk, I'm really going to focus on how you get, how you heard those 20 cats. So the way things folks have done things for a while and still do things sometimes is we have a bunch of collaborators at many different institutions and we write a bunch of pairwise data use agreements. Pairwise data use agreements. So here we have eight collaborators institutions, so that's eight choose two DUAs. Which maybe, you know, it takes 18 months, but then, you know, you get it all set and you're good to go. Are you tired or teared? Tired. I didn't mean tired. You've been tired. And then, of course, if you have a new collaborator from. A new collaborator from a new institution, you want to share the data with the broader research community, but somebody new comes along, you're going to have to do another eight DUAs to do that. So that's tiring. So one alternative to that is to designate a data custodian. So now you have an institution whose job it is not to Whose job it is not to the data stewards, they're not going to say what happens with the data, but they're basically in charge of the operations. Everybody trusts these people to keep the data secure and to grant access only to the folks who have access. So all the institutions sign an agreement with the data custodian, but that's it. So there's now seven agreements among the collaborators, and any time a new institution comes along, you sign one more. So this is the governance structure that we've adopted. The governance structure that we've adopted in the breast cancer risk prediction project with NCI serving as the data custodian. And the key thing here is that the data stewardship role is played by representatives from each of the collaborative institutions. So the collaborators get to decide, yes, this is a reasonable use for our data, and you, data custodian, are allowed to send to give access to this. So that's just another another So that's just another visualization of this data government structure. And then we've operationalized this using cloud tools. So, in particular, we use Box. There's nothing special about Box. But we can use some of their tools for user authentication. So the idea is the data live in the cloud, in the box folder. It's restricted access. New collaborator comes along and wants to analyze the data. comes along wants to analyze the data. The data custodian creates an analysis data set for them, puts it in a folder, gives them access. And now they can analyze it any way they want. We are strongly encouraging people, it's in the DA, that you shouldn't download the data, but you can access it using your local laptop if you wanted using boxes like the go. You access the box folder, you run your analysis, you put the analysis, you save the results, and it's good. And it's good. So, the advantage of that is that it's super flexible. You don't have to learn a new TRE. And if you want to analyze it on your local machine, you want to use cloud computing resources on Canvas. This is our landing page. So please do check it out. There's some details, like all the descriptive statistics will be moving a descriptive statistic dashboard in. Is a dashboard in front of the login. It's on to be risked, sorry. So you'll be able to sort of see how many cases are there, what's the breakdown by race and ethnicity, the breakdown by age. There's instructions on how to apply MR Access. All that goes on this page. So that's sort of building a governance structure and a data custodian structure for legacy cohorts, which is only part of it, because now we need to talk. Which is only part of it, because now we need to talk about data interoperability. So, those 20 cohorts have each collected smoking information in very specific ways. So, how do we get them to talk to each other? So, right, so we'll have these data dictionaries that have very similar content but very different structure. So, this makes it hard to sort of search and see who has which data. It's a barrier to harmonization. Barrier harmonization within and between settings. So, even knowing who I should be talking to to do this collaboration, and then even what you do is still going to be hard to know exactly how you should go about doing it. How fine detail can I actually get? So, what folks have been doing for a long, long time, and this is actually what we're doing in the breast cancer roast prediction project, is project-specific harmonization. So, we, you know, the investigators in the consortium sat down and drafted a data dictionary. Sat down and drafted a data dictionary that roughly we figured most people would be able to harmonize tool. Met our needs for our research purposes, and we figured most people would have this data and would be able to map it to our vocabulary. We sent it off to them. They, each at their institution, had their programmer create a data set that was in that data format. They sent it back to us. Of course, it wasn't quite in that data format, so we sent it back to them, and they sent it back to us. So this is. So, this is, I mean, it works, but you basically have to do it every time you have a new collaboration. It's siloed. You know, it really depends on folks at each institution having the bandwidth to be able to do this. And again, it's limited reusability. So now you've got stuff that works for the breast cancer risk prediction project, but if you're interested in diabetes and cancer, maybe that's not exactly the data that we're trying to care about. Care about. So, what Jaya Pala Subarmani and the data science and engineering group at DCG is working on is leveraging large language models and some of these modern data formats to automate a lot of these steps. So, could you take a data dictionary, theod dictionaries from multiple studies, and say, I want to harmonize it to this set of variables from OMAR. So basically, the LLM will sort of say, well, these four variables in this data entry seem like they're giving you the relevant information. So that's the vocabulary mapping step. The goal stands, the goal here where we'd like to go is to actually have the LLM do some of the work in terms of, and then if you run this code, that'll transform the raw data into the harmonized version. So that's the goal. We'll see if we can get there. But even. Get there. But even, I think we'd be excited, even with the vocabulary mapping, because that would enable basically a nice lookup tool where you could go to a set of cohorts who are willing to share their data dictionaries. That's it, just the data dictionary, not the data itself. And you could query that, and it could give you good information about which of these cohorts are going to have the relevant information and what, and give you a huge leg up in terms of how do you actually go about getting the prominence of that. And once you've got things that are harmonized, And once you've got things that are harmonized, then there's a question of mapping across multiple projects. So, Nicole Grulank, who's the chief data scientist for the Connect study, which I'll talk about in a second, has been working a lot on interoperability within the Connect study. So, how do we not regret the decisions we made now 10 years down the line? Like, why the hell did we name the variable that? So, she's been working a lot on this concept dropping idea. Concept mapping idea. And the hope is once you have a set of concept IDs and do the work of mapping one to another standard, like going from connect to OMOC, then you've essentially mapped from connect to OMOC to a bunch of other resources that are also using OMAP to the mod. So that's hopefully we'll be able to make that out next time. Okay, and then I want to close, given the curve lawsuit, how much time do I have? Given the kerfuffle, how much time do I have? I'm fine. Okay. Then it was the last one. So, in terms of addressing some of the limitations to real-world data, so actually I was really glad that Neil started off the workshop with his refresher of basic epi. Well appreciated. So there are challenges to the analysis of a lot of biomedic studies because the ascertainment scheme is often poorly understood. Poorly understood, and that has implications for what we can and can't do or how we interpret our results. Exposure and phenotyping data, as we talked about a lot yesterday, are not collected for research purposes. They're great. They're an amazing resource. We should take advantage of them, but there are limitations. And there's often a lack of systematic biosample collection. So, why, like in a clinical repository, why tests are. In a clinical repository, why tests are being run, that's often there's an indication why it's there. Like running the test because we think you're not well. So, and you're grabbing something, again, pulling something from the freezers or grabbing going into the bowels of the eighth sub-basement to pull something, to pull a slide out of some storage. It may not be very sort of quality biosample that you need to run modern essays. So, to address some. So, to address some of these challenges, the NCI launched the Connect for Cancer Prevention Study. So, I'm borrowing a lot of these slides with minor modifications from Nia Gardev, who's the chief scientist on this project. So, Connect is aiming to enroll 20,000 sorry it's funny 200,000 volumes across the United States. Across the United States. And we're doing this leveraging our partners in 10 integrated healthcare systems. So, one of the key advantages of this is that we have a defined technical competition. We're basically inviting everybody in the healthcare system to come in. So, we're not going to get everybody and talk at the coffee break about what our yield is. But we'll know who was invited and we'll know who responded. So, in some sense, we can infer back to the study days. The study base. So we're getting survey and EHR data, so survey at baseline, and we're also a rapid case ascertainment algorithm so that we can go out and get some precursor biopsies and things like that in real time. So these are just the catchment areas for those 10 healthcare systems. We recently added, thank you CPRIT for the funding for the site, Gabriel Scott and White in Texas. This is the This is the demographic breakdown. So we always like to do better, but we're doing reasonably well in terms of racial and ethnic diversity. And we're pretty confident that the inclusion of Baylor Scott and White will push up our Latinx numbers recently. So this slide, just the title, it says 30,000. So this breakdown is over 30,000, but we've now enrolled over 40,000. But we've now enrolled over 40,000 people. So we are on track to get 200,000, 27. So at baseline, folks get a set of modules. There's an extra Connect app, and that's where people are answering these surveys. So we're getting some basic demographic and medical history, some key focus. Exposures and some information about where we're doing work. We're actually getting detailed residential histories, which will help us do GIS mapping to the various environmental exposures and socioeconomic as well. These are some of the surveys that are currently in development. So we'll be pushing out surveys sort of in little bits so we don't overwhelm people. And these are the modules that we're hoping to send out soon. So, as part of the So, as part of building the infrastructure for the study, we developed an open source survey rendering application. So we kind of didn't want to be beholden to a commercial company which would control not only the interface, but also the back end. So, this allows us to control our own app, and the data go right into Google Cloud. So to So, and we're hoping that's a useful tool for the community. And as I mentioned, we're doing all kinds of linkages, so we definitely consider that we could link to cancer registries, the mortality from the National Death Index, and having the residential history allows us to link to geospatial data. These are the biospecimens. So, systematically, we're getting biospecimens from folks, either some or they come. Folks, either some or they come into the clinic and get a specialized blood draw. Some collections are doing at home, the fecal collection, and the urine is also being done at home. It's all being collected in a systematic way and being sent to a central biorepository. I've highlighted the fact that we've just in the last year we've moved to collecting blood and stret tubes because that is the one tube that's going to work for. The one tool that's going to work for things like Cell 3D tank. Okay, and right from the beginning, we wanted to build to make sure that the tests the data from Connect were available to the research community broadly in real time. So that includes academic investigators, but also commercial and international. So this is all built into the consent, it's built into the agreements with the healthcare. Of the agreements with the healthcare systems. So the sort of conceptual governance framework is there. Now we just have to build the computer infrastructure. And this is our fabulous, you're not supposed to actually pay attention to the details here. It's just like it's fancy. But again, similar to what we had done with the West Net Service Prediction Project, we're really hoping to make this as flexible. Make this as flexible as possible for analysts so that you don't have to learn a new TRM. If you are doing an analysis, it's a case control analysis of 5,000 people and you can totally run it. Use R Studio on your computer, you want to make sure that you can access the data using APIs in a secure fashion. So you can run if you want to analyze the GWAS data and do a FIWAS. And do a few us, and you want to spin things up in the cloud, you can do that to me. That's where we hope to be going. So, the initial target date for releasing the initial set of data, probably the initial questionnaires, is going to be 2026. So we're starting the genotyping, the GWAS genotyping this fall. So, again, we're going to genotype everybody, so the target is tomorrow, Okay, so again, I just want to emphasize that those are cherry-picked examples of the challenges that we face. We've already talked about a lot of others, and they're also very, obviously bespoke, cherry-picked examples about how to tackle these, and there's lots of other ways of doing it. Federated analysis is actually something we're wrestling with and exploring as well, because in the breast cancer risk prediction project, some of the data sets are Some of the data sets are under a GDPR, so you can't shuttle in using that model. So we're thinking of how to do that in a more efficient fashion, other than just sending them code and asking them to run it. But if that's what we have to do, that's what we have. So just some particular acknowledgments, of course, Castle Thousands on all of this. And Nelanjan is the co-PI on the Blessed Management Respiration Project. Project and these are some of the folks who've been working on building out Connect. And we do really want to emphasize y'all are a part of building out this. So we are open to proposals for sub-studies, adding new questionnaires, new biomarkers to collect assays, all that is on the table. So check it out. I have a one question. Thank you for such a great talk. So I'm not very familiar with this whole area, but I was wondering, how is what you're proposing with this ecosystem similar or different to say data platforms such as Gen, which is dealing with the ecosystem? So that is a good question. So that is a good question, and I am not the person to answer it in any detail whatsoever. So, there are similarities, and there are things we scoped out, but I think because we wanted to maximize flexibility and sort of not lock ourselves into flexibility and independence instead of that one. So, but yes, the So, but yes, there are a bunch of models that are out there that are doing very similar things. And we are also doing similar things. They're on our way to school. Maybe we'll take the rest of the questions for Freight. Um Freight says we'll call Freight Hill ten fifteen ten fifteen ish. Cool. Usually 10 fifteen edge cool.