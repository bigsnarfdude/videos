My machine and discuss the results. Okay, let's briefly consider Massive IoT. So, the Massif IoT or Massive Machine Type Communications is a new scenario which was in 5G, right? For now, it is a problem also for 6G. And the main features are as follows. So, huge amount of autonomous devices connected to a single base station, short data packets, sporadic access, so it's not like human users, right? Like human users, right? These devices with very short packets, and also extremely low energy consumption because the devices are autonomous. We should focus on energy efficiency. And it was discussed many times in 3GPP committee, and it was assumed that grain-free transmission is good and should be considered. There were many suggestions or proposals to 3GPP, like SCMA, RSMA, so it was all. So it was also MUSA and so on. But okay, it is difficult to just compare with these proposals because they are not very well detailed, right? That is why in the literature by Yuri Paliansky, the new model was introduced, the new random access model, which was later called unsourced random access. So the model is as follows. So we have a lot of active users, so K total. Active users, so k total. We will not focus on what follows on this quantity because it's of no interest for us, but we have only ka active users. So it's a classical case. Okay, let me maybe do it like this. It's a classical case of partial user activity. So and we don't know which users are active. But in what follows, we will assume that we know the exact number. It will not be a random number. So Ka will be fixed and known to the transmitter and to the receiver. To the transmitter and to the receiver. Communication proceeds in frame synchronized fashion. And the length of frame is n. Each active user wants to transmit k-bits. And also, so this is the novelty by Yuri, but it is used in random access, but it was, so people were not too focused on it. All the users use the same encoder because of a massive number of users, because of huge number of users. Because of huge number of users, this is actually a good suggestion because, uh, okay, to create different encoders and then to decode the mixture of signals is very difficult. You should understand which users are active at each time instant, then choose proper decoders, then decode. So, same code book or same encoder is much simpler for the receiver. So, and here we also note the usual thing: the power constraint, which is given like this. Constraint which is given like this. Okay, let's move forward. So, this will be the metrics. So, I presented all the metrics which are used usually in the literature, but actually today we'll focus only on user probability of error. So, okay, let T be the transmitted messages. So, this will be the set of transmitted messages. T is the first letter of this word. So, I think this is much simpler to write it as T. R will be the received message. Be the received message, the set of received messages. Then we calculate per user probability of error like this. So, this definition works even for a random number of transmitted messages, but as we know exactly the number of transmitted messages, it is Ka. It can be rewritten like this. So, this is classical definition by Uri and also false alarm rate, which we don't need because our received, so the size of the received set is the same as the size of. Is the same as the size of transmitted set. So we introduce false alarm rate like this, but okay, it can be calculated as a function. For our case, it is a function of per user probability of error. If you know this quantity, you can easily understand what is false alarm rate. Again, I forgot to say that the decoder is only required to output the set of messages up to the permutation. So we split identification following Uri, we split it. Identification following Urily split identification and decoding of messages. Okay, all of these models. So, this model is introduced in this paper, and this paper also gives the fundamental bound on energy efficiency. Okay, let's move forward, and energy efficiency will be calculated like this. So, okay, it is E-bits divided by zero or energy per bit. So, it is calculated like this. Okay, sorry. So, for Gaussian channel, we should divide by two here for my mature. Divide by two here for my material. We will not divide by two, but okay, you will forgive me that I forgot this two here, but nevertheless, all we know how to calculate these quantities. Okay, now this is our main parameter. Okay, AWGM case. So let me tell the story first of all, because it's a talk and I tell the whole story, it's not a paper. So actually, I was excited by several papers on fundamental limits for. Papers on fundamental limits for my Mac channel. I will show them a little bit later. And they used different techniques to calculate fundamental limits. It is called Vanos method or Goodrichen method. So in contrast to Urey method, which is Gallagher's raw trick, applied twice. So this method is different. And it appeared also that in the literature on sparks, so there is a book, the same method is applied for sparks, sparse regression codes. So, okay, it was an interesting question if this method. Interesting question if this method allows to improve fundamental bows for AWGM keys. And for now, I will show how to apply it and the results. And finally, we'll discuss what it allows to do. Okay, nevertheless, let us start with the channel model. Okay, the channel is given like this. I recall that F is the encoder, W is a message, T is a transmitted set, and so everything here, I think, clear. So just some code words which correspond to the messages. Which corresponds to the messages, and then add Gaussian noise, which here which is here and fixed. So, I mean that okay, it has fixed variance, unit variance. Okay, let's move forward. So, we will need such definition. So, it is called CI. So, it's the sum of code words in, so which correspond to the set I. So, in what follows, we will use different sets as I, but nevertheless, CI means that we sum these code words. Nevertheless, CI means that we sum these code curves. And so the decode is usual. So it was proposed by Yuri. It is not optimal in terms of per-user probability of error. It is optimal for joint error probability, right? But nevertheless, this decoder we can analyze, and that is why we will also consider this decoder. Okay, definitely it should be argbin here, right? Sorry, it's a typo. I found it. It's a typer. I found it. I will fix it in the final slides. So, Argmin, definitely, we need to find the closest set of messages, right? Okay, state of the art for the Gaussian channel. I talked already about this paper by Yuri. So, this is also interesting paper by Ilya Zadik, Yuri, and Gauthers. So, this is about improved bounds, and they use so-called Gordon's method, Gordon's lemma. So, it's very interesting, but only asymptotic. Very interesting, but only asymptotically, right? In asymptotic regimes, they achieved, so they found improvement, but for finite lengths, it is still not finished. So, this is interesting open question. Also, I mentioned that, okay, these papers consider fixed number of users. Ka is fixed, but okay, in the third paper, it was generalized for the random number, Ka. So, K is a random variable. So, okay, the receiver should first decide somehow what. Should first decide somehow what is KA and then already and then decode messages and also similar bounds present here in the literature on sparse regression codes. So the idea is similar, but sparse regression codes is similar to different codebook case. It's a little bit different, but nevertheless, the ideas are very similar. Okay, let's move forward. And so this is the usual definition of Gaussian codebook. It is presented here. So this is Gaussian codebook which we use in what follows. Which we use, and what follows. Each entry is sampled, so independently, identically distributed from this distribution. And so, the theorem which can be obtained from this Fanos method is presented here. So, you see that, okay, it's not clear what is going on usually in these theorems, but nevertheless, I will explain the proof of the sketch. I will present the sketch of the proof, and it will be clear. And it will be clear. Okay, let's move forward. So, first of all, okay, we need to introduce T-era events. So, first of all, we say that, okay, the messages which we have decoded correctly, C, the set C, will be the intersection of T and R. So, missed messages is T minus R, false messages are R minus T. So, everything is clear. And we are interested in the following event: E T, that the size of missed messages, the number of missed messages is. Messages, the number of missed messages is equal to t. So here we continue. So we following Yuri, we introduce such probabilities. So per user probability of error can be estimated as follows. So then we also can assume that T is equal to Ka because we use random coordinate bound or probabilistic method. So this is indeed without loss of generality. So we can choose any set, any transmitted set. Then we just Then we just present ET like the union of such events, right? Where the event EMF means that, okay, we found some set R such that the sum of the corresponding code words is closer to the channel output than their transmitted messages, right? So this is a single event ET. So we need to look at the union of such events and calculate the probability. Such events and calculate the probability of this union. So, this condition can be presented like this. So, it is done in Uri's paper, it's clear because y is equal to Ct plus Z. So, here we obtain Z and here we can express, so Ct, just remove common messages and obtain this norm squared. Okay, then, okay, the different thing is the final trick. So, okay, this if, so, how to calculate this union? Usually, union. Calculate this union. Usually, union bound is used, right? But we, if we use it in a straightforward way, so we will overestimate the probability of error a lot, right? We will obtain, for example, 1000. So this is bad, and that is why we need to tighten M-speed somehow. So Yuri used Gallagher's trick, Raw trick, and Fano's method says the following. So the probability of this event is less viable than the probability of event intersected with, so this will. So, this will be a good region, right? So, the event which corresponds to good region. And we can add the probability of complement to our good region. So, if we are out of the good region, we say that the probability of error is equal to one. So, this term says this. Okay, this probability is small, and we can estimate it with the use of union bounds. This is the idea, and so we choose the good region like this. A good region like this. So it will be first of all, we introduce B M, so it corresponds to fixed missed messages, right? Fixed indexes of missed messages, fixed set M, and then we intersect all of them. Okay, so this is done. So we actually, I will discuss it in what follows, but we checked many good regions. So we checked many regions. Actually, we can use any quadratic form here, and we choose, we checked all. We checked all of them and not all, but many such regions. But this one shows the best performance. Also, you see that there is no F here, so it does not depend on CF. This is because we want to avoid union bound when we calculate this probability. Okay, let's move forward. And then we use two lemmas. So one is Chernobyl bound, and the second is lemma about the expectation of the quadratic exponent to the power of quadratic. To the power of quadratic form. It is presented here, so it is rather, okay, I think it is rather well known, but okay, under some conditions, so some matrix should be positive definite. So, okay, this one, I minus 2a. So, we can calculate this expectation like this. Okay, if we move forward, so the idea is only like this, we should present these conditions in terms. These conditions in terms, so like quadratic forms. So it can be easily done in this fashion. We introduce such a vector, which will be Z C M C F divided by some numbers. And then, so this quadratic form can be found like this. So this will be Z, as you can see. So it will be Z, so norm Z squared. And this will correspond to this squared. So it can be easily. So any squared. Any square of the norm can be presented like this. And then finally, we find the final expression which we need to estimate. This is done like this. And for the second condition, we can do the same trick and present it like this. And then take linear combination with use of Chernov bond. So this is the idea, very simple proof. And finally, we can, by this method, we can obtain the result of the theorem. So, okay, it was presented before. So, okay, it was presented before. So, they're interesting. So, and finally, finally, the results are presented. So, in any case, it is not, I believe it is difficult to follow, but the proof is very simple. So, and so it is done. So, you see that in Uri's proof, you should take averaging twice, right? You should average some, calculate some conditional probability. Calculate some conditional probabilities, then take averaging over some variables, then again take averaging over Cf. Here it is done just once. You just calculate the expectation with the proper region. Okay, so the main question, is it better? So I will answer. So let me answer first. I will show the results later. It appears to be very close to Euris bound, but it is not better. So it always a little. So it always a little bit worse than the Euris bound. So I will show the plot at the end. And there is a natural question. Is there a better region? Maybe I was not good in looking for this region. Okay, this question is open. So I presented the region which is the best in our search, but okay, I do not guarantee that there is no bad region. So there is no proof that this region is the best. So that is why this question is. So, that is why this question is open. So, for example, one can use this Gordon's lemma, which Yuri and co-authors used in the second paper, which I presented. And it appears that it also can be formulated as a good region. You see that in this case, the region includes Cf, right? And in order to estimate, you see the main line is here. So, because here it is the same, the main line is here that the probability of union. Here, that the probability of union means that the minimum here is less than R, right? So, here we say union, it means that there is such event, such set, that it is less. So, it means that the minimum is less. So, this is the same. And for this minimum, it appears that we can use some concentration bound and the expectation of this minimum, right? So, we should say that this random variable is concentrated near the expectation. So, this is concentration. Expectation, so this is concentration, and then we can use Gordon's lemma to just estimate the expectation of minimum. And this Gordon's lemma is presented here. You see that the expectation is greater or equal than something and less or equal than something. So it is well bounded. So the bounds are rather good. And in principle, we can use it. So we try it, but it does not lead to any improvement. So actually, the bound with use of Gordon's lemma, which is known as Use of Gordon's lemma, which is non-asymptotic, is here, but it also does not improve the initial bound binary. And then, so it was like this. We decided it was my old goal to consider binary code book, and today it was also considered. So, and Yuri's approach does not allow to just analyze binary code book because we need to take expectation two times. So, the first one by CM and the second one. One by CM and the second one by CF. So, and for binary codebook, it's difficult because we don't have a closed form expression on the first step. But for this approach, it's possible. So, we consider this binary codebook, right? And then we can prove the following theorem. It is a little bit more complicated, but in principle, it is feasible to calculate it. And finally, so let me show the results. You see that the U is bound, so the parameter. That the U is bound, so the parameters here are rather classical. So let me move again this. The parameters are rather classical. K is equal to 100, frame length is 30,000. The probability per user probability of error is 0.05. And we see that Euris bound is the green one, so the lowest one. And our two bounds are here. You see that the black one is Eurium, one is for the Gaussian codebook, and the Codebook and the okay, how to call it orange one, maybe or brown one. Okay, orange one is here, it corresponds to the binary codebook. Okay, the bound for the Gautam codebook is first, but the bound for binary code book coincides with the bound, okay, numerically coincides. We don't have the proof that they are the same, right? But for this regime and for these parameters, we see that these bounds are very close and practically coincide. So, this is interesting. Coincide. So, this is interesting because you know the story of practical schemes for Gaussian channel. So, people started from binary modulation, so plus minus square root of p. And so, this is the best. So, this is the best sparse IDMA, the best scheme which uses binary modulation. And this is by Texas Group, Pajan Francois and Krishna. Okay, and then people switch to. Okay, and then people switch to different modulations, so spreading and so on. And these two schemes use different modulations, like so Gaussian-like, so random. And you see that they are much better and even better than the bounds here in the beginning. So the natural question was, does the modulation make some constraints for the performance? And these bounds say that efficient schemes with binary modulation do exist. Binary modulation do exist, but so we still do not know them. So, in my opinion, this result is interesting. So, this is our result for the Gaussian case. And let me briefly discuss, so I think I have some time, let me briefly discuss the MIMA case. So, I think this is all about Gaussian case, but nevertheless, the question is open: can we somehow improve the region? Because so we found it by computer search, we found this region, but maybe we can prove that the optimal one. Prove that the optimal one should look like this. So, we, in further research, we will address this question. And also, this is an interesting, in my opinion, question. Okay, what is regarding my channel? So, first of all, the channel is here. You see that we have L antennas. And then here we have transmitted messages as previously. Phi is the binary selection matrix, so activity matrix, right? And so, okay, we have the following. So, okay, we have the following. So, this is a code book. Then, some code words are selected by this matrix, and then they are multiplied by coefficients H. So, H is just like this, and all elements are chosen independently from complex normal distribution, circular normal distribution 0, 1. Okay, and definitely, because it's difficult to estimate the channel for all the users, huge number of users, we assume each to be unknown. Users, we assume H to be unknown to transmitters and receivers. So, this idea, so it's very clever to consider such a model because transmitters have one antenna, right? And they preserve this energy efficiency property. But receivers should have L antennas, which is presented here. So, this idea was generated by the group of Josepo Kaier, and so this, so we like it, and we think it's reasonable. And in my Reasonable, and in my opinion, in practice, it should be used. So, nevertheless, we will consider such a channel. And so, the decoders are here. So, the maximum likelihood decoder will work like this. So, it will maximize such likelihood, right? And to maximize it, definitely we should take expectation over H to just marginalize it from such probability, such likelihood. Okay, and so this method was analyzed in the paper. Method was analyzed in the papers which I will present on the next slide. In the next slide, and we will consider projection decoding rule. So, because we started with Yuri, with this method for some time ago, we consider one antenna and this method, okay, we were able to prove something for this method. That is why I like it. So, instead of averaging, we will take just maximum over h. So, it is the same as to find the closest point in some sub. In some subspace, I will show it a little bit later. Okay, and so for my material, so I would like to say that for a long time there were no fundamental limits, right? People just compared practical schemes. And in these two papers, these fundamental limits appeared. And we were inspired by these papers. So, especially, so okay, nevertheless, they analyzed maximum likelihood decoder. Okay, and so this. Okay, and so this projection decoder can be presented like this. You see that, okay, we should, first of all, this likelihood means that we should minimize some distance, right? Frabeni, okay, it is Frabenius norm, sorry, not distance, but sub-Frabenius norm. And then when we express it, we can come to the conclusion that we need to maximize the projection to some uh subspace. Okay, the projection to okay, what how how this decode works. How this decoder works. It checks all sets, R, possible received sets, and then projects our signal, Y, our channel output, to the span of the code words, which corresponds to these indexes R. And then it chooses R, which maximizes this projection. So for one antenna, it should be clear. So we take code words, which we check. Take code words which we check, then project, calculate the projection. So, and then choose the proper subset. Because if we don't have noise, then we will lie in the corresponding span, right? Because h are just linear coefficients which preserve this span. Okay, this is the projection rule. Okay, I will not tell a lot of details here because this is rather complex. Let me just tell the main idea. So, okay, first of all. Main idea. So, okay, first of all, again, random coding precise and ensemble. Second, okay, this is the main theorem. Okay, let's just skip it. The bounds. Again, quadratic form expectation can be calculated even for complex vectors. So it's Gaussian vector. So it is rather easy by this formula. And again, the same story, but there events be like this. So some received. So, okay, some received set has better projection than the transmitted set. Then we use Fanotrick with the following region. It is here. Okay, this region again was chosen by... Okay, we just took the region from the previous paper with Yuri, and it appeared that this region is interesting because it will lead to some improvement in fundamental bounds. Okay, once again, so I forgot to say that this projection. I forgot to say that this projection method allows to work only when ka is less or equal than n. So n is a frame length, because okay, if it is, if ka is greater, then this projection will just be non-interesting, right? So it will be identity matrix or something like this. It will not do, so it will not work. So that is why our key is bounded by n. And previously, in the paper which I showed, they proved this. Paper which I showed, they proved the scaling law that actually we can consider Ka which is greater than m. So this is a drawback of our method. We cannot consider Ka which is greater than m here because our kind of projection decode will not work. Okay, and the main idea is like this. Okay, instead of fixing there, what okay, we should fix code words here. So the main problem in this analysis is that the signal on antennas is ideal. The signal on antennas is dependent because the code words are dependent. We see different combinations of the same code words on each antenna, right? But if we fix code words, then the signals are independent, right? So because they are defined by just random coefficients H and the noise. And in this case, we can analyze. So we can just transform a little bit these projections. So we again transform them as a quadratic form, use traces. Okay, it's some technology. Is okay, it's some tech. So, here are some technical details, but it can be done. Uh, okay, and then obtain the final result, which is here. So, this is okay, if it is interesting, we can discuss it. Maybe you can write and I will explain in the digital words. But, okay, let me go to the end. So, this is practically the last slide, as you can see. This projection method allows to simplify calculations because for maximum likelihood. Because for maximum likelihood method, we should work with matrices of size n by n. Here, we can reduce by just Gram-Schmidt transfer arthagonalization, we can reduce the matrices to matrices of size 2t by 2t. So it greatly simplifies the calculations because it calculated maximum likelihood and projection, and projection is much faster in our implementation. Okay, these are some details. Okay, these are some details about it. And finally, the results. So, this will be surprising, but the next slide will be much better. So, first of all, we use the same setup as in the papers which I showed previously, right? And we see that the projection decoder is better here, but you see that we did not calculate, we calculated up to Ka equal to 1400, right? But our N is 3200. 3200, right? We did not calculate up to n. That is why we calculate, we did our own experiment, implemented maximum likelip. And you see that, okay, this picture is much better in my opinion because it shows the main result. So you see that in the beginning and at the end, as we expect, the maximum likelihood is better, the bound for maximum likelihood is better. But in the middle, so projection union bound so. Union bound, so the bound for projection decoder is better. Okay, how we should understand it. We were also very surprised with it because we know that maximum likelihood decoder is better, but this is estimate. So I would like to say, okay, so maximum likelihood decode is better, and if we use it in practice, then we will obtain better results. But to estimate, so it appears that the projection region. Region is better for this union bound calculation, right? That is why we obtain some improvement in the middle, but not significant, but nevertheless, we see the improvement here. So, this is interesting. It again shows that the choice of good region is important for the union bound, and by choosing different good regions, we can obtain even surprising results like this. So, okay, we did not expect projection decoder to be better. I just wanted to understand how. Just wanted to understand how what will be the gap, how worse will it be. But okay, we obtain the improvement in the middle here for small, for medium, so just moderate number of users. Okay, and the last slide here, so the papers, which so our papers which I present here, and so I would like to say thank uh to my co-authors, so Kirill Andreev, Dario Ustinov, a PhD student, Anton Griebov and Pavel Rybkin. Anton Greebov and Pavel Ripin, who helped me with this. And so, thank you for the attention, and I will be glad to answer your questions.