At least up until now. I'd like to talk about work that I began a few years ago with Rick Kenyon, Dan Krull, and Charles Radin. And has continued with Chris Kosha, Cheyenne Das, Sumit Mukherjee, and Martin Tassi. The last two I think were inadvertently left off my abstract on. Off my abstract online. Okay. Let me proceed. Okay. So the objects of study for my talk, not surprisingly, are, given the topic of the conference, be random permutations of the numbers one through n for a large n, or random permutations with some given property. Property. And so, what sort of statistical property do we care about for large permutations? Well, by analogy with large graphs, where we look at, for example, homomorphism densities and permutations, we look at pattern densities. What is a pattern density? Well, you're going to hear a lot more about pattern densities from some. I'll be answering tomorrow. And I know a lot of you know what they are, but a pattern is think of it as a small permutation. And here's an example of a largish permutation. And an occurrence of a pattern, you pick out three entries of the large permutation. And if their size order is the same as the order in the Order is the same as the order in the pattern, then that's said to be an occurrence of the pattern. So here, this pattern is small, large, medium, and that applies to 243 in this permutation here. So that's an occurrence. 253 is another occurrence. 143 15 153 again, and the pattern density is the number of occurrences of that pattern divided by the number of possible occurrences n choose k. Okay, so if I've computed this one right, the density of 1,32 in the permutation 6214537 is 435ths. Okay, so what is a permutation? Okay, so what is a permuton? Permuton is a limit object for permutations, and in particular, it's a probability measure, a Borel probability measure on the unit square with uniform marginals. You might know it as a doubly stochastic measure or a two-dimensional copula. And so let's do an example. Suppose we take the permutation 1532. Take the permutation 15324. So, this is one-line notation. Of course, this means a permutation that maps the first position to one, second position to five, third position to three, and so forth. And we can model that with a matrix of zeros and ones. And notice that we're sort of understanding this as an analyst would, that is starting from the lower left corner. Is starting from the lower left corner. So this maps one to one, two to five, and so forth. Okay, and if we can change that to a probability distribution just by, in the unit square, by replacing the ones by blocks of appropriate size, in this case, the density of the block. Case the density of the block would be five, so that the measure of the whole thing is one, and that probability is known as the permutime gamma associated with the permutation 15324. Every permutation provides a corresponding think of it as a step permutation. Here is a picture of that permuton for a random permutation of size 1000. So you might be able to see the many tiny dots on your screen. But the point that I want to make is that that permuton is actually very close in the topology we care about to the universe. We care about to the uniform probability measure on the unit square, the bank measure, even though it looks quite different than this picture. And it's close because if you pick some area and compute its probability in either one of these, it'll be about the same. Another way of saying that is that a sequence of permutations converges if their permutons converge in distribution. In other words, their accumulative probability distributions converge. Probability distributions converge. And so, if it's the case that if you pick a point, then the probability that you're below and to the left of it is consistent. Okay. So now we can also get permutations from permutons. Just by choosing independent points from the permuton and then look at how their y and x coordinates behave. So for any given n, you have a permuton, pick niid points from it, sort them by x coordinate, and then record the permutation given by y coordinate. So for example, this is some permutation. This is some permutant, some probability distribution on the unit square. We pick three points. If we happen to pick those three points, then we look at the permutation they generate, and that's just the permutation 231. Okay, so read from left to right. You get the middle y coordinate, then the high y coordinate, then move. All right. All right. So now we know what the density of a pattern is in a permutation. What is it in a permuton? Well, it's just take, if the pattern is length k, take k points, iid from the permuton, and the probability that they make pi, that's the pattern density. So we can compute that with an integral. That with an integral in this case, I've given the integral for the case where the permuton has a density, which they don't always do. Okay, so the pattern density of a permutation is not exactly the same as the pattern density of its permuton, but as the permutation gets. As the permutation gets larger, it gets closer. Okay, now the great thing about permutons is that they are exactly the limiting objects for permutations in this topology. And so, and a permutation is determined by its pattern densities. If you know for every possible pattern what the density is, you have. Possible pattern, what the density is, you have your permuton. And if you have a sequence of permutations and their pattern densities converge, then these permutations converge to a permutation. So it's just like the real numbers are the limit objects for the rational numbers. Okay. So, here's a couple of examples of permutons that arise naturally, just to give you some feel for it. One way to get a random permutation is take a random walk. Make it a continuous step distribution so that you don't repeat any positions. And then those positions in time are just ordered by. Just ordered by their real values, give you a permutation. And it might be natural to ask: well, what does this permutation look like when n becomes large? And the answer is it looks like this. And that doesn't depend on the actual choice of the step distribution as long as it's symmetric. That's a bit of a funny-looking permutant. It looks like this because we've actually cut off the top part. If you take a random walk of n steps, then the probability that's increasing all the way for a symmetric step distribution is, of course, 2 to the minus n. But that's much bigger than the probability of the identity permutation if permutations are taken at random. Same is true for the reverse, and that's why this permutile looks like this. Mutile looks like this. There's an actual formula for it. Here's another case for something that some people in my audience know very well. The random permutations that you're encountered when you take at stages of a random sorting network. Here's an example of a random sorting network. And as conjectured and now proven, they look like these beautiful things in ellipses. Look like these beautiful things in ellipses. The middle one here is just the projection of uniform distribution on a sphere onto the unit squared. All right. So what can we do with these permutations? Well, the idea, our idea at least, is to try to understand what large random permutations look like. Look like by studying the entropy of permutations. So let's suppose that we have a subset of the symmetric group, size n, and the size of the subset is factorial size. So it's a size roughly n factorial times some negative power. Negative power of E. So of the form E to the n log n minus n, that's the n factorial part, plus c n where c is some non-positive constant. So, this is what we think of as a large set of permutations. And an example is. An example is permutations with a particular pattern density, like it's permutations whose pattern density for the pattern 132 is one seventh, for example. Or let's say to be particular near one seventh, just in case. And choose K is not a multiple of seven. And however, And however, we make an important caveat here. If one of the pattern densities required is required to be zero, then we know that the set of permutations we get is only exponential in size, and therefore it's not what we're calling a large set. Now, as many of you know, there's a whole community of people interested in pattern densities, permutation patterns. Permutation patterns, sort of the name of the community and the name of their annual meeting. So they're very much interested in classes of permutations that avoid a pattern. And they will not apply to what I'm saying at the beginning of this talk, but we'll get back to them a little bit later. Okay. So what is the entropy of a Well, gamma n is a probability distribution on permutations within Sn induced by the permuton gamma. And its entropy is just straightforward minus sum of minus p log p and what we'd What we'd like to do is convert that into something that describes the permiton itself. And in order to do that, we have to subtract off the log of n factorial, divide by n, take a limit. And what that's doing is that that's recovering this negative number c that we saw describing the size of a set of permutations. And the theorem is you can recover this limiting number by taking a certain integral if the primiton has a density little g. And we say that this entropy is minus infinity if there is no density or if this integral is infinite. Infinite. So this is a kind of a strange notion here. We're dealing with negative entropy. The top possible entropy is zero, as we will see. And in general, it's a negative number. And it's often minus infinity. Note that this is for those of you who are accustomed to dealing with large deviation principles, this entropy. Principles, this entropy is just minus the Kolbeck-Leibler divergence with respect to the uniform measure. Okay, sample entropies. Well, if we have a singular permuton like this, so this means that all of the probability is concentrated on the main diagonal. The only permutation you will ever get using. You will ever get using this permuton is the identity permutation. So, we talk about a class of size one. The entropy of the uniform permuton that gives you uniform probability distribution and has entropy zero. The entropy of the permuton of a permutation is minus log n if that permutation is size n. If that permutation is size n. Permuton entropy is never positive and is zero only for the uniform measure. Okay, so the large deviations principle or what in this context amounts to the same thing, the variational principle, various different ways to state it and various and prove it. And prove it. And roughly speaking, it looks like this. It says that if we have a nice set of permutons, then the entropy of the permutations they describe is the maximum of the entropy of the permutons. And if that maximum is uniquely That maximum is uniquely taken, that means that you have found a description for this set of permutations. The variational principle then we're saying is to describe and count permutations with given properties. For example, fixed pattern densities, we find the permuton or possibly permutons with those properties that maximizes entropy. How do we do it? How do we do it? Well, that's another matter. Example. The pattern one, two just means the number of pairs of positions where the permutation is ascending divided by n choose two. And there are going to be many permutations. There are going to be many permutations. There are many permitons with that any given density, too, but there's a unique one of maximum entropy. And once we have it, then when we take a uniformly random permuton subject to the condition that it has this particular density, then it will look like this permutant. So here's an example. Well, if it's one, two density. So, if it's one two density is one, then it can only be the identity permutation. If it's point eight, it looks like this picture here, picture of probability distribution on the unit square, which favors the main diagonal and has uniform margins. Here's a gentler one tilted in the opposite direction for 1,2 density 0.4. For 0.2, it's just the 90-degree rotation of the 0.8 case. And for zero, it's the other diagonal. And we actually know from back in 09, from Cheyenne's star, what the explicit density is for this particular permutation. It's not given exactly explicitly here, but it's given by two equations. And here is a graph of the entropy function as this density varies from zero to one. Let's look at a more complex case. A more complex case. So, what is this picture? Well, we call it the scalloped triangle, and it is a feasibility region for graphs with respect to edge density versus triangle density. Okay, so if you want a graph. So if you want a graph with a particular edge density 0.4 and a particular triangle density, 0.1. And because it's inside this triangle, you know you can find graphs with approximately these densities. In fact, you can find a limit graph called a graphon with exactly those densities. So this point here on the picture. Point here on the picture. Can you see my cursor? Yes. Yeah, this point here represents the empty graph. This point here is the complete graph. This point here is the graph of the largest edge density that has triangle density zero. And that, of course, is a complete balanced bipartite graph. This point is a complete balanced tripartite graph. And these edges here are actually. And these edges here are actually slightly concave. So, this was a lot of work by Rosborov, building on others to prove that this is the correct picture for graphs. But once we have it for graphs, we actually also know it for permutations, where edge density is replaced by one two density. And triangle density is represented by one, two, three density. Okay, so how does this happen? Well, to get from a permutation to a graph, we can take the permutation, think of it as an ordering of the numbers from one to n, a reordering of the numbers from one to n, intersect it with the standard ordering of the numbers. It with the standard ordering of the numbers from one to n, and we have a partially ordered set, and the comparability graph of that partially ordered set is the graph underlying each point here. Okay. Well, of course, most graphs are not comparability graphs of anything, let alone comparability graphs of two-dimensional partial order that we can get in this particular way. In this particular way. So it's perhaps a little surprising that the entire scallop triangle still applies as a feasibility region for permutations. But it's easy enough to prove. All you need to do is show that each point on the boundary can be realized by a permutation. Permutation by permuton in particular, and uh, and then it ex then just by topology, you could extend it to the whole picture. The actual pictures of permutons for various points inside the scallop triangle, these are generated by. Empirically. We don't know the equations for it. Thanks to Eric Kenyon for generating these features. And the help of Mathematica, I believe. Yeah, there are all kinds of wild things here. We wish we knew more about how things behave in here, more about phase transitions and whatnot. We do know a little bit. We do know a little bit more in another similar case. So, this is again the feasibility region for graphs. And this time it's for, we call it the anvil. It's for triangle density versus anti-triangle density. In other words, triples of points with no edges. Of points with no edges, and it's quite interesting. There's actually the boundary here is given by two curves that cross, and the outer curve forms the boundary of the feasibility region. If we look at the singular permutons, that so again, the same kinds of arguments. Again, the same kinds of arguments show that this picture applies to permutations, where we replace triangle density by 1, 2, 3 density and anti-triangle by 3, 2, 1. And the permutons that appear along the boundaries of this picture are the singular permutons I've shown here. And there's two of them that are vying for supremacy at this particular critical point where the curves cross. Curves cross. We are able to show in this case that there is a phase transition along a line extending from this point toward the origin. We don't know how far, where the permutons that maximize entropy are tied. They're two different ones that vie for maximum entropy along this line. Okay. Okay. What about these singular permutons? Okay, so again, if you're in the pattern density community and lots of other communities also interested in, excuse me, I meant to say the permutation pattern community and others that are interested in permutation classes that are closed under sub-permutations or patterns. Sub-permutations or patterns. Then you're more interested in singular permutations and therefore ones that are only of exponential size. So here are some examples of singular permutons that arise in various cases. We've been interested in trying to figure out what we can say about these as well. So it's less. But there is a sort of a natural notion of entropy. For classes of permutations that are only exponential in size. And for this, we don't subtract off that log n factorial. We take a limit in basically the same way. And we get a lower notion of entropy, which we call catropy. And this will be something which is. Something which is, thankfully, now a non-negative number instead of a non-positive number. And it typically is useful information and typical for a singular primatizer. For many, it's zero, for some it's infinite, but it's often finite. And it's in fact been computed for something called geometric grid classes. Which I know some of you know. Okay, now let me return to the variational principle for permutons. Okay, so, and also implicitly, sometimes back to large classes of permutations. Okay, suppose that the random permutation sigma that we're interested in is not chosen. Is not chosen uniformly at random from some class or from all permutations of size n, but instead from a base permuton mu, which might not be the uniform permuton. Now we observe that this permutation we've picked has some property which is not shared by that base permuton. For example, it has different pattern density. It has different pattern density from the pattern density of the permuton it was drawn from. Okay, then we would like to know: okay, so it looks like some other permuton, some permuton which is the original, the base permuton, but conditioned on this new property. Okay, so we would like to show that that is something can be obtained by maximizing some entropy, some function. Some entropy, some function, and can it be obtained by optimization? So here's an example. Suppose that we draw a permutation from this permuton here. So this is just the uniform measure on this upper square and added to the uniform measure on this lower square. Added to the uniform measure on this lower square, and nothing here, and nothing here. Okay, so if we drew a uniformly random, we drew a permutation from this permuton. Half the points would be up here, half the points would be down here. The pairs of points here would be decreasing with probability of half. The pairs here would be decreasing with probability of half. Decreasing with probability of half, but the pairs consisting of a point here and a point here would always be increasing. So the result is that the inversion density, which means the 2,1 density, the density of pairs that go down, would be a quarter for this permuton. But there's nothing to prevent us from drawing a permutation from this thing whose actual Actual density is actual inversion density is one-third. Well, of course, if n is large, that will be very unlikely. That's why we're talking about a large deviation principle. So, question is, what would it look like? Well, a logical possibility is that it looks like this. Looks like this. So here's one of the Shannon permutons. Permutation for a permutation with high inversion density. We could have one for the top and one for the bottom. But it could also instead look like more like one of these permitons, where we got the higher inversion density. Inversion density by picking way more points from one of these two parts than from the other two parts. The other part. So, which is the case? So, to solve problems like this, we needed a generalized large deviation principle. And it turns out to do it, we actually need to leave the space of permutons into a much more general space. Into a much more general space of probability measures on the unit square with continuous margins. And temporarily, I'm going to call them continuum. So, and the theorem goes like this. I'm stating this. There are other ways to state this, but here's one way to put it. Okay. All right. Suppose we draw sigma from. Suppose we draw sigma from some base permutation mu and we're interested in the in the permutonic, the empirical distribution basically that we get by drawing sigma. Okay, and we have some nice set of continuons, A. And the theorem says that again, Again, the okay, so this D of alpha mu is now the Kahlbach-Leibler divergence. Well, it's just, it's very much like the formula you saw before for entropy, integral of G log G. But now it's G log G over H, where H is the The density of the base permuton. And this says that if we take the infimum or the closure of A, then we're below the limb inf of these entropy figures, which turn below the limb soup, which is below the. The inf of the divergence over the interior of the set A. And all this again just means we have a variational principle, but the variational principle now applies over continuons. We have to maximize entropy or equivalently minimize divergence over a set of continuons instead of a set of permutons. And one consequence of this is that in the previous problem, let me go back to it for a moment, there actually is a phase transition here. So as the required inversion density moves down, or say up from one quarter, you get pictures. You get pictures, a picture like this for a while, and then there's a sudden change, and you flop over to a pair of pictures like this. So in a sense, we're now getting, as a result of this generalized LDP, we're getting a phase transition caused by just effectively looking at one pattern. Pattern. We don't think this can happen in the special case where the base permuton is the identity, is the uniform permuton. Okay, that is all I have to say for today. I want to leave some time for questions. And I want to thank everyone for their attention. Thank you, Peter. And yeah, questions? Yeah, I've got a question. My colleague and I have a question. The way that you have The way that you have outlined for choosing a permutation from a permuton is by sampling the points IID. That's right. And what if you don't do that? For instance, you could take the points with some repulsion. Does that give you a different, I mean, surely that gives you a different distribution on permutations? This would be of interest, you know, to the random matrix people and so on. Yeah. Yes, it might make some sense. Yeah, the random metrics people are, of course, accustomed to seeing cases where you end up with distributions, for example, among eigenvalues that exhibit some repulsion. Yeah, that's right. And yeah, I know. Of course, everything looks way harder. So I don't know if you should do this. Yeah, yeah. I like the idea, but. But I don't know. I haven't looked at it. I don't know of anyone who has. So maybe there's something to be gained here. Yeah, I didn't understand why. Can you say some intuition about why that phase transition happens at the end there? I didn't. Yeah, the intuition is that. Is that you have the binomial distribution telling you that you want to distribute these IID points half in the top picture and half in the bottom picture? And that's a very strong, you know, the binomial is peaked, it's quite sharply peaked. So if you're not asking If you're not asking for an inversion density, which is far from what you would get ordinarily, it stays with that distribution and just modifies the points a little bit inside the squares. But if you make a big ask, you want the inversion density to be much more than a quarter. In particular, In particular, especially, for example, if you want it to be more than a half, then you can't possibly distribute the points equally between the two squares. You have to unbalance the distribution. So somewhere along the line, you change your mind between just amending the two distributions and keeping them balanced and tilting the whole thing one way or the other. Tilting the whole thing one way or the other. Does that help? I'm not sure I see why the second case makes sense, but why the first thing you said, I didn't get why they got why should they be balanced? Close to the regular density? Yeah, it's just a question. It's basically a question of looking at the At the exponent. That when you're when that's what basically the variational principle is doing for you. If it's not hard for the pattern density to be a little bit off, there's not a big, the exponential price you pay when you're a little bit off. When you're a little bit off, is not as high as the price paid by the binomial distribution. Okay, yeah, thanks. Svanto, would you like to go? You're muted, Svanta. So, okay. Do you hear me now? Yep. Yeah. Okay. So, to continue on that question, in this example, you just had. example you just had and the phase transition there in the phase where you have a you said for high intensity where you have a non-equal distribution between the two parts or there do you in your picture it looks as if you still had uniform distribution inside each of the two smaller squares but i guess i guess at least for high p that can't be true so no no no absolutely not no it's not true at all Absolutely not. No, it's not true at all. No. And let me go back a little bit and I'll see where. Yeah, you see this word at the top? Does it look more like this or like that? That was my out. Yes. And you will see these won't be uniform. No. So they will look something like the top half, but they are now of different sizes. Of different sizes. Right. Yes. Okay. Can you compute the exact value of the phase transition in this case? Is it some nice number or? We haven't computed that value. No. So we don't know. Okay. But then let me also comment. This was, I mean, this was very, very nice, interesting things. It makes me envious that I haven't worked on these problems myself, but it was, but it reminds me, as you know, I work more on graphons, not on. Work more on graphons, not on permutons. And your first part with entropy and maximizing entropy and so on is partly parallel to what I did some years ago with Hatami and Segedi for graph funds. Exactly, yes, yes. So, yes, I think you went further in some cases, but that also makes me wonder. That also makes me wonder your second part here with the large deviation principle. And so, do you know if there are analogs for graphons of those results too? Yeah, maybe not. Yeah, it seems like that the generalization case for graphons is more complex than it is for us. It's maybe because we cheated. Maybe because we cheated in part by going to the continuons for our maximization instead of staying within the permutons. But the situation for the graphons is quite complex. And part of the reason is that it, you know, there's, as you know, there's a similar entropy function for graphons and the similar variational principle of Chatterjee and Veridon. Yes. And but it seems But it seems that when you apply it, you for graphons, you tend to get combinatorial objects. You get these graphs that Raden and Sadhun call them bipodal, bimo. Yeah. My podal, I think. Multipodal. Yes, multipodal. Whereas for permeatons, you always seem to get these nice analytic things. Yes, you got very nice results here. Yes. Well, there's an I haven't really emphasized as I should have in the talk, but there's enormous, enormous lists of things that we don't know the answer to. Some of them. That we don't know the answer to, some of them which look quite simple. So we've seen some pretty pictures, but really we have scratched the surface at best. Okay, I have a final question also before giving word to anyone else here. So can you give an intuitive explanation why you had to go to continuance for your large creation? Uh explanation is this. So if you just apply the standard tools for proving an LDP, it takes you to continue on. And the reason why you don't need them for the original LDP is that there is a lemma which states. That there is a lemma which states that if you have a continuon, that you see, there's a mapping, natural mapping from continuons to permutons where you just uniformize the marginals in the obvious way. And that mapping increases the The divergence, that's sorry, it decreases the divergence with respect to the uniform permuton. And therefore, you can always just go to the actual permutons when the base case is a uniform permuton. But in general, that's not true. In general, you have to maximize among the continuons to get. Continue on to get the right answer. You have to maximize among the continuous to get the right answer for permutants. Right. Okay. Okay. Well, or for the- Thank you again, Peter. Right. I stated that the LDP for permutons, but the same statement is true if I just say let gamma be a continuum. So the continuum in some So, the continuums, in some sense, are the natural setting for the LDPs. You get away with permutons for the original LDP because of a special maximization property with respect to the uniform permuton. Okay. Gil, would you like to ask a question? Would you like to ask a question? Yes. What happens when you try to multiply permutons? Have you considered that? Oh, yeah. So if you think about permutations as algebraic objects, then permutons seem to fail completely to do what you want them to do. You can multiply permutations, of course. You multiply two permutations, you get another permutation. But for permutons, for example, if you multiply by the uniform permuton, what are you doing? You're multiplying by a uniformly random permutation. And the result of that is a uniformly random permutation, no matter what the other factor was. So you don't have to. So, you don't have, you emphatically don't have a group, you don't have correlation. Um, like matrices, you have invertible ones and singular ones. Yes. Um, I suppose, yes, you could do that. Okay, here's another example. It's natural to look at. Cycle lengths, right? If you're looking at permutations, that's an algebraic rather than a statistical property. And so cycle length is not something which is defined for a permuton, but you can, as Jacopo knows, and as Sumit Bukherchi, who wrote some stuff about this, knows, you can nonetheless define. Can nonetheless define probability distributions for these various things on a primutine. And that actually can be quite interesting to study. So can you say things about permittons or permutations coming from permittons together with some information about the cycle lengths? Well, for example, You can take a permuton and you can deduce from it a distribution for the number of fixed points. Okay, so what does that mean? The permuton itself doesn't have fixed points, but you draw permutations from Permutations from it, they will have various numbers of fixed points that will not converge to a number, but instead they'll converge in probability to a distribution. So, for example, in the case of the uniform permuton, it will converge to a Poisson distribution with mean one. Jacopo, you've got your hand up. Yeah, thanks. So just if I want, I can add a little comment on that. They are very useful also for studying the longest increase in permutation in thin permutants. This is a statistic that behaves very well in permutants. And recently they were used. Yeah. But I had another question. Another question: It's okay. You have now this notion of catropy for degenerate pheromotons, and okay, around the literature there are all these random fractal permotons coming from pattern-avoiding permutation. And recently, they are also connected with the, we are working to connect them with the Leewid quantum gravity. And I was asking, first, can you compute the atropy for this random object? For this random object, can you say something? And if yes, it can be useful for extracting some other information. In some cases, in fact, I think maybe a better term would be in rare cases we can compute this. We understand this. We have a singular permuton. We understand it really well. We can try to. We can try to compute it in one of a couple of ways. But annoyingly, it doesn't usually give you the answer to what the permutation pattern people are often looking for, which is the growth rate for a permutation class. And one example is that some permutation is not. And one example is that some permutation classes are described pretty well by what are called grid classes, where you fill in rectangles with these diagonals of one kind or another. And these are bounded below by another class called a geometric grid class, where you actually have straight. Class where you actually have straight lines in them in the primutons. But generally speaking, the geometric grid classes have lower catropy than the full grid class, which is contained inside the permutation class. So we're not getting the right answer for the exponential growth rate of the permutation class by computing the catropy. We're only getting a lower bound. Okay. Bound. Okay, I see. Okay, thank you. I have another question, but I'll ask where I'll be at Darwin. Well, Ben, you have a question? Yeah, I was, yeah, again, I don't know if this is a good or well-formed question. I was wondering what is the sort of the correct, whether there is, whether there are correct notions of limiting objects for Catalan objects. So, I mean, the naive. Objects. So, I mean, the naive thing to say is, well, yeah, so these are just 3-2-1 avoiding permutations or what have you. So the permuton is a perfectly natural limiting object, but there's like a billion bidections between these different things. So for instance, like non-crossing partitions. Is there a good limiting non-crossing partition that doesn't involve? I'm going to refer you to a paper of mine and Malvina Luchak's from years ago, which has nothing to do with Permuton. Which has nothing to do with permutons. What we do there is we construct what we call a building scheme for a particular Catalan set, but works for