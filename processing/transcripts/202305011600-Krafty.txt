 Pittsburgh, Ori Rosen is a statistician at University of Texas El Paso, as well as our scientific collaborators. Ted Hubbert is a bioengineer. I forget which department. I forget which department he's in. He crosses over many. If any of you have worked with ethniers before, the package and toolboxes that are used for statistics and ethniers that were created in the mid-2000s were started by Ted and his group and Allison Hipwell, who's a clinical psychologist. If I can get this guy to work, there we go. Okay, so I'd like to start a little bit about, so this is a rather applied talk that I'm going to be giving. I'd like to start with our Giving. I'd like to start with our motivating study. So, the motivating study is, it starts with the Pittsburgh Girls Study. So, the Pittsburgh Girl Study has been going on for, I think it's 18 or 19 years. So, it enrolls girls starting at age five or five to eight, and it follows them every year. And so, and we have actually really good return rates. So, we have this large cohort where you can look at this kind of timeframe from young girls. Time frame from young girls into womanhood. And we observe lots of things about them. Maybe about 10 years ago, they started to get biological sort of things, but lots of psychosocial things. So you can really follow them to understand what is the true life course and health of a girl and how does that affect her health when she becomes a woman. Now, one really interesting part of this is that we actually, in this study, out of the nearly 2,500 women in the study, 500 women in the study have about 350 teenage mothers. So it gives a really unique opportunity that you can study what life was like, psychosocial factors in a girl's life up until pregnancy and see how that actually affects their offspring. So this Pittsburgh girl study has been taken advantage of, slash took advantage of the ECHO program. So I don't know how many of y'all are familiar with. So, I don't know how many of y'all are familiar with the ECHO program. It was a big call that went out that has lots of different centers that want to look at how the environment affects child health. And they piggybacked on the Pittsburgh Girl Study. We use their the Pittsburgh Girl Study population. And we went through, and anytime someone who's being followed in the Pittsburgh Girl Study becomes pregnant, they enroll in this ECHO study. Now, most of the other sites within the ECHO study. Other sites within the ECHO study tend to look at what you think of as the E for environmental pollution things, right? Those sort of things. We are really looking at psychosocial stressors. So this is going to be the data that we're going to look at. Oh, wrong direction. There we go. So the data that we're going to consider, so the ECHO study, we've been enrolling the mothers in this. The mothers in this is the echo part for about two years now. So we're going to be looking at 82 mother-offspring pairs. And we have to kind of look at three types of data. The first, we have some offspring demographic data, right? We know their age. Their age range between six months to a little bit under three years old. We know their gestational days, so that affects a lot of emotional reactivity, head circumference, sex. We also are looking at questionnaire or Looking at questionnaire or mothers reported observations of child behavior. And so, one of the big questionnaires that's used in this is the infant behavioral questionnaire, the revised version, or IBQ. And so, this is a mother describes certain aspects of their child's behavior, right? We're interested in looking at two of these. The first one is what we call IBQ negative. And so, this looks at negative emotionality. So, it's asking the mother, how often or how much does your child? How often or how much does your child show fear, show extreme anger, those sort of responses? As well, then as look at something a little bit more positive, effort control. So, how much, how resilient is your child in overcoming certain things? How much can they get around certain emotional obstacles? Then, we also are going to have the children partake in a laboratory task being observed with ethners. So, during this laboratory task, it's the During this laboratory task, it's the classical still face task. So, I don't know how many of y'all saw this before. It's kind of a little disturbing when you think about it, but it's for a good cause. So, this task has three phases to it. The first is normal interaction phase, where the mother is playing with the kid and they're interacting as normal. Then it goes into the still phase phase, where the mother loses all affect, stops interacting with the child, and the child often, this is obviously a stressful period for the child. Period for the child. And then afterwards, yay, we start playing again, we're friends again, right? And so, the whole idea behind this is you want to be able to understand how a child reacts to a stressful situation. Now, when we say we want to look at how a child reacts to a stressful situation, well, we want to look at neural activity during this period, and we're going to be using functional near-red spectroscopy or F-Nears to look at this. Or FNIRS to look at this. So, FNIRS is it measures basically, so it measures cortical hemodynamic activity, and it basically measures the amount of red and blue in blood that's floating around. So, it measures the amount of oxygen and deoxygenated hemoglobin. And you wear caps that look like this. They kind of look like an EEG cap. Of course, we have littler, cuter caps for our youngins in our study. Our study. And the data that you get for these looks something like this over here: is that you get kind of a curve for the oxygenated, the red, and for the deoxygenated, the blue, right? And so, again, these kind of do proxies for activity within an area of the brain. So, in our study, so we've had our data, our ethneres were sampled. We've had our data, our ethnicers were sampled at about 7.8 hertz. There was some little bit variability in the amount of time for each task, right? You want to have to, because you can't have the kid and mom line up perfectly. So we had approximately about 120 seconds or kind of two minutes per stage of these phases of the still face task. We then used TED students fancy toolbox that does the traditional processing of the ethneres. Processing of the ethnirs data. We then rescale the data just between zero and one to make life easy for us, where kind of zero to one-third is going to be the baseline, and then one-third to two-thirds is going to be the still face, and then after that is the recovery period, right? And so we're going to have is resampled so that we have 500 measurements within each of these periods of time. And for this analysis, we're only going to focus on the oxygen. To focus on the oxygenated, um, you could do both, they're kind of very highly correlated, and you don't really get much more. Ted said, Let's just look at this one. If you want to do the bivariate, you can do that on your own time later. So I said, fine, we'll do our first paper with your just your oxygenated. So this is an example of the sort of data that we're going to look at. So for this analysis, we're only going to look at four channels, and these are all channels with And these are all channels within the, up in the prefrontal area, right? And we're looking at the prefrontal area because, you know, that's kind of, it works on your reasoning, on your memory, your working memory, as well as has lots of connections to the amygdala. So it kind of works on that emotional control, the memory part of the emotional control, right? So we're interested in looking within this area. So what we have here are data from two subjects, top row subject one. Top row is subject one, bottom row is subject two. Remember that you can kind of see certain characteristics in these that might come through a little bit more later on. If you remember that between like one-third and two-thirds kind of are the boundary lines, this is here the baseline, then the still face activity, and then recovery, right? So, our goal is we have these self- We have these self-report, these data of the mother that reports about the child's behavior. We have this kind of more laboratory, more objective measure of the child's behavior. We want to kind of really understand what exactly is the mother telling us, right? So we want to have an understanding of the association between the IBQ reports from the mother and the child's actual reactivity. Okay, so a couple of questions. Well, actually, so the first thing that we started with this is if I look at these data, so if you say, let's just say you give me one number, I bq negative, right? It's a scalar sort of thing. Well, I got four functions. I can consider these responses, right? I can do a function on scalar regression, right? Cool, cool, cool. That's what Howe Yi did. He pulled up the refund package. He just did function on scalar regression. Hit go. On scale regression, hit go, showed it to Ted. They're like, this is great. You've given us that, you know, we can kind of see how you expect levels to change, mean levels, based on how your covarior predictor changes. But I also want to know, is there some sort of what's the general pattern in the population? I want to know patterns of these as well as associations. What we were giving them didn't quite hit that. Okay. Okay, so going from that, our motivating study and questions are a fewfold. Number one is: can we get kind of a parsimonious representation? Are there sort of groupings? Are there sort of characteristics that we can say, these are kind of what these data in general look like? Take the four-dimensional overtime data and give a parsimonious representation. Then, how can we build How can we build connections between this parsimonious representation and our covariates or predictors that we want to care about? So, our proposed solution to this is kind of three little parts. The first is, well, I want to know patterns. That's kind of just seems like it's a functional or time series clustering sort of thing. I really want to know associations with those clusters. So, let's build a Clusters. So let's build a model just based on a mixture of experts where you have these experts that are these clusters and allocation within each of those clusters or patterns is dictated by the covariates or predictors. So that's our solution. So the result that we're going to get is something that I guess you can consider as kind of semi-supervised learning of dynamic data in that you will get these sort of clusterings, right, which is unsupervised, but it's guided. Unsupervised, but it's guided by these covariates to help you out. The nice thing, it's kind of simple, and you get these finite summary representations of trajectories, which can be easily interpreted. And it simultaneously allows you to do that sort of clustering dimension reduction while doing inference. Okay, so there's been a ton of work in time series and functional clustering, right? So, similarities. Right. So, similarity, entropy-based dissimilarity, sort of metric sort of approaches, that's there's a lot of that. There's also a lot of time kind of modeling approaches to this, hidden Markov models, mixture of ARMAs, ARIMAs, whatever you want to use. There are also kind of more non-parametric approaches in which, so for instance, using underlying smoothing spline models, which we're going to take on. So, mixture of smoothing spline models are fairly nice in that. Models are fairly nice in that it's although they're initial formulations, you know, in infinite dimensional, high-dimensional, you can easily use a low-dimensional approximation, which makes things go very quickly. So our general idea here is we want to adopt a mixture of experts framework. So the idea here is that we think that we have, let's say, in this example, three networks or experts, right? I think of those as the sort of clusters. As the sort of clusters, are there certain patterns of children? Let's say one pattern is just naturally very high, one pattern goes from high to low, one pattern is naturally very low, right? So sort of a clustering of that. And then each child doesn't necessarily have to live 100% in any of these, but they can be assigned a certain probability to each of these sort of clusterings. And when you average all that together, you can then model that individual's trajectories and patterns, right? Trajectories and patterns, right? So, in a way, right, it's just a mixture model so that you have basically, I can think of bases, or you've learned some sort of underlying structure that then get mixed together. So how are we going to approach this? So let's say that I here is going to index our subjects. K is going to index our series. So in our example, we had K equals four series, N equals, it was. Or series, n equals, it was 80 some subjects or pairs. And so let Yik be our observed data at time pj. So J is going to index our time point for the ith subject. And Z IG is going to be a binary variable that indicates which group you live in, right? So Z I G is going to equal one if you live in the Gth group, right? Okay. Group, right? Okay, so given G, so for any single group, we're going to model the data from that group or that expert as some sort of a smooth trajectory, this script F, plus then some sort of random noise at the end, right? And we're going to assume that this is smooth, right? You have square second integrable derivatives. Okay, so to make this a little bit more tractable, we're going to use the More tractable, we're going to use the finite approximation for smoothing splines. So we're going to take our data, let's say for the jet group, our kth component, stack it over time, and we can write that out as a linear model, right? So here, right, we're assuming square interval second derivative. So that's a cubic smoothing spline. So its null space is going to be linear functions, and then you have wiggle functions after that. So let's model. Functions after that. So let's model that. So the first term are going to be just linear functions. x is going to be your design matrix for linear functions. Alpha are going to be the coefficients that live on them. And we're going to make them Gaussian priors with some diffuse variance. The second term is going to measure things that are wigglier than linear functions, right? So w beta of g is going to be a zero-mean Gaussian process that has some value. Process that has some variance that has a smoothing parameter tau out front, and then some variance covariance matrix. So, that variance covariance matrix, we're going to choose the traditional variance covariance matrix for a cubic smoothing spline. And you can see here that your smoothing parameter tau squared, if tau gets really, really big, then w beta g is going to be basically zero with probability one, right? And so then you squish that. And so then you squish that last term, and what you just get is a line. If you allow tau squared to be really, really small, well, then you can interpolate all of your data. So it's a smoothing parameter that balances how smooth is the underlying trajectory. Okay, well, this is cool and all, but if I have n time points, this guy's going to be an n by n matrix, and that's just not going to be very practical. So let's take. Very practical. So let's take the spectral decomposition of that guy, and then we can use a low-rank basis approximation. Okay, so that is how we're going to model each of our experts. So each of our experts or groups are going to be modeled as some smooth trajectory, right? Well, how then do we kind of factor in these covariates or predictors for how these mix to get information for individual subjects? Well, we're going to use multinomial. Well, we're going to use multinomial logistic regression, right? So, your probability of being within any of these one groups, we're going to model that as a multinomial logistic regression, where here your Vi's are a set of our covariates or predictors. Delta are then your coefficients for those guys, as well as a random error term or a random effect for subject. The idea of the random effect for subject. The idea of the random effect for subject is every male kid who's 12 months old with the same head circumference is not gonna have the same pattern, right? And so this allows for us to be able to account for that. Well, you take all this, you glue it together, you got yourself your augmented likelihood, right? And you can go to town on that. So I think I might be running a little short on time. So I put the traditional prior distributions on everything. Prior distributions on everything that will make life great and easy, and you can give sample that really easy and quickly. Now, our sampling scheme for this, a couple of things to talk about. First is our number of components G, right? There are many things you can do with that, right? You can put a Dirichlet process prior up on that guy. You can, if you're sadistic, you can reversible jump or do something horrible like that, right? So, what we ended up doing is just fixing G. Is just fixing G and the reason being is interpretation, right? Is that we want to say that you have these clusters and then you have these parameters that say how you load onto those. And if you allow G to have its own distribution and move around, it's not exactly clear how to interpret those. And so you can use pick your information criteria of choice to kind of help guide selecting G. We can then, this actually follows. We can then, this actually falls out. It's a fairly easy Gibbs sampler that you can do. You can actually, as well as for the conditional on grouping, conditional on Z, it can be run in parallel for different series and different components. So it actually moves fairly quickly. This is a mixture model. Mixture models have to deal with label switching, right? So we have to deal with label switching here. We've just selected one of the most popular ways to deal with that when you have Bayesian. Deal with that when you have Bayesian finite mixture models. Now, one thing to note of this is this idea of correlation. So if you notice here, conditional on G or group, your individual series are independent from one another, right? Okay, that seems like that might not be great. However, the posterior The posterior distribution when you marginalize over Z or your groups are correlated. So at the end of the day, what you get, a benefit is you allow to have a very simple capturing of correlation among series. And at the end of the day, you can get for individual subjects, you can model their correlation, right? Now, the downside with this is that it only allows for interpretation. Allows for interpretation of the covariates on these kind of meany sort of trajectory things, right? Because you're not modeling within those experts that there's this cross-correlation. So that's awesome for FNIRS where the goal is that I want to be able to look at associations on these kind of mean trajectory patterns. Now, if your goal was to look at, say, coherence or those sort of things that are these cross-series, this won't work. So this works. Won't work. So, this works very well for ethniers. It won't work for something such as EEG, where you're really concerned about the cross-series talking. Okay, so we ran this thing. It turned out that two components was on every information criterion we use, the winner. And this is what we got for the means for the two groups, right? And so, for the first group, we're calling this the no response. We're calling this the no response group, maybe no or little response. If you notice here, each of the components goes down during the still face activity, right? So the second component, which we call the response group, each one of these in some way jumps up during the still face activity. Now, in a healthy setting, you'd expect to have more activation, right? This kid. Activation, right? This kid was having fun talking to mom. Then mom still faces him. He's upset. He should be upset. He or she should be upset, right? So in a way, I think of component two as being the healthier-ish one and component one being the maybe less healthier-ish one. So what we have here are the estimates of the coefficients from the multinomial logistic, well, I guess it's just logistic regression. Well, I guess it's just logistic regression, not multinomial, because I just got two groups, which makes life easy of all of our coefficients there with their 95% credible intervals. Right now, the most interesting thing here to note is IBQ negative has a positive estimated odds ratio, right? Which means kids whose mother reported a higher IBQ negative, I should say, ah, wrong direction. Wrong direction. This is the baseline. So, kids with a higher IBQ negative have more of this first group, right? And so, what does that mean? Well, one would think that, right, if you're freaking out, you'll have more activation. The mother's reporting that these children have less resiliency, more fear, these sort of things. But on average, they tend to have less activation in there. So, what we might be So, what we might be getting at is that what the mother's reporting is a general, maybe not actually responding to stimulus, but a general kind of fearful anxiety. And so compared to other kids, they don't have that increased activation because they're already at this heightened response. Now, a couple of things about this to conclude. Number one, this idea with spatial correlation. Deal with spatial correlation. So, this worked very well, not having to deal with that or dealing with that in our experts. However, maybe somehow putting it in there using maybe a simple separable structure with space and time, that seems like it should not be too hard to do, can help give us interpretation with second order cross-series, as well as perhaps give us a little bit more parsimony across space. Right now, we have very few channels. Space. Right now, right, we have very few channels, but let's say that we were to run this in something like 256 EEG, right? It's the conditional on G, you're linear in your number of series, but you're not when you're estimating your mixing parameters. So if you can model the spatial aspect and do some sort of a dimension reduction to that, maybe that will actually help with the computational feasibility when you have a large amount of channels. Large amount of channels. The other thing deals with just the study, and this is actually the study design bias. And I thought of this very recently. So, one of my colleagues, Ben Risk, has an awesome R01 that looks at this issue that we have these big bio-banks of brains, right? And people upload all these fancy brains in there. And he, in particular, is interested in looking at autism. Well, when you look at how many scans get tossed out because of movement and certain things. Because of movement and certain things. But then, also, if you're looking at autism and you have bad symptoms, you move more. So, you're tossing those data out. So it's very clear what's happening there, right? That could also be an issue in our study, right? We had about 10% of mother children didn't complete the study. Well, if you're not completing the study because the kids freaking out trying to get on the bus to get to the clinic, well, that's all. Get to the clinic, well, that's also a similar issue. I have no idea how to perhaps solve that. Ben's solving that by actually running extra scans on people so that you can get extra information and do inverse probability weighting. I don't see how to do that in this setting, but just recognizing that that could be a bias of our study. All right, thank you. I don't know about channels at all limited into two simple ones. First, the damping rate of 7.8 hertz was that bigger than the colours of the city. Okay, well, actually, how ye should be doing that as we're speaking. We wanted to make Ted happy, and thank you. And also, during the primary time to file a screen. Great, thank you. This guy was fine. Oh, well, we set we set to the last group to be baseline at zero. Correct. Thank you. So in practice, how did you determine the number of group, or you basically just have some scientific? We use the information criterion to set it. I mean, we also look at, so use the information criteria to guide it, but also, I mean, one could. To guide it, but also, I mean, one could anytime you do any mixture modeling or clustering, you can get clusters, but do they mean absolutely anything? And when I say if something, do they actually exist or not? Who knows? But if they can be interpreted, I'm a fan of them. I'm happy that for our application, it came up with two that happened to be interpretable by Allison and Ted. By Alison and Ted. But that could be a challenging sticky point. Okay, okay, thank you. We have time for one more question. No? Thank you.