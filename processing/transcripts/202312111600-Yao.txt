Um, so this is the uh work jointly with uh Yuyama and Sorry Won and also Ling Shu. Okay, let's see. I will first give a brief introduction of mutual regression. In this morning, Mutual regression. In this morning, already the speaker talked about the motivation of the mutual regression. So, this part I will just skip. I don't need to use the example to motivate this model. My main talk is in the second part, like the mutual regression with unspecified error density. Okay, so I will talk about if the error density, if we do not assume We do not assume a parametric form whether the model is identifiable or not. So, here I will also compare with the morning's talk, okay, and talk about how to estimate such a model. First, let me give the setting of the mixture of regression. So, I introduce a latent class variable. So, this is sticky. Class variable. So this stick to the theme of this workshop. Okay, so this latent variable, assume we have like, say, M classes, okay, M classes or M groups. So they equal J. So that's the proportion for each group. Okay, proportion for each group. Then within each group, Y and X has a linear relationship. But for different groups, the coefficients, linear coefficients could be different. So beta J. So the Okay, so beta j. So across different groups, they have different coefficients. And then the traditional neutral regression, they also assume the arrow is normal, normal distribution, okay? So that they can write down the conditional distribution. Let's say y team x is this mixture form, okay, mixture form. And then once we have this, the estimation of parameters can be done by the MLE, okay, by maximizing this one using the EM algorithm. One using the EM algorithm. So this part is simple. And for EM algorithm, we are familiar with that. So the E step calculates the classification probability. So here I'm mainly focused on the M step. In the M step, we will update the coefficients. For example, in the J group, we update by minimizing this weighted list squares. Okay, so Okay, so this part very familiar is like traditional least square estimation, least squares. And this part is Pij, is a classification probate, like the posterior probability. The probability, the eyes observation belong to J's group, okay, or J's cluster. So if P I G is just let's say extreme case, P I G just zero one, then this essentially is just the least squares for let's say all observations in J's group. Observations in JS group. This is the M step. Here, the main comment is for traditional linear regression. Okay, for traditional linear regression, the least square estimate, the coefficients estimation, the consistency, actually does not require normal assumption. So we know the consistency only So, the consistency only requires the expectation of error is equal to zero. But for mixture of regression, the main difference is for mixture of regression, the MLE, the MLE or the beta estimation will be invariant, invalid, or inconsistent if the normal assumption is valid. Okay, that's the main difference between. Okay, that's the main difference between the traditional linear regression or homogeneous data and the mutual regression. Okay, for homogeneous data, as I mentioned, the normal assumption is not needed for the consistency of the beta. Just if the area is normal, then this square is the same as MLE. But for this mixture model, this normal assumption is is crucial. Okay, it's very um important to get uh to prove this kind of consistency of parameter estimates. Okay. Of parameter estimates. So then the natural question is: could we relax this normal assumption of the error density? Okay, similar to like to the traditional regression scenario. So that's our main purpose. So here all the previous part is still the same. Okay, so we have the this proportion and also within each group. Also, within each group, we assume 1 and x have this linear relationship. Okay? But now, for this error density, we only, similar to the traditional regression model, we only assume the error is the t is equal to zero. And the error density, for now, we assume satisfy this kind of a scale family. Okay, so this g is unknown, but with mean zero and a variance one. zero and a variance one okay so this uh but the different components have can have different let's say variance sigma j okay but for the g we already standardized okay standard that's the only assumption we have okay for this niche for this new model well now the natural question is after we relax this for niche model the one main uh difficulty or one main challenge is Or one main challenge is if we extend the parametric model to some like semi-parametric model or even non-parametric mixture, where the parameters still be like identifiable. Okay, sometimes if the model is too flexible, then the parameters won't be identifiable. That's the main challenge. So the well, here just to summarize, that's the model we have, the nature model we have, and this epsilon is this, okay. Epsilon is this. To define the identifiability of the niche model, we first need to mention one scenario. Yes, for nature model, there's one scenario. Yes, if we permute the labels of the component labels, actually, like for example, here, if I write down, those are all the parameters. Let's see the pi 1, sigma 1, beta 1, the first component parameters. Component parameters and here, let's say the M's component parameter. But if I permute all those labels, okay, permute all these component labels, actually the density same. Okay, but for mitch model, those kind of let's say non-identifiability is okay. Okay, we still think if this is the only scenario to make, let's say, the parameters, the two commode density the same, only when we permute the labels, we still think this model is identifiable. This model is identifiable. Okay, so here, the definition, let's say the identifiability of the model, we define, we say the mutual regression is identifiable for parameter theta. If these two are the same, implies these two parameters are the also the equivalent after some permutation. Okay, means the theta one, theta two. The only difference is the their labels are are switched. Their labels are switched. Okay, that's the only scenario can make the two densities the same. For such kind of model, we still think they still define they are identifiable. But for our this relaxed mixture of regression, so on what kind of scenario, okay, if we for the GJ for the error density, if For the GJ for the error density, if we do not assume normal assumption, the question is whether the parameter are still identifiable. If you guys remember in the morning, okay, in the morning, in fact, for the speaker, okay, they mentioned for their model, a little bit different. It's like a two component. The first component is known. The second component is unknown. And also, they consider, for example, the location model. They require the second, they proved. They require the second, they proved the second component if it's symmetric. Okay, I also confirmed with them. If the second component is symmetric, they may establish some like identifiable results. But for our model, here, one difference is, of course, we don't have, let's say, the first component is, let's say, is known density, all component is unknown. Okay. And also, we are in the regression setting. Okay, regression setting. In fact, this is. In fact, this is very challenging problems, a very challenging problem is to establish the identifiable result. And we proved as long as these slope parameters are distinct vectors in this domain, in this RP, and also the domain of X contains an open set, then this model is identifiable. Okay, this model is identifiable. Means all the beta J can be. means all the beta j can be identified. And here, the main important part is we do have this X information. Without this X information, then we won't have these results. Okay, we won't have these results. And as I mentioned, actually, if there's no predictors in the literature of the mixture models. The literature of the mixture models, they prove that the identifiable results require, let's say, all GJ are the same and also require the GF symmetric. But once we have X information, so in some sense, for X information, we can condition on X. At each X, there is a location, it's like a mutual location model. But across different X, they can all bring this kind of information about the beta to make the, it's just the Okay, to make the it's just a a rough intuitive explanation. I already like provided one of the most important result is to prove the model is identifiable even without the normal assumption of the error density. Okay, then how to estimate the models? We can also still We can also still just modify the traditional EM algorithm. Okay, modify the traditional EM algorithm. Then this part is not difficult. So in the E step, we replace the normal density with, let's say, estimated G. Okay. I will talk about the G, how to estimate the error density G. Here, in the M step, now the beta, the coefficients, regression coefficients will be estimated by this more like the By this, more like the MLE. Okay, it's not least square anymore. We will use this like a log G. Okay, the G is estimated density. And the estimate density can be by the weighted like the kernel, okay, by the kernel density estimation. Okay. And also, I want to mention some special case. If we assume all sigma G are the same, means if we assume all component means if we assume all component all the components that are homogeneous have homogeneous densities okay all sigma j are the same actually the hunter and young in 2012 they also that's the special scenario of so they study a special case of this okay so provided like a numerical results okay so our scenario is more general and also we we established this identifiable So we established these identifiable results. And also in their paper, they just use, they still use the least squares to update their parameter estimates. And they didn't provide any asymptotic result for beta. So here the main contribution of this paper, I say one is to establish the identifiable result of this model. Result of this model. Okay, so in fact, for this project, the idea is simple. Just to try to relax the normal assumption of error density to see whether we can still get a consistent result. But the main challenge is to establish this identifiability result and also the asymptotic property of these estimates. In fact, we spend a long time to establish this. Well, I should say all. Well, I should say all those some particular results was established by Yehma. She is an expert in the semi-parametric model. So I asked her to do that. But she also spent a lot of time when she gave me, well, we, for me, I mainly provided the, how can I say, once she has a draft, ask me to Google to see whether I have questions. Just provide a few points. Just provide a few points, okay? Comments. That's all of my contribution for this part. And she spent a lot of time to work out the proof. The proof like 90 pages long. I can imagine like she admired her, very dedicated. For me, I can even spend time to copy her proof. I don't have patience to do that. Okay, so because for this part, the main difficult part is there's no, how can I say there's no well-defined objective function for this. Okay, there's no well-defined objective function for this. We only have like a few like this algorithm, have these equations. Okay, that's the main part. If we have well-defined object function, then the proof is much easier. Okay, so this is for our general scenario. So here, let me use a simulation study to explain the advantage if we relax this normal assumption compared with the traditional mixture model. Here you can also see for mixture model. Here you can also see for Mitchell model, use the simulation study to demonstrate the relaxis assumption is very important because in reality, how can I say, in reality, this kind of normal assumption is hardly, let's say, hold. For non-mixture, it's either non-mixture, we don't need a normal assumption. As long as sample size is big enough, the parameter is always consistent. But for mixture model, it's not this case. So here, Is not this case. So here I consider like a simple scenario, only two components. Okay, so the x is uniform and also equal proportion. The epsilon two is 0.5, epsilon 1, the distribution means epsilon 2, the standard deviation, okay, only 0.5. Okay, so they have like a different standard deviation for the two components. And also, we consider a different scenario like aerial density. For all aerial density, the mean is still zero, but the shape is not normal. But we consider one special scenario, like case one. Case one is normal, exactly normal. But in mitch model, actually, there are many works to relax normal assumption, but still, they assume some parametric model. For example, the Some parametric model, for example, the there's like a mixture of t distribution, there's also like a mixture of a scaled distribution, they assume some other uh distribution family, okay, um, to make the model uh how can be also applied to some other special type of data, but still for all those models, they need to assume some parametrical form for the error density. So, here, uh, in this simulation study, I consider all those scenarios, okay, all those scenarios, like different error density. Okay, all the scenarios, like a different error density. Okay, some density, like say they are scaled density, okay, skilled density, but still mean all mean are zero. I considered a few scenarios, like the first estimate is the MLE by the EM algorithm. And for this one, I assume is a normal assumption, okay? Because practically, for many mixture of regression, they just blindly assume area. Blindly, assume areas normal, okay. Apply the package. So here I'll just assume this normal assumption. And also this KDEM, this means this is the proposed method. We use the kernel to non-parametric method to estimate the error density. And also we a special case of this one is assume homogeneous component variance. So we assume all components have the same variance. Components have the same variance, and also the called the LSE version means in the M step, we use the least square, which is least square to update beta. Oh, here I didn't emphasize actually the main part is we need to use this like Use this like consistent estimation of this PIJ. So once PIJ, the updates of PIJ is updated based on this, let's say, estimated non-parametric density, G, okay? Then in the M step, this part, the updated beta, okay, if we use the least square estimate, it's still consistent. This part is okay. As long as when you update the PIG, you use the non-parametric estimation of G. Okay, for this M. Estimation of G. Okay, for this M step, you can use more like an estimating equation, okay? As long as you can use like a least square or use some other, let's say, cubes criteria, they are all okay. So now we provide, like say, the efficiency, relative efficiency, compare with the traditional neutral regression, assuming uh normal density. Assuming normal density. The first scenario is normal. If you assume normal, so the traditional method is the best. So all other methods, the relative efficiency should be less than one because the normal based method is the best. So this MLEM, so we use this one as like a base. Compare all the mean square error with this one. Okay, relative efficiency. The first scenario is because this is the base, so they all relative efficiency. The base, so they all efficient all one, but for all other methods, the case one, they're all less than one. That's reasonable. Okay, the normal base is the best. But for all other error density, you can see this second method is our method. So you can see the efficiency improvement could be very, very large. Okay, if the error density is not normal. So here, just want to demonstrate when the normal error assumption is. When the normal error assumption is valid, so this new method, the efficiency gain could be very substantial. Yeah, we consider all like all other different error density. Okay, so you can see the efficiency gain can be pretty large. Also, real data application. Application. This one is eight horses is located with the ukraine infections, anemina virus infections clone, and also five horses with this vaccine strain. So here, 45 observations were obtained from these eight mixed gender horses. Also, after let's say 50. Uh, also, after let's say 15 days immunization period, let's say five horses, we have vaccine of those. Yes, let's see, okay, 39 of them, 39 observations were normal. This is ID 1 to 39. And three horses who were not vaccinated, total let's say six observations is ID four forty two, forty-five. 42, 45 had a fever and two of them died. So, here the response is a log value of varied loads, which measures the immune ability of the affected horses. And where the predictors are those, I'm not familiar with those, the meaning of those. So, for this data, actually, since we know which horses. Since we know which horses are vaccinated and which are not, so we know like the classification. But when we apply the mixture regression, we assume we don't know the true label. Okay, so we just apply the mutual regression directly without using this label. Then we compare with the true label to see. First of all, this is based on the MLE assuming the normal, based on the normal assumption. Uh, some based on the normal assumption, and this is the without a normal assumption. So, we use a non-parametric method to estimate the error density. And here, we have two components: those are the beta coefficients, okay, beta coefficients. Um, and also we report the correct classification percentage, okay. And if we based on the normal assumption, uh, the classification accuracy is only like 62 percentage, okay, compared with the true true labels. With the true true labels, and but for this new method, we have like 100% for this real data set. And I also draw the scenario. So here, I use this vertical line is a posterior probability for each point, okay, for each, let's say, horse. Okay, the probability belongs to let's say the first component or second component. Okay, and also I use a different sign, like this circle, blue circle, let's say if you. like this circle blue circle let's say is a bit first component yes let's say the the should be like say the first one and this should be the the second one however for the normal based method okay you can see some uh those one belong to the first component but their probability is small okay so those will be like a long classification okay but for all new methods Okay, but for all new method um works well. Okay, all new method like can uh correct rate is one hundred percent. So, how many minutes do I have? Okay, okay. Um so before it's about the Before it's about the each component, the regression is still a parametrical model, okay? Parametric model. And we can extend this idea to, let's say, to relax the linear regression model assumption. For example, like this is one motivation example, like the response could be like a house price index change rate, and the predictor is a GDP. Okay, GDP. Here, GDP. Here, at a different economic cycle, the relationship might be different. And also, within each group, the relationship is not linear. So, in this scenario, then the mix of linear won't be able to apply. So, we need to use like the non-parametric part. And similarly, we can also assume, for example, here. For example, here, one is the possible, like, say, extension is one as assume this latent variable also depends on x. Okay, so you can either use logistic or use the non-parametric method to estimate the pi j. And also, within each component, this relationship mj could be also an unknown smoothing function. The final point is to. The final point is to relax the error density. So, here for non-parametric regression, actually, we can also relax the normal assumption. Just assume this like a location, this is a scale, like a scale family. For this model, we can also establish, let's say, the identifiable identifiability result and also get the asymptotic properties. The asymptotic properties, so this will be like some interesting future work. Okay, so this part I let me skip. Yeah, so the EM algorithm, think this part is sim uh is simple. So I will just skip and those are the reference. Okay, thank you. Okay, thank you.