Not to disappoint you. So, this talk delivered differently. So, this is the research done together with my co-authors. They are here, Lasze Primitze from New York, Abu Dhabi, Bilisi Kutaisi, so he knows better where he is now, than Elias Pirkovsky, who was in New York, Abu Dhabi University. New York Abu Dhabi University, Natalia Duko, my PhD student, and Viktor Aduka from South Ural University in Russia. I have permission to give this talk, but not everything what I'm going to say my co-optor agrees with. So some of things I'm doing on my own responsibility. Actually, the talk, what I'm going to deliver, it's slightly changed after I gave the title, so the song of the last bit is changed. Also, this only the last bit is changed, and it's now how close we are to the factorizing arbitrary matrix function. Even years ago, I would think that such a talk would be really difficult to expect that anyone would be presenting, so I'm also a bit afraid to say that, but I think we are very close to this task. So, as usual, I'm always trying to popularize our nice town in Wales, which In Wales, which is difficult to pronounce, but when you are there, everyone pronounces simply as Aber, so it's not that difficult. So everyone is invited. Usually I have much more pictures, but since the talk is long, so I take them off simply to save the time for this, but only another few. You saw the canoe on the tour. So if you have one in your own possession, you can try to sail up to here. So we have the marine, so everything is okay. We have the marine, so everything is okay. You can even have a place to park your, but as not only you have a typhoon, we also have quite a severe weather time to time, so then be careful when you are doing so. So, actually, this talk is a result of activities which were done by the group of people working with the window hope on the three consecutive events. So, this is the The event. So, this is the Newton Institute program organized five years ago. Thank you, David, for supporting us as a Newton director at that time. Then, on the workshop, a follow-on workshop after the previous one, and now this workshop in Beer's, what we are here. So, then, and because of this, you will see that what I'm going to say was actually driven. To say what's actually driven by the discussion during those workshops, and this actually just proves how important to have workshops time to time and some period, then it's really feeding the new ideas and then it's actually moving forward with some research. So, I will try to show the outline of the talk from place to place and aligning with the colour where we are, otherwise difficult to understand what I want to deliver. Understand what I want to deliver. So, then first, I would like to give really accurate problem formulation. We are speaking now only on something very canonical because we can speak of factorization of this line or that line or that problem, this problem. So, we are discussing only the case where we are on a circle and we consider the everything in a Winder algebra. So, this means that the All matrix functions can be represented in the series, and if you try to truncate, you would have Lorent polynomials, and they would be playing a very important role here. So, what we know already, all of us, and I'm not going to keep you on each definition because you know this probably better than me. So, this factorization, this representation in the form of multiplication of Multiplication of two factors on the left, on the right, which are analytic and not degenerated in the respective subdomain. And inside, we have diagonal matrix, which contain those famous partial indices. And this is right with factorization. You swap the direction, we have left factorization. So, what we know is that if all the partial indices are here, it's a specific case, it's a cannot. Specific case, it's a canonical factorization. The same story as what I mentioned about the right can be said about the left factorization. The interesting stuff is that since matrices are not commutative, of course, all the factors are different. Left and right factorization are different, but even partial indices are different. So, this is really a bit of pain. But the full series, in a sense, was completed mainly in 19. Mainly in 1958, actually, year before I was born. Yes, absolutely. So, then, so what we know is that if determinant is not vanishing on a circle, so then factorization always exists. We know that partial indices are defined uniquely, but the factorization are not unique. We also know now. know now I'm trying to to come back to my talk in the first on the first workshop so for the scalar problem we know in that setting then we are not imposing any singularity on on the contours and everything is done everything is clear you know what is index you can compute it without any problem you can factorize this color function and this everything is effective constructive so whatever you wish to So, whatever you wish to say in this case, it's done. But when we come to the matrix factorization, that's the messages, even though the series already, as just mentioned, built and then was later developed, it clarified the more specific subspaces, more specific algebras, specific subclasses of the matrix function. What we can do is something more than the general theory. Than the general series. So, what we know is that for the partial indices, still there is no general constructive way to compute in a general case. The same story is with the constructive factorization. If you don't have even constructive partial indices, we cannot do factorization as well. Also, even we have some constructive method and we formally can do stuff, then if the oh, sorry, it should be kappa one minus kappa n, sorry. 1 minus kappa n. Sorry, it's a difference between the larger and the smaller partial indices. So if the difference between them is less or equal than one, then the factorization is stable. But if it's more than one, the factorization is unstable. This creates a problem. Okay, nice theory how to use it usually because it's a disaster. You say, okay, I have factorized this somehow, even with a. This somehow, even with a constructive method, but how do you know that you are not making some small error in the compute in the computation and you found your partial indices not accurate because they're not stable, and then everything is disaster. So, the positive news is that there are classes where we can done everything. So, there is a constructive approach, even in the case when the partial indices are not stable, so we have the constructive approach. Are not stable, so we have the constructive methods to do this, and I will use that type of subclass of matrix functions later for in this talk all the time. And we have a discussion. This was my last talk on the first workshop. We discussed what to do with this. And the belief was that in the real world, all the Winner-Hoff problems, what we deliver for the matrix function, that since they're coming from the nature, Since they're coming from the nature, everything has partial indices equal to zero, there is no problem by the god. And if I try to, so if this is the class of all invertible matrix functions on the circle, and you think about this like it's a view from physics, from applications. Okay, and for pure mathematicians, so we had the belief that everything has stable partial indices. Mathematicians would say that the situation is much more difficult. So, this is unstable, this is stable, because you take any canonical put inside the diagonal matrix, you have unstable, and then you just realize that the number of those unstable is infinity. Unstable is infinity larger than this one. But the good news is the physicists recently showed that in the quantum world, there is something what comes with a non-zero partial indices. And this really plays important role for some state stabilities. Don't ask me, I'm not a physicist. So, but this is something what is very good to know that we can really. Can really discuss in the partial instances, we are not speaking about something that does not exist and does not really create us a problem. So, this is I'm discussing now about this funny conjecture, what we finished it with the first workshop. But then the natural question was: okay, I have the matrix function. I even know that probably the world we are living in. Simply give us all the matrix functions with zero partial indices, but can we prove this? If we could prove this, then we could use different types of numerical methods and we are safe because we know that our factorization is stable. So for us, for me, for Natalia as my PhD student, for Victor, it was a task to create the way how to prove that the given matrix function is. function is possess zero partial indices and then we do something what is what is good. Then we met with Ilyas Petrovsky, Lasha and Gia on a small workshop in Abu Dhabi and discussing this continuing discussion because all the people participating also in the first workshop were discussing what to do and the idea was okay let's Was okay. Let's at the same time we finished the paper with Sergei Ragozin on the constructive methods. We collected everything, what we know, okay, from our point of view, what we could find. And actually, one of the reviewers, I am guessing who could be one, told us it's not good enough. You should write more explanation, probably to have this book. So we said, why not to write the book? And then, since we have enough, we Since we have enough, we saw enough expertise in the team, maybe we would understand how to do, how to invent the method to factorize, to develop constructive factorization for any arbitrary non-singular matrix function. So this was a task. At the same time, Ilya and Lasha worked on the problem what Lasha actually showed in his presentation for the case n by n, but at that time they're working on the n two by two. On the end two by two, and even so that they solve the problem. And with discussion, we persuade them that the claim of factorization of any matrix two by two should be replaced with a more accurate sentences that close to any matrix. So it was something like this. And they continue working in this direction. And on a follow-on workshop, On the workshop, we were able to solve and to deliver the task what we asked ourselves. So we were able to identify the constructive sufficient and necessary criterion for the matrix function to possess stable partial indices. And even more, you would see what I'm going to show. would see what I'm going to show and then it's create actually the question whether we could do we could we could move further we will see what what happens with this so this is two messages that were delivered during the the the the this work follow-on workshop so now I'm going to to what was delivered very fast because it would be complete very very important to understand what what is going on why I think we are What is going on? Why I think we are very close to solving the problem. So, what we realized is that there is a notion of explicit or constructive factorization, but there is another notion of exact factorization. So, then think about if you have even constructive factorization, you have some procedure which factorizes the matrix function in a finite number of steps. But if you make a mistake or But if you make mistake, or okay, not mistake, small numerical error on any steps of this factorization process, and your matrix function is not stably factorized, so you cannot believe in the result. But if you do everything in the exact arithmetic, you are free from this error. So then you can really believe in what you are doing. So having a constructive method plus exact computation, you are really on the safe. Computation: You are really on the safe side to say even the matrix function has bad partial indices because you already delivered everything without any numerical error. So it's immediately created. You are shrink your abilities to deliver the computation to something that is rational and something that is polynomial because you cannot do this in any other way. Okay, if someone can do it, it's also possible, but I do not want to discuss this since already to this point it's not. Already to this point, it's not only Adukov delivered the constructive way to factorize polynomial matrix functions. There are some other methods, but his methods actually basing on the essential polynomials. I'm not going to deep into details. It's very, very technical. But what is interesting for that method, what he delivered, that this method simultaneously gives left and right factorization simultaneously. It's the only method I know which is doing this because you. Which is doing this because usually it's factorized left factorization or right factorization. You do not need actually both factorization simultaneously. When you need to solve the problem, you need only one factorization. But this method cannot avoid this. So it's delivered everything in one goal. It's what is interesting. Even more, if you have the left, they would like to have the right, for example, in his method, anyway, should go back and then factorize again. So you cannot deduce. So, you cannot deduce simply from left to right. It should go anyway through the. So, this message was coded. It's in the Maple environment. I'm not going to go through all this, but actually, what I forgot to mention is absolutely crucial that to be able to factorize exactly, even in this setting, the The determinant of the matrix should be exactly factorizable. So, if the determinant is not exactly factorizable, the matrix function is also not exactly factorizable. This does not mean that you need to know all the roots of the polynomials, but they should be factorizable in the normal sense as we factorize the polynomial. So, we should remember about this condition. This condition is crucial, and you would see that finally. And you would see that finally, all of us, some with understanding, some simply by hope, because David mentioned about determinants which is equal to one, remember? And Lasha mentioned of the determinant, which is t in the degree theta. We also would have the same. So, this is actually a necessary and sufficient condition, and this should be always checked. So, this quote checks. This code checks everything, so it's written where full, it's even checked whether you put it to the quote Lorentz polynomial or something else. It's really doing on that line. So I'm not going to discuss about this code. This is how it works. So everyone can use it. The code is open, so it's somewhere published. And you see, the use simply solver is doing the job, and this next command is simply extracting the result. So you can. Extracting the result, so you can write everything in one line, so it will be extract everything. The interesting and nice story: you see that this absolutely simple trivial matrix function gives you two different left and right factorization. One is stable, another is not stable. Okay, and you change only one coefficients, also rational coefficients from a half to one fourth, and then disaster happened. Disaster happened, the code tells you that there is no way to exactly factorize. So it's possible to factorize, but not exactly. So, and since you see that the one of the left or right factorization was not stable, if I'm doing this not accurately, then I should not formally not believe in this. Okay, so we are going the next, and now, later on in my talk, I will. In my talk, I will always use, try always to use two colors. One is a bit something like a blue, another something is a red. So I want to say now that actually we have now two codes to deliver exact factorization, but absolutely in different settings, one based on the essential polynomials of Haduka's algorithm, and another delivered by Lascher based on the Janashe Langdilava. Yanashe-Langilava method. What is interesting is that when we try experiments with unstable partial indices, the algorithms, both algorithms give the result. Okay, so what does it mean experiment? I will put something what I know. I created matrix function with a not stable partial indices. Hide this in multiplying everything. Ask the algorithm to compute the algorithm to recreate this. To recreate this. So it's a magic, but it works. And what I want to say is that it's even recreated in not exactly if messing. So this gave a hope that probably we are close to really do everything. There are two differences, important difference, but altogether this is also very good to have as a basis. Have as a basis. So, the blue algorithm really gives a chance to construct the stability region if the factorization is stable, while the second having completely different idea for factorization and allowed you to manipulate with the partial indices. And the first algorithm indices will be as code-produced. As code produce, and you cannot have any power on this. And the second, you can manipulate with partial indices. This is the part of the algorithm, and we can use it actually very effectively. So then the blue algorithm was reported and discussed in this paper, what is below. And in this paper, actually, we also showing the criterion for a stable factorization of the matrix function while the second. Function while the second algorithm was in that red paper and was published, as I just mentioned, earlier than us, because we thought about completely different issues what we want to deliver. Okay, so I would like now to show why it's important and you would see how it's possible really to have the constructive and constructive way to prove that your matrix function is stable. Matrix function is stably factorizable. So, those who were on the INI workshop saw this few slides because the slides actually showing the idea and the idea actually absolutely strange. We know that if you do not know anything about partial indices, doing any numerical computation makes no sense. But the idea shows that we are really trying to extract information making approaches and making Approaches and making approximation. So, think you have the matrix function, you believe that partial indices are stable. So, what does it mean? So, okay, this is our knowledge that we know that it's stable. Since it's stable, so what does it mean? If this is the matrix function is here, there is a region where changing a bit this matrix function not deviating too much the norm of the matrix, you will stay again. The norm of the matrix, you will stay again with the same set of partial indices. It's typical stability. So, how we can estimate the region of stability for that matrix function, which may be complicated enough. So, what we are doing, we are doing approximation. And what is good with approximation, we can approximate with a matrix function with a specific class. Then we could deliver more than the general case. And since this is in. And since this is in the chosen matrix class, probably for this particular class of the matrices, we can even deliver the factorization by known constructive methods. So the first knowledge is we have no idea about this, the second even worse, because even having no knowledge about the matrix function, we can do nothing about it. But the two others are really not bad. The two others are really not bad. So, if you have the factorization, the approximation of the matrix, then you can find in the literature or you can deliver yourself some constructive factorization. So, it's possible to do. So, the first two tax admission is impossible. The other two are very good. So, we can deliver the stuff. So, let's change the point of view on what we are doing. So, we still believe that the matrix function has a stable factorization. So, partial indices, difference between the maximum and minimum. Difference between the maximum and minimum not larger than one. So, this only expectation, we immediately approximate this matrix function with something what we know, what something what good, and we can factorize this. And since this is something what is really good, nice, because it's our choice, probably in this case, we can deliver the stability region for that matrix, not for the original matrix function, but for the matrix function, this approximate matrix function. And maybe we will. Matrix function. And maybe we would be lucky enough also to deliver the radius of stability. Then, as only our initial matrix would be included in the region of the stability now of approximated matrix, we are done. So, this is completely another way to think of stability. So, we are not going closer to original matrix, we are simply going closer in a sense together with some different rubber. Together with some different radiuses, trying to catch the original matrix. Okay, so now it's much easier stuff. So then we can hopefully deliver everything. Let's see whether it's possible. So what might happen? We have the code. So if you have the code, let's try to do the approximation with our code. So we have the matrix A there. We started going closer and closer along some path. We will see how this path will be created. Path would be created, and if all the passes and all points on the path would produce a stable factorization, then the radius would not shrink. So, finally, we catch the original matrix and we know what is going on. So, we are very happy. So, what may happen also on the way that first attempt was unsuccessful, but if you continue, finally come close enough to the original, and we catch again the original matrix function. But there would be, so then. But there would be so then in both of those cases everything is perfect, so we can do the job. And in other two cases, so first we can come closer and closer, we have stable, we have stable factorization, but the radius actually shrinking, so we never catch the original one. So this is actually means that the probably, not probably, but 100% by contradiction, that the original matrix function was not stably factorizable. This means that in the neighborhood, This means that in the neighborhood you have some set of unstable, and you see, because of this, you cannot catch the original one. Okay, so this is bad news, so we can do nothing about that. And another also looks like bad news, so we are going closer and closer and never have any stable bit. But apparently, what is interesting is that the last two cases indicate two different algorithms. Indicate two different algorithms. So, this indicates this my point. So, this indicates that maybe I should put red color here and blue color there. So, this indicates how would be done by the Lasha's algorithm. So, he's coming closer, cannot catch. So, he changed the partial indices, trying to catch the proper one. But in our case, we are going with unstable, but. With unstable, but they could be unstable, but repeating all the times the same set. This means we are catching really the original unstable matrix function. Okay, so this is the, this, this, I mentioned in Cambridge that this really creates the hope, the last one, that we can create the constructive factorization with a proven partial set of partial indices in general. Okay, so how we can do this? Okay. How can we do this? Okay, we try to realize this algorithm. So we approximated by matrix polynomial, so we have everything. We can have an exact factorization algorithm. So all the coefficients belong to the rational numbers. Now we remember there is a condition on the way. The condition was the determinant should be also exact factorizable. And this is not for free. It may happen that it is, it may happen that it's not. It may happen that it's not. So, this means that this is really our hammer which will be used to go along this way. But this hammer, unfortunately, is not perfect because time to time we can meet the nail, and this nail could not be straight because the determinant is not factorizable. So, we can do nothing about this. So, we should really do something more with that. So, how to make this nail always straight? And for that reason, we're going to We're going to decrease the space of the subspace of the matrices we're going to factorize to the strictly non-singular matrices. Actually, Lasha did absolutely the same, absolutely for another reason, but I will say, so what is important for that? So, why we need this information? Because we will do preconditioning, and this preconditioning is done only. And this preconditioning is done only if you think when we done this, it's only for our case was for the reason to eliminate this unfortunate situation when the matrix function have an unfactorizable determinant. And simultaneously, you do this matrix a bit simpler. But also, it helps actually to do with the stability region as well. So, this is the everyone. The everyone knows what does it mean that the matrix is strictly non-singular. This means that this element allows for factorization itself and the determinant allows for factorization. So we need additional information. Since this is a scalar factorization, everything can be done very accurately. We know all the indices, no problems whatsoever to work with this. But using this additional information, you can create that. That factorization, it's not what we wanted, but this is a minus function, this is plus function, this is something that would be later incorporated inside. And this guy, actually, it's much simpler. Okay, we see that alpha and beta is a function which is not necessarily minus or plus, but the determinant of this matrix, so we want to factorize, but the So we want to factorize, but the determinant of this matrix is always factorizable trivially. So no problem whatsoever to do with that stuff. So, but having this, we are trying to think for the future. We're trying even to simplify it even further. So, since we are working in a Winner algebra, to do the splitting, the splitting is trivial, absolutely trivial. So, it's not even need to use Cache integration. So, we with split. So we split the alpha and beta and create the next factorization, would make those functions a bit more complicated. But inside, it's looking absolutely the same structure that previously, but it's a bit simpler because the number of coefficients would decrease twice. Okay, we have infinite numbers, but at least if you do truncation, it really helps a lot. And the determinant is absolutely the same. Okay, so this is our nail. It works. We are now on the good side, no problem whatsoever. We will factorize internal matrix and then we'll have immediately our factorization. So how to evaluate stability radius? Now it's a question. And if we are on unstable case, so what to do? Should we continue like in case uh 2A or we should stop because we know that we are done and we could do. Know that we are done and we could do no more. So, anyway, when we are speaking about approximation, we should have no notion like a convergence. And this actually notions requires you that at least at each step you have some uniqueness, otherwise it's a disaster. So, how you have the convergence if at each step of your step you cannot control your result of your factorization. Factorization. So then we need uniqueness, and uniqueness is not given for free. Actually, the classic, our founder of the series told us that factorization is not unique and they didn't deliver this message. So thank you to Victor and together with Natalia, they deliver for two by two matrix function the criteria for normalization, which provides immediately uniqueness. Which provides immediately uniqueness. You see, it's not absolutely trivial, but it's something like everyone would probably expect. It's still not finished for P by P case. And Ilyas Pierkovsky was a bit skeptical because he saw that it's much more difficult to do than so. You see, but point. So the left and the right matrix would be different all the time. So if you want to have convergence of the factors as well, how to have the convergence if you're jumping on each state? Convergence if you're jumping on each state like this. So you should really preserve the normalization on each step. Okay, so this we done to have really winner hope coming back from Raymond Hilbert, we ask for a bit more analyticity along the unit circle. It's not necessary, but it would help at least to deliver the estimate from above for the For the approximation of the original matrix function, this internal matrix function, and then using the classical Cauchy inequality, we can have this really the estimate exactly written, even though this estimate looks like exact, but in your particular case, it should not be really exact. And then it's really overshooting from above if you're really. Shooting from above, if you're really doing computation, so I would not suggest using this upper bound because at least it's one of the discussion in our group whether how important to have this upper bound. For me, it's good enough to have a numerical convergence with a high numerical, I mean, the very clear numerical convergence is good enough, but I have it always. But I have vita always telling that it's not good enough for him because after three million steps, something may happen. Okay, so then we have now the theory, which tells us that if we do at each step proper normalization, and if exists a natural n in this approximation, that after this, I do not want to go into details. I do not want to go into details of some pairs of coming from the previous estimate, we will have this condition fulfilled. Then there is stable factorization of the original matrix function. And there is the theory in another direction, more or less the same, simply telling us that it's only enough to have it once and then everything would be okay. So it's actually not completely equivalent. What is interesting that from What is interesting is that from this condition, you immediately visualize what is really the radius of stability for the approximated matrix function. So, actually, sitting on two norms, of the norm of the left factor and right factor being invertible in the Wiener space. And by the way, this is very important to understand that if I'm even doing something with a non-stable factorization, these norms are. Stable factorization, these norms can also be computed, and this would give us the very important information that we are on the stable path. Okay, so now everything is perfect. We can do everything. We have only one minor issue to say, okay, we have convergence of the original approximation. We know that everything is starting from some point to be factorizable. We prove. To be factorizable, we prove that partial indices are stable. How fast factors converge? It's another story because if you want to know your factorization, you would like to have accuracy of the factor. So because you want to have accuracy of the factorization, not accuracy of the initial approximation. It's something much more s much more severe and this is much more difficult. This is much more difficult to prove, and this actually used the fact that we are in Winner algebra, there's toppling separators involved. But what I want simply to underline, you see, everywhere we have the terms which shows us that the radius of the convergence of the stabilities everywhere is present. Okay, this is simply a numerical example. I do not want to go too much into details. It was chosen to be... It was choosing to be canonical. The matrix is the functions are not that nice, so they are ugly, so they have branch cuts and so on in both subdomains. Okay, you can have for free the series, so then you can do the winner analysis. This is the final final upper estimate. upper estimate the convergence of the approximation is here it's numerically not numerically theoretically delivered and what we see the during the during the the the the numerical experiment that unbelievable fast it's going to to to to converge it's 10 minus 20 you are here so we do not have really very so we have the the upper estimate this upper estimate is somewhere here Estimate this upper estimate is somewhere here. But if you try to do guessing approximation, simply compute something what is far away once and then compute the difference between your each consecutive approximation. This is what is far away. You see, depending whatever you're taking, 30, 40 here, it's fitting absolutely very well. So I do not think really for numerical simulation you really need this upper bound estimate. And then. And then the factors also converging very fast, but what is interesting differently and also what is important that left and the right factors actually from theoretical point of view, you can believe in the results starting from different end. But it's minor issue, what I'm not going to discuss, and also factors converging a bit slower than the original approximation. Okay. Okay, the other examples are in the paper. You can read it. So now we are going to the Lasha's case. So, what Lasha done, so I tried to do very, very fast. You already saw some of those files. So, this completely different preconditioning. You see, they kept the indices inside, and those indices really attributed to the first element and determinant, it does not mean that they are really partial indices. Indices so and then they use absolutely the same condition that is strictly non-singular matrix. So then after this, again, Lasha shows how it works with the algorithm. So then I'm manipulating, minimizing the norm of the factors, manipulating with the partial indices to go to the right path. So we have not. Right path. So we have not have this option because our algorithm immediately brings us to the proper path. But this algorithm really allows you to change, to choose the path to go to the right in the right way. And this is example what Lasha didn't show and I allowed myself to show because I want to underline here something. It's exactly what was on the previous slide. Please pay attention on the two experiments, what was done here. Was done here. So if you assume that you stay with a canonical factorization and believe that factorization of that matrix function would be canonical, your factors, even the approximation is close enough, your factors blowing up. But if you put the right partial indices, they are even not stable, but they're proper, then the factors are not blowing up. factors are not blowing up so they're really a reasonable reasonable value so this remember about this i will try to underline this so the argate is completely different allows us to be flexible during the factorization to change factor partial indices and and and the idea what lash already showed idea below actually belongs to gorga crane and by when they created the Ask him when they created the condition for stability of partial indices. They use this counter-example to show that perturbation, a small perturbation of unstable partial indices possible and not preserve the set of partial indices. Okay, so now I'm very close to the end. So, in that paper, what I'm still continuing with Lash and Weslash and and uh Ilya paper. So they uh formulated the conjecture that they really hope and believe that their algorithm would be working good if the partial indices would not be stable. So I formulated on my responsibility another conjecture from our algorithm: what to do that to deliver the same. So I'm telling that if the If the approximation converges to what we want, and starting from some number n0, we can do exact p-normalized factorization of the approximant. And simultaneously, remember these two nice terms, what appeared in the estimation for the stability radius, are bonded and converge. Actually, I'm not so sure that converge is important, but in our case, But in our case, probably it's possible to prove that if they are bonded because of the normalization, they would converge anyway. Then we can prove that we really recreating by approximation the original matrix function and the internal matrix would be the same. We have a statement formulated by Elia, who proved that if you have two different That if you have two different approximations of the matrix function, which should converge to something with a different partial indices, then at least one of those numbers should blowing up. So, you see, all the time we're sitting in the same constraint that something is blowing up, and then we should prevent this. And then we are on the safe side. Okay, so this is actually justification for their strategy, what I call natural factorization strategy for the. Factorization strategy for the upgrade, what Lash and Ilya developed. So we are really very close to the end of my talk. I still have some time. Yes? Yes. Okay, yes. Okay. Yes. Okay. So, what I want to say is that we have the two completely different algorithms. It would be nice actually to have. algorithms would be nice actually to have them in one in one go in one piece in in one the same in the same environment and available for everyone even would be having two from these two different algorithms the same result it's good enough actually to be more convinced that that it works so we have effective proof for the set of partial indices is table it can be independently used by anyone who would like for example to transform wiener hoff factorization Transform Winne-Hohl factorization in the system of integral equations, and then you know that everything would be okay. You do not need to be worried about, you do not need to do Fredhohl factorization, killing all the partial indices, because actually you do not need to do this, because you would probably lose some additional information if it's not, or you would have the result which is not really related to the original problem. So we can. So we can, I believe, can do the stable factorization, we can recreate factorization of any stable matrix function in a set of stable factory indices. And if the conjectures are correct, what we hope so, and I hope we will do this shortly, then everything would be okay. So, just as a reminder, so this if the original matrix was possessed this table set of parts. Possess this table set of partial indices. In this region, you have everything perfect. You do not have the radius of this, but you can have the radius of all the approximation. Everything would be okay. But if the original matrix was not stable, then you can move along different paths. In our case, it's one path. In other case, Lasha is choosing the passes in different way. And then hopefully, we will come to the same conclusion, to the same factorization. Would be nice, actually, David, to have. It would be nice actually, David, to have your third case and to have the third algorithm. Only on the circle would be that do not need to do too much. It might do things a bit easier than in a general case when you're doing all these very fancy curves. And then to have three algorithms inside. Then if three would produce the same, but then I should be really very, very unpersuadable person not to believe in the result. Not to be listening to results, even without the theorem provided. Okay, so we're starting from starting point: was is it possible to deliver the general technique ideas to factorize albutary matrix function constructively? And the preliminary answer explicitly, probably not. We do not know how to do this, still, even five years time we try to think of that, but surprisingly. Think about, but surprisingly, really surprisingly, because you from one hand, you cannot do any numerical experiments if you do not know that the partial indices are stable. But we created something that numerically allows to do accurate as wish you would like to have this uh numerical factorization of even unstable partial with unstable set of partial indices. Stable set of partial indices. So now, one slide only to say something. Okay, we now know what is natural versus exact factorization. So natural is what was with the Lasha's algorithm and exact is with our algorithm. But I would like now to underline something what I'm calling regularized factorization. Think about some physicists come to you and brought you the matrix. Brought you the matrix. You do the analysis very accurately, and apparently, the partial indices are not stable. But the factors, norm of the factors, something 10 and a degree 25. Are you believing in this? Maybe they brought you the matrix with originally small error and they jump somewhere near the original matrix and they simply deform the matrix. So now it's another. So now it's another philosophical conjecture. Not that all real matrices have zero partial indices, but all matrices coming from the nature should have reasonably bound factors. Inside may be different partial indices, but the factor should be reasonably bounded. So why the factor should have norm 10 and a degree 20, 50, or whatever. It makes no sense physically. So this is another. Another conjecture. And if so, then we have the idea what to do. It's another type of regularization. So you have the matrix, original matrix. It was good enough with really bonded factors, but with not stable partial indices. Someone make experiment in this area very close, make some numerical error, delivered partial indices D. With partial indices differently, but have phenomenal factors. So, what to do with this? Is it possible to say if I choose the radius, some and in this region would search for another matrix function with factors as less as possible. And believing that this is really what we Believing that this is really what was originally here. And of course, if you shrink this delta to zero, you would be closer and closer to original, and your minimum norm of this factor would blow up as well. And this is typically like for Tikhanov normalization in the operator. So you add something, which is very nice norm with a small epsilon, you're really helping you pause problem. But if you go with epsilon to zero, then the result would be blown up. To zero, that the result would be blown up anyway. So, this is the way, if physicists would like to use it, simply to indicate them that maybe the physicists or whoever who is measuring and some small numerical error brought into the process of measurement to recognize that something probably is wrong. And if you have some idea from the physics what the level of level of how you have actually two two two small parameters so one parameter is the is the radius where you search for the proper being close enough to the proper matrix another it would be one over epsilon would be norm of your factors and then play with this two you will say okay i'm i'm having this this restriction for the for the value of the of the of the Of the norms of the factors, and you would know the region where you should search for the optimal regularized matrix function. Okay, actually, I'm really very close to the end of this talk. So, the paper is now being prepared. We're all very busy, but I hope this paper would be ready, hopefully, soon. I don't know how soon, but it would be ready. How soon, but it would be ready. And I also hope that the book that with the pain should appear also probably in this year. It's written, we're still reading, polishing, discussing. I'm not so sure that the book would include what I told us. Maybe, yes, maybe not. We do not have the agreement on this. So now is it the end of the history? So is it Fukiyama? it's fuki is it fukiyama fukiyama story and it's not because okay first of all as i just mentioned would be nice to have all the codes really good code available in one bit to to compute all of this then matrices may not be strictly non-singular okay if all the the the coefficient all the coefficients would have at least one zero Efficients would have at least one zero, then it's possible, probably, but it's not clear how to do this story. But it would be too many zeros, would be really so. The determinant would be still non-zero. So everything would be okay from the point of view of factorization. But it would be not strict to singular. Actually, Anastasia mentioned about this exponentially oscillating on the main diagonal. Remember, you mentioned. So it's also the example, but you cannot do anything with that. anything with with that with that because it's not strictly singular you i don't know how to make it strictly singular actually i didn't mention also that it's very important that everything what i told today was for two by two but okay last showed that it was done in his algorithm naturally to n by n in our algorithm we should have a preconditioning done so there's also something to be done so n by n it's still it still should be okay we don't know maybe someone really deliver unified Really deliver a unified constructive algorithm. Maybe something what I told today may help in a way. I don't know. So, and in the practice, everyone knows that we will have a real zero on the contrary, even determinant may be degenerated. So, usually it's so, we're doing this small imaginary part, adding to deform, to take those singularity away from the contour. But if it's so, then it's It's so, then it's it's it should be anyway. Everyone should do this. So it's not that this method helps. So this method tells you have unbelievably nice matrix which have no zeros or determinants or the contour. If you have those zoos, think about what is going on. That's physics. So try to eliminate, try to build. So it's not really the end of the story. And actually, not all the Winner Hoe problem sitting on topologically on a circle. That's something. That's something Raphael could tell us, probably. But circuit, it's nice, of course. But okay, the three dots could be continuing, I believe, long. So Fukuyama is not here. So then I would like to thank to European Union for the grant which called effect facts, so effective factorization. So to Isaac Newton Institute to provide us the facilities. Institute to provide us the facilities to organize first the program and to workshop and really to facilitate what we are doing to be for support for this beautiful, beautiful workshop. And thank you for listening.