Yeah, all right. Can I? Okay, so there's no equanow. All right, so this is my first time here. It's a beautiful location, perfect for doing mathematics, I think. And I'm very happy that I made it here. So, the stuff I'll be presenting is standard material. It's inspired by It's inspired by several standard sources. It's a highly technical topic. My job, I think, is to try to present a digestible version of this topic. I'll mention in the meantime, as I go along, some maybe new results, but mostly it's standard material. So, very broadly speaking, the main object here is to study exponential sums in the context of analytic number theory. This is a very wide label, and here in this room, there are some world experts on this topic. I'm not speaking to the experts. I'm speaking to maybe persons who are somewhat familiar, but maybe want to think more deeply about it. So, I'll start with some examples. Some examples. As usual, the Riemann zeta function. This is an example of an exponential sum. This has an analytic continuation to the whole complex plane with a poll at one. So one important problem in analytic number theory is to obtain bounds on zeta. For example, on the one line, we have this kind of bound from Have this kind of bound from above and below, and on the half line, we have the latest record which bounds zeta one half plus it by t to the 13 over 84 plus epsilon. So these are hard results to obtain. But in general, the growth rate of zeta is this is an important problem because it An important problem because it presents progress towards the Lindloff hypothesis and it has applications to several standard questions like explicit zero-free regions and bounds on s of t, which is the fluctuating part of the zeros counting function for the Riemann zeta. Right, so when we talk about explicit bounds, we mean not simply a big O bound, but an actual An actual concrete function f and constant c and t naught, such that we have this kind of inequality. So we typically want f to be of as simple a form as possible. The reason is it will make subsequent calculations using the explicit bound simpler. And we want T naught to be as small as possible, right? So that the gap we're not covering is not too large. We're not covering is not too large, right? So, in terms of, I mean, the bounds I presented at the beginning were not explicit bounds, they were big old bounds. There are some unknown quantities in them, like the constant C, the number T naught, where the bound begins to apply. So, here I give some examples of completely explicit bounds where we know where the bound starts to apply and what the constant C. Starts to apply and what the constant C is. So I believe these are the latest results for zeta. We have C here is 0.618, T to the 16 log of T. There is a better one asymptotically when T is large enough. It has a larger constant, 66.7, but a smaller exponent in T. So there is a trade-off between the two things so that eventually this one is better, but for a long Is better, but for a long initial range of t, this one works better, right? Also, we have explicit bounds for zeta on the one line from above and below. So actually, these two bounds are derived using completely different methods. I mean, not completely, but really quite different methods, right? But all of the bounds I stated here. All of the bounds I stated here rely in some way or another on explicit processes from the method of exponent pairs, which is, you know, it's a really highly technical thing, but I'll try to kind of motivate it and explain some of it. Right. So I don't want to restrict the examples I give to only the Riemann data. I mean, you can find explicit bounds for the Explicit bounds for Dirichlet L functions. So let's take chi to be a Dirac Lay character module Q, Q is the modulus. And we can construct L1 LS chi, which is the Dirac Lay L function attached to chi and consider this function on the critical line. So sigma is equal to one half. Right, so here is an example of a bound on L one half. Bound on L one half plus IT chi that is applicable. Well, first of all, to actually talk about it, let me define the analytic conductor. We have two quantity or two parameters, we have T and Q. So we define the analytic conductor to be the product of these two things, provided that T is at least three, right? For technical reasons. Right for a technical reason, and so we the bound is stated in terms of the analytic conductor, and we have a constant and explicit constant here, and the bound is applicable when t is greater than 200. Okay, so to derive these explicit bounds, usually there are two. Usually, there are two kinds of additional sort of ranges of the parameter that come into play. The range when T is small and the range when T is of moderate size. This is not a hard set rule, but it's typically the case. Usually these two ranges do not come into question or play when you're talking about bigger results, because you can assume T is large enough and be done with it. Done with it. But for explicit bounds, you cannot do that, right? So for small enough t, this is usually, you know, theory does not, savings from theoretical bounds don't kick in quite yet. So we just can use numerical computation, right? And in using numerical computation, this is actually a substantial problem because to make the result rigorous, you have to either use to do rigorous error at analysis. To do rigorous error analysis or maybe use interval arithmetic if needed. And then once t is past a certain small threshold, you can use simple bounds, theoretical bounds, that are effective for such range of t. For example, for zeta, you can use the bound coming from the Euler-Maclaurian sum that expresses zeta, or you can use the bound coming from the Riemann Siegel formula that approximates zeta. That approximates data. And once you have gone past these two ranges, you can start using more complicated but asymptotically better results effectively. And these more complicated results, at least in the result, the bounds that I mentioned, come from the method of exponent pairs, like an explicit version of that. Right. So, like, where is how can you improve explicit bounds? Usually, the improvement comes from more careful treatment of the overlap of regions two and three. This is the region where simple bounds are used versus more complicated bounds. Right? So, here is an illustration of how this happens. Illustration of how this happens. So, this kind of horizontal segment is where the numerical bound is used. If you actually do the computation very carefully, then this horizontal segment should be down, match the left point here. But to make the computation quicker, like it actually is sort of less careful, so it produces a bigger number than needed. Number than needed. So that's why this horizontal line is higher than this point, but it's still below the target constancy that we want to prove. And then in this range here, this is where a simple bound like Riemann-Ziegel or Euler-McLaurin is used. And here is where a more complicated method of exponent pairs is used. And the bottleneck. And the bottleneck is in this overlap. If you can treat this overlap more carefully so that you can bring this down, then we can improve the whole thing. This is what's determining the value of the constancy that we get. So for zeta, the exponential sums we get would have a phase function which is like t log u or t over 2 pi log u. Right. Okay, here's another example. And again, this is standard material. And you can find more discussions of this in several standard sources. Is the Dirichlet divisor problem. So we'd like to understand the summatory function of the divisor function d of n and the summatory function once you The summary function, once you write it this way, can be understood as the number of integer lattice points below this curve here. The curve is uv is equal to x, and x is the end point of summation. So the high parallel method looks at the symmetry in this region under the curve, and using the symmetry, it produces. It produces a very nice formula for the summatory function of the d of n. And the unknown quantity in the summary or in this formula is delta x. It's unknown in the sense we don't have like a simple expression for its size. So we'd like to estimate the size of delta x. And if you use the well-known Fourier expansion of the first Bernoulli polynomial, Polynomial, which is this, then you can see how to estimate delta x, you might want to look at exponential sums of this form, where the phase function now g of u is c over u. Perfect. So, another example is. In the middle of the root x, but don't we believe that delta x is like x to the quarter or something? Yeah, there's probably. I mean, we believe it's x to the quarter or thereabouts. So maybe this will not give you the best error, right? The expected value of the sun be negative root x, the fraction of parsing one half on average. Yeah, right. So there's a cancellation. Right. So there's a cancellation that's happening between these two. Right. Right. Yes. Any other questions? Yeah. Anyone better. Okay, this example maybe studied less, but it's about the Studied less, but it's about the distribution of square-free numbers. So, a detector for square-free numbers is the square of the Merbius function, right, which is supported on square-free numbers. And then you can look at the remainder term between, you know, when you subtract the summatory function of the square of Merbius from the mean term, which in this case is six over pi squared times x. And And Montgomery, Vaughan have reduced results like this to estimate the remainder term, which I'm calling delta one here. And if you look at what they're doing, one makes a lot of appeals to estimates of this kind of exponential sum, where the phase function now is lambda over u squared. Now is lambda over u squared. Lambda is some constant or some number independent of u, and then you have a u squared here. So the phase functions that I mentioned so far are like things like t log of u, c over u, and now lambda over u squared. Okay, so let's instead of looking at these more complicated faces. These more complicated phase functions, let's look at the phase functions that may be more familiar, namely when the phase function is a polynomial, P of U. Let's say P of U is a degree D polynomial with real coefficients, right? The simplest case is when you have a linear polynomial. So this is when D is equal to one. And then by the closed form formula for a geometric sum, you obtain this kind of bound. Sum, you obtain this kind of bounds. Importantly, the bound you get here is independent of the length of summation. That's why it's so effective. And you can either bound it by a sign, or if you use some trig inequalities, you can replace the sign here by the closest distance between alpha to the nearest integer, or the distance of alpha to the closest integer. Distance of alpha to the closest integer. I said this wrong. Okay, so that's very nice, and but it's just a linear or geometric sum. If you look at a quadratic sum, so this is when d is equal to 2, then we don't have a closed form expression for the exponential sum in this case, right? But what we can do is But what we can do is we can look at the square modulus of the exponential sum. And once we kind of break the bracket and write it as a double sum, write the square modulus as a double sum, then we can divide the range of summation of H here into three ranges. The first and third range are going to be symmetric, and the middle range actually contains a single value of it. Contains a single value of h, which is h is equal to zero. It's like the diagonal sort of term. So this is what you get. The shaded area here. And the shaded area here corresponds to the first and third range that I mentioned. And the white area here is that single term when h is equal to zero. So that single term gives you the n. And then you match this with this, you get twice the real part. Right. So if you just apply a triangle inequality to this expression, then we get a bound on the square modulus of s when the phase function is a quadratic polynomial. Or actually, I didn't really quite use that the phase function is quadratic. It could have worked anyway for other phase functions. Anyway, for other phase functions. Now I'm going to use the fact that it's actually a quadratic polynomial because then on the inside here, the phase function is a difference some, a difference of p, of the function p. And if you do some algebra, this difference is linear in u. u here would be what n is right so basically we're talking about a sum that is linear in a geometric sum Oh, yes, this is you. Yes. Yeah, so this should be you here. Sorry. Right. So because we have a linear sub on the inside, the inner sum is linear, we can just fall back on the d is equal to one case, which is the first case we discussed, and obtain a bound like this. This right, and this can give you substantial savings provided the coefficient C of H is bounded away from integers. So in summary, if we use this bound on each inner sum, we can bound the square modulus when d is equal to 2 by this kind of expression. It's maybe less satisfying than the bound we get when d is equal to 1. The bound we get when d is equal to one, but still very useful. First, we can maybe notice that this bound is sensitive to the nature of the rational approximation to alpha. For example, if alpha is equal to a over q, a and q are relatively prime integers, and say q is odd, so that 2a here is also relatively prime to q, then Then as H goes from one to Q minus one, two HA will also span the same range of residue classes. And therefore, you can express this sum bounded by Q log Q plus gamma. The gamma is inside brackets, right? Using a harmonic sum estimate. So, if you take the original sum, which had length n, and then divide it into chunks of length q, you can apply this estimate, or the estimates I had on the previous slide, to each chunk of length q and multiply by the total number of chunks, which is at most this, and you get this kind of bound. Right? And this can detect a lot of cancellation for certain ranges of Q. For example, Ranges of Q. For example, if Q is about the size of N, right, so a single chunk is almost the original sum, so it's almost a complete sum, then we can get square root cancellation, which is the gold standard and this kind of thing. Right? But if Q is like at the extremes, then this bound we have here may not. This bound we have here may not be so good. Yeah. Estimate to the bottom one half because of the using not just all the reduced residue positive, but some sort of balanced residue posits between minus Q over two and Q over two, and get one half Q over two. Yeah, yes, yeah, yeah, yeah. Yeah, yes, yeah, yeah, yeah. You're right, you're right, yes. I just did it like a brute force way. And yeah, so I mean, yes, with, you know, once you pass one half, you can't flip. Yeah, so yeah, right, right, right. Yes, it is, it is actually important. Yeah. Right. So, right, so this kind of bounce star can be Kind of bound star can be good for certain ranges of q. When q is about the size of n, it's really, really good. For other ranges of q, maybe like not so useful. Here is a picture that illustrates the actual size of the exponential sum when the phase function is just a pure quadratic here. So here we have two parameters. So here we have two parameters A and Q, the numerator and denominator. And this picture has on the horizontal axis Q and on the vertical axis A. Our bound was sensitive to Q, didn't make any mention of A. And the region where we said that bound was really good detecting square root cancellation is basically this blue area here, bluish area. Right, so we have a lot of territory left that is not. That is not kind of covered, but at least here we're where the bound is doing good, the actual value or size of the sum is also small. This is color-coded, so the more blue the dot, the smaller the size. The yellow color means higher size. Right? So our bound is doing well when So, our bond is doing well when the sum is actually small. Right, so we can look at higher degree polynomials instead of just quadratic. We can look at a degree D polynomial. And if you apply the differencing method once, then the inner sum you get will be a degree D minus one polynomial once you do the algebra. Minus one polynomial once you do the algebra. But you can repeat this and to get down to a geometric sum, you do this d minus one times. And if you look at, for example, titch marsh, then once you do this d minus one times so that you get a geometric sum at the bottommost layer, and then you can bound this geometric sum using the estimate when d is equal to one. Estimate when d is equal to one, then you can combine all this into this kind of explicit bound. It's, you know, this is a bound where it requires maybe some work to actually use it. Right, so one important observation about this bound is that even though I define my exponential Even though I defined my exponential sum s to be to range from 1 up to n, the bound is actually independent of the specific range of summation. I could have done it from 100 up to 100 plus n, or 1000 up to 1000 plus n, and I still get the same kind of bound. This is very handy, very useful, or can be. And the reason for it is because the geometric sum at the bottom most layer is not sensitive to the specific range of summation. Of some issue. Here, maybe one can talk about what we could hope for. I'm not going to be really dwelling on this, but maybe one can hope that to prove something like this. So when D is equal to two, for example, you get like n over square root of q. This is the quadratic case. But we're, you know, I'm not. We are, you know, I'm not really focusing on what we, what we hope to prove. I'm focusing on what we can do. I'm not sure. Yeah. Right. So, okay, so now things are gonna progressively get more technical. And I'm sorry about this in advance, but. But okay, so to deal with like not polynomial phase functions, but general type of phase functions like T log U, C over U, lambda over U squared. These are the examples I gave at the beginning. One can still use this differencing method, the while differencing method. And And also, one can refine the differencing scheme by introducing a certain parameter. Basically, the reason M is introduced is to ensure that when you apply a geometric sum estimate, you're applying it to sums that are not too short. The length of the sums you'll be applying to geometric sum estimate will be at least this much. If you just use the original differencing scheme, you'll end up applying geometric sum. Scheme, you'll end up applying geometric sum estimate to sums that are like as long as two or three. This is like not going to be getting good, you know, you better use a triangle inequality. So this parameter M helps avoid this inefficiency. Okay, and in a beautiful theorem, Chang Graham proved this explicit version of the vial differencing with this extra parameter. So, yes. Yes, m is arbitrary, yeah. But but the really the really useful range of m is when m is like not too large compared to m right, so you have the square modulus of your exponential sum with an arbitrary phase function subject to some just really mild conditions. And on the right side, you have a bound for the square modulus in terms of the difference sum. Yeah. Yeah, I mean, that's I mean, that's Chang Graham's paper. That's where I this is stated. Yeah, I think they have some like optimizations for the constants that appear. Yeah, it does look like Vander Corpus. Yeah, well, Chang Graham Vander Corporate probably should add that. Okay, so I mean, it's a Vander Corporate method, the whole thing. Right. So, okay, so just some observations about this bound. If you just apply a triangle inequality to the If you just apply a triangle inequality to the right side, then you bound the square modulus by L squared plus m over 3 times 2l minus m. Right? So this, when m is like less than 2l, this is actually worse than the trivial bound. So the vial differencing, or this, which is called the A process, is Is by itself, is the transformed sum that you get is not detecting cancellation if you just apply triangle inequality to it. The point is that even though you're getting a longer sum when you transform it under the vial differencing, the sum you're getting may have a simpler phase function. And a simpler phase function could allow more detection of cancellation. Detection of cancellation. This is exactly what happens in the quadratic case. The phase function you're getting is linear, which is easy to understand. Right, any questions? Okay, so there is another process that also can produce savings. That also can produce savings. This time, if you simply apply the triangle inequality to the transform sum, you might get savings. Right? This is the B process. Here is an explicit version of it. I mean, I don't want to claim names, but I think this is Patel Karatsova Korolev, although Patel maybe adjusted some of the concepts. Some of the constants that appear. The hard work in deriving this is to obtain an explicit bound on the epsilon or the e here, which is the remainder. That's really the hard part. But to qualitatively understand this, you can sort of focus on the left side sum and the right side sum and what is the relation between them. Right. The way you obtain this is by Poisson summation. This is by Poisson summation, and the transform sum depends or the bound you get depends on the specific range of summation, not only on the length of summation, because it involves the size of the derivative of the phase function on the specific range of summation. And here is an example where the phase function is t over 2 pi log of u, right? Log of u, right? This you can, I think it's Montgomery has an example like this. So I just did this computation in like computer software to like see how it's working in practice. So if you compare the left side and the right side, At the left side and the right side, you can see how the right side, which is the transformed sum, viewed as a walk in the complex plane, meaning as you add terms, you're just tracking where you're ending up. So you view both the original sum and the transformed sum as woke in the complex plane. You can see how the transformed sum is like a caricature, a good enough caricature of the left side. These points on the right-hand side are. These points on the right-hand side, I was the point XV that you had in the previous, yeah, right, yeah. So, this is a one term, one this is when you add two terms, when you add three terms, and so on. Um, right, so the shapes, so the starting point in both cases is here, and the shapes sort of track each other. So, this is like a caricature, it's a simpler caricature, but good enough so that by bounding it, you can actually learn about the original. It you can actually learn about the original sum. And importantly, this caricacheur skips these spirals, which is where you're kind of moving around yourself. So where is a lot of cancellation is happening. If you just take the original sum and apply a triangle inequality here, then these spirals will be counting their kind of the number of times you're revolving around them. When you look at the transformed sum and you apply a triangle inequality, you're avoiding this. Right. So this is this kind of Poisson summation or explicit B process takes a sum, gives you a transform sum. There is another version, which I don't think is known as a B process in the literature. It's just some sort of Changraham lemma. But if you look at the proof of it, it's actually the same as the proof of Poisson. Actually, the same as the proof of Poisson summation. Under condition on the second derivative of the phase function, you get this bound. So you're bounding the exponential sum, you know, you're obtaining a final bound on the exponential sum rather than obtaining a transformed sum. Right? And there's a story behind this bound, which is that That this version was used for a long time, but the constants appearing here are a little bit too large. Basically, the mean term has a constant too large or like, sorry, not a bit too large, a bit too small. So there is like square root of two multiplicative factors. Two multiplicative factor differences between this bound, which was widely used for a long time, and the revised bound. This one is not quite correct in the sense it doesn't apply to all, in full generality, to all types of phase functions. Okay, so right, the reason for the kind of the discrepancy here is The discrepancy here is that this B process, which gives you a final bound rather than a transformed sum, is derived using with the aid of something called the Kauzman-Landau lemma. And the Kauzman Landau lemma used to prove that B process was not correct. It had a constant which was off. Which was off. A cosmic landau lemma is basically a lemma to give a bound on exponential sums of this kind of general exponential sum under like very general assumptions, basically some convexity assumption, and that the derivative is bounded away from integers over the range of summation. So the farther away you are from integers, the distance here is encoded by theta. By theta, the better the bound you get. So, this kind of bound is fed into Poisson summation, which gives you this B process that I mentioned before, which had a version that was widely used for a long time, but was incorrect because it used the Husmin-Land dulema where instead of two over pi, it had one over pi. Over pi. Right. And this affected many results in the literature. Yeah. The apparently the p-process is done sort of bounding trivially the sum you get with the original p-process. Yeah, it's um yes, I mean it's not trivia. Yes, I mean, it's not trivia. It's bounding. Yes, it's something like this, basically. Yeah. Right. When you said this data, when you said that we were error in the data? No, not in the independent papers that relied on this bound. They were fine, as far as we know. It's just that the input to them had some constant that was off. So yeah. Yeah, kind of a rip a ripple effect. So, yeah. Okay, so now maybe just try to move on to more general theory. I mean, you don't need to, you know, we don't, I don't quite use the general theory because I'm working with explicit processes that are specific, but it's useful to kind of go through this. Kind of go through this. So, and this is where things get very technical. So, we define a family of phase functions depending on some parameters sigma and c and we have quadruples n, i, f, and y. f is your phase function. Y Why is like in these three examples I gave at the beginning, like for zeta, the phase function was t over two pi log of u. Why would be this t over two pi number? n is will be used to encode basically the length of summation and at the same time the range of summation because you're looking at dyadic intervals or things. Dyadic intervals, or things contained within a dyadic interval. And your family of functions f of sigma c is defined by these three conditions. Importantly, the third condition is that the derivatives of your phase function satisfy this kind of inequality, where the constant c here is at most one-half. One half. So, because it's at most one half, this means that the size of the p plus first derivative is about the size of this derivative of yx to the minus sigma. Okay, so maybe like what's like if one tries to come up with an example of a phase function satisfying this condition three, say with sigma is equal to one. equal to one then the example one would get would be f is equal to say t times log of log of x right the derivative of law when you take the derivative you get t over x so sigma would be uh one or t times x to the minus one when sigma is two this is the example of the dirichlet divisor problem so you get um c of C over x, and when sigma is equal to 3, this is the square-free numbers example. So, this maybe hopefully makes this like a natural definition or like a that gives a reason why consider this kind of condition. Right, then to say that k and l are is an exponent here means. L is an exponent here, means k is between zero and one half, L is between one half and one. And actually, yeah, the condition C is at most one half was not used on the previous slide, but it is used when you define an exponent pair to ensure that you have a good control over the size of the derivatives of your phase function. Size of the derivatives of your phase function. So to be an exponent pair, you have to satisfy these inequalities and this bound on the exponential sum. And this has to be uniform for all quadruples n, i, f, and y. Right. So if you look at property three from the previous slide, this quantity. From the previous slide, this quantity here is, because c is at most one-half, is approximately the size of the derivative of your phase function. So, this bound, we're bounding the exponential sum by some sort of weighted product of the size of the derivative over the summation interval times the length of the summation. I mean, this slide, the I mean, this slide, the only reason I'm kind of using it is to hopefully get more familiar with this very technical definition. For example, the condition, and you can find this in other standard sources, the condition that L is at least one half actually does not have to be mentioned explicitly. If you just look at the remaining parts of the definition, you're forced to take L at least one half. Have and I give an explanation here, and similarly, the condition that k is at least zero follows from the other parts of the definition anyway, but it's helpful to mention this explicitly. So there are like immediate examples of exponent pairs, for example, zero, one. This is just This is just what you get when you apply the triangle inequality. Another one is one half, one half. This is what you get when you use a B process. Right, so now we have two kinds of things that have been defined separately. On the one hand, we have this object called exponent pairs, and on the other hand, we have these processes called A processes. Have these processes called A process and B process. And the point of the method of exponent pairs is that the set of exponent pairs interacts very well with these processes. So, for example, if you use an A process and look at the difference sum that you get, then you can calculate the rough size. The rough size of the derivative, here it is, of the differenced phase function in terms of the original phase function. And so if you just use that, then you can bound the right-hand side here, which when we previously bounded that right-hand side using triangle inequality and bound it was bad, worse than trivial. But now, using the knowledge of an exponent pair, when you bound this, Exponent pair, when you bound this, you get something that is or could be non-trivial, right? And with freedom to choose M and choosing M to minimize the right side is how you get this kind of bound. V here is the rough size of the derivative. Is the rough size of the derivative of your phase function? So before I wrote this, you know, using the original notation, v would have been n or sorry, y times x to the minus sigma. But here we just for simplicity write v. Some people write L. I find L is like you use this for the length of summation often. So I just avoided that and wrote V. So if you look at the exponents that you If you look at the exponents that you get, this shows that if KL is an exponent pair, then this more complicated expression is also an exponent pair. Capital N. Yeah, you have the length of summation here cal. Yeah, but but I assume that a n n plus l, like that, the range of summation is at a dyadic interval between n. dead interval between n and 2n so uh right so l is at most n v is the uh it's the rough size of the derivative over the right so in the original yeah so in the original formulation v would be this quantity right okay Okay, so this assertion here that if KL is an exponent pair, then this new pair is also an exponent pair can be encoded using this notation. We think of A as a process or some sort of transformation applied to the exponent pair KL, which gives us a new pair. Here are examples. If you apply A to 0, 1, you get 0, 1. That's why you never see A as the right most. That's why you never see A as the rightmost letter in A-B words. If you apply it one-half, one-half, you get one-sixth, two-thirds, just following the formula here directly. Just like the A process interacts well with exponent pairs, so does the B process. The calculation is more complicated. Basically, what happens when you look at What happens when you look at the phase function you get when you apply a Poisson summation, which is phi of V, defined this way, then you can calculate the derivative of V, and you can calculate the length of the transformed sum, which is beta minus alpha. You can do all that, you can, first of all, bound the length of beta minus alpha by the size of the derivative of f, because this is how alpha and beta. Because this is how alpha and beta are defined. And you can bound the derivative of phi, the transformed phase function, in terms of basically the original length of the sun. So basically what happens is that the walls of v and n are switched. Here, the length is n, and the rough size of the derivative is v. Is V in the transformed sum the length is instead V the opposite and the rough size of the derivative is n right and so now if you apply if you use the fact if you assume K L is an exponent pair and apply it to the transformed sum you get this kind of bound where the roles of K and L are or the places of K and L are switched. Or the places of KNL are switched. So here are some examples. If you apply B to 0, 1, you get 1 half, 1 half. If you apply B twice to an exponent pair, you get the same thing back. So you never see B repeated twice in an A-B word. Right. Some of the bounds, like on the second slide that I mentioned, are correspond to bounds you would get using some of these combinations of A and B processes. For example, the bound that had t to the 1, 6 log T corresponds to the word AB. The one with the 27 over 64 corresponds to AB. 64 corresponds to a b a cube b more complicated to yeah to bounding zeta there is a limit here in terms of what you can yeah yeah so a rank and i think is proof this right um another basic property of exponent pairs is that they form They form a convex set. This is just directly direct to see just by a little bit of rewriting of the exponential sums and assuming K1 and L1 and K2 and L2 are exponent pairs, then you apply the first exponent pair to S to the T and the second one here. So you get like this kind of combination or linear combination of the exponent pairs. Uh, the exponent pairs. Um, right. Okay, so as much as we can, we'd like to obtain explicit versions of these A-B processes. So here is a partial list. Here is a partial list of AB words where I know explicit inequalities have been obtained. It would be nice to extend this further. So I like, and you can see in some cases, like the same process is obtained in multiple places. You know, different versions offer different advantages. Offer different advantages like simplicity versus a better or a sharper bound and so on. Right. And yeah. Oh, this one? I think, I don't, I don't think you call it a B process. Yeah, it's like this alternative. So I think in It's like this alternative. So, I think in the paper with Granville, you say we like that we don't use a B process, but I guess one kind of point I was trying to make is that this alternative B process, if you look at the proof of that, that's actually just Poisson summation. So it's actually there, but implicit. Yes, so yeah, so there's this Yeah, so there's this table, and it would be nice to extend it further. Definitely, things are getting more complicated. You can see how, except for this one, how B is always the rightmost letter. You don't want to have A as the rightmost letter, because if you apply A to 0, 1, if you want to start with the trivial exponent pair, because if you apply A to 0, 1, you get 0, 1. So you're not doing anything. So, and the fact. So, and the fact that B is the rightmost letter is exactly why this Cosmo-Landau lemma caused a lot of trouble because it's always at the bottom most layer of these processes, because it's kind of how this B process is actually proved. So, if there's something off in the Kausman-Land dilemma, there's something off in this rightmost B process, and therefore there's something off in the whole bound you get. If B does not occur as the rightmost. If B does not occur as the rightmost letter, but somewhere else in the middle or to the left, then the B process you use is the one that gives you a transformed sum rather than a final bound. Right? Okay. And that's it. Thank you. On that last slide, are like some of them more complicated to do than others? Like say the one with the A B A B like if you alternate it, become more complicated or? Yeah, so yeah, it is definitely more complicated to actually prove an explicit version of this. Yeah, it's definitely more complicated. So when you apply this process to starting with the trivial exponent pair, you apply it. Trivial exponent pair, you apply it like you apply B, then apply A cube to the result you get from B, and like this. But when you actually prove an explicit version of this, you go in the opposite direction. You start with A, you know, apply while differencing, then apply Poisson summation to the difference sum, then apply A cube. So you can imagine how this buildup of processes is giving you something more and more complicated. Complicated. You know, this one B is like convexity bound. This one, A, B, is like the van der Korput bound or Weil bound. And this one gives you a sub-Weil bound, right? Something below, like T to some power less than one-sixth. Yes. Parallel to data we can see about the So, these are explicit versions of these processes. So, the constants you get, even though like qualitatively or asymptotically, they behave similarly, the actual constants you get are different. So, yeah, I think maybe I'm not completely sure. I think maybe the yang, the constants in Yang may be a bit more optimized, but this the one here. But this, the one here has a simpler form. So it's really a trade-off. Do you want to do something simpler to, you know, like simpler calculations or do you want to have sharper bounds? There's actually a huge advantage, simplicity. Simplicity makes discovering errors much easier. And you saw that, how like a single issue, you know, caused, you know, affected many other results. Was affected many other results in the literature. So, yeah, the difference between the two versions is the constants that come up. Yes. Good question. One response to what I discussed earlier. For SAITA, is there a specific K there less about here's naturally? After that, there's no sort of conventional currents. A specific K, the number of times you don't have to. I mean, there is a limit to how the bound you can obtain for zeta using just A and B processes starting with the trivial exponent pair. I don't know in reaching this limit whether the order of K is also limited or not. I don't know. Yeah, off the top of my mind, I don't. Yes. Two the first version of the B process is stationary phase and very carefully. Can you say again? Sorry, the first B process is stationary phase. So that's a good idea if you can bound the derivative of the second derivative away from zero. If you can't do that, you should take more derivatives, right? If that's also got the zero. So it might not be relevant to Zeta or the divisor problem, but like do people play this game with? are do people play this game with with higher um higher derivatives for certain phase functions yes i i mean that's uh like like they make beat processes for like yeah i mean you apply vile differencing like when you apply vial differencing the a process a lot at the beginning you're basically looking at higher and higher derivatives because you're differencing and so the difference is basically like a derivative yeah so yeah so is the Yeah, that's good. So it's the idea that you do A processes until you put two derivatives away from having bounds away from zero, and then you do a B process? I don't know if there's a simple rule of thumb, like of how to decide this. Yeah, I don't know. Yeah. This this result. Yeah. And we did the job because there were some just so in the Android result Andrew Yang result. Oh, the petal one. So, so what's the question? Oh, it's just the computations become more and more and more complicated. Like to use a more complicated A-B word is just a more complicated proof. I mean, that's really the limits. And also, what happens? And also, what happens is that once you use more complicated A-B words, you get better exponents for T, but then you're getting worse constants, right? So the small, so at some point, you're getting such a small improvement in the exponent on T at the expense of a much larger constant multiplying in front that the bound you get, even if very good when T is extremely large, is useless for a long initial range of T, which, you know. Of T, which a lot of people care about. Basically, it's useless within the feasible region of computation, which is right now around 10 to the 40 for Zeta.