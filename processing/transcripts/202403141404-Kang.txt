I would say it's an honor to be able to speak here in front of all of you today. The title of this talk is Learning to Understand: Identifying Interactions via the Lobius Transform. Don't expect you to know what that means, but maybe by the end of the talk you will. This is joint work with some fellow PhD students, F.A. and Lambda, as well as Professor Ramtan Pedrasani from UC Santa Barbara and my supervisor, Professor Khanan Ramahan, who sends all his well wishes to all of you and it's unfortunately wasn't able to be Unfortunately, I wasn't able to. Okay, so this is the motivation behind this talk. We have some review, in this case, this is a review of Shannon's Mathematical Theory of Communication, and we pass that through some language model. And this language model is designed to tell us the sentiment of this review, whether it was a good review or a negative review. And obviously, there's something wrong with this language model because there are no negative reviews of this paper, and so. This paper, and so something is going on here, right? And the question is: can we understand what parts of this text is triggering this language model to give it this score, this erroneous score? So, this is a typical solution. The idea is basically to just mask off certain parts of the text, right, and see if the sentiment goes up or down. So, let's say we mask off some of the text and the sentiment goes up. Clearly, that part of the text probably Clearly, that part of the text is probably contributing to the negative sentiment. And depending on how much it goes up, that might tell us something about to what degree that part of the text is contributing to the negative sentiment. And so designing these masks is actually a very interesting problem. And yeah, this talk is actually also secretly about sparse graphs and group testing. And that's all that's really going on here. So yeah, as you'll see, actually, very interestingly, we will design. Very interestingly, we will design these masks using group testing matrices, but in very non-obvious ways. Okay, so this is the standard way that people are doing this right now. They use this package called SHAP. It's from the University of Washington. And it's based on this game-theoretic notion of importance called the Shapley value. And what the Shapley value does is basically assign a score to each word or in the To each word, or in this case, group of words, to save on computational XID. And the score that it gives is related to the average marginal contribution to the overall score that that word or a group of words provides. And so I added this here just to show that this is a very, very popular package. I mean, I think the paper that this is based on is about six years old and has well over 10,000 citations. So, yeah, it's very, very well used. And we were looking at this and we were seeing if there's any way we can. This and we're seeing if there's any way we can improve on this approach. And two things I think we can do: is one is you know, is it possible to make it faster? Can we use fewer overall masking patterns? And two, can we somehow extract higher order information? Because this is kind of like building a first order model. I'll explain exactly what that means in a minute. So this is an example of a higher order model. Higher order model. So if we have here, this is a new review here. Her acting never fails to impress. Clearly, this is a positive review. And this model that we've trained gives it a score of plus 0.91. I think the score is from minus 1 to 1. And this comes from a BERTS model that's been fine-tuned on the IMDB data set. So it knows about movie reviews. And we can see if we look at the first order scores of some of these words. The first order scores of some of these words, words like never and fails, they have profound negative sentiment. If you see the word never, you see the word fails, you think bad. But English is complicated. And when you add the word, have the word never and fails together, that means something profoundly positive. And so this is a really important second-order effect. And I'd also like to point out that I'm not making up that people care about these types of things because since People care about these types of things because since we started publicizing this a few weeks ago, I've gotten quite a few emails from people who are very interested in up to they say about third-order interactions. So what's really going on here? It's really just function decomposition, right? But we have this complicated function, and we're going to model it sort of with a Boolean function. Input bit one corresponds to not masking a word, input bit zero corresponds to not a not a not a not a not a Masking a word, input bit zero corresponds to masking that word. And really, what we're taking is kind of like a Fourier transform. So, just like you have some complicated function and you want to take a Fourier transform and pull out the important frequency components, here we're trying to pull out understandable and important components of the speech. And so I'll talk about exactly how we get these scores in a moment. But the idea is basically a signal process. The idea is basically a signal processing approach to explanations. And we can decompose this Boolean function that I was talking about in terms of a polynomial, just like you would do with a Hadamard transform. So how do we construct this polynomial? We basically take these scores that I have and multiply them by the corresponding bit that matches the word. So in this case, the third word, we add m3 minus 0.44. Minus 0.44, we have fails, it's the fourth word, we have minus 0.96, and we repeat this process over and over again for all the words. Now, for the second order terms, we'd look at the product of the third and fourth word never fails. And this is representing basically an AND of the two bits, right? That would take multiplication. And so, if we're not masking any of the words, what do we do? We replace all the bits with one and we add them up. One and we add them up, and we get 0.91. Right, okay. So, what if we wanted to evaluate this with masking the third word member? Okay, we do that, and we set all the places where the third bit exists, so we set that to zero, and then we add up, and we get 0.93. Okay, this makes sense, right? Because her acting fails to impress the reviewer is not saying something good about her acting. So. Acting. So everything is consistent. And so this type of transform that I just took, it's called a Mobius transform. I think that just comes from the fact that Mobius did something in number theory. And this guy, Gian Carlo Rota, said, oh, hey, this is actually the same thing for Boolean functions. But the main big picture here is that we're doing a transformation onto an AND basis. So all of the possible AND functions between all the bits of the inputs here are. The inputs here are sort of our basis vectors, and similar to how we would do with a Fourier transform with parity functions, we're doing this kind of projection. And we're going to use capital F to have the corresponding Mobius coefficients, and little f is going to represent the function itself. So I just want to do a big little compare and contrast between the Hadamard Fourier transform and Fourier transform and this Mobius transform because I know many of you are probably familiar with the Hadamard transform. So, in both cases, they're a polynomial representation of the function. For the Hadamard transform, the bits are minus 1, 1. And for the Mobius transform, they're 0, 1. So, this is what gives you the difference between a XOR basis and a AND basis. The Fourier transform has this beautiful property of being unitary, while the Mobius transform does not. And the thing with this Fourier transform. And the thing with this Fourier transform, at least in the context that I'm talking about here, it's really difficult to interpret these coefficients because we're looking at XORs of the bits. And so it doesn't work in the same way that I just explained to you with what we did. But for these AND functions, it's very easy to interpret. And of course, there's a long history of literature on how to efficiently compute the Fourier transform, but that doesn't exist for the Mobius transform. Transform. And so I'm here to solve that, basically, and show that yes, actually, you also can efficiently compute this Mohridge transform. So just a quick history on the fast Fourier transform. So apparently it goes all the way back to Gauss, who used it but didn't think it was important enough to publish. And then in the 1930s, someone named Frank Yates developed the fast Hadamard transform, which is the fast. The fast Hadamard transform, which is the fast Boolean Fourier transform. And then in the mid-1960s, Cooley and Tukey finally rediscovered the FFT, which is a very important discovery, as we all know. And then in modern history, or more modern era, since the development of compressed sensing theory, many people have worked on sparse Fourier transforms. So we have some work out of MIT from Peter Indig's group, as well as Klanan, who worked on this back then as well. Who worked on this back then as well, and many, many others who've done some great work in this field. So, I want to talk a little bit about what our signal model is, right? What kind of properties do we expect of these functions? And one of the things that we observe is that almost universally, there's only a small number of coefficients that are really large. Of course, we're talking about an exponential space. There's two to the n coefficients, and among those exponential. And among those exponential number of coefficients, really, only a small fraction of them tend to be very large. And you can't see very much here because there's only six words. But if you have a much longer sentence, it tends to be the case that only these low-order interactions are the ones that are on the screen. Obviously, there's a lot of small, negligible terms here, but we're really interested from an explainability perspective. Really interested from an explainability perspective in just the largest ones. And I'll also point out that I don't want to knock the Fourier transform, the Heidemar transform here. It actually tends to, in many cases, has even better spectral properties. And we can actually take the Heidemar transform and go back to the Mobius transform if we need to. But for our purposes, because we're interested in this explainability, we want to focus on this Mobius transform. So using the signal. So, using the signal model, we can do some develop some theorems. Everyone loves theorems. So, the first theorem here is just a standard compressed sensing setting. So, we assume we have a K-sparse Mobius transform, exactly sparse, and we can achieve this with sample complexity Kn. Again, K is the sparsity, N is the number of words. By sample complexity, I mean the number of times we need to mask and do inference. So we want that to be as small as possible. So we want that to be as small as possible. And the computational complexity here is kn squared. And this is kind of like an asymptotic result, but you can say something non-asymptotic between two. But yeah, this works well for large K and N. And then the second theorem here, we focus on this low degree setting. So we say we're only interested in these words where, or in these interactions between at most t words. In that case, the sample complexity can be reduced to kt log n. To kt log n, and the time complexity is k poly n. In practice, I'm writing O here a lot. For a sample complexity, the largest constant is really just like six. And this poly is basically cubed. What's going on here is this poly is coming from a group testing decoding part of our algorithm. And I'll also say that in this setting we focus quite a bit on robustness because that's obviously very important. Because that's obviously very important. These things are not exactly sparse, but we really just want to pick off those very important large interactions. And so at the very end, we'll talk a little bit about how the sparse works, or how the robustness works. So just throwing some numbers around here, you know, this exponential space, if we have k equals 1,000 interactions and 100 words, and we're focused on t equals 4, that we're talking about, you know, naively, this is 10 to the 30. I forget. 10 to the 30, I forget about it. And with theorem one, we're looking at kn is 10 to the 5, kt log n times 6 times 10 to the 4. So this is in the ballpark of possible things to do, because this is the number of times we have to mask. Okay, so now I'm going to talk a little bit about the algorithm. So the first part of this algorithm is going to involve sub-sampling. Of course, as I said, there's an enormous number of possible coefficients here, so we're going to have to sub-sample. Here, so we're going to have to sub-sample very aggressively. And this guy tells us a lot about sub-sampling, tells us that sub-sampling causes aliasing, and there is nothing we can do about that. So, what are we going to do? We're going to embrace and understand the aliasing. So, by the way, these blue boxes are things I don't expect you to be able to read. These are, this is a first lemma. What we do here is we construct a matrix H and we choose. matrix H and we choose our masks according to that matrix H. Exactly how we do that is not important and is very convoluted but what it results in is it results in an understandable aliasing pattern. Basically what's happening here is we're hashing all of these coefficients together according to this matrix H. Now one thing I'd like to point out is that this arithmetic that's happening here is actually like This arithmetic that's happening here is actually like monoid arithmetic. So instead of like F2, where you have XOR addition, this is like OR addition that's going on here. And so this is exactly what we see with group testing. Interestingly, if you're dealing with the Hadamard transform, this is exactly F2. And so that's where you can use error-Karton codes, the parity check matrix. So error-correcting code turns out to be a good hash function. But what's a good hash function here? A good hash function here. So, this is a quick example. We go back to that example I had earlier. Let's say we have these four coefficients. These were the four biggest coefficients. And so I'm saying we have, let's say we have an exactly four-sparse Bobius transform that we're trying to uncover. And we construct two sub-sampled functions. Basically, what this is is we do masking and we sort of stack up the scores that we get from those masking patterns. The patterns themselves are given by this. Patterns. The patterns themselves are given by this ones and zeros score here. And that results in some caching once we take this Mobius transform of the subsample function. And so you see this one didn't do too badly. We got two of them that ended up being isolated, which is important. They're singletons. That's ultimately what we want. Yeah, as you'll see at the very end, this is just going to become Aloha again. So I'll come back to massive random access in the end. And the second thing that's going on here. And the second thing that's going on here, in this case, we did kind of a bad job hashing. They all got hashed together. And so the question is: how do we ensure good hashing? And as I've alluded to before, the answer turns out to be group testing. So group testing was originally proposed by Dorfman in the 1940s as an efficient way to test soldiers for syphilis. The main idea is to do pool testing, which allows you to identify. You need to identify the infected individuals with fewer tests. So, this is a setting where we have eight individuals and two of them are infected, and we only have to do six tests to find them all. And so, just like group testing allows you to find T infected individuals out of N total individuals, it also allows you to find T important words out of N total words. And yeah, so because this arithmetic of group testing is the same. Of group testing is the same as the arithmetic of this hashing rule, it turns out that a good group testing matrix is a good hashing matrix. Okay, so the question of how do we ensure good hashing had the answer of exploiting group testing. There's still more to do here. The question is, how do we identify these singletons and multi-tens? And how do we find the Ki? So, what do I mean by that? So, even if we do a good job hashing, So even if we do a good job hashing, we still need to figure out that, okay, there is actually only one coefficient in there. And we need to know what the k is that corresponds to so we can cover the score. And the answer to that, thankfully, turns out to be that we can exploit group testing again. I'm not going to get into the details here, but a somewhat more complicated lemma. We can also design masks using a different group testing matrix. Group testing matrix. And what this will allow us to do is basically get linear projections of the particular k that we're interested in. By linear, I mean a single group testing test, basically. And so if we have a uniform prior, that is, we don't know that these non-zero interactions are all low degree, this takes, we basically have to just use an individual testing procedure. And this takes n samples, and this is a multiplicative factor of n, but if we're doing n, but if we're dealing with this t-degree setting, we can do this in order t log n. And the last part is: okay, now we've done hopefully a decent job hashing. We still need to kind of clean this up, right? And so we can do erasure decode. Okay, so we have our single tents here, which we're able to recover and then subtract. And then subtract, and then now never, you know, we can also recover and subtract, and recover and subtract, and then, of course, we are able to recover all of these coefficients in the end, which was our goal. So just a quick overview of everything that went on here. In the first step, we did hashing. In this case, we took ordered K samples using a sampling pattern according to a group testing matrix. And in the second stage, we did this singleton idea. And in the second stage, we did this singleton identification and detection. It took n samples, this is a multiplicative factor of n samples under the uniform prior and t log n under this t degree assumption. And then finally, for the density evolution to work out for the erasure decoding, we have to repeat this process order one times. Under our assumptions, it was three times. And this is what gives us our sample complexity of Kn and Kt plug n. Are KT plug in respectively this two set. Hey, and this is just some simulation. So these results, I said, were asymptotic, but it works well, very well in these finite regimes as well. Both, this is a phase transition diagram saying we're just matching up with theory, and we also have a fairly reasonable computation complexity as we predict. Now, I'll talk a little bit about robustness. A little bit about robustness. So, the way we model robustness is that we say basically that each of these bins is corrupted from many insignificant interactions adding up, and they're also being hashed into these bins. Hopefully, somewhat even fall. And so, we're modeling this. Here I said Gaussian noise. I think like any zero-mean sub-Gaussian two-sided noise is fine, but we're able to solve this simply by We're able to solve this simply by taking additional redundant group tests. And there's some bits of awesome work recently on noisy group testing theory, and we're able to port these results into our theoretical guarantees as well. And so here I've plotted some of our robustness. I'm plotting the error here as a function of the SNR. And this is a fixed redundancy. So for smaller t, this is essentially more redundant because. More redundant because T, our sample complexity is growing linearly in T, and you'll see that for the more redundant cases, we're able to more efficiently, the error goes down faster, basically. And yeah, there's still some work to do here, obviously, and continuing to improve robustness, but this is a reasonably robust approach. Now, there's one thing, one limitation I wanted to talk about for this particular algorithm, and that is that in our assumption here, we said. In our assumption here, we said that these non-zero interactions are chosen uniformly across the low-degree interactions. But in real, particularly in language, this is not the case. If you have like an important third or fourth order interaction, usually within those four words, there'll be a lot of very important interactions as well. So that's one case where this kind of breaks down in the real world. However, we found that by using adaptive approaches, and you can design adaptive algorithms. You can design adaptive algorithms using non-adaptive group testing matrices or adaptive group testing matrices, or adaptive group testing procedures. We can basically eliminate this assumption and it works much better and it's much more robust. But there's still this question of can we eliminate these assumptions while remaining non-adaptive? Because we're doing inference here, and so what we kind of want to do is batch all this up on a GPU. And so, you know, adaptive is fine, but if Adaptive is fine, but if we can do this in reasonably large batches, that'd be very comfortable. So, just to wrap up, I wanted to talk quickly about some interesting applications. So, I talked mostly about language here, but there's many other places where we can do this, and one of them is in images. So, there's been some awesome work out of Berkeley from YeeMa's group on creating sparse speech risers using transformers for. Using transformers for images. And so I'm really excited to try to apply what I've done here on top of what they've done, because they've done a great job, basically given us exactly the type of features that we need to get these sparse functions for explainability. And I'll also briefly mention that this approach is somehow secretly related to hypergraph sketching. And so just direct application of an algorithm like this can. Like this is state of the art for some types of graph learning problems. And then finally, another area of application that we're very interested in is auction theory. So the Mobius transform in particular is, you know, it's based on these ands, right? And when you think of value functions that people are interested in, you know, in particular spectrum auctions, it might be the case that we need two frequency bands that are very close together. And there's a synergy. That are very close together. And there's a synergistic and behavior to that. And so that's another thing we're thinking about. Can we use signal processing to help scale up auctions and to execute them more efficiently? Finally, just to wrap up, so explaining deep models can be cast as a function decomposition problem, and signal processing and communication ideas can really provide a new perspective. There's lots of open problems, so we talked mostly about the black. So, we talked mostly about black box methods, but there's also lots of things you can do if you have access to attention matrices or just activations and things like that. And then there's also this question of this connection between the attention and self-attention and Mobius transform, in particular second-order Mobius coefficients and self-attention. There's some awesome work out of Christopher Ray's group at Stanford that's kind of trying to replace attention with FFT convolutions. With FFT convolutions. And lastly, there's continuing to improve this, the robustness of this approach with more real-world noise models that I was talking about earlier. With that, I'll wrap up. Thank you.