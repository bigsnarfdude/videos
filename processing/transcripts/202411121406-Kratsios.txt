Today, well anyway, it's almost this slide's almost redundant, but let me just briefly say, uh so FPSDs arise all over the place, and you guys also know this. So in control problems, games, and therefore in finance and economics. This everyone knows. Here's another thing that many of us know, like FNFTs, besides like the theory is pretty well known, but in general they're rarely available in closed form. And okay, and otherwise computing them numerically is extremely challenging. Computing them numerically is extremely challenging. So, this is also the current problem with FSTs. And so, we're going to think about kind of questions along those lines. Okay, so let me just start off by saying one comment, so to make sure we're all on the same page. So, there's a bunch of numerical methods for FBSD. Some of them are like deep DSTD solvers, and I just want to quickly explain what those are essentially doing. How this is fundamentally different from what we'll be talking about. So, in those cases, what you do. So, in those cases, what you do is you specify your FPSD, your terminal condition, your generator, whatever, and then you solve that individual FPSD. So, you give one FPSD and you solve that thing. So, these papers do that. But now let's consider a problem where, suppose for one reason or another, you want to solve several. For example, say you have some uncertainty about what model makes sense, you want to consider some ensemble, or you just want to have fun, but specifically the uncertainty one is the important one. One is the important one. So, if you have to solve several FPSTs, imagine perturbations of one single one, then what I'll say is that basically all these methods are effectively impossible, like completion feasible. So, what I mean, for example, taking FPSC, change the terminal condition by bit, perturb the dynamics. That's what I'm thinking. Okay, so one more comment. So, each of these things were very slow, but suppose any of those numerical solvers were fast. The problem would still persist. The problem would still persist. The reason is because you have to rerun them several times. So let me briefly explain, yeah, show you a lower bound that comes up and to explain how big the invisibility problem is. So consider the following family of FPSCs. And just for temporarily, just to motivate everything, I'm going to consider the simplified case where I'm, okay, so for every g, g is a continuous like C1 function with C1 norm value by 1. I have an infinite family, which I, for example, just Family, which I, for example, just here, it's just a simple example for motivation. Each g just gives me a terminal condition. So this family is essentially indexed by C1 functions for motivation. So, okay, so of course you cannot solve them all using a classical numerical solver because it's an infinite number, so it's impossible. But let's do the next best thing. And so, what's the next best thing? Okay, so this G, this family is infinite, so let's consider a delta net. So, this is like an optimal finite An optimal finite number that well approximates every single element of that family. So the GZ parameters has internal conditions, right? So what I mean by this, you have some capital N, some number of functions, GN, which are delta close to every other C1 function, and therefore in some sense are giving you like a finite reduction of this system. Okay, and so in that case, what would you do? You'd want to solve every single FBSD. So this is just a BSD, so it's no F yet. So there's no F yet. That's, sorry, there's an F in my butt. Don't know. Yeah, that's sort of indexed for one of these genes, right? So really, the question is: how many times you have to rerun your solver? Well, and the lower ground is obviously given by this number. So the question is a simple counting problem, or like a not so simple counting problem. Question being, how many functions do you need to get a delta net out of this C1 ball neighborhood? Okay, and this is solved by, like, I guess, shortly. I guess shortly after Komogoro or something, like a billion years ago. Yeah, so you get a lower bound that looks like this, and trust me, this is quite big, right? So how many times would you have to rerun it out? Like a solver, even if the solver took a second, you'd have to rerun it this many times. So you notice it's a double exponential in dimension, that's quite bad, and exponential in the reciprocal size, like fineness of the delta net. So in other words, this isn't possible, even if you had the fastest. This isn't possible, even if you have the fastest numerical solver in the world. It just goes, you have to run it right. So, what's the point? The point of the slide is that any classical approach that needs to be, that operates on one BSD at a time, cannot be impossible at this point. Okay? Okay, so yeah, no way. Okay, so then please please. Do you want to do it for oncology or just technology? I want to do it for all the technology, you are fluent information. Oh, no, no, no, so every G gives me a new, I guess, yeah, will give me another perturbed G. Yeah, exactly. And I want to solve each of these FQSDs. I think if you are just doing it for a particular G, then you can always retrain start from the trainable parameters obtained from the G, and that will be very close. Yeah, but then you still have to run the thing this many times. So even if you have like a second. Yeah, but yeah, we'll see. And also, I'm not sure if that's necessarily the case. I'm not sure if that's necessarily the case, but I'll think about it. Okay, so yes, so just to summarize the problem, okay, so what are the inputs and outputs? Just like, let's be computer scientists for a second. So what is going to be the inputs to our problem that I'm thinking about? Well, you're going to provide a terminal condition and dynamics to the backwards process. The outputs is going to be a solution to the FPSD, that's what I'd like to get. So I want a function or some operator that takes those things as inputs and gives me those things as outputs. And it gives me those instant outputs. And the key point is essentially, like kind of on the theme of what you said, I want no retraining. So I train this thing once, and then I can just input those inputs, those terminologies backwards, and I automatically get the solution. Okay, so I'm going to consider this funny setting. There's other papers that are related, they'll come out, I guess, this week. One, that considers more general settings, but the setting has a very nice thing, which I'll highlight when we get there. Which I'll highlight when we get there. So here we go. So the fourth process is going to be this kind of forward running motion. Gamma is a positive, symmetric positive definite matrix. We're working in dimension 3 and above for some technical condition. Okay, I'm going to consider random terminal times, tau. So tau happens when the forward process leaves a domain B in D dimensions, and this domain is bounded with C1 boundary. Okay? Okay, so there are random terminal times. And here are the forward processes I consider. So I'm going to have some sort of reference. So, I'm going to have some sort of reference dynamics, alpha. Notice it depends only on X and Y, not on the Marin Gilder. Okay, and I'm going to supply two perturbations. So, the perturbations are specifications of terminal condition in this paper and perturbations of how the generator depends on the forward process. So, you can think, cool, think of this as boundary data and source data. Okay, so now this gives me again an infinite family of FPSDs, and again, they're indexed by essentially what I'm doing to the term. Indexed by essentially what I'm doing to the terminal condition, and how I'm monitoring the perturbations. Okay, so, yep, any questions about it? No? Very cool. Okay, so at a high level, of course, what we want to do is we essentially are interested in the following solution operator. So solution operator takes some inputs, some g, so terminal condition, and f0, so perturbations to my generator. These live in some sufficiently regular sub-look spaces, and they give me back the solution pair. And they give me back a solution pair y and inside. And again, I'm just going to emphasize: yeah, when I write g and f0, this is just saying it's referring to a specific FPS thing. Okay, cool. So that's the solution operator I'd like to approximate, not better. And okay, let me just stop in one second. So one question that might have came up is, okay, Nastasi, like you have some very simple dynamic restrictions. Why is that? Well, I'll answer that. Well, I'll answer that by just making a comment about where this talk is not going to go. So, yeah, so you can imagine this can be done using some interdimensional deep learning models. And there's a bunch of universal approximation theorems out there for such objects. Many of these results are from the PDE literature. Some are from Mark and Milly. But in the last few years, there's a diploma of such results. And you could, in principle, apply a result to check that this thing is continuous under some very mild conditions. Continuous under some very mild conditions for more general dynamics, and say, okay, by universal approximation theorem, you can approximate this. So there is a neural network out there. Okay? But let me draw you the picture of why that's a bad idea. So this is my visualization of this. Okay, so what does the universal approximation theorem say? Well, it says the following thing. I'm always moving shares around. Okay, so imagine this is my class. Let's just keep it at a high level for a second. This is my class of functions, say, from some input space x. Functions, say for some input space X, just some output space Y. Okay, so universal approximation theorem, say for a deep learning model that inputs X and maps outputs Y, says that for every single function, that's any mapping between these two guys, you can always find a normal error-type model, some deep learning type model that can approximate that. Number one, that's a quantitative statement. Quantitative statement will say that every single will say something more, it'll say that it'll give you a bound of the number of parameters you need. How much depth, how much width, how much other things. Depth, how much width, how much other things, right? But what does that bound actually mean? So, if you have an optimal approximation theorem, meaning that bound is sharp, it means that there exists a highly pathological function in your class that takes inputs to outputs for which these bounds are optimal. They can have to be proved. So let's visualize what that looks like in one dimension, and I'll show you why this is not a good approach. Okay? So, suppose I want to approximate any ellipsoids a function, taking the real line to itself. What is the function that realizes? What is the function that realizes these bounds? Like that you have a neural network that approximates it with a given depth and width, and you cannot improve that depth and width. Well, these things look like this. So this is the worst case of just functions in one dimension. That takes inputs R, equals L plus R. You can build it recursively, create the spike, create a little spike between the spikes, keep going, keep going. It's a fractal, and you can just do a basic counting argument and use it like on a number of linear pieces showing that this is the worst case function. Is the worst case function that is guaranteed to be approximable and a universal approximation theorem? Now, I don't know about you, but most things we look like we play with in real life are not fractals. They're not extremely odd objects. And so what's the point? The point is, if you did apply a universal approximation theorem, you can approximate that. The size of the network would be as big as the size of the network that would have to be able to learn an infinite-dimensional time factor. So that's not what we want to do. And our key point is we want to get good rates. So that's why I considered restricted analogs. Restricted dynamics. Okay, so let me write down the theorem. So I'll try to show you where we're going. So I'll give the statements. I'll benchmark you against what would be doable if we didn't throw into via universality. And then as the rest of the talk unwinds, I'll explain to you how the model works. So start of queuing the model first and theorem last. Okay, so a neural operator is just like a fancy word for a deep learning model between infinite dimensional spaces. In this case, these sub-load spaces. In this case, these sub-load spaces and these spaces adaptive processes, okay? Or I want. Okay, so my neural operator, some gamma hats, keep note of the decorations on the gammas. I was going to take, yeah, so essentially boundary information, so terminal condition information, source information, perturbation to dynamics. So these are my sublist spaces. And it does the following thing. Okay, what it says is that for every epsilon bigger than zero, every approximation error, basically. Me, right, energy, terminal, time horizon t. I can find a gamma hat that does the following thing. It approximates by, so who was gamma star? That was the solution operator, we saw before. It can approximate this to arbitrary precision over most of the time interval, then epsilon. Okay, fine. So this still looks like something you get from universality, but here's the key point. The key point is that so the depth of my neural operator, so how many layers you need, and anyways I'll come back to explain this towards the end. I'll come back to explain this towards the end. Is logarithmic in the reciprocal approximation error? That's small. Yeah, the width is O1, okay. And the rank is linear in the reciprocal approximation error. Actually, in the paper, so I wrote these slides right before, you know, put this on the archive. We can now make this highly sublinear. So it gets even smaller than that. Okay, that's pretty small. So how many total parameters do you need? Well, okay, if you did some counting, you can just. Okay, if you did some counting, you can just check that this is quadratic and there's a reciprocal approximation here. Okay, so those are some numbers, but let's benchmark that so we understand how fast that is, right? Okay, so okay, so as a benchmark, my friend and collaborator, Samuel Landaler at Caltech, has recently shown a lower bound for the following more general rule. So we want to understand how good this is by understanding the lower bound for the general case. So if x and y are an. For the general case. So if x and y are nice interdimensional binary spaces, and say you want to learn a Lipschitz operator, non-linear, between these spaces, then you have a lower bound that looks kind of like your time complexity down. So this is significantly better than that. Okay, this is still, so it's exponential in reciprocal approximation. Okay? So again, to further prove my point, like substantiate my point, let's also now contrast this with the lower bound infinite dimensions to see like how fast this convergence rates are. Like, how fast this convergence rates are. Okay, so imagine we're in finite dimensions for a second, forget about all this FPST business. So, imagine you want to learn the Lipschitz function from Rd to LR. In D dimensions, how much, in the worst case, which is optimal, how many parameters would you need for your LR? So, it would be polynomial that reciprocal approximation error, and the polynomial degree is, well, dimension. So, here the theorem is essentially saying that the number of parameters I need, which is one over epsilon squared, is coincides with the worst case complexity of running a function in two dimensions. Of running a function in two dimensions. And we're in infinite dimensions, so this is pretty good. So, how is this even possible? Seems like almost too good to be true. So, to understand why that's the case, just imagine my disgusting picture, and we'll basically argue that we're very far from this picture. So, the key idea is how does gamma work? And to understand that, we'll go back to some older results from like FPSD theory. So, in our case, what do we have? So, we have, so, what is the solution to So, what is the solution to an FPST like in our setting? I'm just dropping the superscripts on F and G, F0 and G. So, okay, in our case, there's results by Paul Du in the late 90s saying that the y is just some function of the Ford process, and Z is some function of the gradient of, so it's the gradient of this function of the Ford process, where U solves this elliptic boundary boundary problem. So, the idea is going to be So, the idea is going to be to take advantage of the structure here that we have to somehow encode that in our approximate solution operator. So, let me explain. So, what we're going to do is break the problem into two pieces. Piece number one is we're going to come up with a solution operator. So, an approximation of the following solution operators, as I said, keep track of the decorations. Gamma plus is a solution operator to this problem, where you give me boundary data, g, and you give me source condition, or source. And you give me source condition F0. This is, yeah, it was in correspondence with the terminal condition and perturbations of the dynamics. So, gamma plus, what it does, takes g and f0, and it gives me a solution to this pd. So, it gives me the u. And so this first step, we're going to approximate the solution operator using some infinitesimal deep learning model that can do it efficiently in a lazop. And second step, we're just going to plug in our approximate solution, our approximation of u. So, approximate operator. U, so approximate output of gamma hat. I'm going to plug x into it. So, in other words, what does our pipeline look like? You give me f and g pair, I give you a u hat, u hat's an approximation of this. I compute, I evaluate its gradient, I pass that to the next layer, next layer just simply plugs in the forward process, x, and that's the whole pipeline, so that gamma hat that we've seen before looks like that. Okay, cool. Um, okay, so the kind of elephant in the room question is: well, why are these efficient uh Is well, why are these efficient, these extremely fast rates possible? Okay, I'm a bit low on time. Cool. So, why is it possible? It's actually quite simple to explain. So, the idea is, okay, how do you solve that? Well, there's different ways, but or how do you construct a solution? We can construct to be a fixed point iteration, okay? And a fixed per iteration of what operator? This following guy. So, since a long time, how much time do I have left? Ten, yeah, got it. Okay, yeah, okay, cool. I saw it was a five-minute shift. I sort of was a five-minute shift. Okay, okay, so I don't have to achieve my potential max speed, which is not good. Please do, such a thing. Yeah, yeah, the talk is over, by the way. Okay, so yeah, so it's fixed-point iterations of the following operator, right? So let me just write everything on the board just to show who it is. Okay, so alpha is given, it's part of the problem. So every time you give me an F0 and a G, I get one of these operators and I just iterate this thing. Iterate this thing, and if I iterate it infinitely many times, I will have the solution to my problem. So, who is this? WG, it's just this simple, like, yeah, it just solves this PD. But the main point, the main thing is, that's not a problem to approximate somehow. The main thing that's going to be challenging is who is this g gamma? The g gamma is going to be the green function that's associated to my problem. Now, okay, so in principle, what I'd like to do is I'd like to, so we all know how deep learning models work. There's many. So, we all know how deep learning models work. There's many talks that mentioned it today. We have several iterations of these layers. And for us, what are the layers going to do? So, you're going to have like one or two layers that are essentially just going to approximate this, approximately implement this integral operator. And then imagine that's like one layer, and so what is the depth doing in our model? The depth is essentially doing fixed point iteration. So, a function comes in, and at the end comes our approximate solution. So, the idea is that if you get a really good approximation of this, then with sufficiently many layers, but not too many, because it's Layers, but not too many because it's a matter of fact-point theorem, then you have approximate convergence exponentially fast. So, what's the challenge? The challenge actually is coming from this Green function. Why is that? It's because the Green function in this case, there's some results from a long time ago and very conveniently from this year. I love when that happens. It can be decomposed into two parts. A smooth part, smooth things are easy to approximate. It's probably not too surprising. But singular things are really hard, things with a spike. In fact, you probably cannot. Things with a spike, right? In fact, you probably cannot approximate some of those things. So it seemed actually, when we got to this point of the problem, it seemed like it was unsolvable because it seemed like this integral operator was going to be impulsive to approximate because it's solo logo regularity due to the singular point. But actually, the really cool thing is now, so instead of just taking some arbitrary neural network model, shoving it into our problem, and hoping it works, we're going to encode the structure of the problem into our neural network. And so the main obstruction is the singular part of the green function for our problem, but likely for us, it's given in a closed form. So we're going to In a closed form. Okay, so what we're going to do is we're going to literally make this takes one line of code. You know, you have your covariance matrix, you have a norm squared, it's super easy, and that constant is also given in a closed form. But even that wouldn't matter too much. So we'll just par code this as part of our normal number of layers. And once you do that, what we'll end up getting is that, okay, like I just said before, we'll be able to really efficiently approximate one of these integral operators by hat always needs a bunch of things. Hat always means a normal thing by one of our neural operators, which has this encoded into one of its layers, and again by link fixed weight iteration with our approximate integrated operator, we'll get this solution. Okay, so now at the very end of the talk, I can show you what the model looks like. Because yeah, we saw the problem, we saw what we wanted to get, we saw why it works. So what is just formally the pipeline? So sorry, this is the type of this should be G and F zero. Okay, so G and F zero come in, or I wrote the V0 for signal as Y. Come in, or I wrote the V0 for signals Y. And what you do is you make the following iterations. Okay, these are like the T hat canvas, the approximations of our integral operator. So what are those going to look like in their general form, for unspecified weights? Well, every single layer, you take an input function or a function pair, I guess. You do a matrix multiplication. This is just an actual matrix. It's not some linear operator. Here, you do a specific linear operator that's pretty simple. I'll show you in the next slide. So you do a sort of convolution. The convolution, so that this thing is. Convolution, the convolution. So this thing essentially you can think of as almost implementing the smooth part of the Green's function or the integral against that. And this is going to be like the hard-recorded part that implements the convolution against the singular part of the Green function. And it's just again standard real errors, like a bias infinite dimensions. You go to the last line responsive recorded. Here we do this again and again for the number of layers. And finally, at the last layer, you do the same thing, but you have to dimension, where in the intermediate layers, you have some component-wise map on linearity. Component y-sman on linearity. For us, it's the square root of the vertex composite. Okay, that's the model. Looks pretty familiar, except the key part is this blue part. So let me just write that down. So what does that look like? The blue part is exactly what I said. It's the hard-coded singular part of the Greens function convolved. And the general part, it's all it is. It's just sort of this finax rank sort of linear operator. What does it do? It takes a function, it computes some dual pairings against some multiple bases. So for us, this was a complex piece of work. Basis for us is those are complexly supported. The Vershari wavelets for this sublime space. It's a bit more than that, but whatever. And yeah, it just essentially second infinite dimensional, like it's a fine-dimensional matrix. Okay, so that's what the model does, quite simple. So let me summarize, right, in the last couple minutes. So we're able to learn the solution operative to some family FPSDs, infinite one. We avoided terrible rates by sort of encoding the PDE structure directly. The PD structure directly into the construction of our neural operator for mild modification. Okay, intuitively, what did I show? The depth is kind of the iterates of this, yeah, this contraction map that you get when you shoot a gamma there when you're solving construction solution to BD. Okay, and okay, the conclusion was, yeah, okay, sorry, I repeat myself, sorry about that. Yeah, the conclusion was we were able to solve infinitely many FPSCs simultaneously, which was previously impossible, just by basic counting arguments. Possible just by basic counting arguments. Yeah, we only needed quadratically many parameters up to log terms in terms of the reciprocal approximation error. This equals the complexity of approximating something in two dimensions, basically. It's quite good. Okay, and unlike classical FPST solvers, we don't have to rerun them every single time. We just run them once. Yeah. And we're good. And then finally, unlike general, like worst-case neural, like universal approximation guarantees for neural operators, we don't need this big theta like. Need this big theta exponential number of parameters in the reciprocal approximation equality, recipes. I just want to re-emphasize the massive gain. Okay, and that's about it. Any questions from the audience? Go ahead. Can we go to the invoice for post mention? Your viewers. So F0G0 is fixed, or you will take a supermarket. Oh, yeah, yeah, so yeah, so I wrote it as a for all. But yeah, so for all these, actually, so there are more technical statements. I just wanted to keep it nice with the slides. It's in a sufficiently small ball in this product, this sub-look spaces. So for every F and G of F zero, yeah, yeah, yeah. Yeah, but then I was worried the slide would be like expansive. But D is like spanning or like so and any domain T. Any domain T? Yeah, it's any C1 domain. Yeah, bounded. Bounded. Sorry. Oh, yeah, yeah, yeah. Yeah, yeah, yeah. I have an unbounded case. I'm not sure how to do it yet. Yeah. Wait, wait, but is that the question? If this is the whole space, you can cheat the fix. But if it's bounded to main, so you may need some stopping time, right? Yeah, yeah, so I had the stopping time encoded into the problem. This is coming, this is giving my boundary condition. Okay, I don't know. Okay, I don't know. So, one of those buttons. Huh? It wasn't the previous button. I'm gonna. Thanks. Thank you, thank you. Yes, exactly. So, yes, this is key. Actually, this is the key thing for the way I try to solve it. But, yeah, try to get that relaxed. So, if I understand your method correctly, you are solving the PD, which is associated with the BSD. Yeah, solution approach to the PD. Yeah. But then, if you have the forward component, then this becomes. Component, then this becomes also time-dependent. But I didn't understand whether time or yes. Yeah, it's kind of cool. Okay, so right now I'm having trouble with parabolic problems for some technical reason. The solution operators. So I get rid of the time. Actually, the reason I told I chose this boundary version, because I wanted to somehow get rid of time, making an elliptic problem, like, which is somehow just terminates when I have the boundary. Yeah, so. When I have the data. Yeah, so that's actually the case. So I answered both three questions together. Yeah, the time components, for some technical reason, having approximate, almost approximate the solution operator right now. And so I bounded it on purpose for this reason. Implementing this singular operator into a background, really, you can implement the computer. Does this convolution involve discretizing a singular operator? And when you do the background propagation, you really. And when you do the background propagation to really train it, does this uh create a very large uh uh creative error in the in the numerical DC degree? It's a good question. Uh so I have not uh implemented, oh it's not me, like the people who usually help me implement my friends have not implemented this one. So I don't know, essentially what you said, like the singular part, I don't know how that works, but we have implemented, so this is like nearest possible answer to your question. We've implemented like the Yeah, we even completed the operator without the convolution for some other elliptic problems and Hubble problems, and it was quite well in JCPS. Yes, exactly. So I don't know how that one will work, but you have ideas and curious. One more time, and then we'll switch to the next talk. 