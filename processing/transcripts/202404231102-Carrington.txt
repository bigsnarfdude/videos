the time independent Schurdy equation, but I'll also talk about methods for solving the time dependent Shuri equation. So the workshop is about differential equations of quantum mechanics. These two are for me the most important differential equations of quantum mechanics. Of course they only make sense if you know what H is. H is the Hamiltonian operator. For me it's always composed of two pieces, a kinetic energy operator and the potential. And both of those And both of those depend on coordinates that I'll call x. x is a set of coordinates. There are big d of them. Most mathematicians seem to use little d dimension. I'm using big d. And I'll talk here only about the vibrational problem, or the shape of the molecular or reacting system, and not its orientation in space. So big D is three times the number of atoms minus six. Number of atoms minus six. The kinetic energy operator can be a fairly complicated creature. In general, it is a sum of terms with two derivatives and terms with one derivative and may also have terms that involve no derivatives at all. And it's not merely derivatives. The derivatives are multiplied by coefficients that are coordinate-dependent. And these coefficients are not. Dependent and these coefficients g and h can themselves be quite complicated functions. They can depend on all of the d coordinates of my problem. The potential, this V term in the Hamiltonian, comes from the Born-Oppenheimer approximation. And Grand yesterday talked about the Born-Appenheimer approximation. We've heard about it several times. So it's central to everything that I will do. It comes from It comes from the Born-Oppenheimer approximation. So, by making the Born-Oppenheimer approximation, we divide the problem up into two pieces: one piece for the electrons and one piece for the nuclei. And after all the electron people have done their work and produced the potential, then I can do my work, which is to solve the time independent or the time-dependent Turing equation to figure out how the nuclei move on the potential that's produced. On the potential that's produced by the Born-Appenheim approximation. So, this is related to the fact that interactions are complicated. It's not just two-body interactions. This function can be very messy, and that makes computing intervals in general hard. What really makes the problem hard is that the dimensionality is high. So, I'm interested in solving problems with 12, 14. In solving problems with 12 or 15 coordinates. Okay, so I would say that to solve a differential equation of quantum mechanics, there are three basic steps. One is one must somehow transform the differential equation into a linear algebra problem by discretizing it. And then, secondly, you need to compute matrix elements so that you can solve the problem. So that you can solve the problem. And once you've got the matrices, you have a linear algebra problem to solve. And many of you are probably mostly interested in this third step, those devising integrators and so on, or down here. And I will talk mostly about the first two, which I hope won't disappoint you, but I have one slide where I mention a little bit about the linear algebra methods that we use to solve the matrix equations. Solve the matrix equations once we get them. And I'm also interested in three, it's just not what I'm talking about today. So we use Krieloff space methods. Everything is driven by matrix vector products. So in order to get solutions of either the time dependent or the time independent Troding equation, what I need to do is calculate matrix vector products. I like to calculate, I like I like to call major connective products MVPs. They play a really central role in everything we do. And clearly, it must be to devise an efficient method, we have to come up with a way of evaluating matrix vector products that scales as less than n squared, where n is the size of the matrix. If you know nothing about the matrix and have no good ideas about how to evaluate matrix vector products, About how to evaluate matrix vector products. You just multiply rows of the matrix by your vector. That gives you an algorithm whose cost scales is n cubed. Because n is very big, that's a non-starter. You cannot calculate the contracted products, right? So n for these problems is sort of roughly of the order of 10 to the 6th, maybe 10 to the 9th. So we have very, very large matrices. The typical way to To design a matrix vector product whose cost scales is less than n cubed is to exploit sparsity of the matrix. That's not what I'm doing. So I'm using other ideas to reduce the cost so that it's less than n cubed. Okay. So now we can talk more about how we solve later, but I'm going to go. Later, but I'm going to go now to points one and two, those that I chose to emphasize, which is how the discretization is done and how the matrix elements of the matrices that you obtain when you discretize are computed. And I encourage you, I plead with you to ask questions as they go. Please ask questions. Please interrupt me. So the usual way to do this is to use what I think mathematicians call a chemeric. What I think mathematicians call a Galerican method, which chemists typically call a variational approach. So, in principle, it's very simple. You take these wave functions that you wish to calculate, that represent them as combinations of linear combinations of basis functions. You substitute this in the Schrodinger equation, and then you put another basis function on the left, and you obtain matrix equations. So, these are, this is the one you get in the time-independent case, this is the matrix. Dependent case, this is the matrix equation that you get in the time-dependent case. And those matrices, H and S, that appear in these equations are shown here. So they have matrix elements that we write in this form. This is a basis function on the left, the basis function on the right, and the angular brackets mean an integral. So we have to calculate those intervals in order to build these equations. These equations, which we then solve with some linear algebra method. So there are two things that make the problem hard. One is that the basis is a big. So I've already told you the matrices are maybe a million by a million or something. So we need to think hard about how to reduce the basis size. And we heard some about that in the previous talk. Clever ideas for parametrizing basis functions. Then you minimize. Then you minimize by varying the parameters. But a second essential part of the calculation is computing the matrix algorithms. So once you get the basis functions, you have to do something with them. You have to calculate these matrices. I'll talk quite a bit about that. Okay. So to use this Galerikin approach, you've got to calculate matrix elements. Calculate matrix elements, this matrix being a sum of these two. And very often, the hardest matrix for which to calculate elements is this V matrix. And this is just written out so that it's literally in your face. It's here a D-dimensional integral. And in general, you can't factorize or decompose this integral. You're confronted with the task of calculating a D-dimensional integral. D-dimensional integral. So, this is the question I asked at the previous talk. In six dimensions, it's pretty straightforward. We just use a tensor product grid, typically Gauss points, and it's okay. But if you try to do it in 12 dimensions or 15 dimensions, it gets tough. So it's even difficult to store in memory this potential evaluated on a direct product grid with 10 to the 12 points. So quite a church. So, quadrature is really a very serious problem. And it's serious because the integrals are multidimensional and complicated, and also there are many, many, many integrals. So if you've got, you have n basis squared or n basis squared over two integrals to calculate. So yes, Monte Carlo, sometimes an option is really not accurate enough for what most of us want to do. And in addition, you better. Better, you have to do it many, many times. So we actually come, we actually solve the Schrodinger equation without calculating these integrals, but we need to have points with which we could properly calculate the integrals. Okay. Can I ask a question? Please, please. So what are your assumptions on the V? I have no assumptions on the V. I mean, so you're you're asking what properties does the V satisfy? The V satisfy? The most important property that it satisfies for me is that somebody can give it to me. So I can't tell you a lot more than that. It's a smooth function. In the simplest case, it's sort of like a quadratic function. There are many interesting problems that have more than one well on the More than one well on the surface, but typically, and one thing I know for sure is that when I pull molecules or atoms apart, that the potential goes up. That stops that configuration from becoming excessive. Some of products form a two? Right, so we'll get there. I'm not assuming that, right? So, I mean, clearly, so what Kaladina is asking is. So what Kaladina is asking is: if this function were, let's start with just a sum of products with one term, if it were a simple product of factors, and if each of the basis functions were also a product of factors, then you could factorize the integrals, get a product of d one-dimensional integrals. You're a weight of the races, as we say, which is that's easy. And it's almost as easy if this thing is the sum of products, as long as these functions. Products, as long as these functions are also products. But I won't assume that, which is why I have a problem. And so, one general strategy for avoiding the problems that I'm focusing on is take the potential that I'm given and massage it into some desirable form so that I can do the interim. I won't talk about that strategy, it's the most used. That strategy. It's the most used in the MCTH community, which we've heard some about already. You can do amazing things with that strategy. The two basic problems are, I would say that, well, first you have to do something with the potential. That means you're making some sort of an approximation. You're not using the past potential that somebody can come up with. And then secondly, as the problem gets harder, the number of terms you need to represent it increases. Represent it increases, and the scale of such a calculation well, scale the cost will scale linearly with the number of targets. Okay, so again, integrals are a significant problem. There are millions of integrals. There are no good quadrature methods for 12-dimensional integrals, I claim. I'd love to have somebody here contradict me. All right, okay, I want to talk. Can you say something about? Can you say something about that? Have you heard of sparse grids? Yeah, yeah, yeah. So I'd use sparse grids quite a bit. And so they help to some extent. They greatly reduce the cost. And we will use sparse grids ideas here as well. Then ancient quantum chemistry drinks that Gaussians are an algebraic, so product of two Gaussians is a Gaussian at the center of gravity. Can you find your potential with Gaussians and all your integrals or an algebra? So, to know your integrals or analytics. There are various tricks that you can do. Something like that might work if you're willing to adjust this potential, right? Or massage it was the word that I used. So that's a possible massage. Yes, right. And you have to worry about whether Gaussians are a good basis function for this problem or not. In the quantum chemistry world, the integrals are much less a problem because you can do, if you use Because you can do, if you use Gaussian basis functions, all the integrals exactly. And that partly comes from the fact that all the interactions are two-body. It's not the case here. Okay. So intervals are a problem. Size of the point or two grid is a significant problem. Just storing the integrand requires, if you have a 12-dimensional problem, about 8,000 gigabytes. Okay, so you saw in the title of my talk, Collocation. Title of my talk, Collocation, I'll talk to you about how to use collocation to completely obviate the need to calculate integrals at all. So, first, as we say in modern English, so that we're on the same page, I'll just define what I mean by collocation. We begin just as we began with the Galerkin method. You represent the wave functions, a linear combination of basis functions. So, collocation is merely a different recipe for choosing those coefficients. That's all. For choosing those coefficients. That's all. There's still a basis. In general, I will use the same basis that I would have used if I was doing Galeric. So here is what collocation is. You represent the wave function, it's the linear combination of the basis functions, and then instead of putting another basis function on the left, you demand that the Schrodinger equation be satisfied at a set of points. So a physicist would say that this is equivalent to thinking about, I think, the last speaker. On to thinking about, I think the last speaker talked about test functions. That's a word that's often used for the function that comes in from the left. The test functions that I'm using are direct alpha functions. But maybe simpler is to say that I'm demanding that the Schrodinger equation is satisfied at a set of points. So if you do that, you plug this in with the Schrodinger equation, and demand that it be satisfied at a set of points by putting a direct delta function here on the left, you get this matrix eigenvalue problem. So it looks a lot like. So it looks a lot like the standard variational problem. This is B. I use B for basis. So it's the basis functions evaluated at points. So this is easy to compute. V is even simpler. V is a diagonal matrix that has the values of the potential on the diagonal. This is the hardest thing to calculate. It is the kinetic energy operator evaluated, operated onto one of the basis functions, and then you evaluate. One of the basis functions, and then you evaluate at a point. So that's all I need to do. And then I have this matrix lighting value on. So it's just a different recipe for calculating these coefficients. So most obvious advantage is that there are no integrals. If you don't have any integrals, you don't have quadrature. It really gives us great liberty to choose any basis functions we want. So typically, as you optimize. Typically, as you optimize more and more of the basis, the calculation of the integrals becomes harder and harder. And if you don't need to calculate the integrals, then you're free to optimize to your hardest lines. It has the allocation, it has the advantage that as the basis improves, as I improve the individual basis functions or increase the size of the basis, I get closer and closer to exact results. Closer to exact results in principle, regardless of how I choose points. So, this is just the idea that you can interpolate, if you have a, say, fourth degree polynomial that you want to interpolate, you know it's a fourth degree polynomial, then you can interpolate it with any points using x to the 0, x to the 1, x to the 3, x to the, x to the 1, x, about x, yeah, x to the 3. And you can use, in principle, any points. There may be numerical problems. Numerical problems, but in principle, you're free to choose any points whatsoever. That's not true when you're doing quadrature. When you're doing quadrature, if you do a lousy job of choosing quadrature points, then the matrix elements in your Galactic approach are garbage, and you get garbage out, even if your basis is good. And another advantage that I like is that this simplifies using a very complicated kinetic energy operator. So I've been complaining about the potential, but. About the potential, but this kinetic energy operator can look even worse, right? Because I may have a big matrix of these G's. And for each one of these elements, I also need to do D-dimensional quadrature. So that's really very messy. And if I'm using collocation, I just need to evaluate G at points. So all of the intervals, not only the potential, but also the intervals for these vectors. But also the intervals for these factors they disappear. Okay. Yes? I know equity collocation. So what I wonder about the statement that collocation has the advantage that the basis improves, the choice of the points becomes less important. So I I mean but isn't it so that if I if I for example uh if I this if I use for example If I use, for example, terminal polynomials defined by choosing a set of nodes, so that would be like a then the convergence properties of the resulting pseudo-spectral method will be very sensitive to the distribution of the points, right? So short answer is no. Longer answer is it depends on the extent to which you can represent To which you can represent the wave function you want to calculate in the basis you have. So, if you can't, to the extent to which there's basis set error, that is, the basis doesn't completely describe the function, then the choice of the points can matter a lot. So I think the easiest way is to imagine, is to think about this interpolation argument that I gave you. If you're interpolating a function and you can represent that function exactly in That function exactly in your basis, and formally you can choose absolutely any points, and you will get the function back. But I mean, this is, it's a bit of a, I mean, the basis will never be, most problems, the basis is never going to be good enough that this is completely true, that the choice of points is irrelevant. And it's also true that there can be numerical laws. And you'll hear more about. And you'll hear more about that. But I guess I'd like to leave it at that for the moment and encourage you to ask again later and come and talk to me later as well. Okay, but so I think the worst feature of this equation is that it's a generalized eigenvalue problem. It's not a regular eigenvalue problem. And that's, I think, a very serious problem. If you want to solve, if it's a Problem. If you want to solve, if it's a big matrix problem, that means you need to use iterative methods. And I love taunting mathematicians with this. There is no good iterative eigensolver for the generalized eigenvalue problem. Again, if you think that's incorrect, I mean, so what most people will do, and what I will also talk about doing, is you just bring this over here, then you get a regular eigenvalue problem. But by doing that, you've created a new problem. You have to deal with. Created inverse problems. You have to deal with the inverse. So that's what we will do. I will bring V over here. Now I don't have a generalized diagram, but now I have this inverse over here. And for everything I've mentioned so far, all of the matrices are square. That means I have as many collocation points as I have basis functions. So this inverse is well defined. It may be very, very expensive to compute, but it's well defined. But it's well defined. Okay, it looks like that this might be the end of the story. So if B is a very big matrix, how in the world are we going to calculate its inverters, right? If B is a million by a million matrix, you can't feed that some routine and inverts, right? And so the general rule for the size of the basis is about 10 to the number of degrees of freedom. So to do Degrees of freedom. So to do this formaldehyde problem that we heard about before the coffee break, you have about, if you do nothing to optimize the basis, it's 10 to the 6 by 10 to the 6. You can't store that. You can't do it. But there's a second problem. Even if it were somehow possible to calculate the inverse of B, we would still have the problem that we need to calculate matrix vector products with. We need those also to be efficient. Each one of those matrix vector products. Each one of those matrix vector products must scale at a cost that is less than n cubed. And even if B has properties that make it possible to calculate matrix vector products with B, which we also have here, efficiently, it may not be possible to calculate matrix vector products with the inverse of B efficiently. So those are problems to look out for. How are we going to deal with this inverse of B? How do we compute it? How do we compute it? How do we calculate matrix vector products with material? So, a clue is there's one case in which it's obvious that it's easy to do. So, if B is a chronicer product, then it's easy to infer it. So, if the basis I have is a tensor product basis, and the grid that I'm using is a tensor product grid, which is quite common when you're doing quadrature, then this B matrix, it's a chronicle product. Matrix, it's a chronicle product. The elements of the B are just products of these elements. Because it's a chronicle product, I can invert it by inverting the factors. They're small, so it's easy to invert. So it's easy to get b minus 1 if the basis is a tensile product basis. And it's also easy to calculate matrix vector products with the inverse of the mean. It's easy to calculate matrix vector products. It's easy to calculate matrix vector products whenever the matrix is a chronicle product. So then you can do that by doing some sequential. I can talk more about that if you wish, but this chronicle product structure makes matrix vector products efficient. Doing the sum sequentially reduces the cost of the matrix vector product from n to the 2d. So here this little n is a representative number of functions for each of the 2. Functions for each of the coordinates. So the total of the big n is n to the d. So the simple matrix vector product, the one that I was calling n squared, it scales as n to the 2d. That's just the squared. So the Kronecker product advantage reduces that cost from n to the 2d down to n to the d plus 1. So that's a huge advantage. So you save many orders of magnitude. It comes, this ability to do some sequentially comes from the quantum product structure. And when we have that structure, we can do matrix products with B and with the inverse B efficiently. So the Kriloff methods that I'm not talking about, but had on one slide near the beginning, make it possible to do everything without storing matrices. But, of course, we have to store vectors. Store vectors. And the vectors themselves get big. So if I have a system with six nuclei, storing just one vector is a problem when I use a tensor product basis. So that means that although I don't need to store matrices, I do need to store vectors, and that will hurt. That will make it impossible to do the calculations I wish to do. So what that means is that somehow I have to go beyond this tensor product base. Beyond this tensor product basis. Because if I use a tensor product basis, I can't store vectors. And so you should worry that if I go beyond the tensor product basis, I will lose this chronicer product advantage. It will then become difficult to invert and difficult to calculate pair-to-rejected products with the inverse. So, to do this, I use ideas that are related to Ideas that are related to sparse grid methods. The first step is to reduce the size of the basis by throwing away some of the basis functions. Basis functions that I believe will be unimportant for calculating the wave functions I want to have. And this is then convergence parameter, something that I test. So for each coordinate, we have a set of basis functions. So here the basis functions are called phi. They're labeled by 0, by 1. They're labeled by 0, by 1, by 2. There's a set like this for each coordinate. C here labels a coordinate. The multidimensional or multivariate basis functions that I use are products of these one-dimensional functions. So if you have a tensor product, then you have all possible products are in there. And I'm going to retain only some of those products. So this is one simple way to do that. You retain only products for which the sum of the products are produced. Products for which the sum of the indices is less than or equal to some threshold parameter that I call p. So, this is exactly the condition of the hyperbolic cross. The hyperbolic cross is a product, not a sum. But it's related. I mean, the hyperbolic cross is not a good thing to do for me. But this works quite well. And there are many generalizations of this. You can choose, instead of this simple sum, you can choose a sum of functions. Choose a sum of functions, and we've also done that that has some advantages but complicates the equations. And I'm not going to talk about this, I'm going to talk only about this. So this is a super simple idea. It just says rather than using this basis, this is a two-dimensional example, use this basis instead. So it looks pretty obvious. In many dimensions, it saves a lot. So this is why the spores group people are. This is why the sparse group people are a business. So for a 15-dimensional problem, it saves about a factor of 10 to the 10. So you can save a lot by doing this. So that means I've significantly reduced the size of the basis if this pruned basis is good enough to solve my problem. Yes? Could you comment on why you don't like the hydrogen code? Yeah, it's essentially because I'm trying to calculate the state. The states, the lowest states. And the lowest states are just well approximated by a basis that has this form. In other words, if you put the lowest states on the grid like this, they would overlap significantly with the basis functions that I'm keeping. The hyperbolic cross would be something like that. It would leave off this big chunk in the middle to get those states. I would have to push the hyperbolic cross, push. The hyperbol across, push up the arms of the hyperbolic cross to make it fat enough here where I need basis for pushing as well. Yes? Some some excitation numbers coming in your video and that gives you the range. So correct way for something which is closed. Correct way for something which is closed. Sorry. Yeah, I mean, another way to explain it would be that the basis functions that I'm keeping here are those really that have the lowest zeroth order energies. And if the zeroth order problem is close to the full problem, then that's what I want to do. Okay. So it's great to be able to reduce the size of the basis. The size of the basis. But as I've been emphasizing, we need to do more than that. So I need also to find some grid that goes with this reduced basis. This was the question that I asked at the previous talk, is if you reduce the size of the basis, but underneath it you have a big tensor product grid, you've still got problems. So I need to find some grid that's as good as, that is as small as, approximately, as the base. As approximately as the basis that I've got. And to do that, I use what's called the spark squared. So I won't talk much about the mechanics of how this works, but I associate with each one of these functions a point, and then I keep the points for which the sum of the indices is less than or equal to this peak. So I'm keeping a set of points that correspond to the basis functions that I've chosen. So essential here. So essential here, I don't know if I mentioned this, but essential is that my set of points is nested. And that's not always done in the sparse grid world. So my basis is naturally nested. That means that the basis with three functions has one function added to the basis with two. To the basis of the 2. So it grows. The point set grows in a similar way. I need to choose the point that goes with my basis function nc equals 0. And then I add another basis function, and I add the corresponding point. And this will give me then a nested sparse grid. Okay, so a while back I warned you that as soon as I give up the Kroniker product structure, which I give up when I stop using utensil products. When I stop using utensil product bases, I risk having problems calculating the inverse and doing matrix vector products with the inverse. So that's what's explained here. So I now need to calculate the inverse of this matrix, where B is, this is how I will write B, it's a Kronecker product matrix that's been chopped. So these C matrices, they chop the Kronecker product. That means C here is the matrix. C here is the matrix that removes columns, and C transpose will remove rows. So if the basis functions I did not want were the last in my list, then this would just be a rectangular identity matrix, that's all. And it may be not obvious that it's so bad to lose the structure. You might think that by chopping, you get a major. That by chopping, you get a matrix that's much smaller. It's always easy, it's always better to invert a small matrix than a big one, right? Well, here that's not true, because the big one is a chronicle product, and so its inverse is easy. So although B is smaller, there's a real danger that it will be much harder to invert, much more costly to invert than the chronicle product. So I worry about the cost of the inverse, I'm worried about the cost of the matrix vector product. And my key problem. And my key problem is that the inverse I want, which is this one, I can't get it by just inverting the Kronecker product and then chopping that. I have to invert the chopped matrix. And these two things are in general not the same. If this were true, it would be great, because applying these chopping matrices to a vector is just restricting the ranges of sums. That's easy. And this we already know how to do. But I can't. What I know how to do. But I can't do this. I need to do that. The first critical piece of information is that triangularity is a crucial property for us. This property that I want to have, that the inverse of the chopped matrix is the chopped inverse of the Kradeker product, it's true if the matrix is triangular. So if I can somehow arrange... So, if I can somehow arrange for my Kronecker product B, so what I really want to do here is B, is either lower triangular or upper triangular, then this works. And then it's clear that it's not hard to invert, and it's not hard to do matrix vector products with the inverse. So, why is this true? It's essentially block-Gauss elimination. The tridots are here. I'm doing the lower triangular example as this form. And if you just do Gauss-Block Gauss, And if you just do Gaut block Gauss lination, you see that the inverse at the top left corner is the top left corner of the inverse. So that comes from the lower triangle area. It's true also, it's upper triangle. Okay, I will use that in a minute. First, I introduce another idea that I think many of you will not be familiar with. What is clear is that, in some sense, that you Is that in some sense that you understand better than I do, Galerican is the best thing to do. Best results. I assure you that for most problems I'm interested in, we can't do Galerican because we don't have the exact matrix of things. So that is, when we do Galerican, we're always doing Galerican plus, Galerican plus quadrature. Collocation is certainly not going to be as good as Galerican in general. There's some penalty that you pay by doing collocation rather than Galerican. Allocation rather than polarity. So, this is optimal, you get the best possible solution in the basis. This is not. You get something that's not as good as the best possible solution. Collocation will be exact when the interpolate that you get for h applied to a basis function is exact. So, if I can interpolate the function that I get by applying h to each one of my basis functions, if I can interpolate all of those functions. Interpolate all of those functions exactly in the bases with the points that I have, then I get back to the larger. This, of course, is really never going to be the case. It's almost always the case when you apply h to a basis function, you get out of the basis. So that's why call operation is not exact. I get out of the basis when I apply h to the basis. So, what I will do is I will. What I will do is, I will show you that it's possible to systematically reduce collocation error so that you can go from collocation to Galerica by using more points than basis functions. That's one of the main ideas of the talk. I'm going to put in more points. Everything I've done up to now has been squared. I'm going to use more points than basis functions in order to get rid of this extra error. Now, that means that I have rectangular matrices. So that's a bit scary. That means, for example, if I'm solving a time-independent problem, which is the simplest one, you have an eigenvalue problem, you have a rectangular eigenvalue problem. So I think the simplest way to explain this idea that I want to go from collocation to Galerican by using more points is to go back to Galerican plus quadrature. If you're doing Galerican, Quadrature. If you're doing Galerican, but calculating matrix elements by quadrature, clearly you will use as many quadrature points as you need to get good integrals. There's no reason to use only as many points as you would have basis functions. So you pour in the quadrature points until quadrature becomes good enough, right? And that enables you to get really Galerican results, just by doing good quadratures. I want to do something like that with collocation. I want to use warp. We want to use more points than functions, just as one would do, of course, when using quadrature. You do that. So here are, before I tell you how we do it, here are a few advantages. I've already told you we want to get Galerican accuracy. That's the most obvious advantage. Another advantage is that using more points reduces the sensitivity to the choice of points. In practice, Choice of points. In practice, there is some sensitivity. I told you at the beginning, you could choose any points in a particular trial case. In practice, the choice of the points does matter some. And you can make that choice less important by just using more and more points, just like you would do with quadrature. But if you do a bad quadrature, you put in enough points, you get the right result. So that's what I want to do. And it's important because sometimes it's not easy to It's not easy to find good points. There are cases in which it is easy. So we just heard about orthogonal polynomials, right? Then you know Gauss quadrature points. They're great points. In general, it's not easy. So it's not easy if you have general one-dimensional basis functions. So where I'm going is towards MCCDH. It uses general basis functions. Its basis functions are optimized. They're not classical or Foggy polynomials. In that case, Polynomials. In that case, it's far from clear how to find good points. It's even worse in many dimensions. So we know how to choose good quadrature points in one dimension. If I want to do a two-dimensional integral, it's hard to find a good set of points in two dimensions that's not just a tensor product, points in one dimension. So this is the sparse graph. In one dimension. So this is the sparse grid idea, I guess. I mean, can be. What's that? Lattice methods. So instead of probabilistic Monte Carlo, we have lattice methods which are non-probabilistic, but somehow are filling a space, a Q, in high-dimensional space, much better than other processes. Yeah, so I don't know much about it. I know that such things exist, and there are also other approaches. I maintain that this is. I maintain that this is tricky. And in a way that finding points in one dimension is not. And therefore this advantage of being able to take more points is more important in higher dimensions than in one dimension. And this last point here is if you're solving a time-dependent problem with a time-dependent basis, then the most natural points are themselves time-dependent. If you use time-dependent, If you use time-dependent points, then you're forced to evaluate the potential over and over and over again. And in many calculations, that takes a lot of time. So if I could use worse points, but time-independent points, compensate for their worseness by taking more of them, that would be a significant gain. Okay. So many people will think of this right away. If I want to use more points than basis functions, I can just use. points in the basis functions, I can just use a pseudo-inverse. So rather than solving this equation, I multiply on the left by B transpose, and I get the pseudo-inverse, which is right here. So that is an option. It has the disadvantage that it's in general costly to calculate this pseudo-inverse and costly to do matrix vector products with it. So what I will show you is an alternative pseudo-inverse or another. Of pseudo-inverse or another way of doing this rectangular problem for which doing matrix vector products is sufficient. I mean, this is okay if, again, B is a chronic product. But in general, I want to do this pruning, this reduction, and the B is not a quantum problem. Okay, so this is how we do it. So this is my generalized eigenvalue problem. So I put tilde zone. Value problem. So I put tildes on here just to remind you that these are rectangular matrices. So my goal is to find some matrix F. It will play a bit the role of a pseudo-inverse that I can put on the left here, and I want to choose it so that M times B tilde, what I have on the right-hand side, is just an identity. So that converts me from a generalized to a regular identity to a Generalize to a regular eigenvalue, which is what I want. And I also want to insist that it's cheap to do matrix vector products with this thing that I have then on the left. Okay. So I'm going to write this B tilde like this. This CG is the pruning that I use, chopping that I use for the grid. This C delta is the additional chop that I do to reduce further the size of the base. The size of the basis. So this Cg times C delta is the chopping that I do for the basis. And then, and this is crucial, I replace this B chronic product with its LU decomposition. Now that's explained on the next slide. But here you see the inverse of it, right? So I've replaced B with the product of the two chronic compromot matrices L and U. And so the LU decomposition is easy. If B is a chronic component, Is easy if B is a chronicle product, L is a chronicle product, and U is a chronicle product. So this is what I'm proposing to use as a pseudo-inverse. On the next slide, I show you why it works. So if I take, so I've just repeated here this M, this is it, and the B tilde is this line. When I go from this line to this line, I exploit the fact that because U is triangular and L is triangular, I can pull the inverse out and put it on the outside. The inverse out and put it on the outside. So, in this triangular case, the inverse of the chopped matrix is equal to the chopping of the inverted chronic required. And then to go from this line to this line, I insert between L and U C G times C G transpose. That's a projector. This is an identity. It's an identity because if you look at C G transpose, L K. CG transpose LKP, it's a matrix from which I removed rows here, so it has zero columns because it's triangular. That means that I can put this CG on the right and change nothing. So, and then this cancels with this, this cancels with this, and I get C transpose, C teltlet, C delta here, and that's an identity. So, this is another pseudo-inverse that one can use. Matrix vector products with it are efficient. With it, they are efficient because all I need to do is apply these chopping matrices. They just reduce the range of indices when I'm doing sums to do the matrix vector products. And then I need matrix vector products with quadrangle products. We've already talked about the fact that they're easy to do. And V is diagonal. Okay, so quick calculations on a molecule with five atoms. This is the sort of result that you get. So in different colors, I have In different colors, I have results calculated with different numbers of points. This is the number of basis functions, and the bottom line is that as you increase the number of points, going from black to turquoise to this pink color, the errors go down of the eigenvalues. This is a plot of the lowest 50 eigenvalues. This is like the idea that by using more quadrature points, you can get better accuracy. But it requires no weights and actually no quadrature points. And actually, know what we do. Okay, I want very quickly to talk. Maybe I'll take five minutes for how to use this with MCTDH. We've heard a couple times about MCTDH. Graham began the conference by talking about MCTDH. It means multi-configuration type partridge. It's a very popular method. It is, I would say, A or perhaps V method of choice if you have some model potential. If you don't have a general potential, but you have a potential potential. Have a general potential, but you have a potential that is, as Cameron said, sum of products, for example. It uses the main idea is that it uses optimized basis functions in one dimension. It's still a direct product basis, but the one-dimensional factors that you multiply together are optimized. So that's the glory of MCTDH, is the optimization for one-dimensional functions. It's a variational or Galerican method. That means you have to calculate integrals. Every time you do Galerican, Every time you do Galactic, you have to calculate integrals. And almost all Lenc calculations are done by assuming that the potential has some z form. The simplest and most common is the sum of quadruplex form. And then, as I explained when responding to Kellogg's question, in that case, the intervals all factorize and nothing is over anymore. So, what I want to do is combine these collocation ideas that we've been discussing with MCTDA. Discussing with MCTDH to exploit at the same time the power of MCTDH optimization and the ability to avoid intercourse. And there is an alternative way of doing this, which I'll be happy to discuss with anybody. Uva Mante developed something he called correlation DVR. It's an alternative sort of quadrature type method. Okay, so this is what MCTDH is. Graham already showed you, this basic structure. What one needs is equations to calculate these coefficients, A, and the functions themselves, because they're optimized. We don't know what they are until we develop and solve equations for them. The equations that we have from Romanta and Titamaya and Sedemo were derived with a very interesting. We're derived with a variational approach. Graham told us, Durac Frenkel. And I'm not doing a variational calculation, so I have to go back to the beginning and derive new equations for these A's and Foucault. The way I like to derive the equations is by using an approach that I think is probably more obvious to most mathematicians, a Galerkin or a Petrov-Galerin kind of approach, right? And so if I do that just to derive, if I do derive Just to derive, if I do that where I come in from the left with simple particle functions, so I'm doing a Galerkin approach, I get, of course, the same equations that everybody uses. The advantage of this is that it opens the door to coming in on the left with something else. That's what I'm going to do. I'm going to come in with a direct delta function. I evaluate a point, and I can use the same tool to derive the equations. To derive the equations. So, this is what we do to get the equation for A dot, that A is the coefficient. We come in from the left with a product of the points. So this is, if you were doing standard MCTH, you would have over here a product of single particle functions. Instead, because I'm doing collocation, I have these points. That means that I get this A equation. So I have these B matrices over here. This is a bit like the generalized eigenvalue problem that I have. Generalized eigenvalue problem that I had when I was doing the time-independent genes, right? So I'm going to have to get rid of this somehow in order to get an equation for a dot. It means I'm going to need this inverse. This h is, as it always is when you do collocation, the h operator acting on basic functions evaluated points. Okay, so it's simple to get the equation for a dot in the direct product case, because in that case, this thing is just a chronic product. We want to kind of get the inverse of the chronicle product. If I'm going to prune chop, then I have the same problem I had for the time-independent problem. I have to calculate the inverse of this matrix. But we know how to do that now, right? Or I've talked about how to use M for that. Deriving the equations for C dot is much, much harder. We just submitted a paper about this. In the paper, I used the word bedeviling, which the postdoc really liked. It took us more than he. Time. It took us more than a year to figure this out. It was really quite nasty. So, this C is the coefficient that I get when I expand the single particle functions in terms of the primitive basis functions, these guys. So, I want to calculate this C dot. To get it, I look at this residual again, but to minimize, I put on the left all of the points here except for one, because I'm deriving an equation for the kth coordinate. So I have all kth coordinates. Kth coordinate. So I have all k prime except for k here. You get, in the direct product case, you get in matrix notation, you get this equation. You can see that it's, all I need to do is identify this. This I matrix is here because there's one point that's left out when I come in from the left. After the pruning or the chopping, the equation becomes a lot messier. We get a different matrix here on the left, which we call B. Matrix here on the left, which we call B-bar, but it has the same sort of structure as the one that we had when we were talking about N. It's a chopped chronicle product. Here there's an I-factor chronicle product, but it's still a chronicle product. The worst problem over here is that when you derive this equation for C dot, you end up with something that has A dot in it, and you have to get rid of that. And you have to get rid of that by substituting in from the equation for a dot. And that becomes now very tricky because the equation for this equation, the one that I have for A, it uses a different grid than the one that I have here to derive this C equation. Okay, so that's what's explained here. I can, what I'll do, I think, is I. What I'll do, I think, is I will. So, I told you the talk was not, I probably have forgotten. My talk is not finished. I got about three-quarters of it done before my computer died. I have, on Graham has let me use his computer. I have no conclusion. I have no results. The results I can show you. I can show you figures from the paper, which is also in Graham's computer. But I'll stop here. If you want to see the results, you can ask me. If you have other questions about the method. Communicate. If you have other questions about the methods and ideas, we can support as well. Thank you. I have a bunch of questions, but I will not exploit my positions. Thanks. I love the idea of solar sampling in control methods. And I've seen this applied in other contexts as well. And I worked with you about just technology. And it works really well because exactly what you say. You don't have to be so careful. You're choosing your collocation points. There is another way of thinking about it sometimes, which is that you can think of this oversample collocation system, rectangular system that you mentioned, as kind of a discretization of like a looking method. And in that way, you can sort of determine maybe a better choice of publication points from understanding how you should discretize an integral in a better current way. Better character. Now, I wonder, you've done something very clever with sort of reduced, sort of changing your whole assembled system to bring sort of this B parameter of matrix away and to reduce it to a simple value problem. Can you, once you've done that, still interpret it as a sort of discretization of some good option method and can it help you inform the choice of provocation problems? Um so yeah, I I'm I'm Yeah, I I'm I'm I'd be interested to know what other oversample collocation people have done. I think the traditional and most obvious route is through this pseudo-inverse, right? Which does give you, I would say, just a quadrature approximation to Galerican. So if I can, yeah, this is just a quadrature. Let's see if we go back up to here, it's maybe more obvious. This is a quadrature approximation to Galerkin. This is a quadrature approximation to what I call the overlap matrix, you call the Gram matrix. This is a quadrature approximation, obviously, for the potential. This is less obvious, but this is a quadrature approximation for. But you can get good results even when these quadrature approximations are miserable, right? Because what you're really doing is collocation and disguise. And you profit from this advantage if the basis is big enough, you get the right result. So, in a way, this is better than. So in a way this is better than a quadrature approximation to Galerican. Then your real question was is whether I can interpret. Yes, it wasn't slide 30. Sorry. Where should I go? Slide 30, I think this is sort of when you slide 30. Having renewed start changing that system, you're going to use the column. Yeah, so I think it's harder to see here and more obvious after I've put the M on the left, right? So I don't know if. So, I don't know if I have an appropriate equation, but I guess so. I mean, the short answer is yes. I can see Galaxy and IDD. Thank you very much. So, you start from the initial problem, and at some point, you signify an intensity. Of course, so it's not here. So uh it's not clear to me that the uh generalized eigenvalue problem that you get uh as uh real eigenvalues. So are these do you need eigenvalue in all the time or how they are eigenvalues and if they are or would so I sacrifice hermeticity because I put a point on the lab rather than putting a basis function. Um and so yes, I get a general And so, yes, I can generalize eigenvalues. Well, it will not have only a real eigenvalues. It has some complex eigenvalues. Those that are converged are illegal. Thank you, Ari. So I agree with you that the B transpose B, an approximation to the Galurkian matrix, if the quadrature rule associated with that has equal weights, when the quadrature rule is rule, Weights. When the quadratule has unequal weights, it should be V transpose WB, which be the approximation where W is the diagram matrix with the weights. So have you considered doing the co-location with square root of the relevant quadrature weights, where you weight each row of the rectangular system up? I think that we can cover that by adjusting the distribution of the points that we The distribution of the points that we use. So, no, I haven't done that. I think I can do something similar by changing. It's nothing good to do. You just scaling the rows of the system. But if you've only got a square collocation matrix, wait till the rows doesn't do anything. But when it's rectangular, it does make a difference to the exact projection you're doing when you go to the microphone. I've thought about it, but no, I haven't had it. Chrom times vector or chrom times matrix or. Vector or prompts matrix of the equation. The way it's implemented with this, you have to fold up your vector into a multi-dimensional array and then contract individual dimensions in that array with the third of the chronicle work. What we found is hugely memory intensive because you have orders and dimensions of hectares to contract them. Have you seen or have you found any way of reducing the memory bandwidth? Reducing the memory bandwidth and the random memory access side that operation because we find it's not slow because of the CPU, it's slow because it hammers the memory so on you I don't do any reordering. I'm not sure why reordering is necessary. So you you just do a direct indexing of the corresponding elements, right? Okay. I'm not yeah, I I I do it with indices. I do it with indices. From the point of view of the CPU perfection, right, if you are just addressing across some index. Right, I understand that. Yeah, so I mean, I'm not doing any reordering, but it's true that I'm not accessing elements of this vector in the most optimal way. So you reorder in order to access more optimally. That's right. So we reorder the indices of that folded multi-dimensional tensor so as to then facilitate the reinspector product. And then we reorder it better. And you just said that that reordering is actually fairly costly. It has some advantage, but that advantage outweighs the cost of the money. I don't know. We haven't benchmarked it. Half a question. So so your hybrid message? So so your hybrid method has the collocation points fixed or do they move as well? For an FCTTH. Right, so we've done both, right? So it is true that having points that move gives you better points. And if we use points that do not move, we have to use more of them. But we can compensate for the error caused by using less than the best points by taking more of them. That's your preferred choice. Yeah, it will depend on how expensive it is to evaluate the potential method. I can show you the plots afterwards if you wish. Two quick questions. First of all, is there spectrum proof? Because we know essentially what you are doing is what is okay. I'll explain. Once sort of your method to compute eigenvalue, Your method to compute eigenvalues is a special and well sophisticated case of what is known as a finite section method. A finite section method, you have infinite dimensional operator, you have a basis, you are computing elements. Acting on the basis, the infinite matrix, you truncate it and you compute the step. So that's what is known as Feynman intersection method. And unless the original operator is self-adjoint and compact, we know that there can be spectral pollution. In other words, eigenvalues that don't exist appear in the solution, false eigenvalues. There are ways of avoiding it, but they are quite sophisticated. You have to work with pseudo-eigeon values and so on. So, did you notice it? What you call finite section method? Finite section method seems a lot like Galerican to me. But okay, but that's just language. So the answer to your last question is no, my operator is permission. That isn't compact. So I don't know what that means. Exactly. It doesn't need to be compact to avoid mental pollution. Okay. To do this week of positive, for example. So I move to the second point, which is actually, I find it quite interesting. I think that using Galerpin to prof, you remove the difference between collocation and quadrature. Because once you, the basis coming from the right is a basis of delta functions, it is exactly the same in both cases. Yes, you said coming from the right, you mean coming from the left. Coming from the left. From the okay, depends how you go. In the inner product, it is not your expansion, but the test functions. So if you are taking the test functions as a linear combination of delta functions at the collocation points, at the quadrature points, this is a special case of Galaxy and Dutch. And then you completely remove the difference between quadrature. Remove the difference between quadrature and collocation. So essentially, you have a sort of superstructure. But don't really understand that. I mean, so when you do collocation, you have matrices that are labeled on the left by a point, rows, and on the right by one of these expansion basis functions. When you do Galerkin plus quadrature, you have different matrices that label on the left and on the right by basis functions. No, no, no. Suppose that you're taking. Suppose that you are taking your test functions, functions that are simply linear combination of delta functions and quadrature points, or strategic products. Then you obtain quadrature. Okay, let's talk about later. I mean, in some sense, it may be true. It's not true in my understanding. No, no, because I think that it is quite interesting that you oftend here some sort of super. That you obtain here some sort of superstructure that obtains both cases. Okay. Okay. Maybe one last question. Let me go back to the generalized eigenvalue program. Which one, let's see. For the that I get as soon as I introduce call fusion. Yes, so the B matrix you obtain is inversion right. I just wanted to make sure, like, in yet the To make sure, like, get at the last slide. Yeah. So, t is a given matrix, right? So, is it true that you compute the inverse and then you multiply with t? Is that what you do? Well, we actually never, I mean, so in almost, yes. I mean, we don't actually build this matrix, but in the simplest case, the direct product case, this thing is a chronic product of information. This thing is a chronicle product of inverses, and we apply that to t and apply that then to a vector with the calculating eigenvalues. The thing I wanted to say is that rather than computing the inverse maybe for all of the blocks, it would be better to solve the corresponding linear problem, right? Yeah, I I um So what you want to avoid is computing the inverse and multiply. What you really want to do is to solve the new problem. Really, want to do is to solve the new problem. Because it's more costly, and if the condition number of the matrix is bad, it's also not stable, basically. Yeah, I understand what you're saying. I mean, rather than calculating the inverse, I can just solve at each system, at each iteration, a set of linear equations. Right, and it makes a big difference, that's what I'm saying. In what sense does it make a big difference? From the point of view of the numerical costs, it's like building the inverse and multiplying. Inverse and multiplying is more costly than solving the linear problem. If the condition number of the matrix is bad, then it's the better idea to solve the linear problem rather than just doing the two steps. I think that's true in general. I think here, when I can exploit this chronic product structure, that's the inverse is not problem. One thing missing from that was that what you would do is you would say. What you would do is you would set up an LU factorization and then you'd use that to compute versus each time. So rather than computing an inverse, you can compute an LU factorization which is cheaper and more stable and solves cheaper and more stable than if you were multiplying by the inverse each time. Yeah, so that's sort of related to what I'm doing because I do do the LMU decomposition. Okay, so the fair number of questions is the best terrible for the talk. Thank you very, very much. Talk. Thank you very, very.