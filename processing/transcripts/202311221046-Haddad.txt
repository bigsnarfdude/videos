I'm going to talk about these two things. So let me start with the basics, which is Steiner symmetrization. Of course, I guess you don't know. Steiner symmetrization is the process of transforming a convex body into another convex body with respect to a direction. So let's say, I don't know if you look, the vertical axis is following a direction B. Okay, so you take the blue set, which is called K, and you Called K, and you write it as a disjoint union of vertical segments, and then you translate all these vertical segments parallel in the direction of V until the midpoint of the segment is touching the orthogonal hyperplane. So you obtain a convex body which has the same volume and has now a symmetry with respect to this reflection. And okay, and if you do it many times in different directions, you can approximate a Eukilamol. You can approximate a Euclidean model. This is very known. So the thing, so if you have an operation on sets, you can extend this operation for functions doing it in legal sets always. If you have, let's say you have an integrable function f, then this is sometimes known as the layer cake formula. It lets you recover the function. It lets you recover the function knowing the level sets. If you know the level sets, you recover the function. So if you take the level sets and apply steinosymmetrization, it's like applying steinosymmetrization to the function. You get a new function having the same L1 norm and the same LP norms. And now the level sets are symmetries. If you do it several times in different directions, then you can approximate, then you can get a function whose level sets are Euclidean poles of the same point. Of the same volume. And this is called the symmetric decreasing rearrangement. Here and the star means the Euclidean bulb of the same volume as this set. Okay? So for now, it's okay. Okay, then let me recall this true very famous inequality. So this is the risk rearrangement inequality. It was proven in the thirties, I think. The 30s, I think, and it is extremely useful. So you have three integral functions, fg and h, and g is, you apply g to a linear combination, so x minus y. Then this inequality says that this quantity goes up if you replace the functions by their Steiner samples, okay? And this is an extremely useful function in analysis. You can for example prove the Fourier principle, you can prove the Foreclosure inequality, You can prove the Father-Kein inequality, you can prove bounds for convolutions of functions, etc. And then in the 50s, I think 54, Broshers extended this inequality to any number of functions and any number of variables. And the functions are no longer applied to x1, x2, x3, but to any linear combination of the variables with these numbers which are. These numbers which are scalar coefficients. So, this multiple integral goes up if you replace all the functions by their steining symmetrizations. And if you iterate these inequalities over and over in different directions, you get the same result for the symmetric decreasing rearrangement. Okay, so this was proven by Rogers in the 50s, and then this result apparently was forgotten for 20 years. Apparently, it was forgotten for 20 years and rediscovered by Praska-Tibb and GÃ¶ttinger. And this inequality is also very useful. For instance, it's at the core of the proof of the Praska-Tib inequality. Okay, so there are many results in the middle that I don't have much time to mention, but let me jump to this very general result of Pauris and Pimojaro, where they show a generalization of the Of the inequality, where, well, it is exactly the same, but there are now more complicated functions. And the hypothesis they put on the functions, they are, so the big difference now is that this function depends fully on the whole set of vectors, not just one linear combination of vectors. Here, this vector is in Rn, and this And this vector is in Rn to the T. Now, this inequality holds if the big functions, the f i's, are what's called Steiner-concave. So let me say right here, if you have a function f defining the Cartesian product of R n many times, then f Then F is stein equal to K P for every vector, let's say it's a unit vector, and every x one x D, or is it T? Yes. D, they are D, they all belong to a rel with xi perpendicular to d. The function the function T one T D in R D. So these are real numbers. So these are real numbers to f of x1 plus t1d xd plus t d v is classical game and even. And even. Okay? So I have to say what a quasi-concave function is. Fasiciconcave means has convex level sets. Okay, so for example, if you take the volume of the determinant of x1, xn, and you compose decreasing And you compose a decreasing function, say minus one, then this function is Steiner convex. If you put plus one, then it is Steiner convex. And this theorem is very useful to prove Gremer-type inequalities. Like for instance, if you put the determinant of any, well, it can be increasing or decreasing function of the determinant, and you put Determinants, and you put functions which are probability measures, probability densities, then this is a global equality. The fact that the expectation of the determinant comes up if you replace the densities by their sign-atic decreasing rearrangements. Okay, so I would like to generalize this inequality a bit more. Okay? And this generalization will explain why there are two sets of functions which There are two sets of functions which apparently are very different. Well, actually, they are in this, what I will try to explain is that they are part of the same thing. The thing is that these functions are already symmetrized. Okay? Is there any question so far? Okay, so before doing that, I would like to point out that the correct, the best The correct the best language to talk about this is matrices. So for a set of vectors in our n to the d, I will identify this take vector with a matrix where I put the vectors as columns, okay? Vectors as columns, okay? So this matrix will be living in D. And the reason, well, one reason could be the following. If you write this condition and you replace, you think now X is a matrix, then what do you get? You read the following. Um this will be I will write the the function f over there. The function f over there. This is f of the matrix x with columns xi plus v, which is one column vector, times the vector of the t's. Right? This is it. So actually the being stein. Being standard concave, apart from the quasi-concavity, if you forget for a minute for quasi-concavity, the even part, which is very important, is equivalent to the fact that this function is invariant by reflections. Reflections with respect to v. So if you multiply the matrix X here by a reflection with respect to V, the function With respect to V, the function has to be invariable. And as a consequence, this means that the function is invariable with respect to all the orthogonal group to the left. Okay? Then, when we have a space of matrices, we have the actions of multiplication to the left and to the right, of course. Now, if you multiply this matrix to the left, and this is very elementary, but sorry, then this is. Then, this is the same as multiplying each column by the matrix T. So, the action of an endomorphism from the left is the diagonal action, okay? If you think it in the product space, you apply the same transformation to all the coordinates. Now, to the right, it's a little more interesting. You can swap the columns of the matrix, you can make linear combinations with scalars, and you can with And you can do exactly this. All right, so then notice that this linear combination is exactly the same that appears in the Blast-Kambi-Plutinger inequality, in the inequality. So this means that the Glasgow-Blue-Pluttinger inequality can be written in a more concise way. Integrating, instead of integrating the Rn many times, you integrating the space of matrix. Many times you integrate in the space of matrices of n times d, n times m. Now I change the letter m. And now all the functions are applied to these linear combinations with scalar coefficients, which can be written as the matrix times a column vector. So for example, in the Rich Viewer inequality, you can write it as a multi-quick integral, or you can write it as an integral on the set of matrices of On the set of matrices of size n times 2. And now the variable is x, y, two columns, and the three functions are applied to the matrix times a column vector. Okay? Z times 1 minus 1 is exactly x minus 1. Alright? So we just change the language a little bit. And okay, now the Paris people are inequality can be written in this way. Written this way. The big functions are functions of the whole matrix, and the small functions are functions of z times the column vector. So what I would like to do in this generalization is allow functions to depend on any number maybe I shouldn't know on any number of columns. So for instance, But for instance, if you have a function going from Rn times Rn to R, then we could try to do this. Sorry, sorry, sorry. We are multiplying to the right. which is a function in x plus y and two x plus three y. This function does not fit in any of these two categories. In any of these two categories, it doesn't depend on the whole matrix, it depends on linear combinations, and it depends on more than one variable. This little left eyes, they all depend on one variable, and this one depends on only two. So the generalization is this. So for any number of matrices, of functions defined in the space of matrices, non-negative, unintegrable, and matrices. Matrices Li, which are any size, any size, but they need to have maximum rank, then this quantity goes up. This quantity goes up. So if you have some matrices depending on only one coordinate, you can call a column vector. Okay? And these are the little functions. If you put the identity, you have a big function. Like the big functions in the Pauli speaker. Like the big functions in the Paulis-Pivovar inequality. But what happens in the Paulis-Pivovari inequality, we always have this Fi, which is the same in both sides, and additionally, we require the big F to be steady-concave. Well, what's happening here is that a function is steady-concave if the symmetrization of the function big f is the same, if f is. Is the same if f is invariant by this symmetrization. Okay, so I didn't tell you yet what is this and which symmetrization I'm using because now the vector v is still one vector in dimension n, okay? And the matrix is defined in rn times rn many times, the function. Okay? So, what I'm going to do is I'm going to take a convex set in the product of Rns, or equivalently the space of matrices, and I'm going to symmetrize this set with respect to one vector in Rn. And this is what's called the fiber symmetrization. So, is it more or less clear so far? Okay, so the thing is, let us remember how Steiner symmetrization is defined. Symmetrization is defined. I said the thing about the intervals going down, but it can also be written in this way. If you fix an x in the orthogonal space and consider all the intersection of this vertical line with k, this will be the set of points in k that can be written in this way: x plus a multiple of b. In this coefficient, alpha is going to parametrize all the points in this fiber. Okay? Okay. Okay, and then if you take any two points in the fiber and you take the vertical coordinate, alpha, and you do this, alpha prime, beta minus alpha over two, then this is going to be maximized when beta is the maximum and alpha is the minimum. So this point will be up here, and it will be minimized when it is the other way. Okay? Well, if you have now I'm going to change for a similar picture, but it is now in a different space. Sorry. In a different space. Now, the space is a set of matrices. I have exactly one vector, and then I create this space, v times v times v. So this will be the set of matrices, all of whose columns are parallel to B. The orthogonal hyperplane, the orthogonal space to that will be the set of columns, the set of matrices, all of those columns are perpendicular to B. And since this is the And since this decomposition is a direct sum, then every matrix can be written in this way. The same way as what we have here in the Steiner concavity hypothesis. And then this space is no longer one-dimensional. It is m-dimensional. The number of columns. And the intersection of the fiber with the convex set is no longer an interval. It's a convex set in the space of In the space of in an M-dimensional space. And when you do the same formula, you take all the points here written as x plus v times s, you think of s as the coefficient, and you take, so the number, the set of coefficients for which the point lies here is a convex set here. You take any two of these points and you create s minus t over 2. So what will happen is that you You are replacing, if you do this formula for all the pairs here, you will be replacing this intersection with the half the difference body. Because this coefficient, s minus t over 2, will be in half the difference body of this set. Now, when you do this, you're not preserving the volume, because half of the difference body is larger than Difference body is larger than the set because of Luminkovsky inequality. It's usually larger. Then this symmetrization will not preserve the volume, but it will preserve other quantities. So, okay, we have this operation on the set of convex bodies in Rn to Dm. We can extend it to functions by applying it to the level sets. And this is the simulation model. Okay? Okay. Um I forgot to put the timer, so what timer do I have? The moment, good. Okay, then let me review the basic properties of this fiber symmetrization. I have to say this fiber symmetrization was defined by Gardner and Bianke, I think. And Groch, yeah, thank you. Um uh and it was studied by Ugively and um Bianke, I think. Very unique also. But okay, this is a particular case because I use it with this structure, with the structure of matrices. Okay, so the volume, as I said, the volume goes up when you take the symmetrization. If n is equal to 1, so we are talking about only one column, then a column vector is the same as a point in a rand, so we have the same steinry symmetrization, half the difference body of Half the difference body of an interval is the same integral center. So it's that is Borotone and preserves complexity. Sorry. If you have many convex bodies in spaces of matrices and you take the product, the product being concatenating these matrices one next to the other, then this semi-triziation gets along with the product structure. So in particular, if you have a convex So, in particular, if you have a convex body here, which is a product of many convex bodies in a ring, it is the same as symmetrizing each one, different. And, okay, this is the important part, this is the connection with Paulis-Pivovar theorem, is that if you have a convex body, then the characteristic function of the convex body is tiny-concave, if and only if the body is invariant under fibers an intersection. And this is why, in their inequality, you This is why, in their inequality, you have the same function, because the function symmetrized was the same function. All right. Now, of course, if you have a matrix multiplication on both sides, you can ask what happens to the symmetrization. So, to the left is the usual, you need an orthogonal matrix, then it's the same to symmetrize and then rotate, then to rotate and then symmetrize with respect to the rotating vector. With respect to the rotating vector. Here, when I write a convex body times a matrix, it's that. But remember that the points in this convex body are matrices. Now to the right it gets very interesting. If you have any matrix, any matrix, and you multiply to the left, then the fibrosymmetrization will commute with an inclusion. And if the right Inclusion. And if the rank is maximal, then it will be an equality. So, in particular, for any invertible matrix, you have commutativity. And this is a large group of invariants. When n is 1, the multiplication to the right is only the scalar, taking a dilation, right? But here we have more. And if you consider the function, this a bar is the function multiplied to the left by a matrix A. Left by a matrix A, then the pre-images and the fibrous symmetricization, they commit. You require M to have a maximum round. And with these properties 7 and 6, it's very easy to get in equality. It follows from just doing it at the level of sets and then doing the level sets, and it goes very well. And using this invariance, you can This invariance, you can add in the end the linear functions. Then, okay, the idea was to do this. Okay, sorry. The volume is not preserved. This is very bad. But some other quantities are fiscal. So, for instance, the mean width is preserved. Actually, it goes down. Well, more generally, any LP norm of the support function. Norm of the support function will go down. And if you have the projection, you take the projection to the end, one of the coordinates, one of the columns of the matrix, then the volume of this projection will go down for your symmetrics. So in particular, for instance, the right-hand side in the Lewis-Whitney inequality is preserved on the phylosymmetries. Just to give an example, Just to give an example. Okay, so the idea was to use this inequality for very specific reasons. So in the end, the applications were not what I expected. There are some applications of this, but well, I will tell you what I intended to prove and what ended up ended up proving. But anyway, I think this is a quite elegant generalization. And and it helps explaining what what happens with this. Help explaining what happens with this static occurrence. If you have a matrix, you can think of a matrix as a linear operator from R L to R M and of course this as suppose you have two convex bodies in our You have two complex bodies in Rn and Rm. So if you assume K and L are symmetric, then these are banana spaces, and you can ask for the norm. So the norm of X will be the norm of X as an operator, and the set of matrices for which the operator norm is less than one is this set, set of matrices such that x, so if you have a point in K, then So, if you have a point in k, then x times w lies in there. Instead of linear function, I'm taking k inside l. One of the reasons I wanted to do this inequality was because I wanted to apply it to this set and maybe return obtain some information about volume rating. So, what we have is that it's very easy to prove that the fibersymetrization of BKL is inside BKS. Inside BKSL, where this is fibrous ammutization and this is steinous ammutization. So as a consequence, if you take volume, you get that the volume of the, you can symmetrize this set and the volume of the one. So um and with um duality argument you can do it also for for K. So if you if you have this BKL and you replace K or L. And you replace K or L with the Euclidical of the same volume, then the volume of this operator norm will go up. So we have one can compute the max volume of this operator norm by just taking K star and L star and computing this number, which can be done. The theorem is now here. And then down here, well, this requires only fiber signification, okay? But thanks to the GraphQL. But thanks to the Braskamp-Libluttinger yes, the Ross's Brascam-Libluttinger inequality, you can also symmetriz functions defined on the operator non-world. So for instance, if you take any standard concave function, like the determinant, you have to take a negative power, so it is concave and not convex, then the integral is going to go up and we replace k over by the incident both of the same volume. So the idea was to use this to find So the idea was to use this to find a linear functional with a bounded operator norm and a bounded from below the terminal, which gives information about the volume rate. Unfortunately, the bound is not good. I had other applications are, for instance, in a previous work with Dylan Lagarde at Elliott and Microboros and at the VGA, what I learned about this fiber symbolization. What I learned about this fibersymmetrization because it helped us to prove a very projection unequality for this. Now this set, what it is, if you have a convex body in Rn, you can construct a convex body in Rn to dm, which is a higher dimensional with this formula and the justification for this set is that it is related to the leaven set of the co of generalized covariance function. In realized covariance function. So using this viral symmetricization, we can prove an analog of the petitory. It's another application. And if you want, you can also do the same trick here. The integral of any standard concave function over this set will go up under February 7. Another application is this difference body defined by Schneider, which is kind of a convolution body, is another way of constructing a high-dimensional Another way of constructing a high-dimensional object from a lower dimension, so you have k and consider all the possible set of translations, here the xi are translations, in such a way that all the translations, so you translate K many times, many times, and you consider all the multiple translations for which all the translated sets have a common intersection. So, this is kind of a convolution body. Convolution body, and one can prove that this isoperimetric inequality holds with forensic metrization. Unfortunately, the volume of this set goes up, so this inclusion doesn't help me to prove an isoprene inequality with volume, but it does with mean. Okay, so the ring helped, and so I'll stop here. Thank you very much for your attention. So does a group also as a version of Blast complete inequality these operators? I didn't go there. But this is just one ingredient of LASComplete. Then you need something like extension of RAS complete when you put operators, right? Yes, but I don't think it's related to it. I I looked into it and I I Into it, and I cannot explain it right now, but I came to the conclusion that it's not okay.