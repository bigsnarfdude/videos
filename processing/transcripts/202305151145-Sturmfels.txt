Barend was supposed to lead this session, but well, you know, he's not here right now, so we're gonna try our best to emulate maybe something. So I think so. The first thing Bernie wanted to do was a round of introductions. So to get to know, you know, maybe you don't know every other person that's here. Every other person that's here. And so, yeah, so we were planning on going doing around introductions, both in person and online. And just, you know, say your name, what's your position, where, what university, and what are maybe your just like two, three keywords about your research interests. And I guess that's what we want to start with, right? Oh, yes. And Bern wanted to. Yes, and Bern wanted to say: if you have a favorite algebraic variety in algebraic statistics, please you had to say it. What's your favorite variety in algebraic statistics? Okay, so how should we do it? Should we start online or in person? Maybe we can start online. So I I give you the I give you the moderate. Maybe I can start. Great. So, hi, I'm Elena and I am an assistant professor at the University of British Columbia in Vancouver. I work in algebraic statistics and causal inference, density estimation, tensor methods, and I have a favorite variety from algebraic statistics. Favorite variety from algebraic statistics. And unfortunately, I don't think anyone will talk about this kind of variety this week. So I will tell you about it very briefly. So maybe a lot of you are familiar with the variety for a Gaussian graphical model. So all covariance matrices that are consistent with a given, let's say, directed graphical model. So this was touched upon by. Model. So this was touched upon by Thomas this morning. And so Gaussian graphical models, they're actually linear models where every variable is actually a linear combination of the causes plus noise. So this is also exactly what Anna was talking about, but in the graph for Z. And so here, if we assume that the noise is non-Gaussian, so the epsilon i's are non-Gaussian, then we can talk about not just We can talk about not just the covariance matrix, but also the higher moments. So the higher moments will give us more information. And in fact, you can identify, if it's a DAG, you can identify it uniquely from data, not just up to the Markov equivalence class, but you can actually identify the graph from higher moments. And my favorite variety the last couple of years is the set of all covariance matrices. Of all covariance matrices and third-order moments that arise from such a model. And yeah, I can see Ros√© is there, so she's smiling. We've worked on this and we studied this for polytrees. So these are DAGs whose skeleton is a tree. But the question of what this variety is for any DAG or any directed graph is completely open at the moment. So I think it's a very interesting algorithm. This discussion, okay. So, should I pick someone from the online people? I think wait. Well, Anna. I know she was here. Yes, hi, hi again. I'm Anna Siegel. I'm an incoming assistant professor in applied math at Harvard. Some key words. I just said how I'm interested in linear causal disentanglement and then more generally I'm interested in questions about causality and matrix intensity compositions that arise and other topics in algebraic statistics that I'm also interested in have to do with maximum likelihood degree. With maximum likelihood degrees and connections to invariant theory. My favorite variety in algebraic statistics, well, maybe it's cheating, but I'll say the variety that I mentioned just briefly at the end of my open problems list. So I think it's Would be interesting to find out what this variety is. So, the variety of all tuples of symmetric matrices that lie in this model I described for a particular latent graph. So, we know some of the equations for this variety. So, we have our topple of precision matrices. And for example, we know that Know that whenever a node is a source of the latent graph, so it has no parents, then when we intervene at that node and then subtract off the precision matrix where we haven't intervened, this difference is rank one. So this gives us some equations. They're the two by two minors of this difference, like theta i minus theta z. Difference like theta i minus theta zero. And we also know that whenever we subtract some slice theta i and some slice theta zero, whenever we compute theta i minus theta zero, this difference will be ranked at most two. So that also gives us some equations for this variety. So those are some minors to get us started, but we don't know. Said, but we don't know, for example, for which graphs are these older equations, what other equations are there, like what is their structure and what do they tell us beyond these rank constraints? So I'd be very interested to find out. Anna, do you know the dimension, the degree? No. Okay. Okay. I guess the dimension we can compute from the identifiability result. So we know that its dimension should be the same as the number of parameters in the model. So then what's that? The number of edges plus the number of nodes plus P times D, like the mixing, the mixing matrix. The mixing matrix. So that should be the dimension. But yeah, I don't know anything about the degree. Do you know if the three by three minors are enough to say that there exists a graph on the latent variables? There exists latent variables. So does there exist a graph? I guess that's the case where it's the full graph, like the full DAG with all d choose two edges. Edges. So, yeah, maybe that would be the first one to look at. But there, you still have a source node. So, it's still the D times D minus one. Say again. If you add all possible edges, so D times D minus one, is it just the three by three minors? Oh. Oh, you mean if it's not a DAG, if it has the reverse? Oh, I don't know. Oh, yeah, interventions then are hard to think about. Yeah, I um yeah, I don't know, but yeah, that would be that would be interesting. Yeah, I mean, in principle, you can allow cycles, like this matrix A doesn't have to be upper triangular, you still have this parametric description for the Description for the model, like H, I minus A, omega, blah, blah, blah. So you want to implicitize that map for different choices of A and omega and H and everything. Great, thank you. So who else is there, Helena? There is Kim. If Kim wants to. Who are you, Kim? I mean, we didn't tell people we were really picking on them, so I don't know. Okay, maybe they're not there. There's Ruri, Rudy. Okay, sorry. Okay, Rudy is here. Oh, she's in person. Oh, she's in person. Yes. Oh, okay. So then never mind. She's because she's also online. Okay, and then there's Yannike Oldekop. I don't know how to pronounce it. Yanike, do you want to say who you are? Hi. What? I wasn't there. I don't know what to tell. So who are you? What your position is, and what do you work on? And what do you work on? What's your favorite right? I'm Yanniko. I'm Carla's PhD student. You can ask him. Okay. Do you want to say what you work on, or you're still starting? We are working on like-neutry of reflexive polytropes. I think that's all from the online. Okay. Well, yeah, well, thank you for that. And so, yeah, okay, now let's continue in person. And yeah, the online participants can also hear. So maybe we start. Well, I guess I should also say something. So, well, I'm Carlos. Well, I'm Carlos, and you know, so I'm assistant professor at Teu Berlin. And my position, the group is called Algebraic and Geometric Methods in Data Analysis. So very in line with algebraic statistics. I would say my research interests would be, I would mention, okay, graphical models is one big one, also maximum livelihood degree. Livelihood degree, and I would say method of moments and moment varieties in general. And I guess I would choose Gaussian moment variety for my favorite variety in algebraic statistics. And these are formed by taking all the moments of a multivariate Gaussian up to certain order. And these are very interesting because basically understand. Basically, understanding the secant varieties of these varieties is the same as understanding whether the method of moments works in statistics and computing precisely fibers of the image that's parametrizing these varieties is doing the method of moments. And so, there are many interesting questions and that are still unsolved. And yeah. That are still unsolved, and yeah, understanding these varieties would be great. So, I think maybe, yeah, let's see. I think maybe we'll have Joe will be here tomorrow, hopefully, and he will maybe tell us a bit more. He has been doing works on this. So, you can get these varieties, and you can get tensors, and then tensor decomposition then connects again to things that Elena was saying, for example. Yeah, okay. Yeah, okay, so that's me, and so maybe Liam, you can now go. Okay. Hi, everybody. I'm Liam. I'm an assistant professor at KTH. I'm part of the combinatorics group there, but also this math for data and AI group that we have. Yeah, I work. Yeah, I work generally in sort of applications of combinatorics, and so this relates a lot to algebraic statistics. And general, I'm happy. A lot of what I do has a lot of connections with graphical models and causality, these sorts of things. I'm generally happy, though, when somebody comes along with a nice combinatorial problem where there's some algebra at play and a solution to it gives us some nice application. Nice application. So favorite variety in algebraic statistics. I don't know. I like conditional independence a lot, so I'm a fan of the independence model. It's also beautiful because I can see it, which is nice. But typically, my way of thinking ends up making it so that there's a polytope in the picture at some point or other. So I'm quite happy. So I'm quite happy if there's a Tauric variety there as well. Yeah. Anything else? No? Okay. Hello, this is Rudy Yoshida and I just got promoted to professor at the university or not university, I mean Naval Postgraduate School and I'm in the OA department and yeah, I work And yeah, I woke a lot of variety things and I started thinking, like, I don't know what I'm doing. But so for with the students, I work a lot with like kind of more like applications of like optimization to reinforcement running a lot. So for example, like recently, like Decently, like he's planning to graduate very soon. But so he's a marine student, marine officer, and he implemented such a nice actually prediction, like optimization tools, just basically finding that where to run, where are the best route to go for the special force, and also that, like, where, you know, anyway. So the program. Anyway, so that program is gonna be implemented in an actual like um tablet where Vichy Marine Special Force Officer is gonna use in a fall this fall. And he got an award as a CCS award like last week. So I've been doing a lot of interesting stuff. I love those. But at the same time, of course, I love Polytop. So, you know, I love. So, you know, I love that Toriku variety, but also that, like, in a research wise, I do a lot of that, like Tory space things. I love developing like machine learning tools or like data analysis over the space of, you know, phylogenetic trees. So that's going to make it like, you know, I like tropical grass mania, especially the space of. Especially the space of ruthenometrics. And, you know, maybe I've tried to understand some kind of like intersection with like tropical hyperplane or like, you know, a female web set because sometimes they don't, they, you know, famatover set, they don't have to be contained in carogulasmoney. So anyway, so like that. So I do a lot of stuff, but anyway, so that's all. That's all. Hello to everybody. My name is Ernesto and I'm a student in Madrid, in Universidad Complutense in Madrid, but I also live here in Oaxaca, so it is a luck for me to be here. And I have a position as a professor in the university here in Oaxaca. University here in Oaxaca. And well, I work on my PhD thesis. It has to do with reconstructing a phylogenetic tree from data. And I studied some tools to reconstruct phylogenetic trees. I could say that. I could say that my favorite variety comes from those that are called livelihood varieties. And well, that's all. Hi, I'm Danai. I'm at KTH in Stockholm. I'm working with Leon and a project we receive. A project we recently had was in causal discovery, so very close to anostopic. So we wanted to, given a system, a complex system, you want to understand the causal relations between the variables. And then instead of focusing on graphs and DAGs that capture the conditional independence relations, we worked with understanding only the unconditional independence. The unconditional independence and then developed an algorithm that eventually you can use to completely determine the structure of the structure of the estimating DAG for the causal system, let's say, because working with Working with the unconditional independence relations, you can directly understand the possible source nodes in a DAG, and then you can target these in the interventions. And along this way, we came across a variety actually that essentially finding a Grubner basis for it allowed us to move between different Different graphs representing essentially sets of unconditional dependence relations. And so that was hi, I'm Jose. I'm at the University of Wisconsin at Madison, and I'm in the math department there. And have a guest appointment in engineering and collaborate with some people there. Some keywords are ML degree, ED degree, or maximum likelihood degrees, Euclidean distance degrees. Degrees, Euclidean distance degrees, just applied algebraic geometry in general, but also Euler characteristics and Galois groups. And I think the favorite variety is three by three rank two matrices. So this has a likelihood correspondence in ML degree 10, and you can compute it using symbolic computation, numerical computation, as well as using some Euler characteristic methods. And even more, it has a nice interesting Galois group related to maximum likelihood estimation. So that's my favorite one. So that's my favorite one. Thanks. Hi, everyone. My name is Rose. I'm a postdoc at the Central da Rosarca Matem√°tica, which is a research institution in Barcelona that kind of brings together the different math faculties. And I'm a postdoc of Marta. And I started working in algebraic statistics like two, three years ago. Statistics like two, three years ago in graphical models with the project that Elena was mentioning before, and also identifiability questions with Carlos and Matthias and some other people. And now I'm working in phylogenetics. And I don't really have a favorite algebraic variety. I think that my favorite variety is the one I'm working with at that precise moment. So I don't know. Right now we have a project with Angelica and Marta. With Angelica and Marta, where we are studying some evolutionary models that are a bit more general than these proof-based models and equivalent models and so on. And yeah, at the moment, we only have equations for trees with three leaves and four leaves. So we're working on it. But I would say that right now, those are my favorite varieties. Thank you. Thank you. Okay, so I am Guido Montoufar. It's really a pleasure to be here. I am an associate professor in mathematics and statistics at UCLA, and I spent about half of my time in Leipzig at Max Bank Institute in Germany. I am broadly interested in mathematical topics of machine learning, such as problems of the representation of functions with neural networks, the optimization of their parameters. Optimization of their parameters, how to relate parameters and functions in such an optimization procedure, and how this instills some preferences in those solutions that you might obtain. So we have a project called Deep Learning Theory and another project called Combinatorial Implicit Approaches to Deep Learning, where we particularly try to see how combinatorics and algebraic geometry or algebraic statistics can be important tools to try and advance in those problems. And tools to try and advance in those problems. So, some of my favorite varieties, maybe things related to Latin variable graphical models, I suppose. I have worked quite a bit on the restricted bulbs machine in the past. That's something that is very close to my heart. And yeah, so that's one. Then generally, function spaces of neural networks, and some of the things that we have been looking at more recently are so these products of matrices with some constraints and that end up giving rise. Constraints and that end up giving rise to discriminant hypersurfaces and some subsets thereof that turn out to be very interesting. So I'm very enthusiastic about them. And oh, maybe I can also mention another one that I think is super interesting. It's more like a feasible set. Well, it's like a semi-algebraic variety, if you want. And that's the set of feasible value functions for a sequential decision problem. Decision problem and that occurs in reinforcement learning or Markov decision processes when we don't have full observations. So, and happy to chat more about those. So, I'm Daniel Bernstein. I'm an assistant professor at Tulane University. I started my research life in algebraic statistics. Research life in algebraic statistics and then started getting really into rigidity theory. And that took me back to algebraic statistics via maximum likelihood thresholds. But broadly, I really like solving combinatorial problems using like geometric methods, especially stuff in like tropical geometry and matroid theory. And I guess my favorite variety right now is probably the projective plane over F2 because I recently got a Of playing over F2 because I recently got a tattoo to my arm. So, yeah. Thank you. Hi, everyone. I'm Lara. I'm assistant professor at the Unam Math Institute based here in Oxaka. I'm not working in algebraic statistics, sorry, but on closely related topics, so I do tropical geometry, toric geometry, and cluster algebra. Geometry and cluster algebras, and anything that is related to those, or especially where these topics can be applied. Two projects that I have right now that are really exciting are applying tropical geometry to solving linear PDEs and hopefully at some point non-linear. And the second project is using Grubner theory and cluster algebras to compute scattering amplitudes in particle physics. And my favorite. And my favorite variety, I'm going to cheat on that one. I just learned, well, I knew it was important in Azure Statistics before. I will say the Lagrangian Grassmanian that appeared in the first talk. And well, it's even more, well, it's even nicer sibling, the Grasmanian itself, that is a cluster variety, has toric degenerations, lots of combatoric. So I like all these things combined. Hi everyone, I'm Angelica. I am a postdoc at the Centre de Reserca Matem√°tica, the institute that Jose was mentioning before. I am a postdoc in the phylogenetics group working with Marta as well. I am interested in generally in applied algebraic geometry. I like to be. Geometry. I like to use algebraic geometry to understand data. So I've had projects in other things in computer vision, and now my first approach to algebraic statistics is in the context of phylogenetics. My favorite variety in algebraic statistics, I would have to say some phylogenetic varieties because those are the ones that I, you know, that has been my introduction to algebraic statistics. Statistics. But in general, I am also a fan of Grassmannians because they have appeared in my previous projects in computer vision, and I've spent a lot of time thinking about Grassmannians, so I like them. But yeah, in algebraic statistics, phylogenetic varieties. Yeah. Is this working? Okay. So, hey, everyone, my name is Pratik. I'm a postdoctoral researcher at KDH. I'm working with Liam on graphical models and causal discovery. Models and causal discovery. I've recently started working on causal discovery and variety. Yeah, I like stuffs when stuffs are Tauric. So I'll have to say like Tauric varieties coming from graphical models. So yeah. Hi, everyone. My name is Pardis Semnani. I am a first-year PhD student at the University of British Columbia in Vancouver, and I work with Alina. So I think. So, I think I've done much less work than most of you here, but since the beginning of my master's degree, I've worked on tensor decompositions a little bit, density estimation, and most recently I've been working on causal inference. And the specific thing that we were working on is causal inference when the causal graph might have directed cycles. So, I guess these are things that I've been interested in. And my favorite variety, I was thinking about it. So, the very first The very first research project that I did as a master's student was about decomposition of tensors, which we, I guess, called orthogonal and symmetric tensor-trained decompositions. And we were working on which tensors have such decompositions. And we found we were working on that variety. And I guess, like, because it was the first in my research, it has emotional value to me. So it's my favorite. Yeah. Hi, I'm Ben. I'm a post. Hi, I'm Ben. I'm a postdoc at the Max Planck Institute in Leipzig for like two more weeks, and then I will be moving to Munich to work with the Matthias as a postdoc. So everything I do somehow relates to a graphical model. So I work a little bit on like model distinguishability. I love finding like vanishing ideals or growner bases of statistical models. And then I also love polytopes like Liam. So I'm always happy to out of there, I'll try to find a polytope related to it. Out of there, we'll try to find a polytope related to it and understand things like the Grosner basis of the polytope in terms of the combinatorics of the graph or whatever discrete object we're parameterizing the model with. And I also really like phylogenetics. So my favorite variety, without a doubt, is the CFN variety of a four-leaf unrooted tree. It's extremely beautiful. It's generated by two binomials, which you can kind of combatorily read off from the split of the tree. So I think, you know. So, I think, you know, I love the phylogenetics varieties, but the CFN model, which I don't know if it will appear this week, it has the most beautiful combinatorics. So, okay. Hi, I'm Tobias. I'm a postdoc at Aalto University currently with Cayukupias. And I like conditional independence very broadly. So my PhD, as you learned this morning, was on Gaussian conditional independence, especially the non-graphical models. Models, non-graphical conditional independence models. And recently I've been getting into graphical models as well. Other things I like are matroids and information theory in the discrete setting, so the non-Gaussian discrete setting. And I also like software and data. So that's very much in line with what Thomas was talking about. So he was my advisor. So that's I got it from there. And I guess my favorite variety is. My favorite variety is the critical locus of the Engleton functional on four binary random variables. So you have this Englton functional. It's a linear combination of the function x log x, where for x you plug in lots of linear functions in the probability mass function. You take the derivative, you get a rational function, you clear denominators, you get a polynomial system. And I really would like to know in 16 variables how this In 16 variables, how this looks like. And if you put it into Bertini or other software, it either crashes or it tells you, oh, this variety is totally a union of 3,000 lines. And I don't believe that. I would really like to know more about this variety. Hello, everyone. So my name is Voteme. So I'm a full professor in KU Leuven in Belgium, but with a tenure track position, not yet tenure. Not yet, Anyo. So, what do I like? So, in the algebraic statistic part, I very much like conditional independence models, usually with hidden random variables, because you no longer get binomial or toric ideals. And I'm in particular interested in kind of understanding these varieties, you know, whether they're irreducible, whether you can write them as irreducible decomposition and in connection with matroid varieties. And then you can talk about their realizability and so on. Their realizability and so on. So, that's one direction. So, another kind of project that they got involved recently is about rigidity theory. That, as Daniel mentioned, like it has also very nice connections with algebraic statistics. So, I mean, it's kind of easy to explain. Let's say you have a bunch of points like in Rn, and then you have a measurement that tells you what are the distance, like maybe pairwise distance, like among them. And then the question is kind of identifiability question: can you kind of? Question: Can you kind of identify the locations of these points up to certain symmetric group actions? And then this problem could be related to this matrix completion if you're working with the maybe Euclidean distance, but you can also, and it's related in that sense to graph rigidity, but you can also introduce this hypergraph rigidity that shows up like a lot in applications in engineering. And then it's very much related to this tensor decomposition, sensor completion problem. Completion problem. And also, one particular application would be what Carlos mentioned about the second variety showing up, like you know, in the study of the moments of Gaussians and so on. Yeah, I think I had the third thing to say, but I forgot about it, but that's fine, I guess. Yeah. And yeah, my favorite algebraic variety, I guess, would be so these conditional dependence varieties, but in relations to matrix varieties that I've been working on. It is that they've been working very recently. Thank you. Carlos asked me to go to the front. So I'm Piot. I'm an associate professor in the Department of Statistical Sciences at the University of Toronto. I mostly work in multi-brade statistics. I like Multi-verted statistics. I like all sorts of combinatorics and geometry that can be applied in statistics. I like graphical models, graphical models with latent variables, many things that have been mentioned already. I got light even. Thank you. What else? I will tell you a little bit about my currently the most favorite variety tomorrow, but it's also related to many of the varieties that. Related to many of the varieties that people mentioned. These are varieties that are related to varieties defined by zeros of an inverse of a symmetric positive definite matrix, but I'll be talking about varieties that are defined by zeros on some other transformations of the covariance matrix. Hello, my name is Luis Garcia Puente. I am professor of mathematics and computer science at Colorado College. I've been doing algebraic physics for a while. So then I guess I'm interested in many things. I guess I'm interested in many things. I started being interested in graphical models and then phylogenetic networks. And so with various groups also, I've been interested in biochemical reaction networks, some things about, you know, real algebraic geometry that appears in some questions there. And so now I'm coming back to some phylogenetics. Phylogenetics things. So, we currently finished, or we are finishing a project on computing nuclear distance degrees for phylogenetic trees and phylogenetic networks. And also, I've been interested in these sort of very old-fashioned algebraic statistics, like the Markov models and applications. Applications. So, so with I, so my current job, I have been there for a couple of years only. So, one of the things is with some statisticians, we started looking at applications of these Markov bases into some very applied problems in psychology and something called differential item analysis. So that's it. Hello, I'm Marta Cazanelas. I'm a professor at the Polytechnical University of Catalonia in Barcelona, and I work in algebraic phylogenetics. And now I realize I've been working there in this field for a while, like maybe since 2004, so 2004. So, and I like the theoretical part of that, but also the applied part. And I'm always looking to develop methods that may help biologists to select or to choose the best phylogenetic tree for their data. And my favorite algebraic variety would be a phylogenetic variety, but in contrast to BERN, I prefer the cumulative three-parameter model. I prefer the Kimura 3 parameter model on four leaves, which is also a toy variety also, because I prefer it because it's my toy example and it's the variety I used to play every day because everything works there. So easy to work on that variety. And then when you go to other models, then it doesn't work everything. But but but yeah, that would be my favorite one. I will tell you more about this tomorrow tomorrow. About this tomorrow. It's back tomorrow. Okay, thank you. So I think that was everybody. Okay, I think that was everybody. So thank you. Yeah, nobody else joined online in the meantime, right? Okay. So yes, so I think that was the main part for what we were thinking. The other thing, so The other thing, so I guess so. We'll have, as we advertise, we have these impromptu talks and discussion in the evening and well, afternoon still, I guess. And we, yeah, so we would like if you have something in mind that you would like to say, so 15 to 20 minutes, you have a topic you want to tell us about, then that will be great. So, I'm gonna be passing this. So I'm gonna be passing this, and you can write your name. And if you have already like a tentative title, then you can also add it. And yes, so then this will be, we're gonna try to schedule it from starting from today. And I guess so we have one tomorrow and one on Wednesday as well. And okay. And okay, what else? Should we say, oh, yeah, so we're gonna have, so there's gonna be, after we're finishing here in like five minutes, then we have a little break and then there's lunch. And then we'll come back and there will be the ideas to have a bit of a discussion of problems and try to work in groups on these problems. So I think some people already mentioned some key ideas, some words that could be used. Words that could be used to have some problems that the speakers also from today gave us a few ideas on problems to work on. But if you have something on your own, so we invite you to think about it during lunch and then you can tell us in this session. And yeah, so I think there is Ignacio maybe here. I would really encourage him to introduce himself. So maybe Ignacio, are you there? Don't be shy. How is it? I don't have a mic in SPC. Well, maybe you can write to us then. You can write in the chat who you are. At who you are, and what is your position? So it's like who you are, what is your position in what university, and what are your research interests, and what is your favorite variety in algebraic statistics. Do you think is there a way to open the chat? Maybe I don't know to see. Or maybe I don't know to see. Or maybe some yeah, he can type it and maybe Elena can write it. Oh, I can do it here. Okay, here. So, okay, then, yes, we would be happy to see your response. So, we can see already that your name is Ignacio Chavez Ustaita Rodriguez. And yes, just to. Yes, just to and you're yeah, you're most welcome, so don't feel he's still there, right? You're ready, you're ready, maybe he flat. Okay, okay, nice, nice. We have something, okay? So he says he's going to be a PhD student with Frank Wetger at the Frank Wetker at TU Eindhoven, again. Thanks, Eindhoven, right? And he's currently studying for his master's at Uncirad Aut√≥noma de Madrid and doesn't have a lot of experience in algebra statistics yet. Oh, yeah, but we know Frank in the community and yeah, so very nice. Yeah, welcome. Exactly. Very good. So, yeah, I think so. It's going around, so it's good. So, I think. So it's good. So I think we'll announce it after lunch. What's the schedule there? So anything else, Elena, we should say? No. Yeah. So, well, then I would thank again the online participants for joining us in this discussion and introductions. Discussion and introductions. And yeah. So, yeah, hopefully, you can join for some of the other online events that we're going to be having here. And yeah, thank you again. So I guess here, yeah, we'll then just have a break and then prepare for some nice lunch. Okay, thank you. 