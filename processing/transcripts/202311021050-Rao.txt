Next speaker will be Suicini Rao talking about new methods for artifact detection, a very large gray sky survey case study. Okay, good morning everyone. I'm not going to say the name again because I started to be large. So before we talk about what artifacts are and why I'll be detecting them, a little bit of background is transience. So transience is anything that goes off and on, on and off. And so why do we look at them? And so, why do we look at them in radio? It's because some of them can be really traced really well in radio, like relativistic jets or supernovas or stuff like that. And so, if you want to find out more about them or you just want to find more of them in the sky, what do you do? You do a sky survey and you figure out all of these are from a sky survey and you identify much more of them and you can do a population analysis case. So, one such sky survey is the very last. One such sky survey is the Very Large Array Sky Survey, which uses, surprise, surprise, the Very Large Array Telescope, which is in New Mexico in the States. And it is an interferometer which has 27 antennas in an inverted vial shape. And it basically observes the sky in 2 to 4 gigahertz frequency, and it has a resolution about 2.5 arcseconds. And so, one of the data products that you get from this survey that is primarily used for Primarily used for slow transients is the quick look images. And these are the images that I'll be working on. And so if you look at these images, you see a lot of these linear structures in and around big bright sources of relatively higher or lower flux. And so these are called, or what we call as linear artifacts. So why are they important? Artifacts. So, why are they important? One, what's the problem here? Is you see when you run a source detection algorithm, all of these are identified as sources or components of sources. So, but you can see that most of these things that are on these streaks are just local maxima on these streaks that you see and not real sources. So, that's a real source, that's a real source. That's a real source, that's a real source. All of these might not be. So that's one of the problems when you do a source detection, when you run a source detection algorithm on this, are you finding real sources or are you just finding local maxima pretending to be real sources? And the other problem is that when these streaks overlap over one of your sources, they kind of enhance or suppress the measured brightness of these sources. So your measurement will be kind of off. Of so, what's causing these streaks is there are two reasons that we can describe. So, one of them is coming from the way synthesis imaging works by itself, the way the radio sky is observed by an interferometer. So, you have this Y shape of all the antennas in the telescope, and that gives you a weird point split function. You have a weird point-spread function for the telescope. And this point-spread function, when you convolve it the image that you see, you get something like these streaks. So these streaks can be explained by the point split function itself, but you also see this streak which is highlighted over there. That can arise because of two nearby sources or two nearby components that can create that. Components that can create that streak over there. So we know what's the problem and we know where it's arising from. So, how do we solve it? We identify these streaks. So, we use something called the Hough transform, which was invented by Paul V. C. Huff, which basically what it does is in your image space, if there is a line, it considers that line as a collection of points, and each of that point is transformed into a sinusoidal wave in the row. Into a sinusoidal wave in the rho and theta parameter space. And so the rho is basically the distance of the source from the origin, and theta is the angle it makes from the x-axis. So every point in here is transformed into a sinusoidal wave, and the point at which it intersects gives you the rho and the theta of that line in the image space. So for example, you have this line here, which will give you a set of sign. Line here, which will give you a set of sinusoidal waves over here, and the intersection point is at around 60-degree angle. And this, when I say 60-degree here, I mean position angle, which means zero at the vertical axis, and you count counterclockwise. Your angle is counterclockwise. So, if you have another line, which is panel to that line, you have another set of sinusoidal bays which are at the same angle but at different position, at different row. A different row. We have another line which is again at a different angle in a different row. So, why can't we just use this and be done with it and say, okay, we've identified all these streaks? So, one of the problem is thick lines that you see in the images, you want to identify them as thick lines going through your source. But when you do this with just the Huff Transform, you get multiple single pixel streaks instead of a thick. Single pixel streaks instead of a thick line. So that's one of the problems. And you identify a lot of false detection as well. These are all the lines that are identified instead of just the main ones. And if you just look at the image here, you see a lot of parallel lines in the images. You want to be able to take advantage of that while finding out bright streaks that are going through your source. So what we do. What we do is we first take when we have a catalogue already available of all the components or sources identified. And so for each of the component identified, you mask out all the other sources in that region. And you clip the image, which is basically converting an image into a binary image based on a specific threshold. And then you take an annular region around it. Around it. So that's your source region, and the background region is basically the same region with all the masking, but all the pixels are just one instead of a binary image. And so you take the Huff transform of the clipped and masked image, the Huff transform of the source and the background region. So this colored yellow region traces out the region that you actually look at with the source region against the Hub transform of the entire background region, which is in the blue. Of the entire background region, which is in the bluish-white rotor. So, here you know how we make use, take advantage of these parallel lines that you see. So, all these parallel lines will give in the Huff transform parameter space will give you these intersection points at the same position angle. So, you compress the Huff transform or collapse the Huff transform in the distance in the row parameter. In the distance in the row parameter, and you get a profile in the theta axis, you get a profile like this. So you can collapse the Hough transform and increase the signal-to-noise ratio that you identify these streaks at. But you see this general trend that you see here and these peaks at 45 and 135 exactly, that's coming in from the half transform itself. It's not because of the signals from the image, it's intrinsic to the It's intrinsic to the Huff transform itself. So, you want to be able to remove that, and that's why we use the background region. We don't have a general background subtraction that we see in regular stuff, but this is why we use background region is to subtract, to make the profile half-transform independent. So, you subtract that and you identify all these peaks. These peaks represent the angles at which you see streaks. At which you see streaks for that source. So, for example, let's take this 17.2 position angle. So, that's one of the streaks that's going through the source. So, we have the angle at which these streaks are going. Now, we need to find the width of these streaks. So, you take the column of at using the Huff transform of the entire image, you just take the column, theta column, at which you find that streak, and that. You find that streak, and that will give you this profile, which gives you all the position angles at which there are streaks in that source region at that angle. It does not need, it need not be over the streak, over the source, but it can be in that region. So, you want to be able to find, so my source correctly is one of the tails here on my side of the image. That's the component that I've taken. So, you want to be able to identify this tree. To be able to identify the streak that's going through that tail and not the one that is adjacent to that. So, you see, you have two peaks here, so you want to be able to identify this and not that. So, you identify this and not that. So, how do you do that? You take the row of the component itself and use that to find out which peak is actually going through your source. So, you have the width and you have the angles at which the streaks are going. At which the streaks are going. You identify all these streaks, and so you find out the brightness of the streak by dividing the streak into four quarters, and then you take the median of the brightness of those four quarters. That's just to make it more robust and to make sure there are no outliers in your calculation. And when you have this brightness of the streak, you subtract the brightness of the streak from your source. So now that your source brightness will be only from the source itself and not anything from The source itself and not anything from the overlapping streaks. And after you subtract the streaks influence from the source, you classify the identified components into whether it's a real source or an artifact that you see in an image. So that's the decision tree that we've used. It's quite long, so I'll just break it down. So you take the signal-to-noise ratio of the streak, sorry, the signal-to-noise ratio of the component before the subtraction, and if it's built. The subtraction, and if it's below 3, that means we're not even sure if it's a real source or not. So we don't consider them, we only consider the streaks which have a signal-to-noise of greater than 3. And next is what we do is we only do this analysis for the ones that we do find streaks. So because our analysis is based on streaks, we only do this analysis or classification for the ones that we do find streaks. So we compare, we now check the signature. We now check the signal-to-noise ratio of the component after the correction. And if it's below 3 sigma after the correction, but above 3 sigma before the correction, it's a telltale sign that most of the brightness that you are seeing is just from the overlapping streaks and it might not be a real source behind it. So you classify them as likely artifacts or non-real things. For the rest of the ones which are still For the rest of the ones, which are still above 3 sigma, we have done preliminary classification where we consider the number of streaks that are coming in from the point spread function itself. And we compared the strength of the streak and the strength of the source itself to classify them as highly likely, or low likely, or mid-likely to be an artifact. And so, some of the examples that I have. Some of the examples that I have here is this. So the VLAS looks at the sky, has looked at the sky in three epochs, so in three rounds, and of the same sky. So you can compare it between two epochs. So when you see this source here, there are other methods that have done the same classification, which have classified this source as an artifact. But when we compare it with two epochs, With two epochs, it kind of looks like it's a real source, and our method classifies it as a real source as well. Another example would be this source. The other method classified it as being a real source. Our method classifies it as an artifact or a non-real source. But when you see these, compare these images, you see that this might just be a local maxima at the point of intersection between two streaks. Streaks. So this is not a real source because it doesn't pop up in the other epochs as well. And such sources, you get like 2% of the catalog is such sources. We're less sure about the other ones that we've classified as higher. We're still working on it. So it's not all hunky-dory. There are some misclassifications in our method as well. So there are these real sources that are identified as artifacts or non-real sources in our method, and vice versa. And vice versa, there are artifacts which we identify as real sources as well. So, what are the limitations? Limitations is when we mask out all the components, we assume that all the signal that we have now is either from the streak or from the background noise. But if there are some components or some sources that they've missed adding it to the catalog or they've not identified those, those can bias your analysis and can cause some miscalculations. Miscalculations. And even if they do identify these components, there might be cases where they've not fit the ellipse fit for those components might not be good ones. So you might not be masking out the entire source, so that can affect your analysis as well. And we assume a uniform noise level when we are doing the background subtraction, calculating the profile. But there are regions where the noise is not uniform, so you get changes in the one lead profile where. Changes in the Mondi profile where it affects the background subtraction, and eventually it cascades down into improper subtraction and then improper classification. So future work, what we want to do is we want to incorporate more statistical and probabilistic way of classifying these sources. And in the catalog that we're using, there's something called the host information, which is basically linking or linking components to already identified sources. Components to already identified sources, we want to be able to use that in while we're classifying stuff. And currently, I've done this only for epoch one of the Very Large Area Sky Survey. I want to expand this to the other two epochs as well. So, in conclusion, what we have is we found around more than 5 million streaks around 3 million components. We've measured their brightness, we've corrected for the components' brightness, and then we've done. Components brightness, and then we have done a really little classification for it. Thank you. All right, we have time for several questions. Adrian. So given that some of your streaks are due to just the configuration of the VLA and therefore the shape of your synthesized beam and all that, for those kinds of For those kinds of streaks, can't you predict what angle they're going to come at in the first place and just do a straightforward search for those ones? Or does this buy you something extra about that? We actually did do that. So during our classification here, we calculate the number of predicted cyclope streaks. So we know the angles at which you get these streaks at. So that's how. These streaks are. So that's how we compare if there are streaks at those angles, and that's one of the parameters we use for the classification. So that's included there. But the other thing I would say is those streaks are not perfectly always at those angles because if there are nearby sources, those Fourier transform can get a little messy, and so the angles can slightly change. So it's not always perfect, but I think we've included that also into our analysis and given it. That also into our analysis and given it a range within that, but that's already included in the analysis. So, how do you decide your threshold? Like, what is the process to say, like, what threshold three? This is just three sigma, so I've just taken the standard three sigma. Okay, have you tried other sigmas to see can use, like make it more. Like make it more what's the word. Yeah, so I wanted to make sure that we're not classifying everything as low as in real sources or everything as high as in non-real sources. So you have to get a balance between that and because you have assumed Gaussian, three sigma is like a good starting point. So that's Starting point. So that's one of the reasons why I've chosen three. But we can definitely try out other thresholds as well. This is maybe more of a math question, and maybe I'm not pretty much figured out. But when you go back to the Huff transform and you have the image space and the parameter space, you have the equation of the line and the rho equals x cos. Yeah, exactly. So my understanding here is that, so the theta is just corresponding to this. So the theta is just corresponding to the slope of the line. Is that like the right? And that's why you get sort of this pinching of the points in the right-hand side because if you have a bunch of lines that all have the same slope. Yeah. Okay, so the theta is corresponding to the slope. But then where does the spread come from? Like kind of between the points? Is that meaningful in some way? The spread comes from the point that the line is this long. So the row of those points at different points Points at different positions give you. The x and y of those points changes, so the row changes, so it gives you as well. So, is that giving you some kind of sense of like the uncertainty in those? Is there, like you're kind of estimating the background that you want to get rid of, right? So, in doing this kind of transform, and in your method in particular, does it give you a better estimate of the background and some of the uncertainty in that? We've not used this for the background at all. We've used another image where every pixel is one, so you get a big... You get something like this, like the blue-ish-white one.