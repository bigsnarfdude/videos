Okay, so the last question was the perfect transition. Okay, and entropy methods are going to be the thing that glue the first two lectures and give you the rule to cook this explicit estimate of the constant. Okay, so before doing that, I want to first to thank Bruno, Pedro, and the Tupateo for the organizing the The organizing the conference, and I'm very happy to present this result because it closes a research program that started like 15 years ago, and I will try to make it short. Okay, so just to collect a little bit what we have learned so far. So we had Bruno's lecture, and it was a virational point of view with the main drawback that you get a non-constructive constant in the estimate. Another remark is, and maybe this was. Mark is, and maybe this was not emphasized a lot so far, is that we measure the distance by the relative Fischer information, which is a good notion of distance because it controls the entropy and the entropy controls some usual LP norms. But it's a stronger notion of distance. In the lecture of Nikita, so we had this convergence in a relative error, and he is going to end its constructive proof, and that was the hard work actually to get this time t. Get this time t depending on epsilon, and this is the key ingredient to glue a different estimate for an initial time layer and an asymptotic time layer. Now, the game is, of course, to establish the stability result, and we rephrase the Galiadon-Ni-Handax-Sobole F inequalities, that is the special family shown by Bruno at the beginning, as entropy-entropy-production inequalities, and that's where the fast diffusion equation enters in the game. Equation enters in the game. Now, what we want to do is, under some constraint, get an improved entropy-entropy production inequality. And this is what we can rephrase as a stability estimate. So, let me, okay, so same people, just a permutation. You can ask the question directly to the chairman if you want. Now, okay, what I'm going to tell you is about the memoir we wrote. So, this is a side effect of the pandemic that we. A side effect of the pandemic that we wrote such a long paper. And I'm going to address chapter two with some details: what is entropy methods, how they connect large-time asymptotic with initial time layer, and you glue them with this threshold time that was in Nikita's lecture. And then I will briefly explain how this can be translated into stability result in the subcritical case of the Galliardoni-Henberg inequalities. And now we can even extend it to the Sobolev case, which is the The Sobelev case, which is the initial problem addressed by Bianchi and Egnel without consortive constants. So, just okay, a general comment. Going to sharp constants, trying to get precise estimate is usually painful work, but it gives you a lot of insight in the machinery of these inequalities. And here, what I like very much in this approach is that you connect the That you connect the inequalities with Sharp constant and even up to the next order term with entropies, which are natural quantities. Actually, you can rephrase the fast diffusion equation as a gradient flow of T's entropy with respect to Wasserstein, for instance. So you connect the inequalities with the flow, and actually you use the flow as a tool. Okay, so you by trying to get things constructive, actually you get some real insight on the. I'm sure you get some real insight on the whole machinery which is hidden behind. Maybe the major thing is that we characterize the base constant as using spectral estimates, spectral gap. And it is not at all obvious that the soble F inequality, the base constant in Soble F inequality is actually given by a linear problem. Okay, so that's the whole machinery which is behind us, the kind of thing we learned on the way. So let me go a little bit into the detail. Into the detail. And the main tool that we have in mind is some relative entropy. Actually, we write it inside similar variables, so we call it a free energy, and that's the name, the reason for the letter F. So F is the relative entropy or the free energy. And the machinery, which is adapted from Bakery and Emery, is to compute the time derivative of the free energy and get what is called the Fischer information I. Formation I. And the idea of back frame was to say, okay, if you want to connect F and I, just compute the time derivative of I. Now, if you are able to get an exponential decay, so di over dt less or equal than minus some lambda times i, then you can integrate along the flow, get that i minus lambda f is monotone, its limit is zero, you prove the inequality by. The inequality by just connecting the initial datum with the asymptotic regime for which it is zero. Okay, so that's the main idea of Backry and Mary, and it's a very fruitful idea because in the asymptotic regime, you can linearize and get a lot of estimates. But on the way, you have some additional information like monotonicity and even correction terms. Now, if you want to go to stability, it's one more step. So, what you have to do is So what you have to do is restrict a little bit your initial data or take initial data sets saying some very specific condition. You will see which ones. And then get a better constant in the inequality. And the better constant, then you can rephrase the original inequality, which was just saying that i minus lambda f was non-negative, you get now a You get now a remainder term, which is here, which is non-negative if lambda star is strictly bigger than lambda, and that's the improvement, and that's what we call the stability result. And I will try to explain how you can do it. Now, the weaker form that was shown by Bruno is the case where, okay, instead of controlling a lambda star bigger than lambda, you control lambda times psi of f, where psi is a convex function. And then the result that you were getting. And then the result that you were getting was this one. And if you think that psi of f is such that psi prime of 0 is 1 and psi of 0 is 0, then essentially this additional term is like f square. Okay, so that's the easy part. The hard part is to get to the lambda star. Okay, so let me explain how, first of all, how you connect the fast diffusion equation with the Galliard-Doni-Handex-Obolef inequality. Hands Soboblef inequality, and the key word is here would be first Rainy entropy powers. And then I will go a little bit in the details and explain what happens for the large time asymptotics and what happens for the initial time layer before gluing them with the threshold time. So far, so good. So, okay, so this is a summary. It's essentially it was said by Nikita. If you have the fast diffusion equation in The fast diffusion equation in the right range, self-similar Barendat solutions play an essential role. They actually govern the large time asymptotics. And since we want to investigate what happens for t going to infinity, then they are going to be the solution around which we linearize. Just notice that this B affixed, this biomass profile is related with the O bandalante profile of Bruno, and we will see soon. Know and we will see soon how this enters. So, one nice remark was a remark of Cedric Villany, who rephrased the back-Hemry method for the logarithmic symbol of inequality in terms of Rainy entropy powers. This is a notion that was introduced in information theory. And then later, Giuseppe Doscani and Giuseppe Savare, they were able to extend it to the nonlinear case. And this is actually something I've been working on. And it is really a non-linear case. And it is really a non-linear version of the Backry-Emery method with no curvature, no geometry, but the gain that we get is precisely from the non-linearity. So it is a very nice thing. Okay, basic things. First of all, the mass is conserved is the exponent M of the fast diffusion equation is in the right range, which means large enough and is the first content that answers. First content that answers. Actually, we need a second moment, and this is very crucial. And this is really a second moment. It's the second moment of the function u, the solution u of the fast diffusion equation. So we need to assume that the initial datum satisfies this. And since we want to investigate what happens for last time, we also need that the Barendat function has a second moment finite, which is another restriction for the exponent. And there is a third restriction here, which is more directly linked. Which is more directly linked with the Sobolev inequality or Gallia-Donian by Sobolev inequality. And I don't really have much time to expand on this, but when you compute the time derivative of this, which is going to be our entropy, the entropy function, which is shown here, what you get is a feature information type quantity, which is here that I call I of U. So far, so good. These are just definition. So good, these are just definition. Now, if you look at the Galiardonian indexable F inequalities and you make a change of a notation so that you introduce U as F to the power 2p. So the F is the F for which you write the Galeadian maximum inequalities. Okay, then you notice that with this specific choice of P, then u to the M is F to the power P plus 1. So the entropy is going to involve the integral of F to the P plus 1. integral of f to the p plus one, while the integral of u is just the integral of the mass. And gradient f square, when you integrate it, is just the Fisher information. So this connects the Galiardonian indexable F inequality with entropies, the Reni entropies. Okay, so the entropy now is this Lp plus 1 normal raised to the power P plus 1. Its time derivative is proportional to the Fisher information I have Proportional to the Fischer information I of u and if you solve the fast diffusion equation. Actually, if you compute the time derivative of the entropy, you get a bunch of terms. The F2P norm is just the mass of U, so it's conserved. The Fp plus one norm, well, it's just E, and if you put all the constants together, you get a differential inequality which relates E prime with a power of E. With a power of phi. So this gives you a growth rate of u to dm. But remember, m is less than one, so it means you going actually to zero. Okay, and it's even better if you know the optimal constants here in the Galiardonian-Maxable F inequality, you know the optimal constant here and is actually equivalent. Okay, so far I'm just connecting the fast diffusion flow with the Galiardonian Maxable F, explaining that it gives me nice estimates on Nice estimates on the entropy. Well, actually, you can do more, and that's where the Carried method of back theory will enter. But for this, I need a bit more, a few more definitions. So, first, you don't take the entropy, but the Rainy entropy power, which is essentially the entropy raised to some specific power, okay, and you compute its time derivative. So, when you differentiate So, when you differentiate E, you get the Fischer information and you get this quantity here. Okay. Now, it's nice to introduce the pressure variable. If you have read the Vasquez book, you know why. And if you do this, it's very nice because, well, for instance, the entropy, which is u to the power m, is proportional to pu, while the Fischer information is just u gradient P. Is just u gradient p squared. Now, the idea of backframe is very simple. You differentiate the fisher information. You take one more derivative. Derivative of E is I, derivative of I, you compute it. And it's a big mess. However, by doing a few integration by parts and using some rearrangement of the quantities so that you complete squares, what you get is a very, very nice. What you get is a very, very nice quantity which says that the sign is given by the norm of the Hessian squared minus Laplacian p squared. Okay, and by the arithmetic geometric inequality, you know that this has a sign if m is bigger than one minus one over d, the m1 that I had before. Okay, well actually you can do a bit more and you compute the derivative of the derivative of the t derivative of the second t derivative of the Rainy entropy powers. And then things once again rearrange in the form of a sum of nice squares. So this is a positive quantity. There is a minus sign in front. So this quantity is decaying. Good. Now, if you analyze the asymptotic regime, you know that the Bahan data are going to play the crucial role, and actually you can prove. And actually, you can prove that the limit of this quantity is given by the limit of the Baronbatt solution. But the exponent in the Rainy entropy powers are precisely taken in such a way that it doesn't depend on the t-dependence of the bindbat function. So it's just the barnbat profile file that plays a role. So it's a constant. And this gives you the optimal constant in the Galliard-Don-Hanbach-SaboF inequality. Okay, so you see, using the flow. Okay, so you see using the flow and using the backfield metric, which is computing twice the t derivative of the entropy, or here the Rainy entropy power, you are able to prove that, well, this quantity is bigger than the limit, which is precisely the Galiadonian dexable inequality in sharp form with the optimal constant. Very good. Well, here I'm hiding under the carpet a number of difficulties. You are doing plenty of You are doing plenty of integration by parts. You have to justify that you can do it so that you have sufficient decay not only on you but on various derivatives. This is not at all easy. Okay, so there is a more standard approach, which is to go to the self-similar variable as Nikita was doing in his lecture. And hopefully, I have the same notation, so I hope you can recognize a few things. So, let me explain how this Galiadoni-Handex Sobolev can be rephrased. The Nierhandaxof can be rephrased in terms of self-similar variables. Self-similar variables, you take the solution of the fast diffusion equation, you rescale it appropriately. So you have a new scale R of t, which is here, a new time scale, which is essentially the log of R of T. Okay, and if you do this, you get the nice Foucault-Panck equation, which is here. I mean, with just this addition. Just this additional term which corresponds to harmonic potential that keeps the solution confined. So instead of having a solution which spreads like a Barrenbatt profile, you just converge to a stationary solution, which is precisely the Baren-Bautt profile. Now you can redefine the entropy as now a relative entropy, and is the F which is here and a relative Fischer information which is here. relative Fischer information which is here and what I have shown long ago by variational methods with Manuel Lepino is that fissure controls four times the relative entropy or the free energy and this is having this four here means that you have the sharp constant okay so in terms of the evolution for instance it tells you that the free energy f decays like e to the minus forty and that is e to the minus minus forty is the charp rate. This 40 is the charperate. Okay, one more step was to understand what happens close to the barren plant. It's a bit the reverse thing. You would think first linearize and then try to do the non-linear case. We did the other way. And actually, we learned quite a few things on the way. So if you linearize, well, first of all, well, first of all, you can extend the results not only to the range M1 to 1, but to a much larger range of exponents. Exponents. It is something of independent interest, but there is a high price to pay. The price you pay is that to do this, you need to be between two Bah and Blood functions with different coefficients, different masses, say. Well, of course, if you are in the range studied by Nikita, everything is fine because you know that after some time we'll enter in this range. And actually, now we know which time it is. Okay, once you have done this, you can. Done this, you can reconsider the entropy entropy production inequality. So, if you linearize the Fischer information, you get just this. If you linearize the relative entropy, you get this L2 norm with a weight. Weight is not the same, okay? But now it is a nice spectral problem. You can identify the best constant. So, this was done by Densler and McCann in a slightly different context. Okay, and in general, this gives you. And in general, this gives you an exponential decay for the relative entropy, the free energy also in the nonlinear range, assuming that you have assumption H. Okay, and the gamma of M depends on the range of M. So let me just show you the picture. Okay, this is the picture of the spectrum of the linearized evolution operator in the right space. So the first thing to notice is that you have some essential spectrum. Some essential spectrum here. Okay, we don't really consider here. Here, in the horizontal coordinate, is just the exponent m. What we are interested in is the range between m1 and 1, which is here. But you can identify the whole spectrum and you have eigenvalues below the essential spectrum. Now, a nice observation. A nice observation is that the four that we add is twice the spectral gap, which is here. Okay, gamma of m when we are in the range between m1, which is here, and one. Okay, gamma of m is two, and the four is given by this twice this spectral gap. So it already tells us that the linearized problem around the banded profile actually captures something which is essential. captures something which is essential in the theory. Okay, now let me go a bit more to the entropy methods and explain you what you can learn for the asymptotic time layer. This is the easy part and the initial time layer. Okay, so as I was telling to you, well, this L2 norm that we have here that was in the RD Poincare inequality before is just the linearization of the free Linearization of the free energy and this L2 norm with a different weight is actually the linearization of the Fischer information. Now the point is that if you take some well-prepared initial datum for the evolution problem, and by well-prepared initial datum, I think just an initial datum whose center of mass is at zero. You essentially kill one degree of freedom by doing this. You kill the transation. this you kill the translation and the translation are responsible for sorry i'm going back for this eigenvalue this eigenspace which is here so with a well prepared initial datum you won't see these eigenvalues for lash time and substantially and just for free you get now that the spectral gap is not two but goes up to the next eigenvalue which is here okay so you just already get some improved entropy Some improved entropy production inequality, which is now written here. Of course, there is a high price to pay. You have to be tried between two bandwidths, but now we are confident. We know that for some time it's going to happen. And you can make this very precise with some eta, which depend explicitly on an epsilon, which are given by some relations, which crazy, which are here, but can be computed. Okay, so here you get already it is improved entropy, entropy production inequality. Improve entropy, entropy production inequality, it's already a stability result, but it's a weak stability result because assuming that you are trapped between two bar and blood for the initial datum is a very strong assumption. So the whole goal is to use the regularization properties of the flow to get a more general result. Okay, so that's for the asymptotic time layer. At least we know that for large time, we already have this for free. Now for short Now, for short time intervals, so what I call the initial time layer, well, we have one more information, and this comes from the Backfree-Emory computation. So, I didn't insist too much on it. I showed you the computation for Rainy entropy powers. Actually, it is the same expression, by the way, as the one that was shown for the rigidity result of Bruno at the beginning. It is a deep thing. If you use this along the evolution, well, you get the existence of a Well, you get the existence of a certain psi and you get the weak stability result with some convex function psi. We want to go a bit further and now we exploit it on the initial time layer. So we introduce the quotient of the Fisher information by the relative Fischer information by the relative entropy. Okay, and the results that we had before can be rephrased without a further assumption as Q is larger than 4. As Q is larger than 4. But we know a bit more. By using this back-henry method, we get this differential inequality, which is here, and that we can use on any finite time interval. So if, for instance, we know that at time capital T we have something which is bigger than four, then eventually we, well, we know that we still have it for any time less than capital T. Of course, the constant deteriorates, but doesn't go to zero. Deteriorates but doesn't go to zero. And that's the way we get some improvement for the initial time layer. Okay, so now let me summarize the whole strategy. We choose some epsilon small enough. We take we compute the threshold time. Sorry, I kept some old notation. It is the capital T of Nikita. Okay. For T larger than capital T, we use the forward estimate based on the spectral. The forward estimate based on the spectral gap, so the improved spectral gap that we have for well-prepared initial datum. So we have to fix the center of mass. That's why Bruno, by the way, was fixing the integral of f to the power 2p times x to be equal to zero. Okay. For the initial time layer, that is from zero to capital T, okay, we use the backward estimate that I just shown to you, and this is where you use the This is where you use the BAC-V Emory estimate. Okay. And then you glue this together and you get something which is improved. So I'm just saying, well, okay, that's what I was saying with words. So this is the result presented by Nikita. If you have an initial datum which has a sufficient decay for the tails, precisely in Uh, precisely in this sense, okay. So, for some constant a, then you get that for any time larger than capital T, T depending only on epsilon A, and of course, of the parameter of the problem, the dimension and m, okay, then you are between the two bandwidth, and then you can apply the asymptotic, the result for the asymptotic regime. Okay, so let's do it. So let's do it. Okay, so that's a result. Okay, take an initial datum which has center of mass at zero, which means this. Assume that you have the tail decay condition so that you can apply what Nikita presented. And then you are able to find some zeta which depends explicitly on A and G. Okay, such that you have the improved entropy-entropy production inequality, which is shown here, with some zeta which is explicit. Okay, and then, of course, you have to collect all terms. So, of course, the zeta is the one that you get in the asymptotic time layer, except that you have to take into account that it deteriorates on the initial time layer, and that's the way you built it. Okay, so I skip all details about how the constant depends. About how the constant depends on the initial data, but they only depend through A and G. Okay, and for instance, the first type of result you get is an improved decay rate. For a well-prepared initial datum, the solution of the non-linear problem decays faster in a relative entropy. The non-linear relative entropy adapted to a non-linear fast diffusion equation with a rate which is improved by Tzeta, which is shown here. Now, it also tells you, actually, you do it the other way. You do it along the flow, and once you have it asymptotically or for any time, you also have it at t equals zero, and that's the way you build the improved entropy-entropy production inequality. Now, if you think to the improved entropy-entropy production inequality, well, you have the difference of the two terms, which are equivalent to the Galiard-Nehranian-Beng-Sobalf inequality. To the Galiardonian solo inequality, control a remainder term, which is now proportional to the relative entropy itself, not the square. Okay, so that's the main gain. Once you have this, of course, you can play some tricks and write that I minus 4f now controls the Fischer information. So you also have it in a strong norm, which is here. Okay, just manipulation, easy. Now, stability results. Now stability results. Well okay what Bruno was doing during the first lecture was not naive. Fixing the moments and so on was the good thing to do and that's the way we can do it here except that we have to be a bit careful. When you look at Galliardonia-Hanbach Sobolef inequalities there is a scale obviously because you have a gradient so introduce it introduces a scale in the problem. Introduces a scale in the problem. But we also have a second moment. It introduces another scale. So it is a bit messy to get the whole thing work. But just look at the bottom line. What do we have in the left-hand side is just the difference of the two term in the Gagliardoni-Handbach over Laf inequality. Actually, just Gagliardoni-Handbach. I'm not covering the case of the critical exponent. The P star equals D over D. P star equals d over d minus 2 has to be excluded in dimension higher or equal than 3. Okay, so I take each side raised to the power 2 p gamma, and the difference now is controlled by the relative entropy, a normalization factor, and this horrible constant that comes from my entropy, entropy production estimates, up to the scalings that I have to take into account if I don't normalize everything. Okay, so I have to take here to take into account all the To take into account all the invariances of the Galiadonian Handbox inequality, multiplication by constant. Okay, this is responsible for this term. The translation invariant is responsible for this translation, which is here. And the scaling invariance, which is hidden in T SM. Actually, it's lambda and kappa both. You have to take into account both. Okay. And by doing this, I can relate the Galiadonian dexterable affinity quality of. The Galiardonian dexterable left inequality of the last line with the entropy entropy production equality that I was showing before. Again, pardon me, but I don't want to go to detail. It's messy. Okay, this can be done. Now, once you have this, you can rephrase it in various ways. So, for instance, if you want to do it in something which is closer to the Bianchi eigenl type result, you introduce the deficit functional as Bruno was doing. Bruno was doing. So right now it's a non-scale invariant form of the Galiardonian Hand-Baxable inequality, which is this deficit. The infimum of this deficit functional is given by the bold J he was mentioning, the OberTalenti profile. Okay. So this fixes the value of the constant here, by the way. Okay. And if you do this, this deficit functional actually is controlled from below. Actually, it is controlled from below by a distance which is almost an H1 distance. Actually, it is the relative Fischer information to all Obern-Talanty functions when you minimize on the whole manifold here. But it's the Fischer information, it's not H10. And that was actually the reason why we were measuring the deficits by the Fisher information in the first lecture. In the first lecture. Now, the dependence on the constant is fully explicit but messy. Go to the memoir if you want to see the explicit expression. It's certainly not optimal. And the only point which matters actually is that you can compute it. And this constant does not degenerate on the Obantalanti manifold. So when you approach it, you cannot make it arbitrarily small. Now, there is a question. Can you remove Now, there is a question. Can you remove the condition on the tails? Well, okay, so far, okay, you expect that yes, because of the variational result. The variable result didn't have to assume this, but not with the method we use because the capital T we get, the threshold time, heavily depends on this and is wrong if you don't assume it. Okay, uh, well, maybe it's time to conclude. So, uh, uh conclude so uh we can also do it in the case of the soblef inequality and you get some constructive result in the same with the same flavor now the deficit is just the difference of the two terms of the soblef inequality squared so it's just this very standard thing as in the unknown you get an explicit consent which is here here well this is the um uh the relative fisher information not h10 but it's a very One zero, but it's a very nice notion of distance related with the Eisenberg principle mentioned by Bruno. I don't have time to expand, and everything is written in the special case where you assume that everything is normalized. So again, the setting of lecture one. If you don't assume this, well, it's a bit more messy, but you can still do it and you get the deficit functional measure, the relative distance, the infimum of the distance to the orbantal. Of the distance to the orbantality manifold, again measured by relative entropies, with, of course, the whole bunch of parameters that enter as in the subcritical case. Well, for the critical case, there is an additional difficulty and I just come back to the spectrum. It's in the asymptotic regime. What happens is when you go to the critical exponent, so 2p equal to d over d minus. 2p equal to d over d minus 2 is m equal to d minus 1 over d. So it is the one which is here. Okay, let me convince you, enlarge it. Okay. And you see that there is some degeneracy, the two eigenvalues, they cross. So you don't have to remove only translation, but you also have to take care of the scale. Of course, SOBAF is clearly scale invariant. You cannot write it in non-scale invariant form. non-scale invariant form. The problem is that it is linked with the second moment and the second moment is not a conserved quantity along the flow. So you have to handle this. Now if you think a little bit, doing a scaling on the function or doing a scaling on the Bahrandat function, on the one to which you measure the distance, okay? Well, the Bahrandat function, they depend on time and it's just translating a little bit in time the Bahanblat or translating the function. The Bahanblat or translating the function. Okay, so you introduce some delay. Now, just believe me, if you want to do this, well, you just have to prove that this delay stays bounded along the evolution. Okay? It's a bit messy. You have to study it. You can rephrase it in terms of the solution of the rescale fast diffusion equation. So, V is the solution of the Re-scale Fast Diffusion Equation, but it involves. Equation, but it involves the second moment. The derivative of the second moment is the entropy v to dm actually. And this is not explicit, so you have to really control this. This can be done using some phase-based analysis and a nice representation in terms of a dynamical system of the various quantities that enter here, but definitely it is would be too long to explain. So, let me just say that by adjusting the Adjusting the scale or adjusting T's delay, we are able to fit the Baren Bell function to the next order. And by doing this, asymptotically, we kill the next eigenspace. And that's how we get, instead of having this improved spectral gap. Okay, so remember we had this without with doing nothing, t is by fixing the center of mass, and then with this best matching, we are able to go up to there. Matching, we are able to go up to here, which essentially means that the rate is improved by a factor two. Not exactly, because when you do this, you change a little bit in time scales, so you have to be a bit careful in the statement. Anyway, this gives the result, and I thank you very much for your attention. Thank you very much, Jeanne, for the nice talk. The next talk, I open the questions now. If somebody has a question, just turn on the meek and ask. Let's see, let's see how you do this. So, just to summarize, Jan, one can say if you put the picture of the slides with the three time layers, it's like we follow. I mean, while in Bruno's talk, there were possible sub-sequences. The problem was to take a minimizing sequence and Minimizing sequence and we do stuff, we choose basically the minimizing sequence to be the flow, the fast diffusion flow here, yeah, yeah, yeah, yeah, no. And one of the difficulty was once you get the global ARNA principle at a certain time, capital T with everything well prepared, with all the quantities well established, there was the forward estimate, which was the improved asymptotic regime. Improved asymptotic regime, but basically, what we say is that with the Global Arna principle, you really have a bit more in the spectral in the spectral gap or something like that. And then you go back. Yes, with the relative uniform convergence, what you get is that you can compare the nonlinear entropy, entropy production inequality with the linearized one. Once you improve the linearized one, okay, you get an improvement in the Okay, you get an improvement in the nonlinear one, and is improved nonlinear, sorry, improved entropy, entropy production inequality is the stability result. Okay. Yeah. Maybe one comment here. It's a bit surprising that, of course, the fast diffusion flow picks one of the Bahamat solutions and it takes care of the invariances. So it does what. So that's why you have to go to the next order term in the extension to get rid of the invariances. Okay, and you really get into the machinery. And of course, if you think in terms of the linearized problem, well, they have to be of higher order. I mean, these modes correspond to faster decaying modes in the linearized regime. What is nice is that this entropy. What is nice is that this entropy also exists in the non-linear regime, and in the nonlinear regime, you also have an improvement. Okay, so now the game, and that was the idea of 2006, if I remember correctly, how do we glue them together? And that's where you need this threshold time. And that's where you need to make the Mozart theory quantitative and then adapt it to the fast diffusion flow. Okay, I should sign the COVID because. I should sign the COVID because otherwise we would never have done this. So it was the beginning of the lockdown. I just killed all participants by saying that. Okay, so if there are no further questions or other remarks or curiosities, we thank. We thank Jean again and all the speakers of the afternoon.