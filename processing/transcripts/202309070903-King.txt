So y'all are really in for a treat. Okay, so nevertheless, insert math property persisted. So the point of this talk is just there are a variety of different mathematical properties that when you track them over time, tracking the persistence of these properties can be informative. And maybe some of these are of interest to you. Certainly some of these, I've heard some of you talking about some of them, but I think there's some things that would be But I think there's some things that would be very interesting. Okay, we're going to start with persistent homology, talk a bit about flags, and then end with what I call potpourri dust, so it's just a couple of sequence of random slides. And I have, I think, like 50 or 60 hidden slides after this. So anything you want to ask further questions on, I probably have a slide for. Okay, so I collaborate right now with Atmospheric Science Institute. Institute, and so this first section is mainly going to be focused on some work with that, but I think some of the techniques apply to things that you're interested in. So, this one project dealt with the mesoscale organization of clouds. So, it ends up that these are, they have these four categories, sugar, flour, fish, and gravel, that sort of describe this mesoscale organization. And there's not a whole lot of hand-annotated pictures. Annotated pictures, understanding if being able to automatically detect and label certain clouds can help the atmospheric scientists better understand what's going on. Okay, so I am going to teach you everything you need to know about algebraic homology on this one slide. This is all you need. People spend a lot of time studying it, but for your purposes, this is just going to be good enough. Okay, so what we have. You didn't know. Okay, so what we have here is we have four different mathematical spaces, and to each of these spaces, I have assigned two numbers. So we have a filled-in purple disc, gets a one and a zero. We got an open blue circle, it gets a one and a one. We have two disconnected green circles, gets a two and a two. And we have two connected red circles, we get a one and a two. So, anyone who has not seen homology before, what do you think these numbers are? What do they represent? What do they represent? Objects and moods. But you said holes just because you already know, because I literally have a slide with your name on it later. That's cheating. Yeah. So it's connected components, so contiguous blobs. So these are all connected. And then the green circles, we have two pieces. And that's the zeroth Betty number. It's also denoted by 8. Some is also denoted by H0 because it has to do with zero topology groups. And then we have the first Betty number, and this is counting holes. So this has no hole, this has one, and these two both have two, regardless if they're connected or not. And these can all be deformed. So if I have like a squiggly closed curve that doesn't cross itself, it would still get one and one. So if I think about these things as being made of rubber, I can stretch them around. Being made of rubber, I can stretch them around and it's not going to change these numbers. Because the rubber, unless it's poorly made rubber that breaks really easily, stretching the rubber does not change the number of connected pieces or where the number pulls off. Okay, so this middle row kind of should look somewhat familiar. We saw animations with slices like this in Felix's talk. So we're basically going to be moving through, one could think. One could think of this as moving, it's an image, but one could think of it as moving through a topography. And we're going to do something called super level set for systemology. Sublevel sets, the common thing in literature for whatever reason, with this cloud data, the super level set, but the basic idea is the same. So what we're doing, so this is an unsigned int image, so all the values are between 0 and 255. And so we look at 254, and we see. 254, and we see: are any pixels 254 or 255? If so, then we record it. Then we go the next level down. So here, this white area represents all the pixels that have value 2 or 9 or higher. So it's the brightest part of the image. So these are just some snapshots of this as we're moving through. So this next one, this is all the pixels that are 151 or higher, 94 and 10. 94 and 10, we have some little whoops. So we have, there are three different main ways that we are going to record this so-called persistent homology. So what we're doing, the idea is that each of these slices as we're moving through, we're going to give it that zeroth Betty number. We're going to say how many connected chunks do we have? And we're going to give it that first Betty number. We're going to say, how many holes do we have? And it ends up that the sort of And it ends up that the sort of algebra underneath the hood that's allowing you to compute it is it's not just taking a snapshot at each time, you can actually connect these things. So in a sense, you know that this one piece is the same as this one piece. And so that is what we're looking at here in red. So the red bars here are all representing the connected chunks. So this long red bar here is saying Like, we don't have anything that's like 255 or 254, but eventually we hit some of the brightest pixels. They become this chunk. This chunk. I should have brought my charger, I guess, but is this a charger? Let's steal that. It's not dead, but it will be annoying if I keep doing that. Okay, cool. Yeah, and so that's what this lone bar is. So you see these two, so as they're moving left. Two, so as they're moving left to right, it's like: is this thing alive? And this one's alive almost across all slices. We have these two little red pieces here. They represent these little chunks that kind of flare up. But not too long after they flare up, they merge in with the main chunks. They go away. And then at some point, we get enough that this closes up up top, and now we have our first hole. We have our first hole, and that's what this blue one is. So, as long as we have this hole, then we have the blue, but eventually this hole fills in, and so that dies. We have this little red here, here. So, this isn't a like a typo. There are just such short bars right here that you can't see. You kind of get little, very, very short lived holes like right in these upper corners for like one level, one moment. And then these two, these like little teeny tiny red. These like little teeny tiny red things are from these tiny little pieces, and so this is showing everything we have. Yes? How do you automate that evening these intensity levels, you can track like what an object is, especially when it's so small? So that is a very good question, and it just has to do with the. So, in order to do this, you're building this giant chain complex called a Viatorus-RIPS complex, and you're actually computing. Computing linear maps between different things. You're building a simple, like, anyways, it's built into the math underneath the hood. But that was my very first question when I first saw prosthemology, because you could also imagine something where at the same second, two things kind of like, something pinches in and you go from one hole to two holes, and somewhere else in the image, something pinches in where a hole goes away. But it's the cool thing about the underlying map is that acts. cool thing about the underlying math is that actually keeps track of that was really what's called a birth of a hole and this is really a death of a hole. Good question. Okay, so that's called a persistent barcode. These other two things, persistence diagram and persistent landscape, are just different ways of visualizing the exact same information. So just really quick, here we have red dots and blue dots, and we have a line at 45 degrees. 45 degrees. So the location of the dot is birth, death. So everything dies after it's born, so you can't have anything below this line. And then this high up red one is just the longest bar. And so anything that's near the diagonal line is a short barcode. And then I'll explain on the next slide how we get this landscape, because the landscape is actually going to be important. Because what do we often need? Because, what do we often need if we're going to do data science? Not always. We need vectors. Like, so many of our algorithms use vectors. These are not vectors. And even like the number of barcodes, even if I take it from similar data, there's not going to be the same number of barcodes. So we have to have a way of converting this into something that we can actually put into an algorithm. There are a couple of different tricks, but for us, for this upcoming thing, we're going to use the landscape. Okay. How do we do the landscape? How do we do the landscape? Landscape is perfect here, surrounded at this natural beauty here in Banff. We're going to turn the barcode into a mountain range. And so basically what we're going to do is we're going to draw a right triangle, an isosceles right triangle with the hypotenuse, the longest part, on the bar of the barcode. And we're going to stack these things up, and it looks like a mountain range. Looks like a mountain range. And so, the way that we often deal with these, both in the vectorization that we're going to do and just how they're kind of done, is we take envelopes. So, we first say, okay, what is the outermost boundary? So, we're going to get a nice little mountain landscape. And then, if we remove that line, that's so we've removed the solid line that's the outermost boundary, we take the envelope of what remains. And so we have these short dotted line. These short dotted line, and then we remove that. And so you can decide how many envelopes you want to take, like how much detail you want to do. So that is a parameter that you can set that gives you sort of a fixed way of giving a similar level of complexity to different data. Okay. And I should say we did not create persistent landscapes, but this image is from our paper. Oh, also, a mathematician. Oh, also, mathematicians, king of the humble brag. So we put just our last initial to pretend like we're humble, but it's really the size of time. Yeah? Sorry, we just sat our height of the last triangle. It's related to it. So you're building a right, an isosceles right triangle, and then the hypotenuse is this. So it's going to be like a square root two. Get it anyway. But yeah. So the height is. Yeah, so the height is related to the, yeah, or maybe one half, anyways, but it's directly related, but it's not exactly the same. Any other questions? Okay, so then how do we turn it, I just want to double it, okay, how do we turn this into a vector? So what we do is we have all of our different little cloud pictures, or maybe some pictures of cells, then you're moving through topographies and you're doing your sub or super level set for systemology. Super level set for mistomology. And then you ask, okay, these seem to have a certain level of complexity. So in our case, we did five of these envelopes. So we took the outermost, and the next outermost, we did that five times. So that gave us five functions over an interval. And then we just chose a sampling rate. So it was F400 or something? Later on. Anyway, so like, or maybe it was 200 points. Or maybe it was 200 points. So for each of those functions, we took the values of the functions evenly spaced for 200 points. And so that gave us, this outer function then gave us a vector of length 200. The next one gave us a vector of length 200. And we glued those all together. And we did the same thing for, so we did this for the zeroth and then the first. So we made a landscape for the connected components and we made a landscape. For the connected components, and we made a landscape for the holes. And then that means no matter what's going on in the cloud pictures, we are then creating vectors of the exact same dimensionality that are kind of going into the same level of complexity. Questions about that process? Okay. So here's just an example of a 32 by 32 length sample image from this. Image from this, the so this is something that's labeled, so we know the ground truth, what it is of the type of mezzle-scale cloud organization, and we can see just how messy real data can look. So then, here's the punchline basically. So, we had vectorized them, we got 2,000-dimensional vectors. Sorry, yeah? On the plus line, if you have. If you have some scaling and of the underlying signal A and some white Gaussian noise at different levels, how does that overall how robust is this encoding? So scaling is just going to like shift things. Like if you make everything brighter, for example, it's just going to take all of these, the barcode. All of these in barcode, and then literally just shift the entire structure uniformly over by the amount that you've written. Also, because it's topology, I mean, this is on a discrete domain, so it's not perfect, but because it's topology, if you're stretching features out or shrinking features, it doesn't change the topology. It's going to change the persistent landscape because this is like a discrete thing that we're taking finite measurements of, but it's going to. That we're taking finite measures of, but it's going to have some similarities. The Gaussian noise is definitely going to add a ton of these shorter features. So, if I'm on this one, like it's going to be adding things along this diagram. So, there's definitely techniques where you're trying to measure noise amount, and then you're sort of looking at the statistics of the things that are very close to the, where there are very short bars, if you want to say the barcode, are very close to the line on the top of the diagram. On the top of the diagram. So noise does affect. But it would destroy the larger ones, right? It would create whole ones. It would not destroy the larger ones. The long ones, this is why this is how persistent homology first came up, is that people were trying to find true homology. So that you would imagine if you have noisy points from a manifold that's like a known topological structure, that the long barcodes are going to be precisely. Let me just skip forward real fast a minute. For a goal thousand man. Actually, just because I have one more slide, let me say this, and then I'm going to say something about persistent homology at points, and then maybe you can see the idea. So, anyways, we were, you know, we had these 2,000-dimensional vectors, and it's like, well, let's just see what PCA does. There's no reason to believe it should do anything. And it ended up that three dimensions captured a little over 90% of the variation of these 2,000-dimensional vectors. So they really collapsed. So, they really like collapsed out a lot of the geometry. And then it ends up, I mean, this isn't perfect, but like the classes, so we did not use the classes in anything, the persistent homology, the PCA, this is all completely unsupervised. And then the classes actually linearly separated pretty well after going down the three dimensions. So it really was somehow connecting or encapsulating some of this information. It's so, yeah. Maybe this is a path if you're trying to understand certain structures. So, yeah. Is there a way to know how to interpret the thing that she's like that are helping to use markets? That would be cool. We don't yet have an interpretation. I mean, that's the thing when you do PCA, is when you're lucky, you can read it. Another thing that we had tried to. Another thing that we had tried doing that we haven't yet really gone much further with was then actually just running a clustering algorithm on all the points and seeing what the clusters look like and how they look relative to the organization of the clouds. Because some of this organization is not as functional, it's more like some of it is. And so wondering if like the clustering rather than trying to figure out Clustering rather than trying to figure out the exact classes was actually going to lead to something more useful, but that's not something that we've done yet. Yes? Wouldn't the lower component capture the louder envelopes? So the way we have done it, we had the envelopes and then we split them, right? We sampled them. Yeah. And then we could put them together and we run the scale. That's what you've done so far. So the larger envelopes on the The larger envelopes are the ones in which the picture changes the most. So would not necessarily what we get in the low components of the PCA? Well, the thing is, though, if you think about these long vectors that we've made, we're sort of comparing the large mountains to the large mountains, and the medium mountains to the medium mountains. So these are all sort of. And then the so these are all sort of they're lined up this way, and so in a sense, not quite, but because the values are going to be lower, but like these finer features that are the really small mountains are almost, at least coordinate-wise, they're as equally represented as the bigger things. Now, their values are going to be smaller, so they aren't going to be as represented, but I don't know if that answers your question. So, this will give us the length scale. This will give us the length scale in which we see most change. So, for example, if we have a cloud of size of cloud, 100 meters. So, we have a bunch of them, so that would be what would appear as the dominant length scale once we do the PC. So, this is the thing, though, is when we do the PCA, it doesn't care about. The PCA doesn't care about the, not PCA, when we do the persistent homology, it's not caring about the patch size. So we can take patches that are different sizes, do PCA, and still vectorize them the same way. So we still go down the same number of scales of the mountain. We still sample at the same rate. And so we could take image passes a different size and compare them. Anyways, yeah. Anyways, yeah. Okay, so now let me say something maybe that'll help for the next question. So, this is a different way to so this was super-level set, but sublevel set's the normal thing. You're scanning through an image and you're looking, you're recording all the pixels that are above or below that value, and then you're looking at the quality. Another way to, or another type of data you can apply for systemology to is point clouds. So, this image, this is from a student back where I used to work. This is actually kind of This is actually kind of showing a little bit of under the hood of how it's being computed, but the basic idea is you take all of your dots and you replace them with balls of a fixed radius. You take that union, you say, what's the number of connected components? What's the number of poles? You grow the radius. And so this is the thing that you're changing, is the radius of these balls. So here, to answer what your question is, if you were to just look at this as a cube and you say, oh, that's approximately a figure. And you say, oh, that's approximately a figure eight. So, what I want is, I want one really long bar for the connected part and two really long bars for the holes, because those are the true underlying topological features, and I will consider all the other short bars as just noise. And that's, so this is just for the holes, but that's what we see. We have two persistent features for the holes, and those are representing the fact that there are these holes that. There are these holes that are persistent over multiple scales, and then all of this stuff here is encoding noise. But with the previous one, those short-lived things we didn't think as noise to be thrown away, that was actually texture that was informative. So in this case, we think of these things as being noise, but they can also be really informative depending on your data. And then just a really quick, so I've mentioned that there was an undergrad that Mentioned that there was an undergrad that Sharp and I had worked with, and it took much, much longer to just get these out. It took the entire semester, so the data science has not yet been done on it. But it's kind of hard to see, but you can see these are these diagrams over here. The connected components are in red, and then the things you want to look out for, the holes in blue. And so, by doing this, you can see this is more highly irregular, and there. Highly irregular, and there are many, many more on these short-lived blue holes. But that's because you can think when you have a very irregular cell boundary, as you grow out the balls, you're going to get these bubbles. And you're going to have these short-lived bubbles that are going to live as you're growing out points on the boundary. And then they're going to, so you're seeing this, whereas this is a lot smoother. And so you don't have as many, like this little thing, I'm sure, formed a bubble and this, but like this area was never going. But, like, this area was never going to, as you grow out, as you're kind of expanding the points along the boundary right there, that's not ever going to perform because it's too regular. So, in this case, here, the short things are noise, but here they're actually encoding critical information. That's the regularity of the boundary of these cells. Okay, I'm about to, oh no, I'm not. Okay, in five minutes. Okay, cool. Okay, cool. I'm just going to say something really quick about flags, and I'm going to speed up a little bit. I have a bunch of hidden slides, so we can talk about more afterward, during the question break, whatever. Okay, key idea principle component analysis. Most, if not all, of you are very familiar with it. It's come up a ton of times this week. I'm going to phrase it in a slightly different situation. Basically, if I have a data set and I want a best k-dimensional linear approximation of that data set, Approximation of that data set, that's going to be a subspace of the base best K plus one-dimensional. So if I find the line of best fit with respect to the PCA cost function, then that's going to be living inside the plane of SFIT, which would live inside the three-dimensional space of SFIT. So these things actually increase. Another idea, multi-resolution analysis, so in wavelets, shearlets, certain other types of transformations. Certain other types of transformations, you get this sequence of subspaces where any of the signals, the images, the pictures, the songs, whatever, that live in one subspace here have more finer detail than in the other. So you're kind of like building up finer and finer detail as you go up and you go down. This structure actually is kind of key to doing fast implementations. And it ends up that both of these are examples of flats. Examples of flags. So, what is a flag? So, let's just say we're an Rn, it can work with any vector space. And then we have a sequence of positive integers that sum up to the dimension of the space. Then a flag of signature, that sequence of numbers, is just a sequence of subspaces that are nested. And the dimension of each subspace, the difference in dimensions, is the thing. So, like, the dimension to VI is the. So, like, the dimension to VI is the sum of all these up to I. So, we can kind of see these both the multi-resolution analysis and the sort of if you want to progressively better approximate things in DCA are somehow built on these platforms. You're getting these sequences. And then the, yeah, okay, skip that. Right, so, oh, okay, what am I going to say? So, basically, then you could extrapolate SVD and that. Extrapolate SVD and actually think about approximating something with an element of a flag. So with not just one-dimensional, not just two-dimensional, but like the sequence that's building up. And this idea actually was something that some colleagues did, and I said, this sounds stupid. I don't understand how this helps anything. And I'll just show you one picture that, without going into too much detail, because we have no time. So there's this. We have no time. So there's this hyperspectral data set of a thing called Indian Pines. So instead of taking three white labelings, you're taking, I think this one's like 2,000, I don't remember, 200, at least 200, 2,000, something like that, many, many measurements, and then you're trying to determine whether or not those measurements are a grass pasture or a corn. And so these top things are just what comes by representing each of these things with a single sub. Representing each of these things with a single subspace. So they took five different points within similar clusters, and here they're actually associating them with flags, so with sequences of subspaces. And you can see that representing them with these sequences of subspaces has better cluster purity. So the two classes separate more naturally. Two classes separate more naturally when actually taking into account the entire nested sequence of best approximations rather than just one. Okay, so really don't have time, but the stuff that I had worked on was we can kind of assign a bunch of subspaces to a flag in a way that's more median-like, so it's more robust to outliers than this other technique that was more. Technique that was more average-like. And so we have here: this is from the data set from YouTube. They have these little video clips that are labeled swinging and jumping. And we just did the dumbest possible thing. We made a subspace by just taking each of the frames of the videos and taking the span across them. So if it was like 15, it wasn't 15 frames, it was like 15 frames, then we took a span of the 15 different A span of the 15 different frames, and then we get a 15-dimensional subspace of whatever the number of pixels are. And then we did this, and we leveraged this sort of distance that we had that we were using to the cost function that we were using to create this median. And we were able to cluster, and then the clusters were fairly pure, meaning they actually were like mainly swinging. So we were doing this unsupervised, and it ends up. And it ends up spitting out like the labeled clusters. Okay, so potpourri dust. If you look at persistent location of responses to analyzing Windows at sort of a cross different scales, that can indicate the presence of a feature. You can also look at sort of persistent across scales responses to two different things to do separation of different data types and then you can. And then you can look at persistence of significant singular values across mesoscales when you're doing, when you partition points and do local escape. So sorry that was even dustier than I was planning with that potpourri dust, but persistent pays off. And thank you for your time. And I do co-organize an online seminar. Not all of them would be of interest to you. It's for mathematicians, but we did have Amit Singer come on, talk about priority MC. A beat singer come on, talk about priority and stuff. So, there are a few things that might be of interest, so check it out. I mean, when I first spoke, I remember some paper I read. Don't read what it was. So, central data, you're data, especially I'm looking for it, you have sparse and noisy. So, then the cause of problems, that's the thing of the cell cycle, right? Then the mitosis part. Right, then the mitosis part is very quick. You really miss that part. You assume that's a really potential thing complete. So, I wonder why you do, I'm not applying this to the real data, what kind of thing we should be careful. So, what you're saying, the problem is that you have things that are not actually closed? Because I don't have enough something. So, you know, how yeah. Well, actually, I mean, with the persistent homology, things that aren't quite closed. Things that aren't quite closed is actually really great data because it's gonna, it'll it might capture that. Well, it depends on how you're doing. If you're doing self-level set, it's not it wouldn't work. But if you're doing something that's like the point balls where you're looking at like the last two slides, then that would sort of know that that structure should be kind of enclosed, even if it's not. So, there was one paper that I had read where they were looking at. Where they were looking at like air sacs in the lungs, which air sacs are not closed, right? But because the way they're analyzing it, then they could kind of count the number of air sacs by running processomology on this data that they had had because it knew how to sort of close them off. So they would have a persistent three-dimensional hole that, even though they weren't actually a sphere. I don't know if that quite answers your question. Well, it's probably what's the you know, their main requirement or something fancy, right? That after a point you might not be able to get a concrete bounce. Is that super I mean, up to a point you can't sorry, say that again. Sampling density of what? Density of what? For the. Well, I mean, if you want to prove something, then yes, you need the sampling density to be a certain amount, I guess. But if you are just running this as a technique, then I don't know what the bounds are, especially for this super level, like looking at the small, these small barcodes. I don't know if there's really a whole lot of proven. Lot of like proven minimum requirements because like it's like these are the things that we can't really prove about. It's like the noise, it's the stuff that's not really easy to mathematically model. But yeah. Yeah, we'll pull that up. Yeah. All right. Yeah, so concerning the noise, how do you do to know if it's noise or technique? At some point, Or a texture at some point, and if you have, if you know you have a part of like a specific texture, how do you could you know? So there are some techniques that I have not yet worked with. This was actually supposed to be the next step of this undergrad project that like didn't, yeah, but there are some techniques and some works on trying to estimate like fractal dimensions using. Like fractal dimensions using output from persistent homology, and so like that should be distinguishing because like the noise is not going to have that should distinguish the noise out. But I think if you have small enough texture, then like it's basically noise. So I think it's a scale issue would be my guess. With the persistence landscape signature effectively using PCAs and stuff, So honestly, this PCA thing was literally just like, I just tried this for next week's meeting and did not expect it to work. So why it worked? So one of the things that I've been thinking about is. Thinking about is how surprising is the result? Like, is there, there's certainly, you certainly can't get all possible 2000-dimensional vectors if you play, if you play the game we just played. So, what is the space of all the possible vectors you could get from that particular algorithm for vectorizing persistent homology? Does that space inherently have some sort of lower dimensionality? I mean, certainly three is not it. I mean, there's obviously something was dimensional. It. I mean, there's obviously something listament of it. How much is it reduced from a mathematical perspective? So that's something that I am interested in because it was just very surprising that it worked. It's literally a, hey, do it and then get back to us and then it worked. But yeah, if there's other, there's certainly like people talk about measuring distance between persistence diagrams using sort of Bosserstein type metrics. I don't, another I don't. Another question might be: Is there a relationship between metrics that you could put on the vectors and then, like, Bosserstein on the diagrams? Maybe there is a relationship, maybe not. I can't answer that question. I don't even know if it's been asked, but that could be a reasonable thing to think about. All right. Thanks again for the presentation. 