Thank you for attending to this talk. My name is Jajetao, and I'm a first-year PhD student from University College London under Dr. Ni's supervision. I guess she's there. Hi. And yeah, today I'm going to talk about the signature optimization via a method called trivialization. So, as many of us have already introduced the signature, I will go brief on this. So, suppose we have So suppose we have a continuous path defined on a closed compact time interval. The signature of the path is just a series of non-commutative monomials, where each of them consists of the iterative integrals with respect to the tensor product. The signature of a path provides a top-down description of the path itself in terms of its effects on non-linear systems. On nonlinear system, since it appears from by applying the Picker integrals, Picker iterations to the control differential equations. Now, I want to emphasize three key properties of signatures, which I'll recall later on. The first one is that signatures are invariant with respect to time parameterization. And signatures have the universality property. So any continuous functionals So any continuous functionals on continuous path can be well approximated by a linear functional on the truncated signature, where the degree of signature is high enough. And the third one is that the expectation of the signature of a continuous stochastic process can completely determine the law of such process under some regularity conditions, such as the infinite radius of convergence of. Of the convergence of the expected signature. Now, going back to our topic, we are interested in solving the optimization problem on the signature manifold. So, given a real value function f defined on the signature manifold, we wish to find the minimum. Why we're interested on that? Because solving the problem can help us in other tasks, such as the generative modeling, finding the curvature formula. finding the curvature formula for all the stochastic processes, the anomaly detection, etc. Now, since the manifold is a Lie group, we cannot use the standard Euclidean optimization algorithm such as stochastic gradient descent or ADAM. One way to solve this problem is to regard the manifold as a Riemannian manifold and apply the Remaining method, such as Remaining Gradient Descent. Remaining gradient descent. However, in that case, the method suffers from the cost of dimensionality since the terms of the signature of growth exponentially fast with respect to the degree we take. And under the remaining setting, for every time we update our data, we need to change the metric and the reference point. And that will be really computationally inefficient. Efficient. So we provide a way to link the classical optimization method on the Euclidean space to the manifold via a method called trivialization. So a trivialization is just simply a subjective map from the inner product space to the manifold. Why are we interested in doing that, applying this generalization map? Instead of focusing on the optimization problem, On the optimization problem on the manifold, we could instead solve another transform problem, which is the optimization problem on the inner product space of this composition of maps. So trivialization methods enabled us to transform the constraint problem on the manifold to an unconstrained one on Euclidean space. Of course, this map needs to be subjective because it needs to cover all the possible elements in the All the possible elements in the manifold, but there's no free lunch. So, if we focus on this optimization problem, we might lose some information about the structure of the manifold. But if this map is furthermore a global diffomorphism, then there's no information lost. And we present the result by citing the theorem from the paper. The theorem from the paper of Mario Escano Casado, which states the following. So, if we take phi to be a global diffomorphism, then solving the modified problem through standard gradient descent is equivalent of solving the original problem using remaining gradient descent for a certain metric on the manifold induced by this trivalidation. It means that if phi is a global diffeomorphism, we won't add any additional. We won't add any additional local minimum or local maximums to the problem. So our algorithm wouldn't stack in such local minimum or maximum. And for the case of signature manifold, we have a very natural choice for this trivialization and for L. In this case, the space of log signature, which is the Lie algebra space associated to the manifold. And if we take And if we take to be the tensor exponential restricted to this space of log signatures, then phi is a global diffomorphism. Hence, the theory can be applied. And now I briefly describe the algorithm I use for the optimization. So we initialize some log signature, L0, and its corresponding signature, S0. We set up the learning rate and the training steps. rate and the training steps and we do this loop so we first compute the euclidean gradient of f with respect to s with respect to the signature then we compute the gradient of this composition map with respect to the log signature as a function of the euclidean gradient once obtained this gradient we could apply any optimization algorithms we wish in this case for example adam to get Example addem to get our updated value in the logging signature space. And finally, we lift up this log signature to the signatures manifold via tensor exponential. And we repeat this procedure until we get some convergent result, S T star. So the hardest part here comes from this red line. But how do we compute this gradient? But first of all, this Euclidean gradient we computed is something wrong because Is something wrong because it disregards all the structure of the manifold we in this case designature manifold so we couldn't couldn't use this one as the gradient in gradient back propagation so the vague idea here is that we send this uh object this gradient to its dual the differential of f with respect at the reference point st in that case we In that case, we get rid of the effect of this inner product structure of the manifold, and then we apply the d exponential to df to put it back to obtain the d differential of this composition map. And then we use the inner product structure of the log signature space to get the break gradient back. And this is summarized by the following statement. So we work on truncation. Work on truncated tensor algebra up to degree n, and the paths are d-dimensional. And this tensor algebra space is equipped with a canonical basis and a canonical inner product. We define the tensor exponential map, and we denote Sn and Ln to be the signature and the log signature state. For Ln, we choose a basis, which can be any whole basis or Linden basis. And this will induce This will induce a map M, which is the inclusion map of those log signatures onto this tensor algebra space. And then after some calculations, we're obtaining the following result, the back propagation rule for the log signature. So here, Q is the curve factorization of this inclusion map M. And the reason why Q is invoked here is Is invoked here is that the basis we choose for the log signature space is not an orthogonal basis with respect to the canonical in the product structure in the tensor address space. So we need to do some orthogonal normalization to this basis. That's why Q is involved. So we have this gradient by propagation, and we can do optimization. And next, I'm going to talk about And next, I'm going to talk about a numerical example, which is an ongoing project currently involved into the generative model for time series of variable length. This is very common in the real world. For example, in financial markets, the limit orders of certain underlying assets during the next fixed term, a period of term. Or in the medical scenarios, we have the medical scenarios we have the number the number of patients that comes within the next hour or the next day etc here as i mentioned before signatures are invariant of the time parameter station so it's a natural candidate to encode the information of such type of time series in a very compact way to build the model we follow the sigw1 gain framework proposed SICW1 GAN framework proposed by Nij, Prush, Sevat, Vidale, Xiao, Huiz, and Liao. This is a variant of the W Vasistan GAN, which avoids the min-max optimization. So in the classical VASISTEN GAN, the laws they use, so-called Vasisten-1 laws or the Earth-moving laws, which takes the maximum and moon all the Lipschitz continuous functions applied on the path. Applied on the path space. And since we have the universality feature of the signature, we could replace this Lipschitz continuous, the family of Lipschitz continuous functions to linear functionals. And it turns out that this maximum has an analytic solution given by simply the L2 norm of this discrepancy. Hence, we eliminate this map and the training is more. And the training is more efficient now because we only have to solve one minimization problem. And the sigw again simply consists of a generator which maps noise from the latent space to the space of a path. Then we rise everything up to this space, we lift everything up to the space of signature, and we calculate this loss and take the L2 norm. And x is the path value random variable that we try to learn. Here in practice, x needs to, one needs to specify the path length for this x. So it's not a desirable model for our time series of variable length. So based on that, we slightly modify the model using the trivialization method I just talked about. So we build an alternative generator. So we build an alternative generator, G telta, which maps the noise and a positive integer larger than 2, which corresponds to the length of the path that we want to generate. And the generator maps these two things to the space of log signature up to degree n, and it's lifted to the signature manifold by the tensor exponential. And the loss we calculate is the following one. Is the following one. So, this is a conditional generator on the length, path length that we try to generate. To illustrate the potential of this model, I prepared a toy example. So, we have simulated a synthetic data set consisting of Poisson random works with shifted Gaussian increments. You can see that in this graph, those Can see that in this graph, those blue lines. And the red lines are the path that is generated by our model. And this is still an ongoing project. And currently, I'm not able to eliminate it. You can notice there's some bias when the time length is large. So currently, I'm not able to eliminate this bias when time length is large. So, yeah, any suggestions? Any suggestions would be really appreciated. And that's pretty much it. I want to talk. Thank you very much.