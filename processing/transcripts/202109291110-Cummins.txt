All right, so yeah, it's my pleasure to welcome Bri to give us a talk, the last talk of the session. Go. All right. Well, thank you everybody for being here and thanks especially to the organizers. This has been a great workshop so far, extremely interesting. I am going to talk about a topic that's come up once or twice before. That's come up once or twice before, which is how do you figure out what regulators are given time series data? And this is work that involves just a ton of people. And the only ones that are listed here are the most recent ones that I have worked directly with. There are a whole bunch of others that are unfortunately left off of the list. So, what I am going to concentrate on is On is discovering from time series data cellular clocks. And what we're going to think of as a cellular clock in this case is a minimal, strongly connected network with negative feedback that is going to drive large-scale oscillations in the cell. So there's a big assumption in there, which is that the cellular clock is actually relatively small. And I was happy to see in some other talks that that is somewhat of a consensus. Somewhat of a consensus that you have a driver, but the driver is relatively simple. And then the question becomes: how do you reduce the hairball that you get through a lot of methods into something that is relatively small? And especially, how can you do that in a non-model organism? That's our ultimate goal is to be able to find regulators for cellular clocks in non-model organisms. Organisms. So, one thing that has become clear over the last several years in a large project that I've been on is that it's important to have a lot of interaction between analysts and experimentalists even before the experimental process starts. So, that folks are on the same page, they know what data are going to be collected, the data are appropriate for the analyses that are available. The analyses that are available are appropriate. The analyses that are available are appropriate to the experimental question, and so on and so forth. A big issue for us has been the fact that if you want to deduce causal interactions, like these regulatory interactions, then you need sufficient data to do that, which in many cases is time series. In fact, all of the methodologies that I know of for deducing causality come from time series. And in regard to the last couple of talks, And in regard to the last couple of talks that we've had, I think probably cell growth is in some sense, can be considered in some sense a proxy for time. So I've got a bunch of methods listed here for ways of trying to deduce causality. What I'm going to talk about is sort of a hybrid discrete ODE modeling system for trying to figure out regulators. And again, as I mentioned, what we need are time series, but the issue is. What we need are time series. But the issue is when you take transcriptomics or proteomics data, you get just a massive amount of it. You have hundreds to thousands of gene products, and we're going to be looking for a core oscillator that's got 10 genes or so, something relatively small. So we've come up with a tool chain to try to do this, which is an iterative hypothesis reduction. So first we're going to try to reduce the number of genes, then we're going to Then we're going to take the top genes of those and try to reduce the number of pairwise regulatory interactions that are going on. Then what we're going to do is we're going to try to reduce the space of global interactions or networks, network models. And then what should come out of this tool chain is actually guidance for doing experimental perturbations. So that's what we would like to do. Given data from some non-model organism, Some non-model organism, how can we help an experimentalist decide what expensive perturbations to go ahead and do? So, the main tools that we have for doing this are DL by JTK for reducing nodes. Now, I think Jennifer talked about JTK yesterday and how she modified it for looking at damped oscillations. We are looking at JTK in the original sense, so non-damped oscillations, but we're combining it with a delective. We're combining it with a De Lectenberg measure of amplitude. And so it is also a modification of JTK. For edge reduction, we're looking at a numerical technique called the local edge machine. And then for network reduction, we're going to be using DSGRN, which Marcio talked about yesterday. What is interesting, one of the interesting things about this tool chain is that each one of them uses different features of the time series. Different features of the time series data. So we're actually milking this data quite a bit. And we also think of this process as doing a set of increasing dimensional reductions. So when we're reducing nodes, we're starting out with a one-dimensional reduction. We have a single node. And then edge reduction is operating on pairs of nodes. Network reduction is then operating on groups of nodes. Operating on groups of null of groups of nodes, or rather, groups of regulatory interactions. So, I'm going to go briefly into each of these methods. What DL by JTK does is it uses the strength of the periodicity and the amplitude versus a baseline that's constructed with the data. So, basically, you look and you see how strong an oscillator each gene is. Oscillator, each gene is, and then those are scored and ranked. This process is definitely helped if you have some manual curation. So, what we have done in some unpublished work is gone through and identified a bunch of transcription factors, and then we have used DL by JTK only on those transcription factors. And when we have done that, we have found that the top of the DL by JTK list. Of the DL by JTK list is enriched by known cell cycle transcription factors. Not to hide the details, there are a bunch of meta-parameters that you have to care about in terms of running this numerical method. And one of the parameters that's meta-parameters that's going to come up over and over again is how you pick your top scorers. In this particular case, we're just going to choose some number at the top of the list. Some number at the top of the list, whatever happens to seem reasonable numerically. For edge reduction, the local edge machine does an optimization process on pairwise Hill models of regulation. So you'll see both the activator and the repressor Hill models below. Each one of these Hill models predicts a trajectory of the X variable. Of the x variable here. So, what we're doing is we're testing the impact of y on x. Given the data from y, we end up getting a model trajectory for x that is compared to the data and better parameter set. And this is explored over parameter space. And you end up getting parameter sets with the best performance. However, the scoring occurs not over just that performance to the data trajectory. Performance to the data trajectory, that similarity to the data trajectory, but also the concavity of parameter space in that region. Because what you want to do is you want to have a model that performs well over kind of a broader region of parameter space. And I think that this is related to the stiff and sloppy problem that John just mentioned. What comes out in the end is a probability score over all possible regulations between the nodes that were. between the nodes that were between all of the nodes that were put into the to the to LEM. Again, there are a lot of meta-parameters and particularly here you've got constraints on the ODE simulation, so on the parameters that are that are allowed to be explored. And then again, you've got a performance threshold that is some probability score. For network reduction, what we're going to try to do is What we're going to try to do is consider the space of all possible networks and for all of the top-edge interactions from LEM and try to figure out who are the best performers using how well they oscillate because we're looking for cellular clocks. And then also how well they match the data in a combinatorial sense using a graph matching technique that we're going to call pattern matching. Now, this, the previous two. This the previous two methods were exhaustive on their inputs. However, network space is so huge that we have to use a sampling method. And furthermore, we have to use a local sampling method. So what we're going to do is we're going to take a portion of the very top limb edges, just a few of them, like two to five, and that is going to function as a seed network in network space. And we're going to do a local search. And that local search And that local search is going to be constrained by meta-parameters that are somewhat non-intuitive, unfortunately. What's very interesting and important to note, what we have discovered is that if you take lem top edges, both as a seed network and in regulatory interactions that you're going to use to perturb around the seed network, that puts us in a highly oscillating region of network space. Region of network space. If you choose the bottom-ranking LEM edges, you end up in a very non-oscillatory region of parameter space. So we already have some indicator that LEM is working quite well. So now I'm going to go into a little more detail on network reduction because we have an a standard view of the data and what we're using to propose consistency with a model. With a model. So here we have some small data sets, two replicates of microarray data of the yeast cell cycle. And what we're going to do is we're going to look at each of the extrema, so the maxima and minima of these time series. And what we're going to try to do is put them in some kind of order. Now, one of the issues is that we expect our time series to be noisy. So Noisy. So the user, whoever it is, is going to specify a specific noise level, which is the percentage of the maximum minus the minimum for each time series. So in this particular case, I'm choosing 1% noise. And we're just going to fatten each curve by plus or minus 1%. And what that ends up doing is instead of having a single time point for your extremum, there's a time interval. There's a time interval in which you can say, Well, I think my extremum occurs somewhere in here. I'm not exactly sure where. And what happens is if your time intervals for two extrema are disjoint, then you know that they're totally ordered. You know that one of them had to occur before the other. But on the other hand, if your time intervals overlap, then your extrema could have occurred in any order. You don't know exactly what's going on there. What's going on there? So you can see, even for similar replicates, you can get noticeably different partial orders. Now, what we're going to say is that if you have any linear extension of that partial order, it is representative of the data at the noise level that you picked. It is consistent with the data. So, what does that actually mean? Well, first of all, Well, first of all, the extrema include endpoints, and they're exact points at which you're going to observe an extrema before you put in the time interval. So in this particular case, we have four to start out with. We have three minima and one maximum at NDD1. And if you look at the partial order at that top layer, you can see that all of those extrema are incomparable. That all of those extrema are incomparable. You don't know in what order they occurred. And that kind of makes sense because that's where your time series started. So you don't know that, but what you do know is that all of them have to occur before the NDD1 minimum. So in any linear order that you have or linear extension, you have to have NDD1 in the fifth place. That is always going to be true. And then likewise, your next three. And then, likewise, your next three extrema are incomparable, so they can be in any order, but then you're forced to have an NDD1 maximum. So this is what a linear extension of a partial order is. And this is, like I said, there's a whole collection of these that are associated with each partial order. And a match to any one of them, we're going to say is representative of the data set. So, that is a combinatorial expression of data, and it turns out that we can match that to DSGRN model output. So, let me just go ahead and briefly remind you what DSGRN does, and then show you the similarity to the data, to the extrema in the data. So, remember that you have a network, and each one of those edges is associated to a threshold. And when the constant Threshold. And when the concentration of one of your nodes moves above its threshold, then a regulation event occurs, whether activating or repressing. So these thresholds divide up phase space, and then using a switching system and a specific DSDRN parameter, what you can do is assign this coarse vector field in phase space that only exists on the boundaries between the boxes. Between the boxes. So we can take this information. Oh, and then there's an important thing, which is that you can only get Extrema on these boundaries. And the reason for that is because the switching system has monotone trajectories inside of each box. So if you're ever going to change direction in a trajectory, it can only happen when you move from box to box. So let me give you an example. To box. So let me give you an example of this. On this top half of the boundary that's labeled theta y, what we can say is there might be a maximum in y here. And the reason that that can happen is because on the right side, you have y increasing, y is going up. But then after you cross this threshold, you This threshold boundary here, y is decreasing. So that is the picture of a maximum here. And then, likewise, if you go to this next boundary piece, you can see that at first, x is decreasing. And then after you pass the boundary, it's increasing. So that's a minimum. And you can do this for all of the pieces of the boundary. All of the pieces of the boundary. The ones that I haven't marked are a little bit special cases, so let's just not worry about them for the moment. What you can see is that we have these extrema going around in a circle, going around in an unstable oscillation. So, what you can see is that if you have data where you have an extremum in a maximum in y first, and then a minimum. First, and then a minimum in X, a minimum in Y, and then a maximum in X. Then that would be, if you have data like that, then that is consistent with this model at this specific DSGRN parameter. However, if your data have a maximum in Y and then a maximum in X, you know that this model can be excluded. It can't actually occur. So remember that in DSGRN, what we do is we turn. What we do is we turn this phase space into a state transition diagram. And for the purposes of assessing consistency between time series data and one of these parameterized models, we're going to label this state transition graph with Extrema. So the process is that you take a proposed model, like the one you see over there on the right, for each DSGRN parameter. For each DSGRN parameter, you construct a labeled state transition graph. Then you also combinatorialize your data, and you look for a linear extension of that data in the state transition graph. And I am not even going to begin to tell you how we do that. I just want you to trust me, there's a match here we checked. All right, to go back to the inherent dynamics pipeline, recall that what we do is we Line. Recall that what we do is we go from node reduction to edge reduction to network reduction. The different characteristics of the time series that are used are amplitude and period for node reduction, the normalized continuous trajectory for the local edge machine, and then the timing of the extrema for DSGRN. So they all three are grabbing different pieces of information. Then, of course, each one of them has a Each one of them has a performance criterion. So from DL by JTK, we're going to choose the top n edges, which is a free parameter. For LAM, we're going to choose all edges above a probability threshold, where again, that's user supplied. And actually, there are two probability thresholds there. One is for the choice of a seed network, which tends to be a very high probability. And then some number of edges or some number of edges with a certain probability. Some number of edges with a certain probability below that to perform a local search in network space. Then, for network reduction, we're going to choose all networks that are above a certain scoring threshold. That is complicated, and I will talk more about that scoring threshold because it's in part, it's because how do you quantify success after you've gone through this pipeline? Pipeline. Because ultimately, what we want to do is provide guidance for experiments. So, how on earth do we choose proper scoring criteria so that we can actually distill information from someone who wants to use this technique? Our current choice is the following. The output from LEM is a ranked list of regulatory interactions. So, what we're going to do after network reduction, instead of After network reduction, instead of giving the experimentalist a big collection of networks, what we're going to do is we're going to re-rank the edge list that we got from LEM. The scoring criteria for top networks are going to include how robust are the oscillations that we see in every network, and what is the ability to pattern match our combinatorialized data. We're going to do this by computing a prevalence score. By computing a prevalence score, which is the percentage of times that an edge appears in the collection of top networks. And if an edge has a zero prevalence score, it's just going to be dropped. It's not going to appear in the re-ranked list at all. So this means that when we're done, the high-ranking edges are predictions for important regulations to the core oscillator or the cellular clock. And remember that because we're ranking things, lower numbers are better than higher numbers. Numbers are better than higher numbers. So if you're closer to the first rank, you're better off. And I will try to remind you of that because it does get a little confusing. We have two test cases that we're going to look at. One is a test case that we made up with synthetic data, and then the other one is going to be some yeast cell cycle data. So what we did was we searched for a highly oscillatory network that was relatively small, five or six. That was relatively small, five or six nodes, that had somewhat complex interaction at at least some of the nodes. So we found the network that you see pictured here, and then we found we looked for three DSGRN parameter regions that we could sample to get Hill model parameters. So the data that you see on the right were produced from three different DSGR and parameter regions. Three different DSGR and parameter regions that had been sampled for real numbers. They're produced with Hill models with, I think, a Hill coefficient of four. Each one of these time series, they have widely spread partial orders. We tried to look for parameters or for data sets that had very well-separated extremal partial orders. So, success in this case is Success in this case is the interactions that you see in that network are highly ranked. That would be a success here. So, here is an example of a re-ranking that takes place after we've gone through the pipeline. So, we have the edge listed in the leftmost column, the prevalence score next, what the original LEM ranking was, and then what the revised ranking is after we go through network. Is after we go through network reduction. So we started out with 34 LEM edges, and most of them had a zero prevalence score, and so were dropped out of consideration. Of the edges that are left, the boxed ones are the true positive edges. And you can see that there's kind of substantial mixing around of rank. Substantial mixing around of ranks when you go from the original edge ranking to the revised ranking. I have skipped over how we calculated the prevalence score. So this is the scoring criteria for what makes a top network. In this particular case, we happen to know what makes a top network. It oscillates over 100% of DSGRN parameters. And we're also going to require a high level of pattern matching. Level of pattern matching. So, given that you have an oscillation, somewhere in that oscillation, there's a pattern match over 50% of DSGRN parameter space. So all of the networks that matched that scoring were used to calculate the prevalence score for the regulations that you see here. Now, I'm going to go ahead and give you some summary statistics. This table shows. This table shows whether or not we're in the top, middle, or bottom data set shown on the right. Our sample, our local search in network space, consisted of 2,000 networks. And this column you see highlighted in orange are what I'm calling validated networks. Now, this is very interesting. A validated network is a network that at some DSGRN parameter, at least one somewhere in parameter space, you have a stable oscillation. Space, you have a stable oscillation with a pattern match, which technically means that, which roughly means that if you had a Hill model, you could find a Hill model of the network, you could find a parameter that would match your data. So basically what we're seeing here is that many, many models in the region of network space that Lim put us are capable of reproducing the data, which is pretty. Which is pretty amazing. I mean, I think a lot of you know that given a sufficient number of parameters, you can fit many different data sets. But after you put in the scoring criteria, the number of networks is reduced drastically. So the top number of networks that we have is quite a bit smaller, is an order of magnitude or more smaller. More smaller. However, the number of top networks is still too many that we'd want to go over by hand, like 66 of them. The next two columns indicate the relative performance between edge reduction and the re-ranking in network reduction. So let me just explain what you're seeing here by looking at the black box. The mean rank of the true positives. Of the true positives that are left after re-ranking is given first. It's 9.2. And then the median of those true positives is 5.5 after edge reduction. After network reduction, you get an up ranking because it's a smaller number in the mean, which is 7.4, but you get a down ranking, which is 6.0 for network for the network rank. And if you end up looking at that. And if you end up looking at that whole all three of those data sets, you don't see that there's a ton of difference when you go from edge ranking to network ranking. However, we end up dropping a large number of false positives. So what network ranking is doing in this particular case is shortening the list of things that you have to consider for potential true regulators. Now, you will notice that in the Notice that in the bottom data set, we had one true positive dropped. So, all of a sudden, we had this question: all right, what's going on there? Why did we drop that true positive? So that ended up being a very interesting story. The true positive that was dropped during network reduction was the edge D repressed by E. It turned out It turned out that there was another edge that was dropped during edge reduction, which was C repressed by D. So during edge reduction, we lost D's only influence on the network. And then during network reduction, we lost one of the inputs to D. So we took a little bit closer. So this leads to the question, is D really part of the Is D really part of the core oscillator in that third data set? And so, what we did was we went back to the data that we generated and we dropped the parameters for D and went ahead and reran the simulations. And the results were incredibly interesting. So if you look at the top row, we found that you kill oscillations entirely if you take out node D. So node D is just critical to that process. It's part of the core oscillator. Of the core oscillator. In the middle row, you still have some oscillations without D, but they're lower amplitude. But in the bottom row, you still have strong oscillations when you take away D. So that is evidence for the fact that perhaps the core oscillator is actually the smaller five-node network that's pictured above. There is further evidence for this because you can recount, you can recalculate the mean and median for each of edge reduction and network reduction while throwing out all D edges. So you no longer consider any of the three edges for D to be true positives. And then when you do that, both edge reduction and network reduction end up having higher scores. So if you look at that bottom row, you can see Bottom row, you can see that the mean and the median for each one of those are higher and therefore worse than the mean and median when you take away the node D. So our conclusion was that, oh, D is not part of the core oscillator in that particular data set. And each one of these three data sets is sort of like a different experimental condition. If you had If you had this single operating network. And so, what we're basically saying is: well, under some experimental conditions, your core oscillator can actually change. But the really big lesson for me was when I make synthetic data, that does not necessarily mean that I know what I'm doing, which was really kind of disappointing because I would like to be able to make test data that gives me an actual answer. An actual answer. But the truth is here, I can't. But what was incredibly interesting was we did not know this until we ran the method. So this was a prediction of the method that we then had to go and find. So that's actually, that's a really good sign that our pipeline is doing something. So now let's go on to the yeast cell cycle data. We have to come up with what we think are true positives in this particular case. Positives in this particular case. And the true positives I think of as higher probability regulators. And because I don't think, and yeast experimentalists in the meeting can correct me, but I don't think it's fully known what are the regulators under all possible experimental conditions. So, what we did was we took a set of true positives that seemed to be. Positives that seem to be supported by experimental evidence. So, true positive nodes that appear to be involved in the cell cycle. And I think we have, I think it's 10 of those. Well, I also picked out a couple of what I'm calling true negatives as well to try and test what happens when you have a mix of true positive and true negative nodes. To get those true negatives, what I did was I took the output of dl by JTK and By JTK, and I picked two nodes that were high-ranking, so they're highly oscillatory. It's this EDS1 and RIF1. However, when I looked at Yeast Tract, which is sort of a composite of all of the information in the literature about potential regulation, there was no interaction between EDS1, RIF1, and any of the true positives. There simply weren't. True positives. There simply weren't any regulations there. So we're going to call those true negatives. For the ground truth edges, we took all of the yeast track edges that we found among the true positives, and then we also added edges from this model that came out of Steve's lab that was published in 2017, where we added in the nodes Clin3 and We5. We5. And those two were added because it is suspected that the feedback from SWI5 and ACE2 to the beginning of this network may be important. For that reason, I also picked a node called ASH1, which is not shown here, but which is also part of a pathway that goes from SWIF back to the beginning of this network. Now we chose a sub-network that mostly got rid of the cyclins and kinases because we're interested in. Because we're interested in the pulse generator of the cell cycle oscillator. So we're interested in this transcriptional wave that travels down the blue backbone here that ends up triggering the cell cycle, although not under all conditions. However, even though we have what we're considering true positive regulators and regulations, we do And regulations, we do not consider that we know the ground truth network. We're not, we assume that that is still unknown or uncertain enough that we can't count on it. All right, so here is a table of results. Let me explain what's going on in that left column. LEM has the option to take in an annotations file. And an annotations file tells you whether or not each node could be. Whether or not each node could be a repressor or an activator or both. So, if you happen to have some manually curated data that tells you how a node is acting, you can actually input it into this system. And that gives you extra information and cuts down hypothesis space even more. So, in that first row, we have annotations and we have only true positive nodes. In the second row, we have only true positive nodes, but no annotation. True positive nodes, but no annotations. In the third row, we've got all of the true positives plus the two true negatives that I talked about and annotations. And then in the last row, we have true negatives and we have no annotations. In this particular case, because we have so many more nodes than the synthetic network, I took a larger sample of network space, 4,000 networks instead of 2,000. That is arbitrary. I did not calculate it in any. Arbitrary. I did not calculate it in any way, and it is not necessarily a good sample of the local network space. I don't, I can't give any guarantees about that. Again, we get a ton of model matches, over 2,000 for each of these situations that are validated. Now, the scoring for top networks changed. Previously, we knew we had a very robust oscillator, but with the cell cycle, we know that we oscillate. Cell cycle, we know that we oscillate, but we also know that the cell cycle has to be arrested in a number of places. So there's no guarantee that it's actually a robust oscillator. In fact, it might be important that it is not a robust oscillator. So somewhat arbitrarily, I said, all right, let's ensure that we have 10% of oscillations across the SGRIN parameter space. But given that we have an oscillation, we must have a pattern match because the peaks. Because the peaks in these genes are highly stereotyped. So that was our scoring. And after we applied that, we ended up with hundreds of networks that still met that scoring criteria. So then we went ahead again and looked at the mean and median rank of the true positives that stayed in the re-ranking at edge reduction and then also at network reduction. And then also at network reduction. And what we found was that when you only have true positive nodes, LEM performs very well, and you don't really need the network reduction step. However, as soon as you throw in two true negatives, then you get up ranking of the true positives after network reduction. So again, you've got lower numbers on the right-hand side. Lower numbers on the right-hand side of that orange block, which means better performance. Again, we have a large number of false positives dropped after the re-ranking. But in this particular case, we also happen to have a fair number of true positives dropped. So again, now we have an exploration, just like for the synthetic network data, what is going on with those true positives that got dropped. So it turns out that. So it turns out that it was the regulations involving Clin3 that got dropped. If you look at the yellow column and compare it to the white column directly to the left of it, you'll see that there are lower numbers in the mean and median. So that is our re-ranking list if you assume that none of the CLEN3 regulations are true positives. So basically you get an improvement. So basically, you get an improvement if you take out CLIN3 as true positives. Now, what is interesting is that if you look at edge reduction, if you look at LEM ranks, then they show the opposite pattern. Performance gets worse as CLEN3 is taken out as true positives. So what that means is that edge reduction likes CLEN3, but network reduction does not like CLEN3. And so there's a fight going on. And so there's a fight going on, and our question then becomes: is Clen3 part of the pulse generator? So I found Jan's talk this morning extremely interesting because we have this evidence that Clen3 is maybe not part of the pulse generator. And my initial thought was, well, what could be wrong with this methodology that would take Clin3 out? For example, did we need proteomics data rather than transcriptomics data? Or is it the issue that? Data? Or is it the issue that DSGRN has not yet been extended to handle post-transcriptional modification in this pipeline? But maybe actually we found something real. So I was pretty excited about that. Maybe under these particular experimental conditions, CLEN3 doesn't actually have a very important role in the pulse generator. So that's interesting, and I'd like to hear everyone's thoughts on that. I have one. That. I have one more slide, which is maybe a terrible slide to end with, but it's this caveat that these results that I'm showing you are data set dependent. So when we used microarray data, we got the story that I told you previously with two replicates from Orlando 2008. However, Tina, who's in this meeting a few years later, took some RNA-seq data under very similar experiences. Seek data under very similar experimental conditions. And we did not get the same results when we ran the pipeline on these data. So there are differences. Like I mentioned, there are slightly different experimental conditions. The time sampling is different. There were different types of pre-processing. There's different levels of noise. So the question that remains for us is, can we characterize data types that will work better in this pipeline? Work better in this pipeline and data types that will work worse. So you know whether or not to go ahead and use this tool in this situation. And with that, I will go ahead and end.