In order to do this, I'll just try to showcase some examples. So if I had an hour, I'll actually present three parts. So it's actually a sequence of three efforts. But for the sake of time, I probably will only talk about two. So in the first part, I'll talk about how to optimize a function on a manifold, hopefully in an accelerated fashion. And then new part two. And then in part two, I will actually leverage on the tool that I developed in the first part to actually study how to generate samples of statistical distribution on a manifold. A hypothetical part three is about generative modeling, but hopefully I can have some time to just briefly mention it. If not, I'm happy to talk more offline. So, in fact, there are many things that There are many things that we do. For example, we actually do a lot of deep learning theory. We do scientific machine learning as well. Generative modeling is something that I won't talk too much about, but this is really what we found exciting. So hopefully you can be convinced by the end of the talk that really there are a lot more opportunities that we as applied and computational mathematicians can do. But anyway, let me Do. But anyway, let me actually start being concrete. So, accelerated Riemannian optimization. So, this is based on primarily joint work with my student Ling Kai Kong and Yu Qing Wang. So, this is a machine learning conference. You can find the paper online. If you have heard me talking about similar things before, I apologize. And in fact, there's another paper that I'll briefly mention. Briefly mentioned. This is also published on a machine learning conference. It's the best paper of World Paper. But anyway, so the problem that I'll be primarily talking about is to optimize a function defined on the Stiefel manifold. So what is the Stiefel manifold? You can think it as a collection of matrices, so n by m, that satisfy this quadratic constraint. Constraint. So sorry, I think it's a little small. But basically, you have an n by n matrix, and each column of this matrix needs to be normalized to one and also orthogonal to all the rest of the columns. So, of course, that requirement boils down to actually having the product being this identity matrix. So, there is a special case. So, if n is equal to m, so that you are dealing with square matrices. So, that you are dealing with square matrices, then you have an additional group structure. So, this manifold becomes a Lie group. Otherwise, you don't have a group structure, but generally just a manifold. So, why do you want to optimize functions defined on CIFOR manifolds? So, let me start with something very simple. So, I give you a matrix, possibly very big. So, n by n matrix that is So, n by n matrix, that is like, for example, 1 million by 1 million. I want to find the largest m eigenvalues. So, m actually can be very small, say 10. So, computationally, this is actually a very challenging problem. I mean, in the era of big data, so you oftentimes come across large-scale problems like this. You can't even store the matrix, or I mean, if you try really hard, you can, but then you don't want to do like matrix, matrix product, etc. Matrix, matrix product, etc. So this is a very non-trivial problem. If n is large, but maybe if m is small, you can do actually extra things. For example, you can actually tackle this problem by solving an optimization problem. Okay, so x is a matrix that is default. And what it basically does is it basically encodes bases of an n-dimensional subspace in n. Dimensional subspace in n-dimensional space. And what this operation does is it basically projects this matrix to that subspace, and then the trace basically returns the sum of eigenvalues in that subspace. So if you solve this optimization problem, you basically look for the subspace onto which the projection actually gives you the largest sum of eigenvalues. So that actually helps you locate the largest eigenvalues. And then, of course, And then, of course, later on, then you, because after the projection, you have a small matrix, and then you can just find the target values if you want. So, importantly, okay, here you have to solve the speedful optimization problem exactly automatically. So, in machine learning, it's a very popular trend to regularize everything so you can approximate basically all constraints, but that's not enough. So, if you have x, that is approximately on C for manifold. Approximately on C4 manifold, basically you mess up the basic structure and then you can get very wrong results. So, of course, you may say, oh, yeah, but how hard can this problem be? It's a quadratic optimization problem. It's actually harder than you think because you have this non-convex constraint, so you don't even have unique minimizer, et cetera. But if you still don't like it, okay, so here's the upgraded version. So it's called projection robot. So, it's called projection robust Washerstein distance. It's an approximation of something called Washerstein distance, which is really popular. So, what is Washerstein distance? So, very roughly speaking, so if you have two probability measures, Washerstein distance gives a way to measure how different they are. So, I won't have time to talk about all those beautiful optimal transport theory, but roughly speaking, so you have two projects. Two shapes of the piles, you basically want to move one pile to the other while being lazy. So you want to minimize the distance of travel. So if you formulate the problem like that, there is an equivalent formulation on the reasonable assumptions, which is this counter-authority formulation. So you basically have to solve some minimization problem in order to get this Washington distance. So that's a continuous picture. So that's a continuous picture. So there's also corresponding data-based algorithms, but this problem is basically very costly if you have a high-dimensional problem, if you have a high-dimensional probability measure. So that's why a group, actually several groups of great researchers proposed this notion of projection robust Washington distance. So the idea is very simple. Okay, so you don't want to deal with high-demand. Okay, so you don't want to deal with high-dimensional, right? So, why don't we actually find a subspace, an M-dimensional subspace, to project these two probability distributions on? And then you look at the Wash-Stan distance between the two projected distributions. If you do that, you can actually see it's equivalent to solving some problem like this, where you basically enclose the subspace. Okay, so here again, So here again, well, okay, maybe I should first say that this is computationally much more manageable when m is much smaller than n. And you can actually get things even beyond this advantage. So for example, the number of samples that is needed to approximate Washer Stand distance actually grows up exponentially with dimensions. So you better want to do it in a low dimension. And also, suppose you have a point cloud, and then there's an Like a point cloud, and then there's an outlier. If you do this projection, basically you find the dominant subspace, and then you actually get rid of outliers, you get robustness, etc. So there are many advantages of this approach, but of course it's only an approximation. From a computational point of view, you still had to solve this maximization problem on the CIFO manifold. And you want to do it exactly on the manifold. Field. So the last example is new. Well, some of you actually heard it when I talked about it before. Again, I apologize. In that sense, it's no longer that new, but it's 2023. So this new stuff is something that we are pretty fond of because it actually provides a way to improve the performance of something called Transformer. So what is Transformer? Transformer is a machine learning architecture that is actually very powerful. Architecture that is actually very powerful. So, for example, it's like the backbone of Checks GPT, etc. So, it was actually first invented for language models, so natural language processing, where your language is viewed as a sequence. But nowadays, it sort of dominated computer vision as well. I'm not going to talk about details, but roughly speaking, okay, so it has an infinite reference window. Infinite reference window. That is why it's helpful. For example, recurrent neural network has a short reference window. So we have heard experts talking about like LSTM and GRU, which is something similar that tries to improve the, make the reference window longer, but the attention mechanism which Transformer uses actually has infinite reference window. So it's accomplished by something called multi-headed, well, it's accomplished by something called an attention layer, which Called an attention layer, which has something called multi-headed attention. So, what multi-headed attention is trying to do is, roughly speaking, it's trying to correlate, well, characterize the correlations between word tokens. And of course, you don't really know how words are correlated unless you actually learn it. So, actually, each attention has trainable parameters. Parameters. These are just some non-squared matrices, mathematically speaking. So you basically solve an optimization problem to match your training data so that these matrices are learned. So it's widely believed that those matrices actually encode how words actually interact with each other. And they are learned. And then we ask ourselves: okay, so there are actually. So, there are actually not multiple ways in which words are interacted, words are correlated. So, maybe if you want to have the potential mechanism to be efficiently characterizing all possible interactions, maybe you don't want to waste those correlations. So, then we ask ourselves: okay, so each column of this matrix corresponds to one way in which words are correlated. So, maybe we Are correlated, so maybe we actually want to make the columns to be orthogonal to each other. And this way, we actually can make attention more efficient. So then the problem boils down to, okay, so when you train, you want to train those parameters subject to steel constraints. So we actually tried this idea on Vision Transformer, which is for images. So once you add orthogonality to vanilla vision transformer, we get performance that is way much better than the Euclidean. That is way much better than the Euclidean version. This is a really, really hot topic, at least at the time, I mean, depending on how you define part, but there are like thousands of later models, many fancier models, we can beat a lot of them by simply just adding alphabetality constraints. No other engineering techniques or anything deeper either. But there are actually multiple ways to do actually. To do actually, see for optimization. We are not the first one who does it, but who do it. But interestingly, for the ones, existing ones that you tried, you may actually get performance that is even worse than the Euclidean version. But if you train by our optimizer, you can actually get this superior performance. We don't have a proof of why. This is a highly non-convex optimization problem, but this is just an interesting empirical observation. Okay, so hopefully I motivated you about why we want to do this optimization. So the real problem is how we do it. So we will be using a variation approach. So we are going to be optimizing a function where x is defined on a manifold. And the variation approach is going to be very friendly to non-Euclidean spaces. And also it provides a way to naturally introduce something called momentum. Something called momentum. So what is momentum? So it really originates from mechanics. So it's the same thing as what we know in real life. So you can't avoid Newton when talking about mechanics. So Newton said, okay, so the rate of change of momentum is given by the force. So if you consider a conservative setup where you take this F as your potential and then you generate a forcing according to negative gradient of the According to the negative gradient of the potential, you can turn this idea into a concrete system of differential equations. And of course, that's great. But then Lagrange came along. Lagrange said, okay, this is actually what appears on the surface of something deeper. So Lagrange said, okay, so if you define a variational principle, basically you define an action functional, which is a function of function. Which is a function of function. So Q is a function of time. It basically indicates how your particle moves. And then, if you define S as a function of the function Q by integrating, for example, Lagrangian defined as kinetic energy minus potential energy, and then you look at the critical point of this action functional, then you basically recover Newton's equation. Okay, so of course, you may ask, how is this relative? Of course, you may ask, how is this relevant to optimization at all? So far, there's zero relevance. The total energy of the system is actually going to be a conserved quantity. So if you run this dynamics, you will just see constant exchange between the kinetic energy value and the potential energy value. So f is not going to be minimized. But that's really because you have actually a translational symmetry of your system, which gives you a conservation law, which is the conservation. You have a conservation law, which is the conservation of total energy. So, therefore, maybe what you want to do is to actually break the time translation symmetry of the Lagrangian. So, that's actually what this beautiful paper by V. B. Sono, Wilson, and Jordan did. So, they actually introduced a fictitious algorithmic multiplier. So, they multiply the physical Lagrangian by some explicitly timed. By some explicitly time-dependent quantity. And then, if you write down the Euler-Lagrange equation, the critical point of the action function of the n, you get this equation. And of course, you know, most of the people here can't easily recognize this is just a damped mechanical system. So it's actually draining energy from the system, so the total energy is being minimized. And you can easily prove that the Q dynamics, the Q component of the dynamics converges to a local minimizer. Converges to a local minimizer of the potential. So, this is how this dynamics is doing optimization. But of course, you know, this is not an algorithm yet. So, you have to discretize the time to get an algorithm. And interestingly, if you pick your R to be something such that gamma is equal to 3 over t, and then one discretization of this dynamics actually recovers a famous method in convex optimization called Lesterov. Called Nesterov accelerated gradient for convex functions. So the discovery of this connection actually made Wei DSU very famous. And there's another situation. So you can take your R to be a constant and then some discretization of these dynamics will actually give you naster of accelerated gradients for strongly convex functions. And another discretization gives you something called heavy ball. So if you use, for example, Heavy ball. So if you use, for example, PyTorch, the default is the variant of this. So all these should be contrasted with gradient flow and its time digitalization, which is called gradient descent. It uses no momentum. And you can see a difference. For example, so, okay, so what is this C stands for? So it stands for convex. So under the assumption of the objective function being convex, this is a good object. Being convex, this is a good algorithm because if you look at how gradient distance converges, it basically converges like 1 over k, where k is a number of iterations. But AGC actually converges like 1 over k squared. And this cannot be improved, actually, it's proved. Okay, so that's why, actually, that's one of the many reasons that people say if you introduce momentum, you can get acceleration. So, of course, all these great classical work. The task is to generalize things to, for example, Riemannian manifold. And if you try to do that at the level of differential equations, that's going to be really, really difficult. But our claim is if you do it at the level of variational principle, that's much easier. Easier. So, why is it difficult to work out a generalization from the differential equation? I mean, depending on your background, this may be trivial, but let me still say it because it's a cute example. So, for example, you may ask, okay, so here you are using the gradient. So, of course, on manifolds, you have to properly define ingredient. Some people may say, oh, it's simple. So, let's see. So, if here is a non-example, so let's just consider. Here is another example, so let's just consider 2 by 2 on top of the matrices. Last function is the determinant of the matrix. So, some people may say, oh, then you compute the determinant, you collect all the partial derivatives, that's your gradient. That's not going to work because the truth is, so the determinant of octave matrix is either going to be plus one or minus one, and on each connected branch of the manifold is going to be a constant, so the gradient should be zero. So, what's wrong? Zero. So, what's wrong? The reason is because this is actually a one-dimensional manifold. It's not four. These ABCDs are actually related to each other. So, you can't really collect partial derivatives. Of course, that's just the tip of the iceberg of manifold optimization, a profound field. So, our contribution is very specific. So, we have an intrinsic way of introducing momentum. Reducing momentum, we get faster speeds of convergence. We get accelerated convergence and we get very good accuracy. Yes, please. Why not use this as an optimization with constraints? I'll come back to that. Yes, you can do that. Yes. Great. Any other questions, comments? Okay. So let's, okay, maybe let me say. Let's let's okay. Maybe let me let me say one more thing. So, so for lead groups, there are different types of lead groups. So, for SON, for example, it's really just an easy constraint, quadratic constraint. So, you can do it. For other Lead groups, it might be different. So, in general, okay, so the variational principle is there to help. So, this is the Euclidean version. So, I talked about it. If you have a manifold situation, If you have a manifold situation, I deliberately change Q to G, which is also a function of time. It's moving on the manifold. You can have G with its time derivative, well defined very easily. It lives on the tangent space. The problem is the tangent space is actually moving. I'll come back to that. But for now, of course, you can say, oh, but then you just have to introduce a Riemannian metric and then you can define an analogy of the notion of kinetic energy. Of the notion of kinetic energy, and then you do this variational principle. If you're really good, you can do it. So, in fact, I know in the audience there are people who are that good. So, in theory, you can actually write down the Euler Lebron equation associated with this variational principle and get optimization dynamics. In practice, the ODE will be actually not very explicit. It's pretty hard to work with. And if you try to discretize it, If you try to discretize it, it's actually going to be even more difficult. So oftentimes you have to do input, solves, etc. So, for example, Melin actually has done many great work in a very general setup. But there are special situations. For example, if the manifold is actually a Lie group, then you can leverage the Lie group structure. Structure. So, for example, there's something called trivialization. So, the idea is, so the idea is G dot is in this tangent space, but this tangent space is nasty because as G moves, this tangent space also moves. So, in manifold optimization, people use things like parallel transport, et cetera, to actually deal with that. We don't do that. Here, we actually use the idea of pulling this tangent. Of pulling this tangent space back to the identity element of the Lie group. So, this is actually, so the tangent space at the identity element is called a Lie algebra. So, this is actually a fixed linear space. So, you can do that by multiplying, roughly speaking, this tangent vector from the left by G inverse. And then you can do everything. You can view your momentum variable as something that is a reduced variable living in this L. Variable living in this E algebra. And then you can define a notion of the kinetic energy. I won't go into details, but if you do that, and then you can actually write down optimization dynamics, which corresponds to the critical point of this variational principle, it looks like this. So it's the system of ODEs. G is the position. So G changes according to velocity. The velocity term is interesting. So it's G times P C. So it's G times C. So you really should think C as like an angular velocity. And then multiplying by the position, you get a velocity. And then this angular velocity actually, this really should be the angular momentum. So it changes according to net forces, but it really should be actually torque if you want to speak about physics. So this net force actually consists. Actually, it consists of three terms. So, you have this frictional term which drains energy. You have actually a conservative term, which sort of is the proper way of doing the gradient. This term is a Coriolis force term. It basically models the nonlinear geometry of the manifold. So, therefore, when you have a Li group, you can actually do things. Things. However, if you have a Stiefel manifold in general, your matrix is not necessarily square, you don't have a group structure, so this reduction idea actually fails. So that's why we actually use a different idea, which is what was just pointed out. So you can actually introduce constraint. So let me repeat. So previously, you have a variational principle. A variational principle, it looks benign, it's not because you have to do a variation in function space. And the function space is nasty because your x has to live on the manifold, and x dot has to live on the tangent space. So this is a very nonlinear function space, very, very hard to do variation inside. So, therefore, we introduce a function Lamrange multiplier. Okay, so this is just to ensure this constraint, this holonomic constraint, is always satisfied. This holonomic constraint is always satisfied for all time. And then you do this new variational principle, but this time in unconstrained function space. It's a flat function space. And then if you do that, in theory, you can write down governing equations. But that's not enough. Why? Because you have this additional Lagrange multiplier showing up in your dynamics. Everything all together have to satisfy some algebraic differential equation. Differential equation. So that's nasty, but we actually have technology to get rid of this Lagrange multiplier when you have a quadratic constraint. And then by the end of the day, you get explicit OD in Euclidean space. So of course, that's all very abstract. I can show you some examples. So you have to come up with some specific metric in order to show concrete examples. So yes. So yes. Sorry. What index of the uh DNA you get in your view differentiation? Uh I think one, but uh I'm I'm not sure. Great question. Okay, so if you introduce some metric like this, then we can I can show you something concrete. It doesn't have to be this, but this, you know, the one that I'm showing already compar uh consists of everything dis discussed is discussed in the literature. So Discussing the literature. So you can actually get some optimization. ODE, the details of the ODE don't matter. What could be worth noticing is, for example, so this guy is actually the Euclidean derivative. So that's the one that I previously mentioned that you should not use. But now you can just use it. The dynamics automatically cracks everything for you. In fact, you know, the dynamics is nice because you just have to work with Euclidean. You just have to work with Euclidean X, Euclidean Q. So n by n matrix, you don't have to worry about the underlying geometry at all. So the dynamics actually secretly takes care of the geometry. If your initial condition is on the manifold, the solution always stays on the manifold. So if you don't like thinking about tangent bundle, et cetera, so the manifold is actually very specific. So x has to satisfy this orthogonal constraint. Q, which is time derivative. Which is time derivative has to satisfy this constraint. Okay? And the dynamics is guaranteed to converge to a local minimizer. So actually the funny thing is, I've already almost spent all my time, but I haven't even started yet. So this is not an algorithm. This is not an algorithm. So this is, I mean, this is pure mathematician's toy, but we have to. But we have to actually numerically integrate the dynamics in order to get a numerical integrator or a numerical optimizer. So there are things, actually a lot of things that we want. So we want, after the discretization, the dynamics to stay exactly on the manifold. This is actually very hard, but this is what we want. And also, you care about computational cost. So we will actually be constructing some. So, we will actually be constructing some method that is explicit. Not only that, we really care about the computational complexity of each iteration. So, we want the computation to be as efficient as possible. So, for example, so previously I mentioned, okay, so your matrix is likely to be having a huge n and a small m. So, I want my complexity to depend as much as possible on m, but as little as possible on n. And of course, you know. And of course, you know, if you deal with manifold optimization or manifold dynamics, oftentimes you want to compute, say, the exponential map, which is in this case, maybe related to some matrix exponential, and maybe you want to use a Cayley approximation to simplify the computation. These are all expensive. I also want to avoid these as much as possible. So I have so many constraints. The problem is pretty. Constraints, the problem is pretty non-trivial, but we can do it. And it'll be really disappointing because I'm going to say, okay, I will not tell you the details of the integrator. But the solution is not very trivial. So we have to adopt some geometric decomposition of the momentum variable. And then we have to create dynamics that make sort of maintain the geometric structure. And we try to then do operators tweeting. It's not going to be enough. And then we have to approximate. And then we have to approximate the split operator, etc. So, by the end of the day, for example, you know, this is an AD-AM version, so adapted time-stamping. So, it's not trivial, but it's actually working. So, let me actually summarize part one. So, by using variational principle on manifold, you can actually do optimization, but it has to be more precise. So, for Lie group, we have a trivialized variational principle. For Stiefel manifold, we have a constraint variational principle. Principle. They gave us continuous optimization dynamics, which are interestingly, you know, even embedded into clearance space, but they secretly keep everything on the manifold. And then you have to discretize to get actually structured preservation and all nice properties. And these integrators actually correspond to accurate and efficient gradient descent methods, this momentum, for example, on Steefel Manifold, which leads to applications like orthogonal tension. Like orthogonal attention to projection robust brush-stand distance, et cetera. Also, there is convergence analysis. I didn't talk about it at all. It's actually very, very interesting, but hard as well. So, I do want to move on to the second part, which is about sampling on LeadLoop. So, what is sampling? So, let me first just bring everyone to the same page in terms of just the notation and terminology. So, sampling in flat space. So, something in flat space is basically the classical Euclidean version. So, you are given a probability density on normalized, and you want to generate samples of random variables that follow this density. So, oftentimes, you know, you use algorithms that are related to something called over-dent Langevine dynamics. So, you basically choose your potential to be negative log of the density of normalized, and then you run this. Normalized, and then you run this Stochastic differential equation, and under very mild conditions, the long-time asymptotic solution actually will be distributed according to Gibbs distribution, which is exactly the same as the target distribution. So, we want to do this on a manifold. Why do I want to do this? I mean, there are a lot of applications, for example. So, if you want to ever deal with the statistical mechanics, With the statistical mechanics of molecules, you have to have some configuration that models the, sorry, some modeling of the configuration of the molecules. So a lot of people actually use bump angles to model the configuration of the molecule instead of the absolute Cartesian coordinates. And bump angles are interesting. So they are angles. So if you have an angle of 2Ï€, it's the same as 0. So this is, you know, you do statistical mechanics. You do statistical mechanics actually on manifolds. So, data science applications. So, previously, I showed you, you know, you want to put the weights of neural networks on a manifold. And there is a Bayesian neural network generalization. So, Bayesian neural network is basically saying, okay, so maybe you don't want to fix each weight parameter to be a constant. Maybe you want to make it like a random variable. And then instead of optimizing the loss, you actually try to. The loss, you actually try to do sampling from the distribution, and then you have to sample from a manifold. A third example is also more classical. So if you deal with atmosphere and ocean sciences, oftentimes you want to do data assimilation, maybe you use a Bayesian approach, and then you want to sample, for example, from posterior distribution. But the distribution is actually defined on a globe, so it's a sphere, not a It's a sphere, not a flat space. So I can keep on going, but maybe I should move on by saying, okay, so that's why there are actually existing methods. So some of them require Oracle. So they require you to have exact implementation of Brownian motion on manifold, but in general, that's actually difficult. Other approaches discretize ground motion, but then you get reduced performance, you need strong assumptions. The performance, you need strong assumptions, etc. I'm not aware of existing methods that utilize momentum. So, our approach is to leverage momentum optimization mentioned in the first part to create momentum samples. So, the way to do it is to basically add noise to the optimization dynamics to have a stochastic generalization. So, how to do this is usually non-trivial. Like I said, I mean, you have to do noise or manifold, but for LEAP, Always a manifold, but for Lie group, the technique of left trivialization really saves the day because, roughly speaking, momentum actually lives on linear space. So then get algorithm that is easy to design, to implement, and you can actually get very pleasant performance. So the question of how to add noise should be elaborated. So for the sake of time, let me actually Sake of time, let me actually skip a lot of details. But basically, you have to create a detailed balance, which is actually pretty non-trivial. So for example, so we actually, okay, maybe this is interesting. Let me just mention that. So for instance, okay, so if you want to minimize a function that is, minimize a function over a convex constraint set, there is something called mirror map. Mirror map, mirror descent method that does so. So, mirror descent method is a classical optimization method, but there is a modern view of viewing it as a discretization of some continuous time dynamics. So, the idea is you introduce a mirror variable, a dual variable, which is unconstrained, and then you do dynamics in the unconstrained space, and then you switch back, and then after discrimination, you have to switch back and forth at each step. Switch back and forth at each step, and then you can get minimization of this dynamics. And then you can try to add noise to turn this into a sampling dynamics, such as you sample a distribution with the support of a convex constraint set. So the way you add noise is going to be non-trivial. So for example, you have something that is related to a Haitian metric generated by Miramap. Again, I'm skipping a lot of details. You know, there is a beautiful, profound optimal. Profound optimal transport theory, etc., behind it. But you have to add noise in a pretty non-trivial way. Let me skip these things. So the good news is we don't have to worry about the lead group situation at all. So we don't have to use fancy machinery doing calculus and dynamics and variational principle manifold. Our manifold, we can just take our optimization dynamics and add noise to it. So the dynamics looks ugly, but most of the ugly parts are actually corresponding to conservative dynamics on the lead group. The only dissipative part is actually this nice linear Euclidean the. And you just have to add noise that matches this to create fluctuation dissipation theorem. So this. So, this Euclidean structure is really helping. So, you just add noise, that's Euclidean noise. So, you just add some additive noise, and you can prove that, well, first of all, the dynamics actually admits Gibbs distribution. This is actually a joint. So, this is a kinetic energy, this is potential energy. So, you have this guy as the inverted distribution. Just in case you care, so the base mass. Just in case you care, so the base measure is hard measure for the compact Lie group, the big measure for the momentum variable, which is actually unbounded. And then the G marginal of this inverted distribution is actually the target distribution. And so, sampling algorithm can be easily obtained from the sampling dynamics. So, we use the operator splitting approach. So, we view Splitting approach. So we view this vector field. I'm making some simplifications, so I got rid of this. I view the remaining vector field as the sum of two vector fields. And each one of them, for example, the first one, actually admits exact solution, because even though you have extremely high nonlinearity here, G is actually going to be a constant. So this term is going to be a constant. You're really just solving a linear SDE. So the solution is analytically available. Is analytically available. And so does the second split vector field. So C is going to be a constant. So you just solve a linear system and then you get this. So you first do evolution of the first system and then you do evolution of the second system. And then you compose the two flows and then you get a first order lead group sampler. So it enjoys a lot of nice properties. For example, it actually is explicit, but it actually Explicit, but it actually preserves the boot structure. Let me skip that. It's explicit. I've already mentioned that. Maybe the final remark is that you actually have great performance guarantee. So this actually takes like a proof of 30 pages, but I will only tell you the results. So typically, when you do sampling, you want to justify the convergence of your sampler, and you want to talk about the Of your sampler, and you want to talk about the speed of convergence. So, typically, you assume that your potential function to have some convexity or some relaxation of convexity. If you have strong convexity, maybe you have contractivity, etc. Or if you have weaker conditions, then you have to work much, much harder. Maybe you have isoparimetric inequalities like Loxobolus or Pancare, and then you you do beautiful you know, it reveals great tools like Vincent. We use great tools like Vilani's entropic hypoellipticity, et cetera. We don't do that anymore. We don't require convexity of f at all. We don't require growth condition at infinity at all. We don't require any functional inequality. And all we need is compactness of the L-group. And also the the function has to have like sort of like continuous uh continuous derivative. Continuous derivative, and we can prove the couples. So that's something that we are very proud of, but of course, I cannot really say anything. So, just in case you know, we use actually a combination of synchronous and reflection coupling, and then we create geometric lemmas, and also we do some optimal transport based on some distance, semi-distance function that we actually printed by ourselves. So, but that's it. So, but that's it. So, we can actually do momentum accelerated optimization only group in the first part, and then we turn this into something dynamics in continuous time. And then we discretize, and then we get simply implementable sampler. They are explicit, manifold-preserving, and you actually have non-asyntalic error quantification to justify its performance. Okay, so skipping all the generative models. So, skipping all the generative bumping part, thank you for your attention. Thank you for having questions. I noted your first order lead group software. Is there any that answering your reasons that? Great question. Thank you. Thank you so much. So, actually, if you go to high order, so okay, so let me back up. So, let's think about a much easier case. So, suppose you have convexity. By the way, you can never have convexity if you have a compactity group. So, actually, you know, it was mentioned. So, if you have like a circle, you cannot have a non-trivial convex function on the one sphere. But that's a different sort. So, if you have really nice situations, Situation, then you can sort of look at the numerical integration accuracy of your sampler and transfer that to the sampling accuracy. So in order to do that, you have to study the numerical accuracy of SDE integrators. There are two notions of accuracy. So there's a local strong order. There's a strong order of local era. Strong order of local error, there's a weak order of local error. So there is an order barrier for the strong order. So even if you use a high-order method, you can get very high-order decomposition and splitting. You can get very high-order local weak order, but that's not enough because you have an order barrier for the strong error. So unfortunately, you need both to actually control the sampling accuracy. So in practice, So, in practice, you can still gain something, but in terms of the theory, it's actually very difficult to quantify what exactly do you gain if you use a high-order splitting scheme. So, that's a great question. I have a general question for these kind of methods. So, the idea behind all the kind of flow optimization is that you're following the trajectory very accurately, but I don't care about the trajectory, actually, I don't get one. So, I don't care about the accuracy of OB at all. Care about the accuracy of flow B at all. I mean, in principle, if I have a quadratic problem and I do contribute gradient testing, I solve the whole thing with one iteration and I'm done, right? And I don't care about the whole flow. So why do I care about the flow so much? So first of all, so it's like an intermediate thing. So in the end, you don't have to care about the flow. The flow helps me construct my algorithm because I do it by a structure preservation, discrimination of the flow. I know how to get the flow. I know how to do I know how to get the flow, I know how to do geographic integrators. So that's how we did it. I didn't have password to just do, for example, integration and optimization with proximal method. And there's all this machinery in optimization that allows you to take very large steps and project the constraints as you can see. So people have tried. I don't think that there are results as satisfactory as this is. But if you had more details. If you had more detailed comments, I'd love to hear. Second thing is that the auxiliary dynamics, the continuous dynamics, actually helps me analyze the convergence of the discretized dynamics. So what we did is like a analysis.