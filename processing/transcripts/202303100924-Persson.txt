So this presentation is based on joint work together with my PhD advisor Daniel Kressner and it's about performing low-rank approximations of monotone matrix function. And what we do here is we use the Neustron approximation of an SPSD matrix to construct a lower rank approximation of a matrix function. A low-rank approximation of a matrix function of that matrix. Hence the name fun Neustral. Alright, so we're given a large SPSD matrix A, which we assume is low rank approximable, and an operator monotone function F that marks 0 to 0. Operator monotone means that if A and B are SPSD matrices and A minus B is also SPSD, then F of A minus F Then f of a minus f of b is also SPSD. So any operator monotone function is monotone in the scalar sense, but the converse is not true. And our task is to find a low-ranked matrix B hat that is a good approximation to f of A. Some examples of operator monotone functions are, for example, f of x equals x is clearly operator monotone. The square root is also operator monotone. In general, In general, x to the power of r, where r is between 0 and 1, is also operator monotone. Log 1 plus x, x divided by x plus mu for some positive parameter, mu is also operator monotone. And each of these functions are important in applications. Even for larger R monotone. It's not operator monotone. x to the power of 2 is not operated on monotone. So that's an example of a monotone function that is not operated monotone. It's a necessary condition that it's concave. Yes, yes. Concave. Yes, yes, so this like it has to be C infinity and concave. That's some requirements. Some composition and positive scaling of such functions are also operator monotone. So I will comment a bit on the operator monotonicity at the end of the talk, but I would like to mention why we assume it's monotone and map series as well. So one consequence of the operator monotonicity is Of the operator monotonicity is that f is continuous. And if A has many eigenvalues close to zero, when you apply F to A, then it still has many eigenvalues close to zero because of continuity. So if A is numerically low rank, then F of A is also numerically low ranked. So low rank approximation makes sense. Also, in particular, if A has had exactly rank K, then F of A has at most As at most bracket. So the ranks cannot increase. Can the rank decrease? Well, not if it's operated monotone, but the thing is, any zero eigenvalue will be mapped to zero. So it will have, if it has n, say, n zero eigenvalues, then f. n zero eigenvalues, then f of a will have at least n zero eigenvalues. Does that make sense? Yeah. Okay, cool. So why do we care about this problem? Trace estimation is a task that heavily benefits from low rank approximation. So if you want to approximate a trace of f of a, what you could do, once you have a low rank approximation, you can just approximate a trace of f of a with the trace of your low rank approximation. In addition, if you want to perform like f of In addition, if you want to perform like f of a times a vector and you want to do that very fast, if you have access to a Low-Rank approximation, you can just approximate f of a times x with b hat times x. And this is a very fast operation. Diagonal estimation is also a task that heavily benefits from low-rank approximation. So first I will present some ideas on how we could solve this problem. Ideas on how we could solve this problem and then explain why this is not perhaps the best thing that we could do. So the first method is to just compute the eigenvalue decomposition of A and then obtain the best rank K approximation to F of A. Since we assume that A is very large, this is not something that we can do. So another method would be to construct the Laurenc approximation using matrix vector products with FOA. So arguably one of the most popular such approaches. One of the most popular such approaches is to randomize SVD. But there is also another approach which is called initial approximation, which constructs low-rank approximations to SPSD matrices. And since we assume A is SPSD, F maps 0 to 0 and is increasing, F of A is also SPSD. So we could apply the Neustrum approximation to F of A. And what it does, and I should mention the reason. And I should mention the reason why we focus on the New Storm approximation is that the New Storm approximation performs much better than the randomized S V D if your matrix is SPSD. So this is why we focus on the Newstorm approximation. And what the Newstorm approximation does is that it samples a random matrix with K plus P columns. P is a small oversampling parameter. Then you get an orthonormal basis capital Q for the range of F of A to the power of small. range of f of a to the power of small q minus 1 times omega. The small q is the number of times we perform subspace iterations. So if you push small q larger and larger, your low rank approximation will be better and better. But it comes at a higher computational cost. Sorry, would you mind sharing your slides on Zoom? Because the camera is malfunctioning again. Oh, okay. Sorry. I'll do it with that. Might be easiest to just join the Zoom meeting from your laptop. Maybe I can just put them here. I mean, they got followed then. You nice if they could, yeah. Yeah, yeah, that's true. For someone in Zoom, would you mind pasting the code the link in the Slack camera? It's there and generally scroll up a little bit. I can tell you the meeting ID. Just a couple screens going on. No, not these. It is that one. It's the first one. This one? Yes. On to audio and screen Does it work now? Does it work now? Yes, thank you. Okay. Where was I? Yeah, you obtain an orthonormal basis, capital Q, and then what you do is you return B hat, and that's your lower rank approximation. And this method will cost Q times K plus B matrix vector products with F of A. So this is a very good low-rank approximation method. What you see here on the y-axis is the Frobenius norm error and on the x-axis the rank. And the blue line is the error that you would obtain when you set this small parameter to be 1, this small parameter q to be 1. This yields a pretty reasonable. This yields a pretty reasonable low-rank approximation. But if you set Q to be equal to 2, this yields an excellent low-rank approximation. And you essentially fall on sort of the optimal low-rank approximation error. So it's a very good method, but there is a problem, and it's that we need to compute matrix vector products with f of a. And this is much more expensive than computing matrix vector products with A. So what you would do when you want to compute f of a times the vector. To compute f of a times a vector is that you run the Lancos iterations or maybe 20 iterations, but that would cost 20 matrix vector products with A. So that's computing f of A times the vector is in that case 20 times more expensive than just computing A times X. Also, the Lange's method can converge very slowly. If f is the square root and A has many eigenvalues close to zero, the Lange's method will converge. The Lange method will converge very slowly. You can also resort to other methods like rational trial of subspace methods. But these methods come with issues on their own because then you're required to solve shifted linear systems with A. So the takeaway here is that computing f of A times X can be expensive and what we want to be able to do is to obtain a low-rank approximation using many, many fewer matrix circular products. Now we go back to the Now we go back to the setting. We have an SPSD matrix A and a monotone function F that maps 0 to 0. Then if AK is the best rank K approximation obtained via the truncated Eigen value decomposition of S V D, then if you apply F to AK, then that is the best rank K approximation to F of A in any unitarily invariant norm. We don't have access to AK, but what we could do is to compute an approximation. What we could do is to compute an approximation to AK using the initial approximation. So we approximate A with A hat, and what we do is then just apply F to that. And that will, because F maps 0 to 0, this will remain rank K. And this method would completely bypass the need to perform any matrix vector products with F of A. So I should mention that there is a similar idea used in the context of trace estimation by Saibaba, Alexandria, and Ibrahim. Saibaba, Alexandrian, Ibsen, and Helma, where they wanted to approximate a trace of f of A, where f of x is x, log 1 plus x and x divided by x plus 1, all operator monotone. But we want to be able to derive bounds and sort of the Fubinius norm, operator norm, and the Neustrat norm. So now we can present Fan Neustrum. Fun Neustrum, and essentially, what it does is that it just computes the eigenvalue decomposition of the Neustrum approximation. Here's a very simplified version of the algorithm. Computing the eigenvalue decomposition of the Newstorm approximation is very cheap. So that's not something that we need to be worried about. And once we have the eigenvalue decomposition, we can just apply f to our lower rank approximation, and that will hopefully be a good lower rank approximation to f-lbas. So the potential benefits is that we don't need to compute f of a times a vector, so we can save a lot of matrix vector products. And actually, what you will see in the numerical experiments is that this method can be even more accurate than applying the Nicholas approximation directly on FOA. So, what we wanted to solve is that we wanted to have a method that is significantly cheaper than initial. Is significantly cheaper than the initial approximation applied directly on F of A, so it's natural to test how many matrix vector products do we save. What you see here on the x-axis is sort of the relative error, and on the y-axis, the number of matrix vector products that you need to do in order to get that relative error. The blue line is the number of matrix vector products from the Fan Nishtra method. So here you can see that in order to get a relative error, 10 to the minus 2, you need to perform approximately. To the minus 2, you need to perform approximately 100 matrix vector products in this case. The red line is obtained by applying the Newton approximation upon f of a, but f of a times the vector is approximated using the Launch method. So you can see that there is a significant difference in the number of matrix vector products that we need to perform with A. So we save a lot of matrix vector products, that's very good, but you could also imagine scenarios. You could also imagine scenarios where computing f of a times the vector is very cheap. But even in these cases, fun Neustrom tends to return a low-rank approximation that is better than the Neustrom approximation applied directly on FLA. So here you have the relative error on the Y axis, the rank of the low rank approximation on the X axis. The red lines are from the Nicholas approximation applied directly on F of A. Approximation applied directly on F of A. Now we assume that FOA times the vector can be computed exactly. And the blue lines are from the Fan Neustra. And I also vary the small parameter Q here. And you can see that the blue line is either lower than the red line or they're essentially on top of each other. So the Fan Neustrom method yields a better low-rank approximation compared to the Neustrom approximation. Can we justify this with some theorems? Yes. So we have tail bounds also, but here I will only show expectation bounds. So if lambda i is sort of the ith eigenvalue of A, and you let this gamma to be sort of the spectral gap between lambda k plus 1 and lambda k, you assume that q is larger than 2 and this term Is larger than 2, and this term here, f of lambda 2, is the best Rank approximation error. Then, in expectation, van Neustrom yields an error that is close to the best Rank approximation error. And also, if you let Q to be larger and larger, you convert to the best Rank approximation error. This assumption that Q needs to be larger than 2 can be removed, but then you get a slightly weaker bound. Excuse me? Yeah? Thank you for a nice talk. Thank you for a nice talk. So P is the over-sampling factor. Indeed, yeah, yeah. True. And what is the meaning of Q? Q is sort of the number of times you perform subspace iteration with A. So yeah. So in the nuclear norm, we also have an expectation bound. Here you don't need to assume that Q is larger than 2. And we also have an expectation. We also have in expectation, Fan Neustrum yields a low approximation error that is close to the optimal one. We have a similar bound in the operator norm. And in the operator norm, what you can do is that the expectation of the error is less than the expectation of f of the error between a and a hat. Then using properties of f you can move in the expectation inside f. F and then you can find bounds for this in the literature and use properties of f again to obtain this bound here. So we can justify this method both numerically and theoretically. So before I finish, I would like to mention some other remarks. In the paper, we also show an application to trace estimation. So what we do is that we combine the funding instrument. What we do is that we combine the Fan Neustra method with Hutch, and we prove a similar order one over epsilon bound that Tyler just mentioned. And the benefit here is that the low-rank approximation phase of Hutch plus Plus can be made significantly cheaper. I promised I would mention the operator monotonicity. Empirically, we observe that once you step outside the set of operator monotone functions, the numeric The miracle experiments suggest that the bands that we have cannot hold. One such example is, for example, f of x to the power of 3. But what we've also observed is that if you just allow for some more subspace iterations, then the van Neustrom method will work very well even in these cases. But to me, it's not clear whether the operator monotonicity is. Operator monotonicity is needed, or if it's sort of a subset of the functions for which this method will work. All right, thank you. That was it. I'm happy to take this more. Could you quickly go back to your writes that you get at the end of the day? Okay. So. So, I guess one intuition I have when I think about oversampling, and maybe this is more for large approximations, but I often think of an oversampling from K to like a K plus P as letting me depend on the gap from the K date only to the P or P plus. Yeah, yeah, I see. Is there any hope for that here? Uh I did think about that, but uh I cannot see how we would do it. But uh potentially yes. But potentially, yes. So you have this hypothesis that you need f of zero to be equal to zero so that the rank of f of a is less or equal than the rank of a. So it makes sense to look for a low rank approximation to f of a in case of a low rank a. But then if a is low rank itself and you construct a closed subspace with a, you should get an invariant subspace in at most like k plus one iteration. At most, like k plus one iteration. So I don't see why standard kilo should perform poorly. I am not suggesting. What do you mean with that it would perform poorly? No, you show that, I mean, Lancash convert very slowly. Yeah. But if A is low rank, you should get in Varian space in almost rank of A plus one iteration. Yeah, yeah, but still, if you need to perform Yeah, yeah, but still if you need to perform five iterations, then that is five times the cost of computing A times X. No, no, I mean that just launches with A. Yeah. To compute A A K, let's say. Yeah, but it will still be, even if it converges quickly, maybe I misunderstand what you mean, but even if it converges very quickly, it will still be a lot slower than just computing A times X. You know, I don't want to construct the subservice with F of A. Subspace with F of A. I want to concern the subspace with A, and it should be able to get AK, so the best rank K approximation to A, in at most K plus 1 iterations. Yeah, are you suggesting that the orthonormal basis Q should be taken from a Krylov subspace? Yeah. Yes, I mean you could do that. Uh yes. How do you yeah, I mean i it would work. Uh it would work. Uh but do you have comparisons in that regards? No, we don't, but uh No, we don't, but I mean, it will work because you're essentially. What we do now is that we sample from a subspace of the crylob subspace because we perform subspaces. So you would essentially just increase the space that you're looking at. So it will work better. But then yeah, I mean, yeah, you're right. But that wouldn't mean that f of A times X is. No, I suppose I can only say that. I commonly see that. Yeah. How do you estimate the rank K for the next one of mass emission? So you would need some like, you don't know what K is, but you would like to find it out. Yeah, so what we, for the Fubinius norm and the nuclear norm, you cannot, I have not been able to derive sort of an estimator of the error, but in the operating norm, you can, because sort of in expectation, the off. The expectation of the error in the operator norm is less than f times f of the error between A and A hat. So if you can estimate this, you can just apply F to your estimate, and then that will be an upper boundary error. And in the Hauker-Markenson drop paper, they have methods on how you would estimate this guy here. Any other questions? Questions? You'll find a question from me? Alright, let's take one from the online audience. Please go ahead. Online? Question? Oh, yeah. Just a quick question. Like, intuitively, if your function is like eight of the one-half, right? If your function is what? Oh, sorry. Say your fun. Sorry, that's all desired. Say your function is like e to the one-half, right? Yeah. Then it feels like obviously you should not apply e to the one-half times q. 8 and 1 half times 2 when you apply 8 times 2 because it's basically you get two iterations for free. Exactly. Is there a theorem where if your function is operator monotone, then it's somehow strictly better to the space you get by multiplying A times Q is strictly better than the space you get. Does that generalize? Like, yeah, is there some theorem that it's strictly better to do A times Q rather than F of A times Q? So actually, in the paper, we have a remark where, like, Paper, we have a remark where, like, maybe this doesn't completely answer your question, but in the paper, we have a remark where we say, like, in like the error is sort of monotonically decreasing in Q. Right. Right, so the more you form subspace situation, then the error will only decrease. That makes sense. Right. And so. I wonder if that can be generalized. Yeah, because if it's like bod queue or something, it seems like it should only better be maintenance key rather than block Q. Only battery maintenance two value block reasons. Yeah, yeah, you're right. Yeah, yes. We can take additional questions to the coffee break. Yeah, sure. Sorry. All right, let's thank Jacob again.