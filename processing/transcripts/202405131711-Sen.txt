Conference, the invitation to speak. This is a wonderful location to be in. So I realize that I'm the only thing that stands between us and dinner. And it has been a relatively long day. So I'll try to keep things, you know, I'll try to avoid a lot of technical complications, but I'm really happy to dive into the details. And please feel free to stop me at any time if you have questions. If you have questions, so I'm very happy to talk about some recent work we've been doing related to community detection in these sort of TV networks. This is a general class of problems that has already come up in prior talks today. I'll try to connect it to what the other speakers have described. This is based on joint work with Xiaodong Yang and Hu Yu Lin. These are two great PhD students statistics. So talk about statistics. Blue graduated earlier s sort of this semester and shutdown is a rising third year. Okay, so what so I'll say something provocative to start about. So I'll make all the sins that the past speakers have warned us to recently. So I won't have mixed memberships. Mixed memberships, I won't have degree corrections, and I will have unsigned networks. So, as you can imagine, as I was hearing the other speakers, you know, as I was listening to the other presentations, I was like, okay, maybe I am assuming away all the challenges in these problems. But I hope to convince you during this talk that there's a few other There's a few other challenges that remain, even if we strip away these very important practical details. So I should say that I'll work under a very toy setting, but we'll see that there are still some hopefully useful insights that we can gain from this analysis. Okay, so of course, I don't think I need to explain community detection to this audience, but I'll still have one initial slide. Still, I have one initial slide just to set up some notation and to kind of prime us for the kind of results that I'm after. So, when I'll think of community detection, at least for this talk, I'll think about the simplest possible setting where I have a graph on n vertices, I have two equal sized communities, and I want to recover these communities based on this graph. And I'm kind of thinking about the very simple stochastic block model with balanced communities. So instead of having sort of deterministic community assignments, I'll sort of assume that there is a generative model. So I first have this vector sigma, which is of length n. The entries are plus and minus 1. I sample this. This is my latent community assignment. So if an entry is plus 1, then my vertex is in group 1. If it's negative 1, then it's in group 2. And then given these community assignments, my edges are sampled independently with probabilities pn and qn. And qn, right? And okay, so with some probability pn if sigma i and sigma j are equal, and with some probability qn otherwise, right? So the base question, of course, that I'll be interested in is, you know, given this graph, when can I recover this latent assignment? Now, of course, here's a question, you know, this question as such is not completely well posed because I haven't specified what I really mean by recovering these. mean by recovering these community assignments. And for our purposes, I'll sort of work under a very, very low SNR kind of setting where it's probably too hard to do exact recovery or even near exact recovery. And so I'll set my benchmark to be very low. I just want to do better than random guessing. I have some latent vector which is of length n, so if I guessed the entries uniformly at random, The entries uniformly at random, then I would basically get zero overlap or inner product with this vector. I just want something non-trivial. And the kind of results that I'm after, you know, these are motivated by these very seminal works of Mosul Minnen and Sly and Laura Massoulier for the stochastic block model, which basically say that, okay, so I'm stating this in this very simple case where, okay, so the stochastic. Where, okay, so the specific thresholds for weak recovery depend on the sparsity of the graphs. So, in these cases, they considered this so-called very sparse regime where these probabilities are of order 1 over n. So, here a and b are constants that are independent of n. And these results basically say that, okay, imagine that my average degree is bigger than 1. And then weak recovery is possible if and only if this parameter lambda. If this parameter lambda is basically strictly bigger than 1, and this is a sharp threshold. Now, the way to think about these two conditions is that this condition that d must be bigger than 1, this is basically the condition for the existence of a so-called giant component. Without this, the graph basically consists of roughly very small trees, and there is nothing much we can do. The parameter lambda is sort of more important in this setting. This is really related to the SNR parameter in the problem. The SNR parameter in the problem: if A and B are very different, then okay, this is large, so this becomes easier the further apart A and B are. And to maybe connect to Jashum and Tracy's talks, this quantity is related to the smallest eigenvalue of this matrix B. And I should say that, of course, in the context of the stochastic block model, okay, we know a lot, right? We know everything about what happens in other probabilities. In other probability scheming regimes, and we also know what happens for other notions of recovery. But for us, you know, because I'll restrict myself to the weekly covering question, I'm just going to focus on this multi-view. Okay. So today I want to generally talk about these questions related to multi-fuel networks. We've already seen these problems in Jenny Vera's talk earlier this morning and also in Earlier this morning, and also in Maryana's talk earlier in the afternoon. I won't try to have a very specific definition of what I mean by multi-networks. Rather, I'll go by examples. I'll talk about a couple of very concrete models. And hopefully, that will give a sense of why I think there's something kind of unifying about all these models that can come up in modern applications. So, there's far So, this first one, this, okay, I think my image is slightly small, but this is an interesting application. It comes from applications of networks to veterinary sciences. So, what this is doing here is that they have the vertices here basically represent pig farms in the US. And they have sort of these connections depending on sort of how far. How far, you know, what the scale of distance they are willing to consider. So, if two farms are less than, let's say, 10 kilometers, then you connect them by an edge. And here they have sort of a different kind of edge depending on certain other measures of similarity. So, the question in these cases is that even among the same set of interacting agents, you can construct different networks which capture different notions of similarity. Different notions of similarity. And the question, in some sense, is: can you discover some common underlying structure by combining the information that's present across all these organisms? This is similar to what Jim and Vera described earlier today related to these brain imaging applications. So, to kind of model this setting, I want to talk about the so-called human. I want to talk about the so-called homogeneous multi-layer SBM. You have the same set of nodes in all the layers. I have the same set of nodes, yes. Not like it was in Lara S. Well, no, I mean, it's one mode again. It's one mode again and the same set of nodes. Exactly. So, what I want to think about is a setting where I have L graphs on N vertices. On n vertices, the same set of vertices at each layer. The way I generate this graph is that I have first this so-called latent global community assignment. What this does is that it again splits these vertices into two groups. Roughly half are in the first group, the remaining are in the second group. But when I form my layers, I don't use this same community structure. Use the same community structure at every layer. Instead, the local community assignments for the LH layer are obtained by perturbing the global assignment. And effectively, what happens is that at each vertex, I flip the global assignment with some small probability of 2. Otherwise, I keep it unchanged. And once the local assignments have been determined, my edges are drawn. My edges are drawn again at each layer independently according to a block model with these probabilities AL and BL. And I have a parameter AL which governs these connection probabilities if the memberships in layer L are the same and they are with this connected with this probability B L over N otherwise. Okay, so I think this particular model goes back implicitly in the statistics literature by many In the statistics literature, by many years, but I'll only point to this very recent work by Zhong Yingma and collaborators, because this is where I first seen this written down very concretely. But I think there are earlier references where this shows up implicitly. I should say that in what I do throughout, I will assume that there are a fixed number of layers, so L will be fixed for me. Fixed for me. And I'll, at least for now, think of rho as something that's constant, so it doesn't depend on n. And the only asymptotics that I do would be in terms of the size of the network. n is what will grow. And I'll let this AL and BL actually depend on N. Okay, so what makes this somewhat interesting? As I mentioned, I've stripped away all the actual challenges. All the actual challenges that come up. But it's still somewhat non-trivial because I'm not assuming that the community assignments in all the layers are the same. Instead, the community assignments across all the layers, these are correlated. They're distinct, but they have this correlation with this common underlying strength. And now, if I give you this model, you can basically ask, you know, how can I, you know, how well can I recover these global assignments, or how well can I recover? Global assignments, or how well can I recover the local assignment? These are both very relevant questions. And the challenge is to combine the information across these layers to do inference for one of these sets of parameters. Okay. Any questions about this first one? Let me talk about a second class of examples. These are temporal networks. Again, these are. Again, these are very commonly studied. This particular example I have is this so-called, this is a gene co-expression set of networks in monkey brains as they develop. So the idea here is that basically you observe these coexpressions in developing monkey brains at different points in time, and therefore, And therefore, again, you obtain these graphs on exactly the same sets of vertices. But these graphs sort of change over time. And to model these networks, I'll use this so-called dynamical block model. But the idea here, again, is that you start with some underlying assignment at time one, and then the community assignments for each. Community assignments for each vertex basically evolve as independent Markov chains. And the evolution is that, you know, as I go from time L to time L plus 1, I basically stay in the same state with probability 1 minus rho. And with some small probability rho, I flip the state. And okay, once I've fixed these temporal memberships and how they evolve across time, Across time, conditioned on that, my graphs are again drawn as independent block models. And I should say again that this particular version of the dynamicals block model appeared in the work of Mitais and Miele, but I think it also was introduced in some earlier papers, but we look at very specific analysis and algorithms in this context. Okay, and here I'll again assume that I have access to the graph. Assume that I have access to the graphs at different time points, and the question is: when can I do non-trivial recovery for any one of these latent community assignments? Any questions about the second module? So for now, I'll avoid defining multi-view networks specifically and I'll just think about these two examples. In our paper, we have a sort of more general definition, but for General definition, but for our purposes here, I'll just restrict these two examples. Okay, so the question I'm interested in is in the same spirit as the results of Moselnum and Sly and Mausphilia that I started with. I want to know when can I actually do weak recovery in these examples. Okay, so here is our first result. Let me Let me lay out certain notations that are helpful. So, the parameter dl basically denotes the average degree in the LFLEC. And this parameter lambda nl, this is defined very similar to the SNR parameter that appeared for the one-layer block model. So, you should think of this as an SNR for the lth layer. And throughout our work, you know, I'm thinking. And throughout, I'll work, you know, I'm thinking about the multi-layer SBM, so I'll think of the parameter rho in the interval 0, half. Now, let's think about the two endpoints. If rho is a half, then basically the layers become completely uncoupled. There's no shared information. And so I can't really do anything by combining information across them. So I let rho be strictly less than a half. The other interesting thing. The other interesting case that has actually attracted quite some attention in the prior literature is the case where the rho is exactly equal to zero. This is the case where all my community assignments are exactly the same, and I'm just trying to recover a common latent community assignment based on the L graphs. Okay, so what can we say? So I need two conditions. The first is that I Need two conditions. The first is that I assume that I'm in some sort of low SNR setting so that the lambda nl parameters converge to some lambda ls. And these lambda ls are order one constants independent of n. And I need the minimum degree to actually grow to infinity, but I don't have any requirements on the rate. In fact, I'm reasonably certain that you can also work under Certain that you can also work under a double asymptotic when you first let n go off to infinite and then you let the degrees grow. Under these assumptions, we can say that weak recovery, either for the local assignment or for the global assignment, is possible under this very specific condition. So, in fact, what this tells us, you know, this tells us exactly how. This tells us exactly how one should be combining the SNR parameters in the different layers for community assignments to be possible. Okay, so two comments on this. Our result actually has a small caveat. What we actually prove is that if you are above this threshold, then community detection is possible. Possible. For the negative results, so if I'm in the converse regime where this quantity is actually strictly less than 1, we have a conditional result. What we can prove is that we actually have a family of parametrized vector-valued maps, and we basically need some sort of concavity condition for each coordinate mapping here. Under the Under the assumption that each coordinate mapping is concave on a line passing through the origin, we can actually show that this is really the right threshold. And we have a lot of simulations essentially indicating that this holds. But at the moment, this is something that we need to assume. Yes, yes. So possibility without is for both local and global. Without. No, no, no, it's the same. It's the same. So essentially, what we prove, so we have, okay, I'm condensing a couple of results here. What we basically prove is that in this model, local and global community detection becomes possible exactly at the same point. But if your rho n is really rho is independent of n for n. That's why this happens. That's part of it. This happens. Yeah, yeah, exactly. So for the problem card and for the possibility card, do you have a specific algorithm or just a problem? I'll come to that in a minute. Yes. That's a very important question. That's kind of this result is information theoretical. I have two equal sized communities. Okay. Any questions on this? Okay. Okay, so a couple of comments. In the special case where L is equal to 1, this reduces back to the lambda equals 1 threshold for SVMs. Let me start with this. The second comment is that in the special case where rho is actually exactly equal to 0, this gives back the homogeneous multilayer SVN. And the threshold for this case was actually derived in. For this case, it actually derived in this work of Shakni Knondi and Zong Ling Ma a couple of years ago. And it basically says that, okay, effectively you end up adding up all these SMR parameters, and if that sum is bigger than one, that is non-trivial community detection. So our result, in some sense, also encompasses this special case. Now, what I find somewhat interesting is the fact that this particular dependence on rho. This particular dependence on rho is not something that I could have guessed if we had. This is something that was intriguing when we first found this. Okay, this is a quick visualization. So, for the case of two layers, this is the set of parameters lambda 1 and lambda 2 for which community detection becomes possible. So, note that if becomes possible. So note that if any one of these lambda i's is strictly bigger than one, then I can just do community detection based on one of the graphs. So the interesting regime is only within square. And I have these different curves for different values of a row. And effectively, I can do non-trivial detection about these curves. And this is the corresponding visualization for a higher number of layers. Of course, I can't plot it in a high number. Plotted in a higher-dimensional cube. So instead, what we are doing is that we are fixing to the case where all the lambdas are equal, and then these are the corresponding curves. So if lambda is bigger than the threshold given by this particular value of L as a function of a rho, this is when detection becomes possible. Okay, and I should say this is sort of to convince you that our To convince you that our assumption in some sense is probably not a values assumption. So, we have had extensive numerical investigations into concavity for this set of maps. They seem to hold cleanly for all the settings we have tried. I'll come back at the end of the talk as to what these maps are and what the challenge is so far we need to. Okay, so we have a similar result for the dynamical SBM. So a similar assumption: I need my degrees to grow, and I'll assume that my SNRs converge to constants. In this case, to get a clean answer for the recovery threshold, I'll actually make an additional assumption that all my lambda L's are actually equal to the same constant. And in this case, I have a similar. And in this case, I have a similar answer for the weak recovery threshold. This is a bit more implicit because it involves this constant theta star that's defined as the minimum solution for some equation. But once, you know, given L and rho, one can solve for theta star, and once you have theta star, it gives you the value lambda that is from. That is wrong. And the result is again of a similar flavor. So there are, it has two parts. One side is information theoretic, saying that if lambda is bigger than this cutoff, weak recovery is possible. Whereas if lambda is less than this, we can show that it's impossible provided one has these concatenations again for a different family of maps. Any questions about this result? There's also a question here. In in your setting, do you have a gap between the weak recovery and the strong recovery? Yes. So by strong recovery, do you mean exact recovery? Exact recovery. Yes, of course. This is similar to the block model. I anticipate that, for example, if you need these are these graphs, sort of I can have very low degrees, so much smaller than logarithmic degrees, for example. And even if the degrees are logarithmic, if the SNR parameter, If the degrees are logarithmic, if the SNR parameters are of order one, then I can only recover a certain fraction of the vertices. Exact recovery is impossible to this region. So can I get all the vertices correctly? I'm just talking about weak recovery, which is doing something better than something in between, like better than 0.9. So in fact, our analysis tells us what the best fraction is that we can recover. Is that we can recover information theoretically, but it will be something strictly less than one. Yes. So can you go back to the previous slide? Yes. No. Yeah, for this case. So I'm wondering: what if I only focus on the global community of civilians? Yes. It's the same. It's the same. That's what we. The issue is that I think this is also what Tracy was asking. The point is, we fix rho to be an order one constant, and therefore. What if rho is zero? That's the easy case, because all the community assignments are exactly the same. And then in SKS you require the the minimum degree equals infinity randomly. Yes. But it can grow to infinite as slowly as you. But it can grow to infinite as slowly as you want. Log n, log log n, any quality. I don't need the minimum degree, d l, you thought even with the more layers, no. That require every layer. Okay, yes, this is this is a good point. I agree. This is a shortcoming of our analysis. In principle, one should be. In principle, one should be able to also handle cases where, let's say, some of your average degrees stay bounded, whereas some grow. That's a possibility. And I think generally what we don't understand at the moment is what happens when the degrees are kept fixed. So this is one of the future directions of all the things that I want to talk about. I think that's an interesting direction, generally to think about it. Yes, I agree. How am I doing on time? Two more minutes? Okay. Not doing well. I think it was not low. I think it was more like seven because he started on the maybe I'll take five more minutes. That's okay. Five or five in this block. Excuse me. Oh no, no, I uh I read that off. Yeah, no, we have ten minutes. I have ten minutes. I'm doing very well. Okay, okay. Okay, okay. Okay, so I want to talk about one interesting distinction between the multilayer case and the dynamical case, because I think that sort of points to an interesting difference between these two models. So of course, this particular threshold is not very intuitive to look at. It's very unclear how things scale here. So to simplify things, what we did. To simplify things, what we did basically was to consider now the case where the number of layers actually grows, but not as a function of n. This is an iterated limit where L goes to infinite at the end of the analysis. And we basically discovered that for the dynamical case, we need, even if L goes off to infinite, we still need lambda to be bigger than some non-trivial threshold. And this is actually different from the multilayer case. In the multi-layer case, if the number of layers goes, Case, if the number of layers goes out to infinite, then any SNR will work beyond the point once L becomes large enough. And this is really the notion that we intuitively have, that as I have more and more layers, I have more and more information. So even if my signal is very small, once I collect enough layers of information, it will help me to recover it. Here, because my community assignments are not independent in some sense, they are evolving as a mark of change. Sense they are evolving as a mark of change. So there is dependence in these models. Therefore, I need enough information that helps me win against this dependence in some way. So even though my L goes off to infinite, I still have some non-trivial requirement for me as a parameter. Yes? I want. So the way to think about this result is that I fix the About this result is that I fix the first layer and then I ask, okay, how large of a signal do I need to recover the first layer assignment? That's how I'm doing this. Okay, yes. So to come back to Joshua's question, our result had sort of an information theoretic component where, okay, beyond this threshold, recovery is possible, but what about algorithms, right? Right? So, in fact, we have an algorithm based on approximate message passing that has good performance in this setting. So, the way to think of this algorithm is that I set up sort of the centered adjacency matrix. So, this is a matrix where the IGH entry basically look at whether there's an edge connecting the vertices I and G in the Lth layer. I center The lth layer. I center and scale it so that it has easy over and spawn. I have these different matrices at each layer. And then I basically run this iterative algorithm. This is roughly like a nonlinear power method. So at each step, I basically have these nonlinearities that I apply to these m kts, thinking about these as row vectors. Thinking about these as row vectors. And the difference in this case with approximate message passing is that I need some correction term. This is just required to make this analytically tractable. The way I think of this, and I think this is really the algorithmic insight that we had, is that the way to sort of combine information across all these layers to do community detection is that at each step, Is that at each step we kind of do this matrix multiplication that gives us some information across all the layers? And then when I recovered the membership in the lth layer, I combine, this non-linearity actually pulls information across all the layers and gives me the right estimate. So, this is where exactly the combination of information across layers happens in this algorithm. So, what we can prove So, what we can prove, I don't have a formal statement here, but what we can prove is that if the degree grows at least logarithmically in n, and if I start with a warm initialization, then this algorithm will give me actually information theoretically optimal performance. So it actually recovers the best possible fraction. But there is still a question of okay, what happens if I start with the non informal dimensional? I start with the non-informal information. There's things here that we don't understand yet. So here's a simulation. What I'm doing here, so this is the multi-layer case with two layers. What I'm doing here, so here, let's look at the middle plot. So I say that the degrees are 20 and 30. So these are the average degrees in the two layers. For these combinations of lambda 1 and lambda 2, I basically run this AMP algorithm with a very Algorithm with a very specific choice of the noisers. And what I plot here is essentially the estimation accuracy. And I effectively see that you know this curve in red is basically the conjectured threshold. And then, okay, as I go up above this threshold, things become easier. So this is reasonably good agreement. And of course, at least in our theory, we need our degrees to be large, but even for relatively But even for relatively small degrees, things kind of look reasonably okay. I'll skip over the left panel here. I'll come back to it in a couple of seconds. Okay, so I should say some comments about this proof. So, one of the reasons why I was particularly interested in this class of models is that. Class of models is that a priori when I look at this class of models, it's not clear to me how I should come up with good algorithms for community recovery, and I don't quite know how to actually guess these thresholds. The only way we kind of know how to do this is to calculate the mutual information between these latent community assignments and the graph data that we observe, and then basically tease out this optimal Bayesian reconstruction error. Reconstruction error from this mutual information. And the recovery is essentially related to a phase transition behavior in the performance of this reconstruction error. So this is sort of the analysis that we carry out. I think we'll see a version of this in more detail, I think, in Justin's talk later in this workshop. And I should say that one of the main technical reasons why we need the DPs to grow is that actually analyzing the sparse graph model is actually very challenging. Graph model is actually very challenging. When the degrees grow, we can basically appeal to a Gaussian surrogate and then actually do the analysis for the Gaussian surrogates by this is somewhat easier. We have more tools for it to get this. So the Gaussian PK is used for the proximate message part? It's also used for calculating this mutual information. So, what we do is that we basically show that if you have the grand. Is that we basically show that if you have the graph model and you have the spiked matrix model, basically the thresholds for these two things will agree in the limit where the degrees grow. For the spikes or gaussian, spike case with gaussian data, it's like relatively easy to figure out the threshold. Yes, it's more it's more work. It's it's not obvious, but we have more tools. You have tools are like kind of calculating the Yes. Yes, yes, of course. I'm being very vague here, right? So, two things. One, so there are two kinds of universality results that we prove. The first is that there is a way to approximate the mutual information in the The mutual information in the actual sparse graph model with the mutual information in the spike matrix model. This was known in the context of the SBM in earlier work by Abhidhashpangi and Montanari, so extend this to the multilayer case. And then the result, we also proved universality for the AMPR models. So these are the two kinds of universality that we need. But these are different results and not everything is universal between the two models. Okay. Okay, so I think I don't want to overshoot. So let me finish off with some questions that don't know at the moment how to do this. The first is, I think, was related to a question that was raised. So I certainly don't know at the moment what to do if, for example, all my average degrees are fixed constants. There are very beautiful results for the stochastic block model. Stochastic block model, for example, related to the long backtracking walk, and the thresholds are known. Don't know what these answers would be at the moment. I would be very interested to know this. If you have any ideas, please let me know. Even in the case where the degrees are growing, my algorithmic guarantees are at the moment based on some warm start or hot initialization. I would actually be very interested to know if there is a way. I would actually be very interested to know if there is a way to develop spectral algorithms in the growing big case that achieve these thresholds. I believe it should be possible. And then I should say that, okay, everything I said here is in the context of balanced two community cases. I don't think there are computational gaps in this model, but for example, ANCROM has been very interesting to simulate related to statistical computational gaps in these cases. I think for higher computational Cases. I think for higher communities, this will again show up. So, understanding something about this mutual interest. Thank you so much for your attention. Let me know if there are questions. In the algorithm, do you assume you know the goal? And in more general, I assume I know everything. Anything, which is clearly a shortcoming. In the theory, is that possible to allow different rows for different layers? Oh, yeah, it's definitely possible to. So, in the theoretical analysis, it's definitely possible to allow for different parameters in the different layers. I think one should not expect sort of teeny answers in those cases. It gets complicated once you have different parameters. Parameters. Essentially, what this analysis turns out is that you essentially calculate the mutual information between these things, and you basically try to see if a certain point is the global optimizer. So, for example, when you have zero signal, then you're essentially, you know, you're not recovering anything. Basically, try to see if that point is a stable global opportunity. So, it's related to some sort of Hessian condition. Sort of Hessian condition. That's how we get all of these. So if we have more parameters in the model, then we won't have as clean answers anymore, but the analysis still goes through. But on the other hand, I think there is the more practical side of things, which is if you actually have to implement these algorithms to do community detection, then in APRIO, at least so far what we are saying, we need information about all these model parameters. And it's an important question if all of these can be estimated from. If all of these can be estimated from the data, I think they should be, but so far we don't have these results. Yes. So the threshold you had, which you noticed by lambda, equals to A minus B squared divided by A plus B. Presumed that A B is that the probability of connection. The connections are A over N and B over N. A over B equals A over V. So A and B. Yeah, but A and and B can depend on N. In general it can be tenses, but there's no problems. Okay. Yes, I remember this very last report that we were just talking about the log uh some constant times log n. Yeah, so this is below that, right? So for example, exact recovery becomes possible if the gaps are of order log n over n. Of order log n over n. So I'm below that threshold. So I'm not able to do exactly that. I see. Another question is about your model. You assume these are basically parents. No, that machine will be correlated with that. Yes. I think in great practice, what happens is to network over time. Right. So. Right. So the networks changes, say, bad room. But that's the second model I looked at, right? Where the memberships change as a mark of chain. Okay. But the multi-layer model is really for settings where you kind of believe that you have some noisy kind of community structure. So every graph kind of captures the same thing, but up to some. Thanks for the clarification. Very nice thought. All right. I propose we meet further questions to dinner. So I think those figures, what time? Inner is over now. Twenty minutes ago.