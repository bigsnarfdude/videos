Welcome back. So our next speaker is Marie Kerjong from CNRS and Université de Paris, and she'll be speaking on from categorical models of differentiation to topologies in vector spaces. Hi, thanks for the introduction and thank you very much for the invitation. So this is going to be a high level talk about how we can How we can build models of differential analogic in topological vector spaces. So, this talk is supposed to be accessible if you have met with differential inalotic or topological vector spaces before. So, please do not hesitate to ask questions during the talk or after if I'm not being clear enough. So, let me begin by saying that I am not. Let me begin by saying that I am not a category theorist. I don't think I am, but I am a happy user of categories. I am a happy user of categories as a bridge between computer science and the theory of programming languages and mathematics, and especially functional analysis. And of course, this bridge is in fact the Is in fact the career-world relation between programs, logic, and semantics, which I have recalled here. And the explorations of models of logics and of programming languages has been especially fruitful for linear logic and differential linear logic. The story of differential linear logic, as Friek has told you yesterday, is one of an exploration of models of models of type theories for the landlady galcus in some some mathematical structure and in fact there is we could divide this correspondence of logic and semantics into a dual correspondence between categories and Categories and what I would call concrete models of logics. That is, you would interpret a proof as a morphism in a category, in some categorical axiomatization, and then you would go to look for a concrete interpretation as a function in a well-known object of mathematics. And so the study of normal factors as a model. As models of taiseman die calculus has given rise to linear logic. In turn, linear logic has given rise to CD categories and all models of linear logic. Tomai Rach and Laurent Rénier studied some vectorial models of some vectorial interpretation of city categories, and this gave rise to differential genologic. And differential genologic in turn And differential genologic, in turn, has influenced a lot of variations of Lambda-Calcuse, resources, Lambda-Calcuse, who have been quite influential in the theory of probabilistic programming languages. So, sorry, what will be of interest today for me is what's going to happen here. What's going to happen here between models, categorical models? It should be here, categorical models of differential linear logic and their interpretation in the theory of topological vector spaces. But of course, we are studying this link, these interpretations in functional analysis in order maybe someday to be able to. To specify or to find new logical or computational structures in logic or in functional programming. So this is what I will be, this is the outline of my talk. I will first be recalling quickly what are the ingredients we are going to look for in the while to build a model of differential linear logic. logic and we will first study the good interpretation for non-linear proofs and that is we will study convenient vector spaces that Rick alluded to yesterday that is we are going to look for the good interpretation of non-linear proofs as smooth functions and in fact the study of these kind of Of these kinds of smooth functions will trigger the study of the interpretation of linear proofs. And the interpretation of linear proof in the theory of topological vector spaces is not at all trivial and it is linked with a lot of choices on the topology of your tensor product or of your dual. And at last, we are going to look for a good interpretation of dual. For a good interpretation of duality and some of its refinements. That is, can we even have a model of classical differential linearity? Can we even have an interpretation for an involutional negation? And this is a question that has revealed to be non-trivial. And that's really the kind of question that can keep you awake at night. Okay, so categorical models of differential. So categorical models of differential linear logic. So what are we looking for? We are looking for a differential category which is stargenous. That is, we want a model of classical differential linear logic. And by classical, I mean that I want to have an interpretation for a linear negation which is intelligent. Another way to see that is to look for, I guess, For, I guess, these four ingredients here: a cartoon method category with smooth functions as morphism, a byproduct, this will be easy in fact, a monoidal closed category with linear functions as morphism, and a star autonomous structure above all that. And one question that one might be interested in. Might be interested in, and is well, is the differentiation at stake in differential analogy the real differentiation, the differentiation we were used to when we were studying mathematics a long time ago. Is it really a smooth differentiation as most of the people picture it, or is it some differentiation which is bound to be very computative, which is bound to be interpreted on time? Which is bound to be interpreted on power series. So, in order to answer that question, we are going to look for a smooth model of differential analogic, that is, start on the most differential categories with continuous objects and smooth functions. So, what are these objects, in fact? So, quite obviously, we can't restrict to finite. So, we want to define differentiation. Define differentiation. So, we are going to look for vector spaces, and quite obviously, we can't restrict to finite-dimensional one because the simple space of continuous functions between two finite-dimensional space is not at all finite-dimensional. So, we have no hope to say to other partition settings, category in that settings. In fact, going a bit marginal, we can't work with non-spaces. There was a tentative by Girard, who is the inventor of Linalogic, to build some kind of model based on banner spaces for linear logic, and it quite doesn't work. You can't have a norm on your space of function. Surprisingly, metrics don't work either. Metricable spaces are not stable in their duality, meaning the dual Is only key, meaning the dual of a matrixable space is not matrixable, so you can't have a star autonomous category of matricable spaces. So it needs to go a bit more general and see the very simple definition of a topological vector space, which is a vector space, which is also a topological space and in which the sum and the scalar multiplications are continuous operations. So in fact, we're going to work with locally convex and separate it. Locally convex and separated topological vector spaces, but I'm not going to dive into that now. So, this is my intuition about this object. A non-vector space is something very basic, where you have like only one neighborhood of zero, and this is your ball of radius one. And all neighborhoods are either translations or dualizations of this ball of radius one. Metric of space is something a bit more complicated, meaning you have a variety of neighborhoods of zero, which might not be dilutions of one another, but you have a denumerable number of it. And a topological vector space, which is not a metricable space, is a vector space with a neighborhood of zero, which are not Are not tenu bearable. You can have a big, big number of neighbors at zero. And a topological vector space, well, you don't even, you can't even work with only with neighbors at zero. So that's something very, very optimistic. So this is the picture we will have in mind during the talk. So let's look for the good interpretation of non-linear alpha. Interpretation of nonlinear functions. So, how you might know what a smooth function is. A smooth function is a function which is everywhere differentiable and whose derivatives are also smooth. It's kind of a co-inductive definition. Well, for the Rodiker, Regel, and Mitter defined a few years ago a way to make this definition go higher order to define smooth functions on smooth functions. And they do that by considering smooth curve as a basic object. So a smooth curve is a smooth function from R to a topological vector space E here. And a smooth function is very simply a function which takes a smooth curve. Which takes a smooth curve in E and transforms it in a smooth curve in F. So, this is a very basic definition which allows you to rewrite quite a big part of analysis and differentiation theory. And it has this very beautiful property that this gives you a Carfield close category. The smooth function from E to the set of E2, the set of smooth functions from f to j are exactly the smooth function defined on the product of enf and going to j. And well, this is very beautiful. You even have differentiation, meaning you have a differentiate operator which is well defined, which is linear, as one should expect to. But there is a small variation. Small variation. This differentiation on smooth function is not continuous, as one might expect. It is only bounded. And in fact, there is quite a big difference between linear bounded functions and linear continuous functions in topological vector spaces. So we are going to explore the world of analysis with linear bounded functions. And there is also another subtlety that, of course, for analysis. For analysis to work well, you need some completeness conditions, and for smooth analysis to work well in this setting, you need a very weak completeness condition, which is machic completeness. So now I will define quickly what are these boundedness and machic completeness conditions about. So in fact, the definition for linear bounded functions is much more simpler than Is much more simpler than for a linear continuous function. A linear continuous function sends the inverse image of an open set to an open set. Well, the condition for a bounded function is simply to send a bounded set into a bounded set. And what's a bounded set in general? So in a normal vector space, you know that a bounded set is like a ball or something. It's like a ball, or something which looks like a ball, or something which is not too big. Well, when you don't have a norm, the definition for a bounded set is just that this set needs to be absorbed by any zero neighborhood. And this is a way to say that this subset of a topological vector space is not going to be too big. It's not too big because we can take any neighborhood of zero and make it grow so that it's going to. Make it grow so that it's gonna hit my bounded set. And in fact, it works quite well. It gives you a monoidal close category of topological vector space, linear maps, and what's called a bonological tensor product. And I won't go any further into this bonological tensor product, but let's say it works. but let's say it works well we have a monolith or close category and as such we have an interpretation for multiplicative linear logic so what's machy completeness about so as you all know a complete space is a space in which every cauchy net converges well a machi complete space is a space in which every machi cauchy sequence converges Sequence converges. So now I need to define what's a machine Cauchy sequence. And it's basically saying, well, I'm a Cauchy sequence, but I'm a Cauchy sequence with respect to some bounded set. And of course, the definition of Machi completeness is very linked with the fact that in convenient vector spaces, we need to use linear bounded maps. So this is the formal definition of a Machik coffinet, but just think of a Think of a Makikoshi sequence as a sequence which gets closer and closer, whose points go closer and closer with respect in some bounded set. And this is a very weak completeness condition. It's very hard not to be magically complete. So it works well. So in this paper, Rick. Rick Toma and Christine Tesson detailed a differential category structure over convenient spaces, which are some restrictions of microcomputed spaces. And it works well up to the point that it is not a model of classical differential linear logic, it is a model of intuitionistic differential linear logic. And in fact, it turns out that And in fact, it turns out that linear bounded maps badly accommodate jewellery theory. In particular, you don't have an unbannach theorem for linear bonded maps. So it works very well, it works amazingly well up to the point that you want to build a star Limus category with some real joint flavor. Then we need Then we need to switch to linear continuous functions and topological duals and topological tensor products. So this is what I'm going to do now. We have studied the interpretation of nonlinear proof and what setting works well for this interpretation. And now we are going to focus on the core of linear logic. We are going to focus on the We are going to focus on the interpretation of multiplicative linear logic in topological vector spaces. And so, of course, the very important thing is that we are going to look for an interpretation of this equality. So, what are we looking for? We are looking for a monoidal close star et alomus category. And if we focus on the star autonomics part, this means that we want to have Have perfective spaces. So the natural interpretation for the dual is for the dual in linear logic is just the dual in topological vector spaces. And the dual in topological vector spaces is the same thing as the algebraic dual in vector spaces endowed with some topology. And the topology on the dual is going to be defined as the topology. be defined as the topology of uniform convergence of all the scalar linear functions, uniform convergence on some bounded sets, on a collection of bounded sets. And the choice of this collection of bounded sets is going to be of a huge importance for the construction of our model. And there is a quite fine interplay between the definition of the topic. Of the topology you want to have on E, your basic vector space, and the topology you want to have on the dual. Because the topology on E defines the topology on its dual, and the topology on its dual determines whether you have an isomorphism between E and its double dual. So it's quite a complex situation, and you have a lot of choices. The difficulty is not so much to find the good. The good to construct something to construct a topology, but it is to choose the good one because functional analysis is going to offer you a lot of these constructions. So you have a lot of operative module depending on what kind of bounded set you want to have. Reflexivity is typically not preserved by the tensor product, and you even can't make a space to be reflexive. You can't say, okay. To be reflexive, you can't say, okay, well, there is a traditional tool in linear logic where if your space is not reflexive or is not isomorphic to its double dual, well, you can complete it by a double joint. Here, you cannot do that because the dual is not a close, it's not an orthogonality. You have many choices for topological duals. Well, you have even more choices. Even more choices for the topology on the tensor product. And this tensor product may even not be associative. It depends on the topology and on the tensor product and of the topology on your spaces. It's quite complicated and we are going to see a few examples where it works. So I'm going to start with a very basic example to give you a taste of how this works. Of how these constructions work. So you can build a model of differential linear logic, which is not quite as smooth as one would like, using weak topologies. So the weak topology on the dual is a topology of simple convergence on points of E. So it is a very, very coarse topology. You have not a lot of Of neighborhood of zero in this topology. You can, in fact, when you have a topological vector space, you can change its topology and define a weak star topology on it. That is a topology of simple convergence of points on scalar forms. And this forces the space to be reflexive. So you have here you compute its dual and endowed with a wind. Its dual and endow it with a weak topology. And then you also endow this topological vector space with a weak star topology. And inside this category of weak spaces, you have a monoidal closed category. You have a monoidal closed category in which the interpretation for the power for the dual of the tensor product is quite natural. It is already endowed with a Already endowed with a weak topology and in which the tensor product needs to be not completed but needs to undergo a closure operator, a shift. It needs to be forced to have its negative topology. And well, that's a very basic model, but on which I think you have no hope to define any smooth functions. So you have very general. You have very general smooth objects, but you don't have smooth functions because you absolutely need some completeness condition to work with smooth functions, to compose them, to differentiate them. So we need to go and look a bit further. So as I said, there is a lot of choices for topological terms products, and it's not easy to know which one to choose. When the tensor product is defined by its universal property, and in particular with this canonical bilinear map from the product to the tensor product, well, any continuity condition on this bilinear map will give you a topology on your tensor product. And there are also other ways of defining this topology. So again, this might Again, this might or may not be associative, and you have quite a lot of choices to make. So it's not easy to find the interpretation of the tensor product of the multiplicative conjunction of linear logic, but it is quite natural to know the interpretation of the par. So for those of you who are not familiar with the linear logic, this weird symbol here, it's named a symbol here it's named a par it is a disjunction but linear logic features two digit two disjunctions and two conjunctions and this disjunction is called is a multiplicative one and it is written like that and I'm not sure I remember why and in fact when you know the properties of this this operator well Operator. Well, it turns out that it is quite natural to interpret it as something which is called the epsilon product, which was developed by Schwartz to define distributions with vectorial values. And the choice of topology for the epsilon product is quite restrictive. The epsilon product of E and F is a space of linear functions from the dual of E to F. The dual of E must be endowed with The dual of E must be endowed with a compact open topology, and the whole space must be endowed with the topology of uniform convergence on a key continuous set. For those of you who are familiar with the linear logic, the Epsom product has this very nice property, which is exactly what is expected of a par. So this gives you a monoidal category. It's associative and commutative on the Associative and commutative on some spaces with on spaces with some weak continuous condition and not weak in the sense of weak topology, but weak in the sense that it's quite easy to be quasi-complete. So based on this idea, you can grow and grow your model, add more and more features to it, define the tensor project, define the duality, and this gives you a model of And this gives you a model of multi-applicative neural logic. And inspired with what is done in convenient vector spaces, you can also define a notion of smooth maps which works well. In fact, with any new notion of smooth map comes a new topology on vector spaces, meaning that if you want to interpret the direction If you want to interpret the Dereken rule of inner logic, you need to have a continuous injection of E into the interpretation of the y-naught, so the space of a smooth function on the dual of E. And this, in fact, enforces a new topology on E. So building on this idea, one construct not only one, but several models of differential in analytics, which are starting this. starting this and so which are models of a smooth model of of classical differential analogy so we are very happy we are very happy uh so this was a work that i did in collaboration with uh joanne dabroski a mathematician in you uh until you look at the details of what we did and we did something very ugly for the differentiation Very ugly for the differentiation. It works well, in fact, for linear logic, but again, when you compute your differentials, you need to have you will have linear bounded differentials and not linear continuous differentials. So we kind of restricted at each step the smooth functions we wanted to work with, and it gets something which doesn't feel natural at all. So, hey, it works, but it's a bit ugly. And the fact that this needed to be done triggers the question of can't we do anything simpler? Is there something justifying for all these changes of topologies that one must do to have a smooth model of linear logic, of differential linear logic? And this is where I'm going to speak about. Going to speak about polarization. So, in fact, it's working, but it's quite hard to have a good model of multiplicative additive linear logic inside a model of differential linear logic. So, as usual, it's not working. Let's change the definition. Let's divide and conquer. There is a is a um in fact inside the formulas of genal logic there are two classes of formulas which are well known and which have a meaning of in terms of proof search and in terms of of computational content of the proof and this division is called polarization and what i'm going to do now quickly is i'm going to show that how polarization is behind Polarization is behind a most smooth model of differential linear logic. So you don't need to read this distinction between the formulas of zeno logic, but just say that instead of looking for a mono-eyed or closed starogenous category, we will look for two categories and for a contravariant assumption. For contravariant adjunctions between the two. And this contravariant adjunctions is going to interpret two negations and is going to give you a way to have an involuntary linear negation, which is made, in fact, of two different negations which result in the identity when they are composed on the good category. Brut category. So this adjunction should be monoidal, strong monoidal, I think. And you need to have another way to go from one category to the other, to go from the interpretation of positive connectives to the interpretation of negative connectives. And this is a covariant way. And this is our call shift. And shifts are also a canonical tool. A canonical tool in the syntax of polarized linear logic. So you have this replacing the, you need to have this equation replacing the involutivity of negations, and you also need to have this equation, and I have a huge people on the slides, I'm sorry for that, which is basically replacing the monoidal closed equation. So the setting I just described, the setting of dialogue. of dialogue categories which was developed by Melias of kin analytics which was developed by Melias after a study of k-insemantics so in fact the the weak model I showed you before the model in which the linear negation was interpreted by the dual with the weak topology can also be described as a chirality you have a duality between the general category Between the general category of topological vector spaces and the category of weak spaces. Okay, I don't have anything more to say, but it works well. What's interesting is when you want to go and look for final topologies. So, as I said, the weak topology in the dual is a very coarse topology. You don't have a lot of neighborhoods. Neighborhoods, you don't have a lot of ways, you don't have any chance to be complete. So let's look for finer topology. You have another topology, which is called the Mackey topology, which is a bit finer and which also gives you a good way to have an involuntary linear negation. And you have the Trump topology on the dual, which is the one which is. Which is the one which is used most of the time. And in fact, what's interesting is that it's not easy to be reflexive with respect to this duality, but reflexivity with respect to the strong dual can be divided into conditions. So the algebraic algebraic equality between E equality between E and its double and the dual of its strong dual is equivalent to some completeness condition. And the topological correspondence between the two is also well known and is called the Barrel's nest. So what's interesting in fact is that you have a You have a the joint between barrel spaces and so weakly quasi-complete spaces, which is the weak completeness condition I just told you about, can also be described in terms of dynamic charities, meaning the negations, the two negations are exactly interpreted as the withdraw and the material. And the MAC hedge. And the shifts are interpreted as the endowing of the WIX approach and the MACIET approach. And it works very well. And it seems, in fact, that it's even the good, to me it seems that it's a good setting to describe reflexive spaces. You should describe reflexive spaces as categorically as this generalization of star autonomous category as a die-doper. As a di-doctorality. Even better, it gives you a way to simplify the model based on the epsilon product I just showed you. It gives you a way to have a continuous differential instead of a linear bounded differential because you are able to To separate the categories and to have on one way some spaces which on which linear bounded functions are always continuous. And these spaces, in fact, are the bonological spaces. And I'm going to talk a bit more now about bonological spaces. So I'm at the end of my talk and I'm going back to And going back to what we studied at the beginning to the setting of convenient vector spaces. So, if you remember, I said to you, well, smooth maps are well defined on machine complete spaces and were studied by Richthoma and Christine in their article. There is a bit more to the story. In fact, convenient vector spaces are multi-complete terminological vector spaces. Donological species are those spaces on which it is exactly the same thing to be linear continuous and linear bounded. And in fact, phonological spaces are perversive in the models of linear logic. They were in particular studied by Christine Dasson in her PhD thesis. Her PhD thesis. And in fact, later on, we showed with Christine that there were no need for these bonological species. What I'm telling you today is that this Bornolo fecality condition is here to give you a model of polarized classical differential mineralogic. Pharmacological spaces are here to give you the good interpretation. Here to give you the good interpretation for positive connectives, are here to give you a way to interpret involved linear negations. And in fact, you can describe the setting of convenient vector spaces and machine complex spaces in Accural. So, this is the end of my talk. What I have tried to do. What I have tried to do here, I have given you a lot of definitions. I have tried to give you a few pinpointers on the smooth model of differential analogy, a way to choose and maybe one day make your own smooth model of differential analogy. So what you would need is to have conveniently smooth functions. I think that it's quite acquired that this works best in the setting. Works best in the setting of differential linear logic. But you will need a way to accommodate with a linear bounded differentiation. You need to be complete, but you might want to have a very, a very weak, a very coarse completeness condition. At least complete might be enough or quasi-complete might be enough. And the choice of your dual topology will be very important. You want a dual topology which You want a dual topology which is fine enough to be complete, but which also needs to be coarse enough to be reflexive. And well, if you have it, please come and speak to me because I will be very interested in such a model. So what are the plastic structures on that? Well, in fact, when you go a little bit deeper in the interpretation of differential inner logic in In our logic, in topological vector space, and when you look at the interpretation of the exponential connective, you can have nice intuitions about the computational purposes of distribution theory. So it works a bit. So we have a way to retrieve, a very preliminary way to retrieve some information on computational information in distribution theory. Categorically, well, I'm a bit frustrated that in categorical axiomatizations of differential linear logic, the starotonous condition comes as the icing on the cake. You know, you ask all these axioms for a differential category, and at the end you say, hey, if you want to be classical, then you can just add this tartanous requirement. Requirement and to me, in the syntax and in the syntax of differential analogic and in its interpretation in topological vector spaces, it seems that joy is not the icing of the cake, but it is the core of the cake. It is the egg or the flower. But it should be. So could we put joy tea at the heart of the axomizations? I would like that very, very much. Very, very much. And of course, the setting of this talk has been quite restricted. I only went to look for a model of differential inalogic in the theory of topological vector spaces. There are a lot interesting things to say if you go into tangent categories or if you go in probabilistic settings. Okay, so I was a bit shorter than expected, but Shorter than expected, but that's it. So let's all unmute and thank our speaker. Thank you. Before we get to questions, I'll just say I would also love to see a reformulation of differential linear logic, which starts with duality. I think that would be really good. Are there questions? Oh, I could ask one. Have you thought about how some of these things like stereotype spaces fit into this picture? Or are you familiar with those? Yes, I know. I went to look at it and so stereotype spaces are a variation of Are a variation of vertex spaces, if I'm right. That's it. And in fact, they replace directness condition with another condition or something else. Is that it? Yeah, they're kind of like LCVS, locally convex spaces that are complete with respect to the pontriyagendu, is my understanding of them. Yes, okay. Yes, okay. I don't remember. I think at the time I thought it wouldn't work, but maybe I was wrong and I was young and stupid. I don't know. If I remember correctly, they are a star autonomous category. Yeah, they definitely are star autonomous. Yeah. Maybe just as I prefer to use what's already done and To use what's already done, and maybe the setting of smooth spaces of smooth analysis wasn't defined on it. I don't remember. JS? Are you expecting or is there a rule or like an identity between that sort of shows the compatibility with par and the differential? Because I can't think of one. Because I can't think of one. If you want to aximatize star autonomous differential categories, I assume you might also want to ask some compatibility between star and the dual and differentiation. So do you know? This might be closer to the setting of differential categories and not Cartesian differential categories because the goal of the power is to, let's go back. The goal of the power is. The goal of the bar is just to say, hey, we can store this color function, it's enough. And we look at the codomain another time and just glue the codomain with the power. So it might make sense to study differential categories with a power in that respect, just to define differentiation on scalar functions. But I guess it's But I guess that's already what's done in differential categories in a way because you have something which operates in the exponential. So does this enter your question? I have to say, I don't have all the actions of differential categories in mind at the time. I'm happy to do them, but I need to look at them. I just, I, yeah, I just don't see. Yeah, I just don't see. Yeah, I guess that's interesting that there isn't an obvious, like, here's how par and the differential interact, which is strange. My understanding, what I thought for a long time, is that kinalities were the way to define another. So these were my extra slides. To define another way to build a category or model of differential analogies, saying that, hey, we have a current behavior, an interaction between two categories. Could we describe the interaction at high order also as a nice curality or contravariant adjunctions? But of course, then you tackle the fact that the differential is not a functor and it gets messy. It gets messy go I don't know cool thanks yes any other questions okay any announcements from organizers nope I got nothing okay so then let's thank Marie again