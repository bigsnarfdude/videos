Yeah, I wanted something to say to see a little temperature. Okay, let's get started. Our last session of the day has three 30-minute talks from students and students at heart. Students of all ages. What's that? Students of all ages. Students of all ages. So thanks for being here, Jane. It's all yours. Great. This is joint work with Arsene Basilian. This is joint work with Arsene Vasilian. We're both students of Roni Mubenfeld at MIT. This talk is about agnostically learning monotone functions. There are like two of you who have already seen it, so take a nap. So we're in the setting of agnostic proper PAC learning. So we're given random examples x, f of x, from some unknown target function. And the distance from f to some desired concept class is going to be denoted as opt. Then we run the samples through some learning algorithms. Then we run the samples through some learning algorithm and output a hypothesis aiming into the op plus epsilon close to f, such that the hypothesis must belong to c. This requirement that f belong to c is what makes this proper learning rather than just learning in general. So we can think of this as finding the closest function in c up to some added error. One way to think of this distance opt between f and the concept class is that it's adversarial noise. We can think of f as actually having original We can think of f as actually having originally come from the concept class, but some adversary has taken an opt fraction of the samples and corrupted them adversarially. And also throughout this talk, whenever you see distance between functions, that refers to Hamming distance over the uniform distribution, so just probability over uniform x on the hypercube that f of x and g of x disagree. So in this talk, the So, in this talk, the concept class that we're concerned with is the class of monotone functions. So, first, I'll set up the problem a bit, talk about two ingredients in this algorithm, and then put it all together. These slides are just going to be an overview of the structure of the algorithm, and if I have extra time at the end, I'll do some of the subroutines in more detail on them. So, alright, the class of monotone functions, these are functions that respect the partial order on the hypercube. So, if x is smaller. So if x is smaller than x prime on the hypercube, meaning every bit in x is at most the corresponding bit in x prime, and there is some bit for which x prime is greater than x, then f of x has to be at most f of x prime. So you cannot flip a bit from 0, or from minus 1 to 1, and have the function value flip from 1 to minus 1. So to reiterate our task is to take samples from a function that's optical to monotone and output up to some additive error the best monotone. Up to some additive error, the best monogenic process. There is plenty of other related work having done with monotonicity, mostly by people like property testing. And there are these related problems, distance approximation and reconstruction, which our algorithm has implications for. And learning has also been studied quite a bit. So here's our main result on proper learning, monotone functions. We give an algorithm that runs in time 2 to the square root n. That runs in time 2 to the square root n, that's given uniform random examples from an unknown function, and outputs a monetone hypothesis, such that the distance between the target function and the hypothesis is op plus O of epsilon. So, to compare this to a previous best running time, I should actually talk a little bit about a statistical computational gap in this problem. So, what we're aiming to do in this work is actually to close a statistical computational gap. Gap. There already existed a previous, well, there already existed an algorithm that solves this problem using 2 to the square root n samples, but required 2 to the omega n running time. And I can talk a little bit about this on a subsequent slide. So a corollary of this is that because we have an agnostic learner, we also have a distance approximator with additive error with the same running time. Just run the learner and compare. Run the learner and compare the distance, it'll give you an estimate of opt. So our algorithm nearly matches a lower bound. This is a query lower bound due to the omega square root n that holds even in the realizable setting, so it necessarily holds for all values of opt. And actually, some people think of this as being actually matching and not nearly matching. The nearly matching part is because the lower bound is only for epsilon above a certain constant. For epsilon above a certain constant. So below that epsilon, there's no known lower bound, and our running time has a dependence on epsilon. So in that small epsilon regime, we're not matching a lower bound. Besides that, though, everything's matching up to constants in the epsilon. All right, so here's where we're going to get into the statistical computational gap, or what was previously believed to maybe be a statistical computational gap. So there's this classic old result. There's this classic old result in Boolean analysis that gives an improper learner for monotone functions. It shows that every monotone function is well approximated by a polynomial of degree root n over epsilon. So this gives you a learner. You just estimate these low degree coefficients and then output a sign of a polynomial that has 2 to the square root n terms. This gives you both sample complexity and running time 2 to the square root n, and with small modifications it can be made agnostic. However, this sine of However, this sign of a polynomial is not necessarily monotoned, which is what makes this an improper learning algorithm. However, from a statistical point of view, this is also a proper learner because the concept class is finite, so you can just enumerate all the possible monotone functions and find the closest one. And actually, you can do better in time 2 to the omega of n, you can solve the linear program that minimizes the distance to your target function, your L2 distance to target function. Your L2 distance to target function, sorry, L1 distance to target function, while obeying the monotonicity constraints of which there are due to the omega of n. So it was already possible to properly learn, it just took a long time. And our aim in this work is to just cut down that computational scale. So, in addition to completely improper learning, we also had semi-agnostic proper learning. So, in previous work, we gave We gave a sort of black box way of taking the improper hypothesis, this sine of a low-degree polynomial, making some modifications to it, and outputting a monotone function, h prime, that satisfies distance between the target function f and h, being a constant factor times opt, and then plus more factors of epsilon. The idea is that this process that transforms the improper hypothesis into the proper hypothesis. The proper hypothesis can blow up the error by a factor that's proportional to the distance between h prime and monotonic. H prime is the improper hypothesis. So if h prime is opt-far from monotone, then this correction process that outputs h could give you something that's two opt-far from h prime, and therefore three opt-far from f. So this extra constant in front of opt is what makes this semi-agnostic rather than truly agnostic. So, this approach of taking an improper learner, outputting some black box hypothesis that's both opt plus epsilon close to f and opt plus epsilon close to monotone, and then correcting it, can inherently do no better than 2 opt. Because intuitively, you can kind of correct in the wrong direction, right? You found something that's opt plus epsilon close to monotone, and you know, you correct it, which modifies the proportional to opt. You know, you may have gone further away from f. So, to get agnostic learning, So, to get agnostic learning, we need to somehow modify the search for an improper learner. Well, if we want to use this approach, if we want to use this, you know, learn an improper hypothesis and then correct it. We need the output of the improper learner to be closer to monotone, so that the correction step modifies it less. So, the question now is, can we find an improper hypothesis that's opt plus epsilon close to f and epsilon close to monotone? So, can we just make it, can we search for something that's arbitrarily close to monotone? If so, then if If so, then if we can run this hypothesis through some black box corrector, then it will increase its error only by factors of epsilon, which are controllable, and not change the dependence on opt. So that's the strategy. Is it clear that we should try to do this in proper learning first and then correct? I mean, I guess you could imagine a strategy that totally bypasses improper. Yeah, it's not obvious that this is the best thing we can do in terms of simplicity. I guess it's the best we can do in terms of pressure. I guess it's the best we can do in terms of parameters because of the lower bound. But yeah, I mean, there may be a better, simpler way to learn properly, like from the get-go. In some sense, that's what's happening, right? Just in at least, you know, if you can drive epsilon as small as you want, then okay, you're not truly getting proper learning, but you're getting like very close to proper because it'll be only epsilon and proper. Yeah, I mean, I suppose, like, no, I was going to say you can set epsilon to 2 to the minus n. To the minus n, but this doesn't work because that would make something else take 2 to the n time. So never mind about epsilon being 2 to the minus n. That would make the hypothesis monotone, but I guess we can't do that. So yeah, the algorithm that we give does require a correction step at the end, but I don't think that's inherently impossible to just properly learn from the start. Just to get it right, I thought one issue with directly approaching one of the problems I mean was that you Directly approaching one of the problems and you was that you don't even know what is the representation output for the target model of function values. That's what is the representation space you're learning into. Yeah, so I guess for this problem we're thinking of the output as just being a circuit. We don't really know much more about the form of the circuit because it's going to be a composition of a polynomial and a couple of L C A's. Yeah, so the form of the hypothesis is kind of weird. Yeah. Is the black box um a monotonifier? Is does it just take the uh the minimum for each function it defines a monotonic function which is the minimum for all smaller values in the partial order in the hypercube? Is that simple? Sort of like upward or downward closure? Yeah. Not quite. Um yeah, doing doing that in general I think could blow up your error by quite a bit. Um yeah it's it's something else. It's based on Yeah, it's it's something else. It's based on local graph algorithms. Um all right, so first ingredient for improving the improper learner is to reframe the whole problem as a convex feasibility problem. So recall that the goal is to find a polynomial that's opt plus epsilon close to f and epsilon close to monotone for an arbitrary epsilon. So we're going to take the space of low-degree polynomials that are now real values. Polynomials that are now real-valued rather than just Boolean valued, and take a distance metric to be L1. And by low degree, I mean degree root n over epsilon. So note that the set of polynomials that satisfy the constraint we want, or the hypothesis we output, is a convex set, but the number of constraints is very large. These are the monotonicity constraints, and the L1 falls around them. So we can't just optimize just reading all the constraints. We can't afford to read all the constraints. Afford to read all the constraints. So instead, we're going to use a method that doesn't necessarily have to depend on the number of constraints, and that's the ellipsoid method. So its running time will be polynomial in the dimension of the optimization domain, no matter how many constraints there are, as long as we can give an efficient implementation of a separation article. Yes? So the root n comes from the degree of the ball market as a concern? Yes. All of these tildes are hiding on a log factor because it's n to the root n. So, the ellipsoid algorithm will find a feasible polynomial, as long as the feasible region is not empty, if it's given a separation oracle for the feasible region. So, the separation oracle takes a point, which in our case, because we're optimizing over a space of polynomials, every point is a polynomial, takes a point as a query. If the point is feasible, then the oracle answer is yes. Otherwise, it gives a separating hyperplane that distinguishes the point from the feasible region. Feasible region. So in this case, hyperplanes are also polynomials. We want a polynomial that has low inner product with all points in the feasible region, but high inner product with the bad point we just queried. So basically, we want to give a witness that the polynomial we just queried is bad. So, oh, yeah. Was that a a breath indicating a question or a still no question? Just Better than the other way around. Feel free to interrupt. We call that the objective we're trying to, you know, the things that define the feasible region, it's an intersection of two convex sets. So the separation oracle we're going to give is an intersection of two separation oracles. Basically, you know, a candidate polynomial can be bad for one of two reasons, or both. One is that it can be too far away from One is that it can be too far away from f, so it can fail to be opt plus epsilon close to f. Giving a separating hyperplane in this case is easy. This is just the subgradient of the prediction error. We can estimate this from samples. But it's not very obvious how to give a separating hyperplane for a polynomial that is close enough to f but too far away from monotone. So our objective now is to find some polynomial that will have high inner product with some specific function that's far from monotone, but have low inner product with But have low inner product with all monotone functions. So, a key observation in this work is that functions that are far away from monotone have a large weighted matching in a graph that I'm about to define called the violation graph. And in fact, the weight of the maximum weight matching is exactly the L1 distance to monotonicity. So, these matchings can be used to produce a separating hyperplane if we can find a way to represent them efficiently. So, they can have 2 to the n edges. So, we need to find a more So, we need to find a more concise representation. So, the violation graph is defined to have an edge between any pair x and y such that f violates monotonicity on x and y. So, x is smaller than y and f of x is greater than f of y. The weight of the edge is the magnitude for violation, so it's f of x minus f of y. So, we're going to truncate the hypercube and just ignore everything that has hamming weight larger than n over t4. Hamming weight larger than n over t plus root n plus 1 over epsilon, and also ignore everything that's having weight is below n over root t minus root n. Ignoring this gives us like an extra epsilon error. We don't care about it. But it does allow us to shrink down the degree of the violation graph. It would otherwise be 2 to the n. But now in the truncated hypercube, it's 2 to the root n, which is good, because all of our graph algorithms run in time that depends on the degree of the graph. So we're going to search for an approximate maximum. So, we're going to search for an approximate maximum weight matching in this graph to serve as our witness that a function is far from what. So, we're going to define a function. So before we were just talking about a matching, just a set of weighted edges, but what we actually need is a polynomial, so that this polynomial can have high inner product, the candidate polynomial. So, first we'll just define a function and not think about what it looks like as a polynomial yet. So, we define m for each x. So we define m. For each edge that's not in the matching, we just set m of x to be 0. If the edge is in the matching, set the top vertex to be minus 1 and the bottom vertex to 1. So this is now an edge that violates monotonicity as much as possible. So the inner product between M and F is now exactly the weight of the matching. Because the bottom part, yeah, because for each edge in the matching, the bottom thing is larger than the top thing. So they both contribute their weight to the matching. But it can have positive inner product. But it can have positive inner products with any monotone function, because for a monotone function, the top thing is always greater than the bottom thing. So it will contribute negatively. So suppose you can just evaluate m of x quickly for an arbitrary x. You don't need a full truth table of m. You just need some circuit that will take x and output m of x. Then you can learn a low-degree approximation to f, and this will behave similarly to m itself in terms of its inner product with f, because f. Of its inner product with f because f is heavy on low degree coefficients. So a low degree approximation to f serves as a separating hyperplane that we need. But what's left is to find an algorithm that evaluates m of x quickly. So to do this, we need to bring in tools from sublinear algorithms, particularly sublinear graph algorithms. So this is the LCA framework, which is introduced in several works about a decade ago for describing algorithms that Decade ago for describing algorithms that read small portions of a large input, typically a graph. So the user will give some query, say, you know, is vertex B in this independent set. And the LCA will make some queries to other things in the graph, usually neighbor queries, ask like, what are all the neighbors of this vertex? And then give the answer just for that specific output, just answer the user's query without storing an entire representation of, for example, the independent settings. Of, for example, the independent setting. Alternatively, and this is kind of the purpose we're using it for, we can think of an LCA that transforms a succinct description of the input into a succinct description of the output. So to use the independence set example again, if you have, for example, a circuit that computes the neighborhood function of a graph, so it's a circuit that you feed in x and it outputs all the neighbors to x, this circuit could be much smaller than the graph itself. Then the LCA is something that you can compose. Then the LCA is something that you can compose with this circuit to make it turn into a circuit that computes the membership indicator function of an independent set. So now you feed in X to this circuit, and this composition of circuits now outputs yes, X is in the independent set, or no, it's not. Does that kind of make sense? So, yeah, LCA, we can think of it as just transforming one succinct description into another. Where you put a copy of Where you put a copy of the circuit that competes graph edges every time the LCA circuit works. Exactly. Yeah. Yeah, so the size of everybody is the card. So we have really two uses for LCAs in this problem. One of them is to give this constant factor approximation to the maximum weight matching, which is to be used to be the separation oracle for distance to monotonicity. And then we also need a local correct. And then we also need a local corrector at the end because the hypothesis the output is not quite monotone. We want to output a truly monotone hypothesis, so we'd like something to make the final modifications to the function so that the output is monotone. Both of these algorithms are built on an existing LCA by Mohsen Kafari a couple years ago. For maximal matching, actually it's for maximal independent set, but the reduction to matching is vocal or this algorithm is polynomial. This algorithm is polynomial in the degree of the graph, which remember is 2 to the square root n in the monotonicity violation graph, which is exactly the running time we want. So our first ingredient is that we give a delta log n to the log 1 over epsilon query LCA for this maximum weight matching that also has additional additive error epsilon that we don't care about. So this is our local algorithm for constant factor approximation. For constant factor approximation of maximum weight matching. So, when running the violation graph, the time to evaluate the distance witnessing function m that I described is 2 to the square root n using this algorithm. And then for local correction, I'm stating this as a separate result, kind of of independent interest, because it's also just an interesting sublinear algorithm by itself. Interesting sublinear algorithm by itself. We give a local corrector of real-valued functions. So f is a function from the Boolean cubed to R. It's alpha close to monotone and L1 distance this time. We give an LCA that makes queries to f and outputs queries to some function g such that g is monotone and the L1 distance between f and g is alpha plus epsilon, or O of alpha plus epsilon. So this does blow up like constant factors, but recall that when we're using this at the end of the ellipsoid algorithm, alpha is going End of the ellipsoid algorithm, alpha is going to also be S1. The running time and query complexity in the hypercube are 2 to the square root n. However, this works in an arbitrary poset. The running time just depends on the degree and the size. So the way this algorithm works is that it's a reduction from correcting real-valued functions to correcting Boolean-valued functions. Correcting Boolean-valued functions is something that was done in previous work. It gives a Done in previous work, it gives a 2 to the square root n query LCA for correcting monotonicity in a Boolean-valued function that has this specific property that it only swaps labels, it doesn't overwrite them. So in a sense, we can think of this algorithm as giving local access to a permutation, pi, a permutation on the vertices of the hypercube, such that f of pi of x is quantum. So you, you know, you query x, and it gives you the identity of some vertex in the hypercube that you should. In the hyper Q that you should swap your label with to give a function that's function. So, a little overview on the integer to Boolean reduction, or well, the real to Boolean reduction, which goes through integers first. If a function is from the hypercube to k possible values, we think of f1 through f log k as its projections onto each bit. So, you know, f1 is the most significant bit, and f log k is the. bit and f log k is the least significant bit. An equivalent way to characterize monotonicity in the k values is to say that f1 must be monotone on the full close set. And then for all i, fi is monotone on the subgraph that's induced by vertices that agree on all the more significant bits. So for example, this is just a small postet, definitely monotone in the first most significant bit. These are both ones, these are both zeros. There are no edges pointing from one to There are no edges pointing from one to zero in the most significant bit. In the least significant bit, it's not monotone by itself. This edge points from one to zero, but these edges should be considered incomparable because they don't agree on F1. So if you only compare vertices that agree on F1, then F2 is one of them. So this gives us a way to reduce from correcting real values to correcting Boolean values. First, round to multiples. First, round to multiples of epsilon. Then, you know, run the Boolean algorithm on these projections, f1 through f log1 over epsilon. This gives queried complexity 2 to the root n with an extra factor of log 1 over epsilon in the exponent for having to use extra log 1 over epsilon rounds. And we give a lemma that any algorithm based on swapping labels cannot increase the L1 distance to monotonicity. So then by triangle in. So then, by a triangle inequality, this gives us a multiplicative error of 2 with an additive error of epsilon from routing. So now we can put it all together, got this diagram, how the learning algorithm works. Looks more complicated than it is. So, you know, starts with the ellipsoid algorithm, we'll spit out some, you know, some candidates. Just some polynomial with 2 to the square root n coefficient. With 2 to the square root n coefficients. And the first thing we're going to do is test whether it's close enough to F by computing the gradient of the prediction error. If it's not close enough to F, then we output a separating hyperplane for P. Then the ellipsoid algorithm will spit out another hypothesis. And suppose we've reached a point where now these hypotheses are close enough to F, but it's not epsilon close to monotone, then we're going to run this constant factor process. Constant factor approximation for the maximum weight matching, which gives us a way to evaluate this distance witnessing function that I was calling m, which has high inner product with the candidate polynomial, even in its low-degree approximation. So the low-degree component of this witness function is now a separating hyperplane for p, which we return to the ellipsoid algorithm, and it'll spit out another hypothesis. When both criteria are satisfied, then we have a polynomial that's epsilon close to monotone. Polynomial that's epsilon close to monotone and ob plus epsilon close to f. We append the corrector to this circuit. So polynomial is a small circuit. We like append this correction algorithm to the circuit. It's not really append, I guess. Compose, you know, it blows up the size by the product of the two. Which gives us a circuit that computes a monotone function, which is close to f and l1 norm. And then we round it to Boolean, and that's it. So that's the learning algorithm. That's the learning over there. Why do you have time? Well, there's like a couple of minutes, so I guess I will not draw on the street slightly later in five minutes. Not too much I add in five minutes. Yeah, any questions, I guess? So what do we know about correcting monitoring city with membership queries? Testing with membership queries? No, testing. Testing, we know there's a separation, right? But with correcting for membership queries with constant factor blow-up in distance or additive blow-up? Either one. Either one. So with constant factor blow-up in distance. So, with constant factor blow-up and distance, this last thing does it in two to the root n time. And for additive blow-up and distance, I guess the entire learning algorithm does it in two to the root n time. But what if I allow, this whole thing works with random examples, right? Yes. But I'm going to let you do have membership queries. But when you say membership, what do you mean just evaluate? Yeah, yeah, so that's relationship. Yeah, yeah, so that's relationship. Yeah, probably. Yeah, so I mean, this last thing is a query algorithm and would not work with random examples. It's just that the thing it's querying is the polynomial that was output from the ellipsoid algorithm rather than the original target function f. But yeah, these corrector algorithms are query algorithms. And so is this matching thing. All of these graph algorithms require queries to a function. It's just that the function is not the original target, but the current But the current candidate probably the correctors clapped on top of the polynomial basically is what your proper hypothesis. The polynomial is not proper because it's not proper. But this is like a like like June said it's a circuit. Like you insert a circuit. So the thing that learns the improper polynomial, right? From the previous part, was that random example, like random input output? Okay, good. All right. So let's sort of stick with that. And now, let's say you have mapship, and if you have mapship queries, evaluation queries, what happens? Can you prove the two to the root end to something smaller? That depends on a lower bound that hasn't been proven, but everybody believes in. So people believe that there is a lower bound for. That there is a lower bound for estimating distance to monotonicity that is 2 to the square root n. The current best lower bound for this problem is 2 to the n to the 1/4 non-adaptive queries. So if that's actually tight, then estimating distance might take only that amount of time. And so correcting might also take only that amount of time. But yeah, no lower bound for correcting is no one percent. So if the current lower bound So if the current lower bound for distance estimation is tight, then it could be that there's a 2 to the end of the 1 quarter time algorithm that gets query access to the function, which would be better than 2 to the root end, which is what you get if you just get written samples. 2 to the n to the 1 fourth for the gene distribution is a sort of a guess. There's no 2 to the n to the 1 fourth is a proven lower bound as in like Is a proven lower bound as in, like, there's a hard family of functions that are hard to estimate to a constant factor, like, estimate distance monotonicity within a constant factor in fewer than that many non-adaptive queries. With adaptive queries, I think nothing super polynomial will see with them. But also, it sounded like you said there's a conjecture that everybody believes, but isn't proved, which is that that 2 to the end of the one quarter should go to 2 to the root end. Yeah. Okay. Yeah. So the lower bound of 2 to the root n for this problem, do you think it should be 2 to the root n over epsilon? Or do you think that small epsilon is something? This 2 to the root n is for learning. And that's also why people think there should be 2 to the root n lower bound for distance approximation. I mean, that lower bound definitely is bright because it matches. Is right because it matches the upper bound. So, dependence on epsilon. Oh, you mean like for small epsilon? Yeah. What should the dependence be? Right. No idea. I mean, the dependence on epsilon in the upper bound is just one over epsilon in the exponent. Maybe that's also true for all values of epsilon. It seems natural. Yeah. Can you take the function you get out of the matching and make it into a low degree at some point here? We want to preserve the correlation with the thing that the LSOID method output. So that's fine, but we also want to preserve the correlation with all the monotone functions. Monotone functions are all epsilon, like 1 minus epsilon concentrated on low degree. So the low degree part is a good estimate of the inner product. So this is somewhat speculative, but could there be a general theorem that says that you can convert General theorem that says that you can convert improper learners into proper learners if you have an LCA? Yeah, so for semi-agnostic, certainly. So if you have an LCA for correcting, that turns any improper learner into a semi-agnostic proper learner, to get truly agnostic, to bypass this issue of correcting in the wrong direction, you might need something stronger. Certainly, if you have a convex concept class and separating oracle, then you can use that. But I think in general, the best way. I think in general, the best we can say is give it something seventy cost.