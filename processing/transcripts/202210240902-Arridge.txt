Which non-linear inverse problem that was maybe not kind of non-linear but not extremely ill-posed, which is unusual, which arises in coupled physics, which several people know about. And in order to do that, I'm going to explain a bit about modeling of light, which is going to be done stochastically. And then the inversion is going to be stochastically. So the idea, the kind of novelty here is that we are introducing this idea of what I call fully stochastic reconstruction, which means that we're using a stochastic We're using a stochastic optimization for a problem which is modeled stochastically, as opposed to a model, stochastic observation for a deterministic problem or deterministic optimization for a stochastic problem. All right, so let's look at, oh, and yes, I thought I'd say at the beginning, some of the collaborators. So I've collaborated with many people over many years. And in this particular talk, these are some of the ones who kind of contributed to Kind of contributed to at least to the ideas and to some of the actual results. So let's look at the normal setting of inverse problems. We'll talk about non-linear inverse problems. So we'll say we have a mapping from a space X to Y. We have observations, observed data in the space Y, and we have a non-linear mapping with this operator A. minus operator a and there's noise as well so we know that a typical way of solving this is to to set up a objective function we have a data fitting term and a regularization term and and within the Bayesian setting we we interpret this negative log likelihood and the negative log prior so a typical way to start is to Think about iterative methods and to think about descent-based methods. So we take the gradient of these two terms, we take a step length, and we iterate until some convergence criterion. Or alternatively, if we have a non-smooth prior, we might take the gradient update of the log likelihood term and then use a proximal operation. Term and then use a proximal operator. Now, when we're in the discrete setting, which is where we are computationally, we'll think of this just as a mapping between Euclidean spaces R into M, and we'll use M as the size of the data. And this, in the case of imaging, may be quite large. So, typically, we might have 10 to the 6, 10 to the 8 observations. And so, as many of us know, it's competentially demanding the classical idea. With a classical idea, we construct a forward operator here. We to get the gradient, we usually have to apply an adjoint operator. Those may be very costly. So the idea of subsets, you consider a partition of the set of data, which are non-overlapping and covering of the whole set. And then we assume that we Then we assume that we subdivide the factors, and we just use a symbol f of x as the functional representing this data fitting term. And we'll assume that it's a sum over components for each of the subsets. And the subset is given by the observation, you know, sum over the elements of that subset with this picking this component, this element of the data and assuming that it's map and just taking the nth component of the projection. The nth component of the projection of x. So, in imaging, at least in medical imaging, this idea has been around quite some time and often it's referred to as ordered subsets. In other words, the partition S is carefully chosen in order to optimize the accuracy of convergence of the iterative algorithm. And there are usually some quite subtle questions to ask about convergence on whether it's. Convergence and whether it enters a limit cycle and so on. And also, a typical thing in tomography, since this is where mostly I've worked, the partitions are based on groupings of angles. So as you know, in tomography, the data consists of a space, which is a product of a spatial part, usually a line or a 2D image, and the angle. So it's something like. 2D image and the angle. So it's something like R cross S1 or R2 cross S1. And usually the partitions only consider decomposition of angles. They don't consider subdividing the projections into kind of pieces. And usually these are chosen to be under some kind of criterion which says that each particular subset is somehow orthogonal in some certain sense to at least the previous or to the accumulated set. The accumulated set that you've iteratively been working through. And this is, for example, the ordered subset expectation maximization algorithm is a huge acceleration from the maximum likelihood EM algorithm. So the paper, many papers, there's a couple of pioneering ones from the 90s. Well, as many of us know, we can, with the rise in machine learning, people have Machine learning, people have paid a lot of attention to what is known as stochastic gradients. And the difference is then that the substance are chosen stochastically instead of with some specific preset sequence. Sorry for a few typos here. And so if we look in this machine learning setting, we typically have something a bit different going on. We have a function to be optimized, which is usually called a loss function, which consists of the difference between an observation. The difference between an observation and the output of a neural network for a space X. So, this, I mean, if we take this literally, what this is describing is a network that's mapping the forward operator. Of course, we could also have written a network for the inverse operator with x here and f theta of y, but this is not the main point I'm trying to make. The idea here is that we have a very large, rather than thinking of these sort of data in terms of. Rather than thinking of these sort of data in terms of the dimension of the range of the operator A, we're thinking of some abstract space in which pairs of data lie, and we consider many examples, which we call the training set, and we optimize this empirical mean estimate of the error of the map over this set of training. And then, when we talk about the subsets in this context, we have this term mini-batching, one of the secrets of One of the secrets of learning how to use AI and machine learning is just understanding the language of mini-batching is what we would call subsets. And with this, we have various variance reduction methods. So obviously the gradient itself is a slow algorithm. So there are a large number of variations on that for acceleration, which are based on. Which are based on, well, there are actually many techniques, but one which I'll be mentioning here are called variance reduction techniques, which have appeared more recently. So what I'm going to do here is see about, there's a particular idea at Spranger Monroe in our group recently, which was to leverage this SGD kind of body of work for solving non-in-inverse. For solving non-linear inverse problems. So, the idea being, can we find some cheaper way to compute the forward and the adjunct operators, allow for errors, and then try to produce the resulting expectation. That's the basic idea. So, now already I have to take a bit too long, so let me just be very quick about coupled physics, which is not the main focus of this meeting. And although I think many people present will have worked in areas like this. So the idea of couple physics imaging or imaging for couple physics is a term we coined is that we're combining two different kind of physical phenomena. So we have these kind of techniques like CTMR and ultrasound, which are commercial devices that are in every hospital. And from the point of view of inner problems, they reasonably well posed. Reasonably well posed, mildly ill posed, let's say. But actually, the images that we have are actually a bit limited in the quantitative output. And one of the reasons you can think about this is because physically, you know, the mechanism of propagation, Lux used just the term wave, even if we're talking about particles for the sake of argument, it's not particularly strongly perturbed by the contrast. So, you know, the So, you know, the X-rays still can train lines even when objects are present. Of course, you know, there is scattering too, but we, in most cases, are ignoring that. And the simplest form of ultrasound is simply looking at a time series reflected from certain objects and then just simply putting back the signal at just converting time to a distance and just painting it back into the image in a simple way. Of course, each of these problems does have a more sophisticated A more sophisticated physical background, and some there are some more sophisticated non-linear versions of these, but not usually adopted. And then we also know that there are many emerging techniques. I mean, we still call EIT an emerging technique. It's been around at least 40 years, maybe longer. Diffuse optical more recent and microwave imaging, often based on inverse PDE parameter recovery, usually from a bank. Recovery, usually from a boundary value problem, usually highly ill-posed, strongly opposed, and non-linear. But people still want to work with them because you get some feature from the image which contains some rich set of information, just at very poor resolution. And the reason that it's interesting is because there's this strong interaction between the object and the wave, which gives rise to the non-linearity. And in the coupled physics imaging, you essentially... Physics imaging, you essentially say we're going to create the contrast with one eye and read it out with another. So we get contrast from the first and high resolution from the second. That's the idea. So there are many techniques. So I'm going to talk mostly about for acoustics and a little bit about acousto-optic imaging. But again, in the interest of time, we can't dwell very long on all these different methods. So I think actually I'll skip a little bit of this and just show there are many. Just show there are many photoacistic methods around. And there's also another technique I'm going to mention: ultrasound modulated optical tomography, where we put in a beam and we use a focused ultrasound wave to vary the density. And in theory, if we could scan this focal point throughout the domain, we can automatically isolate the contrast and put it back into the voxel. But because Into the box, but because there are complex interactions, it turns out to be more non-linear than indicated here. So, these two techniques: the key point about it is that we're using light and sound together. And what turns out is that the forward problem needs, I mean, the full forward problem, we have to model both these problems, but the problem which we call recovery from internal data assumes there is a the readout mechanism is inverse. Are the readout mechanisms inverted? This high-resolution part I mentioned here is this part, the reversal of the readout wave is quite straightforward. And we can get a qualitative image quite quickly. And then to get the quantitative image requires a non-linear inverse problem. And those non-linear inverse problems involve an inverse optical transport problem. Now that brings us to the modelling of light. Brings us to the modeling of light. And to be again a bit brief, the mechanism in the absence of scattering is relatively simple. This is what would apply for X-rays. We simply have transportation of a directional radiance along a prescribed direction, and it's absorbed by the objects. And we can invert this differential, this ODE, to give this exponential decay along the line of propaganda. Along the line of propagation. This is the basis of the ray transform. We take the negative log, the ratio of the output to the input intensity, and we have a linear relationship between the projection and the coefficient of interest. This, of course, forms the basis of X-ray CT we can solve by filtered back projection if we take appropriate complete data. But in the presence of scattering, we have a lot more complex problem because we have scattering. Complex problem because we have scattering also taking away. Now, in X-rays and high-energy modalities, this proportion that is scattered is much less than the proportion that is attenuated and can, to some extent, be ignored and just appears as maybe some noise component. But in the optical working with the visible range or around the visible range, this far dominates the scheme. And again, because we need to be brief, there is a technique which we may do, which formally solves. Do which formally solves this problem. So, S is now this scattering operator, which just takes rays and redirects them in a different direction from the direction of propagation, or conversely scatters them from another direction into the direction of propagation of the line of sight of the detector. So, this can be solved with this essentially Neumann series. And what this roughly says is that we can form a solution by propagating the ray. By propagating the ray and directly recording photons that are not scattered, photons that are scattered once are propagated in some but other direction, rescattered into this direction and propagated, or any more number of times. And this, you know, sort of illustrated here. So if we have a relatively low amount of scattering, we have a beam which is exponentially decaying and some small amount of scattered photons. As the scattering increases, the tension. Scattering increases, the attenuation of the beam in this prime direction is decreasing more rapidly, and the photons are propagating diffusely to a greater extent. So, as some of these people, Kui in particular will know, and several of the people in this meeting, in this workshop, know that the deterministic computational way of solving this is extremely expensive and there are many, many techniques. And there are many, many techniques: finite elements, finite differences, finite volumes. We can use spherical harmonics for the angular direction. We can use second-order methods. There's a plethora of techniques which mostly arose in the nuclear physics industry following the development of atomic weapons. But now, Jose, I think I have a small problem here, which is that I have not produced. Which is that I've not produced the version with animations. I've inadvertently deleted the animations, which is a shame. If you give me one second, I will switch to another version where I do have that. Should have checked that. So let me, sorry. Sorry. Yes, this is fine. This is what I wanted to show. Let me just go to full. Did to show, and let me just go to full screen. So, there is a stochastic version of this modeling, which is known as the Monte Carlo method. And essentially, this Monte Carlo method just conducts this re-scattering process, but determinists. Process, but uh, deterministically, so if we have uh, we put into the system a small number, yes, 10 photons, they propagate, they're rescattered and rescattered, and some of them reach a detector plane. And if we increase the number of photons, sorry, let's not animate that, let's just step through it. Um, if we increase the number of photons to towards, I'll keep doing that. Uh, if we increase the number of photons towards a Would say sort of essentially an infinite number. Let's see if you keep doing this. Sorry. As we get to, if we get to a sufficiently larger number of photos, 10 to the 7 photos in this case, we approach this deterministic solution in expectation. All right, so what is the idea? Well, we can proceed with a deterministic method. Let me just very briefly mention it. Very briefly mention it. In the photoacoustic method, we have an optical source of light, we generate an internal field which we call H, it's a deposited energy, and this generates an acoustic field which we detect. Now, we assume that the reconstruction of H is well posed, and there's many techniques to solve it, which I don't talk about today. And we were going to find this H, which we call internal data, and want to recover from that the actual coefficients. coefficients so the this age is given by this product of absorption and uh the density of photons which are not dependent on the direction because absorption is not directionally dependent so again being a little bit brief if we if we go through um just a classical um method of solving this uh we we arrive at the idea of solving the transport equation for an adjoint field and arrive at And arrive at expressions of this form where we need to find a what's called in geophysics an imaging condition, once a term I quite like when you're solving inverse PDE problems, you always find you have to solve a forward problem, adjoint problem, and combine the forward and adjoint fields with some simple algebraic expression involving the parameters you want to find. So in the case of QPAT, we have to In the case of QPAT, we have to construct this gradient of solution in this way. Let's be brief about also a similar story occurs for this ultrasound modulated problem here. We have the ultrasound focus. This internal field B here is now given by a slightly more complex expression, which involves a primary field phi Q and a lead field phi M from the detector. But otherwise, we can go through to find a And a similar expression for the gradient of this optimization, which involves now a slightly more complex operation with two forward fields phi m and phi q and two abilant fields phi star one, phi star two. So I apologize because of the short time. I won't go through all of the details of this in the paper and I'll make these slides available as well. But this is the key idea, it's fairly familiar, anybody who's solved inverse PD problems is that. PD problems is that you have a computational forward problem, you have a computational actual problem. And usually, one's perception is that we should try to make both of those problems as accurate as possible. And this is computationally heavy. Now, if we take the idea that we substitute, use the Monte Carlo as a proxy for this construction of these fields, we could proceed by running them to a very high number of photons, high accuracy. Photons, high accuracy, and use simply computing these fields in a similar way. So you can use Monte Colour to generate a forward field and an adjoint field, and simply take sufficiently large number of photons in the system to make this as accurate as possible. All right, now the idea then in the stochastic setting is that we want to kind of minimize the computation involved. So deterministic RT, or alternatively, Deterministic RTE, or alternatively, sort of high-accurate Monte Carlo RTE, is computationally expensive. But we can make it arbitrarily accurate, but it has this problem that it's not easily understood. I mean, it's not an operator between spaces, it's simply a photon counting algorithm. So we have to somehow embed it into a representative space to be able to use this algebra. And this is one approach that we tried. This is one approach that we tried and we had some reasonable results with it. But the new idea is if we now run the Monogola with rather few photons, it's inaccurate, but it forms exactly the subset idea that we were just talking about. Instead of choosing the subsets in an ordered way, they're naturally chosen by the randomness of the Monocada itself. And furthermore, And furthermore, we can then leverage these recent methods of adaptive stochastic gradient descent. And roughly, the idea of the adaptive stochastic gradient descent is to say that when we're far from the converged solution, taking random steps with relatively large step sizes and small batches of photons, which mean that we have lots of stochasticity, is a perfectly reasonable thing to do because we are covering the landscape of the movement. We're covering the landscape of the minimization problem. As we approach a minimum, we should be increasing the number of samples so that we have a more accurate step, and we are reducing the step size in order to converge to the solution. So sort of making this connection between gradient methods, so what machine learning people would call batch gradient is all of the data. We would have this normal descent step here. Yeah, and we know that with this stochastic forward problem, if we have n tend to infinity, then we will expect our cost function to go to zero in the absence of any measurement noise. And in the stochastic setting, we take these subsets and we have the property that their expectation will be the accurate solution. All right, let's be a little rapid now. So that's the idea that we're just going to take a small number of inaccurate steps. Small number of photons, inaccurate steps. And the idea, when I show you the experiments, is that we will assign a total photon budget to the whole process. So we'll assume that the cost of the optimization is just the total number of photons that we propagate. But we're going to selectively and adaptively propagate more photons, less photons at the beginning and more at the end. Okay, so I need to move through this. So let's skip a few of the details. Skip a few of the details. The key point is that we adapt the step based on one of these two tests. One is called the norm test, which checks for the variance of the error in the propagation step divided by the true step less than the threshold. If we exceed the threshold, we're going to increase the number of samples. And similarly, the inner product test is the same, but it only considers the expectation in the direction of the gradient and ignores the expectation. Direction of the gradient and ignores the expectation orthogonal to the descent direction. Okay, let's just show the algorithm. So, that's just pictorially. Uh, if we have a deterministic setting with running very large number of photons and just gradient descent, we have very small number of photons, we will want to walk around the solution and not reach the expectation. And the idea is that we will adapt this solution to get to the adapt the step size and the dispatch size to get to the solution. All right. Solution. All right, so this is the full algorithm that we choose an initial photon size. We check whether we've built through the whole photon budget, which fixes it. We run these norm tests to check whether the variance of our solution is below a threshold. The variance of the gradient is below a threshold. If it fails the test, we increase the sample size and possibility updates. Okay, let's cut to the so the three strategies we looked at. One, Three strategies we looked at. One is we have a fixed step size and vary the sample size based on the norm test. Then we have a step size based on the current estimate of the variance and we use an inner product test. And similarly, between these two and three, we're using the square or just the variance itself. And this is showing some results. Let me first of all show the movie. The key thing to look here is the number of photons we're running. We start with a small number of photons and we're going to increase the number of photons. And on the right is the reconstruction as we're getting through the photon budget. Okay, and I think that chat is saying I run out of time. So that is basically the result I want to show. I can ask more. And let's Let's finish there. So I discussed imaging for couple of physics, a little bit, two problems, both non-linear, but both not strongly opposed. And the idea was to combine the stochastic methods of light propagation with stochastic optimization. We call this a fully stochastic inversion. There's quite a few simplistic. We actually essentially was noise-free because we ran to get the observation, we ran a very large number of photons. So we could. Large number of photons. So we could take more noisy estimates and include explicit regularization. That's straightforward. Rather than just L2 loss, we could consider callback labor or something more sophisticated based on a comparison of distributions. And then there are these other adaptive schemes like SAG and SARA. There are also several aspects I didn't mention about because to estimate step size, we need an estimate for the Lipschitz, which is not straightforward for. Which is not straightforward for stochastic forward models. So, I think there is the basis of several interesting things going forward, and I should stop because I've taken my time. Thank you. Thank you, Sam, for a great talk. Any quick questions, comments from the audience? I actually have a question. So, Monte Carlo. So Monte Carlo is not completely necessary here, right? If I just have any deterministic in a solvent. Oh, absolutely. Sure, of course you can. Yeah. But then you would need, yeah, I mean, then you would have to choose the subsets in, but some, you know, you would have maybe not such a simple way to choose the subsets, right? So if you used a, say, a PD solver with a particular source, you would, you would probably then be choosing all of the projections from a single source. All of the projections from a single source, possibly. That's the natural way because when you solve a P, you kind of get all of the direction. You know, I mean, it's trying to be general about this. You know, in the particular case, the RT, you would choose a source. You would get the whole field from one source all together, right? And then if you were going to use a multiple source, you would have to select them by hand. The appealing thing here is that you have some completely unbiased selection of the subsets because it's just arising. Subsets because it's just arising randomly, you know, photon by photon, which is how the data would be acquired as well. But, yes, sorry, anticipating your question. Sorry. You could certainly apply this with what you would call a deterministic forward model. Then you would be in the realm of stochastic gradient optimization for a deterministic problem, which is a fairly well studied. If you used a, you could also use an inaccurate solvent. Also, use an inaccurate solver. I don't know if I'm anticipating a question. I think because I know I remember you've looked at these kind of low-quality forward solvers thing. But then you would have a biased estimator, right? Because you would have systematic error from the particular kind of sub-optimal grid size that you were using, the computational solver. And so then I think. Um, and so then you, I think you'll have a more complex problem because you ought to consider then the correlation of that noise. Here, we have a real clean idea, I think, which is that we know that in expectation we will have the accurate solution, whereas you wouldn't have an equivalent way. Well, I mean, you can.