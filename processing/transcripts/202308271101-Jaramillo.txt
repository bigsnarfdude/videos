And listening to me and for inviting me to give this talk. And I'm apologizing for not being there. I wanted to be there really there at the end. I had some personal problems. I could not be there. So I'm going to talk about how we can engineer gene circuits in a way that it's much more complex than currently possible. Currently possible. But first, I'll start with the most important thing is acknowledging the people who did the work. It's my former postdoc, Satya Prakash, very hardworking. He did 90% or more than 90% of all the experimental work. And then I have to also thank all my collaborators at several institutions. Okay. Okay, so if we want to engineer gene sequence, usually the paradigm is to use standard parts, standard biological parts, and then you assemble from that. Parts can be promoters, RPSs. You can, as we have seen in previous lectures, you can use automated design methods to compute what would be. And what would be the best parameters, but then you have problems when you need to create models, as Howard said. I mean, this is the next step in a challenge for the predicting the global behavior, the function of the whole circuit. But problems like board games, like chess, and that always have moved and pushed in computer science the boundaries. The boundaries just by setting some challenges, and these are far away still for in vivo engineering. So the farthest has been done, it's in vitro and it's playing tic-tac-toe with DNAzymes that are like rubozyme, but with DNA, catalytic DNA. So Estejanovich and Stefanovich. Estejanovic and Stefanovic more than 20 years ago, exactly, and they produced this set of 23 logic gates. These are and not gates to make an automaton able to win against a human. You remove symmetries and etc. And this is not learning. This is not learning, it's just playing expert. Okay, I would like to go beyond that, learning. So, how you get a system that would go to for to that. So, then it's an adaptable system. So, then we have, for instance, direct evolution that would be the usual technology, but unfortunately, it's very slow. And even if you increase multi- And even if you increase mutagenesis 1 million times, still it's too slow. And then you need to adapt to several conditions, which make it even slower. And rational design, guided by digital electronics or computational design, that would be useful is the choice. And unfortunately, I mean, and we have a problem of information and encoding. I mean, getting this system to adapt, to learn, requires encoding a lot of information that it doesn't fit in the standard DNA parts. So how are we going to do that? So we propose to do an iterative adaptive method. We borrow that from artificial intelligence. Okay, so and so instead of doing an Instead of doing some evolution that here, where we replace, we have our gene circuit, and then we can replace promoters, etc. And then the system changes either by acquiring new regulations or losing regulations or changing parameters. And we have done explored that computationally many years ago. And we want to borrow something different. And this will. And this will just be somehow a little bit shocking for you because I'm going to talk about matchboxes. This is the idea of Donald Michi 60 years ago. But I came across that when I was playing with my daughter, I was just looking for some fun game to do together with her. And at that, she's, and we did and the She's and we did and the we built the boxes. These are match boxes that can learn to play tic-tac-toe. So the idea is you have a matchbox and then you have in the matchbox inside color beads. And the color beads are each matchbox have the move of the game, so the seat of the game. So here you have the player. The one the player X is here and here, and player over here and here, and then you have only these five possible positions. So the matchbox will contain only bits of these five possible colors. Okay, so you have all these nine possible colors in general. And then the idea is just you randomly pick a bead from the matchbox, and then you And then you and that the color will dictate in the position. So, if the color here is blue or color number two, let's put numbers to the colors, then it would be in a position two. And then that's the move of the matchbox. And a human can play here, for instance, and then etc. Okay, so and this is how they look, and then okay, so then don't I have. And then, okay, so then Donna, I have to say that Donald Michi, he constructed 304 matchboxes and then himself, and then he played with Agua the whole weekend. And he's a biologist, a geneticist, and he's one of the fathers of artificial intelligence. He was in the, together with Alan Turing, a good friend of him, in decoding the enigma, I mean, in the Second World War and the In a second world war, and the deciphering this in a German code. Okay, so then this is considered, I mean, it's one of the early works of reinforcement learning. So I thought myself, if a matchbox can play tic-tac-toe, then I can make a bacterial cell to play tic-tac-toe. Okay, so that's the idea. So here, how it's possible. And how it's possible that a matchbox, okay, here until now it will just play randomly. So, how we go beyond the random? The idea is just that each time that a match is finished, then if the matchboxes have uh they lost the game, then you remove the the a bit from each of the matchboxes involved in the play. Involved in the play. So here, if you sorry, if it places like that, then you remove 9, 7, 1, and 2. Okay. If it wants, then you add. So by that, then he could get an increasing success rate just by playing and doing this reinforcement through the human feedback. Okay. And so this has been also. So this has been also used by Martin Gardner, a famous mathematician and a writer. He had this column in Scientific American and then Mathematical Games. I'm very fan of that. And then he applied to a chess and three by three chess, six points, three and three. And then you have the boxes. And then here you have two possible moves here and here. Therefore, you have two possible. Here, therefore, you have two possible colors. But then it's the same idea. And so, how we do that with bacteria? So, the idea is that we are going to replace the matchbox by self-sensing a chemical pattern. Instead of having 300 matchboxes, we are going to have not 300 strains, but we are going to have a strain that will change its behavior according to the chemical pattern. Okay, so then it just adapts itself. It just adapts itself. And then instead of nine colors to dictate the moves, we are going to have nine cultures, replicates of the cultures. So here you see one, two, and nine cultures. These are the possible moves. For the beads, and we are going to have plasmids. And the more beads, the more... The more bits, the more the higher copy number of the plasmid. So then you have more molecular copies of the plasmid, then you have more bits. And you count the bits by measuring red fluorescence because we are going to encode a red fluorescent protein in the plasmid. But how do you remove beads? Because in cells, that would imply permanently imply permanently changing the copy number of a plasmid. That is not known. So we have seen that, I mean, for instance, you can change the copy number of a plasmid by making indusible an inducible replicon. You can put the RNA two promoter, for instance, in the COLI1, and then you replace it by the inducible promoter. And then you have a transition factor that activates that, then you can have an inducible promoter. The problem is that if you remove The problem is that if you remove the inducer, then automatically you lose the induced copy number. So how we get a set point of the copy number and that remains there. So for that, we go to the work of David Liu and Tank. Sorry for. Tank, sorry for the pronunciation, okay. At Harvard, David Liu was more a worried for base editors, but in his paper in 2018 in Science, they introduced something that already was known that plasmids that are of the same compatibility group, in fact, they are not so incompatible as the general wisdom says. Okay, so they Okay, so they could and you can get some stability. So they pushed that to the limit and said, Okay, what happens if we just make it exactly the same replication rate and same genetic load? Okay, if you make that, then by having same origin, same size, same antibiotic resistance, and same cellular burden, then they realize that the ratio of That the ratio of plasmids that are co-transformed, two plasmids cotransform the same origin, which is the same, the thing that you would say to a student, never do it. Then they could maintain the ratio more than after 50 replications, they had less than 1% deviation. So they use CRISPR to adjust the copy numbers. But as this introduces toxicity, we thought why we don't use an inducible. We don't use any inducible promoters and we use inducible antibiotic selection. Okay, so this is how we designed and inspired by this plasmid heteroplasmy and a system of plasmids that has the same origin, same size, and same genetic load. But you have one plasmid, it will have reflux. One plasmid, it will have red fluorescent protein, another green fluorescent protein. And we want to use these two pairs. And in fact, it's not RFP. I just put it like that. It's M cherry, and it's not GFP, it's EGFP. And then we use these two plasmids because we use ratiometric analysis to compute through fluorescence the ratio of plasmid. So and we add an antibiotic resistance and a polycystronic to the fluorescent protein. To the fluorescent protein and different to each plasmid, so kanamycin resistant in one and chloronphenico resistant in the other. Notice that the copy number of each plasmid, here is what P1 and P2, the copy number of P1, that's the what is non-trivial remains constant in average and population average, and the copy number of the P2 and D also. And D also remains constant in the population average. And the copy number of the addition remains constant at the signal cell level, not average. But also average, but also signal cell. So, and I'm saying this average or not average because when cell uh device segregates and then there is no antibody selection, therefore in this random and and and uh echi partition. And a key partition. And so then, with that, we noticed that we have inducible promoter encoding and the fluorescent protein, red fluorescent and green, and then you have the anti-electric resistance. So therefore, the anti-electric resistance is conditional to the inducer. And we can, and if you switch off the inducer, If you switch off the inducer, then you have memory and then you have stability in liquid medium or in solid medium. So I'm not going to go into details, but then as I said, it's okay, very good. How we apply this to the matchboxes. So as I said, we need a cell. A cell that is specifically associated to the chemical pattern of a game state. So we are going to just only record the game state of the opponent player. That's an approximation, which is not that bad. But a bigger approximation is the problem that we are going to use this logic of that we are going to use co-cultures of bacteria. Of bacteria, and then if one gets activated, then you get an fluorescence. If any gets activated, you get fluorescence. If all get activated, you get a little bit higher, you get higher fluorescence. This is opposed to the perfect, the ideal behavior will be an and gate, like if you only, only you will have fluorescence, if only induce. only you will have fluorescence if all inducers are at high concentration, but also all the other ones that are not there, then you have your results. I mean, so not if you have positive one, two, three, then you want only one, two, three, but not when four also is present. Okay, so and then this is a brutal approximation. The approximation, but through simulations, we have shown that still you can get convergence. So it's not, I mean, so in fact, it has a logic, but I'm not going to enter to that. And so then you are going to, we are going to have then nine types of cells, each one sensing one position. And then we represent it like that. Remember that all this co-culture here and this co-culture here. This co-culture here and this co-culture here are the replicas for the decisions. Okay, very good. So, as I said, the ref influorescence is going to be proportional to the amount of copies of the P1 plus V, or where we define weight as the fraction, so A divided by the copy number, A plus B. And when and we are going to have a number of We are going to see that when we add canamycin, then we are going to do the following. We add sublethal concentration of canamycin, and then this changes the burden. We said that the plasmid have exactly the same burden in the cell burden. Yes. Then the burden may change if the Change if the plasmids are active. Otherwise, it doesn't change. So if they are active, then the burden changes. And the reason is very obvious because one has antioactive resistance and the other not. Therefore, the cells with more antioactive resistance will grow faster. Okay, so we have done some single cell analysis of that, but I don't have time to show that. Okay, thank you. Okay, thank you. Okay, so then this is a little bit about the stability of the library, but I'm not going to enter into that. So I have already said that. So here we have done all these learnings. And I mean, with antibiotics, in this case, it's canamycin. We have also done with other ones, we come with QPCR to check, we did other sequencing, different types of measurements, alternative. Alternative. Okay. So we can change at the same time several by adding several inducers, we can change several copy numbers of the plasmid. So we can adapt several strains at the same time. So then when we play a game here, just the idea is just we have replicas for all the cells, as I said, but for all the equultures. And then Equacultures, and then we grow with the antibiotic. We activate only the position of the opponents. And so, in this case, we are growing all the ecocultures with OC6, okay, because this is what is mapped to the central positions of the opponent. Sorry. And then we look for the highest fluorescence. This is equivalent to a winner-they call an approach. And then, in this case, it's this. And then in this case, it's this eco-culture the highest, therefore that it would be our choice for the bacteria. Then repeat again, but this time the human place here, for instance, therefore, we just have all these possibilities. So these are all these co-cultures, but we are going to induce now with the position for OC6, this is this position, and the position for Arabinos in this position. Okay, and then In this position. Okay. And then we do the same thing, and then etc. Bacteria loses. And then what we do is reinforcement learning. And what we do is we revisit the positions that are the culprit for the moves, this one and this one. And then we add kanamycin together with the inducers that were incubated. Okay. And then that will decrease their weights. Therefore, they will avoid choosing. They will avoid choosing that decision at the same time, okay? So, then you, in fact, you have a decision tree, okay. And then, what we are doing is we are pruning out the bad decisions, okay? So, if you do that, you have an adaptive process that it gets to from random behavior where you have all co-cultures with the same weights until expert player where you have adapted the system to be unbeatable at the game. And beatable at the gate in this restricted T. We use a restricted T3. And then we have some controls, but we are not going to tell. Okay, so to conclude, I mean, what I'm saying we have adapted this analog memory by TVU camera he called it. But instead of using a CRISPR, we use an in. we use an inducible and chemically chemically induced antibiotic selection and we use an iterative method i mean this is just learning by trial and error and planning the bad branches so you need stochasticity that but cells are stochastic so you repeat the same experiment and they will give just different results which is a good thing and we are shifting the the design information The design information from the encoding that in DNA to a post-reboot. So after you reboot the cell and then it's alive, and then there is where it is going to incorporate the knowledge, okay, afterwards, like a human brain or mammalian brain. And we don't have to set up specific memory levels. The self finds the Finds the memory levels. So if we think about the neural networks, then we don't do back propagation to compute what are the weights for the optimal behavior. No, the cells do back propagation. The cells do the training. And then this is scalable to other algorithms. We implemented also Exapon. You can see our preprinting by archive two years ago. years ago and it has limits yeah it's doing bacteria doing road in a learning and and it cannot generalize from past experiences as in neural networks you can in deep neural networks you can just generalize and beyond the training set which is a very important thing but okay this is a first step okay so very good so again the we start with the acknowledgement Start with the acknowledgement, and I already did it. So, I acknowledge my group, the funding, and my good collaborators, and all of you for your attention. Thank you very much. Hello? Yeah. We have time for one quick question, and if we could have Marcella start to share her screen, that'd be great. Marcella is our next speaker. Any questions for all? Any questions for Alfonso? I have one question, Alfonso. It was on the when you were talking about the encoding for what how do you encode the specific moves that is represented in a TikTok token for like for the self to see. For the cells to see. Because you obviously are able to draw out exactly what you want, but it seemed like you had to communicate with HSL, a host homocerian lactone, to a specific strain to encode a specific position. And then the last, the second piece of that was that how do you, when you say you were using reinforcement learning, what exactly happened there? So. Okay, so thank you for the question. So that's the core of the whole algorithm. And the idea. Uh, but and the idea is that you encode in how the cells see the real world, they see through the chemical inducers, and they you encode in each position of the opponent by a different chemical inducer. So, if the human opponent is in position five, then you are going to use inducer five, and so you culture all cells with inducer five, okay? So, inducer. 5. Okay, so in user 5, it happens to be OC6, but we have nine marinate inducers. And so if the birth state consists in human in position four and position five, then you are going to induce the cells, all the cells, with inducer four and five. Okay, so that's the way the cells know. The cells know what the human is doing. So, and then when you do reinforcer learning, then when what happens is that because you only take four and five to make a decision, you look the fluorescence. So, you are going to look the fluorescence for the plasmids that are encoding promoter four and five. So, then only these two plasmids matter for the fluorescence. Matter for the fluorescence. So, the kinamycin, what it's going to do is it's going to decrease the amount of the copy number of those plasmids, only plasmid four and five, because these are the only ones that are active. And then that will decrease. So, that's how, and because we are looking always for the maximum fluorescence, it's in a winery approach, then now. Approach, then now it's the chances are lower that this is going to be the highest fluorescence. Therefore, and next time it encounters the same movement and the same inducer four and five, now because the number of plasmids four and five have decreased, then the total fluorescence is going to be lower, and therefore the chances maybe other positions will have higher fluorescence. I don't know if I am clear. For instance, I don't know if I am clear enough. That makes sense. Thank you very much, Alfa, for those very detailed. Okay, thank you to you. Okay, so quickly introduce Marcella here. Marcella is a dear colleague of mine at Santa Cruz, and we're so grateful that she's able to present today. Marcela, I'm just going to quickly turn off this recording and then turn it back on again.