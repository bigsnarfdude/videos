Thank you, Johan. So hi everyone. Welcome to my talk. I'm very glad to be here. And this is just a really beautiful place. So today I'd like to talk to you about our new results of fully heterogeneous weakly coupled work of decision processes. And I'm going to illustrate an attempt at connecting this problem to a matching problem. Okay, but if that example feels forced to you, Example feels forced to you, at least we can resort to the mathematical approach, and the connection there is more natural. And the most interesting thing we did in this work is that we can handle fully heterogeneous settings. And this is a setting where more traditional methods tend to fail. Okay, so this is a joint work with an undergraduate student at Tsinghua University, Shang Chung, who visited this EU during the summer, and my PhD. Summer and my PhD student Uganda SEMU. Okay, so here's the plan. Okay, here's the plan of the talk. I'm going to first introduce the setting and an overview of existing results as well as other results. And then I'm going to review the approaches for the homogeneous setting. And that will set us up for the heterogeneous setting. And then in the end, I'll tell you our. Tell you other techniques. Okay? So let's get started with the second problems. They're also called the n arms. And this is like a term inherited from residence banded problems. So those are the n arms. And each arm itself is associated with the MDP. And so this MDP, let's say arm i, it has a state space, action space, both are assumed to be finite and relative. And relatively small. Okay, and then we have a set of transition probabilities if you are in state S given action A, what's the probability of transitioning to a new state S prime? And notice that here, oh here, this is the laser. Okay, this depends on I. So for each arm, it has like across different arms, they could have distinct sets of transition probabilities. So this is one of the So this is one of the heterogeneity in the setting. And then for this model, if you were given the actions, then the state transitions across different arms are independent from each other. And then the reward here, we have this reward function. It's also like it could be different for each arm I. It depends on the current state, current action, and there are certain policy pipe. And we submit. And we sum it across all the arms. We normalize it by the number of arms. So, this is a reward per arm. And we are looking at the long-lived average reward. Okay, that's the criteria we are going to use. Okay, so far, I haven't talked about how the arms are correlated with each other. And here's how they are coupled together. So, we have a set of constraints. We can think of those as resource constraints. So, basically, we have cash. Um so basically we have capital K type of costs and uh for each type like small k here, each arm has a different cost function indexed by i that's a function of the current state action and we want the total cost to not exceed the certain budget. And this budget is in the form of some fractional number timed by the total number of arcs. Okay? Okay, so is it so then the goal is to find a policy to The goal is to find a policy to maximize the average report. So, is the setting clear to everyone? Okay, so then, like, however, if you want to just, so I want to emphasize that this is a planning setting. There's no learning here. All the parameters have gone. Okay, so it's just a computational problem. However, if you want to do this directly, this is pretty hard to do if the number of arcs is large and you can show there like it has been shown to be hard. Like it has been shown to be hard in very strong sense. So, what we are aiming for is not to find the exactly optimal policy, but rather we want to efficiently compute a policy pipe that's asymptotically optimal when the number of arms is large. So, then idea, so this is the optimal reward, this is the reward under policy pi, and we want the gap to go to zero as n goes to infinity. This is referred to as the optimal. Infinity. This is referred to as the automatic gap. And typically, this like Rn star is on a constant order. So if you think about the approximation, it's like small O of one, one minus small O of one approximation. Okay? Okay, cool. So this is our goal. And now here's my obligatory matching slide. So I'm going to connect this to a write-sharing football. This is just my attempt at This is just my attempt at connecting this. It's actually not in our paper. And as you can see, there could be other better ways to model this problem. So let's think about this. So you want to run the right sharing service in the Benford area. And what you can do is you can divide the map into several areas. And then there will be cars like it in different areas. And here, I want to model each car as an arm. As an arm, okay? And I'm going to end up with a model that's similar to a model studied before. So if each car is an arm, let's look at power I. The state could be idling in certain region, or it could be en route, sorry, it could be en route from certain region to another region, or you can do it even more in a more refined way, it could be en route with a customer or without a customer, like different kinds of things. Okay, so this is a state of issue. So, this is the state of each car, and the action could be: you could keep the car idling, you could give the car a customer, or like if you are in a setting where you can kind of tell the car what to do, then you can kind of even route a car, an ad card from one region to another region. And you want to do this to maximize your reward, which is the total reward for serving customers. And then the constraint, the main constraint here is if you count the number of cars, If you count the number of cars taking customers in a certain region, then that shouldn't exceed the number of customers newly arrived to that region. So in our model, we kind of need this to be just a fixed number. And a better model would be like this would be a random number. I think we can extend our setting to that setting, but I haven't tried yet. So if this is a fixed number, then this maps exactly to a Wiki couple the MDP. Couple of MDP, and hopefully, that shows the connection to matching problems. Okay, and I think the benefit of this model is that you can kind of model the state of the car. So, this is like it's not like once it serves the customer, the car disappears, but the car actually is going to move to another region because of the customer. I think that's the benefit of this problem. Okay, then, so yeah, that's an example. So, now I'm going to review existing work. Now, I'm going to review existing work and the other results. So, I'm going to roughly divide things into four regions. So, on the x-axis, on the left-hand side, this is called the residence banded setting, which is the special case of weakly coupled MDVs. So, in this setting, each arm, the action space is binary. So, it's either the passive action or the active action. And there's only one constraint which says the total number of active actions you apply cannot. Actions you apply cannot is equal to a fraction, a certain fraction of the total number of R's. Okay, so this is a special case, but it's widely studied, and that is the more general end, and on the y-axis, this is the homogeneous end, which means each sub-problem has the same set of parameters, and that is heterogeneous, and like different arms have different parameters. And as you can imagine, most work, like fast work, happens here, it's homogeneous. Here is homogeneous residness than this. This includes our own work, and basically, you can prove different kinds of automatic gap results with different policies under different kinds of conditions. Okay, so that's like a lot of work there. And then if we are still in the homogeneous setting, but move to this end. And then there's a paper here. Um it's not fully general because the constraint here, there's only one resource constraint of a special form. Constraint of a special form. And so there, like then you can show like small, you can divide the policy asymptotically out of which basically means small overall like that. Okay? Okay, so now let's move to the heterogeneous case. And somehow when people move to the heterogeneous case, they don't want to study residue spending anymore. They all move to... Oh, sorry, I forgot that. Okay. This is important. Sorry, I forgot this. This is a more general setting. This is a more general setting, and there indeed you can show like optimal asymptotic optimality. So now let's move to the heterogeneous case. And suddenly, people all look at weakly coupled MDPs. So in this paper, it's even a special, special case of, it only has one even special, more special form of constraint there. Questions? Again, that's what I'm going to say. Okay, so for this paper, it's in this setting that we call typed heterogeneous, which means like you divide the arms into a fixed number of types, and when you increase the number of arms, you increase each type proportionally. That's what's in this paper. And then another paper with a slightly more general causal constraint, also like it's the type heterogeneousity. And the known paper. setting. And no paper, no results are for the fully heterogeneous setting. There is a line of work there with fully heterogeneous arms, but it's for finite horizon reward and their automatic gap depends on the time horizon in a superlinear way. Yeah. But this, I mean, are this a fully hypergenerous statement? Did you if you were aware? Sorry. If you're giving asymmetric results. Asymptotic was not a symptom. I'll talk about that. So basically, when we increase N, we have certain requirements on each arm. And we assume, for example, the cost functions should be bounded, and the reward functions should be bounded. And we also have another requirement that I'm going to show a little bit. Okay. And yeah, so that's the finite horizon setting, which doesn't generalize. Generalized. And we are our resources here for the heterogeneous and general cost constraints. And we can show a one-off square root of an automatic end. So yeah, that's an overview here. And now I'm going to review approaches for the homogeneous setting. So here we want to look for either limitations of the past techniques or like Techniques or like ways to generalize those techniques and hints for us to find ways to do it with hesitation. So, the most common approach is the following. So, this is based on a fluid or mean field type of approach. So, basically, what you do is you can first represent the state differently. You don't need to look at the state of each arm. When the arms are homogeneous, Each arm. When the arms are homogeneous, you can just count how many arms are in a certain state S, and you can normalize it by S so you get a fraction of arms that are in a certain state. And you can also do this, which is the fraction of arms that are in state S and the take action A. So you can describe those quantities, and when you take the steady state expectations, they become those small case low c. Those small case lowercase x and y, and then you can look at the LP relaxation of the problem. So, we have seen LP relaxation over and over again yesterday. So, this is a kind of the mathematical connection here. So, here, if you look at here, this is like an average constraint. In the original problem, it's a hard constraint. So, here is like an average constraint. And you know that under any policy, as long as it has a steady state or a time average, then this should steady. Average, then this should satisfy this constraint. So if you then maximize the reward here, this is going to give you an acrebank on the original problem. Okay? Okay, so then like what you do is, sorry, let's call this like the optimal solution y star, and then you can also do the optimal state distribution here. Okay? Okay, so then what you do is you do the mean field analysis and version policy. So once you have a policy, you can prove that this state is. Prove that this state description is going to converge to x small x sub t of s, where a small x sub t is the solution to a difference equation, a system of difference equations. Here's discrete time. If this continuous time is OTs, okay? And then this difference equation has equilibrium x star, which is the optimal solution of the L P. Okay, so then like once you can show those. Once you can show those, and you also need to argue for interchanging the two limits. But once you're able to do that, then you basically show that in steady state, your policy, the state distribution under your policy converges to this, which there's a little bit more to it, but basically this is like asymptotic, how you show asymptotic optimality. So, this is a very common approach, but now if we think about the heterogeneous settings, Of heterogeneous settings, then this can actually handle types of heterogeneity because there the intuition is the insight is similar because for each type of arms, you still have an increasing number of arms there. So then you can kind of do this in a straightforward way. However, if you want to do a fully heterogeneous setting, then it's a little bit unclear what you should do. You definitely shouldn't aggregate the state into this kind of representation. Into this kind of representation. Okay, so that's this approach. And then, yeah, question. In the previous slide, I'm a little bit confused about: are you fixing a policy pie or? Yeah, fixing a policy pie. Once you fix a policy pie, this is how you show the asymptotic optimality of it. Oh, so you start with a good enough policy and then use this method. But of course, the policy design is informed by the LP relaxation process. Yeah. So. Yeah. So you you lagrange those class trains and then the problem decouples I think with Markov strain? Sorry, the thing is. So you uh lagrange those class strains and therefore the problem decouples? Yeah, yeah, that's that's one way to think about it. Yeah, yeah. Okay. Okay, and then like we have, there's another approach for this problem. This is from our own paper, and this is only for the special case of red. This is only for the special case of residence bandit. But let's talk about this. This is what the upper heterogeneous work is built upon. So for this, we still think about the LP relaxation. Here it's simpler because we only have two actions, active and passive, and the constraint is just the number of active actions. Okay, so then we have the optimal solution here. Okay, so now instead of thinking of just this LP, what we are doing is the L. Of just this LT, what we are going to do is we are going to actually think of it as a single-armed LTP, which is just equivalent to this. So we optimize the reward of one single arm subject to this average constraint. So then if we think about that, then for that single arm MDP, we can do the optimal single arm the policy, which directly comes from the optimal solution here. If you just take this statue. Take this stage and frequency and do that. Okay, so that's our optimal single-armed policy. Then we are going to make use of it in the following way. Okay, so what we are going to do is we are going to let each arm first generate an action according to this optimal arm policy. And we call those ideal actions. And for example, it could be something like this. However, this is generated according to that each arm generated its action. Like each arm generated its action separately. So, if you look at them all together, they may not satisfy the hard budget constraint. Let's say the budget constraint is half here, but the ideal actions are like this, but you're only allowed to activate two of the arms. So, then what do we do is we are going to follow those ideal actions as much as we can, prioritizing smaller IDs. So, the ID here is nothing but just like this number one, two, three, four. Just like this number one, two, three, four. Okay, those are the IDs. So, what we are going to do is we are going to start from arm one, we follow the ideal action, and we still have budget left. So, we move on to arm two, we follow the ideal action. Now, we use the up the budget, so then for the rest of the arms, we're going to just use the passive action. Okay, so this is our ID policy, and we can show that it has an automatic gap of whatever squared. Okay, okay, okay, so then. Okay, so then like I'm going to give, I think I need to go a little bit faster. So this analysis for this policy is also based on the alpha fluid analysis. But I'm going to actually repeat some of the elements here later. So I'm going to just skip it for now. But basically, but I want to say one thing. So the analysis is still based on this quantity. This D is a subset of arms. So the X T subsets. arms. So the x t sub d is the empirical distribution of the states of arms in this set, and we are comparing that with the optimal distribution basically. So it's still kind of empirical distribution based. So if we want to do the heterogeneous setting, then again it doesn't directly generalize because now if the arms are different, there's no point of calculating the empirical distribution. Okay, okay, so then let me actually be a little bit faster, let me move to the heterogeneous setting. Okay, so actually, the policy we ended up using is a very straightforward generalization of the IP policy. So let's look at, let's rethink the first of the LP realization here. Now, like instead of one set of just one YSA, we have Y sub ISA. So each I is for each R. Okay? And then we can still do the LP relaxing. And then we can still do the LP relaxation here. Now, this LP actually depends on n, but it doesn't matter. It's still like we have a linear n number of optimization variables here. It's solvable still in polynomial time. And now, like, when we get the optimal solution, we have a set of y star essay for each arm. And we can still prove this upper bound. Okay? And then, like, instead of one optimal single arm policy, now we have I of N of them. We have n of them, one for each one. And then, like, we basically do the same thing. There's a small modification to it. I don't want to get into it. It's necessary, but it's not the most interesting part of the algorithm. But we do need to shuffle the IDs a little bit. But after that, it's basically the same. We sample the ideal actions based on their own optimal single arm policy. And then, like, we try to follow them as much as possible until we use. As much as possible until we use up some budget, we have paid budgets, we have paid constraints. As long as we hit one of them, we stop. So, this is like, so for example, we can follow the ideal actions here, but now that we use up a certain budget, then I'm going to, yeah, I think I forgot to mention this. This is actually kind of important. We assume there's an action that doesn't consume any cost under any state. So, otherwise, it may not be always feasible, actually. Okay, so then we use the low-cost action. Okay? So then we use the no-cost action there. Okay? So, but if you have two constraints. As long as we hit one of them, we set the rest of the arms related to the no-cost. No cost is no cost for all kinds of costs. Okay, so yeah, so this is our policy. And here comes the other important assumption I mentioned before. So we need to make this assumption when we increase N. So Pi is the transition probability. So Pi is the transition probability matrix induced by the optimal single out of the policy. Okay, so because once you fix the policy, it's just a Markov chain. So this is the transition probability matrix. And we assume it's the APRL data unit chain for all the I. So I is like all the Ether numbers yummed in. So then another assumption is this is the second largest eigenvalue in absolute value. So this is the absolute spectrum. The absolute spectral gap, we need to assume that this is the lower bounded for all the body. So, as long as this is satisfied, we can increase it now. So, Shrikan, do you have more questions about the tweaking? Okay, okay, so this is the assumption, and under this assumption, we show that we have one over square root of n automatically n. Okay? Um so for the homogeneous case, the second one is redundant, like as well, if you have the first one or five. Okay, so I think I don't have much time left, but I do want to show you our the optimum function. So, but it makes sense to first overview the approach a little bit. So, very briefly, this approach is like we want to construct a Lyapunov function V or a potential function V that satisfies two conditions. The first one looks a little bit complicated, but it's just saying like the automatic gap should. Saying, like, the automatic gap should be after bounded by the expectation of the Lyapunov function in steady state in a certain way. Okay? And the second one is a more tricky one. It says we should have a negative drift. This is state at t and this state at t plus one, so we should have this negative drift condition. And we all allow like some approximation there because we're only aiming for one over square root of the and altogether you get. And then, like, for example, if you look at condition two, if S T is a Condition 2. If ST is already in steady state, then you can take the expectation on both sides, then this becomes zero. Then, what you have is this thing, and then you can show that this is the O of N or square root of n order. And then you can plug this back into this upper bound, and then you will get the target order there. Okay? Okay, so then everything boils down to finding the React Month function. And as I said, so previously our React Month function is based on quantities like this. Is based on quantities like this. This is like an empirical distribution of arms in the set V. M of D is the size of V. Sorry, I didn't have that M of D. So this is the size of D. So this is still comparing the empirical distribution to the optimal, the ideal distribution. But in the heterogeneous case, we still want to capture this distance to optimality, but we cannot use this thing. So I want to just mention a little bit in case we weren't. Just mention a little bit in case we weren't confused. The ultimate Lyapunov function V reconstructed is not just this function, okay? It's more complicated than that, but this is like the core part, the core ingredient in the Lyapunov function. Okay, so then in heterogeneous setting, this is what we did. So first, we still need to redo the representation of each arm i. So we use this indicator variable. Use this indicator variable which says, okay, if you are in a certain state or not, and then we stack them. Then, this basically for each arm at each time t, this is just a row vector. That's like just one halt vector. It's one at the current state. Okay? Okay, so then this is the key part. Okay, this is a generalization of the previous H function. So let me explain it. Okay, so first, this is the one-halt vector. One halt vector. We can think of it as kind of the initial distribution, that's a point mass, okay, for this arm, the initial distribution of the state. And then we subtract the ideal state distribution, and then we time it by Pi to the L, which means like what is the gap after L time step transitions. Okay, and then we need to do a little bit of tricky here. We divide this by gamma. Divide this by gamma to the L, but the gamma is something like larger than the upper bound on the second largest eigenvalue. Okay, so then this is still, this is should, this whole thing should still go to zero when L goes to infinity. Okay, so yeah, so this basically measures how far away this state is from the optimal distribution, but we kind of look ahead for L steps and look at the difference there. And then like we are going. And then, like, we are going to take a soup over all the time steps. You can also do the song, it should also work. And then, like, what we are going to do is we are going to look at this difference, but we are going to project it onto a direction that's related to the reward or the costs. So, we are not just looking at the distributional difference, but rather this set of GEIs comes from the optimal reward vectors or like the cost vectors. So, basically, this whole inner. So basically, this whole inner product measures the distance either in reward or in cost. Okay? So, yeah, so this is the most interesting thing in the paper. I actually have one more slide for some derivations, but I think I'm going to, and there's a little bit more to it when we construct the full diaphanov function. I'm going to skip all of that. If you're interested, I'm going to show you it's actually very very brief very uh simple. Very simple. Okay. Okay, but let me conclude. So here's a summary. Basically, we considered the fully heterogeneous setting and we designed a policy that's as we thought we optimal and we developed this new optimal function that handles this full heterogeneity. Okay, thank you everyone. I guess let's leave the question. Let's leave the question offline. Just before we take a break, just a couple of quick announcements. So, first, for the later speakers, if you don't talk about matching, that's really okay. But anyway, we embrace diversity and differences. So, it's completely embraced. And also, in terms of