Thank you for the introduction and I really want to thank all the organizers for inviting me. Today I'm going to talk about some machine learning problem while data exhibit low-dimensional structures. Okay, so my work, the presentation today is based on the students and postdoc work. Ming Chu is co-advised by me and Professor Tuo Zhao, and he is not a now a postdoc and principal. Now, postdoc and Princeton. Viraj and Alex are third-year and second-year students, and Hao is my former postdoc. And Professor Tojao is my collaborator. First, I will give you an introduction. So, I started when I was a PhD, I was working on input problems. Then I started machine learning when I was a postdoc, and I realized that many of the classical problems in machine learning are actually English problems. Learning are actually inverse problems. So, for example, if you look at the regression, then we are giving a pair of points x, i, y, i, then we want to fit a function, feed those points with a function. So there is a statistical component while the data are noisy, so we don't want to overfeed the data. Then we can, sorry, we can do linear regression to feed the data like this. Okay, so the question is, what is the underlying function when we are giving pair? function when we are giving pair of points. And another important problem is distribution estimation. So if we are giving high-dimensional points and then suppose the high-dimensional points are located on a low-dimensional surface and we want to know what is the underlying distribution. So unfortunately, most of the data nowadays are high-dimensional. If you look at the images, they are very high resolution. We have a lot of pixels. Resolution, we have a lot of pixels. So we are going to encounter the well-known curves of dimensionality. What it is, I will illustrate with this simple example, where we take the d-dimensional unit cube and uniformly put points in this unit cube. And then for one of the point x, I look for its nearest neighbor and compute the nearest neighbor distance. Of course, this is random. Of course, this is random. You can take the expectation, and then with some calculation, you can show that the nearest neighbor distance is lower bounded by 1 over n to the 1 over d. Well, n is number of samples and d is the dimension. So when in low dimension d is equal to 1, the distance decays pretty fast, like 1 over n. Unfortunately, in 20 dimensions, when I increase the points to 100 to 100,000, so the distance only Thousand, so the distance only decays to 0.26. So points are very far in high dimensions. If we look at images, even for this low resolution image, 64 by 64 is gray scale, so we put it in an intensity matrix, vectorize it, it's about 4,000 dimensions. It's pretty high dimension, even for this low-resolution image. But with the idea of manifold lane, if you Of manifold laying, if you look at the data set containing the same study taken from different left and right poles and up-down poles and right-hand direction, so every data point can be parametrized by three parameters. Mathematically, you can model them as three-dimensional manifold embedded in about 4,000-dimensional space. So, we have the manifold model to describe the low-dimensional structure. The low-dimensional structure. And then, in reality, if we look at the popular benchmark data set in machine learning, they actually have a low-dimensional structure. So, this is the paper by Tom Goldstein in iClear. So, they estimate the intrinsic dimension of those data sets using maximum likelihood. You can see that MNIST is about 10. Even for the very complicated image net, the intrinsic dimension is about 40. Is about 40. So we can see that even the very complicated images has low-dimensional structure. It's because there are a lot of rotation and the translation of repetitive patterns. Okay, so today I'm going to talk about neural network. Well, you know that it has been very popular and Hao Ming gave a nice introduction about this feed-for-world neural network. Well, we put an input vector and pass it through. Vector and passes through multiple layers of linear transformation followed by random network, and then we get an output function fx. Okay, so for neural network, it has been working very well, and we want to understand why it works well. Of course, it's a very big topic, and then many people work on approximation theory, and there are a lot of computational technique. And also, I think people usually agree that neural networks. People usually agree that neural networks work very well because it can capture the low-dimensional structure. So, we want to understand how it captures and why it captures the low-dimensional structure. As an example, you can see we first verify that neural network performance is independent of the ambient dimension. So, Ming Chu took the SAFAR-10 data set and he used the VGG19 and the Rasinet 110 to train the Long zero to train the images. So the images are original 32 by 32 resolution. So he increased the images to 64 and up to 256 resolution. So basically, we train it with different resolution, but it's the same essential same images. You can see that with different resolution, the test error is basically the same. So neural network performance. So neural metal performance should be independent of the ambient dimension, more dependent on the intrinsic dimension. So we are going to visit two problems. The first one is regression. Well, you have seen this formula in many of the talks. Well, we have y is equal to f naught, x plus noise. And our goal is to estimate the underlying function f naught giving the n pair of points x i y. Pair of points xiyi. Of course, we need some assumptions on the function. Usually we assume the function to be a holder function indexed by s. So s is the smoothness parameter where you can think the function has s order of derivative. So for this holder function, the classical literature about non-parametric metric estimation tells us that if we have a holder function If we have a holder function in high dimension, then we look at the min-max rate. So we look at the best estimator I've had, infinum over all possible estimator, and then the worst function in this class, then the min-max rate is in the order of n to the negative 2s over 2s plus d. You clearly see that in high dimension, the regression problem has the curves of the The regression problem has the curves of dimensionality, and there is no way to improve it because there is an infinite number over all possible estimators. Okay? So if we apply for image net, then you can clearly see that the theory predicts a huge number of samples required, but in reality, we only have 1 million images. So what can we do? Then we are going to utilize the low-dimensional. To utilize the low-dimensional structure, right? So, for example, you can think x as images, as an image, and f is the probability for classifying different classes. Okay, so then to utilize, to model the low-dimensional structure, we assume that xi is randomly sampled on a little d-dimensional manifold. Then now our function is now supported on a low-dimensional manifold. We assume it's a holder function. We assume is a holder function on the manifold. Okay, so when we run the neural network for regression, what do we do? We set up a neural network architecture specifying the depth of the network and the width of the network. So we are going to train our function f in the neural network class to minimize this empirical loss. Okay, so computation is a bigger problem in Problem in optimization because it's non-convex. So for us, we assume that we can get the global minimum F hat and we want to know the property of F hat. So what do we do? As we said, we want to specify the neural network architecture. So we want to specify the depth, how many layers we have, and then the width of the neural network little P, and then also. And also, the weight parameter is upper bounded by a constant. There are sparsity constraints. So, we can remove this in our recent work. So, now the question is, when we are giving n pairs of samples, how should we pick the parameters and then make sure that we can guarantee a generalization or estimation error? Okay, so before talking about the theorem, the main result, we need some assumptions. The main result, we need some assumptions. I think our assumptions are made: we have a Riemannian manifold embedded in the high-dimensional space. The manifold is compact, and then the function is a holder function on the manifold. So, with those assumptions, so we proved that if we set up a network architecture such that the depth of the network is log n, well, n is log n, well n is a number of samples, and then the width is a polynomial in the sample size little n and there is a sparsity constraint and so the function output is upper bounded by some constant and then the weight parameter is in the order of a constant so you can check it's all specified in our paper it depends on the curvature like the reach of the manifold the Like the reach of the manifold, the bound of the points. So we specify all the bounds clearly. So the point is that if we train the network to get a global minimizer, we can show that the general mean squared error, which is also the squared generalization error, such that for the f hat, I evaluate at an arbitrary new test point. So notice that x is not a training point, it's an out-of-sample test point. And we look at Test the point, and we look at the L2 arrow of the function difference between f hat and f. So this quantity is random, right? So because it depends on the training samples, we take the expectation with respect to the training samples. This is called the mean squared error. It's also a generalization error because we evaluate x at a new test point. So we can show that this mean squared error is offered. arrow is upper bounded by some constant and n to the negative 2s over 2s plus d. So the point is that the rate actually depends on the intrinsic dimension. Okay, so you do have if you look up clearly, so this term is actually the min-max error for regression. We have an actual log factor. Log factor. So the log factor can probably improve if we improve our proof technique, but we believe that this rather term cannot be improved. It's the min-max error. So and then we also clearly specify all the constant depends on the ratio of the manifold and also the surface area of the manifold. So this is our regression result. Then it shows that for neural network, if we have data on the load mesh. If we have data on the low-dimensional structure, then the network size and the error performance should depend on the intrinsic dimension independent of the ambient dimension. So the ambient dimension does appear in the constant, but not in the rate. So with the assumptions on the network architecture and also the parameters, how do you know that you can approximate the data that you want? Data that you want. Why do you reinforce in a network to be I mean to be essentially approximation of something very smooth? Yes. Like, is there like a consistent check? So for this theorem, okay, so the parameters are specified here. So theoretically speaking, we should set up the network parameter according to these equations, right? Right. It does have some guidance about how we should set up. Guidance about how we should set up them. But there is a constant in the water, right? If that's your question. Yeah, you also have some assumptions on the size of the parameters. In a sense, that I was thinking of maybe that limits what type of thing you can approximate, right? Because you start with something more general and then you say my network has to satisfy those conditions. Satisfy those conditions? It's a very good question. So, I think for the constraint about the upper bound of the parameter, it's kind of natural because when people train the neural network, they usually do this weight mobilization. So it's somehow enforced in the training. Maybe that space is sufficiently large still so that exactly. So the space is definitely large enough for our Definitely large enough for our approximation theory. And we need this upper bound because, besides the approximation theory, there is a statistical error where we compute the covering number of the network. So, you know, the covering number depends on the value. Yeah, good. Thank you. Thank you. Okay, so and then after you say this, you might say, oh, we work on the V4 network, then maybe there are Maybe there are many other interesting network structures. So later, we also considered convolutional residual network. So well, we have residual blocks. Well, then each of the residual block is a convolutional network. So in the conclusion network, we don't have the max pooling. Max pooling is really hard to analyze, but without the But without the maximal pooling function, we can prove a similar approximation theory. And then we also have regression and classification. So in the classification problem, so the function we want to predict is the probability that an image belongs to a certain class. So we can also use network to proxy to To predict that probability, and we get some errors, also error bound depending on the intrinsic dimension. Okay, so next I'm going to talk about the distribution estimation. Well, so we have been like saying from we have seen from Tuesday's talk that generative adversity. That generative adoption network can help us to generate images from some random sample. So, for example, this is a uniform sample in some unit cube. Then we want to push forward this simple uniform distribution, and hopefully, we can generate images. Okay, so this is given by the generator. So this is given by the generative adversarial network. Well, specifically, it follows the two-player game framework as introduced by Homi. So we are giving n samples denoted by xi when i is from 1 to n. And our goal is to design a generator which pushes forward some easy-to-sample distribution to this data distribution. So, for example, for the So, for example, for the digits here, and then we denote the easy-to-sample distribution by row is usually a uniform distribution on some unit cube. So there are two network architecture. There is a generator which pushes forward the random noise to some images. Of course, if we just use an arbitrary generator, then the fake image is not going to look like the real image. Real image. So there is a discriminator like the judge who tells you, who tells us whether the fake image looks like the real image. So if the judge says that for the training sample, the fake sample does not look like the real sample, then we should update the generator. So that is the idea. So finally, we are going to get a generator network which pushes forward random distribution, random move. When the moise to some and some data distribution. Okay, so fortunately, actually, GANs have a very solid mathematical relation. So if we look at the integral probability metric, which is used to measure the distance between two probability distributions, so let the mu and mu be two probability distribution. distribution, then we take the superimum of the evaluation of fx when x is sampled from mu and nu and we look at their difference. So this f is like the discriminator or the judge tells us whether the two distributions are the same. And then this is called the IPM. And in particular, if we take the function class to be one Lipschitz function, we do get Lipschitz function, we do get the Watson Wong distance. So for the we look at the particular type of GAN, what's the stand GAN? Well, we are giving n samples xi, we formulate the empirical distribution, and then we are going to solve this, like the games will solve this min-max problem. Well, you find the generator which Generator which minimizes the inner function. Well, the discriminator is going to be maximized over the difference for the push-forward distribution and the empirical distribution of the data. So essentially, the generator G theta is going to generate some distribution such that the generated distribution and the Generated distribution and the empirical samples are closed under the IPM. Well, the discriminator is the network, is a network class. Suppose we get the global minimum, then you can see that now our data estimation is mu hat pushed forward by this generator network. So, with this framework, With this framework, so we want to understand it from the non-parametric perspective estimation viewpoint, saying that if we have a class of distribution, if we have a given distribution, can we make sure that the generator will be powerful enough to generate whatever distribution I want to have? And then, of course, the discriminator is easier in the Easier in the sense that if I want to use the wall system one distance, then I just want to make sure that the discriminator network can approximate the Lipschitz-Wong function. And also giving samples, can we say something about the statistical estimation rate? So while we are working on this, there are a lot of papers on this. So they will look at the approximation or estimation theory of generating. Of generator network. So around like 2020, so we have the first result. Well, if we assume that the data distribution mu, now is the high dimensional. We start with the high dimensional distribution. It has a density and the density is lower bounded. Then it also has a compact support. The support is. Compact support, the support is convex. So, those assumptions is more based on optimal transport theory. So, we make those assumptions so that there is an optimal transport which can transport the uniform distribution on the cube to this data distribution. And then with the optimal transport, there are also regular. Transport, there are also regularity guarantees. So, if the density is alpha, holder alpha, then the optimal transport is alpha plus 1. So, we establish approximation theory to approximate the data distribution in the W1 distance. Okay, so this is, we get all the benefit from the optimal transfer theory. And with the finite samples, so we add some statistical array. Statistical analysis to show that actually we get if we are giving n samples, we get the rate of n to the negative 1 over 2 plus d. So you can clearly see that if we just have distribution in high dimension, in capital D dimension, we do get the curse of dimensionality. And actually when we submit this paper to machine learning conference, every time we get the question. Every time we get the question, why do we require the uniform distribution to be the high-dimensional one? So, can we explore some low-dimensional structure? So, this is recently done by Alex and Pirage. Well, they take the, so to utilize the low-dimensional structure, now we just assume that the data is on a low-dimensional manifold. So, we know that on low-dimensional manifolds, so the manifold can be globally very Manifold can be globally very non-linear, but locally, if we take a Euclid ball on the manifold, well, the radius of the ball is taken to be rich over 4. And then, so we look at this local distribution, and then we are going to do log map to push this distribution to a Euclidean ball. Now, this is nicely convex, and they did some tricks to make sure that the density. Some trick to make sure that the density is lower bounded. And then now we can design on we can use the open mode transport to transport this random noise to this yellow distribution on the ball and the exponential map will push the distribution to the manifold. So dating this construction explicitly, then you can see that we require the density to be bounded. Now the convexity comes from this particular construction. Construction. And then in summary, we did now we showed the approximation theory. Well, there is a generator which is powerful enough. The generator depends on intrinsic dimensions so that we can approximate distribution of the manifold with arbitrary accuracy optimum. And then if we have n samples to learn, then actually we get the rate of n to the negative 1 over d plus delta. Well, delta is a prescribed constant. prescribed the constant. So if we prescribe delta, then this constant will depend on delta as well. So but the point is that the rate crucially depends on the intrinsic dimension. Okay, so I think I'm almost running of time. I'm going to summarize that. So I talked about a neural network for the regression and the distribution estimation. So our goal is to get some Our goal is to get some estimation bound in terms of the intrinsic dimension of the data. So, this helps us to partially justify that neural network performance truly depends on intrinsic dimension. It's completely the constant beside the constant, the rate is independent of the ending dimension. And of course, you might ask: Is the manifold the real, like, the best? Like the best model for real data? I don't know. So, manifold, at least so far, is the best manageable model that we can do from an applied mathematician. So, finally, I want to thank you for all your attention and thank you for NSAP. Thank you. Thank you very much, Wanjing, for covering a lot of time we've come to do something through. Uh so you your um estimation was under W1, right? Yeah. So I if you check, for example, I know from the textbook that if you use Russian, you always get a even comparing the origin distribution and the empirical distribution, you always get a dimension dependence. That's just because the Russian class. But if you use, for example, MMD, I mean, For example, MMD, the maximum discursive, it can get rid of that D. So the rate will be like multi-carlo rate, which is independent of the dimension. I see. I haven't looked at much of MMRD. I do see that other people use MMRD. So I agree that with process and distance, then it definitely depends on the dimension. Then the good part is that with low intrinsic dimension, it depends on intrinsic dimension. Yeah, so that's that's a good part, I think. That's a good point I think. Yeah, and I I haven't looked at MID but I'm happy to learn about it. So uh do we have an idea of uh the reach of some of those uh data sets? Oh this is a very good question. So um about the reach of the um class like those popular benchmark data set, I don't know. Actually um also there are a lot of papers. Also, there are a lot of papers about how to estimate the rich. It's more on the theoretical side, but I haven't seen anybody really apply to those benchmark data sets. But that would be, I thought naively, that would be just, you know, looking at the minimum pairwise distance divided by two. Pairwise distance, I think, no, you want to look at who is the nearest point on the manifold. So, okay, so I think in order to do that. Manifold. So, okay, so I think in order to estimate the reach, one has to estimate the manifold first. Yeah, so it would be really interesting to see who can get some idea of the reach because it seems to be very important to your theory. Yeah, this is a very good question. I don't know. Thank you. Any other questions? With that, let's thank Julia for the time. 