To start off the talk, so I'm going to talk about what Andrew calls the Earth model, equilateral random polymerase. And I've sort of chosen this model because it's relevant to the polymer people, the chemists and physicists that I most often talk to. But it sits somewhere between Gaussian random polygons. Gaussian random polygons, which are too approximate and mathematically, in some sense, too easy to be super interesting. And self-avoiding things, which are incredibly challenging. So this is sort of a middle ground where there's some interesting theory and I can actually do it. So this is joint work with Henrik Schumacher, who's my postdoc now at Georgia, and Claire. Now at Georgia, and Clay, and also Tetsu Doguchi and Erica Wahara, who are at Okamizu and Kyoto, respectively. So let's see. So Polymer people call this the freely jointed chain model, right? But we just call it equilateral random polygons. There are points in space, and they're separated by fixed distances and And you can either think of it as a set of edge vectors plus a translation or as a set of vertex positions in a space. In any case, it's in some dimension that I don't want to specify immediately. You could certainly have equilateral random polygons in the plane and see that as a model for a random knot diagram. Or you could have equilateral random polygons in space. Equilateral random polygons in space. So can I understand better about fixed distances? Oh, yeah. So you said fixed distance, is that fixed distance is all from one point on my glasses? Oh no, fixed distance is from the next one in the chain. So it's a predecided what the order is. It's predecided what the order is. Okay. Each a fixed distance from the other one. And each of the distance from the other one. Yeah. So it's like. So it's like a graph embedding with fixed edge lengths, but the graph here is just the cycle graph. So one thing just to get comfortable with the space as a Riemannian space is it's 2n minus 6 dimensional. And so you're supposed to believe that because each edge sits on a n squared, and there are n of them. So there are 2n degrees of freedom for the edge. 2n degrees of freedom for the edge directions. But they have to close, which means they have to sum up to 0, so that's three conditions. And you want to mod out by rotations, which is another three conditions. So that gives you 2n minus 6, which is even. And whenever differential geometers see an even-dimensional space, we look for a symplectic structure. And there is one. So you can write down action angle coordinates on this space. Angle coordinates on this space. And the point probabilistically is that the action angle coordinates are not metric preserving, but they're measure preserving, which means that we can do probability calculations, though not distance calculations or self-avoidingness calculations or anything, intrinsically in the action angle coordinates. And so they're one of the So they're one of these things that's obvious once you see them, which is that if you fix these edge lengths and you imagine triangulating the n-gon abstractly in whatever way you like, then there should be n minus three diagonals, so two here for this pentagon. And if I fix those diagonal lengths, I've actually determined the three side lengths of all of the triangles. So I've determined Of the triangles. So I've determined the triangles metrically. The only thing left is to decide how they relate to each other in space, and there's a simple prohedal angle that describes how to glue the triangles together. This is fully intrinsic. It doesn't tell you anything about translation or rotation. So that's an intrinsic description of the space. And if you write it that way, the theorem is just that this is measure preserving. This is measure preserving, so the joint distribution of all the diagonal lengths and the dihedral angles is uniform. And then you just need to stay uniform on what? So the angles, of course, are in 0 to 2 pi there torics. But the actions, the diagonal lengths, those lie in some more complicated domain, as triangle side lengths have to obey triangle inequalities. Triangle inequalities. So there's some system of inequalities that cut out some complicated polytope and minus three-dimensional space. So what we really want to do is understand the moment polytope. Now, I should say, you know, caveat, okay, this is a toric symplectic manifold, but it's sort of almost a toric symplectic manifold because at the corners, strange things can happen. Strange things can happen, at least for equilateral polygons. But measure theoretically, we don't have to care. So here's a result from a few years ago that Andrew mentioned, that you can construct samples from this picture in expected time end of the five halves. And how do you do it? Well, it's surprisingly easy. You make a linear transformation. You make a linear transformation and say, if these two diagonals, i and i minus 1, are connected by a unit length edge, then the difference between the diagonals is between 1 and minus 1. So why not make that the variable, the difference between the diagons? And when you do that, you transform the system of triangle inequalities. And of course, what you get is a sort of random walk. It's a uniform random walk where you go up and down. uniform random walk where you go up and down by something sampled uniformly at minus one to one. But then when you get to the end of the polygon, you're supposed to have closed. So the question is, can I sample bridges, uniform random walks that come back to the start? And the surprising theorem is that it's polynomially likely to just get a bridge if you do a random walk, which I wouldn't have expected, right? I expected it to be exponentially unlikely. It to be exponentially unlikely to come back to your starting point. But it's not, and in fact, the probability of getting back to the starting point is some interesting constant times n to the minus 3 halves. So that's where the 5 halves comes from, because it takes you to order end time to do the checking, and then you get an n to the minus 3 halves chance of success. So we thought at the time that was the best you could do. At least mod some even more clever transformation of the moment-polytau. But it turns out you can actually do a little better because if you check the inequalities as you go, there's this thing which I just learned happens all the time in self-avoiding random walks, which is that if you're going to fail, you're going to fail right away. So in terms So it turns out that if you check them one by one, the expected number of inequalities you check is not order n, it's order root n, which allows you to shave off a square root of n from the expected time. So you get order n squared. Now I think that's really the best you can do with the moment polytope, unless you have some even better description of the moment polytope. But it's maybe the best you can do. Polytope, but it's maybe the best you can do by rejection sampling on the polytope, which is really what we're doing. Just a comment, this involves these sinc integrals, which come up a lot in chord-length analysis for equilateral random polygons. Why sinc? Because it's the Fourier transform of the boxcar function. And so it comes up a lot in volumes and polytopes. Volumes of polytopes in high-dimensional spaces. And the other thing to say is proving this required us to come up with two different symplectic structures with two different reductions and two different moment polytopes, which were equivalent in symplectic volume because it's the same thing. But there should be some combinatorial proof of this, I hope. So it would be. So it would be interesting either to import this symplectic retriangulation thing into combinatorics or understand which combinatorics thing we should have used instead. But either way, it would be cool. Okay, so this is a thing you can do even right now. So there's a GitHub repository. Henrik Schumacher Cobar is linked. Cobar's link.get. I'll explain this part of the name later in the talk. And then if you have Mathematica, you just load the file, and there's this thing called action angle sample. This is the number of edges. This is the number of samples. And if you want the closed polygons out of this thing, here's a closed polygon expressed in terms of its vertex positions. Positions. It starts at the origin, it goes to 1, 0, 0, and then it just keeps going. Okay, so one statement. Oh, yes. So what's the practical limit on? Oh, uh... N equals 1,000? Oh, no. We've gone up to 10,000 easily. And I'll give a more advanced sample. I'll give a more advanced sampling method later in the talk in which I get 100,000 dots on my laptop regularly. So yeah, it goes way up. So, but this, I'll show you a performance graph in a sec. So, okay, so the someone asked me, it's great that you've got these samples in order and These samples in order n squared time. But already, what are you going to compute about this polygon that's not going to take order e to the n time, or at least order n to the high power time, right? Like, what are you actually going to do with that? Compute the radius of gyration or something. And so, okay, here's a way to compute at least some amount of invariance. Some non-invariance very, very close to linear time. So let's just take the Alexander polynomial. So I feel like this is something that probably everyone here knows very well. So I'll just remind you that, you know, it's the determinant of a k by k matrix, where k is, sorry, a k minus 1 by k minus 1 matrix, where k is the number of crosses. So it's a large matrix determinant. And how large? Well, there's a famous theorem due to Diao et al. that the expected number of crossings in a random projection of an n-gon is something like 3 16ths n log n plus O of n or something in the paper. And here's some data going up to 4,000 edges. 4,000 edges, which I did on my laptop while I was preparing the talk. And here's the 316th N login. So one thing to point out is that this does not look like an order of n error term. This looks way better than that. So someone sometime should prove a better error bound than that. But in principle, what you have in practice, 3 16ths n log n is awfully good. Is awfully good. And then, of course, you want to reduce it by doing any obvious Reidemeister ones or twos. So if you do that, you actually get rid of about 60% of the crossings in a random equilateral polygon projection. So here's the reduced Alexander matrix. Now, Andrew. Is that four tenths a heuristic, or is that four tenths done by Done by Dow. What is it? Not justified theoretically, as far as I know. Yeah, I mean, it looks really good, right? So we actually computed this explicitly by enumerating 10 crossing diagrams and got something very close to the four tenths. I think 42% or something. But, you know, that's. But you know, that's one count for one tenth. That's rigorous. So yeah, someone also should prove this four tens, or at least explain it. So an interesting thing to do, just to point it out, is people have better heuristics for reducing PD codes. So it would be interesting to put that into the tool chain and see how much reducing the PD code matters versus doing the calculation. Versus doing the calculation. At the moment, we just do that much, ones and twos. So, one thing about the Alexander matrix is it's really sparse, right? Because every row contains only three non-zeros. There's some term from the overarch that goes through the crossing, and some two terms from the other two overarchs that end at the crossing. So, here's an Alexander matrix for a $4,000. Matrix for a 4096 non. It turns out to have 3,100 crossings after reduction. And because of the way we constructed it, all the non-zeros are sort of above this. Now, we'd really like it to be symmetric, so we just construct A star A, and that's some symmetric matrix. There's no reason A star A should be symmetric. There's no reason A star A should be sparse. Not every A star A is sparse for sparse A. But for the Alexander polynomial matrices, it seems to be true. Which means that if you take a matrix reordering, you can get a really nice factorization of this thing. So that if this is not your regular game, the point of the matrix reordering is essentially to Is essentially to close the graph whose adjacency matrix has ones where this has non-zeros and zeros everywhere else into pieces. It's a little like one of these tree-width decompositions with bags and connections and so forth. And what you get are little blocks which interact for themselves, and occasionally these large narrow diagonal These large and narrow diagonal, these large and narrow row or column matrices, between connections between a sort of tangle and like the rest of it. So here's the Kolesky factorization, and then read off the Alexander polynomial by multiplying down the diagonal elements. Oh, approximate minimum degree. So, you know, the idea is like you started. Is like you start at a node in this graph, and you're like, where should I go next? And the answer is to the guy that's connected to the fewest other things at one remote. It's not the best possible thing, and there are examples that have been constructed to eat it, but in practice, yeah, it's a good hack. It's a heuristic. So if you do this, it's really If you do this, it's really fast. So this is computing three values of the Alexander polynomial. And for 4,000 gons, and create the thing and evaluate the free invariance in like a thousandth of a second, I go here. So it's usably fast. And you can see. And you can see what happens, right, is that our old end of the five halves method crosses over Alexander matrix computation between 100 and 1,000 edges, whereas the new one crosses over about 3,800 edges. So that's where it starts to get slower to generate samples. And this line is about n to the 1. about n to the 1.25. So, you know, plausibly fast. It's a little hard to distinguish n log n from n to the small at this range, so maybe it's n log n. But this little skeep bit is where you do the dense matrix determinant. That's O of n cubed, but for various reasons, for various overhead reasons, for small. for various overhead reasons. For small matrices, it's not worthwhile to do the sparse thing. And for really small matrices it's just the overhead of getting stuff into and out of the Alexander code. So I guess it's worth noting that like, you know, the enumeration of 20 crossing knots or something would be in this, right? It's still in the, like, it's super fast. Okay, but. Okay, but there's a crossover, and polymer people want to study a million guns, right? They don't even want 4,000 gons. So we'll have to go faster. But for now, of course, we should say, what can we do with the ability to evaluate values of the Alexander polynomial really fast? And this is my question for the community. What can we do with the ability to evaluate values? With the ability to evaluate values of the Alexander polynomial really fast. Surely we haven't thought of all the good things to do with this. So, for instance, Mark Dennis and Steve Whittington and their postdocs did a very large and good experiment with the original five-hound Sachs and Engels sampler and the Alexander Paul. Zambler and the Alexander Polymer field. And they propose that the probability of a knot type has this very specific form. And this is worth going into a little bit if you're a traditional knot theorist and this random knots in polymers is not your usual game. So there's a coefficient, there's n to a power, where the power is some offset plus the number of prime summats. Of prime sum ns, and then an exponential decay term, and then some small corrections for small n. So it turns out that this n0 is some characteristic length that's associated to the model and not the knot type. So we expect v0 and n0 to be constant across knots in the ERP model. And for n0 to be about 200. 0 to be about 250 to 55 to 59 and the offset power is about minus a fifth. So similarly, our collaborators propose a slightly different model in which you scale the n and the power term by the characteristic length to make things better, but it's and has different functions. It's and has a slightly different finite size correction, but it's still in the form coefficient something to a power e to the minus n. So if you write down unknots, here's the probability of an unknot up to 3,000 edges or so. And they're hard to get at the end, right? Because down here you're in the Down here, you're in the one in a million kind of random polygons as an unknown. And we wanted to see 500 of them to get the error bound correct. So this was something like 500 million samples and their Alexander polynomials. So we ran that on Clay's Mac Studio over a weekend and some and it worked great. So we get this fit. We get this fit, and we can fit it to the Stu Mark, you know, Taylor-Dennis form, Taylor-Schonk form. And it fits really well. And our coefficients are close to theirs, and this seems really good. But one of their biggest question-oh, yeah, of course. So, how are we checking that these are unknowns? Oh, just by computing three values of the Alexander polynomial. So, it's not a very So it's not a very careful check. But it's the same check that they did. So you're really checking it as outlander balls. Weaker than that. Weaker than that. Weaker than that. But probably equivalent. Yeah. So yeah, so there's no guarantee that they're really all on us. Oh, we don't really. But yeah, there might be some contamination. And the heuristic, at least for the physicists, right, is that, well, if there are weird That, well, if there are weird knots that would contaminate our data, they're probably really complicated and even more rare than finding an unknot at this point. So that should be way down in the sample area. Well, I just want to say, like, something in this range, I can probably check for a slash remote. Oh, that's pretty reasonable. Oh, okay. How? For example, take the exterior sample button. That would work. Oh, okay. Oh, okay. So these would have like 2,000 crossings or something? Oh, okay. Cool. That would be. Okay. We should talk more. Yeah, yeah, I would love to hear about that. Has anybody actually experimentally checked the proportion of knots without center for one that are actually not? Like you said, it's probably not anything more complicated. I know when people. And I know when people have done this sort of calculation before, I think the assumption is that actually computing electronic polynomial in this range or other invariance just becomes prohibited. And I think realistically, people have not. I mean, you bought one or two just to see what's going on. I'm just curious how true, or how old the statement is true. Yeah, absolutely. That's something that someone should know, right? I can say that. I can say that even to actually write down the Alexander polynomial as a polynomial, the coefficients are really big for random things like this. I mean, they're like extended integer type big, you know, hundreds of digits. Millions of crossings at the right end? No, because 3,000 edges is, remember this 316ths n log n, right? So it's 4. It's 40% of 316,000 log 3000. So, you know, it's like a couple of thousand. It's like a couple of thousand crossings. Oh yeah, worst case, like things with stick number 3,000, you know, could be like roughly 3,000 square, and then you'd be really worried. But that never happens. Well, probabilistically, that is not going to happen. Probabilistically, that it's not going to be our problem. So the big question they had was: is there a correction, a power law correction term for the unknown? And they say no. They tried to fit it to a pure exponential in various ways and got significant deviations in pure exponential fit. But that's a multi-parameter fit, kind of sparse data, mostly in the range 2000 to 3000. So we thought, oh, we'll just confirm it now that we have the ability to do these Alexander polynomials faster than they did. They were using a dense determinant calculation, which took a really long time, and 2,500 or 3,000 edges. And somewhat surprisingly, it fits finely. It fits fine for us. So the pure exponential fits just as well as the power law correction term in this range. The actual fit errors for our pure exponential and power law correction. So as far as we can tell, it's not ruled out. Andrew? I think physicists would like there to be no power law correction because that No power law correction because that implies different universality classes of things. Ah, okay. I think the physics argument should be that there isn't any which makes people nervous because it means different universality class that isn't pure self-employed form somehow very corresponds. Okay. Physicists have not self-matter. Oh, okay. So they'd be happy with that. Physicists are happy with the idea that the pure exponential fits. Okay, that's good to know. Okay, that's good to know. Because we don't, yeah, we don't have a dog in this fight. You know, we're just applied mathematicians. So, you know, we try to get this calculation exactly right, and this is where we find. Okay, so we can fit the Daguchi Uhara form very well for things like trefoils. Things like trefoils. So here, what I mean by trefoil is I check the Alexander polynomial and all the seventh roots of unity. Why seven? Because it's not going to pick up the third roots of unity, which are roots of the Alexander polynomial for the trefoil. And I tried evaluating various other collections of complex numbers and got essentially the same fit. And this fit involves coefficients. It involves a coefficient of 0.666, a power law correction with an exponent that's very compatible with this being a prime knot with a slight offset. And you can do the same thing for trefoil, connect some trefoil. This is like a composite knot with the same offset, 2 minus a little bit. And for the figure 8, 0.1 And for the figure eight, 0.152, again the same power law. It's prime naught with a slight offset, offset around 0.2 minus 0.2. And so the question is, you know, my question for the not theory community is these CKs are a really consistent fit to not probabilities. Like, this is a model for the probability function that seems to be really holding up. Really holding up. So now I think it's a good idea for the knot theorist to look at tables of CKs and tell us what that is as a complexity invariant for knots. What are we measuring physically? What kind of knots are likely or unlikely? So here are some rules the CKs ought to follow. That if I take a connect sum with itself, I get CK squared over 2. I get ck squared over 2, 2, because they could be in either order. If I take a connect sum, the ck's multiply approximately. So this has an interesting consequence that if this holds, then the probability of having no trephoids as connect summands goes down exponentially. And there's a specific prediction for how it goes down involving the coefficient of the trefoil. Involving the coefficient of the trefoil, the CK for the trefoil. So this would be a measurable, right, an observable. Now I should say I wanted proved a long time ago, to paraphrase his theorem a little bit, that the probability of being trefoil-free is no more than exponential or negative exponential. So almost everything should contain trefoils. And if you And if you do trefoil freeness, you get this interesting behavior, which is that the Daguchi Yodakara thing works really well for a while, and then it stops working, and there's a completely different power law. So we can't explain the difference between this e to the minus 0.666 and this measured e to the minus 0.384. What do these two numbers have to do with each other? Do with each other. Now, there's a lot of things you could ask about here. First, how did we determine trefoil-free? Well, we just checked at the roots of the trefoil. We just checked at the roots of the Alexander polynomial of the trefoil. And we saw, if you got zero. If you don't, then you definitely don't have trefoils as sumads. But if you do, then you might have an 818 as a sum add and not a trefoil. As a sumant and not a trefoil. So it's possible that some of the things we classified as, you know, trefoil-free or having trefoils did not actually have trefoils. And that would change the curve. So here are another set of questions for the not theorists, right? Okay. How do we distinguish things with a trefoil suman from things with an 818 summit? One way to do it would be we could compute the multiplicity of roots of the Alexander polynomial and see if the multiplicities were the same at the new roots for the 818 potential 818 subn and the old roots for the potential trefoil subn. For the potential trefoil summit. It's an 818, those multiplicities will all agree. If they disagree, then maybe there's some trefoils and some 818s. But how do you detect the multiplicity of roots of the Alexander polynomial? Another question is: do we worry that if you put something in and do all the sparse matrix stuff and you get approximately zero, then whether it's really a root? Yeah, I was going to ask: Do you know what the intermediate values are when you do that determinant calculation? Because I presume there are some big calculations that happen when that determinant comes down to zero. So both the sizes of the intermediate along the way. It's a really good question. I mean, it's not. Yeah. So we use SweetSparse for the AMD ordering because it's a standard AMD package. And we use our Package, and we use our own sparse poles thing from that order. So we could get in there and look at things. And the other thing we've thought about doing is replacing reals with either a rational number class or like the Galois extension of a finite field adjoining the roots of the trefoil. Like we're like, oh, we could do it in Z mod 2 to the 32nd. 2 to the 32nd, you know, 2 to the 64th, adjoin the roots of the Alexander polynomial and look for residues. And then things are trephoid for you. We haven't actually done this yet, in part because we know that some of you have done really good large rational or algebraic computations, and we wanted to get a sense of what the right tools were. So if anyone wants to educate me, that would be awesome. Wants to educate me, that would be awesome. And then, of course, maybe we should just do another invariant. So, my other question for you guys, right, is what other invariants involve things like large determinants? Because we could probably do those fast too. Okay, but go back to this thing. We spent two and a half hours sampling polyons and only one hour computing Alexander polynomial. Hour computing Alexander polynomials. That's two and a half wasted hours. We could have gone faster. So here's a way to go faster. Let's go back to the idea that polygons are a sub-manifold of something. So let's just think of polygons as edge sets, in which case they're clouds of points on the sphere, which means it's a sub-manifold of a product of spheres, and it's an easy. Of spheres, and it's an easy sub-manifold. It's a slice by a hyperplane or a hyperspace where the sum of the points on the sphere is zero. This is the planar picture, which is exactly the same as the three-space picture or the D-space picture. There's a sphere, there's points, we want to know where the point cloud is centered. So, okay, product of spheres, slice that we want to get a handle on. Wouldn't it be great if we could retract the product of spheres, or at least most of it, onto the submanifold we like? Now, of course, these two things have different homotopy types, so we can't retract all of the product of spheres onto the closed polygons. Some of them will be unclosable. And when you think about it, they have to look like this, right? Because you could close this by bending it to the right, you could close it by bending it to the left. Close it by bending it to the left, and there's going to be no way to decide between those two options. So, okay, these are the orange guys, they're unclosable. But once you leave the unclosable guys, you've biased it. It's not hard to decide how to close this one. You're going to keep bending it. Of course, maybe the bending will be weird. But when you're done, you get a closed polygon. So there's a three-gon. There's a three-year. So, okay, so here's the other interesting connection to this conference, right? The way to close them is hyperbolic geometry. So here I have many questions for the hyperbolic geometers. First, we want to be in the Poincaré disk model, and we want to see the sphere that our Poincaré goes on as the sphere of infinity. Once you do that, there's a really interesting construction. At every point in hyperbolic space, there are geodesics going to these points at infinity. And there's a direction that we see those points at infinity. They're like stars in the sky, right? And we see where do the stars look like. If we move around the hyperbolic plane, those geodesics will go differently. And there's some point in the hyperbolic plane where In the hyperbolic plane, where the sum of those visual directions will be zero. That's called the conformal barycenter of the points on the sphere. And it's like a hyperbolic center of mass. Now the nice thing about this hyperbolic center of mass is because it's defined in terms of geodesics and points of infinity, this whole thing is isometry equivariant. Which means that I could do the following weird thing. I could do the following weird thing. I could start with a point cloud that's not centered, so it's an open polygon. I could find its conformal very center somehow. I could observe that if only this point was at the origin, the geodesics would be straight lines, and their tangent vectors would be edge directions that sum to zero. So I could take a hyperbolic translation. Hyperbolic translation that moves this thing to the origin. And by doing so, I would gradually close the polygon. So formally, you're writing closed polygons as a quotient space by the action of the mobileist group on the product of spheres minus some stuff. How did you get from the atmosphere? Okay, so. Okay, so I. The open polygon is a point cloud on the sphere, right? Or at least its edges are. So these points are the edge directions. Oh, so you start with an arbitrary point in the interior? Yeah, I just, I did, oh no, so I start with a point cloud, and then I'm going to have to search for the point in the very center. And I'll tell you in a minute how we do that. But that's a significant part of the calculation. It's finding the very center. Let's find the barycenter. But once you find the barycenter, closing is easy. What's the point that you start with? Okay, so to find the barycenter? No, the point in the interior that you start with. Oh, this one here? That's the barycenter, but I found it by a complicated numerical calculation. It was not obvious where this was. So I started with a point cloud. You start just on the boundary. You start just on the boundary. And then the question is: where in the middle of this disk, like the very center be? But once you found it, right, the closure is sort of clear. So this picture is really due to Milsen and Kapovich in the 90s. There's like some great papers where they established that closed polygons are equipped with space by the Mobius group. And so we're just kind of operationalizing their insight. But that But that was surprisingly interesting and useful to do. But full credit to them for realizing this was going to work. So, okay, so here's the picture, right? I can deformation retract from spheres minus some stuff onto closed polygons. If I can actually do this, then I would have a sampling algorithm on this space. If I could correct for the sampling bias in due time, Sampling bias induced by picking the uniform metric and projecting it down. Okay, the first challenge is to find the conformal barycenter. Well, there's this thing called the Boussomont function, and the Boussimont function has the nice property that at every point, its gradient points along the geodesic that goes from that point to the point x used to define the Guizman function. This could be a sphere geometry thing if you want. This could be a sphere geometry thing, if you like. But the stuff perpendicular to all these circles is a circle. So we're trying to minimize the Boucement function. At that point, the gradient will be zero. That'll be the conformal vary center. Now, the Boucemont function is not Euclideanly convex, but it's hyperbolically convex. So you could do convex analysis intrinsically and find the minimizer. And in fact, that's what we do is That's what we do: is we wrote down a version of Newton's method, but intrinsically, where instead of computing the gradient in the Euclidean sense, you compute the gradient using the covariant derivative. And instead of stepping along straight lines, you step along geodesics. And if you do that, you can have a very fast and very stable algorithm to find the barycenter. In fact, it takes... It takes O of n time. This D is the dimension, right? So it takes O of n time to do it. And in fact, to double precision, you need at most six iterations. So the coefficient is really small. Okay, so then you could sample if you compute weights, and we computed the weights. So it's a coarea formula calculation. A coare of formula calculation. It was hard. We wrote it down carefully, checked each step. And you get these interesting terms, which involve the inertial tensor of the initial point cloud on the sphere, a re-weighted inertial tensor, the product of the conformal factors of the points on the sphere under this conformal transformation that takes the center of mass to the origin. So these are all believable things that would show up. That's all. Show up. That's all I want you to know. And they're fast to compute because the inertial tensor is a 3x3 metric. So this is like a 3x3 determinant. So, and we proved that these were the right sampling weights. Okay, now, I was talking to Andrew, and Andrew was like, well, what really matters is how the sampling weights are distributed, right? Because re-weighted sampling only works if the sampling weights don't have, like, occasional, huge, terrible things. Things. So this is what the sampling weights look like. Here's a histogram. These were 1,020 foregons over a million samples, and that's the histogram. They look about the same for any large end, the histograms. And the coefficient of variation is about 0.45 always. It seems to converge rapidly to 0.45. Seems to converge rapidly to 0.45 and stay there. What is this distribution? I don't know. It's not obviously log normal, which was my first guess. So now I'm stuck. So another question, you know, what is this distribution? But in practice, it's okay. I'll skip past the while computing the sampling weights. Sampling weights. The other thing that I'll say is that there's for a long time been this confusion about should you mod out by SO3 or not? Does it matter? If you're computing an SO3 invariant integral, like radius of gyration, surely it should be the same if you mod out or don't mod out. And I'm here to tell you that that's wrong. There is a new different correction factor if you Correction factor if you take the quotient space. And here's an example. This is the equilateral foregone in the quotient metric, and this is the equilateral foregone in the non-quotient metric. So what's the quotient metric? Take four points of equal length that add up to zero and now quotient out by SO3. So operate in the Riemannian metric on a quotient space. This is the This is the histogram of the length of the diagonal. It's normally distributed. It's a really nice consequence of the symplectic stuff. Here's what you get if you don't plot out by SO3. Now this is an SO3 invariant integrand. It's a length. But the histogram is totally different and involves like some elliptic integral terms. So we should be careful about the difference between Should be careful about the difference between quotienting and not quotienting. Finally, here's the performance slide. Now, sampling is like O of n. You could have worried about the coefficients for sure, but it turns out that indeed it's both faster than the five halves thing almost immediately, and it seems to be faster than the invariant calculation forever. So So how can you get samples? Again, with Cobars link, formal barycenter sampling. Just type random closed polygons. This is the dimension. This is the array of edge lengths. This is the number of samples. And you get back this data structure with everything you might want to know. How fast is it? This is 100,000 2048 guns on my laptop. Gons on my laptop took 10 seconds, so it's about a thousandth of a second per sample. Any questions? So I guess we had a lot of questions already. So we'll convene back. We'll have coffee and then we'll start at 10.40. Yeah, more than quite any answers. I have lots of questions for you. Okay.