Okay, first of all, thanks a lot for this amazing week. It's been a lot of fun and super exciting discussions as well. So I'm going to commit a double scene because there are no loops and there is only partial REST lecture. So I actually modify my title to effect that. So, we already talked quite a lot about branching structures and so on this week, and especially in the context of developmental systems as well. We discussed the formation of lung or salivary glands, like Ignatio Carl had also liver examples, and we just also listened to these kind of systems now. So, this is like a very ubiquitous way of forming a structure to maximize area. Structure to maximize surface area to volume ratio. But also at a very low scale, like at the scale of a single cell, we see these kinds of structures. So the prototype example is, of course, like the shape of a neuron. And here is a very nice case of Birkin G cells. These are from the brain. They are really like three structures, super flat, like very two-dimensional structures. So one of the kind of like recurrent One of the kind of like recurrent themes this week had also been like, oh, like you have these nice networks, but is there any like external influence? Is there any sort of like global guidance? How do they, you know, like, and this is kind of like one of the questions that I wanted to actually answer, although this was kind of like an old project that we worked on like a few years from now. So, how do the question, because the first question that I want to ask is like, how do these First question that I want to ask is: how do these trees know which way to grow? And in the context of neurons, this is, we kind of know that there are at least two strategies we can think about. One is that the initial branch or the initial directionality, the overall directionality, will be given by a deterministic sort of external Q. So there will be some global p-pattern Q that's going to determine the growth directions. And then, And then there are stochastic local interactions that guide the individual branching and the interaction events of these secondary branches. So in the end, we have a final configuration that is a result of the interplay between these two global and versus local interactions. So then I think the kind of like a funny question would be like, you have access only to the You have access only to the final configuration of a branched pattern. Can you infer back and separate these two regulatory mechanisms from each other? So, and for this, to answer this, this was already like a framework that we discussed this week a little bit. I'm going to use a framework called branching and annihilating random walls. Ignacio mentioned this also, Justin briefly mentioned this framework a little bit. What you have in this model is a coarse-grained description of. Coarse-grained description of an evolving active tip of a branched tissue. And this would be like this orange tip, right, here. And what this tip can do is that it can probably stochastically branch to generate two tips. And then if it doesn't branch, it's just going to elongate, performing a persistent annual work. Another ingredient of the model is the annihilation part. The typical sense may. Part, the typical sense neighboring branches in a certain neighborhood, and this will lead to its irreversible termination. So it's going to stop growing or doing anything. It's just going to be frozen from now on. But this framework doesn't include any additional local interactions like tips avoiding each other and this kind of more type of local rules, as well as there is no clock rules. As well as there is no global guidance. So we can think about just implementing these additional rules where now you can allow the TIP to reorient itself when it senses neighboring branches. So we can control the strength of this reorientation, repulsion, that's it, whether it's control by FS. And then there's a very vague large arrow in the background that represents the global guidance. We can also include Guidance, we can also include a sort of like a global bias that will tilt the chip always into the direction of the external guidance. And this also we can control with an additional parameter FC. And now in the absence of these two local and global rules, what you have in the branching and anti-handemic framework is an isotropic degrowing tissue into all directions. One kind of like characteristic signature of this model is that always in the bulk, you're Is that always in the bulk, you're going to have large density fluctuations. And when you only turn on local repulsion, it's still going to be isotropic growing because there's nothing that will break this symmetry, but then you're going to increase the bulk density and reduce the density fluctuations. And only in the, of course, if we turn on the global guidance, we can break the symmetry and we have directional tissue. You can. You can summarize and obtain these different morphologies just by tuning the global guidance strength and increasing local interactions. And not surprisingly, as the global guidance gets stronger, the overall territory size that you can obtain from these structures will be reduced. And then, when you increase the local interactions, it's going to be more like a locally denser structures. Structures. But this type of, like, you can use the simulations and obtain different morphologies, it doesn't really tell you much to really quantitatively tell if there is a global guidance field or not. Because you can play around with these parameters and then somehow looking at an experimental system, say like, oh, maybe this looks like it looks like this, but there is no real statistical problem. So what you can do is uh to really oversimplify the question. Oversimplify the question to ask, like, what's this main signature of a global guidance on my branch strategy? So, what it turns out that if you really, really oversimplify the question of, like, I'm only concerned about the alignment of my branch with a given global guidance unit, then I can just look at this alignment angle and formulate a Fokker-Plan theory on this. So, it's actually possible to make. So, it's actually possible to microscopically derive the full Foucault Kappelle equation, which then gives you a steady-state solution for these branch orientations in the presence of an external balance. So, what this equation then tells you is that all the branch orientations will be distributed with Maguires circular normal distribution with a single control parameter because it's a function for misdistribution. This control parameter acts almost like a Pectay number because it's the relative strength of the extreme. It's the relative strength of the external guidance to the random noise you generate by branching along which events. So then we turn to experimental data on some sensory neurons from this. This is also Justin was showing this in one of the slides. This is the segmented version, so you can see a bit more. So these are sensory neurons from the tail of the zebrafish. This is two examples. There are like eight. Examples that are like eight. And then what we assumed is because of this like hemispherical geometry of the field, we assumed, this is a total like hypothesis, we assumed that an external field, if it exists, could have this radial form. So you can sort of imagine maybe there is a radial external field that acts to direct these nodes. And then what happens when we now look at all branch orientation? Now, look at all branch orientations from these eight neurons and just plot them. There's only one fitting parameter, the one three parameter, that's the strength of the external field. Because all the rest, the diffusive noise and so on, these are all determined from the branching angles from the experimental field. So the fitting is basically just fitting of the standard deviation with the analytical deviation. So this gives us a value for the single parameter. A value for the single parameter that's the external fields. And then you can use this value in the branching analytic random box simulations as an input. That's it. And then we run the simulations. The nice thing is that you recover the overall directionality, but also these potassiums in the morphology space. So the question then remains: okay, maybe we have a signature to Maybe we have a signature to pinpoint. Okay, there must be an external guidance field that acts to shape these structures, but what about local interactions? I completely left that part. Here there is no local interactions at all, right? So there it seems that what was obvious was that when you increase local interactions, at least in this relation, you obtain larger and larger densities. But then the most straightforward way. The most straightforward way of calculating the factor dimension. When you do that, you also see that local interactions actually increase the fractal dimension, but not external guidance. So, external guidance has negligible influence on this, but local avoidance. So, there's maybe if we want to define space fitting in the context of fractal dimension, maybe we can say, okay, this self-avoidance will lead to a better space for these vectors. Which is interesting because self-avoidance. Interesting because self-avoiding random walks, if you just have self-avoiding random walks, typically have a lower flexible distribution than so. But this is a branching and an annihilating random walk that then results in the opposite effect. Increases the effects of the summary. And now to keep you awake, I wanna let me just see how much promise. Okay, this summary is like a the the so we are thinking like this is like a theoretical So this is like a theoretical framework can distinguish maybe in some cases local versus global routes from the final image of a branch tissue. And it turns out that you can even hear the shape of these branch structures. So you can basically map all branch orientations to the frequency space, and then you can basically listen to this. No, you have two. So this is a blues typical blues scale, but you can tune the scale So each of these is just a superposition of frequencies Anyway, you can play with this on the weekend as much as you want But what I was just gonna say that at the end of the stationary stationary noise. Yes. Can you hear the notes? Okay. Look would be just the disappearance of a certain frequency. Okay. Now. Time. Okay, now for the second part. So we talked a lot about optimization and so on. I think I agree we have to be careful when we say these things. But there's, of course, the question of optimization in fluid transport networks, how to optimize this pattern. There is optimization in terms of how do I minimize the wiring cost of a network or optimal traffic and so on. Here I'm just going to Traffic and so on. Here, I'm just gonna separate just to focus on space filling again in terms of branch networks. And the system here that we are gonna look at is a lymphatic capillary network from the ear of the mouse. This is a collaboration with the developmental biology group from Helsinki. So, what you have is this like lymphatic capillaries covering the ear of the mouse during development, and they have to do it because they are going to collect the Have to do it because they are going to collect lymph, like these interstitial fluid. I don't know much about biology, but like also immune cells and everything. So they have to uniformly cover this space to be able to collect these materials. What we saw in the data was it seemed like these intermediate stages of development had kind of like it wasn't as perfect as the final stages in terms of the coverage of these vessels. Of these vessels in the entire tissue. And to quantify this, we looked at the density fluctuations in the receptor. And this is, I think, this was like, maybe I can... What we do is basically you pick sort of like an observation window for your system, and then you just count the number of particles in this observation window, and you just use the same observation in the entire frame, and this gives you a mean number. gives you a mean number and this also gives you a standard deviation of this. So then you use a larger observation window and repeat it and larger and larger and then this should give you the standard deviation using a larger larger observation should scale with a certain power alpha with this mean. And for a Poisson that for a complete random process, this uh this response should be 0.5. And in yeah, what didn't happen exactly, so Yeah, skeletonized skeletonized the branches basically. So each branch is like one. No, it's skeletonized. But I mean like you have like skeletonized objects. Yeah. And you have this wing belt? Yeah. So in this probably one branch has many dots. Okay. Yeah. Um so yeah, and when when you do this procedure, w what what it what's typical for like branching and analytics when mocks Branching analytic mocks, and for many active matter systems, is the deviation from this exponent that like 0.6, 0.7 usually, these kind of mobs. So, what we found in this intermediate stage is that P13 is exactly an exponent around 0.6, which was kind of robust in different samples that we looked at. And then it turns out that immediately after P16, it already collapses onto 0.5, so the large density collaterals are suppressed, and then it remains at 0.5. Suppressed, and then it remains at 0.5. And this 0.6 is very similar to the predictions of a branching and an IT in Learn Walk exponent. No, this is just the measure of how uniform that's the decision. So then the question is: there seems to be, at least in this sense of suppressing that fluctuation. sense of like suppressing that fluctuation there is a there's a there's an um there's an optimization of space free but what's the mechanism so first of all um we looked at we asked okay can we explain the this development of this lithium capitalism with the branching and analytic random mechanism then we looked at the segmented data sets from early stages and we saw that this actually seemed like close like local local termination of tips so this kind of like So, this kind of like resonates with the annihilation rule from scratch. And then all the branch length distributions are always exponential in this database. So, this actually supports the stochastic branch. And what we found is that the optimization probably doesn't rely on pruning that much because, for pruning, you would, in the end, if you sort of unif if you wanted to uniform the if you had a if you wanted to uniformly distribute the branches by pruning in, let's say, two high uh density regions, you would you would you would decrease you would decrease in the end the number of branch segments. What we see is actually exactly the opposite, the number of branch segments is always increasing. So and then there is also a there is also you look for if pruning would happen, there would be empty membrane sieves in the tissue, which we know. In Petit Shu, which we never found. So it seems like there could be an initial sort of stochastic invasion of this territory by branching and having animal. And then there seems to be like a second stage where actually what we found is that there are new branches arising from old tests. So there's like sprouting events that are happening from old tests. And so this is like a branching analytic. And so, this is like a branching analytic and called with side branching. Then, the interesting question is, of course, like, let's say I have a sub-optimal network with large density fluctuations. I'm going to optimize this with side branching. Do I pick it randomly? Do I just side branch randomly? Or is there some sort of local density sensing mechanism? For this, what we looked at is like we picked these side branching events and looked at the directional distribution of. And looked at the directional distribution of all neighbors. But if I can maybe like you have a ves you have a vessel here and it's gonna put a new sprout into this direction. Now I can look at the distribution of all neighbors and check this angle distribution to have a sort of like a notion of how these vessels are distributed directionally with respect to my side branch. And when we do this, what we found is that This, what we found is that actually, this is like 90 degrees and 90 degrees, sort of like the front into the direction of the side branch. Everything is sort of in the back of the side branch. So we can actually collapse it into the density ratio of whatever I have in front of me, all the vessels I have, versus in the back. And you see this nice approach from 0 to 1 that gives you sort of like A gives you sort of like a sensing radius of my neighboring vessels ahead of me versus in front of. So it seems like this side branching is both density depending, but how it knows where the empty regions are, and directionally knowing the empty region ahead with a sort of like a typical lung scale. So then we can run the simulations with side branching by testing these different mechanisms, purely random side branching, only sensing the Branching, only sensing the local density and directional density, which seems that if you have this directional sensing mechanism, you can rapidly reduce the density fluctuations. So you optimize in a very parsimonious way just by adding few branches. And with this, this is the main conjecture then. What could be the driving biological mechanism? You have a sub-optimal network and you are gonna, let's say, invest And you are going to, let's say, invest a certain number of new branches to optimize this. If you do it like a random leap, then you still get some active regions. And if you do it in a directional sensing way, you can optimize much better. Maybe the mechanism could be if you consume your own growth factor, this was the discussion of EHF gradients, but if this would derive the directionality, If this would derive the directionality of the sidebatch, you can actually have a minimal explanation of this kind. So maybe there are exceptional institutions that can derive the statistical scale space in the documentation. And with this, I think I'm going to leave this part. Thank you for your attention. I thank the group, my mentor, and the collaborators. Thanks. Thanks, I have any questions. Uh yeah, um two sidewinders buttons so the the sidewalker and menu yeah so we didn't we don't have live, right? So this is like a mouse that here, yeah. Yes, the ear the ear dilates, yes, that's a very good point. Does does it mean that you have reaction to the problem? So the branch length also increases in time. Yes, but you have proliferation in the passive ducts as well. Not the tip, but the passive ducts, so it just grows. That was part of my question we already answered. So every time you need a picture, you have to keep the mouse, you don't have the same answer. You don't have to keep them out so you don't have to say that you can say this. Which means, how do you know what is the side branch? Because if you gave me a network, I need the rule to say which branch came from the original branching and which are what you call side branching. So what is that rule? So um so the way we looked at the site question was like uh you you know that when when the sort of like the batch number increases ratio? Number increases rapidly around P13. And then the experimental values looked at the sprouting events and then sprouting, like new sprouting events. And then you can distinguish whether it's stick sprouting or lateral sprouting events. But how can you distinguish that? Because there is, you just see a picture of a public network, right? How do you know that it has sprouted or because you you have statistics specifically? You have statistics specifically for these side branches. Yeah. So you're missing something really important. So, but if it's not a terminal tip of a tree, if this product is arising from a not from the terminal. I see. So you are taking all the terminal tips and you come backwards and you see where they connect. And if they connect, you can discard some, assume. Can discard some, assuming these are the tips of the old ones? No, no, with the side branches, we are just looking at the sprouting events, even. We are not even looking at the branches itself. But if you see it, let's say there's a tree. Tips would be these ones. But sometimes you have sprouting wets at the tip, because the tip is still active. Still active. Yes. But what we are looking at is if there are spouting events in the in the in the in the non-deep uh in the ducts. So spouting. That's what's okay. Exactly. Yes, they are like not even there. So you are only choosing the small ones and then you're assuming they're gonna go in the middle. They are small ones, medium ones, don't look at the large ones, yes. Because they are arising. Um if I understand correctly, in the first initial phase where there is no robot graphic guidance, you mentioned there is some consistency. In the initial invasion? So yeah, each tip explored a space not just around, right? Just that we can remember where it was step before. Yeah, because there is a difference in the way you can drive around. Yeah, in the in the simulation, yes. Did you try to change this for a minute? Yeah, in the simulations, yes, you can. You don't observe any big change. I mean complete random or something. This is not gonna influence the front speed. Because the front speed uh actually it will, because it's gonna influence the front speed with the square. So the front speed uh will be uh the front speed will be uh The front speed would be the branching rate times the rotational diffusion. Yes, so this might, I think this is going to be square root of the. So this might influence the front speed. I think that in the tissue there is sort of dynamic transition or not from less dense to more dense. So is there a time code where cybertuits begin, or do they begin from the onset? The onset? No, in the very early phases, like before P11, there was no sprouting, exactly. There were no sprouting events like this. And also, you cannot see these side practices experimentally. And so they somehow happen they somehow start arising after the network reaches the the the ear uh borders. The boundary.