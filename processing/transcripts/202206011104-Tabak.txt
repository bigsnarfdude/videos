   Given to us if you like, and this is something we have some control over. And in fact, we can use this framework for control as well. So this is the action we're going to decide on. And then what is sort of novel phase that is presented for the first time in this talk is this G, the reference group, right? That would allow us to make individual predictions. To make individual predictions and address by serendipity partially things like fairness and things of that kind. Think of the reference group, it can be, so any observation can belong to one particular individual, so that's the smallest reference group, the individual itself. And it will show you how that can be used. It can be some group of some ethnic group, some underrepresented. Ethnic group, some underrepresented group, if you would like, for instance. So you have lots of observations, but you only care about Inui. So this could be belonging or not to the Inuit. It could be recent observations. So you like somehow to bias your predictions on what has been going on recently as opposed to what has been going on in the past. You can refer to a particular host. It can refer to a particular hospital where the treatment will take place and things like that. So, here, in this G, I think this G will give us an occasion to interact on some of the topics we worry about. So, that's my hope. And the goal is to figure out, well, X, right? So, either the probability of it, or rather, what I will be able to do is to simulate this probability, to provide some. To provide samples of raw under any scenario you care about, under the factors characterizing, say if you care about the patient, the patient at hand, the treatment you are considering, and the reference group you want to consider. Is it the patient alone, or some group to which the patient belongs, or some recent observation of the place where the treatment will take place, and so on. So that will be the goal. And so X could be total. And so X could be so. It could be a diagnosis, so you can just say, I will give it all these, I can think of disease and also situations. I want to make a diagnosis out of that, that's a possibility, right? Forecast, so figure out the future, and figure out the future under alternative treatments. And also you can play, so in the spirit of what Lida was talking about the other day, if so, this methodology will give you an answer, but it will look like... It will give you an answer, but it will look like a black box. So, at the end of the day, it's an answer. But it allows you to play with that answer. So, for instance, what does the answer depend on? You can play with the Z's and see how, I mean, even, so is it the weight of this person that is creating this output, or what is it? What current condition is giving me this particular output? So, that's the general goal. And now, let me. And now let me describe to you how you will address this. You are first without the G, so this is... This transparency includes all my prior work. So I would like to wave my hands a lot about it. And how do you do that? Using optimal transport. So, first, in this slide, A, so the action will be one of the many factors. So think of the kind of treatment is one of the factors that will affect the answer. Is one of the factors that will affect the answer, so I will not include it separately, although you may use it in different ways. And I will forget the reference group. That will come later. So, the idea is the following. So, this is a caricature of what's going on, but also is how the whole project started. So, it started this thing many years ago working with a cardiologist, Martín Cadeira. And at some point he asked me to cluster gene expression from patients to see if looking at that gene expression we could somehow look at the clustering, figure out if those clusters I found have any biological meaning related in that case to rejection of a transplanted heart. That was the goal. So we did the clustering and it was very robust, 800 patients. So very robust, the 100 patients divided into three very clear groups. And then when the time came to figure out what is the biological underlying reason for this cluster, that turned out to be very easy too. It was the hospital where the tests had been taken. So we divided it into three clusters. So we have a great way to diagnose the hospital. So it felt a bit frustrating at the time, but gave rise to all these letters. So that came in easy for the All these letters, so that can be busy for the next few years. And so, the first goal was: can we so, okay, so that was an instance of the batch effect. You have different observations, and what distinguishes them mostly, most of the variability is due to something like when you made the experiment, where you made it, and so on. And that's not what you care about. You have to remove that. So, the first idea was, can we remove that? Can we remove the batch effect? And if you think conceptually, what the meaning of that is. If you think conceptually what the meaning of that is, say you have three hospitals here, with data in three hospitals, and in this case X is two dimensional so that it fits on the screen. In that case, it was whatever, 20,000 dimensional, because it was a gene expression. So you have three classes, and you would like to remove... So here, if I give you an observation, you can tell me to which hospital observation belongs. Or at least you can have some notion of the probability of it belonging to one or two. Probability of him belonging to one or another hospital. I would like to remove that. So the idea is then I can just transform the data, removing the effect of the hospital, move all these distributions, or distributions, into a single one in which the data will be independent of the hospital. So I would like to move my X, my X here, to a Y here, to a transformation, so that Y is independent of Z. In this case, Z is a hospital. In this case, C is a hospital. Does it make sense? No, I would like to do that, I would like to do that trying in the process not to destroy all the information I have. Because I could just move all the data to a point, and the point would be independent of the hospital, would be independent of everything else. So I would like to do this, minimizing the distortion of the data. So that was the first thing. So we did that. That was successful. We did remove the effect of the hospital. We did remove the effect of the hospital, and then the remaining data, if you cluster it, it gave you some more biological information. So, in that sense, it was a success. But when you think about it, you figure out that maybe now that we know how to remove the effect of the hospital, maybe we can remove many other things. So, the hospital is not the only thing we know about these patients. We also know their age, their weight, so many things. Can we apply the same procedure? Can we apply the same procedure and remove from the data all the variability that you can explain with Z? So now Z became, what used to be a hospital, I just want a general factor. So everything that I know, can I remove it? So can I move all my data to a place where I have explained away everything that my factors could explain? The original idea was if you do that, then you have the data in a place where you can look for new things. Now I can figure out if here I could look at this as a tree. Figure out it here, I will look at the fact as a treatment, I can try to distinguish different kinds of rejection, whatever. That's the original idea. But now things come, so that's one idea, so remove by explaining away variability. And this is done through automatic transport in the sense of removing this one distribution into another, many distributions into a single one, trying to minimize some distortion, that I could call that a cost, it could be a transportation. It could be a transportation cost, you know, to have transport. But I look at for a new distribution that is as close as possible to all the others, this thing that we call the barycenter. So this is the barycenter, optimal transport barycenter problem. Okay, so that was the first thing. So now saying you have diet already, now all your points are here, have been moved to this new frame, and now they are independent. Frame and now they are independent of Z. And now we have a new patient, and I want to figure out: okay, what will be X for this patient? Well, I can do that easily. So the first step was this, right? Let me, in the rear, to use this. So the first step was X was given by some probability that we don't know, but we have samples of it. That depends on Z, and we have. That depends on Z, and we have moved that to a Y, we have transported those points to Y's that are independent of Z. And next, a person comes with a particular disease, or I'm interested in evaluating the effect of a particular treatment, which I include in the disease. I figure out what will be X under this treatment for this patient, so I invert the map. The map that took me from here to here, I invert it, but I invert it from the particular value of Z I care about. So suddenly, I have So suddenly I have as many samples, say I have a thousand samples among all people, I have now a thousand samples for this particular value of Z. Even though I have observed maybe none before. There was no person with this particular combination of age, weight, and cholesterol level. And now suddenly I have 10,000 samples of that distribution. So that's the idea of how to simulate a conditional probability using alternate transport. Online transport. That's the idea number one. So it summarizes five years of work, but anyway, so does it make sense? Yes? So you were just saying to generate more, like what you're saying is you had them in individual hospitals and now you have more of the same patients. Right, right. So now I have for each hospital in this case I have three times as many patients. I misunderstood it. Who said you have general and hospitals? Yeah, so of course for the hospital, well. Yeah. So, of course for the hospitality, well it it's it's still true. I can manage to so you can think that I managed to put data from three sources together, yeah, so I can have more data. Now, if the C now becomes a continuous variable like the H, now it's a lot more powerful number these many variables, so you are extracting, you are getting lots of samples of a probability for which you have also not a single one probably. Is there a reason why it's better to work in a particular red than versus in a very simple well no just because I want the prediction for a particular patient so I don't want an abstract prediction right of what will I if I if you tell if I tell you what Y will be for this patient tell me what is Y you want to know X so I go back and X depends on Y and on Z right this is yeah so I invert it to get the actual value of the blood pressure not this abstract blood pressure that has been deprived of your age yeah Prime of your age. Yeah? Yeah? Does it make sense? Will do. Yeah, a question. So is T unique? Is this map unique or how unique is it? Well, so a map that achieves independence is not unique. But once I'm minimizing a cost, I'm sort of selecting one. Right, yeah. So that's the idea. And you and you pick one that's invertible. I pick one. And you pick a T that's always invertible. Can you pick a T that's always invertible? Yeah, yeah, the T will be invertible by construction, but yeah, the T needs to be invertible. That's right, so I can't invert. And of course, I mean, I will not talk about, but anyway, so much of the fun is how to find the T and how to, and once you find the T, how to find the inverse, right? But that's not a mathematical fun, but I represent you the framework, yeah? Because even though I have a lot of fun doing that in a particular way, then I talk to my younger students and they all do it through just neural networks, which I for me is another black box. For me, it's another black box. But anyway, that's fine. So, the framework is enough to understand what we are doing. And then, how you solve it is a different problem. Okay, let me give you an example, a meteorological example from some time ago. So, we have hourly measures of temperature in Itaca. We so happy to have data for Ithaca for 20 years or so. 20 years or so. At that time, this was some time ago, so it was 10 years or so, and now it will be 20. And I would like to somehow forecast the temperature. A few days from today. That will be the goal. So X will be the temperature. And Z, and it means three sets of Z, just to give you a flavor of how it works. The first is just, I want to forecast temperature given a time. Temperature given a time of the day, a day of the year, and the year. So, yeah. If I do that, I get this first thing. So, I'm looking at the piece of the data only. And you can see that you can already see, well, the mean prediction is in orange. I'm not plotting the full distribution, but I'm plotting the 95 confidence interval. Yeah, so that's in pink. Yeah, so just in pink there. And the real data is in black. So you are seeing what this is doing. Somehow it extracted from the data the days, so the difference between day and night. So it's doing that. It extracted the season as well. And it extracted somehow the year, because it will be different, slightly different for a different year. Whether it was El Niño or not El Niño and so on. But of course it didn't capture anything of the real weather. didn't capture anything of the real weather. The fact that there was a cold front coming or not, you cannot tell that from the time of the day. But you're basically how it works. It explains something away. Now, step two is take the skeptic man, will say, well, the bad way to predict the weather tomorrow is look at the weather today, right and copy it. Right, and copy it, right? That would be a good prediction. That's it. So don't trust any forecast. So, why don't we add to this Z, so Carol has three, but three Z's, right? Which is time at different scales. Let me add to that the temperature the day before, exactly what we first before, and beta cap. And that gives you a better fit, right? Now you start to see the weather, it's still far from perfect, and now you still see that all days are not born equal. But then you bring a little bit of weather inside and say the weather doesn't stay, even if you are an emeritus professor of Cornell, you don't believe that the weather stays put on top of Cornell, but in fact it travels from west to east, right? So instead of looking at the weather in in at Itaka itself, look at the weather to the west of it, somewhere, pick three locations that look some far far away from Some far far away from at some distance from Itaka to the west. I look at the data 36 hours before. I use now this set of Z's, the static ones and this 36 hours before somewhere else. And now it gives you a much better forecast. And also much smaller variability, right? The pink is becoming smaller and smaller around the forecast, the more Z's you include. And now looking at And now looking at this point, this outlier, right, this day that was extremely hot. You don't capture it with the static things, you start to approach it a little bit when you do look at the day before, but if you look at all this data together, all these three, then you nearly capture it. And you can see that here, I'm plotting the actual distribution you get for this particular point, for this particular time, using the three methods: the static one, the Methods: the static one the day before, and in yellow, the probability distribution using the things to the west 1304, you see that you already this guy is no longer an outlier. The real data is not in this dotted line. Doesn't make sense, it's just an illustration, right? But you get a feeling of how the whole thing works. So, to do this, again, I took all my data that have excellencies, built out of that a map that brought them all to the very center and undid the map. And did the map, yeah, for the particular value of z corresponding to each point. Yes? 36 hours optimal? How did you choose the 36 hours? How do I choose which region? 36 hours. Oh, no, no. So, so, I somehow look at the correlation of the times, of the temperatures, and look at something where the correlation more or less peaked, right? But let's pick it just in. So, I could have. Pick a just in. So I could have picked other cities, more cities. So one goal was and still is to do an accurate weather forecast. But for this, we need somebody who will actually do it. So at that time and still now, we have mathematicians playing. So nobody has the time to go look at real data and then to make a real prediction. But it's still in store. But I present this at this point only as an example. Maybe one day I will. Two minutes? Wow, time flies. Okay. So let me. Okay. Okay, now let me introduce G. So, I do the same process, but now the observations can also some group to which they belong. And I care about one particular group, the blue group in this case. Like we just, the samples coming to this patient, to this anti-group, and so on. So I do exactly the same with all the data so far. I move it to the very center. I move it to the very center. If I will invert for all my Y's, I get what I just did. But I can do the inversion only for the blue point. So I can have a smaller set, but a set that includes only the guys with this particular ethnicity. What am I doing here? The idea is that the Y is like the X from which you can remove everything that you could explain with Z. In other words, what you couldn't explain with C is still in the Y. So if there was any effect of So, if there was any effect of an inund on your blood pressure, that's still in the wire. So, when I invert with only the blue guys, I get that hidden variability, the syncratic one, that I didn't quantify in my math because it was never removed. Does it make sense? So, that's a new idea. And now, let me talk about trajectory. And now, let me talk about trajectories. The one minute will be enough. Yeah, yeah. It's going to be a fast trajectory. But it's optimal transport. So, anyway, so first, what can I do? One thing I can do is to take a patient and I want to figure out how will the, say, the weight evolve over time. So I move all my patients to the very central unit. All my patients to the very center, I u to p with that. I look at only the point corresponding to the patient right now, as my y, and invert it under z's. That one of the z's will be time. So under different z's, that gives me a trajectory of the point. I can include a treatment, A of T, and again, one trajectory of the point over time. Of course, that gives you a point, not an idea of variability. If you were to let it say an idea of variability, If you want, let's say of variability, forget the G. Use all the guys that give you how much you can believe that result. Now, you can do the same if you figure out the effect of a treatment. So, move the patient where it is now to a Y and invert it under different treatments. And that will give you a treatment effect. And now we're going to Lina's question: Can a physician understand this answer? Well, the physician. This answer, well, technician probably cannot understand the answer because we cannot understand the answer either, but can play with it and say, Is it, it was this answer due to the weight of the person? So you change the weight of the person, you change and invert, and you see how the answer will change. So it gives you some, you help you understand the sensitivity of the answer to the various factors. And I think I'm done. Alright, now let me skip this one. This was in case it was extra time, so that was it. It was extra time, so that was a factor, but then we forget that. And then just to say that I think this is a good framework. So it's not this, I shouldn't say it, I should let my grandmother say it, but I think it's a very nice way to put it, to conceptualize the problem. And I didn't take 10 minutes ago how to do it, but that's a different story. So that's, we can talk about the answer, it's more than one way, yeah. Way, yeah, yeah, how to come, how to find this T and how to invert it, mostly. Um, and out of this talk is any biomedical application other than the language of patients and so on. But that's work in progress. With some work in the past, that's a work in progress, so that's hopefully soon. And that's it, and thank you so much. Excellent. Which probably doesn't work quite possible. If there's any pretty good things. There are. I have questions, but later. Awesome. Give that a take. Yes. Can you share your screen volume up on that thing a little bit?