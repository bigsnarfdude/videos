All right, so thank you everyone. My name is Itai. I'm a researcher at Apple's Machine Learning Group. And I'll be presenting this talk, which is based on one of our recent NERVS papers. Has many collaborators in Apple, like Prudo, is here with us. And yeah, so this is probably going to be very different than what we've heard so far. So this is a very deep learning centric talk. So a bit of a learning theory. Was a bit of a funny theory. Yeah, and feel free to interrupt me during the talk. Just ask as many questions as you can. All right, so this work is about learning unsupervised representation. Let's first describe what I mean by just learning representations. Representations. So we take our input data and we map it to some convenient space where we hope this space is going to be helpful for solving some downstream tasks. A typical example of this is just learning some neural network encoder. It gives us some vector representation, and we hope this vector representation is much better than the original representation to solve whatever we want. Alright, so let me describe a few kind of the pain points of doing research in this area. The first one, obviously, it's kind of hard to define what good features are in the general sense, right? It's a very task-dependent thing, and different tasks can require very different features to solve. And in practice, you know, especially the industry, a lot of times we don't really know what the downstream tasks are going to look like, and we don't know what the data. Like, and we don't know what the data distribution is going to look like. So, because of this, it is very important we understand the implicit biases of the models we use. So, in supervised learning, a lot of times we have these metrics we can track, like loss, accuracy on some held out set. But in self-supervised learning, a lot of times we actually don't have any. And the final point is these recent batches of These recent batch of state-of-the-art self-supervised models are very much black boxes. And yeah, in a way, that's true for any deep learning model, but it's especially true for SSL. And that's because it's not only we don't understand the model, we don't even understand the objectives we're optimizing. I'll give you an example. And they contain lots of moving parts, right? So kind of a more complex version of your standard neural network. Neural network. Alright, so here's a fun example for how black box news models really are. So in supervised learning, you know, we have a problem, we collect data, we pick our model class, we pick an optimizer, an objective function, and we just train it, and we hope to see this nice-looking decreasing loss curve. Alright, so yeah, this looks good. I think this probably resonates. What was the XX? What's the x-axis? So, this is, yeah, sorry, this is iterations like Epox, and this is just a loss, you know, whatever the objective is. So, this kind of looks good. And let me show you what this looks like for the state-of-the-art self-supervised learning in Vision. All right, and so not only is the loss not really decreasing, it ends up much higher than we originally started, but this is actually like a very good model. What it looks like just What's the loss? I will describe exactly what the loss is, don't worry. But that kind of shows you that the objective we're actually trying to optimize is not really the objective we want to optimize, right? Because we end up higher than we began. Yeah, so I just saw this quote online. The basic principle is simple, right? Environment, at the end of the day, we only perceive a tiny portion of it because we're Only perceive a tiny portion of it because we're able to abstract information. Okay, so now I'm going to take you back to back in time and show you how we used to do representation learning. And when they say back in time, I mean five, six, seven years ago. All right, so initially we did everything self-supervised. Sorry, fully supervised. So we train a network along with the task head, whatever the task is. Task head, whatever the task is, train the whole thing, remove the task head, and we have our featured representation. So this actually works quite well, to be honest, but it has a few very large shortcomings. One, we need labels. So this is very hard to come by. Labels simply contain very little information. So this just doesn't scale well, so we decided we don't really want to use labels. We don't really want to use labels. So, then we did this. This is basically an auto-encoder. Instead of predicting the label, we predict the input directly. Hopefully, you pick an architecture where you have some kind of a bottleneck and. I think that's the main reason, the identity. No, no, but even if you pick, like, so each one of these methods is like a representative of a very large amount of different models that have been proposed. So, even if you pick like a very low-dimensional representation and kind of compress the data as much as possible, it still doesn't really work. So, the heuristic reason is you're keeping all the information on the interface, right? Be all the information on the inputs, right? So you're not really compressing anything. When you say this doesn't work, what do you mean? Yeah, what I mean is, just in practice, this doesn't work well. So let's say you train this on, so envision you train this on, let's say, ImageNet, and you try to do some downstream tasks with it. So it doesn't learn like very good perceptual features. Right? Again, it's not nothing here as precise, but I'm Nothing to do is precise, but um but just in practice people don't use this. I mean you don't have real control over what type of information is getting lost. Right, and in here actually you're not encouraging to lose any information. If you're using like typically we use a very high capacity model, so you're not actually losing any information. All right, so then we did this. This is called a masked auto-encoder. Quarter where we, if we corrupt the input, in this case, we just pick like a random crop and then we predict the rest of the input, basically. Yeah, and actually this works quite well, but now we have a recent batch of models looks like this, where we take our Where we take our context, right, we predict some latent representation, and then we predict the encoding of our targets. And this is called a JEPA, right? This is called the joint embedding predictive architectures. There's many types of these kinds of models in the literature, but this is kind of the state that we are today. And we will try to uncover a bit of what's going on underneath the surface here. Yeah. So, like you said, Yeah. So like I said, like the same function, or like which of these representations do you use? Yeah, so this is typically the same function, just the same encoder. Sorry, sorry. Yeah, please. Can you go back? So here, you have an Euronet, you learn the represent, you know, you cut some layer, look at the representative, like take those. What do you do with them? So you encode your input, right? And you're predicting the Right? And you're predicting the encoding of your target. Map everything to zero. If you just map everything to zero, isn't that a perfect for me to know? Mag everything. Yeah, yeah, yeah, yeah. We'll talk how to avoid this. Okay, so let's be a bit more concrete and maybe we'll make a bit more sense. So in our setup, we have our inputs x and y. You can just think about them as the spectrum. Just think about them as the specters in RD that share some semantic information. In the previous example, it's the context and the targets. We pick an encoder, f, some function to be simple from RD to RD, the biggest we can get our hands on, and a predictor from RD to RD to be small. Okay? And the loss is going to be, look like The loss is going to be look like something like this. So we're going to minimize over the weights of the encoder and the predictor, so W and V. Then we predict using G the encoding of Y. Right, make sense? So F and G are D? No, no, we're training them. We're training them. We're minimizing over the weights of F and G. Okay, so as someone mentioned, we can trivially minimize this loss by having a trivial encoder or decoder, let's say a zero function. So this is bad. So what we do is we just add a regularizer. And there's many types of these regularizers that prevent. Prevent collapse of the whole thing. But the recent hack that works very well is doing this. So assuming we're training with gradient descent or atom or whatever, optimizer of your choice, we're just not going to propagate gradients through this part, but we're going to copy the weight from F onto this. Alright, so does that work? Does that make sense? No. Nope. Alright, so when you compute the loss, when you compute the loss on this, so in code, you basically put this entire thing under stop grab, or you don't propagate gradient through that part. So you treat it as non-trainable weights. But in the next iteration, you just copy the weights from this to that. That kind of follows your encoder. Folllows your encoder. So it's like alternating minimization. First you reduce a bit the loss of the part. You only optimize this part. It just measures your progress for one iteration to the next. Yeah. Actually, that's the reason why the loss is not really decreasing, because we're not actually. Not really decreasing because we're not actually following the gradient of this loss. But we're going to prove this actually prevents collapse at a simple setting. This with the standard ME loss, which is, you know, predict not the encoding of your target, you predict the target directly. So this is very much similar to standard MELE. Similar to standard supervisor. Okay, and again, the goal of this work is to understand if, or can we say something quantitative or qualitative about picking this objective or that, right? Like, this is a very different paradigm than doing this. Okay, so let's start with an With an empirical experiment. So we pick, we train both models on some data, in this case, I think ImageNet. And we want to measure how well the features we learn in F, so F is our encoder, how well they perform on some downstream task. In this case, it's just imagine a classification. So what we do is we pick or we take the checkpoint of the encoder at different stages of the optimization. Encoder at different stages of optimization, and we see how well it performs on image and classification. To see how well it performs, we essentially freeze the weights and train a linear probe on top of the ball neural network. What we see is that JEPA tends to learn much better perceptual features just by the fact that we can get much better accuracies much faster than it made. So it clearly does something good. Okay, so now we'll do a bit of theory. And to do that, obviously we don't know how to deal with these very nonlinear neural networks. So we're going to simplify the problem as much as we can. And in standard practice of deep learning, we're going to use deep linear networks and very simple data distributions to see if we can understand what's happening underneath. Okay, so the setup now is going to be the data distribution is going to be very simple. So the components are going to be IAD and independent of X and Y. They're going to be centered. And we're going to denote by lambda the expectation of Xi Yi. And regression coefficient, or just the optimal base predictor from predicting yi given xi is going to be given by rho i. Is going to be given by rho i. So these are two important parameters, right? 1 dot i. The encoder is going to be a deep linear network. Okay, so it's parameterized by just a product of L matrices. And the predictor is just going to be a single linear. Alright, and we're going to use gradient flow optimization. Gradient flow is just continuous time gradient descent. Are there non-linearities, or can you collapse all these? You can collapse all these matrices, obviously. It's a one matrix, it's a linear network. So the capacity of this network is just the same as the linear network. But the training dynamics are going to be very different. So it's a highly non-convex optimization problem. And you see that the training dynamics are going to be very different from between both of these objectives. Okay, so now the jet ball loss is just given by this and the A. Loss is just given by this, and the standard MAE or just supervised loss is going to be by this. Okay, and now again, the question is, once we train these objectives using gradient flow, what are we learning in W? W is the encoder for both of these objectives? Right, this is kind of the theoretical stuff. Okay, so Okay, so to make precise statements here, we're going to have to make a few assumptions on the initialization of the waves. These are pretty common assumptions. So we initialize the waves to be very small using some epsilon parameter, which we're going to take very close to zero. And we're going to define gamma i. And gamma i is, we call it the encoder response, the feature i, it's just a projection of the encoder. Feature I is just a projection of the encoder on the I-th component of its input. So it kind of measures how well the encoder learns to represent this feature. And using all these assumptions, we can collapse the entire problem into the set of ODs. So essentially, training these objectives using gradient flow reduces to optimize reduces to these to these ODs. These to these ODs. So we know how to, so the gammas essentially the response, the code of response to feature I evolves according to the following ODE, both for JEPI and MA. So each objective has a set of ODEs. Each ODE depends on the corresponding lambda and row of the feature. Right, so the way this encoder responds to the feature depends. Encoder responds to the feature depends on these parameters as long as I go. Right? Is this clear enough? Any questions? Okay. We can also say that, or we can prove that W you learn with the encoder you load with an MAE, if you take the encoder depth to infinity, is actually equivalent to a JEPA with one layer with a single layer. One layer with a single layer. Stated differently, it's impossible to get deep JEPA dynamics and solution with an MAE architecture. Forgot to mention the solution to these ODEs is basically given by this. So the response for a feature I for JeffBot is going to converge to the regression coefficient to the power of L. And for MAE, it's just going to converge to the regression coefficient of the power of L. To the regression coefficient power of 1 minus 1 over L. If L is big enough, then that just converges to the upper base predictor. That's not too surprising. But notice that for Jebo, we have the base predictor to the power of L, which kind of means it's suppressing features quite aggressively with L-S features with low RL. That's the asymptotics, yes. Yeah, yeah, yeah, yeah. Yeah, yeah, yeah, yeah. Right, so again, second result, it's possible to get a deep Jeffer dynamic to the solution with an MA architecture. But this is all like asymptotic, well except this, the other one was an asymptotic result. Now we're going to do a few simulations to see like the really interesting aspects of this. All right, so we simulate these ODs. Okay, each curve here corresponds to an OD through time. Corresponds to an ODE through time as we train with gradient flow. Again, each one corresponds to a different ODE that depends on different parameters, rho and lambda. And what we see is if we keep rho fixed to like a constant and just vary lambda, we see this very greedy learning behavior. This is called greedy learning low-rank bias in the literature. It's kind of a known. In the literature, it's kind of a known depth property. So the encoder is kind of learning the features one at a time, where the first feature that's being learned is the feature with the highest lambda. However, if we keep lambda fixed across all the ODEs and we only change rho, then all the features are learned at the same time. Right? This is yeah. What was lambda and rho here? The lambda and rho, lambda. The lambda in row lambda is just the covariance between xi and yi, the input and target, and rho is the regression coefficient, right, like the optimal base. Okay, but for JEPA, we see something interesting. So we see greedy learning behavior for both with respect to row. With respect to rho and lambda. Right? So even if we keep lambda constant and only change the regression coefficient, we still see this very pronounced 3D learning behavior where features with the largest row are learned first and features with the smallest row learned last. So this can actually have a surprising Actually, we have a surprising property of a complete inversion of the way features are being learned. So, for example, if we have just a general distribution where we have varying rho and varying lambda, we see that MAE can actually learn in the exact opposite order of JEPA. So, JEPA learns the features, the features that JEPA learns first are the features that MA learns last, and vice versa. So, for example, if you do If you do early stopping, essentially you end up in a situation where you have completely different representations for both of these methods. Should we sort of intuitively understand that one of these is better than the other? Well, that's kind of right? That's subjective. Because we don't have a notion of a loss of a representation. We don't have a lo a notion for a loss of representation. Notion for loss of representation, but in this case, we know: all right, if we're interested in, so this learns features with high rho, this learns features with high lambda. One is better than the other? Or are like one picture, right? That's exactly the intuition. Like, Hiro or its regression coefficient is basically lambda divided by variance. So you're looking for features that both explain the data, but kind of compress the data. But kind of compress the data. That's my intuition. Alright, so we want low noise or low variance, but high predictability. I like to think about it as sounds a bit more like wording features that are compressed, in a sense, compressed but predictive, which intuitively is what you want. But yeah, there's no precise statement here. One is better than the other. It's just each one has a different implicit value. Just each one has a different implicit bias. Okay, so we can't really solve these ODEs to get close form nice solution, but we can tell whether that spike happens. So we define this time T star as the critical time. So the critical time is just the time it takes to learn a feature. All right, and we can prove that MAE, the critical time for MAE, the critical time for MAE depends on Lambda, and the critical time for JETBA depends on both Lambda and Rho. Each of the critical time is to learn a little feature. Yes, the critical time depends on Lambda and Rho. So the critical time, how much time it's going to take you to learn a feature with these characteristics. Yeah, and it also depends on depth. So the deeper the encoder for JEPA, So, the deeper the encoder for JEPA, you're going to have more spacing between the features that you learn. ME, on the other hand, does not depend on the critical time here, does not depend strongly on the Okay, so this is just an experiment with removing all of the assumptions on the model side. So here we just have an MLP, but we train on the same data distribution. Distribution and it's a bit tricky to understand, but here those x-axis are the featured index, so we have feature from 0 to 100. And we have two distributions. In the first distribution, lambda and rho have the same trend. So the higher lambda is, the higher rho is. And in these figures, we basically plot the response of the encoder for each individual ficta. Of the encoder for each individual featured, so the y-axis is iteration time. And what we see is both models kind of learning the same representation or in the same order. But once we reverse the trend for rho, so here the higher the lambda is, the lower the rho is, we see that the Jeopardy distribution inverts the learning order completely. And we end up with the representations that completely do not overlap. Alright, so we have very different features that I'll learn by Jeffra than any. Alright, so I'm nearing the end of the talk. So the limitations of this analysis is that we're analyzing simple data and models. Pretty hard to extend this to nonlinear models, at least analytically. The most important limitation is that. The most important limitation is that real models differ in other ways than what is captured by these linear models. Perhaps the most important limitation here is that MAE decoder, when you use nonlinear models, MAE obviously, the predictor cannot be better than the optimal space predictor. This is not a limitation for linear models because JEPI is also under that sense. But yeah, that's an important limitation. So to conclude, in this work we kind of try to explain or describe the implicit bias for these two objectives in a very simple setting. And the kind of toy characterization of what we find is that first subgrad probably prevents collapse, so you actually end up learning something. Jeff alerts high row features first, suppresses row features, high row, oh sorry, low row features ask. High row, oh, sorry, low row features asymptotically. And MA learns high lambda features first, so it's kind of different in this advice. Yeah, that's the end of the talk. So thank you for listening. Can we actually, for the interest, can we have the discussions during coffee break? And let's take like a 10-minute coffee break that we'll be back. That it'll be back.