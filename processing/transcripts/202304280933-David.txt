So we're very pleased to have two speakers this morning. Andrei is the first, giving the physicist view, and then later on we're going to have Mikael from the statistician's point of view. These are not summaries of the meetings. You're not going to hear a repetition of every form that there's been in the last few days. But we ask the speakers to just tell us their impressions of the meeting. Their impressions of the meeting, what interested them, and so on. And then, afterwards, after they've spoken, it's up to all of you to join in and tell us your thoughts and how we can do things in the future and improve on what we've been doing even up till now. So, Andre, over to you. Thank you very much, Louis, and welcome, survivors who have been able to stay and not have flights disappearing soon. So, yesterday. Disappearing soon. So, yesterday we thought about asking everyone a question to see if there are systematic biases. There was a need. It's been reported. It is, it is, in progress. It's even flashing. So we wanted to see if people had biases, and we decided to put this picture of a certain mountain and ask you what continent is this mountain in. So have a think. So, have a think, and then when you get to the end of the talk, we can have a raise of hands. You know, choose your favorite continent or the continent you've already seen this mountain in, and then we'll get to that. We'll get to that as well. Because some hunters here got to the hunter. Alright, so this is not the physicist's view, it's one physicist's view of the Schottbipla, right? Because apparently. Right? Because apparently, it's the name of this workshop. So, about this view, it's biased, it's incomplete, and Nikael has twice as long as I do. So, you've heard during the week. A factor of two for physicists is very large. So, I'm also happy that I still have battery life enough to finish the slides. What I want to go through here is things that stuck To go through here is things that stuck, that are going to stick with me, things that jumped out at me, and things that I'm pretty sure will force us to continue thinking. Now, this Sean Pipla. This used to be called Fisttat, and when I was looking at the Fisttat logo, I realized that this reminds me of one of the talks we had. But the first thing is that there is an uncertainty on this Pfi-Stat. So it's been a long time in the making, and this has been, you know. And this has been a, you know, it's been a process. And I think that that's actually good because it's a process that will continue. And it's a process that already, I mean, we've heard many times during the week, back in 2003, we figured out that those are important lessons, and it's really good that we've been accumulating them. Now, Feistat is not a proper dimidation because it's the beginning of two words. So I was actually wondering if this would be a better dimidation. Would be a better diminution, especially because many of us physicists just want to do the fit. I mean, just let me do the fit and let me move on. On the other hand, the people who were here this week, we are not your usual physicists and we are not your usual statisticians. Now, I'm not going to say that like these wines, we are all above average, but I have the impression that we are definitely all on the table of the average. That we are definitely all on the tail of some distribution. And in particular, we have Frank, who is the theoretical physicist, who actually puts up with this stuff. He's definitely on the tail of the distribution, and that's not a bad thing. So I think it's, I'm thankful that you are all interested in each other's work, and I really hope that it's helpful to your own work to get to see and learn what others are doing, especially for physicists to actually know how to do things properly. For physicists to actually know how to do things properly. Okay, so this has been a very systematic journey. So typically, we build a detector and we take data, and then we apply some corrections and we get counts. And you know, that's one of the beautiful things about this field, is you have counts, so everything is Poisson, it's nice and understood. And then you get to the middle, which is you apply some corrections and you try to get an uncertainty on that correction. On that correction. So it's, you know, the eternal, you try to reduce your pi's in your results because you don't want to get the wrong W mass. And then you add the corresponding systematic uncertainty to compensate for the fact that you do that correction. And then you actually have to stop and think about what exactly are you doing. And this week is an example of that sort of stop and think about what you're doing on the systematics. And I wasn't there in 2000 and 2003 until late 90s. 2000 and 2003, and the late 90s, when all of this started. But I've seen already some of the evolution. We are no longer discussing the central values. We are not discussing a lot of the statistical treatment of counts, although the on-off problem keeps coming back. We are discussing now the uncertainties on the corrections that we apply. So it's going order by order. We are getting more refined. And all of this, you may, you know, at face valve, it might sound, ooh, what a. You know, at FaceVale, it might sound, ooh, what a waste of time. But with more and more data, we have to do this. We need more accurate theory predictions and we need also more precise statistical methods in order to be able to make inference on these ever-growing data sets. Alright, so usually, this is my understanding of what Banff and Beers are. It's the Center for Arts and Creativity. And creativity, and it's the International Research Station for Mathematical Innovation and Discovery. But this week, it was more like the Center for Physical Arts and Creative Solutions and the International Research Station for Statistical Innovation and Discovery of Particle Physics Analysis Ingredients. And so you may have been shocked by what physicists do if you're a statistician. And we physicists are always shocked that, oh, there's already a solution to this problem. To this problem. Okay, so this process, the way I see it, and you will see that these three things compare with many Greek letters that we heard along and many words that were applied, so I'm going to define my own terminology. But it's basically bringing out from the unknown unknowns something that then becomes sort of, we don't know exactly how to deal with it, but we know it's there, into something that we somehow, you know, fundamentally, mathematically, Fundamentally, mathematically founded way with guarantees coming from regularity conditions and beautiful theorems can be used. So these are the things that I'm going to call errors, nuisances, and counts. So I try to avoid systematics and statistics. I'm going to talk about it later, but I try to avoid those. And then my feeling is that the same way that here in banff you have to close off areas so that wild animals Of areas so that wild animals can go and feed. There are areas where typically physicists are hunting, which is basically bringing, I mean, we always have to be bringing these into the known. We always try to figure out how to deal with these things that we know are there. And then statisticians are called to help when we already have some effect and now how can we model it, how can we deal with it. I just use the word model and different people understood different things. Model and different people understood different things. I'm sorry. Okay, so in this errors, nuisances, and counts, I just want to take one example. So this is, you know, you tend to think of counts as just counts. Unfortunately, they are not just counts. They've already gone through some calibration. And with that calibration come systematic uncertainties. So this is one aspect of it. So when you see these plots with numbers of events, there's already some. Of events, there's already some cooking applied. Now, much of this cooking is what we call statistically limited systematics. So, statisticians would call it statistics. I don't want to go there. But these are things that when we get more data, they tend to become smaller because we get a better understanding of the detector or because we have a better way of doing these extrapolations into these regions. Then there are the processes, and we heard a lot about signal and background. Heard a lot about signal and background, and I'm going to come back to that. And in particular, you have many different processes contributing, and some of them will have some theory input, some of them will be data-driven, and there's some sort of procedure bringing them there. So all of these things end up being affected by systematic uncertainties. And one of the things that I realized from some of the discussions is that I have. Is that I had with some of you is that sometimes it's not very clear what we are doing. So, for instance, let's take alpha s, which is a fundamental parameter in QCD. We use this data to measure alpha s, and alpha s is an input to the theory that we use to measure this data. Okay? So, what I wanted to make sure everybody brings out of this is that this process is reversible in the sense that we derive information about things that we use. About things that we use in the next step or the next steps of the next generation, but this is not all completely unfounded and crazy. In particular, this is one of the things that inside the experiments was hard to get people to understand. Using the exact same events to measure different things does not mean that these different things are correlated. It really depends on what information you're using. And our events, as we've heard, somebody mentioned the event space, there's a lot of different types of information. There's a lot of different types of information in there. So I've also learned that what physicists call double counting, as far as I understand, statisticians call double dipping. Clearly, somebody eats cookies and other people stay in the control room just counting events. All right, so coming back to the notion of signal and background, I've always found this problematic. I think that it's a useful shorthand, but it really clouds and obscures. And obscures a lot of things that you should keep in mind. So, let me give you the example of two vector balls on production and two quarks. So, at the quantum mechanical level, these two processes are indistinguishable. So, we cannot just go and say, ooh, I've measured this, or ooh, I've measured that. And in particular, these processes have terms that are called interference. I mean, it's, you know, a squared is b squared plus 2a star b. To A B. And you have to make some choices here. So I've just quoted, let's say, the controversial parts of this paper, because there's also a part where we just measure the total cross-section for this whole process. But the problem is that sometimes the interesting physical effects might appear in this diagram, but not on that one. So, now, how do you go about taking care of this? And if you do not think of these things as processes, if you start thinking of Processes, if you start thinking of this is my signal, this is my background, all hell breaks loose. Okay, so I think that one way of thinking about this is that there are processes that are very relevant to your inference. There are processes that are not necessarily so relevant, but you may want to constrain, otherwise they can have an effect on your inference. They can impact the power of the inference. And then there are other things which are different. And then there are other things which are different, and for the physicists, sorry, in the audience, pileup is not noise. They are just interactions that we do not want to care for. So noise is, you know, this is the kind of stuff point spread function, but pileup interactions that produce lots of soft particles that murky all the waters, they are still particles. So if you would like to remove them, you could just reconstruct them fully, and that would probably be the best way of removing them. Okay. Them. Okay, enough ranting about that. So we had a lot of discussion on this: what are alternatives? And for me, the summary is that there are things that we should probably call physically motivated deformations. Deformations is a terrible word, but I couldn't come up with a better one. So if you do know how to interpolate between different cases, and there is a meaning to the parameter, and I think that's that there's a physical meaning. And I think that's that there's a physical meaning to the deformation parameter. Then you can do these templates, there's the horizontal morphing, there are things like what is optimal transport to bring things from control region to signal region. So all of those things are physically motivated in a way. And then there are things which are really alternatives. And here I want to quote something that Bob said that I had not fully realized. Indeed, if at some point one of these alternatives looks crazy, we Alternatives look crazy, we are not just going to blindly use it. We are going to go and see what's wrong with it, and we'll try to figure out if there is a better way for that alternative to fit the rest of the data, and then we get a new separate alternative, but it will probably be closer to the data. There will still be alternatives, but it's true that if you have really big discrepancies, you ask yourself the question: what might be wrong with this particular generator or this simulation? Simulation. And then there are theory uncertainties. So theory uncertainties, it's a functional uncertainty. So basically what Frank said, and it's really difficult to think about. He said, you know, there are dialogues. There are all kinds of polylogs. I know that the functions have to be like that. We do know something about the structure of these functions. So I think the question for the statisticians, or the question I would ask, is. Or the question I would ask is: Is there a way of just saying I know what kinds of terms there should be and now construct something that could be used as an uncertainty out of that? So, yes. So, it's not like we are completely knowledge-free, that we don't know anything about what's coming in the next term. We do know what should appear, we just don't know how much of each. Speaking of theory, Speaking of theory, unfolding measurements is something that makes theorists' life much simpler because you're basically removing all of the detector effects, and then theorists do not need to make full simulations in the full phase space. And so it's very important to be able to unfold without bias. And I think that we've gone a long way, and I found these results very interesting. But then you could see the discussion are the Are the intervals central? Can I get the central value as well? So, all of these things, you know, they are coming out of the woodwork, and it's really nice to see. Then there was something that came up, and I think it was an interesting discussion, which was calibrators calibrate, astronomers, astronomate, right? And you do not touch each other's. So at the LHC, these things are all contained in one single These things are all contained in one single community, and we have had examples of things that really go bad in the calibration and then are found by the interpreters, and vice versa. And in many cases, I mean, this is just a rule of thumb. If the analysis has an opinion on the calibration, you stop, you think, and if there is some sort of genuine power. Power to do that, you go back to the calibration group, you make the calibration better, and all the analysis benefit from that. Which is one of the things that apparently in the other way of working, you can probably not do, because you've been given the calibrations, have a nice day. Now, we've never understood why discrete profiling worked, and it looks like we still do not fully. And it looks like we still do not fully understand why it works, but we got a good idea on what it means to have all of these alternatives. And what I took from this is that you have to have an opinion as whether you would like to do this sort of weighted average over the different alternatives, and what does that mean in terms of the measurement you're doing. So I get the impression that the magic of discrete. The magic of discrete profiling has not been cracked, so we still do not know exactly why it works. On the other hand, that is not uncommon with a lot of the things that we've discussed here. So why does Wilkes theorem work at the edge of the interval? There are lots of these regularity conditions that physics practice tends to contrary the regularization conditions of the theorems. And I think that there's a lot of And I think that there's a lot of interesting work that comes out of that because we force these theorems to become more general. Of course, this led into a discussion which was unavoidable because there are dozens CMS people in the audience of what is a spurious signal and this and that. And I think that what's important to take home, and if I remember correctly, it was Chad who said it himself, a statistician. Statistician, the point has to do with what is the uncertainty under the signal. And you know, under the signal, you know, if you have a distribution, you can always think of it in a weighted way. And both what CMS does with the alternative functions and the bias that those functions have and what the Spurious signal does, it's the same thing. It's trying to create something which is put into a gauge. Put into a gauge, it's compared to the statistical uncertainty under the signal. So, at the end of the day, the Atlas and CMS people will discuss a lot. And if you're a statistician, just sit back, relax, and enjoy them saying the same thing. Very heated. Something that for me was really, really nice to see was this systematization that Roger brought forth. And probably it's because I don't follow the literature, because he probably published this quite some time ago. Quite some time ago. And I think that this, so the work on the asymmetric uncertainties forced the understanding that there are these two ways of dealing with measurements. You can ex post facto combine measurement results, or you can do a combined measurement by combining all of the likelihoods. And here in the asymmetric case, it really leads to different ways. Different ways of thinking about the problem. And in particular, then I okay, I asked the question, but there's a big effort to try and preserve the likelihoods in a simplified manner so that then people can do other types of inference. We change the model, what happens now. And it seems clear that if you are in the PDF space, there are a lot of methods that can be used to preserve the PDFs. If you are in the likelihood space, my understanding was you do a My understanding was you do a tailored expansion, and we're sorry, that's all that there is to it. Now, you've seen there that I've mentioned OPAT versus APAST. So I really do not like OPAT, so I invented something even more horrible, which is APAST. And I just wanted to show, because this OPAT keeps on being repeated, and OPAT is bad for my health, my personal health. And so this is what we do with likelihood. And so, this is what we do with likelihoods. And here you have the same example of this diphoton analysis, mass measurement analysis. So, what we do is with the full likelihood, you do a likelihood scan. So, this is the likelihood scan as a function of the mass hypothesis, and you get the total uncertainty. And then you freeze, you fix all of the nuisance parameters to their maximum likelihood estimate, and you get something which is the dashed lines that we call the statistic. That we call the statistical only uncertainty. Okay, so I'm trying to provide a dictionary for, you know, if you hear a physicist talk about these things, this might be what they mean. And then the, let's say, the dark arts of this is that in order to estimate the systematic uncertainty, you don't. You just say it's the thing that in quadrature, if you subtract the statistical from the total, is left. The nice thing about it is that you can now go and release groups of nuisances or one at a time. You usually do it in groups because they are related to certain physical effects. And then you see what the difference becomes. And this is how we try to quantify the impact of nuisance parameters on the uncertainty. So yes, a path, apast, you know, it's all equally horrible. These uncertainties These uncertainties raise the question, which is not, there's no inductive reasoning and recursiveness here. It's just how certain are you of the uncertainty that you assigned, that it does not go all the way to infinity. And this was very, I really like this. And I'm going to ask again the question. So basically, if you have heavier tails, you can allow something to be discrepant and ignored. To be discrepant and ignored. This is my takeaway message. The thing is, fine, for theory uncertainties, all the experimentalists can gather and say, oh, that theory uncertainty is terrible. Let's assign it. Everybody assigned that one of these uncertainties on uncertainty. What I was wondering is inside their own collaborations, what will physicists think that is their Physicists think that is their uncertainty on their experiments' uncertainty versus what they think is the uncertainty on somebody else's uncertainty. So, this is something which is not completely clear to me what's going to happen. There's a lot to understand, but there's a lot of very nice potential here. And I could see one of the theorists very seriously studying this method. Okay, something which I don't have a conclusion on. Something which I don't have a conclusion or anything like that, but I just get a feeling of a theme, of a recurring theme, which is to create hierarchies of observables. And I was not aware of this in the Bayesian world, to give you a notion that you are in control by controlling smaller things that might be easier to go and calibrate. So, this also came up in the Let's Make Detectors Automatically talk, that you might not be able to. That you might not be able to optimize for the discovery of a certain particle, but you can more easily optimize for the resolution on the measurement of a certain particle property. Alright, now oh dear. Sorry? No problem. That's very good. Within Reese. With Reese. It's almost over. Uh yes. Well, it's gonna be another five to ten minutes. Five to ten minutes. Estimate. No central value. Last mile corrections. So there's a big effort in the experiments to correct the estimates for different particle properties. But many a time you go to a particular analysis that is doing something so sharply, so carefully, that you need, you know, this extra. Extra sprinkles of chocolate in your cake to get the cake to be perfect. So, this is another, this is the same example of this diphoton analysis, because it's also the one I know best. And we started by having some properties, shower properties, like the shape of a photon in the colorimeter. And they differ by, I don't know, 10 to the minus 2, but in the shape. But that is enough to get the MVA to be suboptimal instead of actually. To be suboptimal instead of actually doing the best job it could do. So we would like to really be able to calibrate these shapes on the data. And we have beautiful Z to electrons data, and electrons and photons, you know, to some extent are exactly the same thing. So we started by, we take the distribution, we compare what is the simulation and what is the data for the Z decay, and we multiply by a factor and we smear with the gauge. We smear with a Gaussian. So we had to invent a whole technology of how to do deterministic smearing. So that was interesting back 15 years ago almost. And since then, there's been quite some improvement. And I was very excited this week when I heard that there are ways of having multi-dimensional CDFs. So that's something that I've been looking for in another context in the WMAS analysis. In another context, in the W mass analysis. So, for those of you in Atlas, I worked in the W mass analysis. That's why it's so late. But this is what we have now. So, what we have now is this chained quantile regression. You take each one of these variables that you want to use and you correct it, you include it, you correct the next one, you include it, you correct the next one, and so on and so forth. And you can see that this allows to reduce the uncertainty that we assign a systematic uncertainty. That we assign, the systematic uncertainty that we assign to the estimation of this particular variable, quite a lot. So, this is really, really, I think it's very important for the experiments. That's why I've spent so much time in this slide. And the other thing that I've always thought that this was going to be resolved by some form of optimal transport. It's not completely clear, and I'll get back to that on the ABCs. Then, when we are searching for things, Things, this kind of work is really cool, especially because it produces one global p-value. And for all the people who complain about the loss of power, I prefer that. One p-value, no wondering about what's the trials factor, and so on and so forth. The issue I see here is that beyond the standard model physics, it's either very subtle and already among us, and what we can probe, or it Or it is out of our present reach and is going to appear as outliers. So, this was the conclusion of the discussion, well, my conclusion of the discussion we had. And so, picking up on what Michael then said, you know, there's a whole theory for that. And, okay, I thank you for your word. So, I was wondering if there is a way of being able to look for new physics based on outlier estimation. Because, you know, if I see Estimation because you know, if I see a certain point over here, I can see that this blue thing is going down. So, you know, it will keep on going down. So, this is an extrapolation problem, and I understand it's a completely different type of problem. Right, so talking about bump searches, so there was a bump search, and we found we climbed that bump, and we made a measurement at the top, it's very tall. So, it's very So, it's very good. And these were all humans, but then we had a whole section about how machine learning can help actual intelligence. So, it's good to know that, at least now in the experiments, 90% of the cases are agreed to be just loss of optimality. This was definitely not the case a few years ago. There were, you know, huge discussions about what is the systematic and the. The systematic uncertainty on a function that you determined, and it's never going to change. So, this is good. The problem is now the other 10% of the cases. And as Bob pointed out, ABCD is an excellent playground to look into this. And in particular, so I had the opportunity to discuss with some of you all of the density ratio versus optimal transport. And it could be that there is a way of And it could be that there is a way of hybridizing the two to get the best of both worlds. So I think that this is similar to the last mile calibration in a way. It's just a very different application because when you're trying to estimate the background, it's a different leak. So more on this machine learning. There are lots of other applications that can be had. And I called it machine learning. Perhaps I should have said statistical learning and then I would, I don't know, make more. Learning, and then I will, I don't know, make more people happy in the audience. I think that the point is, I was happy to see that people agreed that this should be support tools. And what I have, I think the important thing is that we are able, we are smart enough to include, let's say, the physics symmetries, bake them in, and at the same time, be able to give the machines the ability to go and look outside the box. Look outside the box because we end up programming what is the loss function. So, if we put a loss function that is going to constrain what we already know we should constrain, and for instance, one recent detector design that has changed a lot is the CMS tracker for phase two. And instead of having pieces of detector that are either like this or like that, some of them are tilted. And then, of course, this comes to the interactions that we heard, can it be mechanically built and all of those things. And all of those things. But we have to be creative enough to let the machines go wild and then say, oh, you good machine, you are wild, go away. Okay? But at least they get the opportunity. And we don't have to just imagine things ourselves. We let them do some of the imagining and then we do the coding. All right. This is something which I think is very important for this community. We really need these minimum working examples. It was kind of ironic. Ausara said, it will take me a long time to go to the open. Said, it will take me a long time to go through the open data. And then Tommaso said, it took us one year to reproduce our own experiments analysis with open data. So getting these things to go, it involves work. And so, Tom, you set this up. Thank you for getting this going. And yes, there are many more areas. It's not just YOHE. The LH is just one corner. Sometimes it's dark, sometimes it's sort of well understood. Sort of well understood. I got the feeling that if you are a Bayesian and you want to have some appreciation, you know, you may want to go somewhere other than the LHC and then come to the LHC and say, look, it works over there. You should also be doing it somehow. So it's a feeling. It might be more welcoming. Perhaps it's just my bias. So I think that Sean Pipla was very exciting. I've never seen this. I'm still not sure if it was just collectively hallucinating. And it's not a goodbye. It's definitely an until next time. So I'm not going to go through all of those points over there. I just want to ask, can we please make sure that Slack does not go away after the trial period expires? Because, okay, we can talk the technicalities. And the other thing that I was wondering is look. Is looking at the premises here at the Banff and Beers, I get the impression that they are not equipped for the kind of intensive hall lecture style discussion in this room. They are set up for a different kind of thing. I mean, I'm not saying we should go and do paintings, interpretive dances of our results, but perhaps we can have some preparation during the, you know, the, not lectures, the seminars, and then have some. The seminars, and then have something which is more of let's get together around the table and work on this problem that has already sort of been pre-prepared. And ooh, seven. Not so bad. Thank you very much. Thank you. Thank you very much, Andre. That was very stimulating. We're going to break for coffee in a couple of minutes before Michael talks, and then the discussion will. Talks and then the discussion will be open for everybody to say what they feel and advice for future, suggestions for the future. Any specific questions for Andre now? Or remarks on what was wrong, so I can go and correct it and then upload something saying this was wrong. Where was that peak? Yeah, it was a mountain. It's just here. It was the sleeping bicycle. Okay, we go. Okay, we go. So I had a comment on the slide where you talked about the outliers and the fails. So something I just realized here is that, okay, so we talked about robust statistics and that's a whole field of statistics. But there's also this field of statistics called states of extremes. We actually have one of the experts here who is Anthony right here. So I wanted to ask Anthony, do you think there is a potential use of studies of extremes for these tales? In the sense that there is this term status. Sense that there is this kind of standard model that you kind of observe parts with the extreme tails. And maybe you can, you know, using the kind of techniques that you know about, kind of figure out how that should behave in the tail. And then if you see something in the tail, you might be able to say, okay, this doesn't fit this kind of thoughts. Do you see any scope or something? Oh, no, probably. But I mean the principle extremists as a strategy. Principle extremists is about extrapolating assumption of regular variation. So you would need to preferably have some theoretical knowledge assumptions. In that case, yes, which I think we have here. I think there is some knowledge about how these tails are. I mean, you at least know they're supposed to go down and maybe at some kind of a rough rate. So I think there is some knowledge about how this tells you. And just to say a little bit of caution, right? That's for us in terms of predicting the background, that's exactly where it gets the most challenging, right? Because we make sure we can validate our procedures in regions where we have statistics and things like that. There can also be extra contributions that kind of come in. I'm not saying we shouldn't think about it. I'm just saying that it's somewhere where on the physics side there would also need to be quite a bit of work to ensure that we had it adequate to backup. So I don't think we're going to. Oh, I don't think we are going to claim a discovery because there are two events that are at 100 well, not 100 TV, but I don't know, 8 TV. So that's... I mean, even just, you know, does your slope really go all the way correct, exactly the same shape as you extrapolate cost where you have data? That's tricky. Yes, so what I wanted to gather from the status. To gather from the statisticians, is there a way of doing that that is not just, I don't know, power law versus exponential versus exponential of polynomial sort of thing? If there is, we can try that. Then I also, I still agree that drawing conclusions is always going to be easy. Yeah, I think there's not actually there is, and that's actually the extreme value status. There's a small family of possible limit laws, hidden things or smooth. things are smooth behavior fails but they but and then you know there are there are energy limits on the effort and then I think Heather's point is there are many ways that they can be spooked yeah um I I just want to mention something uh additionally for all this systematics discussion here Or all the systematics discussion the idea. Ladies, to my mind, in particular physics, we are often not rigorous in the sense that if you have a user's parameter, you only use a lighting ratio and doesn't product for one sigma. But you often do actually, our estimation is often based on crude goodness of fifth tests. So I've seen this for instance: when you have a jet energy calibration, yeah? Then you might have some control. Then you might have some control, but our double ratio plot Monte Carlo over data should be around once after calibration. And then you plot this versus 50 bins versus beta. And then you say, ah, this all lies within the band of 2%. This is the difference between data and not the colour. So I assume who understood this parameter 2% uncertainty globally. That's a scale factor. And I think we are often imprecise. I think this has been improving over the years, but I underlay maybe better than you. But I underlay maybe better than me. I mean, that's what I'm saying is we speak that there's an error this way, and I think in LHG, it's not possible to have this always these fitted users' parameters, right? I think it is always possible. And so, for instance, this comes back to the red band that Sarah should not look at. And then I asked our Atlas colleagues if they have a reference on how they construct, I think at some point it was 49 News parameters to describe jet energy. Jet energy uncertainties. The resolution and the scale as far as I've got. So it is possible to do it, and then some people are going to look at it. Oh, by 2%, we're very good enough. In some analysis, they will be good enough. Yes, and that's something we are not always rigorous on the sphere of music with merge kind of