And today I'd like to tell you about joint work with Bodhi Sand. And the title is a mouthful, but I'll explain all of these terms as the talk progresses. And let me just say, as a preview, essentially I'll talk about a new way of dealing with nuisance parameters in large-scale multiple testing. And by new, I really mean that David Cox had anticipated this 50 years ago. But I'll come to this in a bit. So the starting point for this work. So, the starting point for this work, just as a lot of other innovations in statistics in the past or in this century, have been motivated by throughput biology, where a lot of data analysis can be tasked as item by item testing. And I'll give a concrete example in a bit, but let me just say for now that this interaction has really been beneficial for both the science and the statistics. And perhaps this is really most exemplified by the Benyamini-Hoffbert method for false discovery rate control. So, this really has. Control. So, this really has impacted a lot of the statistical research in the last 25 or more years, but it also has impacted how science is conducted or how discoveries are made in some fields, such as throughput biology. Now, simultaneously with the Benjamin Mini-Hoffpert procedure, there's actually been additional statistical innovations, at least within this biostatist community. And actually, these procedures, I would argue, are not as well known as the Benjamin Hooker procedure. So, what these procedures are. So, what these procedures do, they essentially furnish the p-values that are then used by the name in comfort. But the way in which these are constructed is perhaps unconventional. At least, in particular, despite these procedures being so successful in practice and very widely used, I'd say they're not as widely known within the statistics community, and definitely they're not as widely discussed. So, the goal of this talk will be to look at one of these procedures called Limma, and that. These procedures called Lima. And that's basically the, I'd say, the first one. And it's still very widely used. And what we'll try to do in this talk is to develop a non-parametric framework that essentially includes limma. And our goal in doing that will be to generalize it and also to justify it and provide a formal theoretical framework for thinking about this. And the basic idea is actually very simple. We'll pretend that the nuisance parameters are IID, and then we'll proceed with what we call empirical part. Proceed with what we call empirical partial unbase. So, just to give you a sense of the types of applications where these approaches are used, let me just give one example of an application that follows the paradigm we're interested in in this talk. So in this study here by Zheng and colleagues, they collected, that was a very relatively small study, just had three human subjects. And for each of these human subjects, the T cells were isolated. Cells were isolated, so, or some T cells. So, these are blood cells involved in the immune system. And then these T cells were split into two parts. So, first naive T cells, so you can think of these as resting ones. And then essentially, these cells were activated. So, their immunological response was activated. And the question of interest was, how does methylation change after activation? So, methylation refers to a chemical modification that lies on top of the DNA sequence. Lies on top of the DNA sequence, so it's not a modification of the sequence per se, it's just a chemical modification on top of it, but it's very important for regulation and other processes. So this is just an excerpt of the data set. So we just have six columns here. So we have six samples in total, three humans and two activated and naive cells. So for example, the first two columns refer to the first human subject, and the second one would be after activation, and the other one. Be after activation, and the other one is before activation. And each of these rows corresponds to one probe. So that's one specific location on the genome on which we measure the methylation expression. And I'm just showing six rows here, but actually this data set has over 400,000 of these probes. So to abstract this away statistically, I want to introduce this methylation expression matrix, but a lot of studies in high-friend biology can capture this. through biology can capture by this type of structure. And essentially, our data can be summarized as an n times k matrix, where k here would be six, these are the samples we have, and n would be here the number of methylation probes, which here would be about 400,000. And our goal is to find abstract statements to find interesting probes. And at least for this work, interesting means that the mean expression changed after activation compared to no activation. That's essentially our goal is to find Essentially, our goal is to find these delta i, without just binary indicators, of whether we think the probe is interesting or not, and perhaps work your follow-up study. And perhaps the most common paradigm, or one very common paradigm, if I true good biology for figuring out what's interesting, goes as follows. We look at each row of this matrix individually. So we look at, let's say, the data for the JFCR, and then we use that data along with very classical statistics. Along with very classical statistics, in order to compute a p-value, and we compute a p-value for each row, and then we'll use these p-values together in order to make our final decisions as to what rows are interesting. So let's make this very concrete by actually showing you how to compute this p-value in this specific study. It's actually very simple. So the data set for our I-th probe consists just of these six measurements here. And recall that the first two measurements. And recall that the first two measurements are the ones from the first human subject. And we have the naive cell and the activated cell. So if we want to do some kind of t-test, we might do a pair t-test to account for correlations between measurements of the same human subjects. So I'll just compute this difference of after activation and before activation for this probe. And then I can do the same thing also for the other two human subjects. And then perhaps the simplest thing one could do is take the average of these pair differences. Average of these pair differences, compute the sample variance of these pair differences as suitably standardized, and then compute the p-statistic, which is just this average divided by essentially the square root of this sample variance. And if we're under the null hypothesis, where the expectation of these pair differences is zero, and under standard normality assumptions, then the statistic follows a p-distribution with two degrees of freedom. And of course, then it's very easy to compute a p-bunding, let's say a two-sided p-bunding. P-value, let's say a two-sided p-value by doing a p-test. So, in this histogram on the left, I'm showing the 400,000 or more p-values we get by applying the procedure I just showed you on each row of this matrix. And based on this histogram, we'd like to come with some decisions of which probes are interesting. And as I said earlier, perhaps the most common way in genomics is to use the Bengal-Mooney-Cochford procedure to control the false discovery rate. And if we apply the Bengal-Mooney-Cochford procedure, And if we apply the Bengal Mini-Poch procedure here at the level of five percent for discovery rate, we end up discovering only a single probe. So, if we look at the histogram, it seems consistent with actually quite a bit of signal in the data. We spent resources in doing the experiment, but at the end of the day, we only got a single probe, or at least the study got a single probe. So, the question is, can we do better? And also, let us try to dissect what actually happened with this very simple test statistic we actually used. Have to use. So the test statistic we use is just the t-statistic. And let me write it as a ratio where the numerator is this z, that's just the sample average of the differences. And under normality assumptions, this will be normally distributed, centered around a parameter of interest u i and with variance sigma i squared. And then we divide by Si, and that's the sample variance. And who is aware of this? This is basically clear, it's gay of chi squared with two degrees of freedom. Square with two degrees of freedom and also be proportional to scale by the actual variance. So essentially, because even though we don't care about sigma squared, we just care about ui, but we still divide by si because we don't know sigma i. And of course, if we want to do inference, we somehow need to account for the unknown variance. So, what I'm saying here is, of course, very traditional. And it's essentially just the statistical problem of nuisance parameters. And what's more, this is perhaps the setting where we. And what's more, this is perhaps the setting where we have a really good solution, right? The t-test. But nevertheless, in this talk, I want to argue that even in this very simple setting, actually, the fact that we don't know the nuisance parameter counts at a very large cost. So essentially, here we're dividing by this SI. And here, in this example application, we just have two degrees of freedom. This means this SI is very volatile. And of course, this volatility translates in terms of very heavy tails of the T statistic. And then The t-statistic, and then this may reduce our power. And in fact, beyond just a theoretical argument, we can actually see this play out in the data. So, the single discovery we made with the Bengamini Kochware procedure actually is the one with the smallest sample variance among all 400,000 probes. So, the question is: can we do better or can we somehow treat the nuisance parameters in a way that graphs can avoid this issue? And the answer will be yes. So, and the answer will be yes. And we call this empirical partially based multiple testing. And essentially, the idea will be that we will share information across probes for the nuisance parameters. And while before we looked at each row for each probe individually and computed a p-value, now we'll look at all the rows of the matrix in order to compute our p-values. And then in the end, we'll still use the Benjamini Kofbert procedure. So let me formalize this statistically. So essentially, So essentially, I'll assume that whatever data we started out with, or whatever the data matrix was to begin with, that we summarize each row into two numbers. So the two numbers are Z and S squared. And you can think of this as basically being the numerator and the denominator of the P statistic. And under substantial generality, let's say under some homoscedastic normal linear models, we may be able to reduce our data into this form. And essentially, the first one, the ZI, we assume it's a bias for a parameter of enter. Assume it's a biased for a parameter of interest in ui, and it's normally distributed. And then we also assume that we have an additional independent measurement that's the s squared, or it's a sample variance. And this will be unbiased for the sigma i squared, but also be follow the scale chi-squared distribution with new degrees of freedom, where I'm assuming we know these degrees of freedom. Okay, so that's basically collapse the data set into two n numbers. And the goal will be to test another hypothesis whether the mu i or Whether the are equal to zero. So this would be the expected difference in the metal, for example. And we also want to control the post-discovery rate. And the challenge here is that n is very large, so in our application 400,000, but the degrees of freedom will be very small. So in the application, the degrees of freedom were two, but you can think of typical applications, they not have, may have maybe degrees of freedom that are 10 or maybe 20, but not more than that. So that's the regime which we're interested in. So that's the regime which we're interested in. So again, I'm repeating here the modeling assumption on our data here, or on our summarized data. The parameter of interest is mui. The nuisance parameters are the variance. And the basic idea of the approach will be that we'll try to borrow information across the different probes for the parameters we actually don't care about, right? So for the variances. Just to give you a hint of why this may be. To give you a hint of why this may be possible, here's one possible starting point that actually it may seem like a toy assumption, but it's actually very commonly made in the scientific literature. So that assumption would be that all the variances are actually the same. So if all the variances are the same, you can imagine that you can estimate this common variance very accurately, let's say by averaging the sample variances for the different I. So essentially, it's almost as if you know the variance exactly. And in that case, you could compute a p-value basis on the. Could compute a p-value based on the standard z-test with a known variance. But actually, this assumption, even though it's commonly made, it's easy to see that for commonly used data sets, it's violated. But still, we'd like to have some assumption that allows us to borrow information across I. And the way we'll try to do that is through what David Coggs called partially Bayesian. So we'll make the following structural assumption. We'll assume that all the variances are IID from some distribution G. From some distribution G. So, and this assumption here is how we link the different problems together. Now, you may have seen a similar assumption in a lot of the Bayesian literature on computing different types of predictive p-values. And conceptually, these are very similar. But what I'd like to highlight, and I'll come back to this during the talk, is that the interpretation is a bit different. So, in our talk, really, this G, we'll think of it as being the frequency distribution of the unknown nuisance parameters. Nuisance parameters. And I mentioned that I'll pay particular emphasis to this approach lemma that's really successful and was cited tens of thousands of times in this biological literature. And essentially, it makes exactly the structural assumption I made, except it makes a parametric assumption. So it posits that the G, we had a ball takes this parametric form. That's the conjugate prior under the scale time squared sampling model. And it depends on two unknown parameters. And it depends on two unknown parameters. So instead, our goal will be to look at the non-parametric generalization of this. And in the start, I'll proceed in three steps. So I rewrote the sampling model over here, the likelihood, and then I'm making this extra assumption that I'll call star, but all the nuisance parameters are IID from this distribution G. And in the first step, we'll assume that the star actually false and that we even know this distribution G. And then I'll ask, what should we do? How should we proceed? Ask what should we do? How should we proceed? And this will be the partially based part of the thought. In the next step, we'll assume that star holds, but that actually the distribution g is unknown. And what should we do then? And this will take us to empirical partial debates. And in the last part of the talk, we'll ask, what if star does not fall, but actually all the variances or nuisance parameters are fully deterministic, then what guarantees can we still get? This will lead us to the compound decisions. So let's look at the first question. So let's suppose this IID sampling assumption falls and that we know the prior G, then what should we do? Then actually David Cooks in this 1975 paper that has really few citations actually gives the answer to this question. So he says, optimum tests and competence limits are based on the conditional distribution of Z given S squared. So let's try to formalize why that would be the case. So this is a very simple result to prove. So suppose Simple result to prove. So, suppose g is known and it's not degenerate, by degenerate, I mean it's not the pointless, then essentially in this model here, the only unknown parameters are the mu i because we can integrate out of the sigma i squared. But it turns out that still the whole pair z and s squared is minimally sufficient for the single unknown parameter mu i. But on the other hand, it's also easy to see that sigma i or s i squared, it doesn't depend on mu i at all, so it's ancillary. All right, so essentially. Ancillary. So essentially, this configuration where we have some statistics that are jointly minimally sufficient, and one of them is ancillary, is one where the conditionality principle of classical statistics is in full force. And what the conditionality principle is saying is that we should base inference on the conditional distribution of z given s squared, exactly what David Kroff said in the previous slide. So if we want to compute the p-value, then we should compute the tail probability condition value on s squared. Value on S squared. And by iterated expectation, actually, we can write this p-value as follows. So within, we have this conditional expectation. Within it, we have just the tail probability if we actually knew the variance sigma i. But now we actually take the posterior expectation of this given s squared. And the one thing I want you to really notice here is I use this notation with a subscript g. And that's because this expectation actually depends on the distribution g. And in fact, if we were to And in fact, if we were to integrate, if we were to keep sigma i fixed, then z and s squared would be independent. But if we integrate out g, then actually z and s squared do become dependent. And one property that will become quite useful later on is that if we're under another hypothesis, so if mu i is zero, then these partially based and the whole data generating processes, as I described it, then these partially based p-values are conditionally uniform. So these p-values are uniformly distributed. So, these p-values are uniformly distributed, but in fact, this uniformity holds conditionally on the sample variance as squared. And this has practical ramifications, and I'll come back to this later. Okay, so this is just the first step. So, if we knew G, then we now know how we should do the testing. We compute these partially based p-bands and use these for downstream steps. So, what if G doesn't know? What if we don't know G? So, it's unrealistic that we would know the frequency distribution of the. We would know the frequency distribution of the parameters. So we'll do empirical Bay, which is a very powerful framework for learning from the others, introduced by Herb Lobbins. And essentially, the key idea is that if we're facing many simultaneous and related statistical problems, then as an empirical Bayesian, we can mimic an oracle Bayesian that knows the true distribution here. And as one example, going back to this Lemma model, which has this parametric form for the prior with two unknown parameters. form for the prior with two unknown parameters essentially what what what happens then is that this these two parameters are estimated by the method of moments or or maximum likelihood and essentially we'll do also maximum likelihood here but in this non-parametric setting so in our case this g is specified fully non-parametrically so here's how we'll try to estimate this so we'll construct an estimator that's just a function of all the n sample variances so not the n measurement So, not the end measurement Zi. And based on this, we'll do non-parametric maximum likelihood. So, here's the formula states: fix any distribution g tilde. Then we can ask what's the marginal density of the sample variance if we integrate out this distribution g tilde. Or another way to say this is that we take the scaled high-square density and we integrate with respect to a non-sigma squared with respect to this distribution g tilde. With respect to this distribution g tilde. And we can compute, evaluate this at each of our n sample variances and then take the product. And this basically defines the likelihood where the parameter is a distributional object, so a distribution g tilde. And it's well known that we can actually compute this objective over all possible distributions g tilde. And the maximizer G hat is called the non-parametric maximum likelihood estimator. And it's a discrete distribution. And I'm showing this here for the And I'm showing this here for the example from the introduction of the matignation data. So this prior is actually discrete. So I'm showing here the different points of support as well as what mass is assigned to each of them. So you have this very discrete structure. And the only thing I'd like you to notice here are the following. So first of all, the support points vary over multiple order of magnitude. So definitely we cannot assume that all the bearings are the same. On the other hand, we do see some structures here. Hand, we do see some structure here, and maybe the structure could help us also in terms of power. And also, let me say that I wouldn't trust this picture too much because the minimax rates for estimating g here basically scale as one as some polynomial in one over log n. So this may be very inaccurate, but still at least gives us some sense and is consistent. So, given this perhaps very inaccurate distribution g, here's what we'll do. We'll do the simplest possible thing you could imagine. So, what we'd like So, what we'd like to compute are these oracle partially based p-values that, however, depend on the unknown distribution g of the nuisance parameter. So, all we'll do is when computing these p-values, we'll plug in the g hardly estimated by non-parametric maximum likelihood. So, we'll just use the plug-in principle here. And these are the p-values we'll use downstream. And we can prove the following quite strong result. So, suppose the G and G paths have support that's bounded away from zero and infinity. Bounded away from zero and infinity, then actually in L1 error, we can estimate the true p-value or the oracle partially based p-values with the plug-in principle at in terms of this L1 norm at the rate that's almost one over square root n. So up to a logarithmic factor. So that's almost a parameter, almost parametric rate. And in a sense, it's very strong because, as I said, G-hunt is a very inaccurate estimate of G. But nevertheless, we get very accurate approximation of the p-values that. Accurate approximation of the p-values themselves. And as an immediate corollary of this result, actually, we get the following result. So, if we fix one test or one hypothesis that's actually null, then if we look at this empirical partially based p-value, and even if we condition on all the sample variances across all the 400,000 methylation probes, then still conditionally, this p-value will be approximately uniform. And actually, it turns out that even the rate at which The rate at which this decays is also this almost parametric rate as well. So, let's implement this in our data example from the beginning. So, in the left plot, I'm showing all these p-values. So, we use this empirical partially based principle to compute all 400,000 p-values. And then I'm showing two additional plots that are stratified histograms. So, in the first histogram, I'm showing what happens if we look at the p-values. Among hypotheses, the sample variant was in the bottom 10%. Sample bearing was in the bottom 10%. And I'm also showing the p-value. So my hypothesis sample bearings was in the top 10%. And in all cases, we see that at least the tail is uniform, corresponding to the true knowledge. The reason I'm showing this is because I want to show the contrast to the t-test, which as we saw had almost no power, but also in fact leads to near conditionality properties. So the left plot is again the histogram of all the p-values with the t-test. Of all the p-values with the p-test. But now, if we look at the p-values where the variances are small, then the t-statistic gets inflated. So all p-values are very small, and we barely have any uniform tail for large p-values. On the other hand, if the sample variance is very large, then essentially we get no p-value at all with the p-test that small. And so essentially, this is where the conditionality comes into play. So we don't care. To play. So if we don't care at all about the sample variances, they just capture some nuisance aspect of the problem, then this kind of behavior may be problematic. And another way to illustrate exactly the same thing is here I'm showing contours of the p-value functions. So let me elaborate a bit more. As I said, for each of the probes, we collapse its data into two numbers, the ZI and the SI squared. So essentially, these green points here. Green points here, but that's a two-dimensional heat map, or you can think of this as a histogram of the different hypotheses and what values of z and s squared they get. And then, first of all, the gray line, that's the contour of p values equal to 0.005 with a p-theype. So, if we were to reject hypothesis with p-value below 0.005, we'd reject everything above the gray line and symmetropy below zero. On the other hand, this purple line is the On the other hand, this purple line is the same thing, but with empirical partially based p-values. And what you can see is that when the sample variance is quite small, then the empirical partially based p-values, they're more conservative, so they make less rejections compared to the t-test. But on the other hand, as we move on to larger sample variances, then by sharing from across hypothesis, we are able to move the threshold down quite a bit. And this translates into more discoveries of the empirical functionality based upon. Empirical functionally based approach. So, to summarize again how the approach works, we start with this full data matrix. And essentially, we use the information across all rows in order to compute these empirical partially based p-values. And then we apply Bengham Lee Holford, and we get quite almost 550 discoveries compared to just a single discovery with a p-test. And it turns out, under the previous assumption, and the requirement of the effect size of the non-zero mu i that's common in the multiple testing literature. Common in the multiple testing literature, then we can actually prove that this procedure controls the false discovery rate asymptotically. So that finishes the second part of the talk. But now you may argue that we needed to reassume so far that this is our halt. So we made a very strong exchangeability assumption on the variance of sigma i squared. So what if actually these sigma i squared, what if they're actually not random and IID from a distribution G, then can we still get some kind of frequency? Then, can we still get some kind of frequency guarantee? And here, I really like to use this quote from Erinsman Hauerlingen on what he calls the empirical base principle. So, the idea is the following, or here's how he phrases it. Let us use a mixed model, even if it might not be appropriate. So, here, maybe we posited that the sigma i squared are nid from an unknown distribution g, but let us study what happens if this model is not actually appropriate. And it turns out that several of our results, both either Several of our results, even if all of these variances are actually deterministic, but we pretend they're not and run the whole procedure I described. And to prove results in this case, we'll build on techniques from compound decision theory. And for example, the previous theorem on asymptotic FDR control still holds. So asymptotically, we control the false discovery rate even if all the variances are deterministic. So let me just give you one example simulation of the approach. So we'll draw The approach. So we'll draw here. We'll actually draw the variances ID from a distribution G, and then we'll draw the actual effects from this two components here. So we'll have a point mass at zero with mass 90%, and the rest will be normally distributed. And then, of course, the sampling model is as before given by standard normality assumptions. And here will vary both the distribution G and the degrees of freedom. And our sample size will be. And our sample size will be just 10,000. So, this corresponds to the 400,000 in the application. And we'll consider three types of p-values: so, the standard p-test p-values, oracle z-test p-values that have knowledge of the individual variance of sigma i squared, as well as the empirical partially based p-value. And we'll apply Bingham and Copper to control the false discovery rate of 10%. So, in one of the simulation settings, we'll actually make all the bears. Settings will actually make all the variances be the same, so all variance are one, and so the prior is just a discrete point mass. But of course, the method doesn't know that. So in this case, if we have two degrees of freedom, the t-test, of course, controls the false discovery rate, and it controls it in pilot samples, but it has essentially zero power. On the other hand, if we actually knew that the variances are all one and we could do z test, then that post-discovery will still be nine percent, but our power would be up. Percent, but our power would be up 50 percent. Now, the empirical approach and the base approach doesn't actually know that all the variants are the same, but it automatically learns it from the data. So, it ends up having the same power as the oracle approach and controls the false discovery rate. And as we increase the degrees of freedom, essentially these two methods still do the same. And as we increase the degrees of freedom, the power of the t-test increases steadily. So, that's one example. So that's one example. So, second example, we have some more heterogeneity. So, here we have two clusters of different variances, and the performance of the t-test and the oracle z-test are the same as before, but the empirical quantity-based t-test still is able to use some of the structure to substantially improve the power compared to the t-test. Right, so to conclude, empirical base presents a powerful framework for learning from others, and we really get new opportunities from these. Really, get new opportunities from these large-scale inference problems that were not available in class-scale statistics. For example, here, how we're sharing information across the different usence parameters. And this talk talks specifically about empirical partially based multiple tests. And we have a prepared archive that connects this work more to the related literature and also provides a lot of additional theoretical results, applications, and examples. So I'd like to conclude here and thank you all for your attention. 