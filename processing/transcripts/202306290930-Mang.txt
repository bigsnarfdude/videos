And welcome back to the uh I think day day number four, right? All right, so we are uh most of us are back from the uh walk in the park. Yesterday, anyway, it is a pleasure to welcome my fellow Houstonian, Andreas Mang from the University of Houston. The talk will be about the shape classification through the lens of geodesic flows of diffeomorphism. Please. Okay, thank you, Dimitri. Thanks for the organizers for having me. Thanks for the organizers for having me. So, I'm going to talk about two things we've been doing for a couple of years now. So, the talk will be split in two parts. And there will be actually some connections to things you've seen. And in particular, there are nice connections to ideas in uncertainty quantification and machine learning based on optimal transport. So, there is actually a rich mathematical structure underneath what I'm going to. Mathematical structure underneath what I'm going to present you. And but this is very niche, though. So this application I'm looking at is very niche. Okay. So what are we trying to do ultimately? We're given a bunch of brain images and we're trying to assess them in some sense, right? So one example you could think of is learning something about Alzheimer's from changes in morphology, so morphological appearance. Is the morphological appearance of your brain? That's traditional. Tons of people have done that stuff. But, you know, just think about some neurodegenerative disease of your brain that affects the structure of the brain. You give me a bunch of brains and then you ask me, is this patient diseased or not? What is the prognosis? And can you do some prediction? That's the general realm I've been working in for quite some time from all sorts of angles. Now, to come back to the title of the talk, there is a rich mathematical framework that actually allows you to quantify differences between shapes and brains are shapes. And the idea is pretty neat. What you do is basically, so I take two images, A and B, and I want to measure the distance. And you can think of different things, right? One thing you can think of is that One thing you can think of is that you have a mother brain, and from this brain, you can create all other brains by applying deformations. So you just move things around in space, and this gets you to another brain. So now, conversely, I can think of the problem that I'm given two brains, and now I just have to establish how I can deform one to the other in a minimal way. And right then, the minimal distance is the metric. Distance is the metric that describes how much energy I need to go from one to the other. And that's a geodesic. And in our case, this will be a geodesic on the manifold of diffeomorphisms. And that's the title explained in general. Of course, now you can argue. So this is mathematically neat. I'm telling you, I'm going to classify brains and I'm going to do it with just one number. Maybe not, but there are other ways and other things. But there are other ways and other things you can derive actually from these computer diffeomorphisms that allow you to classify and get some idea. So how do you do that? It's a very old problem. It's been around for ages. You take two pictures, whatever those are, in my case, it's always brains. I don't know why. And then you establish point-wise correspondences. So the question you ask is. So, the question you ask is: a point given in one image, where does it occur in the other image? That's the question. That's what I'm trying to solve for. Mathematically speaking, you are given two functions. You can view images as functions in the continuum, compactly supported on some domain. And then you try to find a plausible map such that when I apply it to one image, I, in exact arithmetic, would get exactly. Arithmetic would get, you know, exactly, but that's an ill-post-inverse problem. So we're going to approximately get the other image. That's the problem. And now the question is, what's a plausible map? And that depends on applications. Tons of people have worked on tons of ideas. Of course, biophysics makes a ton of sense if you think about organs, right? So they have maybe hyper-elastic properties. That's what people have been doing. In our framework, plausibility. In our framework, plausibility will be diffeomorphic transformations. It's an ill-posed problem, and I think I really love this as an illustration for inverse problems because it's very neat and very intuitive. So again, the task is I'm given a matrix in essence, right? So I'm given a picture, I'm given pixel values, I have a value, and you ask me from an individual value to reconstruct a vector, which already tells you how difficult. Which already tells you how difficult that actually is, right? So, these are these correspondences. So, that's one way of looking at this problem in terms of ill-posedness. Even if you restrict yourself just to very simple transformations like rotations or translations, you can't distinguish, for example, this translation from this rotation if I give you two rectangles. So, even here, if you're really into analysis and math and inverse problems and proving existence and uniqueness. Improving existence and uniqueness, it already is a huge pain here, right? So, um, then coming back to the notion of plausibility in our setting, again, I'm trying to find this vector field that takes me from one picture to another, given just intensity values. And of course, if I have a five here, nobody prevents you from saying, okay, there's a five there. Let's just assign this mapping. And actually, in fact, you can get the minimal mismatch. In fact, you can get the minimal mismatch for any given image. There is even a paper about this. If you just take the intensities and sort them according to their value, and then that's the best you can do. And the transformation is non-meaningless. I mean, it's nonsensical. It's not plausible. So in our case, we want to prevent this folding from occurring. And this brings us to diffeomorphisms. This is what I do most of the time. So we're going to formulate. So, we're going to formulate this as an ODE or PDE constraint optimization problem. So, I'm going to introduce some model, some simulation that deforms my image. And then I prepare, compare the data set to my proposed deformed image. It's an inverse problem, so I regularize. You've already seen that we work with geodesics, so regularization is not necessary in our framework, not only. Necessarily in our framework, not only motivated from an inverse problems point of view, but actually from this idea of having metrics in a remaniance setting. And yeah, and then you have this constraint. That's one thing you can do. Something else I like doing is not just solve it for an individual point, but then do statistical inference where you learn something about the posterior distribution of your problem. And then this may even allow you. This may even allow you to get away with issues with non-convexity and actually find the true solution. And then, of course, what you can do as well, which I always get when I present something, use machine learning and replace everything I'm doing, which is fine too. So that's another approach you can use to solve these types of problems. And there are many smart people in this room who work in this area and do very interesting things. Okay, so. Okay, so going to the actual model, and this is not based, you know, there's nothing I've contributed here whatsoever. It's very smart French, mostly French people that came up with these sorts of ideas. And it's even older, right? I mean, if you're a fluid dynamics person, you can date this back to Arnold in the 70s. So, what you do is you minimize some distance. I already said this. I'm going to transform my image. Going to transform my image and compare it to my other image. I have some regularity, and then I have an ordinary differential equation as a constraint. And this is the main ingredient here. And the idea is actually really neat. So generally speaking, when we started, I said we're going to search for vector fields, transformations. What I'm now going to do is make my problem actually more complicated. My vector field is a function that varies in space. But now I'm going to say, okay, I'm going to introduce a pseudo-time variable, one image. Introduce a pseudo-time variable, one image lifts at time zero, the other image lifts at time one. And I'm going to model my transformation y as a time-dependent function. This opens up a rich framework, again, related pretty much to fluid dynamics, where you compute characteristics. So, if you're a fluid dynamics person, you have a point here, and then you compute how this point flows to end up at another point in space. And then the neat thing here is. And then the neat thing here is I parametrized it in terms of this smooth vector field, which is the velocity. And if I make my velocity sufficiently smooth, I can prove that this equation is well posed and gives you a diffeomorphism of a certain smoothness class depending on the smoothness of your velocity. In this context of optimal control, we have a regularization that allows us actually to prescribe these smoothness properties using. Prescribe these smoothness properties using some appropriate Soberlev norm. So that's the way how you implement it. And then, tying back to the initial thing I started with, this length here, if I can compute the shortest path on the manifold of geodesics of diffeomorphisms that connects these two pictures, I can use this velocity I used to generate this as a kinetic energy or view it as a kinetic energy. So I can apply. I can apply a differential operator and integrate, and then I get a kinetic energy. And this allows me in a Riemannian setting to compute the differences between these two images. So that's the general idea. And then the simplest thing I can do is just compute all distances to all data sets, build a matrix, and vomit my favorite classification tool at it. And we've done that. And I'm going to present one result. And I'm going to present one result on one slide. The other thing, which I was hoping to be working on more as we, I mean, I was hoping we have some results for this already, but that's not how science works. What you can do as well is instead of comparing all images to each other, you can think of given the population, can you actually give me a brain that represents this population on average, right? This population on average, right? So I give you a thousand healthy brains, and then I ask the question: what's the average brain of those guys? There are easy ways to do this, right? You just select one that's on average, you know, in the middle somehow using some metric. The other thing you can do, you can solve a very complicated optimization problem where you try to find this average image and use this model of diffeomorphisms between the images. So you search for the mappings. So, you search for the mappings and then create an average image. And probably a better way of doing this is again moving to Bayesian framework and do this in a statistical setting. This immediately maybe raises some alarm bells for you because this is insanely computationally heavy, right? So, how are you going to do that? Why would you be able to do that? And that's what I'm hopefully going to be able to convince you now: that we actually have developed tools. That we actually have developed tools to tackle these types of ideas. The first thing is work with colleagues at the University of Houston. Actually, it's a project that Dr. Arsenkott and Dr. He have been working on for quite some time, and they brought me in, so I'm really grateful. And these are the two students that recently contributed to this work. As I said, the talk will be split in two parts. The first will be this. Will be discretized, then optimized, and the other one will be actually optimized and discretized. What this means is I have a variational problem. I formulate it in the continuum. Then I can either decide to discretize it and then differentiate my discretization, or I can use calculus of variation, come up with the optimality conditions, and then do that. And we're going to do both. It's going to be a little bit different now. I'm not working with images. I'm actually going to work with shape. I'm actually going to work with shapes extracted from images. So you have to segment something first. You have to give me a shape representation. So, in computer vision terms, a triangulated surface of the thing you are interested in looking at. And then we are going to work with that. And that's actually neat because then everything is a little bit geometric. So, actually, closeness between structures is intuitive because you can look at how close meshes are. How do you? How do you parametrize this? There are tons of ways of doing this. We're going to work with point clouds. So we're going to parametrize our shapes as point clouds. It's neat, and it actually enables a rich connection to tons of things you can do in also machine learning, because we will ultimately end up working in a reproducing kernel hilbert space. From a computational point of view, it's a pain, but that's why. But that's why I guess I'm in this project. So, this is the discretized optimization problem, and I'm going to give you all the ingredients. So, that's my distance. The shapes are vectors, right? So, points in R3 in our case. I have to discretize my kinetic energy, and then I also have to discretize my ordinary differential equation. So, this is pretty simple. You can open up your favorite optimization textbook and then solve this. As I said, As I said, we're going to model velocities in the reproducing kernel Hilbert space. You have to select some kernel function. For us, this will be Gaussian. You can do other things. And then this allows you to parameterize your velocity as an expansion of some coefficients. So now you go from an infinite-dimensional optimization problem to an optimization problem with just a few coefficients. I mean, not a few, but a bunch of coefficients. But a bunch of coefficients, and you can solve that, and then you can somewhat represent your regularization operator as a quadratic, which is neat. And then shape distances, you can model this so you have these points, and then you can use direct measures. And this leads you in the end, actually again to something that ties this to reproducing kernel Hilbert spaces. So these are kernel distances between these point clouds. So you have Gaussian. Distances between these point clouds. So you have Gaussians and you measure the distance using basically evaluation of Gaussians. And it's an all-to-all, so all point talk-to-all points. You can do smart things here, but there is a ton of work. And then similarly, we use just first order Euler time integration. I replace the velocity with this parametrization, and then I can write my constraint as this and solve it. What we do is What we do is actually a splitting algorithm. So, this is ADMM. But you can date it back to actually what we like to do work from Glovinsky because he was an esteemed colleague of ours. He did a ton on splitting type algorithms. So, we solve the problem where we take care of the constraints and the kinetic energy. We get updates for our shape that we transform and for our coefficients. Then, you solve a second problem. Then you solve a second problem, and then you have the consensus so that things don't diverge. And then this is how it works. I found this actually kind of interesting in some sense. So the velocity, and there are issues here. So you have to decide on, since you deal with exponentials, what's the bandwidth or the scale of your parameter? And this is actually true for the distances and for the velocity space. And it's neat that for the velocities, it's actually kind of insensitive. Kind of insensitive. So, this is a parameter that allows us to select how big the sigma is in the exponential for the velocity, and things converge more or less well. This is all MATLAB prototype implementation stuff, so there is tons of work to be done to speed this up. Just to give you an idea, these are mitral valves, so there are things in the heart. When your heart pumps blood, these things open up and close, and we look at how they deform over time. How they deform over time and how they compare to one another in order to classify actually heart disease in patients. These are two shapes of two different patients. As you can see, they're quite different. And our method allows us to match them well. We can also compute local strain values, which help us in our classification. So you can specify how the tissue deforms on the surface. And then this is how the diffeomorphism we compute. Diffeomorphism, we compute looks, and then this is my machine learning results. I mean, it's pretty standard. We do random forest classification, so I don't want to go into the weeds of how to do that. This is the confusion matrix. So, we basically reach an accuracy of 97% for this data. Okay, switching gears, going to images. We match to some ground cool, yeah. Yeah, yeah. The clinicians have actually identified the disease in the patients, and they tell us these guys have the disease. So it's a we look at regurgitation, which I think means the thing doesn't close properly anymore. So there are some issues with emotion. So, and they can actually see this in the images very well. So, it's a little bit a toy case scenario for us, but the clinicians are actually excited. Clinicians are actually excited about what we are doing, but that's the idea. So they tell us this guy has it, this guy hasn't, then we train and then we do prediction. Thank you. Okay, so now moving to images. So for me, this was actually, you can blow this up, but this was a low-dimensional toy problem. Now we're going to work with medical images, three-dimensional volumes. This is standard clinical data sizes, 256 to the 3. And I'm going to tell you that we can solve these problems, these PD constraint optimization problems in roughly three seconds these days on one GPU. And then you can also consider humongous data sets. So we actually deployed this to a multi-GPU framework where we can use 256 GPUs to solve things for pictures that are of size 2048 to the 3. You may wonder why. I'm going to give you an example. And you may wonder why. I'm going to give you an example why, but there is still a question: why. And you can download this stuff and play around if you get it to compile. I can help, hopefully. Tons of people. These are my students. These guys did a ton of work on the GPU implementation. That's the most fabulous postdoc advisor you can have on earth. And Miriam is PhD advisor of MyTel. Uh, is PhD advisor of Martel, and he was my postdoc advisor, so I can tell from experience. Um, we do now optimize, uh, then discretize, so this should actually be highlighted, which means we compute variations and then discretize that. It's a little bit different now. The ordinary differential equation I had to compute the veomorphisms is now replaced with a PDE, it's a hyperbolic transport equation. It's a hyperbolic transport equation, so we have reasons why we like to do that. And then you can also deform the image. The difference now is we don't see our diffeomorphism any longer, we just invert for the control directly. So the velocity is now our control variable, which was the case in the former case as well. But the state is now transported intensities versus diffeomorphisms. And that's the formulation. It's exactly the same stuff. This is a little bit different. And then, of course, the constraint is different. And then, of course, the constraint is different. And then you have to design different algorithms, unfortunately. We did more things, and actually, a bunch, but what we ended up in our source code, which is the default thing, is we penalize the divergence of the velocity. So this comes from Stokes flows. If you have this to be equal to zero, you will generate a diffeomorphism that doesn't introduce any volume changes. Doesn't introduce any volume changes. So it's incompressible flow. This is too restrictive. So we penalize this slightly. This helps us a ton in a ton of regards. And then more recently, I made one of my students suffer by introducing additional PD constraints that allow us to actually guarantee that all the velocities we optimized for actually are. Four actually are velocities that correspond to geodesics on a manifold of diffeomorphisms. And again, this ties back to actually work Arnold did in the 70s. Optimization, if you want to be cynical, gradient has to be zero. That's it. That's what we're doing. Of course, we have a bunch of things. We have state variables, m. So these are the transported intensities. That's my control. Transported intensities, that's my control, that's the velocity. I introduce a Lagrange multiplier to deal with the constraints, so I also have to deal with additional unknowns. And the more constraints, the bigger this gets. We like Newton methods. That's the associated Newton system. So now you have to solve this linear system. These are the updates for all individual variables. This is, of course, humongous. So these are all in general space-time. General space-time. And then we work with three-dimensional data. So that's a pretty big system you need to solve. So you can't just use backslash. You can, very simple, just use the sure complement of this, and then you call this a reduced space method. The difference now is we don't iterate on all variables at once, but we just update the velocity. The price we pay is we have to solve a ton of PDEs. Which is translated in action here. That's the gradient for the velocity. I have to solve my forward problem, and I have to solve my adjoint problem. If you do machine learning, forward propagation, backward propagation. Hession, again, as I said, pretty textbook. So if you're an optimizer, there is nothing fancy going on here. Hession. Hessian exactly the same structure as the gradient. I have to solve two PDEs every time I multiply my Hessian with a vector. And that's what we're going to do. We work in the matrix-free setting. We're not going to form any matrices. We're just going to apply the hash in to a vector and then use iterative methods to invert. But every time we hit a vector with the hash, we have to suffer by solving PDEs. So to evaluate the objective, you have to solve one PDE to evaluate. You have to solve one PDE to evaluate the gradient. You have to solve one PDE and the adjoint, and then for the hash, two PDEs. So you better solve them quickly and don't solve them very often, which means you have to think about preconditioning. Running out of time, I guess. Time integration, we use semi-Lagrangian methods, which are a hybrid between Lagrangian and Eulerian. Gonna skip how this works. I'm going to skip how this works. The Hessian is, so this is the spectrum of the Hessian. You can see that the eigenvalues or singular values decay rather quickly if you don't regularize. So the data term has maybe low rank. And then as you keep adding regularity, you shift the spectrum. It's a compact operator. So large eigenvalues, smooth eigenvectors. Smooth eigenvectors, and as you go down in your spectrum, oscillations become more of an issue. Preconditioning, standard stuff is spectral preconditioner, which means I use the inverse of my regularization operator. Remember, Sobolev norm. So this is a differential operator. So I have to compute the inverse of, let's think of this as a Laplacian. It's more complicated, but let's think of this as a Laplacian. We can do this very efficiently because we use a spectral discretization. So we just do Fourier transform. Discretization. So we just do Fourier transforms here. So this is actually unbeatable in terms of times of application. So this is for free doing this. However, the hope is that we need to do it very often because it's ineffective. One thing we've done, amongst others, is use a zero velocity approximation. You can derive the expression for the hashing if you use velocity zero and it becomes very simple and all the PDE drop out and you basically have something that's more or less analytic. More or less analytic, the inverse isn't, but the evaluation of the Hessian just is a Laplacian plus the outer product of the gradient of my initial images. You still have to invert this. We do this iteratively using PCG, and then we also did some multi-grid type things for poor people. So, just to show you that. So, just to show you that it works, we do this for brains. It converges since we use Newton-type methods in roughly 10 to 14 iterations, and then you don't see much change in the mismatch. You can still see a ton of change in actually the control you compute. Question is, do people in the application care about this or not? We were actually hoping they would, but they didn't. Of course, if you compare this to gradient descent, we did this. We did this actually gradient descent in the sober lif gradient descent, which is in the you apply the inverse of the regularization to the gradient. We converge much more quickly, as you may expect, but of course, each iteration for gradient descent is much cheaper. But still, gradient descent looks something like this. This is basically all you can see on the internet. This is basically how you can see all the iterations progress. That's the image as it deforms. And here you can see mismatch. White means no difference. Black means a lot of difference. And as you can see, after nine iterations, roughly we're done. We get good accuracy. This is for a ton of, not a ton, 16 images. This is something that clinicians can provide us with. If it's one, it's excellent. We are at roughly 84%. So that's pretty good. So that's pretty good. We compared this to others. The results look neat. We get nice diffeomorphisms, we get smooth Jacobians, we get somewhat smooth velocities, and we get a good mismatch. And we can do this very quickly. And that's the last thing I want to talk about, and then I'm done. 64 to the 3, these are the speedups. This is a CPU implementation. This is a GPU implementation, normalized for energy consumption. For energy consumption. So on a GPU, we're basically eight times faster using the same amount of energy for 64 to the 3, 128 to the 3, 20 times faster, and for 256 to the 3, 25 times faster. And then, as I said and promised, if you do a bunch of tricks and store and increase your memory and you store a bunch of variables, you can go down to under three seconds to solve the entire problem. And then. And then, lastly, you can do this for humongous data sets, and they actually do exist. So, there is this clarity data, which is mouse brains, which is of size 20K by 20K by 1000. And then the question is, do you want to work with these data sets or not? But we have developed code that allows you to work with them if you have a big enough computer. We didn't solve it until this resolution. Until this resolution, but I've, you know, 24 to the 3, we can easily do. And this is one example where it's for 1024 to the 3 roughly. And it gives us some results. Thank you for your time and attention. Any comments, questions? Thanks for the talk. I think it was slide 64, more or less. I'm not sure about the exact number. You showed a video of a transformation. I think it was. Yeah, I didn't quite get what the initial and final images were there. Maybe it's not exactly 64. Yeah, maybe 63. Yeah. Yeah, no, but 64 is where it shows. Actually, I think. Actually, I need to fix this. This runs backward in time. And every time I see it when I give a talk, I'm annoyed that I haven't fixed it yet. It's easy. It's just anyway. This is the three-dimensional volume in its original configuration before I deform it. Yes. This is the difference between this guy and the guy I'm trying to register. Okay, which I'm not seeing here. You're not seeing it. Exactly. And then, but you can guess. Yeah, I have one minus the other, but exactly. And this is from top. Exactly, and this is from top, this is from front, okay. This is from the side, it's a three-dimensional brain volume. Okay, here you can see the difference, and you can see that it when it involves evolves basically after you know one or two iterations, this goes basically matches quite well. It's actually interesting. So, you know, from the mismatch, you would think after one or two iterations, you're done. But as I mentioned briefly in one of the many sentences I said, Sentences, I said, there is still a ton going on in the deformation, right? So you can actually see there is a ton of deformation still happening in this area. And the question we actually were interested in and looked at at some point in time is how long do you actually need to optimize until this stalls, right? And then you can look at how your control updates change over time. And what you observed is not great because you basically converge and you keep iterating, iterating, iterating, and then all of Keep iterating, iterating, iterating, and then all of a sudden, again, it kicks in a little bit. You still, you just optimize the regularizer in the essence, the mismatch stays flat. And then the question is, what does it mean, right? I mean, how good is my prior? And at this point from the application point of view, do you need the distance or actually the map? What's relevant here? I need both. So actually, as it turns out, in the applications we look at, In the applications we looked at, the distance so far for the things we've done is not extremely helpful because, as I everything collapses to one number, and you can imagine that's not super helpful. But if you look at these pictures, that's why I briefly mentioned this. We actually compute the local strain. You can think of this ballpark as the amount of volume that's changed locally. In some sense, this is a little bit different, but and if you then look at this local. But and if you then look at this local information for this particular application, that's actually really helpful. And for this, you need the diffeomorphism. Thank you. Thank you very much for the talk. Super interesting. I have many questions actually, but I just was just wondering, can you connect these to image registration, like elastic image registration? So would it be like a different model for the dynamics? Model for the dynamics instead of stokes, you have? No, no, so typically, elastic image registration operates, doesn't introduce any constraints other than, I mean, so there is no dynamics. It typically works for the map directly. There is linear elasticity, which dates back 1980s. People have started to look into that. Then there came fluid dynamics, which is actually the grounding thing for this, where they introduced this as a regularizer, and then this motivated this one. And then this motivated this work. 2013 and a little bit before that, I just know the 2013 reference because it's a friend of mine. Jan Modositsky and Lars worked on hyperelasticity. There were others. I forgot their name. Hyperelasticity is neat because, I mean, it's neat in the sense that it guarantees the hyomorphisms. You have to compute the fiomorphisms in order for this to make sense. Elasticity does not. Elasticity does not guarantee you diffeomorphisms unless you add additional constraints. Hyperelasticity, if you look at what they've done and the implementation and the amount of work, it's insane. I mean, it's really, I mean, it's not, but they did finite elements, was really complicated. And then actually to guarantee that numerically they get diffeomorphism was also a huge challenge. So that's really interesting work they've done. And then Lars actually worked with me on this when he saw I was doing that. Lars would. I was doing that last word or from Emory, and he loved it because it's so simple. Because you just have to integrate these ODEs, and actually, you get the forward, so the push forward to tie it to normalizing flow and the pull back by just integrating the equation forward in time or backward in time with velocities of difference. So that's neat. You immediately have access to the inverse, which in general, the inverse map is hard to compute. Thank you very much. Maybe I can ask. Thank you very much. Maybe I can ask one more. So, can you also use this technique for something like shape optimization problems? So, you don't have to minimize like a distance between two functions, but you maybe have a different goal, let's say, a different functionality you want to minimize that depends on the shape. I don't know. I wonder, I mean, I've seen people talk about shape optimization and wonder that myself, but never found the time to look into it. Yeah, yeah, yeah. I just don't know enough about the topic. Yeah, yeah, I just don't know enough about the topic. Yeah, also, thanks for the talk. Um, I have a question about when you solve the system with the hashin. Um, can you maybe go back to the slide? I don't know if I understood this correctly, but do you... This or this? Yeah, maybe. So do you actually have to build the hash and no? Okay, you just use the application to the vector. Exactly. Right. To the vector. Exactly. Right. And so then my question is: is there any chance to maybe approximate this with just finite differences when you have problems with the hashing? Because then you could use something like a Newton-Krudov solver, which is Newton-Kronov. Oh, right. So you just approximate it with finite differences then? No, no. We use... So that... Ah, okay, okay, okay, okay. No. You're in a super high-dimensional space. You're in a super high-dimensional space, and you use finite differences to approximate the hash, and you have to do this for each parameter, right? So, I mean, you use gradients, so you have to evaluate a number of unknown gradients. That's not what I want to do. Sure, but it's just in the direction of the vector. That's why it usually kind of works. Because you only have the approximate application on one vector? Yes, sure. Vector, yeah, sure. But that's why I would do it a ton of times to get a good approximation to your hash, right? If you do it with, I mean, maybe I misunderstand. No, what I mean is that every time you apply the hash to a vector, you can you could, I don't know if it works, but that's what I'm asking, you could just approximate this application with finite differences. And that usually works better because you don't have to approximate the whole gradient or the whole hash in each possible direction. Hash and in each possible direction, but just in the direction of the vector. What you're saying, and there's actually better convergence guarantees for this in literature under some conditions. Yeah, I'm happy. I mean, I'm happy to look at it. Okay, sure, yeah. I haven't thought about it. Point it out to me. I mean, I've nobody, yeah, never seen this. Right, a quick up, please. Somewhere near here, you showed that you have a splitting method, and then it looks like you're updating all of the variables simultaneously rather than the sort of ADMM style where you update a block this time. And you elude it there. Yeah, right, right there, the three by the big three by three block matrix and you said something. I don't know the difference. Okay, sorry, sorry, sorry. For the first problem, we do splitting, right? And then we do indeed update the variables. The variables individuals fix one, then you update the other. Okay, this is not splitting. Oh, okay. This is just so this is called full space methods. The general, right? So if you look at your problem, this is the simplest one. If you look at your problem, I don't know the velocity, right? That's what I'm seeking. If I'm given the velocity, I can compute m, but generally speaking, I don't know m. I have this as input. I have this as input, that's the data, that's the data. I have this guy and this guy. That's all. So, this is an unknown. This is an unknown. And then, to deal with the constraint, I introduce a Lagrange multiplier. That's another unknown, right? So, I have three unknowns in this case, which are space-times fields. And then I update on all of these. It's not a splitting algorithm. It's the most basic thing you can do. Open Not Sadal and write chapter six, I think. And write chapter six, I think. And then a little more philosophical question. So, I can certainly understand if you have a patient breathing or their heart beating, why there should be a diffimorphism between one point in time and another. But when we're mapping one person's brain to another, and I don't know really anything about brain anatomy, even though I work on medical imaging, is it reasonable to assume that there's a diffimorphic transfer? No, okay. So, as far again, I'm not a clinician. I think I picked up at some point in time that actually on Point in time that actually on the outside of my brain folding pattern, some structures may not be. I'm not 100% sure. That's what I wondered. Um, I mean, it's a model that's been around, right? And you can also see, I've done stuff like that where people actually, Bruce Fischel, you know, you go through two spheres, right? And then, you know, you do, it's a homeomorphism to a sphere, and then you start comparing things, which in essence is a similar assumption than here. Assumption than here. So I think it's a powerful tool. There are neat questions if it's non-diviomorphic. And actually, Nicolas Chavon, who's joining us soon, worked on implementing in shape space things where you try to think of something like this match to this. And he grows basically. So my understanding is it should tie a little bit to something that's called morphometry. Little bit to something that's called morphometry where you can match really strange stuff to one another. Okay, so there's people thinking about this, and it's actually the holy grail that I get always approached for by clinicians because I've done a ton of tumor stuff as well. Right, there you go. And then you have an image where there is a tumor, and then the patient undergoes surgery and they take out a chunk. And then the question is: at the boundary, can you say something about what the differences are? And that's something they really want to know. Something they really want to know. There are ideas, but it's really difficult, it's a mess. Thank you. Particularly to this example, can you just assign an energy of like if there is something that is not there and doesn't have a diffomorphic connection to another brain, that you give it a penalty and try to keep the energy again as low as possible? Yeah, there are stuff. Jan Modositsky, for example, has they have done local rigidity where they actually added. Where they actually added, then you need a pre-segmentation, though, right? You need someone. I mean, it's a little bit stupid. In essence, we're always after something that's completely automatic. But then, if you had someone just do one click in your image and tell you, here's a tumor, don't worry, different ballgame. So they put a mask, for example, on top of structures where they assume nothing is reliable here. I can't do any matching. And then they impose rigidity, which is basically the incompressible. Basically, the incompressible. No, it's different. So they fix the Jacobian to one, and then things are just allowed to translate and rotate. That's it. So that's done. I mean, it's not solved, but you know. Quick question. At some point, you mentioned that you added a constraint as a student assignment by imposing the velocity. It's a PhD thesis. Right, but the velocity is on the manifold. What is the motivation? What is the motivation there? If you don't do that, I mean, what kind of okay? So when you set out in this problem and in this realm, you always, the motivation I gave was, right, I compute this. Yeah, I wanted to go to the picture. Yeah, that's that. So you compute this geodesic, right? I compute the shortest path. The problem is that most. Problem is that most people use gradient descent and they optimize for a few steps. I mean, we're not much better with Newton, but anyway, you optimize somewhere and then you get some geodesic. And then your claim is this is really the one that is this metric. It's the minimizing geodesic, but it's not. It's just some, it's a non-convex problem. You compute just something, and then the things match, and then it's fine. Now, what you can do is... You can do is you can think of adding constraints such that you get better solutions. So, you want to rule out something. And this comes just from computing the you have this Soberlev norm. So, you have, say, Laplacian times velocity in a product. And then if you compute the optimality conditions for that, so the Gateau derivatives for that, you can show that this leads to these equations. That this leads to these equations. So we basically integrate constraints that come from the principle of requiring that these velocities actually are just velocities that generate geodesics in this manifold. And that's the constraint. And this allows you to compute better solutions. To be frank, I thought the equation is really complicated. Let's try to solve this. That's where I came from.