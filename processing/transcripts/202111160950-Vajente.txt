I would have loved to leave it there, maybe the next time. So, thank you for the invitation. And we'll have a little bit of a change of topic, change of pace, and what we're going to cover. Because what I want to try to explain today is how we can and how we are trying to use machine learning and big data techniques. Big data techniques for what concern the gravitational wave detectors. So, not really data analysis or improving the searches or anything, but how to use gravitational machine learning to improve our detectors. So, first of all, let me briefly explain the basics of how our detector works. Most of you are probably familiar, but it's always useful to just start with a common understanding. So, the current advanced gravitational wave detector. Current advanced gravitational wave detectors are interferometric detectors, meaning that they're based on a laser interferometer. It's a modified version of a Microsoft interferometer. So as you can see in this schematic here, you have a laser and a beam splitter that splits the beam into parts. And the two arms of the max interferometer are augmented by fabriperoresonant cavities with the goal of, in some sense, increasing the circulation time of the. Increasing the circulation time of the photons into the four kilometers long arms, such that they increase the sensitivity to gravitational waves. And then to reach the sensitivity we need to detect gravitational wave, we have to also use other tricks or other techniques like a power recycling cavity and a signal recycling cavity that are used to recirculate the laser power into the system or to increase the response to the signal of the laser light. So, of course, this added complexity. So, of course, this added complexity comes with a price, and the price is that you have to keep all the mirrors and all the distances between mirrors fixed to a very high accuracy to maintain the proper restaurant conditions that would actually increase the sensitivity of your detector. Also, you know that we have discussed already yesterday and today how sensitive we need to be to detect the gravitational waves. So, of course, our mirrors could not sit on ground. Could not sit on ground because the ground is moving by microns on a time scale of a few hertz while we are trying to detect signals that produce a displacement at the level of 10 to the minus 18, 10 or 10, 10 to the minus 18 meters. So there are a few millions order, I mean, a few orders of magnitude there that we need to actually reduce the motion of the ground. So our mirrors are suspended with seismic. With seismic isolation systems, active and passive seismic isolation systems, which basically are based on the idea of pendulums. So each pendulum, there are multi-stage pendulums. As you all know, the payload that is suspending at the bottom of the pendulum moves way less than the suspension point at frequencies above the resonance. But this, of course, comes at the price that you add even more complexity and then at the resonances and at low frequencies. Then, at the resonances and at low frequency, the mirrors are still moving quite a lot in normal conditions. So, the problem, the main problem here, is to be able to actually control these instruments, bring all the distance between the mirror from a random movement that happens at the initial state to the final operating condition when everything is controlled and maintained within a very narrow range around the operating part. And so, this is the problem. The operating part. And so, this is the problem of sensing and controlling the motion of the interferometer. So, the way we do this is by an extension of the laser stabilization technique that has been developed by Pound, River, and Hall decades ago. It's based on essentially imprinting a modulation, a radio frequency modulation on the laser beam, and then demodulating the optical signals in order to optimize. The optical signals in order to obtain a measurement of the relative phase between different fields into interferometer. The details are not important, but the key here is that this is a very sensitive technique, so it allows us to have very high sensitivity and low noise measurement of the motion of the mirrors. But this measurement works well only for a very narrow range around the operating point. So, this plot you see at the right-hand side here, the top. Right hand side here at the top shows what is the response of these optical signals for a motion of one single resonant cavity. So you can see in the zoom inset that there is a linear response of the optical signal to the motion of the mirror only for a very, very narrow region around zero, which is that red box you see there. And out of that, the response of the optical signal is highly non-linear. And this is a problem, of course, because we have a system that is moving a lot. We have a system that is moving a lot because of the seismic motion. And the seismic isolation introduces also long memory in the system, in the sense that you cannot easily predict the motion of the system unless you know its history for the last few seconds. But then the only readouts we have of the actual position of the mirror are highly non-linear functions of the actual physical degrees of freedom, physical states. And so, if you want, the fraction of the total phase. So, if you want, the fraction of the total phase space of the motion of the mirrors where we have a linear control that is possible is less than 10 to minus 9. So, you can see that the first problem we have here is that we can't just sit there and hope for the random motion to really bring the system close enough to the point where we can implement a simple unit control because we would wait essentially forever. So, there's one. And so there's one problem here, which is to find a way to actually actively drive the system to this condition. And this is what is called the lock acquisition. Essentially, how can we go from the initial state where all the mirrors are free swinging by distances that are way, way, many microns, so a million times larger than the actual linear range? How can we find a way to deterministically drive the mirrors to the operating? The operating point where we can then engage a linear controller. And this is a non-linear control problem. And as you all know, every time the word non-linear comes into play, things get orders of magnitude more complicated. So this is the first part I would like to discuss. So how can we use machine learning to actually implement some sophisticated or robust lock acquisition strategies? Then, of course, once you manage to do that, and once you have a strategy to bring the system. And once you have a strategy to bring the system into the operating point, then we are in a regime that we can call the high-sensitivity operation. So it's where the detector is actually producing data that is sensitive enough to detect gravitational waves. So here the system is almost linear, but of course we have very demanding requirements on the sensitivity. So even small nonlinearity, small residual nonlinearities can be important. And so one place where machine learning and data techniques Learning and data techniques can be useful: is how to look at the data coming out of interferality in this configuration and try to improve it by implementing noise subtraction or implementing techniques that could improve the sensitivity. And this is the second part that we'd like to cover today. So, let's move to the first part about lock acquisition. So, as we said, we, you know, even in the most simple cases, we have The most simple cases we have five longitudinal degrees of freedom to control. We have tens of signals that are derived from optical sensors, but every one of the signals is first a mixture of sensing all the possible degrees of freedom. And also, as I explained earlier, the response is highly nonlinear. And so it's hard to invert a non-linear system and to get the physical motions out of the optical signals. So the current approach. Optical signals. So the current approach is based on essentially trial and error and experience, and there's a lot of historical reasons. And we ended up with a semi-deterministic method, which is based on triggering on the power levels on different diodes and using auxiliary lasers. And it's rather complex and it's really tuned and tailored to the system we have now. So the problem is that every time you change something in the instrument, it takes quite a long time to make this. To make this algorithm work again. And it's not easily scalable to the future detectors. So there's some motivation here to try to find a more general and robust way to approach this problem. Then maybe you might ask, why do we care about the lock acquisition? Why do we need to improve? Well, the point is that the time, the typical time that advanced lag detectors takes to recover from lock loss, so when you lose control of the Lock loss. So, when you lose control of the system, you go down and you have to bring it back to the operating condition, it takes about 30 minutes. So, during the last run or three, the typical downtime of the detectors due to this time needed to bring it back to operation is 12%. So, it might not seem a lot, but remember that we just released the last GWTC tree, so the last catalogue of our Three, so the last catalogue of our transients, and we have about 90 events over the entire history of observation or advanced LIGO. Mean the fact that 12% of the time was down means that essentially we missed order of 11 events due to the reloc time. So you see, this means that improving, even reducing by half the time we need to bring back the instrument or making the instrument more robust means just easily getting more events. Easily getting more events without any other observation. So there is really some science there because you know 11 events after 90 is not an equigible number of events. So again, we have a non-linear control problem here. So how to drive the system into a narrow region of its phase space where then we know how to control it in a linear way. But this is a non-linear problem. So the typical approach. So, the typical approach of machine learning for control problems is what is called reinforcement learning. For those of you that are not really familiar with this, the idea is that you have what is called an agent that has some sort of policy. So you look at the value of the different inputs, the different error signals. And based on that, you want to derive what is the best action you can do at that particular moment to try to obtain some sort of reward. And the reward in this case. Sort of rewards, and the reward in this case is would be to be able to bring the system and maintain the system close to the opportunity point. And the idea of reinforcement learning is that there are techniques where you can just let this learning system work and poke your interferometer, and it will slowly learn how to control it. The pros is that this is, in theory, very powerful. So, it can be completely independent from any model of your system. You can just let it. Of your system. You can just let it run with a real machine and see what happens. And if you look up, you know, there's a lot of cases where people manage to use this, but typically with video games, because computer scientists love to play video games, so they love to teach their own computers to play their own video games. So, but Google managed to build AlphaGo that learned to play Go from scratch without knowing anything about that. Our problem is a little bit different because, of course, Because of course, we cannot let a machine learning technique run and play with our instruments for a couple of reasons. First, because it's very dangerous. If the learning, the reforceable learning algorithm do something very weird, it could damage the system. And then also, it takes typically millions of attempts to try to learn something for enforcement learning. And we cannot afford to let this system. Afford to let this system play all the time for a million of attempts, which would be months on the system, because at the end of the day, it would be much faster for us to just try to patch something together based on what we have done so far. So, this is why the reinforcement learning is a little bit problematic to apply in our case. Although we have models, so we could, in principle, imagine to train our reinforcement learning system on models. On models, and then see how this can be applied to the real system. A different approach that I am proposing and I'm working on is actually to construct a non-linear state estimator. So for those of you that are familiar with linear systems, what you would do when you have a linear system is to first build an object, an algorithm, a procedure that takes all the input signals you have and gives you an estimate. You have and gives you an estimate of the state of the physical state of the system. And by state, we mean the position of the mirrors in our case. If the system were linear, there's a lot of theory. The Kalman filter, for example, or the Wiener filter are examples of linear estimator, state estimator in the case of a linear system. There is no such a thing for the most general non-linear system. And so my idea here would be to use a deep neural network, which is in theory a universal. Which is in theory a universal function approximator. So the idea is: let's take all the error signals we have, give them as an input to a neural network, and see if the network is able to predict the position of the neurons. Of course, this is a, so the way I just frame it is a typical example of supervised learning, but we can do that only in simulation, of course. So we can only use our models to simulate. Use our models to simulate a motion of the mirrors, compute the actual error signals, and then see if we can use the neural network to solve the inverse problem. So going from the signals to the positions. And again, this is not quite as having the system learn on the machine learning system learn on the interferometer. But since our models are relatively precise, the idea is that if you first train the model on a simulation and then you deploy And then you deploy the model on the real system, it should be close enough to the actual output. So, the idea is that there will be an interplay of simulation and deployment in the system. Again, because it's not practical to do the actual learning online because for two reasons. First, it's going to take a long time. And so that's too expensive and too dangerous. And second, in the real system, we don't really have the levels, the targets for our super. The targets for our supervised learning. So, the idea is that you first build a model as close as possible to your system. And then you train a machine learning algorithm to solve the inverse problem. So go from the output and produce the mirror positions. And then you deploy this on the physical system where you have actually observable outputs. And then you can estimate the unknown state. And at this point, there are either the system, the model was close enough to your system that it's already working fine, and then you have your known. Fine, and then you have your non-linear state estimator, or you still have some error. So you can imagine to look back and use the error between the estimate and the expected behavior of the system and find to your model and keep doing this until you have a good result. So there are a few problems here. First of all, we are dealing with time series, and as I explained at the beginning, And as I explained at the beginning, the seismic oscillation of the system introduced long memories. So you need to have a neural network system that is able to have some sort of memory of what happened in the last two, three, or four seconds, or whatever it is. So this animation here shows you a simulated trajectory for just a system with two degrees of freedom. So it's even oversimplified. But you see that the seismic motion creates complex trajectory in the phase space. Trajectory in the phase space. And the plots that you see here below the main square are the optical signals. So you see that really, if you are in a place like where the dot is now, where everything is flat, you can't really estimate what's happening unless you have some memory of what happened in the past. You lose a lot of information. So you need to keep a memory of what was happening for quite a long history. But then the other thing you see here is that when you cross one of the resonances or one of the Cross one of the resonances or one of the critical points, everything happens very quickly. And so you need to sample your system, your signals at a quite high sampling rate. And so the problem you see here is that you need a long history, long signals, sampled at a high sampling rate. So you could imagine using a convolutional neural network or a current neural network, but then you're going to feed them training examples that are easily thousands of samples long. And all those methods, And all those methods don't scale very well with such a long time series, especially the current neural networks scale very poorly to long time series. So this is an issue on applying any kind of neural network techniques to this kind of problem. And this is true even in the case of reinforcement learning and in the case of this supervised state estimator, because in both cases, the system needs to maintain some memory of what happened for a lot of samples in the past. Lot of samples in the past. So we probably need to use different techniques, maybe the attention mechanism or something like the neural Turing machines that have an external memory, or maybe use some kind of multi-rate input of the signals, or research work computing seems to be more stable for these kind of things. So this is an upper question. And I didn't say it at the beginning, but a lot of what I'm presenting today, especially in this first part, is not conclusive. We don't have a conclusion. We don't have a result here. But it's just an idea of what kind of Here, but it's just an idea of what kind of help we could get from machine learning techniques for some of our problems in the forelock. So, that was all I had to say and explain about the first part. So, the lock acquisition. So, let's imagine now that we either have solved this problem or we keep using the technique we have now, and we can actually have a system which is working in the high sensitivity state. How can we use machine learning to improve the sensitivity? Well, first of all, let me Sensitivity. Well, first of all, let me try to explain why this is a hard problem. So, this graph here shows you the typical ground motion in an urban area or even at the observatories. And in blue is a typical detector sensitivity in displacement in meters. So, you see that the ground is moving several order magnitudes more, 10 to the 10 times more than what we actually try to measure. So, we need to implement a combination of passive. A combination of passive isolation and control systems to get down to that level. And although the behavior of the system is almost linear, it's not linear in a part in 10 to the 10. So every small variation from linearity and small division from linearity will have an effect. So that's why noise subtraction to improve the sensitivity, a linear approach is good, but it's not enough, as we will show in a bit. So again, the question could So again, the question could be: why do we really need the noise subtraction? So let me try to give you a little bit of motivation of why this is interesting. So the bigger plot here on the left show the typical LIGO-Livingstone sensitivity in blue during O3. And the orange curve down there is what the sensitivity could or should be if it were limited only by fundamental noise sources, so thermal noise. So, thermal noise of the mirrors of the suspension and quantum noise in the lesbian. So, you see that above maybe 70 hertz, it matches pretty well, which is great. But below 70 or 60 hertz, you see that there is quite a divergence there. So, that red shaded area tells you that there's a lot of excess noise with respect to what could be the fundamental limit in the sensitivity. And that noise is only partially understood. To it. And other ways to look at it is this modeling set that you see in the plot that shows the change in the range of our detector for coalescing binary systems as a function of the mass. So blue is what we actually have right now, and orange is what we could have. So you see that there is a lot to gain there, especially in the high mass region. You could gain. You could gain in like 25 or 30 percent in the range, which is you know a lot of range that we are losing just because we don't understand this um part here. And this is even no, the higher the mass and the higher the improvement. This other plot here on the right shows the integrand, the integrand of the SNR. So, as a function of frequency for a specific mass, so for example, let's focus on this green plot for the for Green plot for the for 10 solar masses, so the component masses, this is how much SNR you accumulate every single frequency. So you and again, the design is here in the solid line, and the field histogram is what we have right now. This is just telling you that we lose a lot of the total SNR at low frequency. And this has two important effects. The first one is the one that I just mentioned, that you lose in the total range of the instrument. In the total range of the instrument. The other one is also meaning that we cannot really detect the earlier orbits of those systems because those are completely spoiled by the noise. So if you were able to decrease this noise, to improve this noise, not only would be able to increase the total SNR of the system, but we would also be able to detect them earlier and detect more orbits. Detect more orbits. And so, all the discussion that we had yesterday about testing general relativity and all those things, they are important. The more orbits you are able to detect, and the more information you have, and the earlier you can also send out alerts. And this is even, you know, the effect of this excess noise is larger, the larger the masses are. And so, this plot down here on the right bottom is the best estimate we have right now of the black. Estimate we have right now of the black hole population from the GWTC3 catalog. And so you can see that we can't really say much above 80 or 100 solar masses. Most of these is because we cannot really detect much out there because of our reduced sensitivity. So you can imagine that improving the low-noise sensitivity will also allow us to prove to probe more of these high masses ranges. So it's really important and interesting. Important and interesting, and crucial to be able to reduce that noise. So, of course, now you can say, oh, three, we collected already all the data. What can we do? Well, of course, we can do a lot of noise improvements either online, which is already done and was done in the Virgo interferometer and the initial LIGO interferometer, and it's done routinely in LIGO and Virgo right now. You can also do offline subtraction because you might just figure out that the Subtraction because you might just figure out that there is some excess noise that you can actually witness with some other auxiliary channel, and you can do some offline subtraction. And a very good example is what was done in LIGO during 02, where we had a lot of noise due to jitter, so angular motion of the input laser beam, that we could measure and subtract offline. And that was a linear noise subtraction using essentially a Wiener filter. So if the noise So, if the noise couplings are linear and stationary, then we know how to cope with this excessive noise, how to cancel that. Again, when the things are linear, we know a lot of things. We know that we use frequency domain coherence to find the noises and remove them. We know we can do transfer functions, analysis, or using binar filters, either online or offline, and we can remove nodes. The problem is that what you have, what we have to do is. The problem is that when we have non-linear noise couplings, then things get way more complicated. So, in general, we can imagine that the noise in some auxiliary channel can couple into the main detector sensitivity in a non-linear way. And the only way to write the most general nonlinear coupling is just a function of a bunch of other signals just for times that are in the past, because causality is the only thing we can enforce. Only thing we can enforce. Of course, this thing here, this most general functional form, is too complicated to do anything with it. There's not much we can do. However, in most cases, we expect only small deviations from linearity because all our feedback control systems are designed to maintain our interferometer as close as possible to the operating point. So expanding the nonlinearity in Voltera. Nonlinearity in Volterra series series, if you want, and just talking at the quadratic coupling is most of the time enough. And another important point is that often this quadratic coupling simplifies to some sort of non-stationary coupling. And the reason is that we are interested in improving the sensitivity, for example, around 60 hertz in our detector, but most of the auxiliary channels have a content in their power only at very low frequency. So they're fluctuating, I mean, their power spectral density is. To in, I mean, their power spectral density is almost all contained below a few hertz. And so, the effect of this quadratic coupling is to give you something like what is shown in this spectrogram here on the right side. So, here's an example of the 60 Hertz power line. In the United States, the 60 Hertz is the frequency of the main power supplies. And you see that you not only have the main line, but symmetric side bands around the main line. This is an example of a quadratic coupling where the power, the Where the power, the noise from the main line, which is all contained at 60 hertz, actually couples with a status quo function which is non-stationary, which is fluctuating over time. And this fluctuation is seen as silence coming from the seismic motion, which is upconverted around the 60 Hertz. So this is the kind of noises that we could be able to subtract. And so in this case, the non-linear Noise coupling can be modeled as a non-stationary linear noise coupling, meaning that essentially you can have that your target signal, the strain, is given by, well, of course, there's a stationary linear coupling that takes your auxiliary channel, your witness noise channel, and couples with a linear stationary case. But we know how to deal with that with, you know, linear fields and stuff like that. Then you can have, in general, a bunch of other. Generally, a bunch of other signals, what I call the modulation witness signals, that have only content of very low frequency. And so each of them multiply your fast noise source. And then this modulated signal is then coupled with a stationary coupling into the main target. And the non-stationarity of the coupling is all witnessed by those signals. And you can actually mathematically prove that in the assumption of only a quadratic coupling, Quadratic coupling and the separation of the frequency content. So your noise is around, let's say, 60 hertz or above 10 hertz, and the modulation witness signal have only content below a couple of hertz, so there's this separation, then the most general case is something like this. And so this is a much simpler model than the most general non-linear system. And you can imagine to use machine learning-inspired cost optimism. learning inspired cost optimization algorithm to actually find what are the optical optimal coefficients alpha for this kind of noise subtraction and this is something that was done in a couple of cases so first one where we had some we on purpose we increased the noise in our system by increasing the motion of the signal cyclic cavity just to make it even more visible above the sensitivity and so you go from the unperturbed sensitivity in Unperturbed sensitivity in orange here to the blue line. Then we so we increase on purpose the noise, so we know what we need to subtract. If you implement only a linear subtraction, the best you can do is to get down to this green curve here, because the noise was coupling in a non-stationary way. If we implement the kind of non-stationary noise subtraction that I just explained, with the model I explained in the previous slide, then you can go down to the red line. Down to the red line. So you can see that you can gain almost a factor of 10 more subtraction if you include all the non-linearities. We couldn't go down as much as the initial sensitivity because not all the non-stationarity were captured by our model. Because remember, the model is able to reproduce only non-stationarities that are actually witnesses by some other signals. And you have to choose what those signals are. So if we didn't pick all the signals, then of course you can't see all the non-stationalities. Can't see all the non-stationarities. But this is just to prove that this kind of approach is able to outperform stationary and linear subtraction by quite a lot, as expected. And this plot here on the right shows an application to the previous example of the 60 Hertz line harmonic. So we start from the blue line, which is exactly the same as showing earlier. If you only do a linear noise subtraction, of course, you can only reduce the 60 Hertz peak, because that's the only content in your noise weakness signal. In your noise weakness signal, but then if you include modulations, then you see that you can actually go down to the red curve here, which take care of getting rid of all the sidebands. So not only you get rid of the main linear coupling, but also of all the sidebands of the non-structure coupling. So this is just trying to convince you that although this non-stationary model is not the most general non-linear model, it's Most general non-linear model is good enough to improve the advanced LIGO sensitivity for a lot of the cases that we are interested in. And there are two important things that are crucial in this model. First of all, it has interpretable results. So it's not a neural network, it's sort of a black box. You can go back and actually see what is the value and shape of those coefficients alpha. And this tells you exactly what is the signal that modulates that coupling. So, this is something that you can then. Coupling. So, this is something that you can then learn what is some weak spot of your interferometer. You can go back to the actual online system and improve it. And then, this can be, you know, those transfer functions, the coefficient can be parameterized such that the result is intrinsically causal and in time domain. So, this gives you not only something that you can do offline, but something that you can train offline and then directly, immediately deploy it online without any. It online without any changes to the algorithm. So, this is a very powerful thing because it solves all the issues of how to convert an intrinsically offline system like a neural network into an online. So, the discussion we had yesterday about transient at the boundary of segment is not a problem here because you just what you get is a bunch of coefficients of a time-domain filter that you can implement directly in real time online. So, these are two very important. Very important key points of this. So, just before moving on to something else and getting closer to the end of the talk, this plot here shows what kind of impact we can have on the O3 data by using this kind of algorithm. So, in particular, in the Hanford O3 data. So, in blue here, you see the typical power spectral density at low frequency of the Hanford interferometer during O3. During all three. And applying this non-stationary noise subtraction by removing the longitudinal and angular control noise allows us to go down to this orange curve. So this is a factor of two or three, or sometimes even four improvement at low frequency, so below 20, 30 hertz. So this translates directly in being able to improve the SNR for high mass systems just right away. And also being able to remove Being able to remove a lot of these lines and structures here means that for those pulsars that have a frequency there, like the Vela pulsar, you increase the sensitivity by a factor of a few, which is a lot, because for a pulsar, you know, if you reduce the sensitivity by a factor of two, it's like having four times, almost like having four times more data. So, this is an important result that all this subtraction for all three is ongoing, and the data will be available. Is ongoing and the data will be available pretty soon. So, of course, this is, you know, when we talk about machine learning, I'm pretty sure that one of the questions that some of you have in mind now is, why don't you use a neural network to do exactly the same thing? Well, there has been attempts at using neural networks, both to reproduce the linear noise abstraction, as shown in this case here from this publication. Case here from this publication, or even in the case of the increased signal cyclic noise. So, the thing here is that there are a few drawbacks in using a neural network. So, although in theory, a neural network is able to reproduce any non-linear behavior if it's big enough, it's really hard to train that. And again, the fact that we are mixing low frequencies and high frequencies mean that again we have this sampling problem. Again, we have this sampling problem that we need fast sampling to capture the noise around 100 hertz, but very long memories to capture the dynamics of the system at a fraction of a hertz. And so both convolutional neural network and recurrent neural network do not scale that well with long signals. So so far, it has been shown that a deep neural network can reproduce the performance of linear subtraction. But I personally think it's kind of overkill to use a complex neural network to do. A complex neural network to do a linear noise subtraction because we know we have techniques for that. In all the cases of non-stationary and non-linear coupling, it's been found that although in principle a recurrent neural network is capable of expressing this low-modulated coupling, it's hard to train. And the end of the training, it typically underperforms the results of the other simplified models I was showing. And one key point is that it's almost impossible at the end of this to find, to interpret. This is to find to interpret the result. If you look at the coefficient of the neural network, you can't really tell what is modulating what. So, that's also an important point for us. So, that's one of the reasons why using neural networks is not yet a very well-developed option here. So, there's before I move to the conclusion, there's one aspect of, so we one aspect that I want to touch, which is more One aspect that I want to touch, which is more the big data idea, how to use techniques to actually discover what the noises that are relevant are. Because the reason we were able to do those noise subtractions that I showed here, you know, the improvement of the Hamford sensitivity below 20 Hertz or the removal of the 60 Hertz harmonics, is because somebody, me or somebody else, sat there, looked at the data and decided that that was the noise coupling and that was a plausible source of. Was a plausible source of modulation. And then you put those together and you apply your algorithm, and it works or it doesn't work. One key point would be to find a way to actually discover in some sort of automatic way what are the noises that are relevant for improving the sensitivity. So, in the case of, well, before I'm moving to that, so this is getting close to the idea. This is getting close to the idea of having a big data problem because the typical advanced LIGO detector records something like 4,000 channels in real time during normal operation that are sampled somewhere between a few hundred hertz to a few kilohertz. So this is something of the order 1.6 gigabytes per minute of data. So it's a lot of data. It's not probably big data in the sense of what Google would call big data, but for a physicist, this is a lot of data. For a physicist, this is a lot of data. So, how can we just go through all that data to find what are the suitable candidates for null subtraction? So, the linear case is solved because we know that if there is a linear relation between two channels, then we should be able to have frequency-domain coherence between them. And so, there's a brute force coherence approach, which basically computes the frequency-domain coherence of the gravitational wave strain with everything. So, you pick like 10, 20. So, you pick like 10, 20 minutes of data, and you run this algorithm, and this will return a table like this, which shows you for each frequency what are the auxiliary channels that have the higher coherence with the gravitational wave strain. And you can produce a table like that, which is color-coded. So, I mean, bright red means high coherence, white means low coherence, and then you can look at the results, and this allows you. The results, and this allows you to discover that actually the signal recycling cavity length and the one, some of the angular controls are actually the culprit to produce noise there. And this is how we actually discovered how to get that improvement at low frequency. So this is an approach that has been used, it's been used, and it's been used for quite a while. It works pretty well, but it works well in the linear case. What about the non-linear case? What about the nonlinear case? Well, even if we restrict ourselves to quadratic coupling, the problem is untractable because if we have 4,000 channels, then we are talking about 4,000 squares, so something like 16 million possible combinations. And this becomes, even if you think you just take just the product of signals, so it's not even the most general quadratic copy, just the product of signals, then this is not feasible. Signals, then this is not feasible because you know it's going to take years just to analyze half an hour of data. So this doesn't work. Also, you know, this is just for the most for the simplest point-to-point quadratic coupling. The most general, to find the most general quadratic coupling, you could implement B-coherence or stuff like that, which is even more computational expensive. So, this is not really something that we have a solution right now. We can try to We can try to solve the problem with some reduced scope approaches. So, for example, we know that a lot of the noise couplings that are non-stationary are modulated in our systems because of angular motions of the mirror. And this is some knowledge that you get from your knowledge of the instrument. So, you could, for example, select the few 16 or something like that channels that measure the residual fluctuation of the mirror and look for modulated noise cut. Modulated noise couplings due to those. So then you're not looking at 4,000 square signals, but you're looking at 4,000 times 16 signals, which is still tractable. And what you can do is actually compute all the products of all the angular residual fluctuation, all the channels, and compute the coherence of this modulated strain, modulated signal with the strain. And this gives you results like the one that you show here for the 60 hertz harmonic. Here for the 60 Hertz harmony, for the first 60 Hertz power line case. So you see that you have high coherence with the not modulated monitor of the power supply at 60 Hertz, but also you see that you have coherence on the sideband, not with the main signal, but with the main signal multiplied by some of this angular residual motion. So this is a technique that could have allowed us to discover the origin of the sidebands, even though we mean. Even though we discover them by just looking at them, scratching our head and understanding and knowing how the instrument works. But this is one step toward having a way to discover modulated noise couplings, of course, in a reduced scope, because you can't really explore all the possible non-linear or even not only all the possible quadratic noise couplings. You have to somehow guess what is the origin of the modulation and go and try to understand. Modulation and go and try to understand if some signals are modulated in that way. But it's still a step forward. With that, I want to move to the conclusions and just summarize briefly. So I think there's a lot of promise in applying machine learning techniques to these two main topics that I cover today. One is the non-linear control of our instruments, and the other is the nerve subtraction. So the nerve subtraction, as you have probably seen from my talk, is much more developed. My talk is much more developed, and there are already a lot of results. And the main success there was to not really using the most general deep neural network, non-linear model, but to try to put into our model, our noise attraction model, as much as understanding of the physics of the system as we could. And for example, in this case, just saying that we know that there are, you know, the system is almost linear, quadratic coupling. Almost linear, quadratic coupling is dominant. And this separation of the power spectral densities of the signals allows us to express the non-linear terms in terms of a noise coupling modulation. Of course, it will be interesting to see how can we expand over that. And in the case of the lock acquisition on non-linear control, it's all very experimental right now. So we are trying to start to understand how to do that. Do that, and in the case of noise abstraction, how can we use machine learning and big data techniques to efficiently discover noise coupling? Because I'm quite sure that of all that access noise and low frequency, a lot more could be removed if we were smart enough or we have enough people to put there and try everything. It's just not computationally feasible to just do everything. We need to find a smart way to do that. Then it would. It would be interesting to be able to implement more efficiently neural networks, but the main issues here is really, I think, from my poor understanding of machine learning, is that we are dealing with very long time series, which is something that not a lot of other people do. Because all the application of recurrence neural network or even convolutional neural network have relatively small training examples. And so we had to find a way to implement that. Implement that, be able to train those models in a more efficient way. And at the end of the day, I like to conclude with this photo here, which is just machine learning is fun, it's nice, it's very cool, but sometimes we have to try to find the right tool for the job we have to do without trying to over-engineer engineering too much. So it might turn out that neural networks are not the best tool for the job, and that's fine. For the job, and that's fine. And the goal here is to improve the sensitivity of our detectors, and whatever does the job is fine with me. Thank you for your attention, and I'll be happy to answer any question you might have. Thank you very much, Gabriele. So, any questions here in the room? Yes, we have questions here. Hi, Gabrielle. Here, hi Gabrielle, and nice, nice talk. So, maybe you said it and I missed it. What's the time scale of the non-stationarity you are looking at? Are we talking about the non-stationarity of the order of seconds or less or more? So, there is a little bit of everything from a couple of seconds and down. So, you can see it, for example, from this plot here that in the case of the coupling of the 60 hertz, you see that. 60 hertz, you see that the side bands go as far as two hertz from the main line, but you still have structures around half a hertz, even lower, lower than that. So generally, we see that the angular residual motion of the interferometer are relevant below a few hertz. Okay, I see, thanks. And just a follow-up question: Can you do anything about scattering? Do anything about scattering? I know that may be difficult to tackle, but so my personal opinion, I haven't tried to apply anything to scattering. My personal opinion is that it's very hard to imagine we could do anything about scattering. And the reason is that scattering is by scattering, we mean that there is some spurious photons that are that, you know, when the main laser hits the mirror, there's some roughness of the mirror. The mirror, some roughness of the mirror, scatters photons around, and then they hit some part of the instrument, like the vacuum tube, and then they're recombined back, carrying a phase modulation, which is random noise. So, if we were able to measure the vibration of the points that scatters back the photons with high accuracy, we might be able to implement some sort of model and reduce scatter life. Of model and reduced cut of life. The problem is that we not only the coupling is highly non-linear, but we don't even have a good sensor of the origin of the noise. So we have, it's even more complex than all of this, or all of this. So I think we, in general, we might be able to, so we are typically able to predict where the peak of the noise for some scattering source could go, but to actually produce an estimate of the noise subtractive, I think. Or the noise subtracted, I think it's going to be very hard. But I know there are people trying to do that that don't agree with my assessment. So I hope I'm wrong. From Deep. Deep, go ahead. Hi. Hi, Gabriele. I hope you can hear me. Yes. Yeah. Okay. Great talk, by the way. So I was wondering firstly about a bit more on the technical side, since you talked about long signals. I was wondering, is there any how? Is there any hardware specific hardware requirement that you use or you think would be necessary for dealing with large signals like this? I'm just wondering because I don't know the answer to that. And people in our group are also wondering about that. And secondly, is more of because you mentioned about recurrent networks taking a long time to train. Long time to train in one of my projects, we have used this other architecture called a temporal convolutional network, and the way they advertise that network is exactly that it is easier to train, meaning the parallelism is obtained instead of recurrent network where you have to wait for each layer to train. So, I was wondering if you have taken a look at that. I personally have used only a TCN and don't have a very And don't have a very good idea about what RNN are as opposed to that, but that could be a viable option. But yeah, I'll let you answer the first question. Yeah, so here's, so that's a very good suggestion. And I haven't used temporal convolutional network, but there's one of the many things. I should have added that on the ideas of possible other techniques to use. So the thing here is that. The thing here is that the problem with the recurrent neural network is what is technically called the vanishing of an exploding gradient. So, the idea is that when you train a recurrent neural network, you have to compute the gradient, train it for all the past steps, or past samples. And the point is that every time you go back one sample, your gradients, they're going to either explode or even go to zero. So the idea. Or even go to zero. So the idea is that even if you have a very long time series, and even in theory, the recurrent neural network is able to maintain the memory of what happened in the previous 10,000 samples. When you actually try to train that, you can't really train beyond the response to the very first few samples. And that's, if you want, it's a fundamental problem of recurrence neural networks. It's not a matter of having a different kind of hardware, it's a matter of finding a way to train recurrence neural networks. Funding a way to train recurrence neural networks is not based on back propagation or funding a different architecture. So, convolution neural network or time-convolutional network, they're better for this point of view because for a convolutional neural network or a time-neural network, you can, you know, you sort of feed the whole time series at the same time, and you compute the gradient with all the samples almost on the same basis. The problem then becomes that convolutional neural networks. Convolution neural networks, at least and I'm sure for that, they are way more expensive to compute them at the prediction time because you have to keep all the, if you need 10,000 samples for your convolutional neural network, you need all those 10,000 samples in memory right then. And at every single sample, the neural network has to reprocess everything. So it's not something that is easy to efficiently implement online and be able to run it at the Online and be able to run it at 16 kilohertz as we need. So that's one of the problems. Maybe time convolutional network could be a solution for this. Of course. Yeah, go ahead. Sorry. Yeah, so at least in the original paper for this temporal convolutional network, the way they mentioned that this vanishing or exploding radio... This vanishing or exploding ingredients could be a problem, and they use this residual block between different layers. So it could be a viable option. But sorry, my question about the hardware was mostly about what type of hard hardware are you using for this kind of work that is dealing with long time students? Because we don't use that, but we would be, we were curious. We were curious. Nothing special. So, the experiment I was doing concerning the lock acquisition and a current neural network were trained on NVIDIA GPU, I think it was a Tesla or a Volta, nothing too fancy. For all the noise attraction with the physical and motivated model that I'm explaining here, this is way simpler than lightweight. So, you can train on a And lightweight. So you can train on a normal computer with a CPU. You don't even need a GPU for this. And that's one of the main advantages from my point of view, because this is a model that's inspired from what's happening in the system. It's working in the sense that we are able to reduce a lot of noise. Maybe it's not the most general things, but I don't think we have used it to the extent that it's going to be useful. And the fact that it's so easy to train and then so easy to deploy is a big advantage of this thing. Deploy is a big advantage of the stream. You don't need any special hardware for this. Thank you. Great talk, Ale. Thank you. Thank you. Thank you. I have another question, Gabriel. So following your last picture about finding the right tool is not an easy job. So I was wondering whether for the non-linear control, the non-linear state estimator, whether you explore a you explore a carbon filtering extended and karma filtering for that for the for those problems there you think they are appropriate or or not so um i am i am exploring those those possibilities so especially the most general idea of using some bayesian recursive filter most you know the in the most general framework which kalman filtering and extended kalmar filtering are a particular case for the extended kalman filtering the idea is Filtering, the idea is that you want to describe your system as a linearized version around each point, each state, each observation. And I think in our case, this is not quite something we can do. Let me just pull up the proper slide here. And I think one of the issues is that you can see from this animation here. From this animation here, that you have so you have multiple points in the phase space where the signals can be very, very similar. So, I don't think that given an observation, there isn't a unique state that produces something close to that. There can be states that are very different in very different places that can produce similar observations. And my understanding of the extended camera filtering is that in this case, it would break down, or you need something. Break down, or you need something more. So, I'm considering using the most general Bayesian recursive filter, maybe particle filtering or something like Gaussian mixtures to describe this multimodal distribution, right? The probability of the state given the observations. But this is all very, very preliminary because, well, the main reason is that's not my main priority. Not my main priority. So it's kind of a hobby. So, you know, it takes a lot of time to understand those things. Yeah, yeah, yeah. Thank you. Thank you for that. And I mean, regarding the noise substruction, you mentioned that the problem is the training. I mean, thinking and applying a deep learning algorithm, the problem is the training. So I was wondering whether some generative models can be used here. Be used here. I don't have a lot of knowledge about them, but I have heard that when you have that kind of problem, you can use first a generative model to generate data to tune your system, and then that data can be used to train your model for the operation. Have you heard about that? So I've heard about generative models, but I Generative models, but I don't really know much more than the name, so that's probably a good suggestion. I don't know because I don't have enough understanding of them to say it's a good idea or not, but it's worth trying. Okay, okay, thank you, thank you, great. Well, any other question for Gabriel? No, well, Gabriel, thank you very much again. Fantastic talk. Let's thank Best thing. So I think it's time for a break. Let me check.