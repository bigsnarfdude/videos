Another in actual learning, the power response are not avoidable and expensive to collect. And how to select an effective training subset. So for the two questions, we will try to address in the same framework. For both questions, the first step is that how can fund a statistical model Statistical model. Here we are talking about statistical model for model response between y and x. In the big data, in the big data, not just the size of speak, but also the inside relation between y and x could be complex. If we use traditional statistical models, such as Liddy model or no jet regression model, might not be sufficient for modern religion. And also in the And also in the actual learning, so because we don't have a response yet, how do you specify a model to describe that data? So in this talk, the outline is that first we'll introduce a flexible status model to model ratio. Then we'll talk about how to select optimal efficiency sub-data set, and followed by some empirical example. So, first, let's get steamboat. Now, let me start with some toy examples. Some of you might know this graph, right, picture. Some of you know this picture? It's called, what's it called? I cannot remember the motorcycle, right? So, there are some study about how to model relationships. So, you have y, this is the value of y, this is the value of x. Value of x, can we find her explicit status model-to-model relation? So let's directly relate. So look, this really does. It's a model we build down. It seems this model to the reasonable job to model this data. How, you may wonder what a model looks like. Here is a model. looks like. Here is the model. The model explains y given x is summation omega ix times a simple linear regression, just linear. But you have a coefficient omega ix. This omega ix is kind of like a weak function. And here, this weak function, you can see the format. It's simply like a multinomial, I mean, multinomial regulation function. Open regulation only, that one. And here is a corresponding parameter. So, for this one, you can say you have explicit function of y model y over x. This example is for dimension of x is y. Let's consider another example. This picture was generated by random select. By random select 30% of the pixel of the orange picture. So lots of pixels are missing. From here, what can you say about this picture? What the picture it is? Lina, very good. Sharp eyes. Very good. Now, but what we want to say is that can we see the picture more clearly? More clearly. So we wonder, can we build a statistical model, explicit function, such as that we can predict the missing parts? This very easy result. Oh, here is the data, structured data, you have position x, position x, x, position y, and also the strength of the pixel. So that's the data. Pixel. So that's the data. Now, here is after we build our statistical model, we do prediction. Here is a recover picture. It's not perfect, of course, compared to original, but it is more clear, a little bit more clear than original. Okay. Now, what's the statistical model is? This model is in this function, just like the previous one. And it's also in. And it's also you have a weight function. Here it's simply the linear regression, linear. You have x1, x2, just by linear. Okay, so there are two, there's a two toy example to talk about specific model. So this model is called a mixture of expert modeling. And it's nothing new. It's actually a long time ago. It's proposed by Japan in 1991. It is to this It is to discover the hidden cluster. You just say in your data, you have hidden cluster. And this model is trying to strike the balance between flexibility and interpretability. Now, so more specifically that, suppose you have X1, Y1, X2, YN, there's your data, and you have one gate function and the key regression models. And YI is modeled by XI. Yi is modeled by Xi through one of the experts, one of the model. But which one is unknown for you? It's like it's native variable. And here is a gated function. It's like a machine nominal regression. And now, how to choose an expert depends on the nature of the response. For example, the previous two examples use linear model. And sometimes if it is one. time if is a wide categorical data, you might use generalized needed models or so forth. And so so far we talk about the dimension x is one dimension two dimension. Let's talk about much dimension. So this is another example. There's a bone holder function. This bone holder function is models of water flows through a bone hold. It's commonly used for test method in computer experiment. So the true model is FXE. So, the true model is fx equals this function. And x means that this tu huh l r r w those variable. So you have eight variable, and you mentioned x is eight. And so let's suppose you observe y equal to fx plus ypsion. If you follow some for some noise. Now, so transition, I mean not transitional in computer experiment. computer experiment. Gaussian process was used to try to capture the relationship between X and Y. But Gaussian process is kind of like, you know, it's an entire match approach. You might not have explicit format. Here, let's say we just use, we want to use a mix of expert with whole cluster to do the prediction. Let's say we want to compare with doubting process. And here are the experts And here the expert is just simply X times bit. And we also use a traditional training data set and a test data set. And so we will say, and this is this simulation. So we have the training sample size is N and the single noise ratio sigma square and different setup. For example, you can see when single See, when single notice ratio is five, if you have training data sets 1000, you use a Gaussian process. This one is kind of like MSG based on the testing data. So 36, 38.8, 36.6. When you increase to 10,000, 25.5, 3.4, I mean 100,000, 12.1, 1.4. So, and the similar pattern for others. But we notice that when single noise ratio is 100, that means the noise is very small. In that situation, the Gaussian process do a good job, like I said. But that's probably most likely due to the Gaussian process non-parametric approach, they could capture the complete structure within that. But MOE also did a reasonable job. So, this is for multiple dimensions. I think here it's it's they okay I think this one it's get a little bit the last one last one I think this most likely due to the due to the due to the due to the i think it's uh um because uh it's most likely due to randomness because we always repeat the 100 times i didn't repeat another time and i sent the moe model when you do the prediction there's two parts once is one is the the uncertainty another part is bias now when your single noise ratio is high the bias is not much is not much. I'm certainly not much, but BIOS is still there. The MOE model, because the model cannot be captured the true one, and the BIOS parts dominate. Okay. Now for this model and so when we do estimation, it is a, we have to, this is a likely function. is a likely function and just like typical typical you have a hidden structure latent variable and you it's a there's no close form you might have to use a em algorithm or base approach to do it so the computation could be if you have large data set it will take a long time also depends on number of cluster number of starting value because if the because year model might not guarantee for one starting value could get the converged to the uh true global global and the true global global global maximum maximization. Now, for that total example, we have three thousand data sets to take one hour if which imagine if you have a one minute data property, maybe I'll take two weeks to do that, take a long time. So we introduce the flex estimate model. Next, we want to say how do we handle because the temperature is. How do we handle because it's a computational issue? How do we choose a sub-data set to do the analysis? So there are lots of work in soft data selection. But here, we'll focus on parametric model-based approach. For parametric model-based approaches, there are two approaches. Two approaches is a stochastic approach and deterministic approach. The stochastic approach, they typically are based on supple sampling with sampling probability. So for each data set, they assign different weights to the sampling. And the advantage of this approach is the business and it's against the alternative. But the disadvantage is that it could be limited. is that it could be limited by subsidies. That means it might not be able to take advantage of your large data set. Later, I'll show you that example. The deterministic approach is based on information sub-data selection. So it's based on optimal design theory. So for fixed end, the information increase with capital. And so far for those two approaches, typically it's based on For this two-proof, typically it's based on for classic statistical models. For the model we're talking about, probably it's there's no such work how to do the sub-data selection. Now, look, because optimum design is not a well-known topic, next, I want to use a toy example to explain what is optimum design. And this is a video. I think some of you might know this video. Let me see. Miss it. So it's kind of like it's about six minutes. So let's play this video. Cannot be perfect. It's a video. I think it cannot be played here. Do you know is it possible to play a video here? No, no, it's okay, it is not. A balance is used to while a balance has an accurate reading, it also has measurement errors. We want the weight estimate to be as accurate as possible. Now, the question is, how can we weigh them? we weigh them. Intuitively we may weigh them as follows. We weigh each object each time. We can weigh the apple first. The reading from the first weight can be written as y1 equals to a plus error. We can do the same thing for the pair. The reading for the second wave can be written as Y2 equals to mean plus error. Similarly, we can weigh the orange and the banana and have the corresponding equations. Based on the model, the estimate for the apple is Y1, the estimate for the pear is Y2. Or the pair is y of two, so on and so forth. We can easily see that the variance of all the estimators are sigma square. If we want to increase the accuracy, losing away an object multiple times, then take an average since we know that there is no huge cost to use the band multiple times. Now, let's suppose hypothetically that the boundary is used. Hypothetically, that there's a huge cost to use a balance. For example, $100,000 per use and only one time per week. With such a huge cost per use, perhaps we want to think of a strategy to use the sound more efficient. Can we? Consider the following strategy. We wave the apple and the banana together. The corresponding model can be written as Y1 equals to A plus B plus error. Then we weigh the difference between the apple and the banana. The corresponding model can be written as 12 equals to A minus B plus A. Minus B plus error. A similar strategy can be done for the pair and the orange, and similar models can be established. Based on the models, the estimation of A is y1 plus y2 over 2 and estimation of B is y1 minus y2 over 2. The estimation of C and D are similar. are simple. Clearly, the variance of the estimation of A, B, C, and D reduce to sigma squared divided by 2. As you can see, we still use the bound four times, but the accuracy increases 100%. Why? The reason is that in this strategy, we pull data together to get the estimation. For example, we pull the apple and banana together to estimate both. To estimate both, like the modeling approach. One may then wonder whether we can obtain a more accurate estimation by pulling all available data together. That is, by using all four objects in each weighting. Consider the following method. We first weigh the difference between the apple plus the orange against the pear plus the binay. pair plus the binette. The corresponding model can be written as 11 equals to A minus B plus C minus B plus error. For the remaining, we weigh the difference between the three objects against the remaining one. For example, in the second time, we weigh the apple plus the pear plus the appearance. plus the pear plus the banana against the orange and the model is y two equal to a plus b minus c plus c plus air similarly we weigh the pair versus the remaining three and the apple versus the remaining three the models can be written for a sponsor some computations Some presentations show that the estimation of A, B, D, and D are as follows. Surely, for A, B, and D, the variance of the estimators are half of sigma squared over 2. But for B, the variance of the estimator is sigma. So while we pulled all the data together, we did not Together, we did not improve the accuracy. Is there a way to improve it? Consider the following method. We will slightly change the weighing setting in the first weight. We move the pear to the left panel and we weigh the difference between the apple plus the pear plus orange versus the banana. Orange versus the banana. The corresponding model is Y1 equal to A plus B plus C minus D plus error. The other three remain the same. There is no change. It is still the same model. With the change based on the new models, we can show that the estimators of A, B, D, and D C, C, and D as below. We can verify that the variance of all estimators are sigma squared divided by 4. We reduce the variance by half and more with a small change. Okay. Oh, no, no, no. Okay. So that's a video. I made it as a video just to explain to give the introduction: what is optimal design? Introduction: What is optimal design? How it works? So, for example, let's go back now. You can say, use the master one to compare method one, measure two, master four. And if we use the same cost, and the variance will be sigma squared, sigma squared over two, sigma square over four. Now, if you want to achieve a similar variance, master one, probably, this is a cost of mass one, cost of master two, cost of method three. Now, what this example tells us. The example tells us first, a model approach could be helpful. Second, an efficient design matters. Just think about from design and measure two versus measure two to measure three. Although we put data together, but we don't do it efficiently. Your evaluation actually gets increased. Last we say, a modern approach plus an efficient design couldn't make a significant difference. Now, there's originality for this one. It's simply because this is a linear model, y equal to x beta plus Yipschelon, the best linear unbiased expert, x prime x inverse x prime y minus what the x prime x inverse sigma square. So our question, how to select x such as x prime x is minimized. For measure one, this is the matrix, measure two, this is the matrix, this is master four, what's a matrix. What's a metric? Simple computation, we know. This is where it actually background this one, this is called a hot mat matrix, the conjecture. I think that conjecture has not improved yet. I mean, the conjecture said hot match matrix exists as long as the dimension is a multiple of four, but that has not been proven. So now, what's your application? Now, what's your application for optimal design? First, in a click trial, because in click trial, it's very expensive. So, if you design an efficient one, it could save time, also could save the cost. And now, for the question we're talking about here with big data analysis, we could choose an efficient subdivis to address trade-off between the computation continuity and the statistical efficiency. In the actual learning, we want to achieve the best performance by using a high score. Best performance by using a highest quality sub-sample or labeling. Now, however, when you choose an optimal sub-data, this is different from optimum design because in design, you can choose a perfect data point, but in reality, perfect data points might not exist. So you have to choose from what data points are available. So essentially, you want to So essentially, you want to from bigger n choose small and data. So this one is intractable problems. And it's a discrete nature. There's no tools to do that. So typically MP hard problem. Now, let's see. So this another take a break. I always like to use the toy example. This is nothing to do with this talk. I just curious, just something interesting. So do you know your drop of water? So, in a drop of water, how many atoms in a drop of water? In a drop of water, how many atoms there? Any idea? Yes? Venus. Venus. Very close. Okay. It's 10 to the power 20. Okay, it's 10 to the power 21. Okay, now this number versus from capital N chose 50. Do you know what the capital N should be? So some such as it's corresponding to this, I mean, roughly equal to this number scale. What's the capital issue? Yes? And equal to 80. From 80 to 50, this number will be roughly the same scale as 10 to the power 21. Okay, now let's think about another question. If we increase 80 to 220, do you know how many drop of waters contains the atom equivalent to the from capital N from 220 to choose 50? What do you guess? So, guess a bottom, a river, a lake. What's that? More than water on here or in Mexico. Yes, you're right. Actually, the whole earth, not just water, and everything. Everything. Maybe the chair, maybe somewhere in Africa, the trees, everything, the number of atoms. The number of atoms, same scale and 12 to 20, choose 50. Now, for your information, I'm curious. When A equals 300, what's it corresponding to? Number of atoms. No, not too much. Solar system, solar system. When N equal to 500, galaxy, galactic. When N equal to 900, Lesson when n equal to 900, the whole university so far we can observe. And the whole university so far we can all observe the number of atoms there, it's equivalent to 900, which is 50. So here, just just for fun, when we want to talk to the combination number. Okay, let's back to business. So the question is that from capital N choose N, this is a huge number. And how do we choose an efficient number? So we. So we divide what measure is called information-based optimal sub-data selection. And the Z1 is for the linear model, y equals X beta. That's to characterize the design maximum information matrix. Then we develop an aggregate to select the sub data based on the characterization. So here we can show that. So for example, this is a capital M and the small And this small n is fixed. I think a small n here is the baseline is 1000, 1000. And this one is use IBOS project. I mean, you choose a subset, subset. So although your subset size is 1,000, but when the capital N increase, the information again is close to the whole data. We still use 1,000, but there's 1000. But this 1,000 choose from 10,000, choose 100,000, choose 1 million. And the information from the subdata, it's close, of course, cannot be more than four data, but it's increased around the whole data. But for sample random sample leverage, it's kind of like a flag. Yes? No, I mean, yes. No, I mean, yes, that's a good question. Here, let's here, it's actually, for this one, we're talking about X is from the T distribution, generally from T distribution. It's not no longer X, the restriction from negative one. Yeah, it's different. So I didn't give the detail about this. So let's see. So let's say now that algorithm is specific designed for linear model and it might not work for the model, the flexible status model. But that work should ask if we use information-based sub-data selection, we can extract the switch information. The rich information can the big data set. It cannot equivalent big data, but you can get maximum information as much as possible. Now, for mixture of expert, there are some challenges here. The first information matrix, there's no explicit form. So you don't have objective function work. So we use the strategy that we instead work on the information match. On the information matrix, and which we don't have closed form, we'll think about work on the information matrix. We found a surrogate information matrix to work on. So I didn't go to detail here. And now the question is that how to tune a sub-theta delta such as to maximize this maximize this determinant here? That's the question. And here are some. And here are some results to show why this approach works. So I'm not going to detail. Now eventually we need the algorithm to do that. So the algorithm based on the IBOS, the previous algorithm is called the IBOS. The IBOS algorithm is based on the characterization of optimum design. The advantage is that it's very fast. The disadvantage is that the characterization might not be fixed. The characterization might not be feasible because you need to derive the optimal design first, then, based on that characterization, then design algorithm. And also, it might not be efficient, although we show that it can get extra information, but that is just approximation. So we want to derive algorithms such as from discrete nature, we can still get that guarantee it is optimal. Guarantee it is optimal or efficient, highly efficient type thing. So we use a new strategy that we use approximately fungi optimal design approach. So in this approach, it's based on approximate design context. In that design context, there is a powerful tool. It's called equivalent serve. The equivalent serve, the advantage of equivalence serve is that. Of equilibrium sermons that you could, when you get a design, you could verify where your design is indeed optimal. So, but this is approximate design. So, that means it's not an exact design. So, the sub-data selection is we have to select a data and either the corresponding weight equal to zero or one over n. Or one over n. But when we talk about approachable design, we restrict us in a design such as corresponding weight between 0 and 1 over m. So while searching in this design space, found the optimal design here. Then we transfer the corresponding design to the exact design. Of course, we hope this from the approximate design to exact design. The approach design to exactly that we hope this, they are very close. So now, for any, because the optimal exact design exists, but we don't know where we don't know, we cannot find it. So the efficiency is bounded by this. The true efficiency is bounded by this true quantity. So once you found appropriate. So once you found the approach multimedia design and all the design transfer convert from the optimal design to the exact design, so the true efficiency will be bounded by these two. If these two values are very close to each other, then the efficiency will get almost like the true efficiency. Now, to make this strategy work, first you need to derive an efficient design or optimum design. Second, you want to make sure list. Second, you want to make sure these two values are very small. So this is the equivalent circuit. And so I'm not, I will skip this. This is a corresponding algorithm. How do we derive the corresponding optimum design? And I will skip it. Here is a convenient term. That means you need to make sure that your serm, your operator can converge to the global. Your algorithm can converge to the global optimal one. Skip this. Okay, suppose you derive your optimal design. Then once you derive the optimal design, you output the endpoints which has not just widths, not just the widths, the endpoints. That endpoints were your optimal subset. Endpoints what you opt to subset. So let's talk about an example. Suppose you are consider the logistic quadratic efficiency model, not y equal to x1, x2, x1 times x2, x1 squared, x2 squared. And you have one million data set. You want to choose 1,000. And I mean, using that algorithm, we can do it. It takes about two seconds. So virtualized, you can see how we choose those points. You can see it's here, some points are here. Now, if you do the comparison with some existing approach, for example, this is proposed approach here. The efficiency is more than 99.999%. And if you use an average approach, leverage approach is kind of like the stochastic approach, and they choose. And they choose a sign of weight, the efficiency is about 11%, 12%. And a simple random cycle, 80%. So, and even use the corresponding IPOS approach, it's 55%. So, that's a difference. Now, yeah. Two seconds. Two seconds. I mean, for this example. Seconds. I mean, for this example, two seconds you can do it. For one minute, choose one thousand two seconds. And if your dimension is not, for example, you have a notion y equal to, let's say you have 20 different correct public, take one minute or something. Yes? Exactly. Correct. That's a very good question. Very good. So in this talk, I didn't go to the detail about this one. Now, for the mixture cluster, the information matrix. So the inside of the information matrix, for each cluster, they have a component path there. So if you want to maximize the So if you want to maximize the information matrix, you have to choose some points from each component. If you don't choose, your information matrix will be goes to zero. That means the corresponding determinant will go to zero. So it's automatically, they must choose the points from different clusters. Yeah. You mean the basis of right but the first part itself here is just a classical mode. That's why you have this one. What happens if that Oh, yeah. What change? The picture could be completely different. Right, right. Because the second part is, I gave a talk, it's also one hour talk. I have lots of different pictures to show, even for the same model. And if your sampar change, the location will change. Right, right, right. It's kind of like a given, given, suppose you're given one for here, you're given one minute, data, one minute, X1, X2, give you one minute, right? And this one minute could change different. It could be arbitrary you're given. Based on that one, you choose the optimal. So let's consider an empirical example, stimulation. So suppose we have one, I mean, 100,000 points. We choose 10,000 points. And X from normal distribution, X from normal distribution, three dimension. And this is for mixture of X per model. And so for the gate function, it's The gate function is by this generated by here, and the expert is also Node.js regression model. I mean, multiple NodeS regression model. And so we want to say, suppose you choose sample random sample, choose 10,000, or and the sample random choose 20,000. Or use full data, 100,000. Or you use optimal sub-data set, and choose 10,000. And then you first use 3,000 samples. First, use 3,000 sample size, 3,000 samples for simple random sample to get some initial information. Then the optimal subdata choose 7,000, they combine this 3,000 and 7,000 to get 10,000. We want to check and what is the, I mean, you have testing data set and what's the performance. So we criteria with the computation time and efficiency. The efficiency is for given X, what's the probability of Y? This is root mean square error of. This is Zoot mean square error of prediction error. The size of test data is 100,000, repeated 100 times. So you can see for simple random sample, you use mixture export. This one is 0.0555. If you increase to 20,000, 0.0513. If you use full data, 0.0497. If you use optimal one, it's 0.0394. This is a very interesting observation because. Observation because we imagine you use four data, you should we expect this one should have the performance best, but actually it's not. I think the reason I it's also I got confused, but later I think the reason is that, because this is a mixture of expert model. Each time you run it, the result kept changing because you do not guarantee your result estimation. Your result estimation is a global one. And for data set, but for the optimal sub-data set, because the points we chose, it has a strong signal. So it's more likely to converge the global one. And you can see it's quite significant: 0.049 down 0.039. In terms of computation time, sample random sample, 20 seconds. Random sample 20 seconds. If we increase the double is 50 56 seconds, four data 300 seconds, and for optimum one, 24 seconds. The white 20 process, we break down the first step, simple random, I mean 37.9 seconds. We use 1.2 seconds to select and then the full analysis 40 process. Yes? Correct. Correct. That's why. Because here you can see, and we assume your data is generated by this model. Generated by this model. Do we have an alternate? No, there's no outlier here. That's right. There's no alternate. Okay, so the base, so the nast is that we want to check for some real examples. We want to check for some real examples. Correct. We don't use why. So that's the reason I say this is also related to active learning because we don't use y information. Yes? Oh, that's a very good question. Let's see. So you can see in this approach, we first do sample random sample for 3000. The reason is that this is the model, we mixture model, has two Najeso regression models. I mean, multinomial Logistic regression model. For multinomial Logistic models, a nonlinear model, the corresponding information matrix depends on non-parametric. Depends on unknown parameters. So, then what do we do with that? Because we don't know the unknown parameter. So, we need to use a small portion of data to get roughly estimation of the unknown parameter. It could be rough. That's okay. But you have some sense for that. Then, once you get that information, we choose the remaining 7,000 that we use. Then we combine both. Band both. Actually, this one we have never, for us, we just kind of random. And what's the best solution we don't know? Not dimension of X. As far as I try, X is, I think, up to 100, we can do that. Still, 100 dimension, we can do that. We can do that. But technically, more time. Now, let's go to some real examples. Number of clusters, right, you mean? Yes, that's the number of cluster when you do analysis. You use some ASABS criteria to determine which number of cluster is best. But on the other hand, it's not very. That's it's not very important. Roughly, if a number cost five versus six, huge difference? No, difference is small. So, but you need to select number. So, let me, oh, this is conservative example. So, this real example is for human activity recognization. And it's a benchmark data set for researchers to develop innovative machine learning approach. Develop an innovative machine learning approach for precise human activities recognization in free leave. Then they have a professional annotated data set containing eight older adults, subjects, seven to five years old, wearing a full 3x as meters for around 40 minutes. The sensors were attached to hair back and also to the 3D. Now, and the data itself, I mean, for all data put together, about two million data sets. And here it's a data, it's kind of like a signal. It's kind of like this is for omniseconds. Well, that means you have this, you have eight features, two times three sensor signal, one, two, three, four, five, six. One, two, three, four, five, six signal. And you have time, and each time the time series data. And you have labeling. Label is several categories. So based on this signal, you determine this person is night down, walking, or something like that. So. And this one is based on the or because this data is benchmark data in UCI and they have paper for that. They combine seven categories into four categories for working. Category into four categories: what are walking, standing, sitting, and the night. The reason they come down is that there are three categories. The proportion is too small, it's just 0.1%. So it's very difficult. So they just combine together. So they consider five seconds windows. So after they condense, they have the total operation 9,100. So the XI is six times 250 metrics. So from that paper, you can see. So, from that paper, use the machine learning approach, the X2161 feature. So, they use extremely grading boost approach, and here the classification accurate rate is following, 95%, 87%, 97%, 97%. So, this is from that bit bench metadata. So, here we use a mix of expertise, we use statistical models. Of expert, we use statistical model and we use two clusters and we can sell many effects of 12 features. We cannot use, or we don't want to use 160 features. We just use 12 features. And we can say use a full data approach, use all the data. Or simple random sample, select 1,000 data set and use optimum sub-data set, totally 1,000 data sets. Simple random sample, 500, optimal subset, 500, total 1,000. We repeat 100 times. We repeated 100 times, 90% for training, 100% for testing. So here is the result. For the full data approach, use a Walken is 97%, standard 88%, City 90, 90%, 90-80%. Compare the benchmark, 95%, 87%, 97%, 97%. The full data stems to a really virtual is better, slightly better. A reasonable job. It's better. That's not a better job. And use simple random sample approach: 96%, 87%, 98%, 95%. It's still readable. But use optimal, 96%, slightly less than four data sets. Standard is slightly better than standard. In CT, slightness. And nine is slightly less. But it seems comparable with XGP approach. But think about this approach. Think about this project. You have explicit model, use 12 features. Okay. So, okay, let's end the metal. Yeah. Here, I think here, it's kind of like I s it's a We are kind of say, you let's say, suppose you have one minute data set. We choose one sound data set to analyze. We don't want to say for each data set, use different weight. We just say kind of like just to this update, and you want to use the original approach to analyze. Actually, if you can see, if let's let me go back to here. So let's let's use a simulation data set, for example, dearly optimal. And this is either zero, one of And this is either zero, one over n. For the optimal one, it's, I mean, truly optimal one, the weight could be between zero and one, or it could be anything. But they are very close. 90, 90, 99%. There's not much difference to distinguish them. So that's just it's kind of nearly optimal. Active learning, you mean no, so far, so far, in fact, if you're talking about active learning, this approach we need to fairly revise because in active learning, so far, we talk about, you can see this one or Schumer model, right? Even if it's mixed of X model, we think it's flexible. But in reality, the true model could be even more complicated than that classic model. So in the end, mode. So in actual learning, there's one approach is like say you assume the true model fx minus f0x less than zero. This one is your work model next mixture of x rule. But this one is true one. It's unknown for you. Now, the question is that under this framework, how to choose a subset such a result under this two model you use but use You use, but use this model to choose update. It's the best. This framework is close, but still, we did one more step under this framework. But this, it can be done. It can be done. The framework is there. And I did not try it. Right, right. Yeah, it it it's still yes, it you also can do that. Right. You can you can incorporate the white information here. That you can do that. Name of the mixture of expert mixture of expert means that you can you can imagine this model, it's a convenient. Model. It's a combination. It's kind of one step further than a classic statistic model. For example, classist model is a linear model, right? In the beginning, actually, that even that Linus picture, if you use linear model, you cannot predict that. But you can say if we combine with use a mixture of expert model, you can actually capture that image. But for each expert, it's simple linear model. It's a simple linear mode, but we add a week function there. So it's kind of one step failure that the classic mode. And the last example what we're talking about is the response is categorical. So if we go to generalized need. So, any more questions? You say, okay, you have to call. And here, the example size is quite large. You also learn the whole logistic regression compact. And now And now we all know, okay, um in order to do the classification, we can have other methods. I say if I run my game world, I know would the result be better or worse? So, yeah, good question. Actually, I don't know deep learning, so I never tried deep learning to the, but obviously, I think deep learning would do a better job, better job. But here, what we're trying to say is we want to have a more. What we're trying to say, we want to have a model have an explicit format interpreter model. This is what we, yeah. I also understand that, and I remember sometimes highly creative, and also sometimes we use the different world, which makes the difference between the talk and multi-pound and some of the other people and they feel very amazed because actually even though. even though let's say we choose like one thousand and the result compound with the whole text you can get in the few preferred but time of saving and everything you take a lot of time and take a lot of sorry but i i i understand the interoperability also especially for statistician we see for that but for fear people They don't care that much. They don't care about that much. Right, right, that's right. That's true. That's true. But here, you can see there's really example I give it's quite interesting, right? There's a real example. I directly compare with the machine learning approach. This is a benchmark data set. This benchmark. I don't know that much. I just directly copy. I don't know that much. I just direct copy the result from the paper. And they use 161 features. And it's 2023. It's a new, it's not an old paper. It's a really new paper. And I think from this one, we the 97 versus 95, 88 versus 87, 99 versus 97. I think in classification, when you get 90 something, you improve 1%. Yeah, yeah, of course. It's a big deal. So, and also, this model is interpretable. You know what's going on there. So, and at least for this example, it seems it's comparable. As a please we don't talk about it. 