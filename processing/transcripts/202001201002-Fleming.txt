This talk is basically just going to be a survey on some squares and hopefully you learn something new. I'm going to try to do it from more of the algorithmic point of view rather than the proof complexity point of view, although I'll cover all the proof complexity stuff so that it'll be something new for you guys. Okay, so sum of squares is a powerful proof system whose proofs correspond to a family of semi-definite programs. So recently, it's become a popular tool in algorithmic design. This is because This is important because it's extremely powerful. So it captures a lot of the well-known approximation algorithms, such as the Gomans-Williamson max-cut algorithm, and this really neat result by Ragovandra, which says that it gives the optimal approximation ratios for any CSP under the unique games conjecture, optimal amongst all polytime algorithms. And it's And it gives a pretty simple prescriptive strategy for designing algorithms. So, summa squares proofs are automatizable in a certain way. And basically, you can turn a proof that a solution exists into an algorithm for finding that solution. So then the algorithm design problem really becomes trying to prove that a solution exists and then maybe designing some ramping scheme to add to this. Scheme to add to this. Okay, very quick overview of what sum of squares is. So, this is the outline of my talk. So, we're going to start by developing the sum of squares relaxation from the relaxation perspective rather than the proof system perspective. And so, generally, when we start with the sum of squares, we want to solve a polynomial optimization problem. That is, we want to. Optimization problem. That is, we want to maximize a linear polynomial over a set of non-negative polynomials. So we can plot the points, the feasible points in n-dimensional space. So these are the feasible solutions to the polynomials here. And I'll denote this set by k sub p. And so, yeah, so polynomial optimization problems are very Are a very broad, like broad class of problems. They capture a lot of the problems we want to solve, and so we'd like to come up with a tractable algorithm for solving them. But it's also pretty easy to see that they're NP-hard to solve in general. And so our goal is going to be to come up with a tractable relaxation that achieves a good approximation for many of the problems that we actually care about. Okay, so this is gonna be just kind of like a thought experiment. Going to be just kind of like a thought experiment for designing these sum of squares relaxations. So, right, we have these points in n-dimensional space. We want to come up with a relaxation. So, a standard way to do this is via convex programming. So let's start by thinking about taking the convex relaxation of our points. So we take that convex hall of them. Of course, this isn't practical to implement, but just a thought experiment to motivate it now. And let's also assume that the set of polynomials actually has a solution. The set of polynomials actually has a solution, so our convex hull isn't empty. Yeah, and so because our function or our polynomial r is linear, maximizing over this convex hull is the same as maximizing the original polynomial optimization problem. Okay, so now it'll be useful to take a distributional view of the points in this convex hull. So because every point is in the Because every point is in the convex hole is written as a convex combination of the vertices of the polytope, or like the optimal solutions to the original polynomial optimization problem. We can view them as distributions on the vertices of the polynomial, the solutions to the optimal, or to the solutions to the polynomial organization problem, where the coefficient of each of the points is just given. Of the points is just given by, or the probability of each of the points is just given by its coefficient, the coefficient of that point in the convex combination. So for example, this point mu here, it gives you the distribution which sets point two with probability two-thirds and point three with probability a third. And the actual coordinate of mu, like the Of μ, like the point that corresponds to an n-dimensional space, is just given by the expectation taken over μ, right? So then we can just rewrite our maximization problem as maximizing the expectation taken over the expectation of R taken over distributions which are supported on the true solutions to this polynomial optimization problem. So all we've done so far is just rephrase. Done so far is just rephrased our original polynomial optimization problem in this like distributional language here. There's no relaxations, yeah. So we can view each of these points, these distributions mu, as being described by their aggregate moments, so like all of their moments. So the evaluations of the expectation under mu of the monomial x to the. The monomial x to the i for all i, where I'm going to use x to the i to denote the product of all of the variables that are occurring in the multi-set i. Okay, so now we have this nice distributional view, and we're viewing each of these distributions as sets of moments. And this kind of suggests a natural relaxation. So these collections of moments are extremely complicated, but if we just Complicated, but if we just look only at the moments up to degree d, then if d is a constant, these objects are fairly simple. They're described by a polynomial amount of information. And we could therefore hope that maybe we could optimize over them efficiently. Unfortunately, it's NB hard to actually determine if you're given a set. Determine if you're given a set of moments whether those moments agree with a true distribution on solutions. So because of that, what we're going to do is we're just going to look for an efficient test which takes in these moments of degree B, so the evaluation on monomials of up to degree B, and does a good job of distinguishing moments which look like a true distribution on solutions. Which look like a true distribution of solutions from ones that don't. Okay? So, what we've done so far is basically we've just rephrased everything this distributional language, where every point in the convex hole, we're just looking at it as a distribution. We can describe these as moments, and then we're just thresholding at moments of degree B. So, right now, the way it is stated, I don't see the difference between the NP-hard problem and the thing that the NP-HART problem and this? And this? The thing that you want to do actually, the last one. Yeah, yeah, yeah. So we can't actually, so this, so the MP hard problem would correspond to actually optimizing over the convex hull. But what we want is we just want to look at the degree D things and come up with a good set of tests which test whether these moments agree with true distributions or don't. And the test is going to be faulty, but it's going to work on many of these. On many of these distributions. Does that make sense? Sort of. Okay, maybe it'll be a bit clearer once I can define what the tests are. So, right, so we have these moments of degree up to D. We can think of these as a linear function because we just have evaluations on all monomials up to degree d. And again, we want a set of tests which distinguishes these moments from. From true distribution, from those which don't agree with true distributions. So, one obvious test we can enforce on, and so this is a linear function acting on degree at most d polynomials. And one obvious test, which we can enforce on such a linear function, is that it sets every square polynomial to a non-negative value. And so, this has nothing to do with. So, this has nothing to do with our original polynomial optimization problem. Right now, this constraint is basically just enforcing that these moments look like they behave like a true distribution. So you should think of them as like consistency between all the evaluations it gives to the different monomials. Like they're all behaving consistently because we're forcing that every squared polynomial is in the nick. And then we also want that, right, it's a distribution on solution. Right, it's a distribution on solutions to our polynomial optimization problem. So we're going to force it to evaluate every polynomial in the set to a non-negative value. And then we can also throw in these consistency conditions of degree activity d. So we're going to say that not only is every polynomial p non-negative, but if we multiply it by square polynomials such that the total degree is at most d. Degrees at most d, then this should also be non-negative, right? And any distribution satisfies these properties. Okay, and we'll call this object degree D pseudo expectation for P. And we're also going to add in this extra constraint here, which just says that the one is being mapped to one. And all it's doing is making sure that everything is scaled correctly. Scaled correctly. Okay, so this is, yeah, degree V suit expectation. And so the set of all degree V pseudo expectations for polynomials P is our relaxation of the original convex hull. So any point that exists in this convex hull defines a true distribution and therefore defines a pseudo-expectation, but there are also Expectation, but there are also these linear functions that satisfy the conditions of a pseudo-expectation, but don't correspond to the moments of any true distribution on solutions. And then the sum of squares realization in this pseudo-expectation view is just the set of all degree V pseudo expectations for P, and you can maximize R over it by maximizing the pseudo. It by maximizing the pseudo expectation of this. Okay, and there's n to the d variables, one for each monomial. So, is that good so far? So, our goal was to come up with a relaxation that is tractable to solve. So, we want to show that you can actually solve this relaxation. To do so, we're going to phrase it as a semi-definite. To do so, we're going to phrase it as a semi-definite program in polynomial in the number of variables. Okay, so the idea is that basically if we rewrite each polynomial as a vector product, a product between a vector of monomials and the coefficients, then square polynomials kind of naturally give rise to PSD constraints. Okay, so we need to introduce the monomial vector, which is just a vector of monomials up to degree d. Then you can write any polynomial as its product with a coefficient vector. So for example here, use the monomial vector of degree 2 for n equal to 2, and this polynomial is given by this product here. Okay. And so And so now we want to rephrase this constraint here as a PSP constraint. So plugging in the monomial representation, you get that, and then by the linearity of the pseudo-expectation, you get this. So now this is kind of a natural PSD constraint here because we're running over all polynomials q, and therefore running over all like coefficient vectors for q, which is just all vectors. For q, which is just all vectors, that is basically to say that this matrix here, the expectation of vd transposed times v, is supposed to be PSD, because we want to denominate. Okay, so we can define this, so we're just going to write this pseudo-expectation of Vv transpose, sometimes Vv, as a matrix. It's called the moment matrix. So, yeah, the entry. So, yeah, the entry at coordinate i and j is the expectation of x to the i plus j. So I'm using plus to mean multi-set union. Think I have an example. Yeah, and then the constraint that every squared polynomial is non-negative becomes the constraint that this moment matrix is positive stand definite. So here's the moment matrix for Moment matrix for the degree 2 moment matrix, right? Yeah. It's the product of all monomials of degree up to d. Okay? And then similarly, we can rephrase the constraint that every polynomial in p times the square polynomial is not negative by doing a very similar thing. We define a moment matrix for For each polynomial in P to just be P times inner product of, or the outer product of two moment matrices. And then we just kind of cut it off at degree D. So this is what the ijth entry becomes. It's just the polynomial p times this evaluation of xij. And And so then this constraint can be rewritten as another positive semi-definite constraint, which just says that this moment matrix is positive semi-definite. So it's not super important to understand exactly what this is, but basically you can just rephrase each of these by rewriting the constraint as vector product, and then they naturally become positive set of different constraints. Constraints. So the sum of squares relaxation, which is probably what you're more familiar with, is this SDP here, where we just want to maximize the pseudo-expectation of this linear polynomial R, subject to that these moment matrices are positive semi-definite. And so this is SDP with SDP with has basically complexity n to the order d. And because of this, you can solve it in polynomial time in the size of the SDP times this log one over epsilon factor to within like an additive error of epsilon. And then a solution that you get from optimizing this is on n to the d variables and To the d variables, and to get a solution to the original polynomial optimization problem, you need a way of projecting this solution on n to the d variables down to n variables. This can be done by either a linear projection or by some more complicated rounding scheme. Here's a picture of what we've done. Started with the convex relaxation. We took the semi-definite or the sum of squares relaxation of this. Or the sum of squares relaxation of this, which lifted us to into the d-dimensional space. And then we're just projecting back down to the original space using some method. Okay, so as an example, this is how you can phrase the max-cut polynomial optimization problem. So the objective isn't linear, but you could just linearize it by introducing a few more variables to stand for the Process Sij. So, yeah, so the degree D or degree 2 sum of squares relaxation, you have the constraint that every square polynomial of degree 2 is non-negative. And then each of these constraints, you want them to be evaluated to non-negative values. You never multiply these by square polynomials because you're already at your degree two threshold. Okay, and then the semi-definite program. It's basically the same thing. So here you can see the moment matrices for it. Okay, so the sum of squares relaxation gives you a hierarchy of relaxations. As you increase B, you're just adding more constraints. So it becomes a hierarchy of spectahedrons, which is Of spectahedrons, which is the object that a semi-definite program corresponds to. It's the intersection of cone of PSD matrices with an affine space. So they're tightening, so it's natural to wonder if this converges to the actual convex hull of the feasible solutions to the original problem. So this is not known to be true in general, but it can be guaranteed to hold. Can be guaranteed to hold under some pretty light conditions on the constraints of the polynomials p. And this is going to follow from duality. Okay, so next part I want to talk about duality for sum of squares, and this is where sum of squares proofs come in. Okay, so natural question is: so we wanted, we had a, we wanted to come up with a relationship. We want to come up with a relaxation such that it's tractable and you can get good approximations to many of the problems you care about. So, how do we show that it obtains a good approximation? So, the natural way in complex programming is duality. So, if we want to find some upper bound on the value of the polynomial r over the sum of squares of realization, this This corresponds to finding the minimum lambda, lambda is a real number, such that lambda minus r is non-negative over the sum of squares relaxation. So the sum of squares relaxation contains, all of the constraints are square polynomials. So this corresponds to finding a good sum of squares decomposition of lambda minus r of x. And particularly we want to look at. Lambda minus r of x. In particular, we want to find a minimum lambda such that lambda minus r of x can be written as a sum of the constraints of the original program, like a primal, or in particular a sum of squares of degree d. So to check that this is, in fact, the correct dual, we can immediately prove the following weak duality theorem. So here's the primal, and here's the dual. Primal, and here's the dual. And so, weak duality says that if you have that any solution to the dual is going to be an upper bound on the solution to the primal. So, let's suppose that we have some pseudo-expectation, which is a solution to the primal. I know the sum of squares proof. So, I've just rearranged it here, so you could rearrange it to lambda minus r of x is equal to the square polynomial. And then the weakness. And then the weak duality proof is pretty simple. So the pseudo-expectation of R, you know, because R is equal to lambda minus this sum of squares decomposition, you can apply the pseudo-expectation to that. Then by linearity, you can bring it in. Because the pseudo-expectation sets one to one, the expectation of lambda is just equal to lambda. And then because of each of these polynomials is These polynomials is defined to be greater than or equal to zero. This is just bounded above by lambda. Okay, so that gives you your weak duality theorem. And so writing lambda minus r as a degree d sum of squares is a sum of squares proof that maximizing r over the relaxation is going to be. Is going to be giving you a value of almost lambda. Okay, and so you can study sum of squares proofs in proof complexity. So a sum of squares proof is just going to be a sum of squares proof of the polynomial r for a set of polynomials of p. Yeah, so a degree d sum of squares proof of a polynomial r from a set of polynomials p is just going to be this. You get square polynomials. This, you get square polynomials up to degree d. So, usually, when studying some of squares proofs, we care mostly about the degree, but it's also natural to talk about size because we're interested in proof complexity. So the size of a sum of squares proof is just the minimum-bit representation of that proof. You can also study monomial size, which I'll get to in a minute. Minute. And a refutation is, yep. How do you represent sizing over R, is it important? Yeah, I'm not completely sure. I don't think it should be in, yeah. Yeah, I'm sure there's edge cases, but I Cases, but generally improved complexity it shouldn't be. So, in general, your depends is going to depend on the description of your polytope, right? So, if you've got the constraints that are integers and certain number of bits and so on, that might be I mean, I don't know what those. So, you think it's a upper it'll be upper bounded by the actual I have no idea, but but it's gonna be dependent on that no matter how you define it. No matter how you define it. Yeah, yeah. I don't think it's going to be a huge problem in like proof, like when we deal with like CNFs, but okay, yeah, so a proof's just a derivation of negative one, and because each of the original polynomials were non-negative, if you can derive negative one, then this certifies that the original set of polynomials can't have a feasible solution. Okay, and Okay, and also in proof complexity, we tend to care about proofing lower bounds. So the standard technique to prove lower bounds on the degree of a sum of squares proof is by exhibiting a degree D pseudo expectation. So this is a consequence of our weak duality theorem. But if you can show that there exists a degree D pseudo expectation for P, then there can't be a refutation of P. Basically, it's just saying that there's a point in the Basically, it's just saying that there's a point in the sum of squares polytope. So you couldn't refute it. Or that's what you drawn. And the proof's pretty simple. It's basically just our weak duality proof again. You suppose you have a refutation, you apply the pseudo-expectation to both sides, and you get that negative one is greater than or equal to zero. So you couldn't have a, both of these couldn't coincide. Okay, and so in proof complexity, we care about reputations of CNF formulas. So you need a way to encode these CNF formulas as polynomials. This is pretty standard. I'm sure you've all seen this. You take each clause and you encode it as a polynomial, where the polynomial is just the sum of the positive literals plus the sum of 1 minus the negative literals. And you set it's greater than or equal to 1. And you also include these Boolean. And you also include these Boolean axioms, which say you're only going to look at Organ solutions. So, sum of squares with these Boolean axioms is sounding complete. So we can study it as a formal proof system. And it's actually complete over a more general assumption on the set of polynomials P. So that is this Archimedean assumption. Basically says you have a constraint which says that the Constraint which says that the set of solutions is 2p is contained in some ball of radius r for some r. And the axioms, the Boolean axioms already satisfy this kind of transforming it to that form. Okay. And now, so completeness follows from this theorem from, well, it's a version of the positive Stalin stats, which is Stalinsatz, which is a fundamental theorem in real algebraic geometry. So, this version, due to Putinar, works under the Archimedean assumption on the set of polynomials P and says that a polynomial r is positive over this set of solutions, the polynomials P, if and only if it can be written as a sum of squares proof. Now, this doesn't give you any bounds on the Bounds on the degree of this proof. So, yeah. So, it's natural to ask if you can actually get a bound on the degree that you need. And this can be enforced under things such as the Boolean axioms, which I'll talk about a bit later. Yeah, so it only holds for It only holds for strictly positive. I don't have a good answer as to why it's only strictly positive. Is there a counter example? Yes, I think so. I think so. If all QP R x is bigger than zero, then R next is equal. So if you that also would be true? Yeah, it's it's not true if you replace this strict uh Replace this strict positive by a greater than or equal to. But this is enough for completeness, where we like refutational completeness. So there's the if and only is it just which direction does it fail? That's the question. So it's the eve igoros. Yeah, right. Maybe. May not be if, yeah. Yeah. Yeah. This one we're building. Yeah. The ones we built are actually. Right. Yeah. Yeah. But luckily, this is enough if you want to only refute things. So sum of squares has a degree two refutation of the pigeonal principle. I'm not going to go through this for time. It's fairly standard. It's also small size. But I wanted to cover, I wanted to go back to the primal view of sum of squares and tie up a couple. And tie up a couple loose ends. In particular, I talked before a little bit about convergence: whether we can guarantee that if you take D large enough, that we actually get the set of true feasible solutions. And also a strong duality theorem between the primal and the dual of some solutions. Okay, yeah, so convergence is whether if you take d going to infinity, because this Going to infinity, does this relaxation, does the sum of squares relaxation actually give you the convex hole? So it's not known to hold in general, but luckily it does hold under the Archimedean assumption. So basically for sum of squares, the Archimedean assumption is like the nice assumption. Like everything, well almost everything works out if we assume it. And it's not super restrictive. Yeah, so if we have the Archimedean assumption, we get this convergence. We get this convergence. And the proof of convergence follows by combining our strong duality theorem for sum of squares with completeness, so this Putin artist positive still in sense. Okay, yeah, so that's our strong duality theorem. So it says that the, you know, it's standard strong duality for SDPs, the optimal The optimal of the dual is equal to the optimal of the primal if we assume this Archimedean assumption. Yeah, and positive concepts I've seen. Okay, so it's natural to wonder if we can get faster convergence because taking d to go to infinity is infeasible. So, if we have axioms such as these hypercube axioms or Boolean axioms or hypersphere axioms, which just, you know, the first ones are saying you have to have 0, 1 solutions, the second one's saying plus minus 1 solutions. Then you can guarantee convergence in degree 2n plus the degree of the original polynomial's p. Okay. Okay. And so the last thing we need to do for completeness or for convergence is to actually prove this strong duality theorem. And to do so, probably the easiest way to do it is to rephrase the sum of squares dual as an SDP, searching for the coefficients of the sum of squares proof, and then just use strong SDP duality, which holds, luckily, under the Which holds luckily under the Archimedean assumption. Okay. So our goal is to rephrase the sum of squares dual as SDP. And it's pretty similar to how you rephrase the primal as an SDP. Basically, PSD matrices naturally give rise to square polynomials. So we can, by the Cholesky decomposition, you can write any PSD matrix. You can write any PSD matrix as the square of another matrix. So then multiplying this on both sides by the monomial vector, vector of monomials of degree most d, this naturally gives you a square polynomial. So, yeah, so even if you multiply this, you're going to get a polynomial q plus 10 square. Okay, so then we can rephrase our dual, which was just saying that lambda minus r is a sum of squares polynomial, as finding the minimum lambda such that it's a sum of squares polynomial. Now I've replaced this with the square polynomial with a moment multiplying the moment. Multiplying the monomial vector on both sides of a PSD matrix. And then, same thing. And then the last step, we just want to get rid of the x variables. So we're going to break this apart and have one constraint for each of the actual coefficients of the monovials. So we're just getting rid of the x variables, and now we just get one constraint for each coefficient. This is the This is the indicator vector because we only want lambda to show up for the constant term. And yeah, it's not super important how you see that you get from this one to this one, but that's the sum squared dual as an SDK. Okay, so there's the primal and the dual. And then to get strong duality, you can just apply SDP duality. So it turns So it turns out that these are, well, it's not too hard to see that these are the formal primal and dual SDPs. Okay. And so for the last few minutes, I wanted to talk about some of the upper bounds in sum of squares. And I'll start with some of the issues with upper bounds. So it's often claimed that you can find sum of squares proofs in find sum of squares proofs in time n to the d, or n degree d sum of squares proof. But well, so the reasoning behind this is that you can phrase the dual as an SDP and then you could probably solve it by using the ellipsoid method in time polynomial and the size of an SDP up to some additive error epsilon which we won't really care about. This is not strictly correct. This is not strictly correct, though. In fact, it's not known to hold even under the Archimedean assumption or if you include the Boolean axioms. And the reason is that this basically ignores some of the conditions of the ellipsoid method. So the ellipsoid method requires that your convex body that you're optimizing over contains a tiny ball and is contained. Tiny ball and is contained within some large ball. So this large ball constraint, this ball should have radius polynomial in the size of the SDP. This large ball constraint is basically just saying that you need a proof whose bit complexity is polynomial in the size of, you know, in N to the D or the size of the SDP. Yeah, but this is obvious that this exists. And this was first noticed by And this was first noticed by Ryan O'Donnell and then was expanded on by Ragoventer and Weitz. So they gave an example which showed that there's a small coefficient degree two set of polynomials, P and R, such that there is a degree two proof of R from P, but even up to degree almost square root of n, there's no small, there's no There seems like no small size proof of opera from P. So, this is a big problem. However, there's some good news. So, they gave a set of sufficient conditions under which you can guarantee that the sum of squares is a degree atomatizable. These conditions are a little complicated. It would be actually really nice to get. It would be actually really nice to get some conditions that are a bit more natural, so I won't mention them here. But some of the things that do satisfy these conditions are luckily a lot of things we care about. So they include max CSP, max clique, balance separator, and max bisection. Yeah. Okay. So then is the, comes to the natural question. Is uh comes the natural question. So, in proof complexity, we generally care about size atomatizability rather than the degree atomatizability. Um, so we want to find a proof in the polynomial size and the shortest proof, or the polynomial time, the size of the shortest proof. Uh, so it's still an open problem. Um, it seems, you know, naively, it seems fairly attackable with Sirius and Mulis new techniques. The current like The current best upper bound, which only holds for a special case, in particular under the Ragaventer and Weitz assumptions, follows by, is also in terms of the monomial size and follows by the size degree trade-off. So the monomial size is just the minimum number of monomials in any sum squares proof. And the size degree trade-off gives you this relation between. Trade-off gives you this relation between the monomial size and the debris. And I think there's going to be a talk later on this new size-degree trade-off. And so then you can combine this with debris atomatizability under the Ragoventrum-Weitz conditions to get this upper bound, which only beats trivial brute search or the end to the D algorithm. Search or the end to the D algorithm in very special cases. It's not very. It's the same trade-off roughly as we get for resolution, the same upper bound roughly as we get for resolution proof search. Yeah, yeah, yeah. Or PCR proved search. Yeah, but it's only holding under these special constraints. So it's, yeah, I don't see as much of a barrier. Yeah, okay, so because of this, because of the issue. So, because of the issues with automatizability, usually upper bounds and sum of squares proceed by leveraging strong duality together with the n to the d time algorithm for solving the primal, which works under the Archimedean assumptions. So, this allows you to transform a certificate that a proof exists into an algorithm for finding it. And then you can combine this with some clever rounding scheme or something like that. Rounding scheme, or something like that. So the early works on some squares just showed that you could rephrase many of the well-known approximation algorithms as instances of some squares. So the degree two Gomez-Williams, or degree two sum of squares captures the Gomans-Williams algorithm together with the hyperplane rounding technique. The first instance of higher degree sum of squares was in this. Was in this work by Aurora Round Botsirani, I believe, which uses degree 4 sum of squares to get a square log n approximation to sparsest cut. So then following this line of works beginning with Kot, Kilner, Mozell, and O'Donnell, they uncovered a deep connection. They uncovered a deep connection between sum of squares and the unique games conjecture. This culminated in the work of Ragavendra, showing that, assuming the unique Games conjecture, that degree 2 sum of squares gives you the optimal approximation for every CSP over polynomial time algorithms. However, it's interesting to note that this doesn't actually tell you, like, even if unique games is true, it doesn't actually tell you. Gains is true, it doesn't actually tell you what this approximation ratio is. Okay, so in a surprising work in the other direction, a line of work showed that there's a sub-exponential time algorithm for unique games, and it can be captured by sum of squares. So, in doing this, they introduced this global correlation rounding technique. So, it gives a very high overview of what this A very high overview of what this is. So, if you have a pseudo-expectation, a natural way to round, to obtain a true solution, is to just take the degree one moments of this thing and just round according to those, or in particular, round each variable with probability given by the expectation of that variable. However, this can perform really well. However, this can perform really poorly if you have correlations. So, what global correlation rounding does for degree two CSPs is it says that if you condition on a large enough random subset of variables, then in expectation your global correlation drops, but your optimal value remains the same. So, this has become a cornerstone technique in designing algorithms. Cornerstone technique in designing algorithms using semi-squares. And so following this, a work, a line of work by Barack, Cathari, and Stoyer developed new rounding techniques for higher degree semi-squares. And they used it to get algorithms from problems in quantum interformation theory, such as S-separable state. And then, most recently, I guess there's been a long line of works that have focused on. Lab works that have focused on designing average case algorithms using sum squares. A lot of these are due to this average case rounding framework, which was introduced by Brock Cathari and Stoyer. I think I have five minutes left. So I can cover a little bit of lower bounds. So we'll start off with what's known between some squares and other proof systems. So here we're like a solid arrow line. A solid arrow line indicates that the proof system at the head simulates the proof system at the tail, and there's a separating example. So there's something that requires high degree in one and has short or small degree in the other. So many of these simulations, as well as this really cool, or separations, as well as this really cool simulation of polynomial calculus by sum of squares, are due to this nice paper of Birkenholtz. So, in terms of degree for these proof systems, everything's solved. But in terms of size, things are a little less known. So, we can compare more proof systems in terms of size here. So, now a dotted line with an arrow means that the proof system at the head has short proofs of something that the other proof system does not, but it's unknown. Proof system does not, but it's unknown whether the proof system of the head can simulate the other proof system. In many of these cases, it's probably not the case, but it'd be nice to have a separate example. So examples of this are AC0 Frege cutting planes and RCP or this stat and planes proof system. So we're not sure how they compare. How does a Frege simulate some of squares? I think it. I'm pretty sure it does, but I don't have a good example, or I mean a good answer. Is it for CNFs? Yes, yes, it should be for CNFs. Yeah, that. Yeah, I should have said that. So all these are here for CNFs. It's likely not the case, I think, if. I'm not sure it is cool or If rather a stimulant sum of squares, but maybe there is some easier I think it is true. I'm not sure it is really. Yeah, I don't have a proof of it, but I thought it was true. And do the clearing both the sound is flagging and both. Oh, yeah, yeah. I'm not sure if you can hear the oil finish. Oh, it's a part of the proof. Just check that this thing keeps uh a positive part. There are real ones. Okay, uh let's let's do this offline. So I'll get through a little bit of lower bounds, I think, the last couple of minutes. Um Bounds, I think, in the last couple of minutes. Yeah, so there's been a lot of work done on lower bounds. It's kind of an exciting area because you can, lower bounds on summarized proofs imply approximability results for a lot of these, for this powerful class of algorithms. So if you can show that degree v sum of squares cannot refute a set of polynomials p together with a constraint saying that r takes value at least lambda, then maximizing. Lambda, then maximizing R over the set of polynomials P will give you a value of at least lambda. So you can get approximability results here. And the standard way to prove lower bounds, at least degree lower bounds on sum of squares, is by constructing a degree V pseudo expectation. So the first result in terms of sum of squares lower bounds was by Grigoryev and then independent. By Grigoriev, and then independently Schoenbeck. They showed, amongst other things, lower bounds for random 3XOR equations. Their proof is reduction to resolution lower bounds, or can be seen as this. And this builds on earlier results for Milstall and Satz and polynomial calculus. And Schoenbeck also showed that this implies lower bounds for max 3 sat and For max3 set and max independent set. And so sum of squares has a tight connection with CSPs. And so you can actually, you can get good approximability results for CSPs. So I'm going to take a slight detour and talk about the idea of approximation resistance before we talk about the results for sum squares. Results for sum of squares. So basically, what approximation resistance is, it says that problem is approximation resistant if the best polynomial time algorithm is basically just give it a random solution, a uniformly random solution. So Chan showed that assuming P0 NP, any predicate which is pairwise independent and algebraically linear is approximation resistant. It's not really important what these two conditions are, but... What are these two conditions, but basically pairwise independent means that the set of solutions to this predicate supports a distribution whose pairwise marginals are uniform distribution. And algebraically linear means that this distribution is also uniform over some subspace of GF2. Okay, and then following this, it was shown that assuming the unique Games conjecture, any predicate which just satisfies the pairwise uniform Satisfies the pairwise uniformity condition is approximation resistant. Okay, so now we can talk about it for sum of squares. So before they were interested in showing approximation resistant for every algorithm conditionally, but now you can show it... You could also show it for a particular algorithm, but unconditionally. So for sum of squares, an algorithm is, or for sum of squares, a CSP is approximately. Squares, a CSP is approximation resistant if there exists an instance such that their random assignments are optimal, but Summa Squares believes that almost every constraint can be satisfied. Okay, so Tolciani showed that any CSP supporting a pairwise uniform and algebraically, any CSP on a predicate that supports a pairwise uniform and algebraically linear distribution. This distribution is approximation resistant for omega of n sum of squares. So he also introduced a method for doing reductions in sum of squares and showed weak lower bounds for vertex cover and independent set or weak approximation lower bounds. So it's open problem to a nice open problem I think to try to improve the approximation lower bounds that we have for Lower bounds we have for vertex cover for sum of squares. I think this is a fairly long-standing problem. And then following this, Barak Chan, I believe, and Kazari showed that this pairwise uniform condition is enough for refuting CSPs and sub-squares. Then finally, there's been There's been a lot of work done on average case lower bounds for semi-squares. So, Qathari, maybe Moitra, Donnell and White proved, so they characterize the number of constraints you need in order to refute a random CSP for a given predicate P. This matches previously known upper bounds. Upper bounds. Then a line of work showed lower bounds for plan to clique, which were culminated in this work by a long line of authors, Barack Hopkins, Kathari, Kilner, Mechan Poteschin, I believe. Moichran. Yeah, yeah, yeah. And they introduced the pseudocalibration framework here, which is like a computational Bayesian way to design pseudo-expectations. Bayesian way to design pseudo-expectations. Okay, so I have a couple more slides on lower bounds using SDP extend formulations, but these are basically covered by Paul earlier, so I'll stop now. Thank you. Questions for Noah? So ls will do n depth n will be able to do CNFs. So if you have the number of variables after n rounds. So you mentioned about 2n or something like that. 2n plus the degree of p. So for p the let's assume that's one. So s so there is a gap there is a slight uh shift there. Is that because you're counting the degree as of the things that are getting squared? Is that Of the things that are getting squares, is that? Yeah, yeah, exactly. So, like, one way to see this is that degree 2n sum of squares implies, 2d sum of squares implies that it's, like, kind of constrains it to be a true distribution on points up to D. The positive semi-definite constraints don't actually force the rest of the higher than D things to be a true distribution. So it's going to be running slightly behind schedule. Let's thank the speaker again. Remaining questions on flight. And so let's maybe