Okay, hi everyone, thanks so much. So yeah, so I'm Sarah Marzen. It says here that I'm from Claremont McKenna College. I'm actually from the WM Compact Science Department, which services Pitzer Scripps and Claremont McKenna College. They're the Claremont Colleges in Southern California. You might have heard of like Carbon Mudder Pomona. They're within walking distance. Anyway, so I'm going to talk about the relationship between prediction and dissipation and most of the time I'm going to spend on the Chalk Talk is going to be Spend on the chalk talk is going to be spent trying to get faster calculations for prediction dissipation metrics. So, by the end of the talk, I'm hoping to have convinced you that these things called causal states, which I'll define, are really useful for getting faster calculations of these metrics. And then I'll talk about what you can do with some toy examples to get some intuition for the relationship between prediction and anticipation. Okay, so the basic setup that we have is this. Oh, by the way, I should say, um I should say. Conditionally, so the technical thing that I'm going to be doing is conditionally Markovian channels driven by unifel or hidden semi-Markov processes. It's basically most channels driven by complex outcome. Okay, so that's the short way to come. What is unique? You'll find out. That's a good question. You'll find out. So, I'm going to use machines like this. So, this is a machine right here. I'm going to use a machine like this to generate outputs of living. To generate outputs of ligand concentration that look like this: it could be ligand concentration, it could be temperature, it could be something else. Something that you vary in the environment, and the thing that you would vary in the environment affects a conditionally markovian channel, which is some sort of sensor. So the example that I have here on the screen is a ligand-gated ion channel, in which if you're open, there are end ligands bound, and if you're closed, there are no ligands bound. And so, this ligand concentration comes from a complex machine, and what we want to do is we want to And what we want to do is we want to look at the state of this molecule, this ligand-mated ion channel, open or closed, and try to use it to predict what happens in the future for the ligand concentration. Okay, so that's the prediction part of it. The dissipation part of it is that this dissipates some heat into the environment around it, and we want to know how much that is. We want to relate the two things somehow. So, the questions that we're going to try to answer in this talk are the following three. So, the first one is what I'm going to First, one is what I'm going to be spending the bulk of my time on. So, can we develop a fast way to compute seemingly uncomputable quantities, such as the mutual information between the sensor state and the entire future of the input? And I'll read that again, the entire future of the input. So, really seemingly uncomputable. And that way we can calculate how much these sensors predict and dissipate. Okay, so once we do that, we're going to get into some toy models and try to get some intuition for the relationship between prediction and dissipation. Between prediction and anticipation. So, first, you want to answer: can we make our sensors more efficient predictors just by making them larger? So, you might think, well, this is an easy problem to solve. All you have to do is make sure that your sensor is more complex, then it will have more memory, it will be able to predict better. So, what if we just do that? And then I'm going to tell you about a thermodynamics of prediction bounds, and I'm going to talk about whether or not it's tight, because, of course, as we all know, the great thing about these bounds is that they exist, the bad thing about these bounces is that they exist. That they exist, the bad thing about these balances is that they're not often tight, and so they don't provide too much intuition. Okay, so here are the prediction anticipation metrics that we're going to be dealing with. So the first one was what I promised. How well can you predict the entire future of the input? So this we're going to call iFuture. This is the mutual information between the sensor state and the entire future of the input. So this is the X, this is the Y. The second one is a component of iFuture, but we call it iMemory. Of iFuture, but we call it iMemory because we are paying homage to a paper written by Santa Still called Thermodynamics of Prediction. So, this is the mutual information between the present sensor state and the present input, and it's basically a component of IFuture, but not the same thing. And then these I call dissipation metrics, and you'll see why I call this a dissipation metric in just a second. So, this is called a non-predictive information rate. In Suzanne Still's seven old paper called the Random Prediction, this was just called non-predictive information. Prediction. This was just called non-predictive information. It was a discrete time thing, so this t plus delta t was replaced with a t plus one. But if you want to move to your continuous time application, all you have to do is replace t plus one with t plus delta t and divide by delta t. And you get what's called the non-prefective information rate. And basically, it's you have some memory. What amount of memory is useful for predicting what happens immediately in the future? Subtract that off, divide by delta t, and then we have the dissipated work. So this is the temperature. Work. So, this is the temperature normalized power extracted from the environment. It's also the heat dissipated to the environment because we're going to be dealing with non-equilibrium steady state. Okay. So I just wanted to go through the thermodynamics of prediction with you because I don't know if you've seen it, but it's actually a very simple proof. It's very general. You don't have to assume no feedback. The only thing we're going to be assuming is that there's a non-equilibrium steady state, and there's a version of it that is. And there's a version of it that applies when there isn't any type of. So, this then turned to a chop talk, assuming I can look more sad. Okay, the basic idea behind this proof is that the non-equilibrium free energy is a Lyapunov function for the dynamics. So, what we're going to imagine. What we're going to imagine is that we have the environment xt at time t and we have yt at time t as well. Okay? Okay, so then what's going to happen is we're going to have two steps. You can break it up into work then heat, or you can break it up into heat then work, it doesn't actually matter. It doesn't actually matter. So, we're going to break this up into first the environment advances. So, this right here is going to be a work step. And the reason we call it a work step is because all the energy changes that happened are ordered energy that we can actually use. And then, what's going to happen is the sensor relaxes, okay? Sensor relaxes, okay, and this is a heat step. Disordered energy is produced, and together, the work step and the heat step constitute one whole time step. Okay, so this is a completely artificial distinction. Like I said, you can just change this around. So you do the heat step first and then the work step. The only thing that actually matters is this to this. And if you've read the certain dynamics of prediction paper, this is my only gripe with it. My only gripe is that they made it sound like there was an equality, but the equality wasn't. Was an equality, but the equality was for the work step only. So, anyway, so basically, there is a non-equilibrium free energy, and I'm going to write that up now. And the non-equilibrium free energy is given by this. So basically, if you knew, basically the idea is that if you knew something about the environment, you would be able to do a little bit better than the equilibrium free energy, where you don't have these conditional dependencies on X. So this is almost equilibrium free energy. The only difference is things like this Y given X. And this is going to be crucial for the proof of the thermodynamics of projection. Okay. Okay, so the only thing that we need in order to prove the statement on the board is the following. We're going to say here Basically, what I'm saying is that during the heat step, you relax towards equilibrium given the environment. And so what happens is this non-equilibrium for the energy becomes a lapimal function for the dynamics. Okay, that's all you really need. You can now rearrange this entire thing so that you get the following expression. I'm going to write it, I'm going to try to write it on. It on the rest of this board, but it's a kind of a long expression. So you get beta limit as delta t goes to zero average value x t plus delta t y t minus e x t plus delta t minus e plus delta t so volume low So for me to zero. And here we've done an ensemble average over the environments. Or if you have herbidicity, we can say this is a time average. This per term. Yes. Yeah, sorry. Okay, so this term right here is going to be a heat. This term right here, after a few information theory identities, is going to look away. Few information theory identities is going to look a lot like non-predictive information writing. I won't go through the entire derivation. All I'll do is assert that if you play a few tricks using the fact that you're in a non-equilibrium steady state and using a few information for your identities, you can get back this. And I'll go back to the previous slide so you see what quantities we're relating. So, for instance, to turn this into mutual information, what you have to do, you have to rewrite this as the rewrite this as the entropy of y of t plus delta t minus the mutual information of x of t plus delta t and y of t plus delta t. So just stuff like that. So that's not so illuminating, so I'm not going to show it, but this hopefully shows you how few ingredients go into this proof. So this is a very general statement. Okay, so that's a bound that we would like to test its tightness for. So this is a cool bound in the original thermodynamics. In the original thermodynamics of prediction paper, you can use this to tighten Landauer's bound on an erasure. So let's see how tight it is. Okay, let's go back to the questions we would like to answer. So the first one was, can we develop a fast way to compute these seemingly uncomputable quantities like mutual information between the sensor state and the entire future of the input? And I'm going to argue that yes, we can, but we just need to use these things called causal states. Can we make our States. Can we make our sensors better just by making them larger? I'm going to use the fast calculations that we get over here to compute things that you would not be able to compute from simulation to answer the question with no. And then, is the thermodynamics of prediction that type? And the interesting thing is, yes, sometimes, but just where the sensors are maximally predictive is sometimes where it's not that type. So it's kind of an interesting thing going on there. Okay, so now I'm going to tell you all about these causal states. These causal states. There are basically two things that I want you to get out of what I'm going to tell you for the next however many minutes. The first one is there are these things called forward-time causal states, which are what you need in order to predict. The second thing is that there are these reverse-time causal states that are what you need, what you can predict about the future. So these are what you need in order to predict. These are what you can predict. And we're going to play around with them. They're going to make a lot of calculations simpler. What I'm going to do now is I'm going to go on to What I'm going to do now is, I'm going to go on to the most basic example of one of these causal states and the Hidden Markov models that come from them. These are going to make the calculations a lot easier, but first we have to move from discrete time to continuous time, and then we have to do some partial differential equations, and it's going to get pretty messy. But we're going to start simple. So, what we're going to do, how many of you have seen human work on models like these before? Okay, some of you. So, this is how you read them. So, this is how you read them. This has two states: it has a state A and a state B. In state A, this is an edge-emitting hidden market model. So, in state A, what happens is you flip a fair coin. That's why you have probability a half and probability a half. And with probability a half, if you see, you see a zero, and if you see a zero, you transition to state A. With probability a half, you'll see a one. If you see a one, you transition to state B. And then if you're in state B, you always see a one. You always see a 1, and you always go back to state A. So that's how you read these kind of Markov models. So, one thing to notice about this new Markov model that makes it a little bit weird is that there's this unifolarity property going on. So this is how I'm going to define unifolarity. If you know what state you're in and you know what symbol, you see, you know what state you go to. Okay, so for this Hidden Markov model, if you're in state A and you see a zero, you know you go to state A. If you're in state A, If you're in state A, you see a 1, you know you can go to state B, and if you're in state B, you always go to state A. So that's completely deterministic. So some people call these probabilistic, deterministic automata. So sort of capture the fact that there is a probabilistic component to what you see. However, there is an underlying deterministic component to how you transition between the states. How would you make it non-unifielder? I'm not sure I understand. Yep. This is a non-unifielder. This is an example of a non-unifielder in. This is an example of a non-unifield in Marvel model. So if we look at state A here, there's just two states. If we look at state A, if you see a zero, you're not sure if you're in state A or state B. Okay, so it's not a good feel. Okay, so now we're going to play a game. I'm going to write down a series of symbols that could have come from this hidden markup model. So there you go. Okay, so let's say that we just saw two zeros and we see three ones. Take 15 seconds, make your guess as to what comes next. By the way, we actually operate, my collaborators and I are actually doing cognitive science experiments on humans with stuff just like this to see if humans can predict well. So just see how good of a predictor you are. You just saw two zeros and three ones and some stuff that came before. ones and some stuff that came before that I assert is not too relevant. Okay, um is A 0 and B 1 or is A 1 and B 0? Not quite. Yeah, these are hidden states so you don't get to see them. So A could be 0 or 1. I mean A is not 0 or 1. A is just a hidden state that can emit a 0 and can email 1 and B is a hidden state that always emits once. Does that make sense? Does that make sense? So A and B are like the environments? This model is going to be used to generate an environment. So zeros and ones are more like the environment. So zeros and ones are what we see. Like over here, I'm telling you I see 0, 0, 1, 1, 1. I'm not telling you anything about A and B. So A and B are hidden states. If I think of this as a communication channel, what's the source? And what's the channel? Um, I'll get into that, but I don't think that's a I'll get into that, but I don't think that's a great way to think about it. Hold that question. Isn't the source of two-state markup process given by A and B? Is it transformation? Oh, you could think about it sort of like that. So you're thinking about it as A and B form an underlying Markov process, and then there's a channel where if you're in state, if you see an A, there's a noisy channel, and then if you're in state B, there's a non-noisy channel. That's one way of thinking about it. Yeah. Okay. Okay, so hopefully everybody's made their prediction. Everybody close your eyes. I just want to see what happened. If you guess a zero, raise your hand. If you guess a one, raise your hand. Okay, everybody guessed a one. Can somebody tell me why they guessed one? Do you have to state me because you've had a lot of ones? Great. Is it a lot of ones or is it something else? Can somebody clarify? Yeah, because you're in zero, right? So you got zero, zero, one. So you start one. One, one. Perfect. So I'm going to rewrite this model as something else, which is that state A is actually the collection of all paths that end with a zero and an even number of ones. And state B is the collection of hashtags with a zero and a non-number of. Pass that into the zero and a non-number of ones. And now you have the same things going around. So some people look at this and they go, oh, this is an order R Markov process. This is finite order Markov. Because, you know, they look at this and they say, well, we can figure out exactly what these states are. These are not really hidden. They're unhidden. The one thing we can't figure out, what if you see a past of all ones? So you need to go back arbitrarily far into the past. So that makes the process. So that makes this the process generated by the same Markov model an infinite order Markov process. So not unifiller, it would be where you can't take all binary numbers and assign them to either A or B. That's right, that's right. And we'll get into exactly how you convert from a non-unifiller into a unifiler next. Yeah. Okay, so there's a formal way of defining this. I'm going to write it on the board because this is a mathematical audience. Okay, so we're going to now consider two paths, x equivalent to x prime. We call this an epsilon equivalence relation because there were some reasons from chaos theory to do this when Jim Hartfield invented all this. We're going to consider these equivalent in the probability distribution over futures given the past. The future is given the past are the same. And essentially, what you do then is. Yeah, so this corresponds to, this is what you see, these x's. This corresponds to the entire future of what you're going to see. This is the entire past of what you're going to see. And this is a realization of the random variable, which is the past. Okay, so then what we're going to do is we can use this to coarse grain and divide up. So course we're going to divide up the set of all pasts into 1 to the infinity, dot dot dot, 0, 1 to the 2n, and dot dot dot, 0, 1 to the 2n plus 1. These causal states are the only thing I need to know in order to predict. They are the minimal thing I need to know in order to predict. So you can't make it any smaller. This is the best you can do. And if you construct a hidden Markov model out of these causal states, Markov model out of these causal states, you will always get something that is the minimal unifular hidden Markov model that corresponds to generating the process that you see. The other way of putting it is minimal maximally predictive model, and these two things are exactly equivalent, and it's not easy to see that that's actually the case, but there are some nice papers with some proofs. Okay, so that's a causal state. The reason it's going to be useful for us is because if you know what you need in order to predict, then you can do a better job at calculating these. Then you can do a better job at calculating these prediction metrics. Okay, and you'll see how. Okay, moving on. So now we have, so I want to make two claims with this slide. The first claim I want to make is that generically we're dealing with infinite order Markov processes. The other claim I want to make is that generically, we're dealing with causal, an infinite number of causal states or infinite epsilon issues. So it looks like it's intractable, but it's actually tractable. But it's actually tractable. So, this is a two-state machine. It generates a series of zeros and ones. The special thing about this is that it generates a renewable process. So, if you see a one, then you see some number of zeros. You see a one, you see some number of zeros. There is some intra-event distribution that governs the number of zeros you see, and that's all you need to know. Okay, so there is a way of turning this into the epsilon machine. The Epsilon machine. And the reason I mentioned this is mostly to illustrate that you can do this for any process you like. So, the key thing to know is: first, you have to figure out how you synchronize to the hidden state, and then you have to figure out how you keep track of the probability distribution over the hidden states in a non-unifilar hidden Markov model. So it's a mouthful, but basically, in order to predict what you need to do, you need to keep track of your probability distribution over these hidden states. These hidden states tell you how to predict. States tell you how to predict. So, what happens is that you sort of get a counter when you look at the cosmo states. Cosmo states are going to count how many zeros you've seen since the last one. If you see a one, you know you're in state A. You know how to make a prediction. If you see a zero after one, you don't know if you're in state A or state B. You don't quite know how to make, you can't use one thing you could do that would be suboptimal would be to say, Optimal would be to say, oh, I'm basically in state A, or oh, I'm basically in state B. That would actually be sub-optimal. What you want to do is keep track of the probability that you're in state A and the probability that you're in state B, given that you just saw a zero. And you want to keep going down that line. And what happens is the epsilon machine ends up taking this form. So at the top, you have, this is when you've reset when you've seen a one. Then you go here and say A. And you keep on going down as you see zeros, zero after zero after zero. See zeros, zero after zero after zero. Down when you see an infinite number of zeros, you're in state B. And if you want to figure out how to predict the process, all you have to do is know which one of these things you're in, figure out what these transition probabilities are on the edges, which there is an explicit formula for, and then compare this to this, figure out which is greater than a half, and predict it fully. Okay, so let's see if there's anything more I want to say. So you can do this for any model. Again, so just Again, so generically, everything is very hard to deal with. Generically, everything is infinite order Markov. Generically, everything has a countable or even worse, uncountable infinity of causal states. We're going to be focusing on the cases where the causal states are tractable. Yes? Good question. For the A to B transition, if you modify this into the 1 minus P1, then does it mean there is nothing we can infer at all, or shall there? Infer at all, or shall there still be something we can infer? Oh, no, it would just have a completely different structure. So, one of the things that's true about these causal states and epsilon machines is that they are very sensitive to the underlying structure of the process that you're using. And if you turn this into a one rather than a zero, you just, it's a completely different machine. So, this would be like, I think this would be a very, it would be a very simple epsilon machine. Yeah. Yeah, thank you. Okay, so what I want to do is, I want to demonstrate now that these causal states are useful by talking about something that maybe it doesn't seem like we should care about yet, but I will argue that this is an interesting thing to care about. So what if what we want to do is we don't want to get the causal states because there's an uncountable infinity of them, but we want to get, or like just a huge number of them, but we want to get approximations to them that predict pretty well. So what we're going to do is we're going to take So, what we're going to do is we're going to take the past of the input. If this is reminiscent of the information bottleneck method, you've got it completely exactly right. We're going to take the past of the input. We're going to have some sort of memory trace of it. This is our variable to compress. We're going to compress it into an R. What we're going to try to do is minimize the mutual information between these two so that we have minimal memory, but maximize the mutual information between these two so that we have maximal predictive power. Now, the weird thing about this is that. Now, the weird thing about this is that the only information that comes between this and this comes through the past. To know about the future, you have to know about the past. So there is essentially a bottleneck where these two things are competing. On this diagram, the way to envision this is on the x-axis you have something that looks like a memory, which is the mutual information between your sensor state and the past. And on the y-axis, you have And on the y-axis, you have prediction, which is the mutual information between your sensor state and the future of the input. And you can make, there are some sensors that are allowed, they exist. There are some combinations of memory and prediction that are allowed. They actually exist. And there are some combinations that are forbidden. So for example, if you have this memory right here, you can have up to this predictive power and no more. Okay, so there's a line that divides the allowed from forbidden. Causal states sit right here, they're the minimal. They're the minimal memory that you need to capture all of the predictive information that you possibly can. And so, what we want to do is we want to actually compute these lines. I'll give you a reason for my own research. So, we looked at, we're looking at humans right now, like I said, playing this game with them, and we are essentially making these curves and then plotting where the humans are on these curves. And it turns out that humans are very, very close to these curves. They are basically near optimal. They are basically near-optimal memory-limited predictors. So, we want to actually get these curves right. Otherwise, we'll come up to false conclusions saying that humans are optimal when they're not optimal, or saying that humans are very far from optimal. But the most horrible thing we could do would be to have a curve that's incorrect. Say the curve looks like this. Say the human is right here. We say they're optimal. They're actually far from optimal. This is scientific malfeasance, in my opinion. So, let's try to get these curves. So let's try to get these curves right. So, what are these curves? The curve is this. What we're going to try to do is we're going to try to maximize the predictive power over memory traces that have a limited memory. You can turn this constrained optimization problem into an unconstrained optimization problem. Then you can take the derivative of this with respect to the memory trace and set it equal to zero, turn that into an iterative equation, and solve it. That's called the generalized logarithmic regular algorithm. Yes? I gotta open them, yes. I'm sorry, can you explain the restriction again? Yeah. So, this is what I would call memory. This is the mutual information between the sensor state and the past of the input. Basically, we don't want to have too much storage for what the variables that our sensor is. And so we say we don't want this memory to be above a certain value R. Yeah. Okay, so this seems very nice. So, this seems very nice. You might have spotted the problem with this. The problem with this is that it doesn't look like you can do anything with these. This is an example of a seemingly uncomputable thing. So what did people do before a certain paper that I'm going to be talking about in a second? They said, well, we'll look at finite length pasts and finite length futures. Now, if you've dealt with entropy, block entropy approximations, you'll know what the problem is. There's a person dimensionality. Is there's a person dimensionality. So if you're trying to get the entropy rate exactly on a point, for example, you're going to be looking at futures that are of like L, even for a binary value process, that means you look at 2 to the L number of pasts, and it gets harder and harder to compute these things. So if you want an accurate approximation, you want L to be as large as possible, but your compute power scales exponentially. So the question is, how bad is it? Let's take the process that you Let's take the process that you guys predicted so easily. So, this generates the thing called the even process that you predicted really well. And this is what happens if you set L is equal to 2. This is what happens if you set L is equal to 3, 4, and 5. So it kind of looks like it's saturating. It kind of looks like you're doing a pretty good job of getting out what the allowed region is and what the forbidden region is. In fact, but there's a key to tell you that that's not. But there's a key to tell you that that's not quite true. In fact, if you look at the total predictive information that you can capture, it's way up here. So something is going wrong. And it turns out that the true line is actually this blue line right here. Okay? And in order to get this, we needed a new theorem that involved these four timing reverse semi-states. Now I'm going to write down what a reverse time causal state is. So this is the The four-time causal state. So the reverse non-causal state is a little bit different. These two futures are coarse grained together, they're clustered together if the probability distribution of past given futures is equal. Okay, so you do this for the even process. Turns out it's exactly the same as it is for the for time case. And it's important to know that if you have a hidden Markov model, you can get the joint probability distribution of forward reverse temp causal states. It's not easy, but you can write it. It's not easy, but you can write up the code for it. Okay, so it turns out that this theorem gives us everything we need. We can replace this past with the four-time causal states. We can replace this future with the reverse-time causal states. And the basic reasoning is, well, the only thing we need in order to predict is these four-time causal states. So we can always form a sensor that is better by recasting it so that it only uses the four-time causal states. Forward-time causal states. And then you know you only need those. And then it turns out that these are the only things we know how to predict. These are first-time causal states. So you can replace the future with the reverse-time causal states. And then you can actually nail this analytically from this really simple process. Okay, so I would like to argue from this that without these causal states, you're kind of lost. What does that mean? So let's say that somebody gives you sequence data. Somebody gives you sequence data, you have basically two ways of approaching getting something like one of these predictive information curves. The first way of approaching it is to say, well, I'll just deal with a curse of dimensionality. I'll look at longer and longer futures. I'll look at longer and longer pasts. I'll go straight from here to here. I'll try to do a good job with entropy estimators, all that stuff. Another way of doing it is to say, well, I'm going to actually infer a model that generates what I see, and then I'm going to turn the model into my prediction for. To turn the model into my predictive information curve. So, two ways of getting at it. There is actually another note, which is for the experiments that we're doing with these humans, these cogs eye experiments, we actually give them the stimuli, and we didn't just put down zeros and ones randomly. We had a hidden Markov model that generated it. So, we know exactly the hidden Markov model. So, we go straight from here to here, and we bypass this step entirely. So, this can be very powerful for dealing with that. Powerful for dealing with that kind of experiment, and that's exactly what's going to happen here. So, we're going to be dealing with toy models where we know exactly what model generated the ligand concentrations, and we know exactly what our sensor model is, and we're going to be looking for design principles. So, that gets into these continuous time epsilon regions. So, in order to actually do the calculations of these prediction and dissipation metrics that I want to do, we have to move into continuous time dissipation. We have to move into continuous time discrete event processes. And I'm going to summarize a lot of essentially PDEs on one slide. And I'm not going to show you the PDEs. So the first thing I want to show you is that, do you remember the counter, the 0, 1, 2, 3, 4, all the way down to infinity? In continuous time, it turns into this line. And basically, if you look at a renewal process, this is the epsilon machine for the renewal process. This is, if I Renewal process. This is if I see an event, I reset. This is, I keep on going as long as the time since the last event keeps on going. So this is again a counter. It's just a continuous time counter. In continuous time, what do these epsilon d she's look like? They look something like this. This is a beautiful artistic depiction by Jeff Critchfield who loves these art and science mixes. So underlying this is the promised unifeler. The promised unifeler hidden semi-Markov model. So, what you have is three states A, B, and C. And you have nearly the same thing that you had for the discrete time discrete event processes. The only difference is you have a probability of emission, you have what you emitted, and then you also have a dual time. Okay, so here we see that with probability one in state A, you emitted two, and this occurs for a dwell time drawn from this dual time distribution. In state B, you flip a coin, with probability You flip a coin with probability a half, you emit a zero, and you draw a dual time from this dual-time distribution. With probability a half, you emit a one, and you draw a dual time from this dual-time distribution. And then state C, with probability one, you emit a three, and you draw from this distribution. And then what you end up getting for the epsilon machine corresponding to this generative model, because this is a generative model, not the minimal maximum predictive model, is something where you have these like These conveyor belts that correspond to the time since you've last seen the last symbol. And as soon as you see the next symbol, the conveyor belts shoot down to the next state. So you can actually work with these. It's tractable. It's just partial differential equations all over the place. And essentially, the causal states correspond to knowing if you're in state A, B, or C, knowing what symbol you just saw, and knowing. What symbol you just saw and knowing the time since the last symbol. And that's it. Okay, so on to the thing we actually want to compute. I am now going to write up. As promised, there's going to be some PDEs in here. So what we want to compute again are I future and I then and non-predictive information rate and Competitive information rate and dissipated work. And what I'm going to focus on for the purposes of illustrating that you can actually use these things to do calculations is I'm going to focus on that feature. So, how are we going to compute this idea? How are we going to compute this I future? So, this was the mutual information between the reverse sign causal state. Oh, sorry, I skipped ahead myself. This is the mutual information between the sensor state and the entire feature of the input. It turns out that you can play the same game that you did a few slides ago and replace this with the mutual information between the sensor state and the reverse time positive of the input. Okay, so now you've already got. Okay, so now you've already got down to something that's tractable. And in order to compute this, all we need is P of y is sigma plus. This is a realization of the four-time positive state. The reason for this is we know two things. P of sigma minus g sigma plus right p of y t plus delta t plus delta delta t. So this is a property of our input that after an entire PhD thesis I was able to get, this is a property of the sensor state that just comes from knowing a conditionally marked protein channel and having somebody tell you this is what the conditionally marked protein channel is. This is what the conditional wave mark on the channel is. So, what that means is that in order to compute this, this relies on p of y and sigma minus. What we have is just p of y and sigma minus is equal to the sum over sigma plus of p of y and sigma plus p of sigma minus sigma plus. I'm sorry, do you mind trying to find your job? I'm sorry, do you mind finding exactly what signal plus the signal minus? Yes, of course, sorry. So this is realization of s plus t. This is a realization of s plus t. Okay, so basically, if we managed to get this, we've got To get this, we've managed to get everything. So, now let me tell you how we're going to do it. It's basically just going to be a Chapman-called Mogrov equation, and it's just going to be a little bit harder to solve than most. Okay, so what we're going to do is we're going to say the Java-Colmograph equation in essence is really simple. It says the probability distribution at time t. probability distribution at time t, y and sigma plus is equal to the sum over the ways of getting from y prime and sigma plus prime to y and sigma plus times the probability distribution t minus delta t of y prime and sigma plus prime. Okay? So John Fitzo, yes is the causal state, so it contains all the information of the past. This has all the information about the future that we need. Yeah, this is all the information about the future. Sorry? Yeah, so this is this corresponds to, the notation is this minus corresponds to this is the only thing we can predict about the future. This notation corresponds to this is what we need to know about the past. This is what we need to know about the past. So, this is predictive features, and this is what we can predict. Yeah, sorry, it's getting a little hairy in here. I just want to sort of demonstrate the power of this, and then can somebody give me a time check, by the way? We have 20 minutes. 20 minutes, okay. Okay, so what we need to do now is break down this transition probability into things that we actually know. So, this is going to turn into So this is going to turn into p of y of p of y given y prime and x prime times p of sigma plus given sigma plus prime delta t. And I claim that if we know what the sensor model is and we know We know what the sensor model is and we know what the thing is that generated the process, the ligand concentration, we know both of these things. So we are then able to get partial differential equations that allow us to compute everything. I'm going to come back to this if people want me to, but I'm going to skip it. I'm just going to tell you it's a couple of partial differential equations. You can solve it exactly. They have some special structure. PD? Couple of PD? Couple of PDs and the picture. It's actually in. So it's a it's like there's a PDE along every conveyor belt in the Epsilon machine. Yeah, so I'll come back to that if you want, but for now, maybe just trust me, I'm able to actually get all these prediction metrics and these dissipation metrics. And all I need to know is I need to know this, which I can get from, you know, after my PhD dissertation, and this, which comes with the conditionally. And this which comes with the conditionally market sensor. Alright. So, what are causal states' key takeaways? I would say, please take away that causal states are what you need in order to predict. And there are also hidden states of machines that can generate output that you can't predict. I'm really hoping that you think after some of this that these causal states can be useful. I'll show you more examples of why they would be useful. Of why they would be useful. But basically, the idea is we're going to use the output of these epsilon machines to stimulate sensors. And because we know what the causal states in the epsilon machine look like, we can directly calculate all things having to do with memory and prediction using nothing more than a channel and commodity. That was the basic point of all that. Okay, for the rest of the talk, the input that we'll be using is less complex, it's just a semi-markup process. Just a semi-Markov process. So if you're in state A, you generate a linear concentration of XA with a dwell time drawn from this draw time. Note that it's not exponential, so this is not Markovian. And in state B, you have a linear concentration of XB drawn from a different draw time. And so what you'll see is something like this, back and forth, back and forth, back and forth. Okay, so I claimed the first thing in my title is faster calculations. Jim's artistic depictions. Okay, okay. The first thing is: you know, how would you do this if you didn't have all this machinery? Well, obviously, with iFuture, if you don't have this machinery, you actually can't compute what's going on. But let's say that you use just the fact that you need to keep track of these are first-time causal states. You know, let's say you're trying to calculate the memory. You know, how well can you do? You know, how well can you do with the usual simulation techniques? So, the simulation technique that you would use is a temporal Gillespie algorithm. It's like a Gillespie algorithm, except your rates time vary. And basically, if you have a two-state sensor with randomly drawn kinetic rates, here are two ways of depicting how close you get as a function of simulation time. So, this is in unison of the simulation time in like in Simulation time in the actual simulation of the environment. This is not compute time, this is related but slightly different. So this is how close you get to an estimation of the memory if you're just doing, if you just estimate the probabilities of X and Y given the frequency that you're in X and Y. And what you see is that star set tends to negative 1. Starts at tends to the negative one, goes down pretty quickly. This looks like it's a pretty decent estimate. However, if you look at the total variational distance, so this is a total variational distance between your estimated P of sensor state and input symbol versus the probability of the actual sensor state and input symbol. You see the total variational distance goes down a little bit slower, and this is for a two-state sensor. Two-state sensor. So if you're trying to bump up to, say, a thousand states, it's going to be really hard to do it using the total SBS. Sorry, are you using this method of expressing the comparisons of the S. Yeah, so the point of this slide was just to say you can try to do it using simulation, but it's faster if you just, you know, use all the techniques developed over the last 20 minutes and just nail it simulation time zero. Simulation time referring to pass? Simulation time referring to past or future? Simulation time is, so this, so imagine that what's going on is you have, you draw, you know, a ligand concentration environment from the model. And this is not exactly the answer to your question because I think I'm being confusing. You draw a ligand concentration from this environment, and then you use a temporal Gillespie algorithm to look at where. To look at what would a probable sensor be doing at the same time. And then based on, so what you would end up having is you would end up having something that would look like this, and at the same time, you'd have ligand concentration going like this. And at the same time, you'd have this going on for your y's. And temporal Puleski algorithm will give you some reasonable realization for the x and y's. And then you use both of these to compute an approximation for the x and y joint. And then you use that to. And then you use that to, and then we compare that to the ground truth. So, but what are x and y malface effect? Um, yeah, sorry. So, x is the ligand concentration, so the environment, and y is the sensor. So, this is open, closed, open, closed, open, closed. This is you have this much concentration, then this much concentration, then this much concentration. Than this much concentration. So basically, the deportable SP gives you an alternate way of getting estimations of these probability distributions that we're trying so hard to compute using all these coupled partial differential equations. The coupled partial differential equations turn into closed-to-form analytic solutions, so they're pretty easy to implement. But we want to compare it to something that we want to see how much of a benefit we get from actually having done that. You're right, this joint distribution is the same. You write this joint distribution. This isn't like the stationary joint distribution or the joint distribution of pads. No, this is because we're in a non-equilibrium steady state, this is going to be a s a stationary distribution. And this is for calculating... So X and Y in alphabet. They are not scalars. They're like No, no, no. This is this this was designed to calculate IMEM and IMM doesn't care about paths. IMEM just cares about what the actual value is at a particular point in time. Actual value was at a particular point in time. So, in other words, this really is, you know, what we're going to do is we are going to go through this entire example. We're going to look at how many, what is the frequency with which we see X at this liquid concentration and Y at that value. And we are going to look, and that frequency is going to translate into these probabilities. So, in this one example, So in this one example, would it be different from writing down a four-state Markov process with all stationary distribution of it directly? So what I've done with these coupled partial differential equations is an improvement over that because the environment is non-Markovian. That's the key. Because one of these processes is non-equilibrium steady state, it's a non-Markovian input. Markovian input. So you need all this machinery and actually to just do that computation. Okay, so this picture that you show now just describes how the input evolves. That's right. It's describes how the input evolves. The output, how is that coupled to this? So basically, you can use this model to generate pictures like this. And the way you do it is you say, if you're in state A, you draw entropy time from this double-time distribution. Okay, and this double-time distribution is non-exponential, so we have a non-Rark Covian. Exponential, so we have a non-Markovian system. This is all still for the input, and then you see XA for that amount of time. So, this would be the dwell time you see XA for, and then you see XB for a different dwell time drawn from this distribution, you see XA again drawn from this distribution, and so on and so forth. And the channel model for Y? And the channel model for Y, so for this example that I was just showing you, it is a conditionally Markovian channel. A conditionally Markovian channel with randomly chosen kinetic rates. So basically k of y raised to y prime has some dependence on x. And basically what I did is I just did the stupid simple thing of I'm going to look at xA and xb, and for each of those I'm going to randomly draw these kinetic rates uniformly from subdistribution. But they're fixed. They're fixed. They're f yeah, they're fixed, but they're dependent on X. So you can, in theory, get some information about the gain concentration. Sorry, that was so confusing. Okay, so with our method, we're able to scale up to just quickly, just immediately nailing in, you know, no time flat what the predictive, the memory and the predictive power of sensors are when there are a thousand states. So that's what you see over. There are a thousand states. So that's what you see over here. Over here is like 300, 100, and it kind of goes down from there. And what I did is I brought back these diagrams. So we've got memory on the x-axis, prediction on the y-axis. This right here is our predictive information curve. This is the allowed. This is the forbidden. And what we see here is that these are, so these are large random channels. I drew kinetic grays in the same way. I just said I'm going to make my sensor bigger. That I'm going to make my sensor bigger, I'm going to have more Y's, I'm going to draw more genetic rates randomly. And this line here I got from the theorem one that I put up on a previous slide. And what you can see here is that these are nowhere close to that line. So they're not good predictors. And as you increase the size of these sensors, something weird is going on. First of all, there's a concentration of measure. Second of all, even though they're getting more memory, they're getting... Second of all, even though they're getting more memory, they're getting worse prediction. So, basically, what this means is that they're getting better and better at memorizing something about the dwell time distribution, but they're not using it to predict well. They're not using that memory to predict well. You're falling farther and farther away from the curve. Sorry, what precisely is the size of the constant? Yeah, so this is, um, so instead of having a two-stage sensor, you could have it in a thousand-stage sensor? Oh, okay, we need that. Sir? Oh, okay. We need that. Yeah, sorry. Thank you. So, the reason what's actually going on underneath the scene is that basically all of the iFuture is taken up by the memory. So, all of iFuture essentially corresponds to being slightly larger than this memory information, the mutual information between the sensor state and the present symbol. And so, they're not actually using the information that they're capturing. Using the information that they're capturing about the time since the last symbol in order to predict the time to the next symbol. So that's unfortunate, but large random sensors are just not going to do it for us. There's some weird central limit theorem going on that's killing them. By the way, I have not been able to prove the concentration of measure, even though I have closed form expressions. I think that's an interesting problem. And then let's look at the Um, and then let's look at thermodynamics and prediction. So, for every single one of these number of states, we can look at that number of states. That's now our x-axis, the number of states. And this right here, our dissipation metrics. So, in green, we have the temperature normalized power we're extracting from the environment. In blue, we have the non-predictive information rate. So, the distance between these two lines is how tight the thermodynamics prediction graph is. And what we can see. Bounds. And what we can see is that this thermodynamics of prediction balance is not tight at all. It gets tighter and then it gets very tight. But basically, you go off towards what I would call death. And what we can see is that right around where they're best at predicting, we're sort of in this region over here. Not a very tight bound, but maybe tighter than most. But maybe tighter than most. But we'll see another example of this. Now we're going to search over all Hill molecules to conclude with a toy example that has been promised since slide two. We're going to look at Hill molecules with these ligand concentrations. And what we find is that again, I mem tracks I future very well, and that as n the Hill coefficient increases, they both increase together. So at a Hill coefficient, So, at a Hill coefficient of 10, you have the most predictive power on the spot. At the same time, at the Hill coefficient of 10, that's when the distance between non-predictive information rate and temperature normalized work dissipation is greatest. So, this is an example of when the bound is loosest when you have maximum predictive power. Maximum predictive power. So it's unclear if this found is going to be incredibly useful at analyzing the most predictive sensors. Now, if you wanted to try and build a sensor that both predicted and didn't dissipate too much heat, you would put together something like this. We're going to try and maximize alpha times I future minus the temperature normalized or extracted. What you find out is that as you vary alpha, sometimes you have a local and a global maximum. And what this can result in is hysteresis. So say that you start over here with some alpha, and you change your alpha, and the system adapts to become optimal and optimal and optimal. And you keep on changing your alpha, and you get to here, and you change your alpha, and the system adapts to be optimal locally. You have some memory of the past alpha, and when the alpha increases again, you get back up to the solution. Again, you get back up to the solution instead of being down here. So, hysteresis is something that you can see with these kinds of calculations. Okay, questions that we've answered. Can we develop a fast way to compute these architectural quantities? Yes, we need to use causal states. Can we make our sensors better just by making them larger? No. And is this sort of animating prediction bound height? It's not bad. I think in comparison to lowest bounds, it's pretty good. To most counts, it's pretty good, but it's not perfect. Sometimes, where the sensors are maximally predictive, it's not that type. And some questions I've gotten over the years, what good is all this? So the basic question that a lot of people have asked me is, if you need to know the sensor and the input model in order to calculate these quantities, then what good are they? And I have two answers to that question. The first answer is, you can use these quantities, you can use these closed-form expressions. Quantities, you can use these post-form expressions to get new plug-in estimators. And also, if you want to uncover design principles, then you may actually know the input of the sensor models, as I did here. And then what have we learned from playing with toy examples? The thermodynamics and prediction bound is very general, as I talked about earlier, but not super tight. And hysteresis and optionally predictive and efficient sensors is easier to find than you think. And I'd like to close with this. And I'd like to close with this figure, which is basically just: I got around a cursive dimensionality. I didn't have to go from here to here to here. I went directly from here to here. And that's super powerful. And I hope if you have questions, you'll talk to me about maybe using causal states in your work to do something. And I'd like to thank Jim Crutchfield, Stand Still, Tony Bell, who gave me the inspiration for the entire thing, and the two sources of money that I had in. For disappointing that I had in register and postmark. Thank you. Probably time for questions. We had a lot of questions, but if there could be more, yes. So I really like the idea of but in a sense there's a little bit, right? So you kind of need the ray of formulating hole or like you could put together a diagram, but it's most likely I don't know if you're familiar with these sort of more larger scale learning algorithms. Older versions like restricted bulks. So these are things so you have many data layers that you you try to model a system and sort of put it higher connections between them. So I haven't talked about how those things could Yeah, let me yeah, let me answer this. So basically, the way I read that question is: what are oh, did I turn that off? Yeah, what do you hit the button so we don't forget? Yeah.