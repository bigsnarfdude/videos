Okay, so this is going to be more of a tutorial than a talk. We have seen some tutorials that were more like talks, so this is going to be the opposite. So it's about quantum conditional entropies and some new developments there. So it's Um so it's based on the archive paper. So if you want any proofs or even like the correct statements in case I make mistakes, that will be in here and it's joint work with Roberto who is is here and Milard who is postdoc in my group. So okay, so what are entropy Entropies. I guess you all know what entropies are. Conditional entropy, the one you probably know, hopefully, is this one. So the for normal conditional entropy, which one can define conveniently in terms of the relative entropy. So this is the identity operator, and we can just write. Identity operator, and we can just write it in terms of the relative entropy actually in two different ways. I will get back to that. So we can write it like this, but then we can also realize that if we allow this second, this marginal here to be arbitrary and maximize over this, well, then the maximum is going to be taken by the marginal of rho b. So we can also write this as this maximum. Okay, so that's the condition entropy. And just in case somebody doesn't remember this, the relative entropy defined like this minus log sigma. Okay, so the goal kind of of this research agenda that I'm interested. Agenda that I'm interested in at the moment is to get a more complete picture of what kind of information measures should we even consider. Somehow, try to get an axiomatic view on information measures in the quantum setting, which turns out to be quite a complex field with many open questions. But at least we're making a little bit of progress. So the first thing to think about. The first thing to think about is what kind of properties do we even expect from a conditional entropy? Right, so maybe make this a little bit interactive. What properties does this have? Do you remember any of the properties of the conditional entropy? It's less than the entropy of A. Yeah, so this is also true. Yeah, so this is also true quantumly. In fact, there is a stronger statement even that is called strong subadditivity. So it's a kind of a conditional version of your statement. So A given B C is smaller than H of A given B. A given B. Okay, so that's what one property. Any others? There's some simple ones. Non-negative. Okay. And what was the other one? What was the other one? Yeah, okay, so yes. The chain rule, yes. Um, right, a given Given AB given C, it's the same as A given B C plus H of B given C. We're missing some very fundamental properties. Data processing? Yes. So data processing. So, data processing, DPI, I call it data processing inequality. So, what does it tell us? It tells us two things actually for conditional entropies. The first is if we do any quantum channel on the side information, that can only increase our entropy. So, this has to do with kind of the fundamental thing that we want to describe with the conditional entropy, right? We want to describe uncertainty. Want to describe uncertainty about the system A from the perspective of an observer that has access to B. And so, if the observer does any channel on this observable B, that cannot increase the knowledge about A. So it can only increase the uncertainty. So let's say we have row prime AB as some channel on B. on B and some channel on A. Let me, or maybe call this A prime B prime. And we have the channel from A to A prime and a channel from B to B prime. Then we need to put some conditions. So these are supposed to be quantum channels, but the one on A is a bit more tricky. On A is a bit more tricky, but I'll get to that. But if we define this, then we have that A given B of rho is smaller or equal to H of A prime given B prime. Actually, I don't need to. So entropy is increased. So as I kind of argued that this channel on B can be arbitrary, this one needs to be This one needs to be subunital. So, this is actually kind of a little bit different thing because what we can also say is that if we apply some mixing channel, some pure noise channel, in a sense, on the A system, then that will also can only increase the entropy. So, we're kind of adding uncertainty about A with using such a channel. Using such a channel. Okay, so that's the data processing. Then we have almost everything. Exactly, yes. So for Norman entropy, these two are actually equivalent. I mean, or at least follow from the other ones. So one thing you can do to see that DPI implies strong subjectivity is just by tracing out the C system here. C system here, then you immediately get this strong subjectivity. The strong subjectivity itself has many different forms. This one is not really what we usually call strong subjectivity. Usually, strong subjectivity would be an equivalent version where we say that A given C plus B given C is larger than H of A B. Of A B given C okay, we're still missing some easy ones. What if you tensor dates? It's also very useful. So it's additive on the tensor product. Right. Let me see if I can still fit this. So let's say that we have A A prime B B prime If the the state buttons. A B A prime is a product state. Okay, and then there's one left that I think is quite important. And that's interesting for cryptography mainly. There are many people working on cryptography. Yeah, what is if you don't want to say something about the entropy between A and B, but you actually, or A and E, let's say the eavesdropper, but you don't have actually have access to the eavesdropper, you don't know what kind of quantum system eavesdropper holds. What can you do? Any ideas? So let's say you want to know the entropy of A, give me a majority of the A Say you want to know the entropy of A given E, right? But you don't know what the E system is. What can you do? What do you usually do? Purify and then, so let me try to fit this in. So there's this duality relation. So the duality relation says that H of A given B plus H of A H of A given C is zero if rho ABC is pure. Okay, this one is very quantum. And it's very useful because it kind of allows you to make statements about entropy that an eavesdropper associated my eavesdropper has by just looking at properties of the honest part. Properties of the honest parties. You can build security proofs for QKD, for example, from this kind of relation. Okay, so we have many properties. This one, the duality. It actually, it's very simple if you just write it down. So it follows from the fact that for pure states, For pure states, for pure states, the entry of the marginals is the same. You can see that from the Schmidt decomposition, for example, of pure state. So if you just use this fact, then you get immediately the duality relation. Good, okay. So many desirable properties. Now I want to focus on two that I think are absolutely necessary for any entropy to have. So the first one is this DPI because this really boils down to kind of the operational interpretation of. Of the operational interpretation of entropy that we want to have, right, as a measure of uncertainty. If it doesn't satisfy this, then it's not going to be a useful measure of uncertainty because suddenly an operation on the eavesdropper system, for example, could increase, could decrease the entropy. So it's not really measuring the right thing then. So this would be one we really need. The other one is more of a kind of convenience. Convenience, maybe. So, for product states, we really want this to be additive. So, if we have two copies of a system, we want the entropy to be twice as much. Otherwise, we kind of get in trouble when we try to do any protocol. We need to have a way to kind of this entropy to build up. Okay, so good. So then we can ask what are the measures that kind of satisfy these two properties and maybe some of the other desirable properties as well. But what they need to satisfy are these two. So one way to So, one way to get there is to start to define them in kind of in this spirit from divergences. Because actually, this data processing inequality, if you look at the definition here, it follows from the data processing inequality of the divergence. So the data processing inequality of the divergence just says that if you apply any channel here, we should remember that from the previous talk, if you apply any channel, then this divergence. Then this divergence is non-increasing. And if you just use that for the type of channels that I described, then you see that data processing for the conditional entropy holds here because the way we define it. But now there are many different divergences that we could start with. So the type of divergences we're going to look at Is the most general class we know at the moment? Use the so-called alpha C divergences. And those are quantum generalizations of Reni divergences. And they look like this. This is alpha over 2C, and this one, 1 minus alpha. 2c and this one 1 minus alpha over c and to the power c the whole thing and then take the trace so this is what i call a reni type divergence now for these objects we know exactly for what parameters they satisfy data processing and this is kind of a A famous figure, I'm just gonna famous only in a small community. But okay, so if this is C and alpha, then it kind of so this would be C equals zero and one, you get something like this. So here, this is the whole. So, here this is the whole area. Then, we have here we can go with alpha to infinity, the same slope. And here we have some. It should be different slopes, sorry. More or less parallel here. So, this is where data processing is satisfied. Now, this is a Satisfied. Now, this is a particular parameterization. So, this region looks quite a bit strange, but there are some important classes that people have studied extensively. So, this line here, this is the so-called sandwiched Reni divergence. So, here we have alpha equals D. And the other famous one is maybe that the PET one, this would be this line. One, this would be this line, so this is heads. Why are they what are the two axes? Oh, alpha and C, the two parameters of the function. So, this is the region where data processing holds for this. For this function, and only there, so we know exactly this region. So, this has been, and there are many works that kind of establish this, but the latest one is by Zong in QIP, maybe three years ago, where this was finally kind of completely solved. So, we know data processing. So, then we also know that whenever. So then we also know that whenever we define a conditional entropy from this, then it at least satisfies the data processing inequality. So we can do this so that the most often used, and maybe some of you have seen those definitions. So those definitions now are just essentially based on what we did. Based on what we did there. So there are two versions, one of them by just plugging in the marginal. And then again, one where we optimize so that the arrows just indicate that it's larger than the other one. And those now, these optimizations don't necessarily give us the marginal state. So these are now generally different. And by the way, one can even do this classically, right? Even classically, these two definitions would be different. And classically, this would be generalizations of conditional Reni entropy. And you would get two different generalizations that have been used in the literature. That have been used in the literature. Good. So, as I said, if we use this definition, we get DPI for free from the divergence. What about other properties? Well, that's a little bit more complicated. So, additivity for this quantity is clear, right? Because the divergence itself is also additive. So, if you just put intensive products, you Just put intensive products, you get additivity. For the one where we optimize, it's a bit more complicated because I mean, kind of we get one direction for free, but then the other direction, we need to say something about this optimizer. So that's a little bit more tricky, but one can show that these are actually additives. Okay, so even in kind of what kind of entropies do you know? You know, for example, the min entropy, maybe? Yeah, so for different problems, we need different entropies. So, for example, if you want to describe how much key you can extract in a QKD setup. QKD setup, you would want to bound on the mean entropy, conditional mean entropy. So the conditional mean entropy is essentially taking the limit here with alpha going to infinity. So that's a special case of this class. And then you're essentially, so with the alpha, you're essentially interpolating between different types of entropy in some sense. I don't know if that helps you, but you can think of this describing the whole. This describing the whole spectrum of the log likelihood ratio. So, classically, the entropy would just give you the conditional entropy, would just give you the expectation of the log likelihood ratio. So, log p divided by minus log q. And this kind of gives you the full spectrum. In fact, it's a cumulant generating function of the log likelihood ratio or related to it. So, this gives. So, this gives you more information about the distribution, not just the expectation. So, that's what we need the alpha for, and that has many operational interpretations also, for example, in terms of error exponents and things like this in information theory. Now, the C is a different story. The C is kind of a completely non-commutative part here. So, if you go to the classical special case, then the C drops out. But in the con case, we just have different ways of kind of arranging these terms. And C is one of the ways to do this. Now, we can write down many different generalizations of the classical quantity, but the question is then, does it satisfy data processing for quantum channels? And that is highly non-trivial. But for these kind of forms, But for these kinds of forms, we can show data processing. So the C doesn't have any meaning per se, except that we know at least we need two values of C. So this alpha equals C and C equals one case, because both of these quantum divergences have clear operational meaning, at least in hypothesis testing. So we need them somehow, but we don't really understand why it's those two cases that. Understand why it's those two cases that seem to be emerging as the relevant quantities in quantum information. So essentially, I'm just exploring here what makes mathematical sense and hoping to find out also what makes why some of them appear in operational problems and some of them don't. But in order to do this, I want to understand all of what. To do this, I want to understand all of well, not all of them, and we're not there yet, but as many, as big of a class as possible, and then try to nail down the ones that are actually that we actually need to consider. Okay. Good. Okay, so we can do this. And as I said, we have additive DVD and DPI. So, this is the DPI region, DPI DPI. And this, yeah, I mean, I didn't, this is like Stockton one half. We have different classes that have been explored. This is called the reverse sandwich, and we have sandwiched. So, it's quite a complex picture already. So, I'm just gonna, if I get to I'm just gonna. If I get to there, I will make it even more complex. Yes, yes, it does. So these, if we define it like this, then it satisfies additive. Good. Okay, so the next thing I want to, I'm interested in is duality relations. For example, because For example, because there is one special case of the duality ration that I used for security proofs, which is if one of them is a min entropy and one is a max entropy. So that's a special case of this duality ration that directly applies to the min entropy that we use in cryptography. But there are many more general relations known. So maybe I just give one example. So if we look at the pets, Example. So if we look at the pets one, so C equals one. And we have, for example, this. So we have this alpha and alpha hat, and they need to satisfy alpha plus alpha hat equals two. Okay, so this gives a duality relation. okay so this gives a a duality relation for for this this more general class of of entropies but but it changes you need to change the parameter so this is kind of clear that you need to change the parameter because um all right we know that this holds with equality for for neumann entropy so if you make one of them smaller you need to make the other one larger and then they're they're ordered in this this parameter so so for example you see if if one of these alphas is You see if one of these alphas is smaller than one, then the other one has to be larger than one. Alpha equals one is the case of von Neumann entropy. So if you take the limit alpha to one, you get the von Neumann disentropy. Sorry, this is also the down. But there are other ones for up arrow. And yeah, there are even some that, so maybe I just mentioned this one. There is one that mixes these pets and sandwiched. So here we would have alpha hat, alpha hat. So this is a sandwiched one. But then we have here alpha. But then we have here alpha hat times alpha equals one. So there are various of these these relations that that kind of connect different quantities and in fact one can use this to define a dual quantity right it's just the quantity that adds up to zero with it with the yes yes yes yes um Yes, yes, yes. Yes. I guess, so I mean, in general, yes. But I mean, maybe you can find some states where this doesn't depend on alpha, and then it would vary it. But yeah, for general. Could vary it, but yeah, for general states, yeah, it needs to be exactly this. So, you can actually use this to define a dual quantity by just looking at just moving one to the other side and then defining that as the dual. And the interesting thing is that these dual quantities, because of essentially via Steinspring dilation, one can show that we define the dual quantity, it also needs to satisfy data processing. So, this follows from kind of So, this follows from kind of this definition as a dual. So, that brought us to the motivating question here. The motivating question is, is this set of conditional attributes that we can define this way, is that closed under this duality? So, if we define the dual, do we always get another alpha C type conditional? Type conditional entropy. And so, what we realized is that we don't. So, it's not closed. So, which means that this certainly is not the most complete thing we can think of. There are more entropies that also satisfy data processing and additivity, but are not of this form. So, they're not kind of optimizations. Of optimizations, well, maximizations or just a marginal taking from a relative entropy. So that was kind of the motivating starting point. So I don't know how much time I have. Four minutes. Okay. So I can very quickly throw the definition at you. So the definition. So, the definition that we found looks like this. So, this will discover this opt means. So, we can mix these two things. So, you see, this is the same as in the alpha c, but then on the second argument, we kind of The second argument, we kind of take a little bit of the marginal and a little bit of a state we optimize over. Okay, and this is going to be have the same exponent as this, and then we have again the rho AB alpha to C. This whole thing. This whole thing to power C. Okay, this optimization depends on the range of parameters. It's either an infrimum or supremum over sigma b. And so we found that, and again, I have to refer you to the paper for the details, but it satisfies DPI for some region. The region is it's given in alpha c by this region here, but then this this lambda this additional parameter can also vary. Okay, and so the region is quite complex. I'm not gonna won't really help you if you're interested, look at the paper. Interested, look at the paper. It's closed under duality now. So that we get. So we have this closure. And obviously, it contains all of the ones that we've looked at before. But what we now get is we get new chain rules. So these chain rules, there are also some chain rules known, especially for the Chain rules known, especially for the sandwiched conditional entropy, and they turn out to be very useful. So now we get some generalized chain rules. Those are of a different form than the one for Norman entropy because we can't expect to have an equality anymore. Okay, but we get various inequalities. And again, And again, there are lots of conditions on the parameters, but they are of this form. Okay, then we have a third set of parameters being given C. And so the direction might also change depending on the parameters. It's a whole collection of possible. Of possible ways one can write this down. And those are very useful when we want to add up entropy. Let's say we know this conditional entropy, and then we have many system, and we always want to kind of add this to the rest to accumulate entropy. So then what happens when we use for Neumann entropy, it works very nicely. For Norman entropy, it works very nicely. But if you use any other entropy, we always lose something in the parameters by doing this. And this is kind of fundamental. But now we have many more chain rules, so we're still kind of exploring or hoping that somebody else will find an application of them. So if you think they can be useful, please look at the paper. But at least we have much better understanding now of these. Better understanding now of these chain rules. Particularly, we have some for the PETS conditional entropy, which we didn't have before. And those should be useful. Good. Yeah, there are many more things. We found a nicer coordinate system where this DPI and duality actually look a little bit nicer. Because, yeah, you can kind of. Yeah, you can kind of change coordinates. What else? Yeah, change rules. Yeah, you will have to look at the paper, I think, to get more detail, but I'm happy to answer any questions on.