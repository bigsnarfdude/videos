All right, cool. Thanks. Yeah, so I'll talk a little fast for this. Thanks, everyone, for coming, and thanks especially to the organizers for inviting me to give this talk. Part of this talk is based on joint work with Nathan Williams. I'll indicate which parts are joint with him. And also, I cannot see the chat. So if you want to just interrupt any time with questions, feel free to do that. Okay. So broadly, my goal here is to try to kind of pitch this area that I'll call non-invertible combinatory. That I'll call non-invertible combinatorial dynamics. And when I think of combinatorial dynamics, what I have in my head is we have some set x of combinatorial objects, and we've got some function f from x to itself, which is defined in some kind of combinatorial way. And the goal is to understand the dynamics of f. So one way we can visualize this is to draw what I'll call the diagram of f. This is just a directed graph where the vertices are the elements of x. And for each element, we just draw an arrow. For each element, we just draw an arrow from it to its image under this map F. So each connected component in this graph is going to have a periodic cycle, and then you might have these trees that map into the periodic cycle. And typically in dynamical algebraic combinatorics, people are interested in invertible operators. So in these cases, you just have only periodic cycles. And sometimes this is because maybe you just have an operator that's naturally invertible, like row motion acting on the order ideals of a finite poset. In other cases, Poset. In other cases, maybe you have something that's not immediately invertible, but maybe you say, Okay, well, we only care about the invertible part, we only care about the periodic cycles, so we'll just get rid of these trees by chopping them away. So we'll just chop away the trees and ignore them so that we can focus on the periodic cycles. But I'm here to speak for the trees, for the trees have no tongues, and I think they have a lot of interesting things to say, so they should not be chopped away. I did copy this slide from a previous talk, so if you saw me give that. A previous talk. So, if you saw me give that previous talk, I'm sorry for repeating the joke. But yeah, I'm going to be focusing on these trees within these diagrams. And actually, a lot of the operators that I'm going to be talking about will have diagrams that look like this, where just this one periodic cycle, which is just a fixed point. And then everything eventually maps to that unique fixed point. And before I go any further, let me just define the forward orbit of an element. The forward orbit of an element little x in our set x under the map f to be the set O sub f of x. And this will just be the set x, f of x, f squared of x, dot, dot, dot. Usually this is going to be a finite set, even though I wrote the dot dot dot, just because the iterates are going to start repeating themselves. Okay, so let's say you have some interesting non-invertible combinatorial dynamical system in your hands. Okay, so you've got some map F. Okay, so you've got some map F. So, what kinds of questions can you ask about this? Well, there are a lot of questions that are just boring for invertible maps, but they become interesting for these non-invertible maps. For example, you could ask, what are the periodic points? Actually, this is a question that is essentially what Brian Hopkins was asking in his open problem about this Austrian solitaire map. And so, for many maps, this is going to be, but in other cases, maybe this is a really interesting. Cases, maybe this is a really interesting question. You could ask, what is the image of your map, or how big is the image? Maybe this is like an interesting enumerative question, or if you're trying to describe the image in some kind of way. You could ask, how can we maybe compute the number of pre-images of an arbitrary element under this map? And maybe this is a really ambitious goal for some maps, but in other cases, maybe you can do this. And if you can do this, this could give. This and if you can do this, this could give a lot of interesting information about your map F. And maybe this could be too hard, but in some cases, you could maybe ask an easier question, like what is the maximum number of pre-images that an element could actually have? And one other question that I kind of like is just how many elements have exactly one pre-image? The reason I like this is because I've done work on something called the stack sorting map, which is not really. Sorting map, which is not really the same as what I'm going to be talking about here, even though they have very similar names, they're kind of different operators. And in that case, I found that this question has what I think is a pretty interesting answer. You could ask, what is the maximum possible size of one of these forward orbits? Or maybe even more generally, just what are the forward orbit sizes? But a natural question, I think, is just what's the maximum forward orbit size? And if you can understand the maximum forward orbit size, Can understand the maximum forward orbit size. You could then ask, well, what are the elements that actually attain the maximum? And then one last question I have here is, how big is this forward orbit on average? Okay, so obviously this is not a comprehensive list. Maybe if you have some specific map in mind, then there could be other interesting questions to ask, but this is just sort of a list of questions that I think is maybe a good guideline for things that you could ask about your favorite non-invertible combinatorial. Your favorite non-invertible combinatorial dynamical system. Okay, so I'm going to be focusing on the pop stack sorting map as well as variance of it. So what is this thing? So I'm going to let Sn be the set of permutations of the numbers from one to n. And this pop stack sorting map is this operator, which I'll denote by pop. It goes from s n to s n, and it acts by just reversing the descending runs of the permutation. So here's Mutation. So here's an example of this. So a descending run, by the way, is just a maximal consecutive decreasing subsequence. And in this example, I have the permutation 872496153. I've separated the descending runs with these bars. The bars are not part of the permutation, but they're just separating the descending runs. And all POP is doing is just reversing each descending run. So 872 becomes 278. 4 stays as 4. 961. Is four nine six one becomes one six nine and five three becomes three five so it's pretty easy to describe what this is doing but the dynamical properties are in some cases uh or depending on what you ask it can be very hard so uh one natural thing you can ask is what happens if you iterate it just over and over what's going to happen in the long term and it's not hard to see that you'll eventually get to the identity permutation this is because every time you reverse descending run Every time you reverse descending runs, you're decreasing the number of inversions in the permutation. So, if you keep decreasing the number of inversions, eventually you'll get down to the identity. And the identity permutation is fixed by this map. So, then you can ask, okay, well, what's the maximum number of iterations that we actually need to use to get to the identity? And it follows from this inversions argument that it's at most in choose two because a permutation of size n has at most int choose two inversions, but actually you can. Two inversions, but actually, you can do much better. There's this theorem by Peter Unger from 1982. He showed that the maximum possible size of a forward orbit is n. So this is equivalent to saying that the maximum number of iterations you need is n minus one, just because the forward orbit is going to be w, pop of w, pop squared of w, all the way down to the identity, and then it's fixed. So it's going to have size one more than the number of iterations you need. And I mentioned first of all, this theorem is not trivial. This actually kind of requires an interesting, clever idea on Unger's part. Also, Unger was not coming from the point of view of combinatorial dynamics. He was really coming from the point of view of discrete geometry. So he had this question about the directions determined by a collection of points in the plane. And he managed to reduce this geometric question to something about what we're now calling the pop stack sorting map. So this is actually. So, this is actually the first paper where this POP stack sorting map appeared, but now it's appeared in other papers, often from an enumerative point of view or a dynamical point of view. Okay, here's a picture of the diagram for the POPSTAC sorting map on S6. So there are 720 of these vertices representing the different permutations in S6. And you can see they all eventually get mapped into this blob, and the identity permutation is summary. And the identity permutation is somewhere in that blob. So basically, this tree, and so the goal again is to try to understand the structure of this tree. And I have this quote from the lower X. Okay. So here are some open questions about this pop stack sorting map. There's a lot we don't know about it. For example, I said that the maximum possible size of a Ford orbit is n. This is Unger's theorem. But we don't know how many. Theorem, but we don't know how many permutations actually attain the maximum. So, how many permutations actually require n minus one iterations to get to the identity? We don't know. And then another conjecture that I have is that if we let dn be the average forward orbit size, so here I'm just averaging over the whole symmetric group, then the conjecture is that the limit as n goes to infinity of dn over n is one. dn over n is one. So it follows from Unger's theorem that this limit is at most one, just because the maximum forward orbit size is n. So the average forward orbit size is at most n. So this ratio is always at most one. But the conjecture is that in the limit, it actually tends to one. And I'll just say it's not too hard to prove that the liminth here is at least a half. The reason I'm writing lim inf is because we don't actually know that the limit exists. So part of That the limit exists. So, part of the conjecture is that the limit actually exists and equals one. And I don't know how to improve the one-half. I've thought about this a little bit, but I think really, even if you could tell me how to improve this one-half to like 0.51, I think I'd be interested to see that because I don't know how to improve this. I have a question. Yes. Is there a place we could look up terms of that sequence that is sort of hiding inside the question? What is the sequence? What is the sequence exactly? The function of n who's available. Oh, this. You mean the first question, right? Yeah, first question. I think actually maybe at one point I computed it and did not see it in the OEIS. I don't think I added it, though. But I think it's not hard to compute. So, yeah, it can be computed for small values, certainly. Value certainly. I'd like to see it at some point if you can email it to me or share it with you. Yeah, I can do that. Okay. So is that good? Yeah. All right. All right. So I want to generalize this PopStack sorting map to the realm of Coxeter groups. But before I do that, I just want to give a brief crash course on Coxeter groups just to make sure we're all on the same page and just so that I can fix my notation. So I'm going to let Notation. So I'm going to let S be a finite set. This is going to be my set of simple generators for the Coxer group, also called the simple reflections in the Coxer group. W is going to be the Coxer group. It's this group generated by S satisfying certain relations. Here E is the identity element of the group. The really important relations here are the ones where S and S prime are the same. And this is saying M of S is one, which is the same as saying that each simple generator is an involution in. Simple generator is an involution in this group. So think of it as some group generated by involutions with these relations like this. So W is the Coxeter group. I'm going to always assume in this talk that it's finite and irreducible. You can extend some of the things that I'm going to say to infinite Coxeter groups, but for simplicity, I'll just focus on finite ones. And the irreducible assumption, this is saying that the group is not a direct product of smaller Coxeter groups. That's, we're not really. Groups. That's we're not really losing anything by making this assumption because if you can understand the irreducible Coxeter groups, then you can understand the reducible ones. So the prototypical example of a Coxeter group is the symmetric group Sn. It has these simple generators S1 through Sn minus 1, where SI is the transposition that swaps I and I plus 1. If you have an element little w in your Coxver group, you can write it as a word in the simple generator. In the simple generators. This is just because the simple generators generate the group. If you look at all of the words representing your element W, then the ones that have the minimum possible length are called the reduced words for W. And the length of one of these reduced words is called the length of W, and it's denoted by L of W. Now, a Coxeter element is an element C obtained by conjugating something that you get. Obtain by conjugating something that you get by multiplying all of the simple generators together in some order. So, what I mean is you write down your simple generators in some order, you multiply them together in that order. What you get is called a standard Coxeter element. And then a Coxeter element is just something that is conjugate to a standard Coxeter element. And I'll denote Coxeter elements by little C. It's known that all Coxeter elements have the same order, so we can define the Coxeter number. Define the Coxeter number of W, which I'll denote by H. This is defined to be the order of this Coxeter element. Okay. Now, the right weak order on a Coxeter group is a natural partial order, and it's defined by saying that an element V is less than or equal to an element W if there is some reduced word for W that has a reduced word for V. Reduced word for V as a prefix. And the right weak order is a lattice, so this just means it's a poset such that for any two elements, they have a unique greatest lower bound, which is called their meet, and a unique least upper bound, which is called their join. And a right descent of an element, little w in our Coxer group, is a simple generator S, such that if I multiply W. That if I multiply w by s on the right, the length goes down. I'll let dr of w be the set of right descents of the element w. And then finally, if I have a set J of simple generators, I'll let w naught of j be the longest element of the parabolic subgroup generated by j. So j is going to generate some subgroup that's called a parabolic subgroup. It will have a Parabolic subgroup, it will have a unique longest element, and that unique longest element I'll denote by W naught of J. Okay? Okay, so now I can define pop stack sorting for Coxster groups. So if I have my Coxer group W, I'll define this Coxer popstack sorting operator pop sub w from w to itself. w from w to itself and it's defined by saying that pop sub w of x is the meet in the right weak order of the elements covered by x in the right weak order. So this here, this is the set of things that are covered by x in the right weak order. And the wedge symbol here is just the meat of all of them. So this is the greatest lower bound of all of those things covered by x. I'm also taking the meat of this with x itself. With x itself, which is not going to do anything for most choices of x because this meet is already less than or equal to x. The only reason I'm doing this is because I want pop sub w to fix the identity element. I want pop sub w of the identity to be the identity. And the identity element doesn't cover anything. So if x were the identity, this would be the empty set. The meat of the empty set, I guess, by convention is the top element of the lattice. And that's not what I want. I want pops of w of the identity to be the identity. Pops of w of the identity to be the identity, so that's why I add this little x meet here. But again, for most things, just ignore this part. Okay, it follows from some basic Coxer theory that there's an equivalent way of formulating this. So you can write Pops of W of X as X times W naught of D R of X inverse. So remember, D R of X is the right descent set of X. W naught of D R of X is the longest element of the. Is the longest element of the parabolic subgroup generated by dr of x. And then I'm taking the inverse of this and multiplying by x. I don't actually need the inverse here because this element is an involution. But I like to have the inverse there anyway, and there will be a reason for this later in the talk. But anyway, this is an equivalent way of formulating this definition. And it turns out that in the case when W is the And it turns out that in the case when W is the symmetric group, this is really just this pop stack sorting map that I was telling you about before. So, this thing that's reversing the descending runs of permutations, that's exactly this constant or popstack sorting operator in the case of the symmetric group. So, I think this is sort of a natural way of extending the definition of the popstack sorting map to arbitrary Coxford groups. And so, I have this theorem which generalizes Unger's theorem. Which generalizes Unger's theorem. Remember, Unger said that in the case of the symmetric group, the maximum possible size of a forward orbit is n. And n is the Coxeter number of the symmetric group Sn. But more generally, if we have a finite irreducible Coxeter group W with Coxeter number H, then it turns out that the maximum possible forward orbit size is exactly H. And I'll just say I'll just say that the proof of this is type uniform, so it doesn't rely on any specific combinatorial model of Coxer groups in type B or D or anything like that. For the proof, I actually don't handle type A. Type A is a little weird, but we already know type A is true because it's Unger's theorem. So I actually don't need to prove it in that case. But it's basically a type uniform proof. I think it's kind of interesting because it. I think it's kind of interesting because it combines the right and left weak orders in a subtle way with the strong Bruha order and these maximal elements of parabolic quotients. But I'm not really going to say anything too much about the proof other than just it's type uniform. So yeah. Okay, so here is a picture of the Coxster popstack sorting operator on H3. So H3 is one of the exceptional Coxter groups, it has 120 elements. It has 120 elements represented by these vertices. I didn't draw the arrows here, but you can imagine that all the arrows are directed upward. So each vertex just gets mapped to the thing right above it in this tree. The identity elements here at the top and it gets fixed by this map. So we have a little loop. And I'll just draw your attention to the fact that the maximum forward orbit size here is 10. For example, if I take this element down here, its forward orbit has size 1, 2, 3, 4. 2, 3, 4, 5, 6, 7, 8, 9, 10. And 10 is the Coxeter number of H3. So this is agreeing with the theorem that I said before. So this is kind of just a side remark, but it's mainly meant to say that this is kind of related to something else that people have studied. So there's this thing called Brieskorn normal form, which Brieskorn. Normal form, which Brieskorn introduced. Also, there's a paper by Brieskorn and Saito, which talks about this. And they were phrasing this in more generality in their discussion of Artin groups, but you can also think of it just for Coxergroups. Basically, this is some kind of normal form that represents your element of your Coxer group as a product of longest elements of parabolic subgroups in some kind of greedy way. So Brieskorn calls these longest elements of parabolic subgroups the fundamental elements. Groups, the fundamental elements, but really they're just longest elements of parabolic subgroups. And it turns out that constructing this brief score normal form is really equivalent to just applying this pop stack sorting operator iteratively. And so, for example, it follows from the theorem that I said a few slides ago that the maximum number of fundamental elements appearing in the breezecore normal form of an element of your Cox for group is h minus one. So there's a minus one that shows up, but that's. That's sort of an equivalent way of saying the theorem that I had a couple slides ago. Hearing noises. Okay. So, yeah, so this is just sort of a side remark. Okay, here is an open question. So, suppose I take J to be a non- I take J to be a non-empty proper subset of the set of simple generators, then I'll let JW naught be this maximal element of this parabolic quotient JW. So one way you can think about this in the language that I've already introduced is that this is the set of things that do not lie above anything in J in the right-weak order. This is the set of elements X such that X is not greater than or equal to anything in J. X is not greater than or equal to anything in J in the right weak order. It's called a parabolic quotient. And it has this maximal element, JW naught. And the conjecture is that this element will have a forward orbit of size H. I'm not conjecturing that these are all of the elements of maximum forward orbit size, but rather just that these are some of them. And Unger proved this in the case when W is the symmetric group. And I proved this in the case for arbitrary Coxer groups when For arbitrary coxster groups, when J is the entire set of simple generators minus one element, but it's open more generally. Okay, so now you might notice, especially if you saw my open problem presentation a couple days ago, that you don't really need the right weak order of a Coxer group here. This definition that I was giving kind of makes sense really for any lattice or really any meat semi. Lattice or really any meat semi-lattice. So this is just a postet where you can take meats. So you can look at the greatest lower bound of some set of elements. I guess I kind of should fuss about, you know, maybe not any meat semi-lattice because there could be pathological examples, like uncountable things, but like for the cases that I like, you know, this makes sense. And so we just define pop sub L to be this operator from our meet semi-lattice to itself, defined by saying that pop sub L of X is the meat of the That pops a bell of x is the meat of the things covered by x. It's exactly the same as before. Again, I'm taking the meat of this with x, which doesn't make any difference for most elements. The only reason for doing this is to make sure that POP is going to fix the bottom element of the meat semi-lice. And so, in particular, if you take the right weak order on a Coxeter group, then this agrees with the Coxeter pop stack sorting operator that I was telling you about before. Was telling you about before. Okay, so I have this paper, and in that paper, I study this POP stack sorting operator on what are called new Tamari lattices. These are some interesting lattices that are these vast generalizations of Tamari lattices. They were introduced by Previer-Ratel and Vienna. And they have a lot of interesting combinatorial properties in relations to geometry. And basically, I was trying to understand combinatorial properties of this top stack. Understand the combinatorial properties of this Popstack sorting operator and its dynamics on these lattices. I'm not really going to say what I did here, but I will just say that I think there are a lot of other interesting families of lattices. So if you have your own favorite family of lattices, maybe this pop operator will have some interesting dynamical properties. Maybe something interesting can come out of that. Also, this paper discusses some connections with lattice congruences. Congruences. There are some applications of this. For example, if you like Cambrian lattices, I can show that if you have a Cambrian lattice L, which is obtained by taking the quotient of the weak order on a Coxeter group W by some Cambrian congruence theta, then the maximum forward orbit size here is at most the Coxeter number of the Coxeter group. I don't think this is always going to be an equality. Think this is always going to be an equality, but it is an upper bound, and this follows basically from some of the things that I've written down in some of these papers. But yeah, so if you're not familiar with Cambrian lattices, that's fine. I'm not really going to be talking about them in the rest of the talk, but just for people who are interested, I thought I'd mention this. Okay, so what next? I can't see the top. Okay, this is always a problem for me. I can't see the top of my screen. I can't see the top of my screen because I don't know, there's this bar in the way. So I always kind of have to guess and like try to remember what I put there for like my segue into the next slide. Anyway, okay, so yeah, right, that's what this is. This is a picture of POP operating on a new Tamari lattice. I haven't defined new Tamari lattices, but they're just, you know, they're these lattices. The elements themselves are lattice paths. So it's like two uses of the word lattice. They're lattices of. The word lattice, they're lattices of lattice paths. These orange arrows are just representing the operation pop. So it looks something like this. Okay. So now I want to tell you a little about the connection with row motion. This is going to be basically a repetition of what I told you on Monday in the open problem session, but I'm not going to assume people were there. So if you were there, this is just going to be a repeat of that. Okay, so I'm going to say that a lattice L, here, now L is a lattice, not a meat semi-lattice, it's a lattice. And we'll say that it's meat semi-distributive if, for all elements A less than or equal to B, the set of things whose meet with B is A has a unique maximal element. So I look at all the things that I can meet with B to get A, and I require that this set should have a unique maximal element within the lattice. And a lattice is join semi-distributive if basically the dual condition holds. So now I want to say that for all a less than or equal to b, the set of things whose join with a is b has a unique minimal element. And then we'll say a lattice is semi-distributive if it is both meet semi-distributive and joint semi-distributive. So Emily. So, Emily gave this talk on Monday where she told us how to basically extend the definition of row motion to semi-distributive lattices. She was calling this the kappa map, but I'm just going to call it row motion because it is generalizing row motion. And so maybe I should slow down and say what I mean by this. Usually we think of row motion as being some bijective operator on the order ideals of a finite poset. But if you order the order ideals of a finite posset by inclusion, you get a Of a finite postet by inclusion, you get a distributive lattice, and every distributive lattice arises in this way. So, really, you can think of row motion as a bijective operator on a distributive lattice, on the elements of the distributive lattice. And so, what Emily is doing is basically just extending this definition in a very natural way using these edge labelings that she described to get this row motion operator on semi-distributive lattices. And it's a bi-just. And it's a bijective operator. And so Nathan and I have this paper. He's going to tell you more about this in a couple of days. It's about semi-distrim lattices. So we have this theorem in that paper. And if you specialize that theorem to the specific case of semi-distributive lattices, you get this theorem, which says that this row motion of x, which is what Emily defined, this is really the unique maximal element of the set of Of the set of things whose meet with x is pop of x. So, what's going on here is we take x, pop of x is the meat of the things covered by it. So, this is some element that's less than or equal to x in the lattice. We can look at the set of things whose meet with x is pop of x. And because L is semi-distributive, in particular, it's meet semi-distributive, so this set should have a unique maximal element, and that unique maximal element is row motion of x. Okay. So, the more general theorem that we have is like: if you have a semi-distrim lattice, which Nathan will say more about, then row motion, which we define, is a maximal element of this set. But in the case of a semi-distributive lattice, it is the maximal element of this set. But in order for this set to have a unique maximal element, you don't really need L to be semi-distributive. You just need it to be meets. Be semi-distributive. You just need it to be meat semi-distributive. And so this naturally leads to a definition of row motion for meat semi-distributive lattices. So here we're going to assume L is just meat semi-distributive, not necessarily semi-distributive. And we'll define row motion by saying we declare row motion of x to be the unique maximal element of the set of things whose meet with x is pop of x. Okay, so this is further extending the definition of romo. Extending the definition of row motion from semi-distributive lattices, which is what Emily defined, and it further extends it to these meat semi-distributive lattices. So this is a picture of a meat-semi-distributive lattice that is not semi-distributive. I've labeled the elements A, B, C, B, E, 0, and 1. And then over here, this is the action of row motion. So E goes to A, A goes to B, B goes to 0, 0 goes to 1, 1 goes to 0, and so on. And you can see it's not in row motion. And you can see it's not invertible. So, this is kind of weird because this is, as far as I can tell, the first, what I would say, natural definition of row motion, which is not an invertible map. And actually, we have this theorem which says that if you have a meat semi-distributive lattice, then this row motion operator is invertible if and only if the lattice is actually semi-distributive. So, if you have a meat-semi-distributive lattice that is not semi-distributive, then this map. Not semi-distributive, then this map will not be a bijection. So here's the picture again. And pretty much everything is open here. So this is basically what I was presenting in the open problem session. If you remember, I have this slide with like the eight bulleted questions about things you can ask about your favorite non-invertible combinatorial dynamical system. And they're pretty much all open for these meat-semi-distributive lattice. Distributive lattice row motion operators. So, for example, you can ask: what are the periodic points? What is the image? What's the maximum number of pre-images that something can have? I also think it would be interesting if there was some nice family of meat semi-distributive lattices that are not semi-distributive, where you can study this operator on those specific lattices, and maybe these questions have some interesting specific answers. But, yeah, any. Uh but yeah, any anything here is really open. Okay now in the original abstract that I submitted for this workshop, I said that I would tell you about crystal popstack sorting. So crystals are certain post sets associated to irreducible representations of simple Lie algebras. And Nathan and I have a notion of popstack sorting on crystals. And I'm not going to tell you about it because And I'm not going to tell you about it because I decided I would rather tell you about this row motion for meat semi-distributive stuff. So, if you're interested in hearing about this, I would be happy to answer questions. I'm sure Nathan would as well. You can also email me or just look at our paper, but I'm not going to actually say anything about this. And so the last thing that I will discuss is this topic that many people have asked me how to pronounce. And this is. And this is Coxderpop SAC Torsing. I think that's how you pronounce it. I don't think I've actually ever heard Nathan pronounce this, but I assume there's only one way to pronounce this, which is SAC. And basically, the idea here is we're going to take a dual approach to Coxeter groups. And I apologize, I'll probably go through this a little quickly just for the sake of time. But we're getting near the end, so maybe it's okay if I go a little quickly. The idea is we replace the set S of simple reflections with the set T. Of simple reflections with the set T of all reflections. So T is just the set of conjugates of the simple reflections. And we're going to replace W naught with this fixed Coxeter element C. We're going to replace the weak order with the absolute order, which is a partial order defined in a way that's similar to the definition of the weak order, but using the alphabet T instead of the alphabet S. I'm going to let N C of W. I'm going to let NC of WC be the non-crossing partition lattice with respect to the element C. So this is the interval in the absolute order between the identity element and this fixed Coxeter element C. And the important point here is it is actually a lattice. So you can take meets and joins. And finally, I'm going to replace this W0 of DR of X. If you remember, this is appearing in one of the equivalent formulations of these Coxeter pop stack sorting operators. Stack sorting operators. I'm going to replace that with what we're calling the non-crossing projection of x. This is pi t of x. And this is just the join in the non-crossing partition lattice of the set of reflections lying below x in the absolute order. Okay, so this might seem a little weird, but this has a pretty natural combinatorial interpretation in type A. Also in types B and D. There's a natural combinatorial way of thinking about this. Combinatorial way of thinking about this. And this is a natural analog of the W naught of dr of x because w naught of dr of x is the join in the left weak order of the set of simple reflections lying below x in the left weak order. So we're sort of replacing the left weak order here with this non-crossing partition lattice and the absolute order. So then we define the Coxeter pop sack torsing operator. Okay, so Top stack torsing operators. Okay, so if it was not clear before, the reason for the name is that we have swapped the roles of S and T. So instead of stack sorting, it is stack torsing. So yeah, that's the name we chose. And I'm going to define pop sub t of x to be x times this pi t of x inverse. Okay, so yeah. Yeah, this is how we're going to define it. And here I do need the inverse because this is not always going to be an involution. So here's a picture of what this operator looks like on B3. So here the elements are sign permutations. And I'll just say, first, I fixed a Coxeter element in order to do this. So we have to fix a Coxeter element. This element down here, 3 bar 1, 2, is C inverse. That's the inverse of the fixed Coxeter element. Coxeter element. And you can see that this element has a forward orbit of size six, which is the Coxeter number of B3. Notice that it's longer than all of the other forward orbits. All the other forward orbits have size at most five. And also, this forward orbit is kind of isolated in the sense that except for the identity element at the end, none of the elements of this forward orbit have any pre-images lying outside of the forward orbit. Outside of the forward orbit. So it's isolated because it basically looks like one straight line that then maps into the identity at the end. So we have some theorems here. Again, W is a finite irreducible Coxster group with Coxner number H. We have a fixed Coxner element C. One theorem says that C inverse has a forward orbit of size H, and that forward orbit is isolated in the sense that I was just describing. Describing. This works for all finitely irreducible Cox for groups W. Now, if W is of type A, B, D, or H3, then the identity is the unique periodic point. This is not true in other types. So in other types, there are these kind of mysterious periodic cycles that show up that are not attached to the identity. But in these types, the identity is the only periodic point and it's a fixed point. And in these types, the maximum forward orbit. And in these types, the maximum forward orbit size is h. And if you're in one of these types, but not type B, then actually C inverse is the only element that has a forward orbit of size H. So this is what I was showing in type B3, where the only element with a forward orbit of size 6 was that C inverse. This is true more generally in types A, B, and H3. And then finally, we have a few enumerative conjectures. We have a few enumerative conjectures about this popsack sorting, popsack torsing stuff. So, as I said, in type A, C inverse is the only element that has a forward orbit of size H. In this case, H is N. You can ask, what can you say about the elements that have a forward orbit of size N or N minus one? So, almost the maximum. We conjecture that this is 2 to the n minus n choose 2. You can ask a similar question in type B. Again, similar question in type B. Again, C inverse is the only element that has a maximum for has a maximum forward orbit size, which is 2n. But we conjecture that the number of elements with a forward orbit of size 2n or 2n minus 1 is 2 to the n minus n. And then in type D, C inverse is not the only element that has a forward orbit of size H. In this case, H is 2n minus 2. And we conjecture that the number of elements that actually have a forward orbit of size 2 and minus 2. Of size 2n minus 2 is this expression here, 2 to the n minus 1 minus 2 times n plus 1. Okay, so that is everything that I have to say. Thank you for coming. And yeah, that's all. All right, thank you. Okay, we have a few minutes for questions. Are there any questions for Colin?       All right, cool. Thanks. Yeah, so I'll talk a little fast for this. Thanks everyone for coming, and thanks especially to the organizers for inviting me to give this talk. Part of this talk is based on joint work with Nathan Williams. I'll indicate which part. Nathan Williams. I'll indicate which parts are joint with him. And also, I cannot see the chat. So, if you want to just interrupt any time with questions, feel free to do that. Okay. So, broadly, my goal here is to try to kind of pitch this area that I'll call non-invertible combinatorial dynamics. And when I think of combinatorial dynamics, what I have in my head is we have some set x of combinatorial objects, and we've got some function f from x to itself, which is defined in some kind of combinatorial way. And the goal is to. And the goal is to understand the dynamics of f. So, one way we can visualize this is to draw what I'll call the diagram of f. This is just a directed graph where the vertices are the elements of x. And for each element, we just draw an arrow from it to its image under this map F. So each connected component in this graph is going to have a periodic cycle. And then you might have these trees that map into the periodic cycle. And typically, in dynamical algebraic combinatorics, people are interested in involving. Combinatorics: people are interested in invertible operators, so in these cases, you just have only periodic cycles. And sometimes this is because maybe you just have an operator that's naturally invertible, like row motion acting on the order ideals of a finite coset. In other cases, maybe you have something that's not immediately invertible, but maybe you say, okay, well, we only care about the invertible part. We only care about the periodic cycles. So we'll just get rid of these trees by chopping them away. So we'll just chop away the trees and ignore them so that we can focus on the periodic cycles. So that we can focus on the periodic cycles. But I'm here to speak for the trees, for the trees have no tongues, and I think they have a lot of interesting things to say, so they should not be chopped away. I did copy this slide from a previous talk, so if you saw me give that previous talk, I'm sorry for repeating the joke. But yeah, I'm going to be focusing on these trees within these diagrams. And actually, a lot of the operators that I'm going to be talking about will have diagrams that look like this, or just this. Diagrams that look like this were just this one periodic cycle, which is just a fixed point, and then everything eventually maps to that unique fixed point. And before I go any further, let me just define the forward orbit of an element little x in our set x under the map f to be the set O sub f of x. And this will just be the set x, f of x, f squared of x, dot, dot, dot. Usually, this is going to be a finite set, even though I wrote the dot dot dot, just because the iterates are going to start repeating. Just because the iterates are going to start repeating themselves. Okay, so let's say you have some interesting non-invertible combinatorial dynamical system in your hands. Okay, so you've got some map F. So what kinds of questions can you ask about this? Well, there are a lot of questions that are just boring for invertible maps, but they become interesting for these non-invertible maps. For example, you could ask, what are the periodic points? Actually, this is a question that. Actually, this is a question that is essentially what Brian Hopkins was asking in his open problem about this Austrian solitaire map. And so, for many maps, this is going to be, but in other cases, maybe this is a really interesting question. You could ask, what is the image of your map, or how big is the image? Maybe this is like an interesting enumerative question, or if you're trying to describe the image in some kind of way. You could ask. You could ask, How can we maybe compute the number of pre-images of an arbitrary element under this map? And maybe this is a really ambitious goal for some maps, but in other cases, maybe you can do this. And if you can do this, this could give a lot of interesting information about your map F. And maybe this could be too hard, but in some cases, you could maybe ask an easier question: like, what is the maximum number of pre-images that an element could actually have? That an element could actually have. And one other question that I kind of like is: just how many elements have exactly one pre-image? The reason I like this is because I've done work on something called the stack sorting map, which is not really the same as what I'm going to be talking about here, even though they have very similar names. They're kind of different operators. And in that case, I found that this question has what I think is a pretty interesting answer. You could ask, what is the maximum possible? You could ask what is the maximum possible size of one of these forward orbits, or maybe even more generally, just what are the forward orbit sizes? But a natural question, I think, is just what's the maximum forward orbit size? And if you can understand the maximum forward orbit size, you could then ask, well, what are the elements that actually attain the maximum? And then one last question I have here is: how big is this forward orbit on average? Okay, so I'm. Okay, so obviously, this is not a comprehensive list. Maybe if you have some specific map in mind, then there could be other interesting questions to ask, but this is just sort of a list of questions that I think is maybe a good guideline for things that you could ask about your favorite non-invertible combinatorial dynamical system. Okay, so I'm going to be focusing on the POP stack sorting map as well as variance of it. So, what is this thing? So, I'm going to let Sn be the set of permutations of the numbers from. Be the set of permutations of the numbers from one to n. And this pop stack sorting map is this operator, which I'll denote by pop. It goes from sn to sn, and it acts by just reversing the descending runs of the permutation. So here's an example of this. So a descending run, by the way, is just a maximal consecutive decreasing subsequence. And in this example, I have the permutation 872496153. I've separated the descending runs with these bars, so the bars are not. Runs with these bars. The bars are not part of the permutation, but they're just separating the descending runs. And all POP is doing is just reversing each descending run. So 872 becomes 278. 4 stays as 4. 961 becomes 169. And 53 becomes 35. So it's pretty easy to describe what this is doing, but the dynamical properties are, in some cases, or depending on what you ask, it can be very hard. Part. So, one natural thing you can ask is: what happens if you iterate it just over and over? What's going to happen in the long term? And it's not hard to see that you'll eventually get to the identity permutation. This is because every time you reverse descending runs, you're decreasing the number of inversions in the permutation. So, if you keep decreasing the number of inversions, eventually you'll get down to the identity. And the identity permutation is fixed by this map. So, then you can ask, okay, well, what's the Ask, okay. Well, what's the maximum number of iterations that we actually need to use to get to the identity? And it follows from this inversions argument that it's at most in choose two because a permutation of size n has at most in choose two inversions. But actually, you can do much better. There's this theorem by Peter Unger from 1982. He showed that the maximum possible size of a forward orbit is n. So this is equivalent to saying that the maximum number of iterations you need is n minus one. You need is n minus one, just because the forward orbit is going to be w, pop of w, pop squared of w, all the way down to the identity, and then it's fixed. So it's going to have size one more than the number of iterations you need. And I mentioned, first of all, this theorem is not trivial. This actually kind of requires an interesting, clever idea on Unger's part. Also, Unger was not coming from the point of view of combinatorial dynamics. He was really coming from the point of view of discrete geometry. Coming from the point of view of discrete geometry. So he had this question about the directions determined by a collection of points in the plane. And he managed to reduce this geometric question to something about what we're now calling the pop stack sorting map. So this is actually the first paper where this pop stack sorting map appeared, but now it's appeared in other papers, often from an enumerative point of view or a dynamical point of view. Okay, here's a picture of the diagram for the POPSTX. The diagram for the PUP stack sorting map on S6. So there are 720 of these vertices representing the different permutations in S6, and you can see they all eventually get mapped into this blob, and the identity permutation is somewhere in that blob. So basically, this tree, and so the goal again is to try to understand the structure of this tree. And I have this quote from the Lorax. Okay. Okay, so here are some open questions about this pop stack sorting map. There's a lot we don't know about it. For example, I said that the maximum possible size of a Ford orbit is n. This is Unger's theorem. But we don't know how many permutations actually attain the maximum. So how many permutations actually require n minus one iterations to get to the identity? We don't know. And then another conjecture that I have is that if we let have is that if we let dn be the average forward orbit size so here i'm just averaging over the whole symmetric group then the conjecture is that the limit as n goes to infinity of dn over n is one so uh it follows from unger's theorem that this limit is at most one just because the maximum forward orbit size is n so the average forward orbit size is at most n so this ratio is always at most one but the conjecture Ratio is always at most one, but the conjecture is that in the limit it actually tends to one. And I'll just say it's not too hard to prove that the liminth here is at least a half. The reason I'm writing liminth is because we don't actually know that the limit exists. So part of the conjecture is that the limit actually exists and equals one. And I don't know how to improve the one-half. I've thought about this a little bit, but I think really, even if you could tell me how to improve this one half to like 0.51, I think. To like 0.51, I think I'd be interested to see that because I don't know how to improve this. I have a question. Yes. Is there a place we could look up terms of that sequence that is sort of hiding inside the question? What is the sequence exactly? The function of n who's available to the. Oh, oh, this. You mean the first question, right? Yeah, first question. I think actually, maybe at one point I computed it and did not see it in the OEIS. I don't think. Not see it in the OEIS. I don't think I added it though. But I think it's not hard to compute. So yeah, it can be computed for small values, certainly. I'd like to see it at some point if you can email it to me or share it with Google. Yeah, I can do that. Okay. So is that good? Yeah. All right. All right. So I want to generalize this pop stack. Generalize this PopStack sorting map to the realm of Coxeter groups. But before I do that, I just want to give a brief crash course on Coxeter groups just to make sure we're all on the same page and just so that I can fix my notation. So I'm going to let S be a finite set. This is going to be my set of simple generators for the Coxer group, also called the simple reflections in the Coxster group. W is going to be the Coxer group. It's this group generated by S satisfying certain relations. Here, E is the identity element. Here, E is the identity element of the group. The really important relations here are the ones where S and S prime are the same. And this is saying M of S is one, which is the same as saying that each simple generator is an involution in this group. So think of it as some group generated by involutions with these relations like this. So W is the Coxeter group. I'm going to always assume in this talk that it's finite and irreducible. You can extend. Irreducible, you can extend some of the things that I'm going to say to infinite Coxeter groups, but for simplicity, I'll just focus on finite ones. And the irreducible assumption, this is saying that the group is not a direct product of smaller Coxeter groups. That's, we're not really losing anything by making this assumption, because if you can understand the irreducible Coxeter groups, then you can understand the reducible ones. So the prototypical example of a Coxeter group is the symmetric group SN. It has these simple generators S1 through S. It has these simple generators S1 through SN minus 1, where SI is the transposition that swaps I and I plus 1. If you have an element little w in your Coxver group, you can write it as a word in the simple generators. This is just because the simple generators generate the group. If you look at all of the words representing your element W, then the ones that have the minimum possible length are called the reduced words for W. And the length of one of these reduced words. And the length of one of these reduced words is called the length of w, and it's denoted by L of W. Now, a Coxeter element is an element C obtained by conjugating something that you get by multiplying all of the simple generators together in some order. So, what I mean is you write down your simple generators in some order, you multiply them together in that order. What you get is called a standard Coxeter element, and then a Coxeter element. And then a Coxeter element is just something that is conjugate to a standard Coxeter element. And I'll denote Coxeter elements by little c. It's known that all Coxeter elements have the same order, so we can define the Coxeter number of W, which I'll denote by H. This is defined to be the order of this Coxeter element. Okay. Now, the right weak order on a Coxeter. The right weak order on a Coxeter group is a natural partial order, and it's defined by saying that an element V is less than or equal to an element W if there is some reduced word for W that has a reduced word for V as a prefix. And the right weak order is a lattice, so this just means it's a poset such that for any two elements, they have a unique greatest lower bound, which is called their meet, and a unique least upper bound, which is called. And the unique least upper bound, which is called their join. And a right descent of an element, little w in our Coxer group, is a simple generator S, such that if I multiply W by S on the right, the length goes down. I'll let DR of W be the set of right descents of the element W. And then finally, if I have a set J. If I have a set J of simple generators, I'll let W naught of J be the longest element of the parabolic subgroup generated by J. So J is going to generate some subgroup that's called a parabolic subgroup. It will have a unique longest element, and that unique longest element I'll denote by W naught of J. Okay? Okay. So So now I can define popstack sorting for Coxster groups. So if I have my Coxer group W, I'll define this Coxer popstack sorting operator, pop sub W from W to itself. And it's defined by saying that pop sub W of X is the meet in the right weak order of the elements covered by X in the right weak order. So this here, this is the set of things that are covered by X. Things that are covered by x in the right-weak order, and the wedge symbol here is just the meat of all of them. So, this is the greatest lower bound of all of those things covered by x. I'm also taking the meat of this with x itself, which is not going to do anything for most choices of x because this meat is already less than or equal to x. The only reason I'm doing this is because I want pop sub w to fix the identity element. I want pop sub w of the identity to be the identity. Be the identity. And the identity element doesn't cover anything. So if x were the identity, this would be the empty set. The meat of the empty set, I guess, by convention is the top element of the lattice. And that's not what I want. I want pops of w of the identity to be the identity. So that's why I add this little x meet here. But again, for most things, just ignore this part. Okay. It follows from some basic Coxer theory that there's an equivalent way of formulating this. equivalent way of formulating this. So you can write pops of w of x as x times w naught of dr of x inverse. So remember dr of x is the right descent set of x. w naught of dr of x is the longest element of the parabolic subgroup generated by dr of x. And then I'm taking the inverse of this and multiplying by x. I don't actually need the inverse here because this element is an involution. But I like to have the inverse there anyway, and there will be a reason for this later in the talk. But anyway, this is an equivalent way of formulating this definition. And it turns out that in the case when W is the symmetric group, this is really just this popstack sorting map that I was telling you about before. So this thing that's reversing the descending runs of permutations, that's exactly this constant or popstack sorting operator in the case of the symmetric group. So I think this is sort of a So, I think this is sort of a natural way of extending the definition of the PopStack sorting map to arbitrary Coxford groups. And so I have this theorem, which generalizes Unger's theorem. Remember, Unger said that in the case of the symmetric group, the maximum possible size of a forward orbit is n. And n is the Coxeter number of the symmetric group S in. But more generally, if we have a finite irreducible Coxeter group, Have a finite irreducible Coxeter group W with Coxeter number H, then it turns out that the maximum possible forward orbit size is exactly H. And I'll just say that the proof of this is type uniform, so it doesn't rely on any specific combinatorial model of Coxer groups in type B or D or anything like that. For the proof, I actually don't handle type A. Type A is a little weird. Type A is a little weird, but we already know type A is true because it's Unger's theorem, so I actually don't need to prove it in that case. It's basically a type uniform proof. I think it's kind of interesting because it combines the right and left weak orders in a subtle way with the strong Bruha order and these maximal elements of parabolic quotients. But I'm not really going to say anything too much about the proof other than just it's type uniform. So yeah. Yeah. Okay, so here is a picture of the Coxster popstack sorting operator on H3. So H3 is one of the exceptional Coxster groups. It has 120 elements represented by these vertices. I didn't draw the arrows here, but you can imagine that all the arrows are directed upward. So each vertex just gets mapped to the thing right above it in this tree. The identity elements here at the top, and it gets fixed by this map. So we have a little loop. We have a little loop. And I'll just draw your attention to the fact that the maximum forward orbit size here is 10. For example, if I take this element down here, its forward orbit has size 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. And 10 is the Coxeter number of H3. So this is agreeing with the theorem that I said before. Okay. So, this is kind of just a side remark, but it's mainly meant to say that this is kind of related to something else that people have studied. So, there's this thing called Brieskorn normal form, which Brieskorn introduced. Also, there's a paper by Brieskorn and Saito, which talks about this. And they were phrasing this in more generality in their discussion of Artin groups, but you can also think of it just for Coxer groups. Basically, this is some kind of normal form that represents. This is some kind of normal form that represents your element of your Coxster group as a product of longest elements of parabolic subgroups in some kind of greedy way. So, Brieskorn calls these longest elements of parabolic subgroups the fundamental elements, but really they're just longest elements of parabolic subgroups. And it turns out that constructing this brief score normal form is really equivalent to just applying this pop stack sorting operator iteratively. And so, for example, it follows from the theorem that I said a few slides ago. That I said a few slides ago: that the maximum number of fundamental elements appearing in the Breezecore normal form of an element of your Coxster group is h minus 1. So there's a minus 1 that shows up, but that's sort of an equivalent way of saying the theorem that I had a couple slides ago. Hearing noises. Okay. So So, yeah, so this is just sort of a side remark. Okay, here is an open question. So suppose I take J to be a non-empty proper subset of the set of simple generators. Then I'll let J w naught be this maximal element of this parabolic quotient JW. So one way you can think about this in the language that I Think about this in the language that I've already introduced: is that this is the set of things that do not lie above anything in J in the right weak order. This is the set of elements x such that x is not greater than or equal to anything in j in the right weak order. It's called a parabolic quotient. And it has this maximal element j w naught. And the conjecture is that this element will have a forward orbit of size h. I'm not conjecturing that these are all of the elements of That these are all of the elements of maximum forward orbit size, but rather just that these are some of them. And Unger proved this in the case when W is the symmetric group. And I proved this in the case for arbitrary Coxer groups when J is the entire set of simple generators minus one element, but it's open more generally. Okay. So Now, you might notice, especially if you saw my open problem presentation a couple of days ago, that you don't really need the right weak order of a Coxer group here. This definition that I was giving kind of makes sense really for any lattice or really any meat semi-lattice. So, this is just a postet where you can take meats. So, you can look at the greatest lower bound of some set of elements. I guess I kind of should fuss about, you know, maybe not any meat semi-lattice because there could be pathological examples. Because there could be pathological examples, like uncountable things, but like for the cases that I like, you know, this makes sense. And so we just define pop sub L to be this operator from our meet semi-lattice to itself, defined by saying that pop sub L of X is the meat of the things covered by X. It's exactly the same as before. Again, I'm taking the meat of this with X, which doesn't make any difference for most elements. The only reason for doing this is to make sure that POP is going to fix the bottom element of the meat semi-less. Of the meat semulax. And so, in particular, if you take the right weak order on a Coxeter group, then this agrees with the Coxeter pop stack sorting operator that I was telling you about before. Okay, so I have this paper, and in that paper, I study this pop stack sorting operator on what are called new Tamari lattices. These are some interesting lattices that are these vast generalizations of Tamari. These vast generalizations of Tamari lattices. They were introduced by Previer-Ratel and Vienna. And they have a lot of interesting combinatorial properties in relations to geometry. And basically, I was trying to understand combinatorial properties of this pop stack sorting operator and its dynamics on these lattices. I'm not really going to say what I did here, but I will just say that I think there are a lot of other interesting families of lattices. So if you have your own favorite family of lattices, maybe this pop. Favorite family of lattices, maybe this pop operator will have some interesting dynamical properties, and maybe something interesting can come out of that. Also, this paper discusses some connections with lattice congruences. There are some applications of this. For example, if you like Cambrian lattices, I can show that if you have a Cambrian lattice L, which is obtained by taking the quotient of the weak order on a Coxeter group W by some. Group W by some Cambrian congruence theta, then the maximum forward orbit size here is at most the Coxeter number of the Coxeter group. I don't think this is always going to be an equality, but it is an upper bound. And this follows basically from some of the things that I've written down in some of these papers. But yeah, so if you're not familiar with Cambrian lattices, that's fine. I'm not really going to be talking about them in the rest of the talk, but just for people who are interested. Talk, but just for people who are interested, I thought I'd mention this. Okay, so what next? I can't see the top. Okay, this is always a problem for me. I can't see the top of my screen because the, I don't know, there's this bar in the way. So I always kind of have to guess and like try to remember what I put there for like my segue into the next slide. Anyway, okay, so yeah, right, that's what this is. This is a picture of Pop operating on. Pop operating on a new Tamari lattice. I haven't defined new Tamari lattices, but they're just, you know, they're these lattices. The elements themselves are lattice paths. So it's like two uses of the word lattice. They're lattices of lattice paths. These orange arrows are just representing the operation poly. So it looks something like this. Okay. So now I want to tell you a little about the connection with row motion. The connection with row motion. This is going to be basically a repetition of what I told you on Monday in the open problem session, but I'm not going to assume people were there. So if you were there, this is just going to be a repeat of that. Okay, so I'm going to say that a lattice L, here, now L is a lattice, not a meat semi-lattice, it's a lattice. And we'll say that it's meat semi-distributive if, for all elements A less than or equal to B, the set of things whose meet with B is Whose meet with B is A has a unique maximal element. So I look at all the things that I can meet with B to get A, and I require that this set should have a unique maximal element within the lattice. And a lattice is join semi-distributive if basically the dual condition holds. So now I want to say that for all A less than or equal to B, the set of things whose join with A is B has a unique minimal element. Minimal element. And then we'll say a lattice is semi-distributive if it is both meet semi-distributive and joint semi-distributive. So Emily gave this talk on Monday where she told us how to basically extend the definition of row motion to semi-distributive lattices. She was calling this the kappa map, but I'm just going to call it row motion because it is generalizing row motion. Motion. And so maybe I should slow down and say what I mean by this. Usually we think of row motion as being some bijective operator on the order ideals of a finite poset. But if you order the order ideals of a finite poset by inclusion, you get a distributive lattice, and every distributive lattice arises in this way. So really, you can think of row motion as a bijective operator on a distributive lattice, on the elements of the distributive lattice. And so what Emily is doing is basically just So, what Emily is doing is basically just extending this definition in a very natural way using these edge labelings that she described to get this row motion operator on semi-distributive lattices. And it's a bijective operator. And so, Nathan and I have this paper. He's going to tell you more about this in a couple of days. It's about semi-distrim lattices. So, we have this theorem in that paper. And if you specialize that theorem, And if you specialize that theorem to the specific case of semi-distributive lattices, you get this theorem, which says that this row motion of x, which is what Emily defined, is really the unique maximal element of the set of things whose meat with x is pop of x. So what's going on here is we take x, pop of x is the meat of the things covered by it. So this is some element that's less than or equal to x in the lattice. We can look at the set of things whose meat with x is pop of x. Whose meet with x is pop of x, and because l is semi-distributive, in particular, it's meet semi-distributive, so this set should have a unique maximal element, and that unique maximal element is row motion of x. Okay. So the more general theorem that we have is like if you have a semi-distrim lattice, which Nathan will say more about, then row motion, which we define, is a maximal element of this set. But in the case of a semi-distributive lattice, But in the case of a semi-distributive lattice, it is the maximal element of this set. But in order for this set to have a unique maximal element, you don't really need L to be semi-distributive. You just need it to be meat semi-distributive. And so this naturally leads to a definition of row motion for meat semi-distributive lattices. So here we're going to assume L is just meat semi-distributive, not necessarily semi-distributive. And we'll define row motion by saying we row motion by saying we declare row motion of x to be the unique maximal element of the set of things whose meet with x is pop of x. Okay, so this is further extending the definition of row motion from semi-distributive lattices, which is what Emily defined, and it further extends it to these meat semi-distributive lattices. So this is a picture of a meat semi-distributive lattice that is not semi-distributive. I've labeled the elements Distributive. I've labeled the elements A, B, C, B, E, 0, and 1. And then over here, this is the action of row motion. So E goes to A, A goes to B, B goes to 0, 0 goes to 1, 1 goes to 0, and so on. And you can see it's not invertible. So this is kind of weird because this is, as far as I can tell, the first, what I would say, natural definition of row motion, which is not an invertible map. And actually, we have this theorem which says that if you have a meet-semi-distribution, Theorem which says that if you have a meat semi-distributive lattice, then this row motion operator is invertible if and only if the lattice is actually semi-distributive. So if you have a meat semi-distributive lattice that is not semi-distributive, then this map will not be a bijection. So here's the picture again. And pretty much everything is open here. So this is basically what I was presenting in the open problem session. If you remember, I had Problem session. If you remember, I have this slide with like the eight bulleted questions about things you can ask about your favorite non-invertible combinatorial dynamical system. And they're pretty much all open for these meet semi-distributive lattice row motion operators. So for example, you can ask, what are the periodic points? What is the image? What's the maximum number of pre-images that something can have? I also think it would be interesting if there was some nice family of meat semi-distributed. Nice family of meat semi-distributive lattices that are not semi-distributive, where you can study this operator on those specific lattices, and maybe these questions have some interesting specific answers. But yeah, anything here is really open. Okay. Now, in the original abstract that I submitted for this workshop, I said that I would tell you about crystal. I said that I would tell you about crystal popstack sorting. So, crystals are certain post sets associated to irreducible representations of simple Lie algebras. And Nathan and I have a notion of popstack sorting on crystals. And I'm not going to tell you about it because I decided I would rather tell you about this row motion for meat semi-distributive stuff. So if you're interested in hearing about this, I would be happy to answer questions. I'm sure Nathan would as well. You can also email me or just look at our paper, but I'm not gonna actually say anything. But I'm not going to actually say anything about this. And so, the last thing that I will discuss is this topic that many people have asked me how to pronounce. And this is Coxderpop SAC torsing. I think that's how you pronounce it. I don't think I've actually ever heard Nathan pronounce this, but I assume there's only one way to pronounce this, which is SAC. And basically, the idea here is we're going to take a dual approach to Coxeter. Take a dual approach to Coxeter groups. And I apologize, I'll probably go through this a little quickly just for the sake of time, but we're getting near the end, so maybe it's okay if I go a little quickly. The idea is we replace the set S of simple reflections with the set T of all reflections. So T is just the set of conjugates of the simple reflections. And we're going to replace W naught with this fixed Coxeter element C. We're going to replace the weak order with the absolute. To replace the weak order with the absolute order, which is a partial order defined in a way that's similar to the definition of the weak order, but using the alphabet T instead of the alphabet S. I'm going to let NC of WC be the non-crossing partition lattice with respect to the element C. So this is the interval in the absolute order between the identity element and this fixed Coxeter element C. And the important point here is it is actually a lattice. So you can Lattice. So you can take meets and joins. And finally, I'm going to replace this W0 of DR of X. If you remember, this is appearing in one of the equivalent formulations of these Coxeter pop stack sorting operators. I'm going to replace that with what we're calling the non-crossing projection of x. This is pi t of x. And this is just the join in the non-crossing partition lattice of the set of reflections lying below x in the X in the absolute order. Okay, so this might seem a little weird, but this has a pretty natural combinatorial interpretation in type A, also in types B and D. There's a natural combinatorial way of thinking about this. And this is a natural analog of the W naught of dr of X because W naught of D R of X is the join in the left weak order of the set of simple reflections lying below X. Reflections lying below X in the left weak order. So we're sort of replacing the left weak order here with this non-crossing partition lattice and the absolute order. So then we define the Coxeter pop stack torsing operator. Okay, so if it was not clear before, the reason for the name is that we have swapped the roles of S and T. So instead of stack sorting, it is stack torsing. So yeah, that's the name we chose. And And I'm going to define pop sub t of x to be x times this pi t of x inverse. Okay, so yeah, this is how we're going to define it. And here I do need the inverse because this is not always going to be an involution. So here's a picture of what this operator looks like on B3. So here the elements are sign permutation. Elements are sign permutations, and I'll just say first, I fixed a Coxeter element in order to do this. So we have to fix a Coxeter element. This element down here, 3 bar 1, 2, is C inverse. That's the inverse of the fixed Coxeter element. And you can see that this element has a forward orbit of size 6, which is the Coxeter number of B3. Notice that it's longer than all of the other forward orbits. All the other forward orbits have size at most 5. And also, this forward orbit is kind of isolated in the sense that, except for the identity element at the end, none of the elements of this forward orbit have any pre-images lying outside of the forward orbit. So it's isolated because it basically looks like one straight line that then maps into the identity at the end. So we have some theorems here. Again, W is a finite irreducible Coxinger group with Coxinger number H. Useful Coxer group with Coxner number H. We have a fixed Coxner element C. One theorem says that C inverse has a forward orbit of size H, and that forward orbit is isolated in the sense that I was just describing. This works for all finite irreducible Cox for groups W. Now, if W is of type A, B, D, or H3, then the identity is the unique periodic point. This is not true in other types. So in other types, there True in other types. So, in other types, there are these kind of mysterious periodic cycles that show up that are not attached to the identity. But in these types, the identity is the only periodic point, and it's a fixed point. And in these types, the maximum forward orbit size is H. And if you're in one of these types, but not type B, then actually C inverse is the only element that has a forward orbit of size H. So this is what I was showing in type B3, where the only element with a forward. Where the only element with a forward orbit of size six was that C inverse. This is true more generally in types A, B, and H3. And then finally, we have a few enumerative conjectures about this popsack sorting, popsack torsing stuff. So as I said, in type A, C inverse is the only element that has a forward orbit of size H. In this case, H is N. You can ask, what can you say about the? Are what you know what can you say about the elements that have a forward orbit of size n or n minus one? So almost the maximum. We conjecture that this is two to the n minus n choose two. You can ask a similar question in type B. Again, C inverse is the only element that has a maximum forward orbit size, which is 2n. But we conjecture that the number of elements with a forward orbit of size 2n or 2n minus 1 is 2 to the n minus n. And then in type D, C inverse is not the only element that has a forward orbit of size H. In this case, H is 2n minus 2. And we conjecture that the number of elements that actually have a forward orbit of size 2n minus 2 is this expression here, 2 to the n minus 1 minus 2 times n plus 1. Okay, so that is everything that I have to say. Thank you for coming. And yeah, that's all. All right, thank you. Okay, we have a few minutes for questions. Are there any questions for Colin?