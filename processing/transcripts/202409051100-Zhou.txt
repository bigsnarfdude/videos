He's interested in Bayesian methods with applications to clinical trial designs, statistical genomics, missing data, infectious diseases, and veterinary sciences. So we have about 30 minutes or so for you. Yeah, after each talk, we'll have a. All right, so let's welcome Tianjin. You can feel free to get started. Welcome. Thank you. Thank you for the introduction and thank you for inviting me to this session. Unfortunately, I cannot join in person because of teaching duties, but still I'm very glad to be part of the great conference and to have the opportunity to share my thoughts on an interesting problem. This presentation is based on my publication in the New England. My publication in the New England Journal of Statistics in Data Science. So, if you're interested, feel free to check that out. I'll start with a little bit of background on sequential trial designs. I would like to mention that this is a super simple problem, in my opinion, and many of you may have already known about this problem, but still, I hope you can get something new out of this presentation. So, patient entry is sequential in clinical trials. Patient entry is sequential in clinical trials, and as a result, it is desirable to perform interim analysis, which allow for study modifications. For example, at a certain interim analysis, if there is sufficient evidence that the treatment is pretty good, then it will be desirable to declare efficacy at the interim analysis and potentially stop the trial early. That allows us to save time and resources in drug development. And resources in drug development. A famous example of the planning of interim analysis is the recent Pfizer-COVID-19 clinical trial, in which they had four interim analysis and a final analysis planned. At each interim analysis, they calculated some stopping boundaries corresponding to the number of cases to be observed for early futility stopping and early efficacy stopping. For the subsequent discussion, it may be helpful to introduce some notation here. Throughout this presentation, I'll use y subscript i to denote the outcome of patient i. Without loss of generality, I'll assume this is a continuous outcome, which follows a normal distribution with an unknown mean parameter and the non-variance parameter. Again, assuming the variance is known is for simplicity of the discussion. Of the discussion. The unknown mean parameters theta here parameterizes the treatment effect. And I'll assume when theta is greater than zero, it means the treatment has a therapeutic effect. And when it is not, then the treatment has no therapeutic effect. As a result, our goal is to perform the hypothesis test of whether or not data is less than or equal to zero. We assume there is a total number of k-analyses, including the interim and final analysis. Including the interim and final analysis. At each analysis, this hypothesis test is performed, and if a certain stopping rule is triggered, the null hypothesis is rejected and the trial is terminated for efficacy. Of course, you can also incorporate futility early stopping rules, but here, again, for simplicity, I'm just focusing my discussion on efficacy early stopping rules. So very commonly, this stopping rule is based on comparing the Z statistics. Is based on comparing the Z statistic with some pre-specified stopping boundary. The boundary is specified by the design. We all know that with sequential hypothesis testing, the overall type 1 error rate is the probability of falsely rejecting the null hypothesis at any analysis, or in other words, the probability of the stopping boundary crossing, this statistic crossing the stopping boundary and any other. The stopping boundary at any analysis, given that the treatment actually has no therapeutic effect, or in other words, the true theta is equal to zero. And it's a common knowledge, actually, that without adjusting for interim analysis, or in other words, without adjusting for the stopping boundaries, this kind of sequential hypothesis testing leads to type 1 error rate inflation. As you can imagine, if we let all the stopping boundaries to be the 95% The 95 percentile of a standard normal distribution, then as the number of analysis increases, the overall type 1 error rate also increases. We can see with 10 analysis, the type 1 error rate is as large as 17 percent. In classical settings, these stopping boundaries are chosen to maintain an overall desirable type 1 error rate level. And famous designs include the Pocock design, the O'Brien. The Pocock design, the O'Brien-Fleming stopping boundaries, and the arrow spending approach, among many others. And as an illustration, these lines show the corresponding stopping boundaries, which are all above the nominal 1.65 threshold for a 0.05 overall type 1 error rate. So that's kind of a little bit of a background. I believe many of you. I believe many of you, even if you don't work on this field, are familiar with this kind of notation. So now I'll start by talking about Bayesian sequential designs. I think most of us attending this conference are Bayesians. So under the Bayesian paradigm, in order to make inference about an unknown parameter, we would put a prior distribution on this parameter and then calculate a posterior distribution. As a result, at each analysis, the As a result, at each analysis, the stopping rule can be based on the posterior probability of the treatment being efficacious, for example, or in other words, the probability of data being greater than zero. If this posterior probability crosses some threshold, then we can proceed as before. We stop the trial early for efficacy. A pertinent question is then, do we need to adjust for interim analysis in a Bayesian sequential design? Analysis in a Bayesian sequential design. Here, adjusting again means that should we adjust our choice of prior or our choice of the thresholds depending on, say, the time and frequency of the interim analysis. Now, again, it looks like a very easy problem, but I would say that even till today, there are still some controversies about this problem. For example, in a very recent article published in 2020, people Published in 2020, people still discussed this problem. So, if you look over the literature, you will actually find some contradictory viewpoints on this problem. For example, here is a screenshot from an FDA guidance, which says that adaptive design proposals should address the possibility of type 1 error probability inflation. These include Bayesian adaptive designs. In other words, they mean In other words, they mean we need to adjust for intro analysis in a Bayesian sequential design. On the other hand, if you look for articles written by some Bayesian statisticians, you will find a different answer. They said, for example, this is a screenshot of an article written by Professor Don Barry. He said that no adjustment is necessary when an instrument analysis is carried out with a Bayesian approach. Bayesian approach. So, these are examples of some references. So, it looks like while this seems like a very simple problem, people had different viewpoints on this problem. And our goal is actually to summarize these different viewpoints. And our investigation shows that when people said we need or need or we don't need to adjust for interim analysis in a Bayesian sequential design, they actually meant. Sequential design, they actually meant pretty different things. And our goal is to kind of summarize their perspectives and try to disentangle the implications of different perspectives. Specifically, we summarized people's opinions into three perspectives. I'll start with the frequentist-oriented perspective. So, from this perspective, if frequentist properties are of concern, then adjustments to Concern, then adjustments to Bayesian sequential designs are also necessary. While a Bayesian sequential design is inherently Bayesian, we can still evaluate the frequency properties of Bayesian procedures. Here, by frequentist properties, I mean the long-run average behaviors of a statistical procedure in repetitions of an experiment. Of course, here an experiment is a clinical trial with some given fixed parameter value, theta, or in Parameter value theta, or in this case, theta is the treatment effect. Without adjustments, the type 1 error rate inflation is obvious. Imagine that we use a diffuse prior or improper prior on theta, then it can be easily calculated that the stopping boundaries based on posterior probabilities are equivalent to the stopping boundaries based on comparing the z statistics with some thresholds. Of course, people may. Of course, people may argue about the use of an improper prior here, but my point is that even if you use a very diffuse prior, these swapping boundaries are kind of approximately equivalent. So if frequentist designs are prone to type 1 error rate inflation, then so as Bayesian sequential designs. In this case, we have to adjust the Bayesian sequential design to achieve a desirable, say, overall type 1 error. Desirable, say, overall type 1 error rate. And there are two ways to adjust it. One is to use a more conservative trial with a smaller prior variance, shrinking the treatment effect to, say, zero. Or in a different way, we can, with the number of analysis increases, we can increase the threshold values, similar to increasing the stopping boundaries in a classical sequential. Classical sequential design. Of course, you can also do a combination of both. These ideas are originated from some existing papers. As an example, here the picture shows the swapping boundaries. Now I put everything on the same scale in terms of the z statistic obtained based on classical group sequential approaches and the Bayesian approaches. As we can see, when we use a pretty conservative Can see when we use a pretty conservative trial with a pretty strong shrinkage of the treatment effect towards zero, then we can obtain an overall type 1 error rate of 0.05. Notice that the stopping boundary at the Z statistic scale is kind of in between the linear arrow spending function and the O'Brien-Fleming stopping boundaries. Of course, if we do a combination of both, Do a combination of both changing the threshold and for posterior probabilities and using a more conservative prior, you can basically get all kinds of swapping boundaries. So in summary, from a frequentist-oriented perspective, our choice of prior and threshold values for costerial probabilities should be adjusted based on the time and frequency of interim analysis. Now, the second perspective is what I refer to. Is what I refer to as a subjective Bayesian perspective. That corresponds to the article you just saw about we don't have to adjust for interim analysis in Bayesian sequential designs. From this perspective, our choice of prior and threshold values should not be affected by the planning of interim analysis. They should be purely based on subjective belief and personal tolerance of risk. Of course, in this case, we face the Of course, in this case, we face the challenge of type 1 error rate inflation, but people argued that, first of all, the type 1 error rate is defined by kind of averaging over possible realizations of the data conditional on a hypothesized value of the parameter. In other words, that's conditioned on an unrealized event, which are irrelevant to the evidence about the parameter value according to the likelihood principle. And people also argued that the type 1 error rate is not the quantity that one should pay most attention to because, again, it is calculated conditional on an unknown parameter value, which we have no knowledge about. So it is conditioned on a hypothesis rather than conditioned on some actual data we observed. There's a concern about sampling to a foregone conclusion. Sampling to a foregone conclusion, again, under the type 1 error rate framework, if we repeatedly test a hypothesis infinitely many times, we will reject the null hypothesis with a probability one. But again, that's based on the type 1 error rate argument. And Professor Barry argued that it's not a threat because if instead of the type 1 error rate, we look at the sequence of posterior probabilities, it is actually a martingale. It may decrease or increase after the next observation. Or increase after the next observation, and the expected hidden time to raise it to a specific value, say 0.95, is infinite. As a result, I include a famous quote here by Edwards et al. They said that it is entirely appropriate to collect data until a point has been proven or disproven, or until the data collector runs out of time, money, or patience. So, so far, we have seen two different. So, so far, we have seen two different viewpoints, but of course, we can see different implications. One, the frequency-oriented perspective. The implication is that we care about the type 1 error rate, while from a subjective Bayesian perspective, we should not care about the type 1 error rate. Of course, you may wonder: the frequentist-oriented perspective looks too stringent. Here, stringent means in means in stringent in terms of type 1 error rate control, while the subjective Bayesian perspective looks too lenient. Is there a middle ground? And after searching the literature, I found another viewpoint on Bayesian statistics that's called calibrated Bayesian. The idea of calibrated base was originated from Rubin's article, John Rubin's article, and Professor Lee. Article and Professor Lido's article. And they argued that for practitioners and regulatory agencies, operating characteristics of Bayesian designs in repeated practices can still be pertinent. If this is the case, we found out that adjustments may be necessary. Here, by operating characteristics, when people talk about operating characteristics, many people will think they are the same thing as frequent disk. The same thing as frequent disk properties. But here I define it in a more general way. Specifically, when I say operating characteristics, I refer to the long-run average behaviors of a statistical procedure in a series of possibly different experiments with possibly different parameter values. Recall that the type 1 error rate is defined conditioned on a specific given fixed parameter. Fixed parameter value. But here, in terms of operating characteristics, the true parameter values can be generated from a population. And that exactly matches with kind of the philosophy of Bayesian statistics. In particular, the family, the distribution that was used to generate the parameter is exactly the prior. So here So, here in terms of clinical trials, I proposed, not that I proposed, I considered two operating characteristics that I think are relevant and kind of similar to the notion of type 1 error rate control because the overarching goal is that we want to make less erroneous decisions. So, first, the false discovery rate is defined as the relative frequency of false rejections among all trials in which the null hypothesis is rejected. Which the null hypothesis is rejected. I want to mention that this is kind of different from the definition of the FDR in the famous Benjamini and Hosberg definition under the classical frequentist framework. It is defined more like matching the definition in the classification literature, for example, and some people refer to this as a Bayesian FDR or positive FDR. On the other hand, I also consider the On the other hand, I also consider the false positive rate, which is the relative frequency of false rejections among all trials with non-positive treatment effects. And specifically, when we simulate all the trials from a point mass at zero, then the false positive rate is site one error rate. Our proposition is the following. If the prior that we used to analyze the data is the same as the actual treatment effect distribution in repeated trials, which we use. In repeated trials, which we use to calculate the operating characteristics. And also, if the data model is correctly specified, then the false discovery rate and false positive rate of Bayesian sequential design are upper bounded regardless of the time and frequency of interim analysis. Again, I would say that's a pretty obvious conclusion because we all know the implication of Bayesian posteriors. Say, if we sample a sequence of Sequence of parameters from our prior, then the calculated credible intervals from the posterior will have correct coverage. It's the same argument here. So the implication of this result is that we don't have to adjust for the planning of instrument analysis in terms of controlling the false discovery rate, false positive rate, of course, under a condition that the model, including the prior and likelihood, is correctly specified. Is correctly specified. As a result, when we choose our prio and data model and threshold values, the prio should mimic the actual treatment effect distribution in repeated trials. And definitely that won't be a point mass. That's kind of the key message, because when we have repeated clinical trials, some treatments will be effective, some will not. But when we look at the distribution of treatment effects in historical trials, it won't be. In historical trials, it won't be a point mass at zero. The threshold values should reflect the desirable false discovery rate and false positive rate upper bound. And of course, here's another famous quote. All models are wrong. So when the prior is not correctly specified, then an easy straightforward consequence is that the FDR and FDR are likely not controlled. However, we can still run simulations and see how much the impact is when we falsely specify the prior. We can do that by examining a range of plausible distributions for the true treatment effects, which may deviate from the prior that we used to analyze the data. To give an example, I simulate clinical trials. Simulation is a common way to evaluate operating characteristics. Evaluate operating characteristics not only in clinical trials but also in other fields. So I simulate treatment effect from a normal distribution with some, for simplicity, I assume it has a zero mean and some variance, u0 squared. From this model, then with each simulated treatment effect, I sequentially simulate data from a normal theta one distribution. And I consider different varying possible. Different varying possible numbers of interim analysis. And I consider another prior for statistical analysis, which is for simplicity, another normal prior, but with a potentially different variance parameter, mu squared. And I consider stopping the trial early. If at any analysis, if the posterior probability of theta greater than zero is greater than 0.95, then we stop the trial early and claim efficacy. And we record. And we record the false discovery rate and false positive rate with different combinations of the true population distribution of the parameters and the analysis prior, in which case the analysis prior may not be equal to the true population distribution of treatment effects. So here are some results. In the first simulation study, I consider a very small true variance for the true treatment effects. For the true treatment effects, and I consider analysis trials ranging from 0.1 to 10. Obviously, if the trial is correct, then no matter how many interim analyses you performed, the false discovery rate and false positive rates are nicely controlled, and the resulting credible interval has reasonable coverage. However, of course, if your analysis trial is pretty different from the Is pretty different from the true population distribution of the treatment effects, then there will be inflated false discovery rate and false positive rate. And in particular, the larger the analysis trial variance is, the larger the false discovery rate and false positive rate. So the lessons learned here is that we should be cautious about using diffuse priors if the true effect sizes are believed to be small. And similar simulations were done for another true distribution standard deviation parameter, and for the same four combinations of the analysis prior. Again, if the analysis prior matches with the true population distribution of the parameters, we have good false discovery rate, false positive rate, and coverage. But of course, if these are not correctly specified, then they deviate from the desirable levels a little bit. The desirable levels a little bit. Same for larger variants for the true distribution. But in practice, of course, if we have 1,000 patients, we won't really perform 1,000 analysis. That's kind of too much. So from the simulation tables, if we only plan to conduct less than or equal to 10 analysis, then setting the analysis prior variance to be one is kind of sufficient for controlling the first discovery. Sufficient for controlling the fossil discovery rate and fossil rate at below five percent. So, if we go back, of course, when you perform 1000 interim analysis, you will see a large inflation of the false discovery rate and false positive rate. But practically speaking, if you only perform 10 analysis, then all of these are still controlled on the reasonable levels. So, using a prior like normal 0, 1 is kind of reasonable, even if you perform. Even if you perform 10 intrigue analysis. So, here is a summary. We have different perspectives with different goals. Hopefully, through this presentation, you kind of see the different implications about different opinions. And I would argue that these different perspectives are suitable for different applications. For example, the frequentist-oriented perspective is kind of stringent in terms of controlling the chance of. Of controlling the chance of falsely approving non-effective drugs. So that's suitable for large-scale confirmatory trials. The subjective Bayesian approach is, in my opinion, suitable for trials for rare diseases and pediatric trials for small populations. And finally, the calibrated Bayesian is good for animal studies for drug screening in terms of controlling false discovery rate, which allows us to discover some drugs for further development and early phase trials. Development and early phase trials. I would say the three perspectives are not mutually exclusive. For example, if an investigator is conservative, which is a personal belief, then subjective base can as well lead to low false discovery rate, false positive rate, or even low type 1 error rate. And while the subjective base is ideal in principle, we could often rely on frequencies type metrics. And probably you can see my preference here is. Probably you can see my preference here is I kind of prefer, I kind of like the calibrated Bayesian philosophy. There are other considerations. So far, what I have talked about is only a small part in the manuscript. If you are interested in more details and more considerations, feel free to check out that manuscript. So concluding remarks, we answered the question, do we need to adjust for interim analysis in abaying sequential design from three perspectives? In a Bayesian sequential design from three perspectives. And each of these perspectives, while providing different answers to the same question, has different implications. And thank you. And feel free to reach out if you have any questions or even after this talk or like through email. And all of my presentation sites are available on my personal website. Thank you. Thank you, Tinjin, for the really nice talk. I want to open the floor up to a quick question from the audience or from someone online. Very nice talk, Yanjian. You mentioned it briefly in your last slide, but so just the policies cover rate and policy positive rate is obviously not the only thing we do in particular. We do. In particular, one might be concerned about post-selection inference. Do you have any quick comments? Like you kind of mentioned maybe you could worry about it, but you didn't tell us how. Oh, you mean like, for example, coverage of credible intervals, for example? Yes, you're being very specific. I meant like if you report inference on a treatment effect after early For instance, a treatment effect after early stopping, there might be a bias because when we stop, when it looks good. Exactly. So, yeah, if we are worried about frequent test properties, again, bias is a frequentest property, then there are corresponding adjustments to the treatment effect estimator. For example, the MLE, for example, in this case, is biased. It is known that it's biased. But again, from a subjective basis, Again, from a subjective Bayesian perspective, for example, based on the likelihood principle, for example, the planning of interim analysis should not affect what the posterior distribution of the treatment effect is. So, I mean, if we adhere to the likelihood principle, then we should still report a credible interval based on the observed data condition on the observed data. But I think that's. But I think that's still an open question: like, how should we best report the treatment effect estimate? It's not fully addressed in this paper. In this paper, we aim to provide some perspectives, but that's a good future direction to consider. And thank you for your comments. Okay, so in terms of time, let's move any further discussion offline. Further discussion offline, and thank you, Tintin, again for your great presentation. Yeah, thank you.