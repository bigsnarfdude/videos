For not being able to make it there. It looks like a wonderful workshop and a very well-organized program. So, thanks a lot. In particular, I really appreciated the scheduling because now I have to do significantly less work since you had already the excellent tutorial by Gallen. Here, I would like to elaborate on a specific point on which he actually touched upon, which is the connection between spectral estimators and approximate message passing. So, he, at the beginning, he said he compared or he looked at AMP as a sort of non-linear power. AMP is a sort of non-linear power iteration, which is something that a few people have thought about. So, here, what I'll try to do is that I'll start by talking about spectral methods, and then I will try to go back and to use AMP to prove spectral gap and to prove the performance of spectral methods in settings where I can't quite do the random matrix theory. So, in settings where the spectral gap is not given to me by random matrix theoretic tools. All right, so this is somewhat the pitch. So, I'll start from spectral. So, I'll start from spectral estimator and then I'll talk about spectral initialization of AMP. And finally, I'll try to go back using AMP to prove stuff about spectral estimators. And let me also be aware of the time. So let me have my timer here. So this is joint work with a number of people. So I guess Andrea is the one that hooked me up on this topic when I was at Stanford. And so we had a paper on spectral methods back then. And with Ram G, we have started. And with Ramji, we have started to think about the interplay between the two, and then we have been able to go back mainly due to my postdoc, Ihan, that has led the two authors. And this is also a collaboration with Hon Chang, that's also a postdoc here in the group of Laslo-Erdos. Now, okay, so I'll start with the spectral estimators. Now, I'll present the model. So, I'll be looking at generalized linear models. So, again, apologies, this is somewhat the signal processing notation. So, if you're a statistic, Signal processing notation. So, if you're a statistician, of course, your n and these are so the n becomes an n and the n becomes an n. So, I'm going to use the signal processing notation here. So, I have a signal x that I wish to recover, and this is in d dimensions. Now, if I had a linear model, then I would have access to linear measurements. So, I would have access to stuff like the inner product between x and ai. Okay, and the ai's are known. I mean, the ai's would be Gaussian at the beginning of the talk, and then on the Will be Gaussian at the beginning of the talk, and then I'm going to consider slight variation on that. Again, d is the number of unknowns, so d is the number of dimensions of x, and n is the number of sensing vectors, so n is the number of observations. Now, I don't quite have a linear model, but I have instead a GLM, a generalized linear model. So, this means that I just get the measurement, and the measurement is some noisy version of the linear model. So, it's distributed according to a certain distribution P of. According to a certain distribution p of dot given the inner product. And you know, there are many, many possible examples of this: logistic regression, phase retrieval, noisy linear regression, even. And the specific regime in which I'm going to be interested is a very classical one, which the audience I'm sure is very familiar with, which is the high-dimensional one. So I'll be looking at the case where both the number of unknowns, so the number of dimensions of the signal x is large and the number of measurements. X is large and the number of measurements is also large. And I mean the proportional regime where the ratio is held fixed. Now, the specific algorithm that I'll be looking at in this talk, so I have a fair amount of slides, so I have 23 gallon at only 27 for a talk that's almost twice as long, but I hope to make up for it because my talk is very modular, so many slides really repeat. So this is one of those. So the method that I'll be looking at really in this talk is a spectral method. And the spectral method And the spectral method is the top eigenvector of some data-dependent matrix. Now, the motivation for this is that many algorithms in practice, including AMP, but not limited to it. So, if you do EM, alternating minimization, SGD, all these algorithms are iterative in nature. So, this means that you have to initialize them somehow. And this is often done by spectral methods. And in particular, here, I'm going to be using this spectral method here, which is just a sum of rank one objects. Of rank one objects, and these rank ones are just the columns of the design matrix. And then I modulate them with something that depends on the data. Now, okay, maybe it's not entirely intuitive why that's a good idea even, but one thing that you can do to convince yourself is that you can take the expectation of this guy. So if you look at the expectation of this guy, f how is say the modulus, and you look at phase retrieval, then the top eigenvector of the expectation is exactly the signal x. So the first results on this area were really just to prove matrix. We're really just to prove matrix concentration here, but they don't quite access the linear regime, or if they access the linear regime, they don't quite get the threshold right. So, the key questions here are, of course, what's the performance in terms of, say, normalized correlation, MSC, whatever you prefer. And then, if you have access to the performance, how can you optimize the spectral method? Because, of course, if I take, for example, t to be the constant function, then I'm not going to get anything, right? Because in that case, the d doesn't even depend on the signal. And doesn't even depend on the signal x. So it's clear that I need to choose the pre-processing function t somewhat cleverly or somewhat carefully. So again, the problem is I have this spectral estimator, so I want to understand the performance and I wish to optimize the proposing function. Now, before doing that, let me go back. And this is again a place where having listened to the talk of Galen should be really helpful because I would like to relate this to phase transition in random matrices. And this is exactly the thing that it. Matrices, and this is exactly the thing that he talked about. So, this is also known as the BBP phase transition. So, in this case, your problem is slightly different. So, you have a matrix, and I promise you that this matrix is the sum of the, is basically a rank one perturbation of a noise matrix, xn, and then the signal. And theta here is the signal-to-noise ratio. Okay, so given xn plus theta uu transpose, you wish to find uu transpose, you wish to find u. Okay, and the celebrated BBP phase transition is a result. Phase transition is a result that tells you what's the value of theta such that you can do that when x is GOE. Okay, this has been very much extended. And here I'm referring to a nice paper by Nada Gouditti and Benach Georges, where they prove this for a wide class of matrices, in particular rotational invariant matrices. And here they observe the same phase transition phenomenon. So the phase transition phenomenon is as follows. So there is a critical value of the SNR, theta, let's call it theta critical, such that if I'm It theta critical, such that if I'm below the critical value of the SNR, the largest eigenvalue of the n is the same as the largest eigenvalue of xn. So I'll refer to xn as the bulk. So the eigenvalues of xn, you know, they're n of them. So they form the bulk of the spectrum. And the n may have, may or may not have one outlier, and it's going to have at most one just by interlacing inequalities. Okay, because it's a rank-quant perturbation. So if theta is smaller than the threshold, then the eigenvalue doesn't escape. Eigenvalue doesn't escape. So the eigenvalue collapses on the right edge of the bulk, and the top eigenvector is uniformly distributed on the sphere. So you can't do anything. So spectral methods would be ineffective in this case. Instead, if theta is bigger than theta critical, then the largest eigenvalue escapes from the bulk, and the largest eigenvector becomes correlated with the signal u. In particular, here, the thing that I would like to stress is that there is a one-to-one relationship between eigenvalue gap and eigenvector correlation. An eigenvector correlation. So when there is an eigenvalue gap, then the largest eigenvector becomes correlated with the signal, which is what we care about, because somewhat the signal processing or machine learning problem here is I want to estimate the signal u. I don't really care about the structure of the spectrum. So eigenvector correlation is the desiderator. That's what I would like to show. Now it turns out that for spectral methods, something very, very similar happens. So you can identify some analytic condition that depends on delta and depends on taus. Depends on delta and depends on tau, such that if this analytic condition is satisfied, and again, here I want to stress that this is about a single-letter formula, so this doesn't really depend on n. Then if this function is smaller than zero, then the largest eigenvalue of dn is equal to the right-hand side of the bulk, and the largest eigenvector is uniform on the sphere. On the contrary, there is an outlier if this condition is satisfied. So, if the function is bigger than zero, and then the largest eigenvector becomes. zero and then the largest eigenvector becomes correlated with the signal. One thing that I want to mention is that somewhat here there is a null model. So now yi depends on Ai, because Yi is really a function of the inner product between X and the AI. So a null model here would be one in which the Ai here is replaced by an Ai tilde. So Yi and Ai are statistically independent. So this is the null model here, and this you can study, and this gives you the bulk of the spectrum. And this gives you the bulk of the spectrum. Now, the fact that now I have a signal x means that I have essentially a rank one perturbation because AI depends on x, but only be a one-dimensional perturbation. So you can actually reduce this to a rank-1 perturbation. That's actually how the argument went in the first few papers that looked at this. Okay, now let me state a result. And again, this will be really the form of all the results in my talk. It's a little wordy, but let me try to go through this. So the first line. Try to go through this. So the first line just tells you about the assumption. So this result was about design matrices that are IAP Gaussian. And then the structure of the result is the following. If a certain condition holds that depends on delta and depends on the pre-processing function, and this is an analytic form, so I can write it down. I mean, I didn't do that because it's a little bit involved. It's a fixed point equation. Then two things happen. So there is a spectral gap. So if I compute the limit of the top two eigenvalues of the n, they're lambda one and lambda two, and then the spectral gap is the gap. Lambda 2 and then the spectral estimator works in the sense that the normalized correlation between the top eigenvector and the signal is bounded away from zero. I have expressions for all these things. So I have expression for the top eigenvalue, for the second largest eigenvalue, I have expression for the correlations. On the contrary, if this condition does not hold, then the largest eigenvalue collapses on the right edge of the bulk, so the limit of the top two eigenvalues are the same, there is no spectral gap, and the correlation is zero. And the correlation is zero. Okay, and there is a few papers that looked at this. I think the first one is by ULU and Jen Lee that used to be group, where they do this when tau is positive. Tau positive kind of makes sense because then the null model becomes a separable covariance matrix. So, and this has been very well studied in RMT. So, then with Andrea, we extended this to general tau by using basically a free probability trick. And this allowed us to obtain. And this allowed us to optimize for the spectral threshold. So, for what's the tau that makes delta as small as possible at the threshold. And then, also, ULU in the follow-up work was able to optimize the correlation. This is LAL19. There was also some follow-up works that looked at a HAR matrix. Hey, so A, not IID Gaussian, but sub-sampling from a HAR matrix. All right, so now let me introduce some. All right, so now let me introduce approximate message passing here. Probably I don't have to do so much work, so I can go pretty fast and maybe spend more time on going back, which I think is the interesting bit anyway. So how can we solve, say, phase or shivo, or how can we solve generalized linear models? Well, most algorithms are iterative, as I mentioned, so they require an initialization. Here is just a very quick literature review. And so people have looked at alternating minimization methods, iterative projections. Methods, iterative projections, Wirtinger flow, which is essentially a sort of gridian descent algorithm, and many, many more. Now, let me focus on AMP, because this is some of the topic of the talk. So, for GLMs, this is slightly different form from the AMP that you looked at before. This is mostly because for low-rank symmetric, I only need to track one thing. Generalized linear models are akin, if you wish, to estimation of a rank one matrix asymmetric. Of a random matrix asymmetric. So, when your data matrix is something like UB transpose plus W, where W is AIAB Gaussian, but now you have two signals, so the signal is not symmetric. Because in that case, you have to track two things, the left and the right. Now, in GLMs, the two things that you have to track are the signal and then the linear combination, so X and AX. Okay, so B in this case tries to estimate X and U tries to estimate AX. Not too difficult to convince our. Ax, not too difficult to convince ourselves because if b is perfect, then this is basically x, and then ut would be something proportional to ax. This has two feet, so one feet is the Onzaga correction, so this is the same thing that Galen talked about. So this essentially allows for state evolution to hold. So this allows for UT to be a component in the direction of the object that you wish to estimate plus independent Gaussian noise. So this is the right correction to get that. Fnt. To get that, F and T are up to you, so they have to be Lipschitz and act component-wise for GAMP, but you could add extensions to non-separable, and this is also what Gelan talked about. And again, BT and CT are the on-saged corrections that have to be chosen carefully so that you are able to exactly remove the right component in such a way that the resulting noise will be Gaussian. And here is a result by Rangan and also. Rangan and also extended by Java Martin Montanari. So, this says something about the empirical joint distribution of the iterates. Again, you can make this in terms of pseudo-Lipschitz functions. You can look at non-separable. So, there are plenty extensions of this. Here I just stated the most simple one. So the simplest is that the empirical joint distribution converges to a law of a pair of random variables. So I have a single letter characterization, if you like. And the single letter tells you that u is in the direction of basically Ax. Direction of basically Ax times the coefficient plus independent Gaussian noise, and V is in the direction of x plus independent Gaussian noise, and I can track these coefficients. So I have a deterministic scalar recursion for the mu's and the sigma, and this allows me to optimize them. And perhaps one reason why people like AMP is that this also allows you to reach the base optimal limits of estimation. And again, here I put, of course, a caveat because it's well known that there are statistical to computational barriers even if. Are statistical to computational barrier, even in phase retrieval. So, in phase retrieval, it delta is between one and say 1.12. It's known that AMP has two fixed points, and then there are difficulties how to get to the right fixed point. But I would say that for pretty much all problems that I'm aware of, one of the fixed points of AMP is indeed optimal. Okay, now the statistical to computational barrier comes in when you have multiple fixed points and then it's unclear how to jump to the right one. Okay, now let me connect a Okay, now let me connect AMP to spectral methods. So, AMP has to be initialized. So, for problems like phase retrieval that are even, if I initialize at zero, I stay at zero. So, zero is an attractive fixed point of state evolution. So, if I start there, I end up there. And so I have to initialize some. Also, one option is well to use the spectral estimator that I talked about. Now, here there is one key difficulty. So, the key difficulty is that I can't just plug it in unless I want to split sample. So, one thing I could do is that I could split So, one thing I could do is that I could split samples and use some for the initialization and some for EMP, but that's not very clever. The issue is that if I reuse the same sample, so if I use the same samples for initialization and for EMP, then I'm introducing additional correlation. So, if I look at this expression, the on-sager corrections are exactly built in such a way that basically at every new iteration of A and P, it is as if I'm acting on a new independent Gaussian matrix A. So, if I add correlations via Add correlations via spectral method at the very beginning, then I'm going to have a problem. So I really need to change the analysis, and I also need to change the Onzager correction at the beginning. Okay. Now, this was solved by Ramsey and Andrea for low-rank matrix estimation via a different approach that basically uses a conditioning argument. So I try to condition on the direction that spanned by the spectral method and devise an on-sager correction for that. And Arian and his co-authors in this paper. And his co-authors in this paper here, Mario Malechi at Columbia, they had a nice heuristic argument that suggested was the right correction. I mean, here, the truth is, I did try the different approach that I mentioned here. And this was back when I was at Stanford and Ramji was visiting. And I failed. So we tried, I think, for a couple of years. And I have still 30 drafts of various conditioning methods on my laptop. And I failed to a scale that. I felt to a scale that at some point we gave up and we came up with a different approach. This different approach was quite general and can be thought as a tool or as a strategy to analyze spectral methods. And that's what I would like to present today. Okay, so here we present the AMP correction and the provable state evolution. Now the idea is very meta. So the idea is to analyze AMP with AMP. And in particular, the way we'll do it is that we'll design an artificial AMP. So this AMP is impractical, it's artificial, so I don't need to do So, I don't need to do much. So, I can initialize with a signal if I like to. And so, the idea is that in the first phase, I want to devise an AMP that actually runs the power method. And this is very similar to what Gillen was doing. And so, now the initialization can depend on the unknown signal. So, I have no problems initializing. And in the second phase, I'll try to mimic my true AMP. Okay, and why can I do power method? Well, I mean, I can choose the denoisers FNG in such a way that the That the t plus first iterate is proportional to the nth times the tth iterate. Okay, and then I know that vt is aligned with the top eigenvector in a number of iterations that's dimension free. So that's actually pretty nice. Now it's dimension free, but it depends on the spectral gap. Okay, so that's somewhat the big caveat. Now in our case, we have the spectral gap. So for the problem that I mentioned before, I proved the spectral gap before. So this is somewhat went through and we managed to write a paper. Now, what I would like to do now is to go back and to Would like to do now is to go back and to actually use this for problems where I actually don't know the spectral gap or I can't quite prove the eigenvector correlation. Okay, so I'll be looking at two at two examples really. So one is mixed GLMs and the other is structured GLMs. So I'll just present really the results. I'll state the theorems and if I have time in a couple of minutes, I'll say something about the proofs. And again, this is motivated by applications in biology. I've been looking at this for the past couple of years with a colleague here. With a colleague here. So, ask me more about it if you're interested in the applications of AMP to biology. Okay, so mixed models. So, in the mixed model, basically, I have exactly the same model as before. So, I have again a GLM, but then I have two signals or multiple signals to estimate. And each observation corresponds to a single signal. But the problem is that I don't know which observation corresponds to which signal. So, for observation in a set S, I get X1, for observation. I get x1 for observation in the set as complement. I get x2. Okay, now you can model this again as a GLM where I have an excel latent, and the latent selects the signal that I find. And this is super popular in statistics. I think of mixed linear regression. Okay, again, the speckle estimator that I have is precisely the same. Now, of course, I'm going to look at top L if I want to estimate out signals. And again, I want to find what's the performance and how to optimize. Performance and how to optimize. The result is also somewhat the same. So, again, here the big caveat is that we were able to prove this for signals that are uniform in the sphere and independent. Okay, and again, Gaussian designs. So, there is an analytic condition on delta n tau such that if this condition holds, then I get some normalized correlation bounded away from zero, and the same for the second signal. And I have explicit expressions for all these things. Now, the people that perhaps are paying the most People that perhaps are paying the most attention will notice that I don't have a converse, and this is somewhat a true gap. So, some of the proof strategy here only works for achievability, if you like. So, proving that this does not work is hard, because proving that power method doesn't work is hard, because when there is no outlier, then I don't really know where I converge. I mean, I will go to the right edge of the bulk, but I can't quite say which is the right eigenvector. So, we have some ideas on how to do the converse, and Sudako-Fernik probably would. Pseudocofernik probably will work out together with the version of CGMT, but this is somewhat still work in progress. Now, once you have this, you can optimize the pre-processing both in terms of spectral threshold and overlap. Okay, and this is what we do in this paper. The second setting is GLMs with structured designs. Then structure here is given by covariance. Again, high-dimensional regimes, and now the difference is that the covariance have. Now, the difference is that the covariates have a non-trivial correlation. And again, non-instopic covariates have been looked widely in statistics. Now, have been looked widely in statistics, but not really spectral methods. And this is the spectral method again that I'm going to look at. So this is the top eigenvector of some data-dependent matrix. Okay, and the results look again the same. So if x is an IAD prior and the covariates are Gaussian with a given covariance. Covariates are Gaussian with a given covariance under some technical condition on T, like Lipschitz, and it's decent, and some general decency, really. If a certain condition holds, then I have a spectral gap, so the limits of the top two eigenvalues of dn are different, and the spectral estimator holds. And the spectral estimator works in the sense that the normalized correlation is bounded away from zero. And I have explicit expression for all these things. Okay, I would like to do a couple of considerations on this. So, first of all, this allows you to optimize. This. So, first of all, this allows you to optimize, and I can optimize for the threshold. So, I can tell you what's the smallest delta. Now, delta depends on the covariance, such that there is a function that satisfies the sufficient condition. So, there is a function such that the corresponding spectral method will work. Okay, and again, I don't quite have a converse. So, my converse is now if I am below the threshold, then there is no function that will satisfy f of delta sigma tau bigger than zero. Alpha sigma tau bigger than zero. Okay, I do believe that this is the right threshold, but this is what I have now. And we have explicit expressions for both the spectral threshold, so this delta star of sigma and the tau star of sigma, which is the optimal preprocessing. Now, this stuff corresponds to a few things that people found out. So, the first observation is that tau star depends on sigma only via the trace. So, that's nice if you're really thinking about this as a stat. If you're really thinking about this as a statistician, because then what you can do is that you can estimate tau star consistently from the data. The problem is that, in most cases, it's not very reasonable to have access to the covariance of the design matrix. And if I want to estimate it from the data, this actually has an error in the linear regime. So you can't estimate consistently in the linear regime the whole covariance. So I can't plug it in. So if I use the plug-in estimator, I'm going to make a mistake. Now, this also addresses. Now, this also addresses a conjecture by, this should be Mayard Zakala Lud Zeborova, that looked at spectral methods for rotational invariant designs. Now, this model is not rotational invariant, but if instead I consider a sigma that's rotational invariant, it becomes so. Now, this induces a somewhat weird set of spectral distribution for the design, which is the free multiplicative convolution of Marchenko-Pasteur with some PD measure, with some positive measure. But for those, we somewhat recovered this. We somewhat recovered this conjecture. So, what they conjectured is that precisely the same form that we find here is indeed optimal. So, this covers that for a special case, admittedly. And also, interestingly enough, the spectral threshold that we find recovers the information-there's weak recovery limit. This is somewhat two steps away because I said, okay, well, I don't have a cover, so fine. But this is even information-theoretic. So, this is a specific threshold only for spectral methods, but this does recover this conjecture. But this does recover this conjectured information theoretic recovery limit in another paper. And this, I think, is again Majard. This should be Logereo Zacalazaboro. And this works well in practice. So we tried this on a few synthetic setups where the design matrix is taken either from GTX, just because these days I seem to be obsessed with genetics and quantitative genetics. That's a nice data set that you can download from MIT Broad, and also computational imaging. So here are the Computational imaging. So, here the take-home message is on delta is on the x-axis, on the y-axis, I have the overlap, so high overlap better, and then red is better than all the rest, and red is somewhat, however, a heuristic choice on top. So, in the four minutes that I have left, if I am not mistaken, I would like to say something about the proof. So, what are challenges? So, for mixed GLMs, you can still characterize the eigenvalue and prove a spectral gap. Spectral and prove a spectral gap using pre-probability tools, but it's not very obvious, at least to us, how to study the eigenvectors. And for structured GLMs, it's not even obvious how to characterize the eigenvalues, because here the difficulty is that the model is not rotationally invariant. So the tricks to reduce this to a rank one perturbation don't seem to work, at least straightforwardly. So, what we do is really we develop a new strategy to analyze spectral methods that's based on approximate message passing. Okay, so this is really the going back part. Again, what was the idea? Again, what was the idea? So, I mentioned this before. So, I said that if I design an AMP that looks like a spectral method, which I can do, then I'll find the top eigenvector as long as the n is a spectral gap. Now, this looks awfully circular because in some cases I may not know that. Now, for mixed GLMs, I'm lucky because for mixed GLMs, I can actually prove a spectral gap via an independent technique, via fair probability tools. So, then I can write now my AMP. Now, my AMP simulation. AMP. Now, my AMP simulates the spectral method, so this gives me access to statistics of the top eigenvector. Okay, so now I have the spectral gap, so spectral gap separate, and then I study the eigenvectors via AMP. This is somewhat the mixed model, how the mixed model turns out working. Okay, so we combine, if you wish, RMT tools with AMP tools to get to the final result. So somewhat half of it is done via AMP. For structured GLM, we do it all by AMP, and I'm particularly proud of that because here it's not really. That because here it's not really clear how to characterize eigenvalues. So, the idea is the following: it's very, very simple. So, first, we still need a little bit of random matrix theory. So, the random matrix theory we use to characterize the right edge of the bulk. So, to characterize the second largest eigenvalue. But that's much, much simpler because the right side of the bulk you can do without local laws, if you know what that is. So, that's in principle easier than characterizing outliers. And then, what we do is that we implement an AMP that looks like a spectral. We implement an AMP that looks like a spectral method, then we read off state evolution the L2 norm of the iterates. If this L2 norm of the iterates is bigger than the right-hand side of the bulk, then bingo. Because I have found an eigenvalue, because I'm doing power method. And this eigenvalue is bigger than the second largest eigenvalue. So it must be the largest. And this is somewhat how the argument goes. And I'm almost running out of time. I'll maybe mention this. I still have one minute on my phone. So I'll still mention this. So, I'll still mention this how this goes. So, this is basically what AMP implements. So, it implements a power method. So, first of all, I boost the spectral gap, okay, because I want two things really. So, I want both the eigenvalue and also I want the correlation. So, I boost the spectral gap first. Now, I look at sizes. So, I look at L2 norms. Now, I break this in two parts. So, I project in the direction of the top eigenvector. And then I have all the rest. Now, all the rest is at most the second. The rest is at most the second largest eigenvalue, which I can control. So, if the second largest eigenvalue here is smaller than gamma in this pie, the extra piece goes to zero. Here I have to take a few limits. So, I have to take the limit in D because I want state evolution. I have to take the limit in T because I want the fixed point. So, I want the eigenvector. And then I take the limit in T prime because I want to boost it up. So, here is the strategy. First, you guess what's the right value. And this you can do basically by running an MPL. And this you can do basically by running AMP, looking reading off from state evolution the right result. Next, you compute the edge of the bulk. So, here is somewhat where the expertise of Hong Chang in this paper was very well appreciated because he did the computation of the right-hand side of the bulk. And now we verify that this is actually smaller than the conjectured value. And now, at this point, we do the thing that we did the slide before. Now, this says two things. So, first of all, this says that the largest eigenvalue has to be really gamma. Value has to be really gamma because if it's smaller than gamma, this goes to zero. And if it's larger than gamma, this goes to infinity. Okay, and then I can also read up the correlation because from here I have the correlation between the tth iterate, which is more all the power method, and the largest, which is more alley the signal, and then the largest eigenvector. Okay, so I can also read off the overlap. And that's somewhat how the proof works out. And I'm very excited about this, mostly because this really gives you a new. Mostly because this really gives you a new way of analyzing spectral estimators. And I really think this is broadly applicable if you survive the technical details. So, a few examples could be rotational invariant designs, matrix estimation, where the noise is not IID, but it says some form of correlations, and also potentially proving universality results, because AMP has some universality results. So, all these results would be automatically inherited by spectral statistics. And with this, I would like to thank you for your attention, and I'm happy to take questions. And I'm happy to take questions.