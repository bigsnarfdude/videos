Thanks, Mika. And yeah, thank you for putting my talk on the first day because there is a lot of open problems in this talk and we'll have the whole week to solve maybe some of those. So I'll be talking about SIRANC and trying to prove some lower bounds for some problems about SIRAN. Right, so this is based on two joint works. One is Two joint works. One is already paper is going to appear at random: lower bound methods for sunrank and their limitations with Puya and these three, William Prince, Rantao and Rosie Jiao, are students of the Guild. Twelve of them graduated actually with William at Policy. And then there's some really the very recent results from actually two weeks ago where Cove was visiting me in Montreal. So In Montreal. So, this is these new results are joint work with Javier and this student again with Maguir, she and me. Alright, so I'll be talking about sine matrices, and I'll mainly take the point of view of thinking of them as binary concept classes. Kind of, I find it easier to motivate the problems using this, but they're really equivalent to communication problems, and I'm gonna Equivalent to communication problems, and I'm going to mention those two. So, right, matrices, rows are indexed by some set X, columns by some set Y, and the entries are plus 1, minus 1. And sometimes they're infinite matrices, so they're not necessarily finite. And they can represent some concept class, say if a person X likes or dislikes a movie, that your entry can represent that. This is unintuitive. This is unintuitive. Or whether some image represents some object, so some object could be. Or, say, email to someone, is it a stamp or not stamped? Something like this. Okay. Let's look at this, some example to motivate this geometric representation. So, this is a very common thing, linearization, kernel trick, and stuff, to represent concept classes. Concept classes geometrically by linear functions. So let's say we model y is the set of all restaurants, and we model each restaurant with three numerical features. Let's say food quality, service quality, and price. These are three numbers that we assign per restaurant. And we represent people with, say, these four numbers, x1, x2, x3, which are the weights they would assign to these three features, like how much they value, say, food quality versus service quality, just these numbers. And x4 is just a three. Just these numbers, and x4 is just a threshold if the restaurant needs a threshold or not. So we get some relation like this: that if this weighted sum of these y's for this person is larger than the threshold they have, then they like it, otherwise they dislike it. So this gives us a geometric representation of this concept class in r to the 4 by the following inner products of these vectors, right? So people are represented by these x1 to x4 and restaurants by 1. To x4 and restaurants by y1, y2, y3, and then putting a minus 1, take care of this threshold thing. So you get this four-dimensional representation of this thing. So I'm just going to use this to motivate the definition of sirens, kind of the more geometric definition of it, let's say. So I'm going to work with vectors, unit vectors. So s to the d minus 1 is the unit sphere in Rd, right? The usual notation for it. And so I'll define the sine. And so I'll define the sine rank in the following way. So we have a sine matrix. Remember, the entries are plus ones and minus one. So the sine rank is going to be the smallest constant D, such that I can represent my matrix on the D-dimensional sphere, the sphere in Rd. So I'm going to have a map that maps the rows to points on the sphere, and another map that maps the columns to the points on the sphere, and the entries are just. Here, and the entries are just the inner product of the signs of the inner products of this. So, it's really a representation as points and half spaces, but defined with unit vectors. And the reason and the fact that I assume these are unit vectors, it's really without loss of generality, because I only care about their sign, so I can always normalize them and make them into unit vectors. So, here is an example. Let's say if the point x is represented by this point, By this point, 5x, then you look at this half space that's defined with this line here, because we're in 2D, and anything above it, so all these y's that are mapped above, they get plus 1, and the ones that mapped to below it, they get minus 1, right? So it's a representation by points and half spaces. So you can think of it as that too. So an entry is 1 if and only if the point defined by y belongs to the half space that's belongs to the half space that's defined with 5x. So 5x is the normal vector to that half space. So this is a geometric representation of the matrix. So when you have such a representation, there is another very natural notion from the point of view of learning, and that's margin. So obviously having low dimensional representation is good, but there is another parameter that we can optimize, and that's That we can optimize. And that's how close these points are to the boundary, to the decision boundary. So here, if you look at this point, it defines this hyperplane. And this point is kind of close to the hyperplane. So it's not very clear which side is, let's say. So if things are a bit fuzzy, then you wouldn't know. So that's the motivated definition of margin. So if you give me a representation, the margin is really that you look at all the points x and all these points y. And all these points, y, and look at what is the closest you get, like what is the closest point to a decision boundary. So you're really minimizing this inner product of phi x, phiy, its absolute value. So that's called the margin of this particular representation. You can also look at the entries of this representation, which is defined by this inner product, and that's the smallest entry that you get. So you want these things to be large. You want your points to be further away. You'd want your points to be further away from these decision hyperplanes. Right, now we can forget about dimension and try to optimize this. So then we get to the notion of margin of the matrix. So this is independent of the representation. So this is the largest possible margin that we can get among all representations, all such representations in all dimensions. So now you can look at instead of D, some really big dimension, and maybe you can find. Big dimension, and maybe you can find a better representation which gives us better margin. Points are further away from these hyperpoints. So, this is the second notion that one would naturally optimize. So, that's called the margin of the matrix. So, we had the note MA. So, dimension we want to make it a small, margin we want to make it large. So, this is a number between 0 and 1, and we want to make it bounded away from 0. Okay, now, so we know VC dimension is. So we know VC dimension is important. If if a concept class is VC dimension is accurate, yes? Compass motions related to regular motions, for example sine rank or rank and so sign rank is a is a relaxation of it, so it could be much smaller than rank. So another definition of sine rank is you're allowed to change the entries of your matrix to make the rank smaller, but you have to keep their signs the same. To keep their signs the same. So if it's positive, it has to stay positive. If it's negative, it has to stay negative. But you can change them otherwise. So that you minimize that. Okay, so... So what's margin related to some natural objects? So margin, I'll give you, there's a nice theorem which actually fully characterizes it. Alright, so bounded VC dimension, pack learnable, that's great. Bounded sine rank, it means you can represent. Psi rank, it means you can represent your concept class linearly in low-dimensional geometry, right? There is this cursor-dimensionality people talk about. So it's nice when your concept class has low dimension. And margin bounded away from zero, if your concept class, if you have such a representation, then your concept class is amenable to algorithms such as Perceptron or support vector machines. So there are these learning algorithms which can actually very well tell you. Very well, how you correctly learn the points which have large margins, right? So they're great for that. So, all these three you can think of them as some notion of complexity for your class of class. You want them to be these two to be small and marginally large. Okay, so Syrank, what do we know about SIRAC? SIRANC is quite, it's a very natural thing. It's geometric, it's natural from the point of view of communication complexity. Of view of communication complexity learning, but it's a very difficult notion. So we know very little about it. So in the next couple of slides, I'll kind of quickly survey what we know about it. And this whole work came from two open problems that I'll come to later. So we're trying to actually prove sign and lower bound for these two problems. And we really went over all the papers that we found and tried to understand what are different techniques that you can use for lower bounds and how do they compare. For lower bounds, and how do they compare? So, what is the best one we could use? Right, so I'll go over these things. So, quickly after the definition of Simrank, which is 85 by Petrui Simon, Simon Alan Franklin Rodel showed that the number of matrices with small sine ranks is small. So this is a really nice paper. So for D less than n over two, there are only these many matrices of sine rank D. These many matrices of sirenic D. So 2 to the dn log n. So think of d, say, as a constant. It's 10. So you get really few, 2 to the n log n. This is much smaller than the total number of sine matrices. So you get from this that most matrices have large sine rank. So if you pick a random matrix, it's going to have very large sign rank. So, I mean, we know these things in complexity, circular ones and all those things. It doesn't necessarily mean that we're going to. It doesn't necessarily mean that we're going to be good at proving log ones for sign. Actually, this theorem is interesting. Like, I mean, it's not an easy counting argument. So, the counting comes from these works of Milner and Thom and then some refinement by Warren in the 60s, where they really, like, these were algebraic geometers and they counted the number of connected components of real algebraic varieties. So, the sign patterns of polynomials fall into that. So, they are interesting works. Are interesting works, and you can use them to get this kind of balance. Okay, so we get good lower bounds for random matrices. So, what about explicit matrices? So, for explicit matrices, we know very, very little. So, there is an easy lower bound, VC dimension. So, we are representing here our matrix as points and half spaces in D-dimensional space. So, that immediately gives us a bound on VC dimension. The V C dimension can't be more than D. VC dimension can't be more than D, right? If you have points and half spaces in dimension D, the VC dimension of this class is at most D. So you get a lower bound. If your VC dimension is large, si rank is going to be large, right? So you get a lower bound. And this was already kind of implicit in paper of Petrubian sim. Right. So that was for a long time state of the art. And then came Forcer around 2002, when Stafford is published. I don't know when it appeared first. Was published, I don't know when it appeared first. And he proved the first super logarithmic lower bound on Seirank using a beautiful convex geometric argument, like transforming the vectors into isotropic position. It's a really beautiful paper. It's very short, too. And so, and then this paper, this method, his method got, so this he proved lower bounds, for example, for Hallamard matrix and so on, really strong lower bounds. Really strong motorbath. And after that, his method got refined further. There are like follow-up papers by Linnell and Schreikman, Shersov and Shersov-Rasborov, they found further refinements of it and managed to prove lower bounds, so it's, for example, for AC0 and so on. But one thing that you, like after reading all these papers and trying to understand them, is that they kind of all, you can formulate them as that kind of a lower bound. So remember, I talked about margin. So remember I talked about margin. We can similarly talk about average margin. So where instead of just caring about the closest point, you care about on average how far the points are from these boundaries. And what Forster's method and all these follow-up works kind of tell us is this kind of a lower bound. So what they give us is a lower bound for 1 over the inverse of the average margin. So forcers. So Forcer's theorem can use its proof to get this inequality. So this follows from his proof. It's what's stated in his paper. That one over the average margin is at modest sine rank. And then all these refinements are basically proving lower bounds for this left side of it. So they prove that this the inequality that you have stated, that's equivalent to Foster's neural norm? So I don't know what you call forced lower bound. It's not equivalent to a spectral one. But it it follows from isotropic setting. But it's not equivalent to the spectral one. No, at all. Yeah. Right. So this inequality, what we got out of this is now it actually you can use this to understand the limitation of all these refinements. Like you can bound whatever you get from them by just looking at this average margin. If your average margin Average margin. If your average margin is large, basically all these methods are hopeless and giving you good lower bounds. And finally, there is a third method of proving lower bounds. So this hasn't been much looked at, I think, in this complexity community, but it's a very important tool in discrete geometry or like the study of geometric graphs and computational geometry. So this is Geometry. So, this is from a paper, I don't remember all the names of the authors, unfortunately, alon Janis Bak. I don't remember the others. Sorry about that. From 2005, where they proved that if the sine rank is small, then your matrix has large monochromatic rectangles. So if your n by n matrix has sine rank d, then you get a monochromatic rectangle of size n over 2 to the d plus 1. And yeah, it's a non-trivial theorem. And yeah, it's a non-trivial theorem. We can get something similar to this from Forcer's, not from Forcer's method, but from isotropic positioning of the method. So there is, there are, this follows from a paper of Giao, which I think they use for Sukula. But there is a different paper of Chazelle, which uses the cutting combinatorial argument, which gives you a different bias. Which gives you a different bound. So there are different ways of proving this. But anyway, yeah, so you get large monochromatic rectangles, highly non-trivial. Like think about Sierrac 3, so you get really large monochromatic rectangles, and it's non-trivial. Right, so I'm not going to formally define this rectangle ratio, but basically, if you look at this theorem, it gives you a way of proving lower bounds for sine rai. You just have to show that your matrix doesn't have large monochromatic rectangles, or some sub-matrix doesn't. Or some submatrix doesn't have a large monochromatic rectangle, right? So according to that, I define this monochromatic rectangle ratio, and you get such a lower bound. Log of this is a lower bound for sine of with a factor two, that's why I was currently thinking. Okay, so. So I assume that this is maybe true for sine reasons. No, I don't think so. I don't think it's true for recitation. Like random graphs, sparse random graphs, don't have large Markov rectangles, but have small recycled. Rectangles, but that's small celebration, right? Yeah. Right, so just to summarize, that's all we know. So there are three different parameters that we can use to lower bound sine rank, V c dimension, one, like inverse of this average margin, and log of this monochromatic rectangle range. Now, we're trying to prove some lower bound for these two problems that we're working on, and so we have these three options to see what works. See what works. And one thing we try to do is to compare them, see what is the best one we can use. So, this is our first theorem. So, this one I would then call our theorem. It's kind of, I mean, the definition of average margin, we have our own definition, but this is kind of what that's called. So, BC dimension is kind of the weakest. Square root of it is always bounded by average margin. But the more interesting thing that came up from our work is that this is bounded by this rectangle ratio. So note that here I have a log. So if you actually care about the actual bounds, like not just constant versus super constant, then this might be better tool to prove things. And indeed, like Forster's method is the only one that gives us, can give us super logarithmic bounds. But if you just want to prove some reasonable lower bound or something, then this rectangle one is the strongest one you might try to use. And this inequality is kind of a dual. Is kind of a duality argument, minimax, but it's not a trivial minimax, so it's not equality. So we don't know if these two things are comparable or not. But yeah, it's less than or equal, sorry. What exactly do you mean? It is actually less than or equal. For every matrix, this is less than or equal. Yeah, so this is for every matrix, yeah, that's good. Yeah, yeah. So do any of these uh bounds survive uh approx approximate better than approximate to the Better than approximate to the sign to the ring. Like you're allowed to. Obviously, it's a much harder problem. Like rigidity, you mean? Yeah. I don't think so. I'm not sure. Does that even survive? I haven't thought about it yet, but because I just assume rigidity is so hard to. You can't think about it. Um for the average it means that for the average margin it also satisfies that it's No, no, it's the other way. Right. So. I don't. It can be equal, but I don't know if... I mean, this could be bounded theoretically by some function of this, but I don't know that. For example, it can be like a vector tangible value of n. Value of A will be larger than so rectangles, yeah, I don't know exactly. But one thing is this log of rectangle and VC dimension, they're at most log n for n by n matrix. That's something to have in mind. All right, so what this inequalities really tell us is that if this thing is constant, you're not If this thing is constant, you're not going to be able to be at a super constant lower bound using EC dimension or forced. Okay, so but maybe they're never constant if your sine rank is actually large, right? It could be that actually maybe you can buff this from below by sine rank. So yeah, we kind of looked at that. And this is maybe a little bit surprising, maybe not, I don't know. So there are matrices with very large sign ranks, so N2, like one. To like one-third, actually, you can put here pretty much for n by n matrix, such that this rectangle bound is just constant. So these are matrices of very high sign rank. We have no way of proving Laura bounds for any of them. You get just constant by using Forcer or just rectangle. So it's a counting argument. So the proof is by counting arguments. So we construct a large family of matrices with this constant rich. With this constant Recannobound, and we know the number of sign matrix is not that large. So you get just wrong over. Okay, so right, so a problem that you get from this theorem is that construct an explicit sequence of matrices such that their rectangle ratio is constant, but their sine rankle is infinite. So matrices that doesn't have monochromatic rectangles. And none of their sub no, they have large monochromatic rectangles. They have large monochromatic rectangles, and also every submatrix of them has a large monochromatic rectangle inside it, which is comparable to its size. Right, so I didn't give you a formal definition of that. But this is what it means. It means every submatrix of it has a monochromatic rectangle which is comparable to it in size. Say it's one-tenth of it. So construct something like that, such that the sine rank just goes to infinity. Infinity. And the nice thing about this problem is to solve it, you have to come up with something that can be semi-at least in a straightforward way. Maybe there's clever variables. Okay, so let me also now talk about the two open problems that actually motivated this whole thing. So we were trying to solve these two problems. And right, so one of them is not really, I mean, actually, none of them are our problem. But this is a really great problem. A really great problem. So, I think it's quite a fundamental problem. So, it's about semi-algebraic matrices. So, let me define them. So, what is a semi-algebraic sine matrix of complexity D? So, these are the matrices where your rows and columns are defined by points in R to the D. So, they are geometric. Every row is a point in Rd, every column is a point in Rd. Now, the entries are defined by polynomial, so this is the semiological. Polynomial, so this is the semi-algebraic thing, by at most d polynomial equalities or inequalities. So it's like if your x and y satisfy this list of inequalities and equalities, then the entry is 1, otherwise it's minus 1. You can even allow quantifiers like there is this for all. So this is, these are, yeah. But each entry there is a system of No, the system is fixed, exactly. Yeah. But you can plug in X and Y to it and yeah, that's great. Thanks for asking that. Thanks for asking that. Now there's another, this D appears three times. So one was that the points are the number of relations that you have, at most D, and each polynomial is also of low degree, sub-degree at most. So you just call anything like that a semi-algebraic matrix, so complexity. Now, if you look at matrices of sine rank D, these are semi-algebraic with complexity points D. I mean, the representation was with points in R D, so you already have. With points in Rd, so you already have D there. I have a polynomial of degree 2. This is only of degree 2. And I have only one polynomial here. So it's one inequality, degree 2, but my points were in R to the D. So you get a complexity D. So that's an example of a semi-algebraic set. So generally, when you have a family of matrices, it's called semi-algebraic. If D is a constant, say 10, 100, something, then you call it the semi-algebraic family of matrices. So many architectural, I don't know any other examples. I don't know any other examples really, right? Any natural geometric graph that you take is going to be semi-algebraic. All these graphs that are defined by points on the plane, let's say, interval graphs, so you have two coordinates of the beginning and the end of the interval, and you ask whether they intersect or not. These are algebraic relations. Unit distance graphs. So you have coordinate points, you want to know if your distance is exactly one. That's an algebraic relation of That's an algebraic relation of degree 2. Degree 2, yeah, exactly. So you square those and add them up. Intersecting segments, intersecting disks, intersecting other regions, like parallel boxes and so on. So these are all semi-algebraics. And this is a huge area. And there are many great papers, like just a couple of people who are very active in this area, along Janspak, Techopath and Russ. They have many papers on this area. Many papers on this area. And yeah, there are many new exciting results, like the almost solution of Ruth and Katz and Herbus distance problem. And yeah, so a lot goes on here. And the tools that they use are often really, they generalize properties of low sine-rank matrices to semi-algebraic matrices. So things like existence of large monochromatic rectangles, very strong regularity lemons. So you can generalize, extend those from Extend those from low sign rank to semi-algebraic. And then sometimes they use tools for algebraic geometry, like Hilbert polynomials and so on. Or like these interesting geometric partitions and so on. Right, so as I said, matrix of Gossine and Carson algebraic, so here comes the open problem. Are these actually the same thing? Do we actually gain anything by align more than one inequality? So this is really about the limit of linearization. Limit of linearization. So for the definition of semi-algebraic, you're allowed to use any polynomial you want. That you can linearize, that's fine. But you can have a list of inequalities, and that's where the issue is. You don't know if you can linearize that. But if this is true, it's a great result, really. So it says instead of studying semi-algebraic, you can just study this much simpler, just one inequality class, right? Degree of the polynomial degree? Degrees also, everything is bounded by D, right? So when I say semi-algebraic, everything is bounded by some constant. And you're not going to get the same constant, but maybe you get some... not being able to write polynomial because it's not a Boolean case. Yeah, so I'll get to that exactly. Yeah. Alright, so this is a problem. It's a bit hard to think about it in this form, but it has really nice, simple. It has really nice, simple reformulations. So, one is exactly what Mark is a bit ahead. You can reformulate it as the following. Is it true that for every constant D, there is another constant C of D such that if you give me two matrices of sine rank at most D, their and, or you can take their OR, has sine rank which is bounded by this constant. So if you give me two mages of sine rank 10, I want to bound the sine rank of their pointwise then. To bound the sine rank of their pointwise n, this is the pointwise n. So they have the same dimensions. Can you bound it by, say, 100,000 million? So that's equivalent to this semi-algebraic. Like you really just need to prove this special case of that. And it's really a beautiful question. For what d do you know? Yeah. So we know it for d positive to two. Actually for a while I was trying to put disprove it, so I I really don't know how much I believe that conjecture, because it's like so strong. Leave that conjecture because it's so strong. Wait, is this violent the triple intersection now? No, I'll get to that. Yeah, no. Because the C D could be anything. So it's non-trivial for the equals to 2. Actually, I was trying to disprove it, but Shai Moran showed me this proof that actually they had for some problem that I'm going to show in the next slide. And from that, you can get equal to 2 like this. So yeah. Open for t equals 3. So in three dimension, we don't know. We don't know, but sine rank 3, sine rank 3. You take the and, can you show that the and the sine rank almost a million? We don't know. And so, yeah, that's what you're mentioning, this intersection thing. So, there is this constant, this is just asking about any constant. You're happy with anything. And there's this lower bound from a couple years ago by uh Bob Mande and Justin Taylor. And they proved that the C D, if it exists, it has to be at least two to the log. It has to be at least 2 to the log d squared, right? So it just says it has to be relatively large. You're not going to get, say, d or linear d. It has to be larger. But potentially it could be 2 to the d. Maybe it's enough. Right, so that's this formulation. There's another formulation which is quite nice, which is just looking at intersection of half spaces. So now, this is my concept class. Instead of points and half spaces, I'm looking at points and intersection of two half spaces. I'm looking at points on the intersection of two half spaces. So I have these two points now, x1, x2. Each one defines a half space, and I'm going to put one if y is in this intersection and minus one otherwise. You can see it's kind of related to n. Right, so it's a concept class, and what is the reformulate your conjecture is that this has bounded sine rank, this matrix, or every finite sub. Matrix, or every finite submatrix, I would say. This has a sign x bounded by some constant. This is not known and it's equivalent to the previous question, although you're going to get a different C D, like there is a reduction going on. So this one, so this was what Chai Maran told me. So they had a proof for d equals 3. From that, this 3, you get 2 for the other one. And alright, so it's open for dimension 4. And after you get 4 relative, what's the result you get? Bad for Rolette? What is that too bad for who? Oh, 60 or something. Yeah, can probably make it a bit better, but yeah, not very small. Okay. So this was the first problem. The second problem we were trying to solve is about margin versus sign rank, right? So we talked about these two parameters that you can optimize and want to know how much you lose and gain by optimizing them. Right, so okay, let me define this formula. Defined as formula. So, this is, I'm attributing this to this paper of Linear Mendelssohn-Scheffman and Schreikman from 2007. They ask a question which is equivalent to this. So, the question is, suppose I have a concept class which is nice. It has large margins. So I have a representation of this concept class, maybe in some very large dimension, such that, yeah, the margins are large. Points are away from the boundaries. Does this mean that? From the boundaries. Does this mean that this matrix is coming from low damage? Does this mean that I can change the representation to some other representation in low damage? But yeah, so that's the question. Okay. And so when they this, they asked this, I'll say what the form they asked was. But back then, actually, the margin, people didn't know much about it. Like, it was a study, there were lower bounds on it. There were lower bounds on it, but then there also the discrepancy which is studied and there were lower bounds on it. And they proved in 2009, which is a really nice result, not very difficult, if someone tells you to prove it, that these two are actually the same. So margin, it's really our own discrepancy in publication components. They're pretty much the same thing. We just have a factor of eight difference. This is the usual discrepancy. Take the hardest measure. Take the hardest measure correlation with the rectangle with a rectangle combinatory or rectangle. So these two things are the same. If you want to think about margin and equivalent, you think about discrepancy. Okay, so with this, and discrepancy has all sorts of equivalent forms, you can get four times two, eight different formulations with this. And whichever parameter you are more comfortable, we can work on that. So margin is bounded away from. So margin is bounded away from zero, so it's omega one, or discrepancy omega one they are equivalent. And discrepancy on randomized communication complexity are equivalent. Discrepancy is really a two-bit randomized protocol, right? You have just one rectangle. So you can amplify that. You lose some exponential, but we don't care. We only care about this constant. So you might as well, or you could, if you prefer, think about constant randomized computation of this in public point. So it's important as public quantum. And it's public quantity. So there are very few examples we know of this. Matrices which have constant randomized communication problem, complexity. I mean, equality is the iconic one, actually. And if you're familiar with gamma two norm, there is also this equal to approximate gamma two norm. So that's another analytic way of thinking about it. And right, so that's about sign rank. So we want to say, say, randomized communication. So, we want to say, if say randomized communication complexity is constant, then is sine rank constant. And sine rank is also important to unbounded communication complexity for those who know in the private coin model. It's really the log of this. That's the other one. So it's a question about complexity. In the original paper, they asked about randomize so this equals the Gua one versus siren. So that was their formulation. But you can get one market same question. That's really the point of market. Same question. So, yeah, like one nice communication complexity formulation of it is this. If randomized communication complexity is constant, a public coin, does it mean that unbounded error is constant? Like, there is this general perception that unbounded is easier than bounded, but there is a difference, right? Bounded here is with public coin. This has to be defined with private coin, otherwise it's not an interest function. Well, no, that perception is because if you add a No, no, that perception is because if you add an o-gen then this yeah, yeah, exactly. But yeah, like in this range it's not true. Especially when you think about margin and flagrant things really. If you know to answer a different, can you answer the previous one? No, the two semi-algebraic thing? No, I don't see direct connection. Maybe there is. You question if the Handbarren complexity of the Unbounded anti-complexity of A is bounded by the unbounded table of the question of B is bounded, then the unbounded later. Or the unbound. No, wait, I mean unbounded is equivalent to SignRank, so what unbounded plastic the previous one was about sign rank, right? Yes, but sign rank and unbounded they're pretty much the same. It's just a lot difference. They're pretty much the same. It's just a log difference. Right, but if you have D, you can prove a weaker statement about random and stronger statement. Sorry. Why is it the same? So the numbers can be very long. For what? So if you have constant sign rank, why is it down with the location companies? So there is this protocol that you can use. There is this protocol that you sample right you sample to be able to sample from a product in very long numbers. You have to send the index, right? That's the issue. No, the index is fine because the index I guess is constant. But then you need, so I have a number, you have a number? You need, so I have a number, you have a number, and you need to sample somehow proportionate to this product. Yeah. So I sample for unbounded, I sample privately according to my coefficients, right? Oh, I see, you just literally sample. Yeah, yeah, exactly. Okay, so we have a candidate for this. We have a candidate for this to disprove it, so I don't think it's true. And the candidate is just the adjacency matrix of the hybrid. So, as a communication problem, one person gets an n-bit string, the other one another d-bit, and they want to know if they differ in exactly one portion. So, this has randomized communication complex dimensions constant, comes from randomized parity decision tree. You just sample a bunch of parity. For you, just sample a bunch of parities to figure this out. And right, so the conjecture is that the sine rank of these matrices grows. You just have to show that they're not bounded by some fixed functions. You just want to say as D grows, the sine rank of the cyberkey adjacency matrix grows. And we don't know how to do it. So that's something to try to work hard. Just two questions. Just a quick question. So this randomized algorithm has a band error, I guess. Which one? Would a hypertube or? Yeah, so I mean, suppose what it would change the conjecture to say, you know, if instead of approximate gamma kilome or something, you'd have exact gamma kilome or something. Right, great, yeah, that's an excellent question. It's in my later slides. Okay, yeah, very agree. Okay. Okay. Yeah, that was really good. Right, so these were the two problems. So, intersection of half spaces, which was the semi-algebraic versus sin rank, and then the hypertubes. Can we prove this? So basically, we tried to disprove this one and prove this one. And turned out both of these, the rectangle bound is constant. So the methods are hopeless for both of these problems. So that's basically what happened, how we got into those graphs. Okay. Okay, um right, so this is the new part, which is just done two weeks ago, and kind of written it down quickly, so hopefully it's correct. Right, so here I'm going to look at the same problem, but for partial functions instead of total functions, right? Hypercube was a total function. But here I'm allowing myself to look at partial functions. Okay, so the same problem. Okay, so the same problem. Can you come up with some partial function with a large margin which has like sine rank has to grow to infinity? Or it can ask for constant randomized. And as I said, we don't know it for total function, even though hypercube, I think, is a good candidate. So for partial functions, what can we do? Here, like, I don't have to go and look for candidates. There is a canonical, the Candidates. There is a canonical, the hardest candidate. And that's just pick anything that has large margin. So here's my function. I take two points on the sphere, so x and y, both on the sphere. If they have large margin, if it's positive, I'm going to put one. If it's negative, I'm going to put negative one. And if the margin is small, it's a star. So we don't care. Right? Like, I don't have to look for any other function. This is the hardest thing I can pick. Like, I put as many entries as I can. I put as many entries as I can. So this is the natural candidate. So we should look at this function. Now, right, yeah, so we actually proved for this. So we proved that, well, it's maybe surprising actually, we didn't think we'd get exact bound. So we showed that for every, if epsilon is 1, you're really just looking at the equality function, whether x and y are the same or antipodal. But if x is. Antipodal. But if x is if epsilon is less than 1, then we get that the sine rank is actually full. We can't even go 1 below. So the sine rank is exactly. Yeah. So that's the theorem. So no matter how you fill these stars, if you want to be correct on these points, you can do better somehow. And not that this is sharp because, I mean, you can fill out the stars with the natural just in. out the stars with the natural just inner sine of inner product that gives you sine rank d for this yeah it says you can't go even one below that and right so it gives us for at least partial this separation so this us easy randomize right so you just pick a random hyperplane and see if you fall into different sides of it and but yeah so the unbounded you get a lower bound at least fine right The proof is short, it's really not that difficult, but I mean it uses Bose-Colam, and that was the key idea really. So it's a different technique, like it's not any of those three techniques I mentioned before. It's a direct application of Bose-Colam, which says every continuous map, if you want to collapse this sphere in dimension D to dimension D minus one, you have to map two antibodies at the same point. So if our mapping is satisfied. Our mapping is satisfied, it's continuous, of course, it's not going to work, right? If our phi was continuous, you can map two points which are completely opposite to the same point. But the issue, the reason you have to work a little bit is that you don't have the requirement that this phi and psi are continuous. So you have to make them continuous without quite proof. Yeah, I'm quite excited about this. I don't know if there is a different easy proof that doesn't use Boris Colon. Because the same issues are here. Like, I mean, the same issues are here. Like, you can't use the previous techniques because the margin is large. Right, and there's a similar problem that I just want to mention here, which actually kind of made us actually look at this problem, the partial case, which is in a recent paper of Alon Haneke, Holtman, and Shah Moran, from 2001, about learning partial classes. And they asked the same thing for VC dimension. So you have the same function, but you can fill out the stars with pluses and minus. You can fill out the stars with pluses and minus ones. You want to make the VC dimension as small as you can. And the conjecture that you can make it very small, it has to grow as D grows. So the best VC dimension you're going to get has to grow to infinity. If this is true, then it also would have provided a lower bound for what we have. But our lower bound is pretty sharp, right? It's D. So you're not going to get D here. You can actually bring down this dimension. Okay, but it's open. It's a nice problem. Nice problem. Yeah, it's very seems very doable, but then knowing we can't do it. Right, so one thing about the example was it was about sphere like continuous infinitely many points. So you might care about having some discrete version of it where the number of points is small. So we can look at the Humming gap Humming basically, but with large gap. So it's the same thing. So now our points are just 0, 1 to the n. 0, 1 to the n. Oh, this should be minus 1, minus 1, 1 to the d 0, 1. Sorry about that. And you care about the inner product, whether it's really larger than 0 or it's smaller than 0, right? So you have this. The usual range people look at is square root d, but here we look at the much larger range. And yeah, so actually, I think if the computations are correct, you get actually a very good. Are correct, you get actually a very good lower bound for this, that the sine rank is actually larger than d over log t squared. I have to really check this. I mean, this is these are very recent. It's just last night's question. I thought it was supposed to be square thing, but I didn't actually get. Yeah, yeah, so yeah, definitely polynomial in but maybe not correct right, but it's constant. But yeah. Alright, so a really good lower bound for this really concrete problem, hopefully. Really concrete problem, hopefully. It could be used for other stuff. And yeah, so it gives us a separation, right? Yeah, randomized computation complexity is constant, unbounded. Log of the center axis is logical. Which is sharp by Newman's lemma, if you care about dimensional measures. Okay, right, so some more open problems which I'm gonna get from Palm just mentioned. So, yeah, so exactly what this asked. So, the conjecture about hypercubes, right? Remember, we had this conjecture that this doesn't imply that the sine x is small. We proved it for partial functions. We don't know it for total functions. As I said, they are equivalent to this approximate gamma two months. So, instead of randomized communication complexity, can we get approximate gamma two months? And we believe this is not true. But what if, yeah, we ask this? But what if, yeah, we ask this question? What if you make it stronger, you just look at now exactly gamma 2 norm. And gamma 2 norm is quite nice. I mean, if you don't know it, maybe the more communication complexity version of it, which is the nuclear norm, is basically you write your matrix as a linear combination of rectangles, monochromatic rectangles, and you care about the sum of the coefficients. What is the smallest sum of coefficients that you can get? The smallest sum of coefficients that you can get, like just L1 of L1. So if you have good randomized protocols, you get this linear combination of rectangles which approximate three functions. But yeah, so here we want exactly. Every entry has to be exactly. So that's really just gamma-to-norm. So it's like an analytic version of rank, really. It's just, instead of the sparsity, we care about the sum of the coefficients. Okay, so what if, what about this? So we had this conjecture from an earlier world. So we had this conjecture from an earlier work with Liana and Puyat. And that if this is small, if the gamma 2 norm is small, then the deterministic communication complexity with access to equality oracle is small. So equality basically corresponds to identity matrix. And identity matrix has constant gamma 2 numbers. One gamma 2 number. So you have to kind of take that into account. And this says that that's really the And this says that that's really the reason, the only reason that you can not be of low rank. You just have to make this quality queries. So this is like a communication complexity characterization. But it's quite a deep conjecture. So let me tell you why. So first of all, before going to motivate this conjecture with one, let me tell you what one consequence about sign rank, because we're talking about sign rank, is, yeah, so in this paper, that's the Is yeah, so in this paper that the first part of the talk was about, we proved that SIRANC is bounded by four to this deterministic communication accuracy with quality oracle. So if your function has a good protocol with equality oracle, then its sign rank is small. This would have followed, this is semi-algebraic when you have this. So it would have followed if you knew semi-algebraic set the same a small sign rank, but we don't know that. So we have to prove this directly. But we found some tricks to get around that and actually directly prove that. Around that, and actually directly proof that. It's quite a nice and short proof, but I don't think it's super trivial. It's like a good exercise. Right. So if this conjecture is true, then yeah, that would be true. That small gamma two norm actually would imply small sine line. Unlike what we think that's true for atmospheric conjecture. But it relies on this conjecture. This conjecture is, I don't know, it's almost too good to be true. And the reason for it. Be true. And the reason for it is: okay, so we verified this using some d theorems for XOR functions, and it's actually even more general. It's functions that come from groups. XOR is really, you look at F2 to the N and you look at XOR of two points, right? So it's defined according to that group. It's a lifting function. But you can look at other abelian or maybe non-abelian groups. And the proof really shows that this is equal to. The proof really shows that this is equivalent to this theorem, which is Green and Sanders' quantitative version of this coin's identical theorem. So, this is quite a hard theorem. It was proved for some version of it, was proved by Coin in the 60s, was a big result in harmonic analysis, like he won a margin awards for that. And then it took 50 years for Green and Sanders to actually prove a quantitative version of it using modern tools from LT. From altitude rhetorics, so it's not an easy to get. And even it's generalized to non-abelian groups, locally compact non-abelian groups by Sanders. So we know this for those kind of functions too. But these are all hard theorems and important theorems. And if this conjecture is true, it really generalizes them to matrices. So basically, it says forget about this group structure that underlies. It's true for every matrix. But it almost sounds too good. Matrix. It almost sounds too good to be true, even though we know that it's true for such a rich and large class of matrices. So I hope it's true, but yeah, the proofs completely fail. All these kind of proofs really heavily rely on representation theory, Fourier analysis. You use the group structure in so many different places. But yeah, it's a really neat problem and it has generalizes such important theorems. It generalizes such important theorems if it's true, like in different areas. I think, can you gain any insights on the side to co-metrics of this comic discussion? Gain any insight from what they're used to prove special cases of this kind of thing. Oh, like if you go the other way, uh I mean, we don't have much partial progress, right? Like, yeah, so these things we really take these tools and apply it here. And yeah, I tried to generalize those proofs and I've been working on them quite a bit. A little bit very difficult. I don't know what Collins article here is. Can I interpret it as the conjecture where A is some muscle? A is what? Where A satisfies some group structure? No, oh here, right. So in this group setting, it means that So, in this group setting, it means that you're. So, the Fourier version of it is that if your function, if the sums of the Fourier coefficients, of the absolute value of the Fourier coefficient, is constant, then your function has to be a linear combination of subgroups, basically. Affine subgroups. Constantly meaning affine subgroups. For affine subgroups, this thing is constant. And this says that's the only reason we get a constant. We just put them together. And that's true for non-abelian groups. Yeah, like non-abelian groups as well. It's like the group of standards. But here, instead of these subgroups, you get these equality queries, which correspond to something which is called blocky matrices. It's really like a blow-up of the identity matrix. So you want to say you are a linear combination of those. That's helpful. Yeah, and I think that's yeah, thank you so much. I wonder if you take this matrix, this product matrix that you have a load with D lower bound for sign random take random restrictions. Do you know anything about what happens if you take random restrictions of say poly T coordinates? What to be signed up to this metric? Like the partial function, you mean? Right? So you randomly restrict some of the coordinates. How that depends how many. I guess if you don't restrict too many, then your margin is still going to be like it's going to be just the same version, a version of the same function with different epsilon, which is fine, right? With lower runs, kind of just depends on epsilon. It just depends on epsilon. You can adjust epsilon. Oh, I see. You just you don't restrict the coordinates, you restrict the points? Yeah, you restrict the columns and rows. Yeah. Yeah, good question. I think it will drop, but I haven't yet. I doubt yet. I don't know anyway to improve it yet. Anyway, to improve TM. These are really new, so I'm yeah, but yeah, I was thinking maybe you can derandomize it in that way and get better bound. But might not be possible also because of this relation between psi rank and randomize, right? The Newman lemma, right? If your matrix has constant randomized, public coin randomized communication public C, then its psi rank is at most rank is at most log n, right? Log log n for the unbounded combination. Or log n for some concept, polylogy. So I don't maybe you can push it a little bit, but I don't think you can get something very small. You're taking a random restriction of these, so you think it you get I think you can get you'll get small sign rank. Yeah, small like it drops quite fast. Yeah. Yeah. Yeah. Yeah. So this uh for this conjecture, so both directions are often no one direction is easy, right. Yeah. If the uh if this thing is constant then the gamma two part is constant. Because equalities are just constant. Yeah. So the hard direction is from left to right. So if I sort of uh change the D to the EQ by randomized then this fails? Randomize A and then I mean bounded area. Yeah. I mean bounded error. Yeah, yeah, bounded error. So then it's true because no, wait. Yeah, if gamma 2 is small, then approximate gamma 2 is small. Oh, the other direction now. In one direction of medium. Yeah, in another direction of string. Yeah, it's fair. It's a great question, actually. So hypercube is an example. Hypercube has a large gamma 2 norm, but yeah. The adjacency matrix of hypercube has large gamma 2 norm, but small. No, no, but I'm talking about randomized community. So randomized is equivalent to approximate gamma two norm, right? So those two are that. Oh, but here is in order one term. Yeah, yeah, exactly. What about you know, if you had a a communication function with exact quantum communication straight entitlement? And straighten time limit, which was constant, but for which you can do with a deterministic algorithm with equality works without this group is conducted? I don't know. I'm not very good with quantum stuff. I think the exact gamma genome should be allowed back on exact quantum communication with shared time, I I believe. Okay, I'll look into it, but yeah, can answer the question. Yeah, can answer the question. But I I think that we don't know if exact quantum is different than deterministic now. Total functions. What was the question? I mean, I mean it in the O one case, I don't I don't think we know that. I mean I don't think we know that I mean I mean I think you you would surely have separations in just for say a lifted query function if you if you don't care about the O ones that in a broader setting they should not be equal. Can I ask a question about the Orsakolan thing, the proof that he used with the That proof that he used with them. So, the very next problem he described, the Hamming distance file, doesn't it imply some lower one or the biggest problem already? Oh, for the total function. Like from the Hamming distance, you want to go to the partial function? Yeah. Right. I mean, you're not going to get... So for hypercube. I mean, it's a sub-matrix of that, so whatever lower bound you get, you're gonna also get a lower bound. Okay, and does this proof use the version also? Which proof? The analysis of the timing maybe or not? Having distance. The having distance is a conjecture, right? Yesterday. No, sorry, I see a recognition. Oh, discrete version. No, yeah, that's a reduction. So you take the previous one and you discretize it. Yeah. So if you can tell the hand, yeah. So it's not. Okay, fine. It was just marketing values. So like somewhere down there it loses because yeah, it's already large. Yeah, it's not a dire it's not a good do you have any other potential candidates for constant marketing that are not constant marketing? Constant margin, but not a constant standard? No, yeah, that's another excellent question. We really don't know many examples of functions with constant margin or constant randomized public one. I think it's really a good problem. Just come up with something different, right? The only thing we know is like equality, like constant deterministic, and also this having distance which is constant. Like not necessarily exactly one you can do two. Not necessarily exactly one, you can do two, three, whatever. But really, I don't know any other example which is fundamentally different from these. I mean, you can combine this with other examples by allowing Oracle queries to this. But yeah, like something that's completely different would be interesting. That's another great question. Is it qualitatively different if instead of having two sets of vectors we have one set and treat it as a graph? I don't think so. I don't think so. It's not like growth and make them call it. Yeah, I think you can embed this in the collapse. But then at least you should get some lower bounds from... Like you can't embed a large click, negative click, into a small motion. I think that's a precise dimension, I think. No, what large click is the identity, large net. Identity, mate. True to say, large negative. So you cannot have a huge set of all negatively query set. So yeah, yeah, we can move it up. That's not math, actually. Not the biper data. Yeah, but the click thing. So the click you're taking up, so you, if you're at the same number. So you, if you're the same node, you put minus one, say plus one, if you're different nodes, you put minus one. Isn't it just the identity matrix with ones here and minus one on the other entries? That has three. But okay, so then it means that I don't think you can embed it as a graph. I don't, yeah, so then it means that the big part of the game. Oh, I see. Now I see what you mean. Like you're embedding this. Now I see what you mean. Like you're embedding this pi inside of the same. Yeah, yeah, I doubt. Because there, you wouldn't need to mention that. Yeah. Yeah, yeah, yeah. Okay, so now I understand. But yeah, I haven't thought about it, but I doubt. Or questions? So, Home, if you don't believe this conjecture, I guess you don't have counterexamples like that. For what? For this XOR. For the projection, I guess. Yeah. No. I mean. No, I mean I really want it to be true. It's just uh I think it's too big. It's a very big theorem which is true. It's surprising that it hasn't been discovered. Do we know of any statements that are true for X-ray functions, but not for more general statements? But just trying to look for candidate country samples. Right, so one thing is this, back to that question about just Hamming cube. Like we have very few examples of Boolean functions where approximate gamma2 norm is formed. So now if you go to actual gamma two norm, the situation is no more abstract. So it's really, yeah, that's good to actually come up with examples for this things. Maybe that's the way to describe them. So you can take this group step and So you can get take these groups up and get examples from those, but yeah, then it's kind of any gases because the temperature is going to be yeah, yeah, it's just I'm saying even like forget about group structure, just so we don't have many examples of functions with small approximate gamma tunnel. It's always equality that you give as an example. Okay, so let's come to the end. So you said that confusion had two major other problems. And you said that for one of them you can't resolve techniques. And once we're at the start, so we have this type of cube. So we're using something. Yeah, how are you going right now? Um well right that's a good question. I don't have much time. So the thing is there's this conjecture of trees archad, shape is the third author? Yeah. Yeah. Whereas the conjecture that every maker is a small computation. So, if that's true, yeah, that's also actually. So, it's very related to this last one, actually. Like, I mean, the first thing you want to do is you want to at least find some last one. No, we barely. We only went for an hour. Yeah, so like a few days away, no data sheet problems. It's weird. Or any exact new ways. I mean it's it's small, but like it's actually cool. Like I realize that it's two tiny little small things. I don't know how many of us are explained and dinner uh ends. I try to run to the gym and finish before. Yeah, I'm gonna have to make sure that should be a must be a little bit more. And this comes from basically for a few idea is that you take a base class of graphs and then you say And then we say I want to have you do a JSON CS product and then sort of basically start with K2 or whatever, and you make a problem. But if you start with some other classes, I don't know. It could really be anything if you've got a constant cost of communication protocol for any crafts and when you take the current you can't. Yes, you can have a constant constant. Oh, I see. And in fact, you get one at the start. And in fact you get one nice start if you can do K and K and base K. Well, the first step of the protocol is like no is to check that you have basically like a touching down. But then the second step is a little bit trickier because yeah exactly. So what you do is what you can do is You can write down what your message would be. You can write down what your message would be for this and then text them all together. So it's a little bit weird because you can't just like there's there's also if you want to do k distance as well you can do You could do random partitioning for that XOR things inside the spin. Now it seems like maybe it could be reduced to k distance because the random partitioning is particle. But it's not an oracle call to it. It's using it for a similar thing, but it's not an oracle call to it. Similarly, it's not like taking the XOR of everything, then how do you simulate the Oracle on this last message? Last message. So I haven't tried too hard, but I had no ideas how to prove that you couldn't constantly. Because if you could, yeah, I that's why I was asking, because I don't know anything that's provably not reducible to K-Hamming distance. That would be really great. I mean, if everything was reducible to K-hamming distance. I doubt it, you know. I don't think it is. It's just, yeah, so that's great as an example. Uh 'cause yeah, exactly. And there are all these conjectures and it's not clear whether they're correct. You want to look for examples and it's just still fingerprinting. Yeah, like in most of the examples that I found like looking at graphics, it's like stage of producing better quality. It's kind of not that exciting in that sense. So like something that reduces stage. Or just kind of market an article, yes, for the data. Yeah, something that's implementing how. Implementing how this partition product, I think it's the only candidate that I am aware of that you might not be able to reduce all of it. Yeah, well that's what the dog does. How do you define fingerprinting? Sort of like you can do it with the simultaneous protocol. Oh, really? I think I know what you mean. You mean that the players agree on something and then they just each send their bits and they know the answer. Camera last year interfaces? Yeah, that's one data. But those are looking at only one person just as well. That can't be true actually. So you're saying you're basically saying that two rounds are one round with that. Are you saying that anything you can do with random as quickly as you can do with like everything I know how to do randomly is either