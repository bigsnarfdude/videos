Thank you, Rene. Yeah, so I'm going to be talking about equivariant polynomials for a sparse vector recovery problem. And this is some joint work with my advisor, Soledad, and also the excellent Josue. And also Nick Marshall at Oregon State and Angie Lee at St. Thomas Aquinas. And it's also, so the work is on archive. So if you want to take a look at the juicy details, feel free. Okay, so what's our problem? Let's suppose we have a vector V naught in Rn that is a sparse vector with unit length. And then suppose I also have another d minus one noise vectors sampled from some Gaussian with some covariance. Some Gaussian with some covariance. And so I want to think about the span of those vectors. And then what I'm given is any orthonormal basis of that span. And that's going to be my matrix S. So that's an N by D matrix. And then the problem is, given just this orthonormal basis, can I recover the sparse vector, the single sparse vector? Single sparse. So, this is like a subroutine in some different sparse data problems, but then people also study it by itself. So, yeah. Right, right. So, yeah, V0 to all plus all the noise, noisy guys. And yeah, that's going to be our space. Yeah, that's gonna be our space. Yeah, so I'll show an example too. So just sparse vector. So let's say we're in R3. The sparse vectors, the one sparse vectors have one entry that's non-zero, and then the rest are all zero. So those would just be the axes. Okay, we knew what some sparse vectors are. And then this is maybe an example of the. An example of the subspace that we're going to get. So we're going to get an orthonormal basis of some subspace. So this is a two-dimensional subspace. And so we get these like blue vectors here. And we want to recover maybe a sparse vector that's hiding in here. So in this case, that would be one of these vectors along the y-direction. And we can also slightly relax the definition of sparseness. So, this is like the strict, you know, one entry, and the rest are zeros. But we can also have this L4 condition of sparseness, where we have some epsilon, and then for this L4 norm greater than one over epsilon n, our sparse vector can live anywhere as long as it satisfies that condition. Satisfies that condition. So, for a particular value of epsilon, the regions where the sparse vectors could live in this example would be kind of these green caps here. So, this guy here is a sparse vector that would satisfy this relaxation. And this is like, okay, maybe we have noise, or maybe the surface is not totally aligned with the axes. This is a nice relaxation that we want to use. That we want to use. Yeah, I think that the one will make that. Yeah, so the it's a unit length. Sparse vector. And so the people you said it before, I just put the condition. Yes, yeah, sorry, yeah. Yeah, so depending on what the value of epsilon is, it could look something like this at these like little types. Yeah. Uh I think that sounds like what we're gonna do yeah there we're what we're gonna do is well what some of the other methods do is they're they're trying to like maximize uh like the L4 of Of the yeah, of the vectors that they're trying to find. Yeah, so that's actually what some of the methods do. So yeah. No, go ahead. So yeah, we're like, we're a couple steps removed from the applications here. So this is like in the dictionary learning problem. A lot of times they're like going to be doing this kind of thing repeatedly. They're like going to be doing this kind of thing repeatedly to find sparse rows and to get like a yeah, to solve the dictionary problem. Yeah. Well, I guess. No one knows how to filter. Yeah, so there's we'll see there's some results which have, you know, in certain sets. Have, you know, in certain settings with certain assumptions, they can get good stuff. So, what we're trying to do is trying to expand that. And yeah, you'll see. Okay. Yeah, so what some people do previously is kind of this sum of squares strategy, which is just a general technique for optimizing a polynomial subject. A polynomial subject to some polynomial constraints. And then there's also some spectral methods that they use. So if I have some polynomial and if it's homogenous and it has even degree, then there's a matrix representation of that. So this is my silly little example for a degree two polynomial. And then the problem becomes a spectral problem. So spectral problem so finding the max eigenvalue and the eigenvector associated with that is going to give you it's like one way to solve this problem to in the yeah in this just this general setting of we're trying to maximize some polynomial and so okay so how do they use that for this particular problem uh oh yes and also Oh, yes. And also, it kind of depends, right, how you're going to set up A. So for this problem, and that's your matrix or your polynomial. So for this problem, what we do is, okay, we have our input matrix S. We're going to call the rows of S AI. We're going to have some function H, which uses those rows to construct. Of those rows to construct a d by D symmetric matrix, we are going to get the top eigenvector of that, and then we're going to output that as our estimate of our sparse. And so this is kind of the general strategy. And some of the previous papers, they construct a specific H and then they prove that this is going to solve the problem. That this is going to solve the problem. So, really, this is the general framework that we're going to look at. And the whole game is what's H? How do you actually construct that? And is it actually going to do what we want here? Okay. So, this is some previous papers. So, they, so this is, these are column vectors. So, the AI, AI transpose is going to be, it's the outer. Going to be the outer product. And then it's going to be scaled by some scalar. And that's going to be the matrix A that they is a question or? No. Okay. Yeah. Yeah, I yeah, a lot of the papers, that's what they look at. Yeah. Look at the yeah. Presumably, your result will be above the capital. I hope so. Yeah. Yeah, and then there's this is another strategy from another later paper. And so they have, this is like kind of the theorems they come up with. There's a lot of details here, but what I'm trying to get across here. But what I'm trying to get across here is that they've constructed these matrices in a certain way. And then they do a lot of work to come up with the theory. And then, you know, for certain assumptions like on the sparsity, like the blue quantities, based on the noise vectors, what they look like. And with conditions on how the ambient dimension relates to the smaller dimension, they get some nice statistical results. Nice statistical results. Okay, but I don't want to work that hard, right? So, what I'm going to do is, and also I want to know: okay, how do these things operate when I don't have these assumptions? Can I get results when I don't have these? And I need to find the parse vector in that vector. So I am just very confused about all of these formulations that look like a different problem. Why don't we tackle the problem again? This is what I said. S times I would minimize the zero normal S times. S times X is important. Subject to X having the difference. To have a digital or five yeah so they yeah so they write the problem as S X X like this, and then that's where they get the sum of squares. So I think Yeah, it could be like a like a speed thing that they want to be really fast. I don't know. I don't know. It's uh yes yes. Anybody Yeah, this is going to predict the final place that's what I that's exactly what I'm trying to get. Trying to get it in. This is the problem, and I have a hamlet, which is a simple sticker. I just wanted to say the authors that are cited there, I don't know these papers well, but I know the authors, and the authors all focus on statistical and computational benefits. A lot of what they're trying to do is find the optimal dependence of the part for the figure that would be the only really and so there's a chance that doing something like DPCP might not. Doing something like DPCP might not result in the RC level, which is why they're everybody speculating with that. Typically, the way these authors have results are like this is the lower bound for the statistical limit, this is the lower bound for the computational limit, and we have the result that fits the exact computational. So that might be why there's a Yeah, I'm not sure beyond. I'm not sure beyond. I have a different hammer that I'm going to use. Maybe that's also the wrong hammer. But yeah, okay. So, okay, so what's the game here? So can we remove some of these assumptions and instead try and learn the best H from the data? So this kind of follows some of the. Kind of follows some of the work on like neural algorithmic reasoning, where we have algorithms, we have neural networks and machine learning methods, and we combine them to get the best of each. And so in this case, our algorithm is kind of laid out like this. We're just going to replace the part that maybe we don't know for the certain setting that we're looking at, which is the Looking at, which is this. It's the calculating the H. And we're going to replace that part with a machine learning method and see how it works. Yes. Okay. So there's another part of this problem that is going to be important, which is basically, right, we're given these, the basis, we're given these blue vectors, and I said we. These blue vectors, and I said we could have any orthonormal basis for this space. So I could get these blue vectors, I could get these blue vectors, and in both cases, I want to get the same sparse vector out, right? So the idea is that the sparse vector is going to be invariant to the choice of the orthonormal basis S. Or to put it another way, if I have S and I have some. have S and I have some orthogonal matrix Q, then S times Q for S tilde is another valid input for that problem. And I should get the same sparse vector. So in our paper, we have the little proposition that says if, okay, I have S, I have my rows, I have my H function. If my H is OD equivariant. H is OD equivariant, then my estimate is going to be invariant, OD invariant to those rotations that we just showed. And so just to review equivariance, although I know we had Lachlan's talk earlier, the definition is probably the same, hopefully. Okay, so if I have an F from a space X to Y, and I have a group G that has an. And I have a group G that has an action on X and Y, then if F is equivariant to G, if for all G and G and all inputs X, they commute. So I can apply the group action and then the function, or I can apply the function and then the group action. So in this case, what that's going to look like is I can apply the, so if I have some orthogonal matrix Q, I can apply it to the inputs. To the inputs, the A1s, and that's equal to doing the function and then applying it to the output, which is a matrix. And the action there is by conjugation. So Q, H, QT. So that's what I need my H to satisfy. And so in the paper, we have kind of, this is like kind of the main theoretical result in the paper, which basically Result in the paper, which basically says I can characterize the functions where the inputs are, if it's an OD equivariant polynomial, my inputs are tensors, my outputs are also a tensor, then I can characterize this polynomial. And I haven't gone over all this notation because I just want to focus on the corollary for our case, which our inputs are going to be vectors. Our inputs are going to be vectors, our output is going to be a two-tensor. And if that's the case, we have this pretty nice form where, okay, so my inputs are the A1 through AN. That's going to be the sum of the outer product of some of the input vectors times a scalar function that only depends on all the pairwise inner products of the vectors. plus the identity times another scalar function that again only depends on the pairwise inner products of the vectors. So this is going to be useful for us. Okay. So the methods we're going to compare are some of the methods I showed before from the two papers. Then there's going to be a baseline method where it's just a Where it's just your baseline MLP. We take all the components of the input vectors, treat them all as one big input to an MLP, send it through, and the output is the components of our symmetric matrix. Then we have our method based on the corollary, where it's going to look pretty much the same. Going to look pretty much the same. We have the basis of the outer products of the vectors. In this case, we need the output to be symmetric. So we only have like a symmetric basis there. And then we have these scalar functions. And these are useful for us because if we get things down to scalars, we don't need to worry about the equivariance anymore. We can just put it through your. We can just put it through your favorite neural network. So, these blue parts are going to be the learned functions in our model. And then we have kind of a model that is halfway between this one and some of the other ones. So it only has, instead of having the pairs as the basis, it only has just each vector outer product with itself. Each vector outer product with itself so that it kind of looks like some of these methods. And then the input is only going to be an input of the norms of all the input vectors. Okay. So those are our methods. And then we need to talk about some of the what the data is going to look like. So for the noise, the noisy vectors, we have to pick a covariance matrix. Covariance matrix. So that could be the identity matrix, which is kind of the setting for the theorems of the previous papers. It could be a diagonal matrix that's not the identity matrix, or it could be any random covariance matrix. And then to generate the sparse vectors, we have a couple different ways of doing that. One is just an accept-reject method. We generate a vector, we throw it out if it doesn't satisfy. We throw it out if it doesn't satisfy the sparsity condition. If it does satisfy it, then we keep it. Then we have Bernoulli-Rademacher vectors where the entries are zero with a certain probability, or they're plus or minus the scaled value. And so we should note that this doesn't satisfy the, if we generate sparse vectors like this, they don't always satisfy the L force. Satisfy the L4 sparsity condition, but they do in expectation. And then we have two more distributions. So this one is they're zero with some probability, or we sample them from a Gaussian. And for this one, the expectation of the L4 sparsity condition is 3 over epsilon n. So it's much higher, it's more sparse. Higher, it's more sparse. So then to get a distribution which looks like that, but is equal to that, we have this kind of messy guy where with some probability, the entries are from a Gaussian with a small variance, or with the probability epsilon, the entries have a larger variance. Okay. Okay. And so we get some results. So we'll go kind of one at a time here. In the setting where the covariance matrix is the identity, which is the setting of some of the theorems of the previous papers, they do pretty well, right? So they beat our learned methods in those settings for these guys. But when the covariance matrix But when the covariance matrix is something else, like in the diagonal case, our diagonal model generally does best, except for the Bernoulli-Gaussian guy. And then in the case where it's just any covariance matrix, in general, our method does the best. The kind of the exception is this guy where Is this guy where the old methods seem to do better always? And I think that's because the vectors are a little more sparse, so it's the easier setting. That's kind of what we're hypothesizing there. So, to conclude, the learn models succeed in some settings where we don't have good theory yet. Have good theory yet. And it also seems like the equivariance is pretty necessary for the learned models because the baseline guy doesn't do well at all. And then I think there's a bunch of future directions. Part of it is: can we take these learned models where they do better and then go back and figure out what the proper What is the proper solution, and like actually generate theory for it? We haven't done that yet. There's also some permutation symmetries that we're not incorporating because the input vectors, you should actually be able to change the order. And we don't do that. And then there's also kind of a setting where can we learn models for where the input vectors are. Whether the input vectors are length d, and then test on some other dimension, like a d', because the input is only inner products or norms of the vectors. So maybe it shouldn't matter what the dimensions. So that is it. Thank you, everyone. Any other questions? Yeah. So the claim you're making that based on the empirical evidence, equivariance is necessary in the learned models. It seems like we can actually make a stronger claim. Yeah, I think I wouldn't say I'm not super confident that we, you know, we've found the best baseline model. You know, it could be we just didn't do it right. So I. We just didn't do it right. So I wouldn't. They are, yeah. Which is like one of the first things that we notice is that, you know, they are equivariant. Yeah. Yeah, it seems like it's improved, at least, but your method is sensitive to exactly which basis you have. Yeah, so we do have this proposition, which is that the equivariance is sufficient, but we don't have the other direction that it's necessary. But I think it should be. Yeah. All right, so I'm going to take a few extra minutes to conclude the day. 