So George and I are going to give an introduction to informatics and it might not seem relevant to what we're doing since this morning, but it should by the end of the week, hopefully. Okay, so very high level, we're going to focus on electronic health record data, but before that, this is kind of like a summary of all of the types of data that people in uniforms care about. People in uniforms care about. So there's obviously the electronic health record, there's genomics, there's the environment that's a little cloud out there, there is wearable more and more and patient generated data. And then there's also social determinants of health, which we know are making a difference in the care of patients. And this, I think it's supposed to be an insurance card, I'm not sure. But that's why I found it on the non-project. The non-project. But there's two types of social determinants of health. There's some that are community level, so where you live has an impact on your health. And then there's individual social determinants of health. So your behavior outside of the clinic, do you have housing? Do you have food insecurities? These kind of things. And then there's other types of data that is not directly relevant to patients. Directly relevant to patients individually, but is still very useful in informatics. It's all of the literature that is being published. It's a lot of knowledge. It's in a very unstructured form, and so there's a lot of work on trying to structure this and keep track of knowledge. And I put in there also health news, which is kind of like an interesting public health question: of like how is research being translated to individuals and how do we Individuals, and how do we as scientists ensure that the data we provide and the research findings we have gets actually trickled down in the right way to individuals? So, I'm going to focus on EHR data. I had a timer and I forgot to start it. Okay. So, you can't see the arrows, but there's basically arrows going in many places. So, as a snapshot here, you have a patient and You have a patient, and maybe the patient is in the hospital in an inpatient setting. Maybe the patient is just coming to the emergency department or is going to their primary care provider or specialist. And between what's going on with the patient and what's stored in the electronic health record is a clinician or a team of clinicians. And it's important to think of it this way because what's in the AHR will talk about it much more tomorrow. We'll talk about it much more tomorrow, is really a reflection of the care of the patient rather than a reflection of what's going on with the patient directly. So there are different things that are being recorded, which lab tests are being ordered and their results, vital signs, diagnostics, clinical notes, and then there's also a lot of back and forth between the patient and the provider. And those are, you could think of it as all the interventions that are happening. So the EE chart contains all of this data, and if you think about it through time, this would be, for example, one patient's timeline. And depending on the type of interaction they have with the healthcare system, there could be, you know, regular visits, or they could be like two and a half years without any visit to your hospital. Different types of things get recorded. So that's one type of data that we care a lot in informatics. The other type of About in informatics, the other type of data we care a lot about are knowledge bases. So I'm going to mention two of them, two types of them. One is terminologies and the other is our ontologies. And they're very important to us because they kind of provide the glue to putting all this data together, maybe across different types of practices or across societies or across even different data types. Across even different data types, so say the literature and patient fragments. So, this is an example of what we would call a terminology. This is actually an old terminology, the ICD is an international classification of disease. It's a big hierarchy. At the time, there was about 16,000 different types of conditions that could be built into for a patient, and now it's more like 70,000 in ICD-10. But, nevertheless, it's But nevertheless, it's pretty much the same idea. It's a big ESA type of relation. And so you go from now in ICD-10 a chapter, so it would be neoplasms, and then you could kind of like navigate down up to very specific types of disease. What's interesting to know about these terminologies is that they're built by humans. So whatever is there is a reflection of how humans are thinking about. Of how humans are thinking about representing diseases. And there's a lot of really interesting work looking at the fact that not everything is documented at the same level of granularity. Some diseases might have only one way of being diagnosed and others would be extremely, extremely specific. Nevertheless, I think what's nice about this is that we can go and take all the different diagnoses that are recorded in the patient record and maybe we can abstract to different levels. To different levels, let's say, of ICD codes. The other type of example I want to give you is SNOMED City, which is an ontology that's used a lot with electronic health record data. And these diagrams are taken from SNOMED CT's website. And the way you can think of it is that there are concepts. I'll give you an example of what a concept is. There are descriptions about these concepts and there are relationships, relations between the concepts. Relations between the concepts. So, this could be a concept. It's a unique concept that is supposed to put together all of the different synonyms and descriptions of, in this case, a hard attack. And there are some more descriptions here that are not very useful, but not very useful when you think about modeling knowledge. But the idea here again is that The idea here again is that if we see someone talking about a myocardial infraction and that's how it's coded in one EHR and it's coded as cardiac infarction in a nose EHR, now we can harmonize these two things and have a single concept ID to operate over. And then finally, instantities are relationships and those are extremely useful because maybe they can help us reason over knowledge. And so there are things like Things like Isa, but also there's many, many different types of things like a medication treats a disease or things like that. And so, again, you can imagine that these ontologies are extremely dependent on what we know at the current time at which the ontology is being maintained. And so there's quite some a lot of work in informatics that is about updating these ontologies, making them more useful, user-friendly, etc. Useful, user-friendly, etc. SNOMED CT, we use it a lot. While ICD looks at diagnosis only, SNOMED is more ambitious and so there's all sorts of things. There's even some of these social determinants of health that I was telling you about that are put into this different structure of concepts and relations among concepts. Okay, so I'm going to let George talk now that phenotype. George doesn't know that he's identified and then I'll be back. All right, so the word so tomorrow I'm going to be talking what's wrong with the data in the electronic health record because that's the challenges day. So I'm going to go into more detail, but just for now take it that the data are noisy and difficult to interpret. And so this is kind of what we do about this. First, the word phenotype. Phenotype is stolen. Phenotype. Phenotype is stolen by informatics from biology. In biology, there's a genotype, what's hidden in the DNA. The phenotype is the thing we can see, blue or brown eyes. In informatics, it started off as that it's the phenotype is the thing that's observable on the patient, except it's become a latent variable. So it's the concept that's true about the patient. Do they have diabetes? As opposed to what we observe, which is what people say about the patient, which is not necessarily true. So we've added this extra step where these raw observations we need These raw observations, we need to go to something else which is better, which is what's true about the patient. Not genetically true, but phenotypically true. So it's like it's twisted it around because now what was previously observable in biology has become unobservable in informatics, nevertheless. Now, the data electronic rot kit, I'll tell you what's all wrong with it tomorrow, but what's good about it is it has all the information we need. Clinicians use the record for care and they've used it successfully. And they've used it successfully, but what they have is an inference machine in every doctor with a lot of training, and they can interpret it. So, when we want to use these data, we have to do the same kind of interpretation. So, we need to kind of deconvolve the truth the same way you would do a CT scan by taking many x-rays and calculating what really happened. So, that's what people do when they read the record. They infer what happened. So, we've developed this thing that we call phenotyping, which means going from the raw data through From the raw data through some mechanism to the phenotype, which is a concept that's true about the patient. Do they have a disease? And then you do your experiment on this thing, on the phenotype instead of on the raw data. So does the patient have a disease, recent rash, drug-induced liver injury? I'll come back to that example in a second. So if you want to know if someone has type 2 diabetes, you just look it up, right? Do they have diabetes in their record? Well, that doesn't work because they're often missing the thing. Because they're often missing the thing or accidentally have it because of a miscoding. If they have type 2, there's some type 1 diabetes in their record. If they have type 1, they have type 2 in their record. So it's hard to tell. So we infer it based on evidence. So yeah, we do look at the diagnosis codes. What medications are they on? Do they have glucose? Do they have tests suggested that they have diabetes? Do they have the results of those tests suggested of diabetes? The complications. We read the notes with natural language processing. Do they have exclusions? Processing? Do they have exclusions? Drug-induced liver injury was a particularly tough one because it's a diagnosis of exclusion. Drug-induced liver injury means that the liver injury wasn't due to anything else we know of. So you have to go through the entire record and think of everything that can cause liver injury, say it doesn't, it's not one of those, therefore it might be the drug they were on. So that one took six months for our postdoc, Casey Overby, to generate that thing in such a way that we could use it at Columbia and a network at 10 medical centers around the country. How do we generate these phenotypes? Well, one thing we do, first of all, just to be honest, is we just use the diagnosis code. So I'd say most of the research published in the literature just says, do they have diabetes? Yes or no? Look up the diagnosis code, which is not very reliable, but that's what they do. The next step better than that is a rule-based method where you have a knowledge engineer, say an informatician, and a domain expert or analyst, domain expert like a doctor, a nurse, et cetera, iterate on making up a rule. Iterate on making up a rule. So we would look at all these criteria from the previous slide and literally type out some algorithm based on that mechanically using logical statements. And then we run the thing on a test set, we see what it says, and then we manually review the results and say, here's the sensitivity and specificity, maybe the positive predictive value. And we calculate an R or C area. Another thing that's now that's. Thing that's now that's so there's a couple articles in the literature about that, but mostly we've moved beyond because that's like too simple. So, if you want to publish a paper on phenotyping, you can't do that. So, then you have to do one of these things. So, used to be supervised machine learning where you just learn it. You know, you could use logistic regression, you can get fancier, you know, random farce, whatever you want to do. The problem is you need to create a training set where you know the answer for a lot of people. We don't know the answer, that's the whole problem in the first place. You have to get doctors, maybe a set of them, to sit down with charts, say, yes. Of them to sit down with charts, say yes, no, yes, no, yes, no, on each of these, and then you put them into the supervised machine learning. Adam Wilcox showed that's actually probably less efficient to actually have someone sitting there saying yes, no than to just create the rule in the first place. Another way to do it is unsupervised machine learning where you have a cluster the things, then you say what this cluster. So you hope that it comes up with the right one, with it with reasonable ones, and then you assign those to clusters, and you use those for your phenotypes. And then there's various forms. Phenotypes. And then there's various forms of assistance. So silver standard means I don't have a gold standard, but I have something else that's a little bit orthogonal to the EHR, train it on that, then I'd use manual curation to look at the rules that it generated automatically and come up with a better one. Or active learning when it tells me which cases are the most useful to manually review. And I exploit ontologies like Noemi showed. But I'll just say, if you want to say, like, what hypertension drug should I take? What diabetes drug should I take? Is COVID bad? These drugs should I take? Is COVID bad? Is the vaccine safe? Most of them do the thing I didn't even show on the slide, and the rest of them do this, and none of them use any of this. So, this is where all the literature is on how to do phenotyping, but I don't know a single useful study that has done any of these where that's been trusted. So, that's where our research is, but not our usefulness. George, this is the good news day? Well, no, the good news is that, of course, you should see the bad news day. Come back tomorrow. So, Odyssey, observational health data sciences and informatics. Observational Health Data Sciences and Informatics, this International Collaboration Columbus Coordinating Center. So, this is a pipeline for the rule-based one to speed it up. Let's make one that people are actually using a little bit better. And so, there are several steps, and I'll just show you two, for example. Phoebe is this thing which uses machine learning not to generate the rule, but to help the rule writer. And so, you make your rule, then it goes through the rule and says, here's all the things I thought you missed. And it uses lexical and semantic techniques on ontologies to say, you know, you say you're doing. To say, you know, you say you're doing diabetes, but there are these codes over there in a different part of ICD-9 that you forgot about that I can tell you the lexically or semantically through relationships, you may want to include these. So that's like the kind of thing we're doing where we do use machine learning, but not directly in the rule. And the other thing is to help us, so you don't have to manually review all those cases to see whether your algorithm, whether it's machine learning or rule-based, we're using machine learning to estimate the sensitivity and specificity you would have. The sensitivity and specificity you would have gotten if you did it manually. And what it does is it creates like a hyper, instead of a silver standard, instead of gold, it goes to a platinum standard. That's not, it's only a few cases, but it's pretty sure it got those right. And it estimates it from that and does some corrections to say what your sensitivity and specificity might do. So that's kind of the state of the art in improving the. That's not talking about machine doing human learning. Yeah, yeah, we're trying to, you know, now we're going the other way. So I was, but you know. Of the way. But basically, research is going on. We are publishing papers, so it's not all bad news, but that's kind of a state of the art in phenotyping. The phenotypes are interesting because, as Bob said, they're kind of our abstraction of those patients. It's kind of like what you would think is in the EHR in the first place, which is, does a patient have diabetes, yes, no? And maybe at what time the person. And maybe at what time the person got diagnosed. Okay, so I'm going to be talking about: if I wasn't in this workshop, I would have written modeling, but I felt like that was a tricky word. And so I said processing. But basically, we're not talking about so much like of the implementation that's for later, but that's the kind of questions that we'd like to be able to answer with electronic house record data. So With electronic health record data. So, what's going on with my patient? Like, do they have diabetes? What else do they have? Is my patient at a specific risk for an outcome? How do I manage the day-to-day of the patient? And for patients, how do I get to a shared decision-making where knowledge about the EHR is really important for patients and clinicians. And here I kind of put two examples of kind of how clinicians are. Kind of how clinicians would summarize patient data. And a lot of the work that we're doing is one creating a machine-readable version of this, and also using some of that in time to be able to do things like predictive tasks and other things. So we're going to be talking about four things. One is natural language processing, just because there's so many notes and doctor authored and married. Doctor authored and nurses authored, and social worker authored, and many types of providers documentation in the electional health record. And so there's a lot of very valuable information in there. I'm going to very roughly give you quick examples of the type of descriptive analytics that are done right now in the field on EHR data, predictions, and then prescriptions. Descriptions. So, natural language processing, that's kind of a classic task of NLP: here's a note and here's a big tree of ICD code and give me all the ICDs that can summarize the best what is going on with the patient for that particular note. So, say we have a patient who was in the hospital for a week, we want to know how to build them, it's teachers to do manually, and so maybe we want to get some automatic. And so, maybe we want to get some automated help. There's a very practical use of this, and there's a more researchy use of this, which is that we have a lot of structured data about the patient, but maybe extracting more structured data from the notes can give kind of like a complementary view of what's going on with the patient. Good question. I don't know if this is the case in the hospital when you're in a private practice. Private practice, the doctors often lie about the codes in order to get insurance reimbursement for the patient. Yes, and perfect, because I'm going to talk about that's one example of balance, an issue, which is because it started as a, you know, these diagnoses that are in the EHR used for care also have a completely different target used, which is billing. There's going to be weird mapping, in fact, between what happened. Mapping, in fact, between what happened with the patient and what is actually mapped. And that also means that there's going to be maybe differences across institutions about how these codes are being assigned. So that's yet another reason why we think that looking into the actual narrative, it's not ideal still, but there's going to be maybe a better option for finding what actually happened for the patient. The way a lot of informatics research is. A lot of informatics research in natural language processing goes is that there's it goes like kind of like sentence by sentence and so here it says patients should come back if severe facial rash occurs and we do what's called named entity recognition where we basically have a template so we know that somewhere we are looking for a mention of a disorder and there's a few pieces of information that we want to be able to fill out automatically from this sentence. So for example we So, for example, we want to recognize, we want to find that this is an entity, facial rush, that it's normalized into a terminology to this particular concept unique ID. And in this case, there's actually some modifiers that are being mentioned that could become important if we try to extract and reason this information. So, the fact that we're talking severe, the bigger one is that there's a big if here, and so we don't want to simply say that. So, we don't want to simply say that the patient indeed has a rash. We want to kind of keep track of the fact that it's a conditional thing. How is this useful? Yeah, that was a good question. Yeah. The data base you have so large that it would not be useful to look at them again to be able to put it into. If like put it into boxes, then that's why you use the code because that's kind of one core of it. So I think like historically there was this idea of like, and even now with vendor electronic house records, there's this idea that documentation could be entirely structured and it's all a matter of like entering, you know, clicking the boxes. It turns out that clinicians, first of all, don't like it, don't have the time to do it, they just don't want to do it. They're not thinking in terms of structured. They're not thinking in terms of structured data. And so the free text part of the EHR is quite present. And so, no matter what, there's always a whole lot of information in there. It's actually a very difficult task to do this name entity recognition because here is this is a very nicely behaved sentence. Most of the time, clinicians don't write that well. And so there would be all sorts of abbreviations that are ambiguous, for example, and we're trying to understand. Example, and we're trying to understand. So, CA is a very known and classical ambiguous advertisation, which could mean cancer, calcium, California, and, you know, or Canada. How can I forget? And in general, NLP, there's some principles that help you identify what are these synonyms, or not synonyms, but how to disambiguate these phrases. Disambiguate these phrases in clinical notes, you could very well have you know the patient's sister who lives in CA has breast CA. And so there's a lot of like interesting problems that are to be to be felt. So it is actually useful. And there's a lot of documentation. And you say exactly by hand, if I type. So it's typed. I should have said that. Yeah. So this is an example of system that just, you know, that's one of the uses. You know, that's one of the uses is that you can go and take indexing of all of the notes that are being written. This is, for example, part of one, actually two inpatients, and there's many more that are there. And we can go and index everything, and we can find all of the synonyms, etc. And so it gives us again some sort of like abstraction about what is going on with the patient. This is more for humans to look at. Humans to look at, but you could imagine doing the same for more machine input. So, a lot of informatics research in natural language processing means give me a text that is about a patient and or about some medical concept and structure it. Identify this concept of medications or diseases, etc. There's also a lot of work in natural language processing to Natural language processing to actually generate text automatically. So, this is an example of my student is generating automatically some brief hospital courses. And there's a lot of really interesting research questions there, but I just put it there just so that there's any NLP person in the room. I'm not, you know, there's much more than identity recognition out there. Okay, so I want to tell you about description. I want to tell you about descriptive. So, this is an example of George's work, actually, where we're putting into action this idea of looking at a whole lot of patients. This is part of Odyssey. There's a big, a lot of work that goes into building this visualization and it relies heavily on this harmonization and looking at ontologies to abstract from one patient to the next. Abstract from one patient to the next what's going on. And so this is a treatment pathway where it's a sambur. So those are like all of the different first-line medications that are being found in patient data. And then each spray is a sequence of medications that a patient or multiple patients would be taking through their timeline. It's very useful, it's very simple, there's no real modeling happening, but it's very useful. Modeling happening, but it's very useful to do these cohort characterizations because, again, it kind of shows a disconnect sometimes between what is known in medicine, what are the guidelines, and what's actually happening in practice. Another reason why we like using ontologies, this is a characterization of claims data this time, but because we're using similar ontologies to represent the data, we can very easily transfer this characterization. Very easily transfers this characterization from one site or electronic health record and claims data. And so here we're looking at time to diagnosis for a whole lot of phenotypes actually. So each row is a phenotype and time is time to diagnose between different groups. So an example of unsupervised learning, we use a lot of that because it's actually hard to get access to labels that we like and that we can trust. Can trust, and so instead we generate our own labels which we trust and are maybe not as good as anybody else's. But this is just one example of basically a mixed membership model that takes all of the structured data possible and all of the unstructured data, all of the nodes, and learns, you can think of it as like a very big dimension reduction step, learns a distribution of what we call phenotypes. Of what we call phenotypes. They're slightly different than the way George has described it. But what's nice is because we're probabilistic now, we can think of a patient as a distribution over all of those phenotypes. So now rather than saying, does a patient have diabetes, yes, no, we have what is the likelihood of this patient's documentation to be representing diabetes, but also all these other 150 or however many we want to have as phenotypes. And that's the kind of As phenotypes, and that's the kind of data that we can again look at, either fit it into another model or just have it used to generate cohorts of patients. That's another example of how you would use this type of data, where now you can kind of like look at probabilities at different time slices of what is being documented about the patient. About the patient. And the reason we can do this is because we're unsupervised again, and so we have a way of reducing the dimensionality. Like all of these different observations here are being summarized by this one type of variable. Go for it. Maybe I'm getting more time. So uh you basically uh do something for PCA or clustering pattern recognition. Or clustering or having recognition? And then you do actually label it somehow? So, in this case, I haven't told you the whole story. Normally, the RIX membership model or any clustering wouldn't label anything. We have another process on top of it to be able to label it. But we're you know we want something that's probabilistic because we want to be able to infer it for new patients and see how it how it works. But uh the labeling part is that human or also method? Like the naming part? Because you could just have one like a human look at a few examples of that. Yeah, so we, you know, as good researchers, we came up with a super fancy method and then in practice there was like some fourth-year medical student who was like, that's lupus. That's. But it makes sense. I mean, because the person doesn't have to have to go through all of Because the person doesn't have to have to go through all of them, I guess, but it just has to be a lot of fun. Exactly, you just learn at the clusters. Like it's not patient-specific. Absolutely. That's examples of unsupervised learning. So again, the way we think about it for the EHR is really dimension reduction so that we can do something else after present it in a fancy way to clinicians, but also like send it into some other model. This is a site that Rajesh knows. Rajesh knows where we're looking at predictive modeling. There's a ton that's kind of like the bread and butter of informatics and machine learning right now is predictive analytics. But here, I guess I'm talking about like interesting research questions. The big deal is that, so take a look at what's on the top. We have, because of the way EHR data is, we have people who just show up at some point in the Up at some point in a hospital and start having data be documented about them. And then at some point, they might have an outcome of interest, like they now have diabetes, or they might never have diabetes, or they might just leave and will never know if they had diabetes or not. And so there's a few interesting questions here, which is that the patients are not aligned in time. So, for example, in survival analysis in biostats, it's very well curated. It's very well curated. You have a time zero, you start collecting your data, and everyone has a new diagnosis, and you want to know who's going to go and progress to the next stage of the disease or something like that. And here we don't have that, so we need new methods. This is also interesting, is that the fact that patients generally don't have one disease, and it's in fact better to think about modeling all of the diseases at the same time. But that makes computer. Time, but that makes computation a little harder. And then the last type of modeling that we have is reinforcement learning that starts to be used a lot with electronic health record data. This is a very famous paper from 2017 that looks at ICU data and tries to learn. Basically, you can't see it at all, but whatever. There's like various times for interventions, and so we want to be able to learn what is the best sequence of. What is the best sequence of actions for this particular patient? And here we have a lot of physiological data and ventilator data access to, so it's more of an ICU setting. This is, you know, you could think of this idea of what's an optimal sequence of treatment at a very different time resolution. And so this is thinking about a patient through time, maybe now over years, and there's different prescriptions. Prescriptions of drugs for this patient, and you want to know again what is the best way to through maybe yours. Okay, so I think we're stopping there. That's kind of a hint towards some of the modeling questions for tomorrow. But thank you. Yeah, you can should be stoppable. Should be stoppable.