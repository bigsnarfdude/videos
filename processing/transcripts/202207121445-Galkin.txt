Yeah, it's the last talk of the day. I try to keep it a bit more entertaining. And then we can go to spend the evening somewhere. I'll describe what is this magic pot, what are we talking about here, and how we can mix Steve Raywan and Norwegian black metal. So generally, I'll be talking about this problem of inductive graph reasoning, what it is, and what will be the two approaches to such kind of inductive graph. Such kind of inductive reasoning. So, first, our setup is that we're talking here about relational graphs. It means that we have typed edges. Usually, they come from a certain vocabulary, particularly from knowledge graphs. And this is an example from Wikidata. And in Wikidata, we have 6,000 different relationship types. So, in our graphs are multi-graphs too. So, generally, our graphs are directed. We have explicit relation types. And the cruxes, input, non-pictures are not given. The cracks is input non-features are not given. So often we do not know anything. There is no even no textual description. So we cannot even file a language model, not even get some features. So the tasks here becomes geometric already from this basic definition. And what are the graph reasoning tasks? The standard is the simplest one, node classification. They just given the graph structure and some node, we want to predict some types. It can be also actually multi-layer. Some types it can be also actually a multi-label task because most of those knowledge graphs are actually allowing many types for one node. Then, uh, simple link prediction, which is uh oh, some fancy formatting problem problem here. Uh, when we have a node and some relation type, and we want to build a distribution over all nodes in a graph to predict actually to rank all other nodes, so it's not a classification, it's a ranking objective. And then the third, more the most interesting task. The most interesting task, real reasoning, is complex query answering when we have a first-to-do-logic query and we have some intermediate variables which we do not know, and you just have to match the pattern onto a graph, knowing that certain links might be missing. So for instance, this link might not be present in the given graph. We have to predict it on the platform. And those are three big graph reasoning tasks. Inductive reasoning tasks mean that at inference time, we might have new nodes, those orange ones, and we have to perform. And we have to perform all those tasks at inference time over unsympods. And that's the main question. Then, if any of our standard GNI models need some input features, like X, what will be transformed, but input node features are not given, then the main question is how do we get those inductive features? Like we don't give them text, like what shall we use as those features that will be expressive enough to perform classification, link prediction complex? We're answering. Conflicts we're answering, and so on. And for years, the literature world was only focusing on this transductive learning. Transductive means that whatever nodes we have at training time, we also perform reasoning at test time at the same set of nodes. And those algorithms date to more than 10 years ago already. So there were some basic approaches published at Isinal Nerves 10 years ago. After that, the literature accumulated hundreds embedding models. Actually, there are so many. Embedding models. Actually, there are so many that I stopped caring about them. And the literature is stale. That big state of the art was from Mila guys from like January 19 over here, this one. And as you see, then the progress is stale. There hasn't been any big improvement in this transductive learning so far. The problem is that, well, we have we face a sort. We face a certain bottleneck here, and there is a certain ceiling that we need better representation mechanisms. And so generally, they are all of them focused on this transductive supervised setup based on triples. And what is interesting, what we have right now with the appearance of geometric deep learning, is actually this whole bigger set of tasks, including not only transductive triple learning prediction, but we can also do it in inductive setups, we can do it in multiple setups. Inductive setups, we can do it in multiple setups and many different tasks, including theoretical understanding. Like, for instance, we do not know much about expressiveness of relational GNNs. There has been a little vast literature about expressiveness of normal GNNs, so spectral forms and message passing forms, but about relational genes. So we have relation types. And interestingly enough, there is maybe only one paper at ICLR last year, two years back, and that's it. And we know too much. And that's it, and we know too much. So it's still pretty much a blue lagoon. And another problem with those transductive approaches that those are shallow embeddings. It means that for each entity, which we have, we learn the embedding vector. So it's like a huge embedding table of vectors. And it's, of course, a representation learning challenge because when we consider inductive approaches, it means that at each person we have a new set of nodes for which Have a new set of nodes for which we do not learn anything. In transactive setup, the set of nodes is the same, so our camera is fixed. But in the inductive setup, we learn on some one set of nodes. At inference time, we might have a different set of nodes or extended. So a superset or a different set of nodes. And it means that we have to have some expressive enough features. And we cannot learn shallow netting matrices. And yeah, another interesting observation. Yeah, another interesting observation. Though those shallow models are super hungry, so this is an excerpt from OGB on the WikiKG2. This is a standard link prediction. This is a relatively small by, or I would say medium size, just about 2.5 million nodes. And this is the state, which was Bayern OGB one year ago, that those models are super hungry, like 1 billion parameters and at least 500 million. And for comparison, if you take BERT, BERT made a revolution with only 300 million parameters. With only 300 million parameters. And so there's, if Berg made a revolution at 300 million parameters, but for kgmedics, we learn billion size parameters. Where is the revolution? Given the revolution. So no revolution and super parameter hungry, super GPU hungry too. So that's what we, yeah, that's what we have every time when we try to learn some big model and fit it on a small GPU. Often it doesn't work. Yeah, so there are two featurization strategies that appeared recently, and I'm happy to be participating in both. So, first of them is, well, it invites us to travel back a bit in time to 2014 in the world of word to vect and glove, and when we represented all words as show vectors. And if you remember those days when you And if you remember those days, when you do some basic word analysis, you don't know the first thing you don't know that is like the pre-trained vocabulary of something like 3 million or 5 million words, 300 dimensional things, which were four or five gigabytes large. And everything which was not in that vocabulary, you marked as OOP. And that was the standard processing pipeline. Then you send those vectors to LSTM or some bag of words model or something like that. But this is what was the basic thing. Then what happened in a few years? Happened in a few years, thanks to those progress in compression algorithms, 2016. There was an idea of compressing this to bypare encoding or word pieces. It means that we first scan through our whole vocabulary, through our whole textual purpose, and we find most common parts, which we call tokens, and we just merge them together. So, our vocabulary becomes not just only letters, but also some most common n-grams, and we build a vocabulary of those word pieces. So, in fact, current Word pieces. So, in fact, current all language models, with exception maybe of some character or some more exotic models, they work with this representation of byte paramodics or word pieces that we tokenize each word into specified tokens depending on their frequency. And for instance, apples can be tokenized into those two tokens. And this two hash symbols means that it is the end of some. The end of some different token, and so on. So, usually, those vocabularies are medium-large, like 30-50K tokens, or some multilingual models, there might be some 110 or 150k words, but still relatively small compared to sizes of those models. So that's the trade-off right now, that in language models, we have relatively small vocabulary and we have efficient encoders or graph representation. For graph representation coding, for knowledge graphs, we have just superlaries that actually go out of the slicer. We don't have any input. There are shallow models which only use some basic translation or rotation vector space. So those are linear. There is no learnable parameters. And we cannot use a GNN on top of this because the models are the graphs are so big. And it's also parameter hangar. So the idea here is, can we invent something like a BPE for the graphs in that sense? The graphs in that sense. If not, if we consider nodes as words, can we further discretize them, even though those are already discrete nodes, can we discretize and further and create those vocabulary of sub-word units, which will have this inductive property? And yeah, that's what we came up with, with a bold claim of claiming this as an old piece with the homage to word pieces, where we have some fixed-size vocabulary and relatively expressive encoder. Uh, relatively expressive encoder, but that's still a lot of things to uh to be discussed and to improve there. Well, anyways, um, positioning that in the graph world comparing to language world, that uh in language shallow models are pretty much dead. No one uses them, maybe for some basic baselines. In graph worlds, in graph world, right now, everyone to date has been have been using shallow nating algorithms. And while language Calimating algorithms. And while language world transitioned to BP and board pieces, graphs in graph world, it hasn't been anything. And likely, we take this small gap with notepiece. Yeah, so that's the smaller, like the TLDR idea of the whole notepiece, how we do this tokenization, like how we build this. So we build a vocabulary of anchored nodes and relation types. So those anchore nodes are specifically mined. Core nodes are specifically mind nodes in the graph, which can, and some, and arguably, they might not be nodes, actually. They can be some things which will be in the graph invariants. But for simplicity, we can say them as other nodes and the relation types. Relation types we can consider to be invariant because, even in some inductive setups, we usually do not change the relation types. So, we can, and they are not so many, like 6,000 relations for super large hundred million nodes graphs. Super large 100 million nodes graphs, that's we can afford to learn that. So, this whole idea boils down to tokenizing each node through a set of nearest anchors here and their distances and finding those sub-set some sub-sample of the relational context around. So we have the horotoric node, we have some three anchors and some sub-sample of those colored edges. In details, but generally, how we came up with this thing is that we were. Came up to this thing is that we want to represent this red entity a set of K-mo similar tokens. And there can be many ways to find this, to find this, to parameterize this in the early function. In the very most basic case, we'll enter this Euclidean space, although we can now go to more complex ones for sure. That's still largely work in progress. Although this particular version has been. Although this particular version has been published, I appear this year. And yeah, the ideal case, what we might want is to have a dominating set. Unfortunately, dominating set is an p-hard program dominating set. It covers the ideal dominating set that those are just a set of nodes such that all other nodes are at least designated number of hops away from those nodes. So for instance, this dominating set colored nodes are dominating set, and every other node in the graph is one hop away from any nodes, any nodes. Any nodes, any node from this dominating set. So, this is a classic MP complete algorithm. Even its approximations are NP hard. So, yeah, that's a bit of a problem. So, there is no polynomial algorithm to approximate this. So, we have to find something more efficient or something that will not suck that hard, but still work okay. And from our experiments, we found that what works relatively okay is that we take just some. Relatively location is that we take just some centrality measures and mix them together, like sprinkle a bit of randomness on top of PPR and degrees. And the tokenization cell is just the BFS. So once we identify the set of those anchor nodes, then we just fire a BFS for each node in the graph and we search it until we find the designate number of anchors. It's pretty much the, we have fixed size word length, number of tokens per word, and we allow the parades until we find those numbers. The spiral BFS until we find those number of handworks as we need, and that would be the tokenization. And that's exactly the idea behind this magic pot. So, in graphs like this in Wikidata, if we have all mobile prices anchor nodes, then we can tokenize Einstein as the collection of those anchor nodes and some outgoing relations. And the concept here is that even though it describes these things approximately, but already the sub-sample. Approximately, but already the subsample of those relations like occupation or academic degree award received might already give you, or give an activist for the model at the red, purity talking about scientists here. And indeed, we found the experiment that increases, it has a pretty nice recall for on large graphs. And in the same way, we can apply this magic pot and tokenize John Mayer through Norwegian black metal. So in fact, Norwegian black metal is just three cops away from John Mayer. Norwegian black metal is just three cops away from John Mayer in the big knowledge graph extracted from Wikipedia. So that is indeed true. That's it's possible to find the link. Yeah, and the model is inductive. So if we have some unseen node inference time, surely we can fire BFS and do it in linear time. And just, well, we know the anchors, their sequence will be different. But since we learned the vocabulary of tokens and it's Learn the vocabulary of tokens, and it's the same. We apply just the tokens, we learn vocabulary in different order, and we tokenize new nodes in the same fashion. And if graphs are completely disconnected, so we have like completely different graphs, but with the same relation types, then we can just drop anchors, not learn anchors at all, and just tokenize every node through its relational context. And yeah, it works well enough. And as a set encoder, so this is important part: encoder. So, this is an important part: encoder, how do we build representations from those handwriters? And in language models, here you would slap a transformer right away. Here, it might be too expensive because we're talking here about sizes of 2 million nodes and many, many millions of edges. And we have to represent if average length of node, say 40 tokens, that can be pretty lengthy. But generally, for a certain But generally, for a set encoder, we might use a number of functions from something permutation invariant, like averaging or one ello transformer, to something not permutation-invariant like MOP. Just concat all those tokens together, pass into NLP, and here we go. And this is more of a trade-off between speed and expressiveness that you want to get. And the interesting thing that since we build those representations before we send Before we send them to any browser task, they can actually solve a lot of tasks with that. So, either send it directly to some decoders, like they have been in the literature for 10 years, or we can just materialize all of them and send to GNN. And it actually resembles some sort of a 1WL based on relations. So, instead of neighboring nodes, which representation do not know, we have We have relate those colored relations, and this can be seen like running GNN.dead can be seen as sort of basics of this relational WL test. Yeah, and a few words about exponents we found. That's the first question is: how many anchors do I need? What kind of vocabulary do I need for my graph? And we found that it largely depends on the density and the number of relation types. So, this is, for instance, free base. So, this is, for instance, Freebase, all data set about from Freebase, discontinued Google data set already like 10 years as dead right now. And for that, we do not need big large vocabulary. So you see that saturation happens already at this point when we around for 15,000 nodes graph, our vocabulary consists of only 100 anchors. And tokenizing each of them through 20 anchors per node, it already gives a good saturation point. And the word net is a more sparse graph. More sparse graph. It only has 40k notes and the average degree of two. And here we need at least like thousand or 500 notes as a vocabulary. And we need to increase the anchors per node, like the word length bigger and bigger. And then we see the saturation. And summarizing the results, we found interesting thing that having much fewer parameters, we retain quite a good performance of transactive hopy. good performance of transactive op and in the same in the same way we enable inductive inductive inference because our model actually doesn't depend doesn't depend if we have seen or answered notes anything at inference time the procedure is exactly the same and in some experiments no anchors is better so if you only use tokenization through relations we have even fewer parameters and we have even better numbers so this is a node classification experiment with multi-class Classification experiment with multi-class, multi-label, semi-supervised classification. And yeah, the numbers here are surprisingly good. So this is a basic MLP. This is a GNN over the runable features and adding some tokenization on top, it works surprisingly good. Yeah, well, yesterday here was a slide, but then ghosts of Kelowna flew this one this evening and they just stole the slide. Slide. So I don't know what happens after this, but I'll just skip it. Now, today what we, if you're the beginning of this talk, I showed you that there were in top of OGB, there were large models of 500 million parameters and 1 billion. And now the current top of OGB on this on this benchmark are all models based on NodePs, and they are much smaller. And they are much smaller. So, this is our original model had only six million parameters. This was a first different decoder on top of it. It just got about seven million parameters and so on. So, generally now, all the models on OGB for about a year, they use our documentation approach, which is pretty cool. Yeah, an interesting fact that we, if we drop, and of course, this is the line, interesting line. This is the line interesting line. If we drop all anchors that do not learn 20,000 anchor vocabulary, and we just even shrink the model to 1 million parameters, we still have much better MRR compared to models which are 1,000 times larger, like and by a good margin better. So we attribute that to some inductive basis, but which inductive basis we still cannot explain. And last slide about this is inductive prediction. So when we have a difference, So, when we have it inference time, completely new graph with the totally unseen nodes. And those are some logic programming approaches, some rule-based things. And node piece-based line is surprisingly good, even compared to MVFNet. And I'm going exactly to this neural demo for NETS and the second tokenization approach, featurization approach. Yeah, so the first one was featurization through tokenization. The second thing is featurization approach. The second thing is featuredization via labeling trick. So, this is the second approach for building those inductive representations. The idea of labeling trick, it's recently, it's pretty fresh, so it's been published in New Root 21, is that for each prediction task, we instantiate a graph with unique features, with unique labels. There is some nice story in the paper how if those nodes v1 and v3 here for models would might. Here, four models might look exactly the same for the task of lean prediction if you do not use this unique instantiation. So, the whole idea of the stabilizer or relational graphs has been applied in this Euro Delmont 4 paper, which was also New York 21. And the idea here is that for each task, which is like a link prediction, given a head and relation when we build a distribution over all other nodes, for each of those. Nodes. For each of those links in a data set, we instantiate its own graph with the so-called boundary condition, which is like we assign a learnable relation vector query, which might be a relational MIDI, to the starting node, to the head node. And then we start this message passing process. It's called Dolman for the iteration, but in fact, it's just a relational GNN. And in the end, we have a distribution conditioned by the heading relation for this particular particular. Had in relation for this particular prediction task, and we just take representations of all those nodes and feed them to the classifier. So, this thing is almost also inductive because we only learn representations of relations and we are completely independent from representations of nodes. We do not need them. And yeah, this thing has shown very good results even on non-relational graphs predictions. So, seal is another, the first version of the labeling trick. We can say that. First version of the labeling trick, we can say that. And the FNAT was slightly better and much better on it's currently state of the art actually in relational link prediction tests. So it still has some good number of theoretical properties, which are yet to be to be discovered. And something that we have recently accepted to SML, but we won't be able to present because I don't think I'll forget the reason. This is the extension of those MVF nets. The extension of those MVF nets to multi-cop reasoning. So that's the most complex graph inductive graph reasoning task when we have first-order logic query and we want to answer those logical queries over a graph. This has a quite a number of colors here. So I'll just say that each of those links is a link prediction task, and we have to combine them to answer queries with intersection, with union, and with negation. And we model those logical steps as certain fuzzy logic. Logical steps as certain fuzzy logic operators over fuzzy sets. Yeah, so we apply the same idea that first we instantiate the graph with some query vector and then we propagate this information over a graph to find those to find the missing link. So kind of at each step, we'll be adding more and more true links to the graph. I'll probably skip that. So the essence of inductiveness of those two approaches, what is What is uh uh how do they model how they approach inductiveness in two cases for for node piece for the basic setup versus without n course we tokenize each node through the sick through a collection of relation types adjacent relations uh unique ones so those colors denote some directions and types types of edges from based on this training run and uh for uh gn and key which is GNIQ, which is the version of NBFN for query metadata, we have those relational structures. So it's not just a set of edges or edge types, but it is a relational structure which we iteratively build from applying many steps of neural Bellman for it. And we applied that to inductive reasoning. So that is my currently work under submission and the ravio in the task of running the inference over much larger grounds. The inference over much larger graphs. So, we were interested in how we can apply this to problems where we have to answer those logical complex queries over graphs which can be 500% larger, so five times larger than the original training graph. So, here the sizes are in the ballpark. So, training graph might be 3,000 nodes, and 100% algorithms that inference graph becomes 50. Larger means that inference graph becomes 15,000 nodes. And as you see, all GNN-based models, so it's like node piece with GNN or NBFNet, they all suffer from increasing the graph size at inference time. So it's still a big problem. There has been quite a number of publications or submissions to New York state try to combat these things. They propose a few GNN mechanisms, how we can make them Can make them not deteriorate that fast. But so far, since the original vanilla note piece, although it performs not that good compared to other in terms of ranking performance, but at least it's stable. At least it's stable. So it gives us some signal that better representation learning approaches here might be pretty effective and they might not be that susceptible. That's susceptible to the increasing size of the inference graph. Yeah, that was pretty much it. And as the last slide, so that was the state of the tasks of graph learning on the relation graphs a few years back. Just pretty much transductively prediction triples. And what we have today is such a much larger landscape of tasks. And that's in fact how we select which papers we'll write. Fact, how we select which papers we'll write. So just run the roulette and saying, like, okay, like select something from this menu, like transductive, hyperrelational, supervised uni model, small, entity matching. Cool. That can be our next paper. Yeah. And something that we've been working on recently, the idea of extending that to database world and replacing those big And replacing those big query engines, query engines of SQL and Sparkle and so on with MIPS retrieval. So, this is something that is very big right now in the NLP world with those retrieval-based language models. We can do the same with graphs. And our database can consist either of those super large graphs or consists of millions of molecules. For instance, those molecule prediction or generation tasks. And yeah, we can build the whole interaction with this database of vector. With this database of vectors with the help of those differentiable logical queries. But I'll stop here. And yeah, I think.