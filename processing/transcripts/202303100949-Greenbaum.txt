Optimal palace is an approximation to rational matrix functions. Alright, yeah, so I will be talking about using the method that was just pointed out not to be as good as the other one, but this is for approximating rational matrix functions. This is joint work with Tyler Chen, Cam and Krisnusko, and Natalie Wellen. And Natalie Wellen. I'm mostly going to talk about. Oh, how do I do this? I'm mostly going to. Oh, goodness. So what do I do? All right, we're going to have one slide to that. That one. There, there we go. All right, so we've talked about the Lantos and Arnoldi algorithms. They are simply methods for computing an orthonormal basis for a Prelop space. And you can write them very succinctly without putting in all the details. You've got A times QK, the matrix with the orthogonal. With the orthogonal basis vectors for the prelob space equals QK times, well, an upper Hessenberg matrix if A is non-symmetric, or a symmetric tri-diagonal matrix if A is symmetric, plus a remainder term. Or you can write that as QK plus 1 times the K plus 1 by K upper Hessenberg or triangular, tri-diagonal. Tridiaconal matrix. And we want to use these vectors to approximate a rational function d of A inverse times N of A times B. Oops. That's backwards. I was pressing it backwards. That's the thing. Up is backwards, down and forwards. Okay. So the usual way of doing this is called Lanzoch. called Lansocha. I think Tyler mentioned it earlier, where you would approximate it F of A by taking QK F of HK times QK star transpose. Okay, that would be your approximation to F of A, and I should put it B in here. And this is lots of And this is Matson F A. But there are no optimality properties associated with this. And so we were looking for could we optimize, could we find the optimal approximation to F of A B from the Krylov space? And well, what we could do was find the optimal approximation in the D of A star D of A. In the d of A star D of A norm. That's the denominator. Which is the same thing as the two norm of the residual, namely N of A times, well, QK times the first unit vector, that's B, minus D of A times your approximate solution. And, well, this is just completely analogous to what GM res does if you're solving a linear system. If G of A is A and the numerator N of A is just the identity, then this is just GM res or for symmetric matrices, min-res. I'll mostly talk about non-symmetric B. So we'll minimize the A star A norm of the error, the difference between A inverse B in our approximate solution. B in our approximate solution, which is the same as minimizing B minus A Q K Y, our approximate solution, the two norm of that. That is, it's the same as minimizing the two norm of the residual in the linear system. Okay, so how do we go about minimizing the d of A star D of A norm of the error? Well, first note, we had our original equation, A times Q. Original equation, A times QK is QK plus 1 times this K plus 1 by K, upper Hessenberg matrix. If I multiply that by A, then I've got A squared QK is A Q K plus 1, H K plus 1 K. And now I'll plug in for A Q K plus 1. That's QK plus 2 times a K plus 2 by K plus 1 matrix times a K plus 1 by K matrix. I'll get this product, the name HK plus 2K. k plus 2k and so on so that if our denominator is the sum v j z to the j and our numerator is the sum n l z e to the l, then plugging in these expressions for the powers of, well now, powers of a times qk, this is what we want to minimize, n of a times qk dy minus d of a qky is just equal A y is just equal to this expression. There. Okay. Um, no, that moved backwards. I don't know where this is going. Okay, yeah. Okay, we got it. Okay. A neater way of writing this and thinking about it is let m be the maximum of the degree of the numerator, the degree of the denominator. We have to run in extra m iterations to get this matrix hk plus m. But what we want to do is minimize the difference between n of h k plus m times the first unit vector minus. vector minus, we'll take all rows and columns 1 through k of d of h k plus m and we'll try to solve this least squares problem. So this is a k plus m by k least squares problem. We're looking for y to minimize the two norm of this difference. We can solve this least squares problem using QR factorization. Our matrix is no longer Our matrix is no longer upper Hessenberg as it would be for GM res if the degree of G is greater than one. But we can still, it's still the case that the coefficient matrix for our least squares problem at step K differs from the one at step K minus 1 just by adding one column in one row. So all we have to do to solve the least squares problem is apply the previous rotations that we've used to the last column. We use to the last column of our new matrix. And then we have to choose m new rotations in order to eliminate the entries in column K, rows K plus 1 through K plus M. We then have the QR factorization of our new matrix and can solve the least squares problem that way. Sorry, does that apply also to the right side? So N. So, and oh, yeah, we have to apply those to the right-hand side as well. Yeah. So, we apply the previous rotations and the new ones to the right-hand side also. Okay, just to give an indication of how this works, suppose for a very simple example we want to solve a squared axis. We want to solve a squared x equal b. Well, this is the convergence of our known EOR. If we applied GM res directly to the problem a squared x equal b, then we would be using a less good Kryloff space. It would be one based on a squared instead of a. And the number of iterations would go like this. If, in fact, at every iteration, in order to multiply by a squared, we had to actually multiply by Squared, we had to actually multiply by A twice, then the number of matrix vector multiplications would go down like this. So this clearly wins over apply G of RAS to A squared. Half the time this moves forward and half the time it stabbers, and the other half of the time it doesn't move at all. All right. Okay, so I wanted to talk about some error bounds, and these are mainly for non-symmetric problems. Okay, so we're minimizing the d of A star, D of A, or S norm of the error. Our approximate solution, QK times Y, that's a K minus first degree polynomial in A times the right-hand side vector, B. Vector v and it's the k minus first degree polynomial that minimizes the s norm of the error. So if we want to bound on the two norm of the error, the difference between r of a v and our approximate solution, we'll have this extra factor cap of s to the 1 half times how well we can approximate r of a with a k minus first degree polynomial. If a is diagonal If A is diagonalizable with eigen decomposition V lambda V inverse, then we can express this problem in terms of how well we can approximate the rational function r on the set of eigenvalues of A using a K minus first degree polynomial. But we have to multiply by the condition number of the eigenvector matrix. If it's a normal matrix, like a symmetric matrix, then this is one. Matrix, then this is one, so that's not a problem. However, although you'll often hear people say, well, GM res converges great because my eigenvalues are all close to one, without some information about the eigenvectors, you really can't make that claim. Yes, you can do this approximation very well, but this number may be huge. Huge. And in fact, one can show for Lanto, or for Arnoldi OR that, just as one can show for linear systems, that you can make any non-increasing convergence curve, and you can find that with the matrix having any given eigenvalues. So you can't really base an analysis solely on the eigenvalues. If A is not diagonalizable, A is not diagonalizable, or the condition number of V is huge. One can use an error bound based on work of Couset and Valencia. They showed that the numerical range, the set of values inner product of A, Q, and Q, such that norm Q is 1, is a 1 plus root 2 spectral set for A, meaning that for any function f, norm f of A will be less than or equal to 1 plus root 2. or equal to 1 plus root 2 times the maximum value of L on the numerical page. It's conjectured that the number is actually 2, but that's still an open conjecture. So using that, we can base our error bound on we've still got this kappa of s to the 1 half, 1 plus root 2. How well can you approximate this rational function by a polynomial on the numerical equation? If you prefer to work directly with the S norm, the D of A star D of A norm, this Cruzet-Palencia result still holds, except that you have to define the numerical range in terms of the S inner product, which turns out to be the numerical range of S to the 1 half, A S to the minus 1 half. So one can express the error bound in that way. Be error bounded that way. The previous bounds are not useful if the rational function has a pole in the numerical range. Pruset and I recently showed that you can take the numerical range, remove a disk of radius 1 over the numerical radius of Ci minus A inverse, about point C. The numerical radius. The numerical radius is the largest absolute value of something in the numerical range, and still have a 3 plus 2 root 3 spectral system. And so one can estimate or bound the norm of the error by how well you can approximate this rational function by a polynomial on w of a minus this dist. Okay, let's look at a few examples. Let's look at a few examples. So I just took the A to be a random 100 by 100 matrix. I added 5 times the identity. It's not terribly non-normal. The condition number of the eigenvectors is 98. Condition number of D of A was 228. Looking at the two-norm of the residual, our null DOR has to win because it's optimal in the two-norm of the residual. That's the black curve. Of the residual. That's the black curve. The red dashed curve, that's the behavior of Arnoldi FA, the more standard way of approximating FA using the Arnoldi vectors. It's not guaranteed to be monotonic, and it's obviously not monotonic. And in this case, it's quite a bit worse than ROR. The green is one that you can't in a practical way compute. It's simply the projection, the orthogonal projection of the solution onto the Kryloff subspace. So if you know the solution, then you can get the optimal two-norm approximation from the Kryloff subspace. So this is what things look like if you compare the two norms of the residuals. If you look at the two norms of the errors, Look at the two norms of the errors, of course, this green curve has to be the bottom line. And now Arnold EOR and Arnold EFA are sort of tied. FA bounces around a bit, but that's the way they behave. These are the eigenvalues of this matrix. This dark solid black curve is the boundary of the numerical range. And these red O's are the poles. Are the poles of the rational function? I took the numerator to be a random quadratic function, the denominator to be a random cubic function. You don't have to know these poles. They're just there. You just have to be able to apply f of a to a vector, or a to a vector, and f of h k to a vector, or h k plus m. So anyway, because the So anyway, because these poles are so close to eigenvalues and well inside the numerical range, convergence is pretty slow. After 80 or so iterations, we're only getting down to an error of 10 to the minus 2, or residual, an error of about 10 to the minus 4. All right, if we take the same problem, but just instead of adding 5i, we add 15i, well, now things converge much better. Well, now things converge much better, and in fact, all of these curves are very close to each other. Now the poles of the rational function are outside the numerical range. Probably the best error bound would be the one based on the condition number of the eigenvectors and how well you could approximate zero on the eigenvalues. I wanted to look at how well. I wanted to look at how well you could, or sorry, how well you could approximate functions of rational functions on the eigenvalues. I wanted to look at how well you could approximate the rational function by a polynomial on the numerical range. That's hard to do. If you put the numerical range inside a disk, although the aspect ratio is off, this cyan dash thing is a disk, a circle. Circle. Then you could say, well, how well can you approximate the rational function by a polynomial on this gift? And even that can be hard to do. So you can do that approximately by using a Fiber series. So I computed the Fiber polynomials. And based on that Fiber series, I got this bound on the error, which is quite a large overestimate in this case. Because the poles were so much closer to the boundary of this disk than they were to the eigenvalues of the matrix. Now I've added 25i to my random matrix. Again, there's very little difference in the behavior of Arnoldi OR or Arnold EFA. We don't yet know how to explain that, but that's the case. And now if you use And now, if you use an error bound based on how well you can approximate the rational function on this disk, in a relative sense, it's further from the poles than it was before compared to the distance from the eigenvalues to the poles. And so you do get a better error out. And finally, to do a more interesting example, this is the famous Yurger matrix. You can get it in MATLAB. You can get it in MATLAB. I have the gallery of Derrida 100. The condition number of V is huge, much larger than 10 to the 16, so I don't necessarily trust what they tell me the condition number is. The condition number of the denominator, here I'm just taking the simplest case where we're solving a linear system. So B of A is A, N of A is the identity. The condition number of A itself is not bad, it's 3.6. 6. And once again, if you look at the 2-norm of the residual, it behaves like this for Arnoldi OR. Arnoldi FA is not quite as good, but it's not that much worse either. Okay, so I will finish up. And this is looking at the two-norm of the error. There's a There's a pole right here at the origin. So you can't, the numerical range would have this piece filled in. You can't get an error bound based on approximation on the entire numerical range because there's this hole in there. But by removing a disk about the hole, one can get an error bound, which is not a great error bound by any means, but at least it shows that it will converge. It shows that it will converge. All right, to finish up, using the max of degree of numerator and denominator minus one extra steps, one can find the optimal in d of a star d of a norm to the rational function from the Kryloff space. To do this, you solve a k plus degree of d by k least squares problem. Least squares problem. You can reuse Gibbons rotations from previous steps in order to solve this least squares problem efficiently. The residual in the least squares problem should be equal to the actual residual norm, so you don't actually have to form the approximate solution until the residual in the least squares problem becomes small enough. And finally, for highly non-normal problems, a priori bounds can be based on how well one can approximate this rational function using a polynomial of degree at most k minus 1, either on the numerical range or other k-spectral sets. You warned me that you have bad luck with technology. Any questions? Maybe we can take one or two questions. Yeah, I have a question. So there were two results. Like one, you had to approximate out of the eigenvalues, and then you paid the work. One you had to approximate out of the entire neural range and didn't pay that. Like, intuitively, is one stronger than the other, or are they sort of? Stronger than the other, or are they sort of incarable? So if the matrix is normal, then the condition number of the eigenvectors is one, so that's the one to use. So that would be the one to use for symmetric matrices. If the matrix is, only if the matrix is highly non-normal, kappa of D, the best condition V, is huge, would you really expect to get better? Would you really expect to get better results by approximating on the numerical range? Will that blow up, like when cap is huge, will that make the numerical range, like much outside the very likely. Okay, but it's like maybe less. It's like a trade-off. Yeah, maybe, yeah. Yeah, yeah. Well, if we could go back, I would. But there's no direction. Yeah, I'm sorry, but yes? And this is hi, it was a great talk. That was a great talk. It captures the departure from normality, right? Yeah. But that wouldn't necessarily go up. So, sorry, what? I mean, if you think about the departure from normality in the Shui decomposition, the nilport will have a triangle part. That wouldn't necessarily go up. Yeah, they're different. That's partition number becomes unbounded. Unbounded. Different ways of measuring non-normality. I mean, think about a Jordan block, right? Yeah. The body from the spine, but the conditional part doesn't exist. Right, it's not diagonalizable. For a Jordan block, the numerical range is a disk of radius cosine pi over n plus 1 about the origin. The eigenvalue is 0. So I don't know if you. Value is zero. So I don't know if you call that blowing up. The eigenvalue is zero. The numerical range is basically the unit disk. I guess in a ratio sense, it blows up. It's not a huge set, though. Any other questions from online? Just a quick one about so yesterday at the end of the day. quick one about so yesterday that we were some of us were talking about like what is some hard class of f and people were saying like threshold at sign of course the paper you were testing also like rational functions are there other classes of f like you know well you could always approximate something like e to the a or square root of a by some sort of rational approximation. Approximation. So, yeah, it could be applied to those functions as well.