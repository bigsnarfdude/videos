Thank you. Thank you for the opportunity here. I don't know why all these senior processors are sitting here. This is a session for young researchers, and you won't get anything from this. Yeah. So, as the name suggests, we tried to introduce the concept of quantum probability from linear algebra and the basic probability theory. And yeah, it's very basic, and we avoid all the generalities. I don't have any originality to claim here. Education: so I fondly remember Professor Tiyar Par Saharadi, who passed away last year, from whom I learned almost everything I'm going to talk. And this sentence from his book is very interesting. The essence of quantum probability lies in taking into account the possibility of using arbitrary self-adjoint operators or operators. Self-adjoint operators or operators, self-adjoint matrices or operators of the instead of only the diagonal matrices, and pushing the basic ideas of classical probability to their logical end with the tools of extraordinarily rich theory of operators in the Hilbert space. So our aim is to understand this code in some sense. Okay, that's the aim of this talk. So if you're very new, you might not understand these terms. These terms. What do you mean by using arbitrary self-adjoint adjoint operators instead of diagonal matrices? So I'm trying to explain that and then give some basic ideas and also to talk about some differences from the classical probability theory. So this is a kind of generalized, this is a generalization of classical probability theory, but some differences come into the picture and we try to talk a little bit on that. All right, just start with our spectral theorem for ourselves. Just start with our spectral theorem for self-adjoint matrices. If you have a n by n matrix, you can diagonalize it using a unitary matrix, and you have some real entries in the diagonal. The spectrum of a self-adjoint operator is a real diagonal matrix, real, and you get a real diagonal matrix here, and you have a Q entry matrix. And this thing can also be seen as a sum of a linear combination of projections, a real linear combination of orthogonal projections. And this is something we just recall and keep it aside, okay? We'll use it when time comes. So let's talk about the basic classical probability setting when you have n numbers. Let's say omega is set 1, 2 up to n, and you have the probabilities on 1, 2, 3 up to n, like p1, p2, etc., pn. Then, let's say you have a random mirror, you have a function, real-valued function depend on this set. function, real valued function depend on this set. And this is going to be x1, x2, xn. Like x1 is the value of 1, value the function takes at 1 and like xn is the value the function takes at n. And then the expectation of this random variable like everybody know in this probability is summation pj xj. All right. And this expectation can be viewed in a different way. You can think of this number. Sorry. Yeah. You can think Yeah, you can think of this number as multiplying two vectors, like a scalar product between two vectors. And you can also think about it in like the trace of the product of two matrices. This time, both the matrices are diagonal matrices. Okay. And then the product of these two matrices, if you take the trace, that is the expectation of your random variable f, which you started with. And don't worry. Chartered with. And don't worry about the distribution right now. And this is what KRP was mentioning in the first this quote. So you replace all these diagonal matrices with corresponding self-adjoint matrices or positive matrices, and then try to do classical probability theory. And that is the essence of quantum probability. So, So, okay. So, we are going to replace this with a positive matrix and this with a self-adenal self-adjoint matrix. And when I say positive, these things should add up to one. So, it's going to be something known as a density matrix. Okay. That is a positive matrix with trace one. All right. And in this setting, we have one more notion. So, in this particular setting, this is very simple. In this particular setting, this is very simple. But generally, if you have a omega, f omega, and p, a probability on a general space omega with some you know sigma algebra, you talk about the distribution of a random variable. So distribution of the random variable, let's call it this P F. And for a set E, this is the P of F inverse of E. You have a set omega and a function from omega to the real line, and you take a subset of the real line. Take a subset of the real line, consider the inverse image in omega of that subset, and consider the probability of that space. And the probability of that set is the definition of PFOP. That's the distribution of a of a random variable. All right, simple things. And then, for the development of quantum mechanics in the 1920s and 30s, especially the foundations of mathematical, mathematical foundations of quantum mechanics. Mathematical foundations of quantum mechanics warranted a generalization of these ideas, you know, and that is how the more general self-adjoint operators and the states come into the picture. And just to note, this expression underlines the idea that the space of all real-value random variables is a real linear space of dimension n. And the probability distribution is a non-negative element of its dual. Okay, so real linear space of. So, real linear space of random variable, and then the real linear space, and the probability distributions in non-nevity element in the dual of the space of random variables. That's one idea. And from the second expression, you can talk about, you can see that this can be changed to a general self-adjoint matrix. And you can see the probability distribution is an element of the n-square-dimensional real inner space of complex self-adjoint operators. So, that's the basic idea here. All right. And coming back, so now what we are trying to do is you replace a probability vector P1 bit2 Pn with a density matrix, which is a positive semi-definite matrix with trace one. And then we are going to replace a self, the random variable on that one, two, three, et cetera, with a general self-adjoint matrix. All right. And then the expectation of x in the state row can be. The state rho can be taken as trace rho x, okay, which is nothing but the summation xj trace rho pj. Okay, and the distribution of this x in rho can be thought of as that let us write it down as distribution of so p is the distribution of x in the state row that we write it as p rho x of e is P rho x of E is just sorry, it's difficult. Yeah, sum over all j such that xj is in e trace row pj and with respect to this kind of a this representation of x. So that idea is really important. I want to write down on the board if you have. If you have x is equal to summation xj pj, this is spectral decomposition of the self-adjoint matrix. You have x1 dot dot dot xn on the real line. And if you take a subset E on the real line, you can go back to your Hilbert space where you are staying. For in our case, it is Cn. Okay, it will do CN for us. See and for us. And then you can consider this number: trace row of summation j pj, where this j is running. So in this E. So J. J is here. So all those corresponding j's for which this xj's are lying there, you know, and take the inverse image of that in this sense and j. Such that xj element of e. Basically, just like what we had thought about in the classical probability case, inverse image of a set you want to take there for the distribution. Here, you take the inverse image in the sense of spectral theorem. So, when you take the sense of spectral theorem, the inverse image is going to be a sum of projections. And that is what this definition is. And this idea can be extended to even infinite dimensions because of the Even infinite dimensions, because of the spectral theorem. I'm not going to discuss that. If we have time at the end, we can discuss that. But this is a very crucial idea. In this way, every so-called quantum random variable, which are now self-adjoint matrices or self-adjoint operators, gets a probability distribution on the real line. So, this distribution is defined: this is the this is p rho x of e. This is This is defined on the real line. So, corresponding to any self-adjoint operator and a state, you get a classical probability distribution on the real line. That's called the distribution of x in the state row. And this is very crucial for us. All right. Yeah, this idea works in the infinite dimensions also. Just like the inverse image, instead of a sum, here you will have a class of projections where you take integration. Projections where you take integral, you will have integrals. Okay. Yeah. So, density matrix works like a probability measure and self-adjoint matrix works like a random measurement. But one thing which I did not speak about till now is the idea of this thing called sigma algebra. In classical probability theory, we have something called sigma algebra. called sigma algebra which are the set of all events okay so when you deal with discrete probability you don't mind about these things because all subsets are measurable there but if you want to talk about general measures on real line or any other spaces you want to talk about what are your pass of all events what are the events for which there is a postpositive probability okay and here also we need such a thing so in this setting of quantum probability there are several different versions of Quantum probability, there are several different versions of quantum probability, but the setting of quantum probability which we are taking, we replace this omega with a Hilbert space h, and then this p was replaced by a state rho, which is a density matrix. And this thing, the algebra of sets, is called sigma algebra in the classical probability, is replaced by P of H, which are known as the projections. Projections orthogonal projections on the Hilbert space H. Okay. And that is what is going to replace the sigma algebra. And they have very nice properties in relation with the usual set theory. Okay. So let's look at them one by one. A is an event. In set theory, it is A element of A, the big set A, and here PJ, the A and here PJ suggests A is a projection. Okay, basic. Then an event A implies B in classical probability is A subset of B and in quantum probability it is A less than or is equal to B. So here as I said initially the events are the projections on the Hilbert space and an event A implies an event B means the projection A is contained in the projection B. Okay, that is A less than or is equal to B and B and the idea of an event A doesn't occur is A complement in set theory and in quantum probability it becomes the perpendicular of A, the orthogonal complement of A. One of the events A or B occur is A union B and here it is the soup projections. If you have two projections you can take the subspaces of the corresponding projections and take the max subspace, take the closed subspace containing both these subspaces and you have the sub projection between these two. These two. So, this bigger subspace has a projection on it. That is what is this sup projection. And that means one event, one of the events A or B occur. Okay. And both the events A and B occur is intersection in set theory. And here it is a mean projection, just like the intersection of the subspaces become another subspace. You get that. And the event which always occurs is the full space. And here that is the identity operator. And event which will never identity operator and event which will never occur is the null space and here is the zero projection and event a and b cannot occur together means this intersection intersection is empty is the classical set theory thing and then these are disjoint you know uh projections that's what um that means okay so one crucial difference here is this you don't get distributivity so for set theory if you have if you take a intersection b Have if you take A intersection B union C is A intersection B union A intersection C, but you don't have that for the lattice of projections. That's a very crucial difference, which gives rise to a lot of problems, you know. And you can easily find an example in C2 itself. So now we have discussed the basic We have discussed the basic some basic ideas in quantum probability. Let's talk, and we have also seen the similarities. Many things are similar here, but there are many differences as well. I want to talk about three such important differences here. The first difference, which I want to say, is that trace row, this is think of it as E union F. Okay, that is this trace row of. So, I didn't talk about that. I guess I should need to write down. Talk about that. I guess I need to write down one more thing. If x is a quantum random matrix, that is a self-adjoint matrix, Shrey's rho, x is the mean mean of x in row. That's the expectation of the random variable x in row, just like you have. X in row just like you have integral of f dp so replacing the integral with trace and p with rho and integrating them just like that the mean of a random variable is this one similarly you can talk also talk about the variance trace of that we need later so that let me just write x minus m m rho of x M rho of x x minus m rho of x the whole square that's the variance variance rho x variance of x in row okay yeah so this one is the the measure of Is the measure of the set E union F, if you think in classically, okay? That we know that it's usually less than or is equal to the measure of the first one plus the measure of the second one, right? What I'm trying to say is that measure of A union B, if you have a classical probability measure P, P of A union B is less than is equal to P of A plus P of B. This is something. Of B. This is something which always happens in classical probability, but in quantum probability, it does not happen always. And expectedly, this fails to happen when you have this commutation relation. If the events don't commute, then you don't get this relation in general. So that's a very important difference here. And we have how many minutes? Many minutes? All right. So I will go through the other distinguishing features and then we can come back to the proof if we have time. So it's a very easy linear algebra to prove. I can send the slides as well. So the idea, I want to stress is that that thing which you had always was given in classical probability theory does not necessarily hold in quantum probability. So it is now, it should be noted that. Now it should be noted that there is for some state row, the inequality is false. Okay, I'm not saying that this will never hold because it said the generalization will hold for the classical case at least, but it will not hold in general. So there are states row for which this won't hold. And explicit examples can be seen here. Yep. Where am I going? Yep, that was the lemma which gives you that. Okay, so what the main thing about this lemma is just this lemma. This theorem you get by proving this lemma. So the sub projection of E and F is not necessarily less than or is equal to the projection E plus F. That is why you are going to get the previous theorem. That is the lemma which we used to prove that theorem. Prove that theorem. And once you have that, it's very easy to create, give a proof of that theorem. All right. The second distinguishing feature, which I want to say, is something which you have seen throughout the day today. This thing which you might have seen x plus ij greater than or equal to 0. This was something which you have seen throughout the day, throughout the. Uh, they through the different sessions today, which people refer to as Heisenberg's uncertainty relationship. So, that in the Gaussian state scenario that appeared for the second moments of the Gaussian state here, and this was a compact way to represent the different uncertain relations. But in more general setting, or in some sense, even with the, you know, that this is about the continuous variable setting, and even in the discrete variable setting, which we are discussing today. Discrete variable setting, which we are discussing today, we have that thing. So, what we can prove is that if x and y are observables and rho is a state, for simplicity, let's assume that both of them are mean zero. Okay, first moments are zero, then you can show that variance of x in rho times variance of y in rho is greater than or equal to one by four times the trace rho i times the commutator. So, here also the commutator comes into the picture, and if you see. comes into the picture and if you see if x and y commutes this is equal to zero right so it just says that that is positive it doesn't it is classical probability again but if doesn't commute you get this relationship a manifestation of this relationship is what we have seen here okay and this is this can be uh thought of as the Eisenberg's uncertainty relation and a proof of this And a proof of this, yeah, I didn't explain the first line. In the first line, you have the post-1 bracket here and the commutated bracket here. It just naturally appears here. Okay. And to prove, it's very similar to the proof of Cauchy-Schwarz inequality, you take the consider this x plus zy dagger x plus zy. And since it's of the form a dagger a this part of A dagger a this part of can use since this part is of the form a dagger a this is a positive operator and rho is positive therefore this number is a positive number okay the trace of that and if z is r erais to i theta you can expand that like this okay you get a second degree polynomial in r and this polynomial what you know is that is always positive okay a second degree polynomial in r which is always positive because of which you know the discriminant should be negative and Should be negative, and from there you get that this inequality. This inequality you get, this is from the discriminant equation, okay? AC, so b square minus 4ac greater than less than or is equal to 0 is your discriminant equation, inequality. And you remove the 4 from both sides because you have a 2 here, and then just do some algebra. You get this thing, and then this right-hand side is maximized when x is equal to x over square root. when x is equal to x over square root of x square plus y square and y is equal to y over sine theta cos theta is that and was sine theta is y over square root of y x square plus y square and that is what gives us this theorem okay very simple uh Cauchy Schwartz kind of proof all right and now the third distinguishing feature which I want to talk is here the extremal state Here, the extremal states, the extremal states means extreme points of the convex set of all states. We know that they are rank one projections, and the dimension of that class, okay, that is 2n minus 2 dimension, okay. While in the classical probability, if you have a n point probability, extreme points are the Dirac, which are just n of them, you know, they have a cardinality of n only. So, this is also very, it is also a divergent, this is. Divergent, this is also where we diverge from the classical probability. And these are some things I wanted to highlight. And then let's have a recap of what we have done. So in this setting of quantum probability, the sample space is a separable Hilbert space, while previously we had a set omega and you can take Cn, which is a finite dimensional space, or L2 of R, which is an infinite dimensional space, whatever you can take. Which is an infinite dimensional space, whatever you can take. And the class, the class of subsets of omega or equivalently, the indicator functions are the events. Here, the events are closed subspace of Hilbert space or the corresponding projection. And the algebra is, you know, sigma algebra is the, you know, algebraic structure here. Here, this is the algebra of the projections. And random variables are real-valued functions on your sigma on your. Your signal on your sample space, and which is actually composed of indicator functions. Here, the random variables are real-valued operators, meaning self-adjoint operators, where the in they which are composed of projections. You know, real linear, in the finite dimensional case, real linear combination of projections give you the any self-adjoint matrix. In the infinite dimensions, you integrate using the spectral projections, you get the spectral. Using the spectral projections, you get the spectral measure, you get the any self-adjoint operators. So that's what the random variables are. And this idea, x takes the value lambda j in ej is in classical probability is replaced by x takes the value lambda j in the range of p j. So when you have a matrix like this, x is equal to summation xj pj, it just means, sorry, one minute, yeah, I am it just. Yeah, I'm pretty sure. It just means that your whole space is a direct sum of orthogonal direct sum of these closed subspaces coming from pj's. And on each of those part, x acts as a multiplication by xj, right? x is just equal to xj in that, in those subspaces. Similar to that, that is what x takes the value xj on pj means. Okay. And the probability mass, we have a measure in general, and here we have a Measure in general, and here we have a state. State means a density matrix. And this is the short introduction to quantum probability. And this has got a lot of applications. And I have referred these books for creating this talk. Thank you. Yeah, I think most of this talk came from this, the first chapter of this book, and also the first chapter of this book. And yeah, I have read those books only. I have referred only this book, so I can say about first two books. Yeah. Yeah. Oh, I'm sorry. Yeah. Paul Andre Mayo, yeah. Thanks, Piker again. Thank you.