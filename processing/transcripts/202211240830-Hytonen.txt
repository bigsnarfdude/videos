With the morning session of day four, uh the first speaker is Thomas Sutanin. Uh, if you're talking about uh diary diary for metal method statements, okay, thank you. Thanks for the introduction, uh, thanks organizers for the opportunity. Very happy to be here. So, I will inspire by some some other thoughts, I will I will begin with the I will begin with the case of Rn, just as uh the case ma maybe the the first point should be should be more or less known to known to everybody more like to fix some notation and the script D is the collection of collection of direct cubes, which is Okay, so now it's n. I might have, let's then say I decide from the beginning that n is equal to d, just in case I have to use d at some other level we have the translates of the Translates off the unit cube and then scales at the level to the power minus k. So now the maybe the two key properties are that each dk is a partition of Rn and then the next one dk plus one refines dk. So as probably So, as probably many of you know, these are very useful for many constructions. But maybe not then the point B some variations of this, what one can do in the context of the Euclidean space. Let me talk a little bit about random direct cubes which I parametrized by a point omega. In a while, I will say. Omega. In a while, I will say, what is my little omega? Okay, I can write it like this now, or I can actually let me write it like this. So it is then. And also parametrize, I have this symbolic plus dot. So starting from any cube in my original system, I have Original system, I have this formal translate, and what is the formal translate? PSQ dot plus omega means that I move it. So here, k is an integer, and now I sum up. Up for as long as this one remains smaller than the side length of the cube, and then I do like this. So, here then, so what is omega? Omega. Omega is an infinite sequence of binary d vectors. So this one here, so in some sense, by q plus omega, I formally translate by an infinite binary expansion. But then at every level, I truncate it. I truncate it out. I truncate it up to the side length of the cube. And the point of point of why I well, I I mean, of course, for an individual cube, it would make a difference if I truncated at a different level. On the other hand, for the whole system of cubes, it does not make any difference. Because in any case, for the now I try a one-dimensional fixture. For example, if I consider the cubes of side length one. And then I translate them by an amount of Q. Of course, for an individual cube, it would make a difference if I translate it here or if I translate it here, but for the whole system of cubes, it does not make a difference. So I can just as well translate it at the level of the side end of the cube. But, nevertheless, I like to parametrize the whole thing by these infinite binary expansions. One benefit out of this is that, of course, this one here has a very natural structure of we can very, in a canonical way, equip this space here with a probability measure. Just put the uniform probability on these ones and then do the usual construction of the probability on an infinite. probability on an infinite infinite probability space or in infinite product space. So this is a can be made into a probability space. So now each some properties of the random systems are that each d omega well is like the original d, in the sense that each Now d in the sense that its dk omega is a partition of the space, its dk plus one omega refines the previous one. So now we have many, many systems. But what is the benefit? Benefit of this, if we look at the whole collection of these dialectic systems, then we can we can get a control of some boundary effects that are sometimes a little bit tricky. Are sometimes a little bit tricky in there if we are using just a fixed dialectic system. So maybe the maybe one key property is this one: that if I I will also use this notation later on in the talks, the epsilon boundary of a set is the points which belong to the set but are epsilon closed. But epsilon close to the complement, and then the analogous thing on the other side, the cubes on the points in the complement which are epsilon close to the set itself. So this is the definition. And then, so now if I take the if I take any fixed point in R. Fixed point in Rn and then I consider the event, let's say also, let's also have a fixed fixed k maybe and then I'm interested in the event. So, this is the probability. So this is the probability probability on this probability space, capital omega. The event that when I select my system of direct cubes randomly, that my fixed point will end up epsilon close. The sum of the any of the boundaries of a cubes on a fixed level. So then one can fairly easy to show that this is just that it decays like epsilon when epsilon goes to zero. It is sort of It is sort of the formal proof is not hard, the intuitive idea is even easier. If I have a fixed point here and I move this picture randomly, these epsilon boundaries make up this kind of portion of the full picture. So the probability of any point x landing close to the boundary The close to the boundary is in this probabilistic sense. Notice that here I have not put any underlying measure. Of course, in Rn there is the Euclidean, there is the Lebesgue measure, would be the natural choice, but here I don't think about any, there is no measure on Rn involved in this one. I'm thinking of in terms of this probability measure. I can have the smallness of the boundary in this. Have the smallness of the boundary in the stochastic sense. And the reason why this was invented, this is a there are several papers of Nassaro Trail Bohlberg from around 2000 to consider singular integrals in the Euclidean space, but with respect to not necessarily doubling measures. Typically, measures of the type. measures of the type where you where the measure of a ball okay and I said that D is equal to N, let's say capital D is some different number, the measure of a ball is at most the power of the radius, but no lower bound. No lower bound. So just the touch the Touch the upper bound. Excuse me. This estimate is you need a K to be negative or. Oh no, no, K doesn't. K K can be positive or negative. Yeah, this looks a little bit. I could write the point is that this is the side length of any of the cubes appearing here. Here, this is 2 to the power minus k and this is epsilon, so the boundary consists of. Epsilon. So the boundary consists of roughly epsilon divided by the side length flaws and if k is empty, right in sign is large or one k. If k is negative. Well, I mean we think of that the estimate is interesting only when epsilon is small relative to this one. Yeah, yeah, it's a trivial estimate otherwise. But yeah, we like to think of the estimate that epsilon is smaller, small relative. Epsilon is smaller, small relative, becomes a trivial estimate when epsilon is big relative to the side. Yeah, indeed, I use this somehow that there are, maybe I might not be completely consistent in my papers, throughout all my papers, whether the dk is with the negative k or the positive k. Now, now I more recently I preferred this parameterization. More recently, I prefer this parameterization that the increase in k indicates increasing refinement, not increasing scale. Okay, but now, yeah, I shouldn't spend the whole lecture in just in Rn, but maybe I there is the there is the point point C that I point C that I Point C that I want to mention. This is what I used to call adjacent. So there is the D alpha and here dK alpha This is actually a special case of these ones, as I will show in a moment, but this is kind of a very particular special case. And here, what is alpha? Here, my alpha is my alpha. My alpha my alpha is um one-third, two-thirds, and two power n. My alpha is any vector with each component of which is either zero, one-third, or two-thirds. And then on the consecutive levels, I shift the cubes by either one-third. By either one-third to the right or one-third to the left. It's important to, in order to have this refinement property, it's important to have this minus one to k factor that we you draw the picture, you realize that in this way you get the same property. So again, well I could write here each each d alpha is like the original d, so on. The original D, so on a fixed level it is a partition of the space, and always the K plus one level is a refinement of the K level. But yeah, omega ones, those are you know zero, one half, one kind of thing, right? All of those are they shift only on to the direct points. But here your alpha is going to shift it on to one-third. Well actually actually let me let me so so actually this actually the d alpha the d alpha is in fact a special case of the d omega for a very particular omega alpha, where omega alpha is so the omegas are infinite binary expansions. Here I have kind of a little bit different notation so that's that's what The different notations, so that's what makes it not immediately obvious to recognize the connection. But here, then let me write it formally like this. So now the binary expansion is either that you repeat 0, 0 indefinitely, or 0, 1 or 1, 0. So the point is that the one-third in the binary expand In binary numbers, this is like this, and two-thirds is like, okay, you have to repeat it indefinitely, of course. So, that's the connection. So, these are kind of the distinguished infinite binary expansions which are as far as possible from the pure zero vector. Of course, why don't I take the why not this one? Because this Why not this one? Because this is just equal to one, which is zero modulo one. So, again, on the level of the full system, I would be shifting all the queues forward to the next queue, but I would be recovering the same system of queues. But with this one-third and two-thirds, I get the ones which are somehow as far away from the original one as possible. And what is achieved in this way What is achieved in this way is that for a fixed alpha it is just like the original one, but then collectively I achieve the following property. So if I have the, let's say, any ball for any ball in Rn there exists a cube A dyadic cube in one of these one of these systems such that my ball is contained in the cube, but the side length of the cube is only a constant bigger than the bigger than the radius of the ball. So, this is this is now this is the benefit of. This is the benefit of the adjacent systems. That of course with just a fixed system you could do something like this, but now in a way the poles are kind of up to constant, fully comparable to cubes, but nevertheless you only need finitely many of these dyadic systems and each of the individual And each of the individual systems have these nice covering and refinement properties. This is, by the way, also sometimes called the one-third trick for obvious reasons. Okay, now I spent quite a while in the Euclidean space, so now that's the point. The point of the talk is that basically all what was here presented here can be done in a can be also done in a metric space. Assume X D metric space doubling in the metric sense, obviously, because I'm not imposed. Obviously, because I'm not imposing any measure. So, any ball of radius R can be covered by at most n a fixed number of balls of radius r halves. Okay, so the point A was just the dianic cubes. So now I claim that there exists B So Tk a partition of X consisting of oral sets Tk plus one refines Tk And okay, this is something like this would be done even maybe on there is no no connection to the no connection to the metric yet. The important thing is that these are still so for every q equals q k alpha so I this is just the way I write the q's in the in the I write the cubes in the in the in the case level we have there is some point which I we can think of as a center point that the Q K oh fuck q k alpha contains a ball roughly of radius delta to power k and it is contained in a ball of the same center and a bigger constant. This is supposed to be a small C, a capital C, again delta K. And for natural analogy, I refer to the delta K as the To the delta k as the side length of the cube. This is just a notation in a way. I cannot define what is the side of a cube, but I still call it the side length of a cube. These are some borel sets which are roughly like balls of radius delta to power k. So here delta is some small number in practice. But kind of it's kind of But kind of it's kind of like the analogue of one half in the in the Euclidean context. Actually, one could, if one insists, one could do the construction in such a way as to get the delta equal to one half here, but I but in many arguments it's more convenient to take delta rather small small number. small number. So this is the this is a so so so so something like this exists in a in a good level of generality. Maybe the the the the the main main reference for this is is Michael Christ from 90. It's not the first one nor the last one but it has the kind of the essential generality. There were some constructions of GDAVID earlier in a little bit more the Alphors regular method. Regular metric spaces, and then in fact, the risk construction still refers to an underlying measure, but that's which is not completely essential, but for practical purposes, the essence of the matter is in this paper. Ah, maybe maybe before I move to the random annexation, there is one other one other thing that I should One other thing that I should mention about these ones, which is then so if X D has put it this way, so emphasize that if mu is any doubling measure on X D On X D, then the so this is kind of a sort of a big geometric property of the cubes, but then there is also a they have the following smallness of the of the boundary property. Well, it depends on xd view and it's it's positive in general quite quite a small number. Of course this is yeah in the Euclidean space it would be it would be one in general we we cannot also I I don't know how to how to get it uh get a very very good number. A very very good number good number here. On the other hand, for the applications that I have cared about, it usually the fact that there is even just even just something positive is quite enough. I mean, but that's of course biased by the problems that I have been considering. But for example, the point is that if you think of in a Euclidean space, integrating something like x to power. Something like x to power n is a problem because it diverges. As soon as you get here, even a small eta, it's okay. So that's sort of a very vague reason why even a very small improvement gives you something. Sorry, you said that there is a delta for which we have this nice partition, or for any delta we have nice delta. Actually both are true, but one needs to work a little bit more for the other one. I think that it's more natural to take a very small delta. The other the other the doing it for a for a bigger bigger delta is a little bit little bit artificial and and does not does not really not does not really yeah so i maybe i maybe i prefer to prefer to state it in state it in this way that if you if you take a small enough delta depending on the parameters of the space then that goes fine the point is that you could somehow artist once you have done it for a small delta you can kind of artificially fill in the remaining scale between the sort of you think that in Rn you would have constructed the diagram Would have constructed the direct cubes of scale one and then scale to the power minus one hundred and then to the power minus ten thousand and so on. And then you kind of fill in the gaps in between afterwards. But it's that the construction is most natural to carry out with a small interview. Okay. We now both do the inter x's and and the and uh well actually yeah maybe the maybe I I but I don't need to spend spend too too much too much time with this so too much time with this. So on the abstract level, you can just think that, again, there exists a probability space. You can do the basically the same thing that I showed in Rn. You can have this D omega each D omega is like It's like like the original D and then collectively you have what you had in the had in the Euclidean space that the probability that a given point X ends up not like this, but so Q in the U in the K and the epsilon boundary of these ones smaller than epsilon divided by divided by the side length of the cubes in question, and then you have some. You have some theta here. But here, actually, yeah, I purposely use a different letter because here the nice thing is that you can, okay, theta depends on the various parameters with xd. Okay, now no measure involved. It also depends on what delta you choose in the construction, but you can. But you can so, okay, so it is between 0 and 1. If you here, it's useful that if you choose a very, if you, well, if you want it to be close to 1, you pick any number close to but not equal to 1, if you choose delta small enough, you can achieve a boundary regularity as close to as close to power one as you like. I don't know how to I don't know if also this is one open problem they might discuss discuss later. discuss later whether not by the construction that I know, but maybe by some some more clever way it's it is possible to get exactly one in here. And then yeah, the adjacent systems are actually again the same thing each okay alpha in some some A finite each the alpha Each d alpha is like D, and collectively, for any ball in your space, there exists Q in one of these systems such that P is contained in Q and the side length of Q is controlled by the control. Controlled by that radius of the bulb. Can I ask, so on RL you were achieving the omegas and the alphas by translation? How is this done in the metric space? Very good question. Yeah, let me try to in the remaining seven minutes give a little hint of how everything is done. Yeah, that was maybe the maybe the have to take away something. Ah yeah, maybe I like erasing, I give some credits. The random construction, the first version I did with then my PhD student, Andrew Martikainen, who is nowadays based at the Washington University in St. Louis, and the first version of First version of the adjacent systems, the finite number of adjacent systems with my back then my PhD student Anna Kairema. Maybe, yeah, even if I'm short of time, I can't resist that in a little anecdote. Some years back, somebody asked me, how is your student, Anna Karenina, doing? Going. So, and Karenina is a fictional character of Lil Tolstoy. That's not my, unfortunately, not my favorite. So, it's not a problem. Yeah, so my student was called Arema, not Karenina. Yeah, okay. But yeah, so how is this constructed? So, basically, the crisp construction starts from the It starts from the so we have this it starts actually starts actually from these collections of collections of center points and this is like a delta k net in x and then x x k plus one refines x k so there are points So there are points on the level K, and then there are some more points on the next level, and even more points on the next level. And now we want to construct the cubes in such a way that these ones become the center points. And the idea is that among these pairs k alpha, we pairs k alpha we introduce a parcel ordering which satisfies the following two basically two axioms one is that if the distance of a point on the next level if one of these points is very close to one of these points so if this one is smaller than some small constant c to power delta k constant C to power of delta K, then we so this is just a formal partial order between these kinds of points. And on the other hand, if this is true, then we require the opposite opposite inequality, but with a bigger constant. And once we have this kind of a puzzle order, then we have sort of a pre-cube, which is just a countable set is going to be the collection of all points x, l theta such that this one is below k alpha. And then the closed And then the closed dielectric cube is just the closure of this one, and the open dielectric cube is the interior of this one. And then it's possible to choose kind of a half-open version, which is somewhere in between. But in a way, so in the initial stage of the construction, so for we choose some of these ones and then each of them picks some of the some of the following ones and Some of the following ones, and then thus we proceed. But maybe I but actually, now if we look a little bit, now let me change a little bit the notation for this. Let me call this R little RK and capital RK and I call these the inner and outer inner and outer radii in the construction. So these are kind of So, these are kind of three parameters that we can. We have a little bit of freedom in picking these. And so, the point is that the cube will roughly consist. In any case, if I want to construct the cube at this point, I take everything which is in the inner radius. In their inner radius, and all of these also take everything within their inner radii. But then I also want to, but then I have the outer radii, and then it becomes a little bit more complicated because these ones will be overlapping. So let's say that for I need to introduce kind of a priority order between my points, but it's a countable set, so I can just think some order them and enumerate them in some order. Let's say that this is. them in some order. Let's say that this is the first one. So for this one I pick everything, I always pick everything within the inner radius and then everything within the outer radius except what happens to be already inside some other inner radius. And if this is my second point, I do the same. So I pick everything within the inner radius plus everything within the outer radius minus everything that was inside some other inner radius. That was inside some other inner radius or inside the outer radius of some point with a higher priority. And I proceed like this. So this is the, so kind of my first level approximation of cubes will be. So this is one of my but this is not yet the final cube, but this is one of kind of a a pre-cube. a pre-cube and this is the next one. And then of course the it it will I I will do a because then I have the points here and around them I will do a do a similar construction so that so this will get perturbed on the next level and again it will get perturbed on the following level. So the so the final final cube will have this uh subsequent levels of of perturbations on top of the original approximation. Of the original approximation. But now that now the point of the random cubes, very roughly speaking, is that I take my Rk is little C times 1 plus delta omega K and actually capital R K is the same with the capital C and here my And here my little omega k is chosen at random from the set one, two. And the random cubes I achieve by picking each little omega k independently of each other from this distribution. So it is once again an infinite product. So, it is once again an infinite product probability space. Whereas the adjacent dyadic cubes I achieve by taking just one omega zero like this, and each omega k is equal to one. So, this is a finite probability space in which I achieve the adjacency control. Just with the final echoes of the bell I'm concluding, thank you. Thank you very much. One thing that comes up in certain things related to combinatorial modulus is that you don't using, well, you use often thyadic cubes or some versions of them in defining, say, combinatorial loader property, but then the annoyance is that these are not quasi-symmetry, invariants of their quasi-symmetry. Invariants and requasi symmetries because they don't preserve exactly scales. So, is there a version that you're aware of that say used in harmonic analysis of dyadic cubes where the cubes are not always of the same size, but say the neighbors are always comparable and they're always comparable to their parents. And that would be variant under quasi-synergies. Okay, I don't immediately know of a version like this. Um on on the other hand it doesn't seem like an possible challenge to construct. But yeah, I I I I'm I'm not able to give it up myself either. Oh, we're already going in. 