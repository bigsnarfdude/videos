For the introduction, and uh, thank you, Tinio, for uh, in my opinion here. Um, it's been a pleasure so far. So, uh, I'm going to talk about PDs on Wasserstland space, and I will arrive at these PDs in three different problems. And one of the problems that I've worked on for the last couple of years was prediction problems, and somehow those also let the carbolic PDF. The power of the PDs that you see in control microsoft systems. So I just want to first start with that and see how much time we have and what we can do. So this is a joint work with Ibrahim McCrand, who's at the University of Michigan, and Xinjiang, who's a former PhD student, who is now going to start at NYU, really, as he was at the University of New York. He was at the University of Vienna for the last couple of years. So just keep on the slide. So here is a brief upline. So I'm going to talk about these prediction problems. First, discuss the full information setup and then look at the adversarial case, which will lead to these failures of such nice space. And then we'll talk. And then we'll talk about the PDs of water shared spaces are going to be second-order partial differential equations, in the second order in the measure. Then we'll talk about how to define Viscouse solutions for this case and prove uniqueness like in HEZ demo. So first let me start with the prediction problems. So this is one of the first setups on machine learning. Machine learning. This is called Expert Problem. It's a game basically on adversarial setups, commercial game. So there are two players in this one. So, first, there's a forecaster. And there are some series of experts. So they are enumerated from 1 to K. And there's a forecaster. So forecaster chooses an expert. So the goal is to forecast. And then there's an adversary. And the adversary chooses a subset of these experts. And adversary has And adversary has powers, so it sets up the gain of the experts chosen to one and the others to zero. And the gain of the forecaster will be G of IT, right? And the forecaster's goal is to minimize its regret. So this is, you can either write it cumulatively, these variables G or little G. So you can write it in both ways. And so hindsight duplicates. So he looks at So, hindsight duplicate. So, he looks at the gains that he obtains. So, this is G G G T, and then the maximum again expert obtains, right? So, there are two K experts. So, this is the maximum over the K experts, so hindsight K, and tries to minimize it. So, no probability here, just like minimize the regret. So, the objective of the adversary is to make the regret worse, and then you want to minimize it. I'll minimize your return, right? So, this is we are not making any statistical assumptions on this, right? There are no statistical assumptions on the data. It's an adversarial setup, thank you. So, this is a commonly used criterion for the gap of optimality with respect to a constant strategy, right? So, these are strategies basically, if you think about it, these are constant strategies. I always choose expert one, two. Expert one, two, or K. So these are constant strategies. How will I do against constant strategies with right? That's what it's trying to do. And this is one of the, as I said, one of the first frameworks for online learning. We, in fact, have some connections to financial math as well. You probably heard about Tom Coward's universal portfolios. So, in fact, the first expert problem was discussed by Tom Coward, not accidentally, in the 1960s. So, these are So, these are some applications that it has, right? So, let's look at two different setups that I'm going to consider. So, the full information setup basically is that the forecaster observes every expert, right? Observes the gains of every expert. These are, there are, so these time here is the rounds, right? So, rounds of the game. So, these are, this is the round one, t equals one. Capital T is the round t. So, they keep playing this game. This game. So let's see where we are. Okay, so I'm going to, so it's going to be, of course, the gap will be or the relevant variable. So to each expert, I assign a gap, xi. And so I can recast the problem in this fashion, right? So it turns out that they have to both randomize. That's where the probabilistic framework comes in. So alpha is the strategy of the adversary. Of the adversary, alpha for adversary, and the other one, beta. And we try to basically minimize. I mean, this is our regret function, right? Pi of XT. So there's an extracted value now because they both have to randomize. In the second case that I'm going to talk about, this is the adversarial case. So, for example, multi-armed bandit setting, you can think about it. So, in that case, you basically only observe the expert that you pull. That you pull. So you otherwise you don't actually observe the experts. So you choose, meaning the expert you choose, you observe, the others you don't. And again, this time this relevant state variable is not expert, but it's actually a conditional dissolution of X. You can write your problem now in this fashion. So in terms of the conditional dissolution of X, the relevant state variable becomes that object. So people have asked a lot of questions on this. So what is the optimal algorithm? So, what is the optimal algorithm for minimizing regret? What's the optimal regret value? And they're interested in, people are interested in here, not my interest, but people have been interested in the growth rate in T. And does the optimal algorithm have a description, good description? And there is something known as exponential weights algorithm, and the exponential weights algorithm is very easy to compute. Is very easy to compute. So, how does it compare? So, does it is it worth actually following the optimal algorithm, which might be difficult to compute? So, how does it compare to the exponential weights algorithm? And what are the hardest sequences adversary can give me? So, these are the types of questions people ask. So, let's so we already discussed the gains. So, the gains of expert I is increased by I if he's chosen by the adversary. Remember, I adversarial choose a subset. Remember, I'd also choose a subset of these, and the game process doesn't change if it's not chosen. And the forecaster only has a gain if he chooses the right expert. So again, we have a min-max problem here, and the min-max holds here because these are the criteria is nice and. And we can define a value function for this problem, right? So, once we can define a value function, we can think of it as a game control problem. So, we can try to tackle it. And the nice thing is that, so remember, they are interested in how it behaves asymptotically in T, so we can then scale things nicely and you have some expected, to expect it to behave. It turns out actually the growth is square root, so it's not linear and T. is square root, so it's not linear and T. You can do actually better than linear. So there's a good strategy. So you do this parabolic scaling. And then once we do the parabolic scaling, you can of course can write a dynamic program for this problem. When you do that, you get something like this. And then you do some heuristics, right? So you could actually expand this. And what you see is actually that when you expand this, then T is large, right? So when you do the heuristic expansion here. Right, so when you do the heuristic expansion here, you see that actually this beta has to be the gradient of u. So the one of the forecaster strategies completely determined, and then this problem which started the game actually reduces to a control problem, right? So the control problem. And the control problem is a bit degenerate here because here E, J are the vectors whose entries are one, then it belongs to J, right? Right, uh, so this is a very digital problem, and of course, uh, if I just said the value function is a viscous solution of this, etc., then you know, they wouldn't care about it. What we actually end up doing is computing this explicitly. So, one can compute this explicitly for k equals one, two, three, four. So, we did the four, we actually were able to compute for four. So, so before before before, like in the this is like. Before, like, in the, this is like this interest in this somehow peaked from 2016, 17, 18, like in Microsoft research. Before actually, they had results for when both K and T go to infinity, right? And multiplicative experts actually optimal in that case. And then people were asking, okay, so how about finite number of experts? So that was a question. It was asked by UL Perez when he was at Microsoft Research and his group actually were asking these questions. And then here's a We're asking these questions, and then here's a group from Berkeley. Peter Park Ledin's group, they were, they considered the case for three experts, etc. So they basically use property of random models to do this. What we really said is use actually complex analysis, stochastic analysis, and PDs, so mainly PD tools, which is not enough, but this is the starting point. So we have a PD. We want to solve the partial solution of the partial differential equation. So what the growth of regret is this. So, the growth of regret is this, actually, and you need to determine this value, right? So, you need to solve the PDE, and the optimal learning algorithm actually exists. So you can verify this in the hindsight once you get these. And there was something called home strategy that we showed was optimal. So, we had to compute this PDE. Surprisingly, we were able to do it. Like, I there are not many cases where you can actually compute it. And the solution of the PDE is actually. The solution of the PD is actually like a couple of slides. You can fit it, but we were able to do this anyway. And we did this without using this because somehow even they point out that you really need finer and finer and finer properties of random walks. So we didn't need this. We somehow were able to study this stochastic, use stochastic analysis called plex analysis. Use stochastic analysis, complex analysis, PDs, etc. We solve this. And main tool basically is for us to get scaling limit and talk about PDs. Right, so that was not the full information setup in the information setup in the case where you don't observe all the experts, only the expert you chose, right? So your information structure is a little bit different. Your information structure is a little bit different, but still, the of course experts, even they're chosen by the adversary, they're incremented by one, right? Same strategy. It's just that what you observe is whether you chose a good expert or not, right? So the only observation you have is that whether you chose a good expert or not. And then here also, in adversarial bandits, there are some results, how things grow with respect to T and K. How things grow with respect to TNK, right? And these are actually Sebastian Hubeck, who is a principal now at Microsoft Research. He has some lecture notes describing these things. So we want to tackle this problem with also with PDs. So they did for sodio regret, et cetera, that kind of stuff. But we want to actually look at how the regret looks like. So we. So we now the update algorithm for our conditional distribution actually looks like this. So whether you choose a good expert or you don't choose a good expert, you have a random evolution of your distribution, right? And here, these are actually just Bias in Baya's formula applied. So maybe I don't get into the details, but this is basically Bayesian computation, right? Basically, by as in computation, right? So you can, this is just shift here. Let me not get into explaining that all that much, but there's a random evolution that you can describe for this conditional distribution of x. Then you can actually again write down a problem, a dynamic program principle for the problem. So for the dynamic program principle, you have min-max, but this time min-max. So you have mean max, but this time min-max theorem doesn't hold because now you have really weird dependence on these controls, right? It depends through this A, and that's not at all nice. So we have to make a choice. So you want a min-max or max-min, so you can compute the analysis. So upper, lower value of the scale. So it's lacks convexity. So again, you can do the parabolic scaling, right? So you heuristically write. So, you heuristically write this equation down again, and then you can actually expand, right? You can do Taylor expansion again. And then, so you do this, you keep the leading order term, right? And then remember, they want to compute this, what the coefficient in front of this t, square of t, you do the expansion, and then you write down the equation for this. So, for this equation, now we need some more tools, and people know this here. Some more tools and people know this here, right? So we need derivatives in one suction space. There are two types of derivatives, right? So this is the linear derivative, what's known as the linear derivative. You use the convexity of the space of proto measures to drive this. And then there's something called Wasachland gradient, or sometimes it's called Lyons derivative. So it's a derivative with respect to x of this. So this is basically the This is basically the this is actually more relevant somehow for us because it's somehow the sensitivity along push-forward directions. If you think about that, the meaning of this one. And we need second derivatives, second order Pe that we're going to talk about. So you differentiate it twice, and this one is that. And then there is something called Walsarshan-Haitian. So Walsarshan-Hacian is given by this red equation here. So that's you hear it whenever you see a bracket. You hear whenever you see brackets, so there are variables here, x and y, and now you integrate them with respect to the measure m. So that becomes, so this represents the average with respect to that measure. So this is the Wasashan-Hessian. It's going to become important for us later on. And again, this is second derivative along the push-forward direction. So I'm going to come back to that. So you do the scaling limit, right? So you do this. limit right so you do this you take the limit you get uh this derivative that i introduced right so you put the limit of this is equal to that the limit of this is equal to that you plug it back and then you have this then you look at the further error you know second order term right so first order term is given by this one over squared of t and then the one over t term both way right so the further error is given by uh these second second order derivatives so we naturally come to a second order differential equation We naturally come to a second-order differential equation that right in the measures. So you do that, yeah, you do some computations, back of the ML calculations, and then we basically write down a relevant equation, and the relevant equation looks like this. So you see this Walterschlines showing up, and then this first, I mean, second order, but first order measure type of derivative. And then you come to an equation, and now, and you can actually, so first. And you can actually, so first you can say, okay, so we have an equation, but what is it useful for? Right, uh, what we can do is we can actually look at super solutions of this equation, and that would give us some strategies for which we can, you know, have, so we can, using which we can provide upper bounds, right? So it's square root of t times pi, zero, zero. I have a smooth super solution. I calculate it. Here's the strategy would achieve phi zero zero. Would achieve phi zero zero squared of t. I have the strategy and I have an upper bound for it. So you can actually say, okay, let's look at simple super solutions. So phi is of this form. And then you plug it, plug it in, you get an actually easier equation to solve, right? This is now the type of partial differential equations you know how to handle. So then you can construct smooth super solutions and get some upper bound series. So we do this, and then you can actually beat Beat you know, beat to the regress balance that were obtained, but with a caveat, which let me put this under the right. So, here our adversary actually has to learn a little bit. So, a forecaster has to learn a little bit of the distribution of adversary. It's not exactly the adversarial bandit setup. So, it's like you can think of it as a gang of bandits. But that's a little bit technical. So, let me so you can do if you do that, like put cookies on people's computers. Like, put cookies on people's computers and sort of like get averages. Like, let's say you're surveying people, right? And you don't get their identity, but you sort of get their overall behavior. Like, you have like lots of adversaries and your forecast is maybe trying to forecast people's likes and stuff. So, you can put cookies into the computers of people, get statistical information, and then you can improve actually your regret. All right, so as you can see, we come to a So we arrive at a PD, right? So we could do some solutions, but then you could insist: okay, I have a PD. I want to analyze this type of PDs, right? So this is now the collection number one. So this is where we first came to this. I was working on MiField game receptor, but generally I wasn't even using the master equation approach. The first time we actually come to the master stuff as through this. The Washington stuff as through this, so like, okay, so why don't we analyze this? Then we, of course, then look back at other problems, right? So, one of the problems that you have, another problem that actually shows up this type of nature is filtering problems, right? So we go back also, like you have less, probably not necessary, but you have a stochastic control problem, right? You choose alpha adapted to both V and W. Choose alpha adapted to both V and W. And your usual stochastic control problem, you can write a Hamilton-Jacquel-Bellman equation like this, right? This is the bread and butter of people in our community. But then you say, okay, I will restrict my alpha to be only adapted to W, right? Then all of a sudden, this tractability goes away. And now you need to compute conditional distributions. And that becomes your state variable. And then you can actually. Variable. And then you can actually write down using some tricks: write down a Hamilton-Jacob-Galvin equation corresponding to this equation. And what you see here is again a PD in the Wattershine space, right? So immediately when you change how alpha is chosen, you can write down this. This is well known to people. What you see, of course, here is that this is a second-order equation again, right? And you see the Wasserston-Hessian showing up. So it's a common. Showing up. So it's a common thing. Post-shot hexion showed up again. Okay. Another problem that you see is the mean field control problems, right? So you have a particle system of n particles. You put the empirical distribution and then you do a control problem on this. So it's not mean field game, but mean field control. Then you say, okay, what's the limit of this? The limit of that will solve again. That will solve again a PD on the Wasserstein space, which you have seen many times before the conference. All right, so the key thing here is, so it's a second-order PDE, right? So people have studied obviously. It's an interesting PD, right? So especially to the McMillan association, when you actually don't have this noise component, right? In the mean field control, the system, you don't. Mean field control, this is you don't have a noise component, right? Then people have attempted to derive notions of solutions for this. Like we are in Farm Andrea Kosso et al. recently gave some nice approach to defining discuss solutions, the weak solutions for this thing. Second order somehow was elusive, right? So the approach was one of the first order so that we are like, okay, so we are coming across all these interesting problems. Across all these interesting problems with second order. So, what do we do? Right? And in all these problems, the quantity that shows up is this one, right? And this actually has an interesting structure. In fact, it can be written like this. So you take a push forward of m with x, right? And you take the derivative, second derivative with respect to x. You do the computation. It's actually the Plausion Hessian. What does it tell you? What does it tell you? It tells you that actually it's a finite dimensional, it has a finite dimensional structure, right? It is, I can, you can see that I can actually, you know, the right-hand side, which looks complicated here, right, or like this one, left-hand side, I guess, is it can be computed more simply, right? So, okay, so let me introduce some notation. Here I'm going to center things. Here I'm going to sample things, right? Because of this, I'm going to sample things. So, Walshstein distance is equal to the means, difference of mean squared plus the Walsh Lein of the non-sent the centered distributions. Then, basically, what we can show is we have to talk about like a particular class of test functions. So, these are the partial C2. C2 and then define basically the viscous solutions in the normal way. And then we can actually get a comparison principle for this PD. So we can show actually under some certain assumptions on H, these require some regularity assumptions, et cetera. Our class also has to be, and this is what we want to do, but we need some, you know, we need to restrict our class for like at this iteration, right? So. So let's look at before we go ahead, let me mention some literature. So Hilbert space, this is a long, like also papers on Hilbert spaces. But the Viscusa theory works almost with Verbaton for Hilbert spaces. But we don't have Hilbert spaces. We don't assume we have densities for these distributions, right? So there's no Hilbert space approach. We are no more such density space. We are on a section space. So, in it, other people have looked at this type of equations, right? So, what you see is the lack of second derivative. And then Gango-Mayorga CVA looked at this, this version. So, it appears linearly. There is a nice paper by Martin Larson and friends where they have that structure, but they are doing this for marking yields. Martingales, there is some martingale thing model, and there are very specific problems that they do, and their solution, notional solutions are stable somehow. But they have some nice results here. Because you discretize the marketing, you can stay within the marketing, so you approximate it in that way. You can reduce the problem to finite dimensional basically somehow in that case, directly. Again, Goes is VA. I repeat it myself here. It's myself here. And then we have this problem, right? Bandini, Kosto, Pulman Fund, they have this filtering problem that they looked at. Again, they try to do this by lifting into the Hildebrand space. So there's a technique introduced by Leos, right? So they try to do this, but there are some problems with the discussed solution, like a classical solution instead of a discussed solution. But that's kind of problems apparently. So we want to directly tackle it. So what you do is basically one accomplishment. So, what you do is basically run a comparison principle, right? So, we uh so what do you do for a comparison principle? You do doubling of variables argument, right? So, you look at a classical one. So, in the classical case, you just subtract one over two epsilon x minus y squared. Use the doubling arguments. Important things are that you look at where the maximum is attained, and then you want this to go to zero, right? And you do issues that. And you do ishi dilemma. Basically, you do subconvolution on vexify things and the idea, right? So, even now you compare there, but there's subtract to two things, et cetera. You do this type of approach. We can do the same thing here, right? So, you want to do the same thing. So, main difficulty is that actually we work on a space, P2, and the normal, the thing that the metric that we have here is actually. Actually, it's not nice, as it's not like this, it's not differentiable. So, you have to put something smoother, right? So, and this we got this idea from Metasolar. So, there are nice smooth metrics here for your Wasachland metrics. So, we put that there. So, that's nice. So, we centered it. We changed it a little bit, right? So, to do this, of course, you have non-local compactness for this. You can use variational principle. Use a variational principle, right? So you're in a loss action space, you could do variation principle. So you could do other tricks as well, right? So we realize later, but there are many other tricks you can do. One of them is variational principle. And this is motivated by KOSOPAM, etc. And we don't have an issue lemma, right? So in this space, so what do we do? Because we're not in Hilbert space. Well, we realize the finite dimensional structure of H. Is the finite dimensional structure of H. So, in fact, the problem for the problems we consider H is as a finite dimensional structure. So, here is the metric that we use. This is variances, this means, and then this is the, we have like a nice tool formula for the Fourier Rosserstein metric. So, this is what we use. It's topologically equivalent to W2, but not complete, et cetera, but doesn't matter. You can actually differentiate this nicely, right? Because this is what we want to compute, right? So, eight hashian of rho of hash. So, a Hessian of rho of F easily computed. And this has a nice structure. It's just a finite matrix. So basically, here are the steps. So we start, okay, so each is Lemo. Let's say that this attains a maximum m star and star. You transform it, right? So this is a very crucial step. You transform it to a different function, V1, U1, where you don't have this anymore, but you actually have. You don't have this anymore, but you actually have the difference of the means, and they obtain the same maximum at the end, star, and star. Now you move to a finite dimensional dimension by taking the su, right? And once we have the finite dimensional structure, we know Ishii's lemma here. We apply Ishi's lemma. We get the jets of U2, V2, and Z star with the star, and both work backwards and get the jets for the jets or derivatives. Jets or derivatives for VU for ORCS. So that's the proof structure. And that's it.