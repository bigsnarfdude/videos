Thank you very much, Thomas. It's awesome to be here. It's always great to come to Mexico. It's always great to come to Oaxaca. Happy Independence Day, viva MÃ©xico. So like Thomas said, I care very much about understanding what networks that we train in the wild sort of compute and what these functions really are. Today, what I'm going to do is focus on sort of an unsupervised problem in the context of. Supervised problem in the context of English problems. So, to begin with, I want to put this up for you. This might be a little bit controversial. Many of you might have seen this already. I'm just going to go ahead and read it out. It's from the blog post, The Bitter Lesson, by Rich Sutton, who many of you may know. So, Rich writes, the biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective and by a large margin. Seeking an improvement that makes a difference in the shorter term. That makes a difference in the shorter term. Researchers often seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is leveraging computation. We want AI agents that can discover like we can, not those that contain things that we have already discovered. And this is kind of indeed a bitter lesson and very sobering, particularly for many of us in this room, because we often take a lot of pride into sort of understanding domain knowledge, distilling it down to some basic analytical principles that we can go. Analytical principles that we can go and apply into the real world. And it sounds like, according to Rich, we sort of shouldn't worry about this so much so long as we can scale computation and data up. So from a pragmatic point of view, it's hard to argue with this, but a little bit more philosophically, it's a little bit disappointing because what we are left with is with these sort of not transparent boxes and functions that are. And functions that are doing really nice things. In fact, sometimes they solve problems in better ways that we could solve before, but we have learned very little about the problem that we wanted to solve. And it's hard for us to understand how is it that these functions actually allow for the solutions that we are looking for. And this is not just an intellectual problem. It's also very much a regulatory problem. This is a screenshot of the fact sheet of the presidential office of the White House from last year. Basically, From last year, basically putting forward a bunch of guidelines in terms of how we should think about regulating AI if that ever comes to exist. And there's a big stress here on understanding robustness and understanding also aspects that have to do with opacity and complexity of these models. So these are things that in general I care about a lot. But today I will be focusing on inverse problems, which is something that most everybody in this room, I believe, is familiar with. Familiar with, these are settings where. Oops. I think these buttons aren't doing what they're supposed to do. Okay. So these are settings, as you know, where we're trying to recover some underlying vector, in this case, X star, that has undergone some imperfect measurement process. We're going to assume that we know the forward operator A, and there's potentially some noise. And all of these settings that you're seeing here are essentially. these settings that you're seeing here are essentially instances of these inverse problems such as super solution or mri reconstruction or maybe you went on vacation with a partner and you're no longer with them and then you need to uh take them out as it just came out actually in a conversation five minutes ago um regardless um i think you can appreciate that this is an imposed problem in other words there's an infinite number of solutions that are possible and so the question is how do we pick among those that are measuring and the change Measure. And the traditional way to do this in this variational form is to pick some regularizer function that hopefully sort of biases us towards solutions that we believe are likely under whatever model of the world we have. And I just want to stress that the reason we do this is, again, this is nothing new, is because what we are really trying to compute is a map estimator, right? So what we're trying to do is find the solution x that maximizes. Is find the solution x that maximizes the posterior probability given my measurement y. And this is true as long as this regularizer is essentially proportional to minus log p of x. So indeed, the regularizer should be inversely proportional to how likely are things to appear in the world. Okay. So it sounds like to solve these problems, we need to pick a model of the world. We need to pick a regularizer. Of the world, we need to pick a regularizer. So, what have people been doing? Well, if you go back 100 years and ask someone, you know, give me something that describes natural data, maybe they would say, well, you should ask for some things that are low in L2 norm, perhaps as opposed to white noise, for example, which has very high L2 norm. And maybe noise, someone else, but I'm not going to say who. That's not going to take you very far. And so maybe you want to ask for some L2 norm, but maybe under some operator. And maybe the L2 norm is also not amazing. Maybe you want to refine something. And maybe you want to look at things like total variation norms. Or maybe you want to change this Laplacian and look at wavelet frames. And then you ask for sparsity in the wavelet coefficients. Or maybe you want to generalize this even further and ask for over-complete dictionaries. Or over complete dictionaries and look for the dictionaries themselves. And of course, this is not a complete picture of all the models that are out there. There are many other notions that people have used to formalize what it means for an image to be likely under this distribution of natural images. And right around the time when I was sort of doing my PhD, deep learning came along. Bastin, do you want to take a guess on that? Bastino, do you want to take a guess on that? And on the one hand, it was great because it allowed us to solve, as I said in the beginning, many problems, particularly the inverse problems that we were trying to solve for a long time, way more efficiently and to a better extent than we could solve before. On the downside, though, it got us to be a little bit more lazy because now, supposedly, we didn't have to think about good models of the world. All we needed to do is just provide a bunch of data. Is just provide a bunch of data and these models will figure it out somehow, but we never got to see what that good model of the world is. So, what I'm going to try to do today is present something that hopefully bridges these two ways of thinking about inverse problems. Okay, so we do want to use deep learning. We do want to use these tools that perform really well. How have people been employing these methods for inverse problems in particular? So, the first thing you need to do. So, the first thing you need to do is give me a bunch of training data. Xi, remember, are the ground truth, yi are the observations. And then, if you give me enough of them, I'm going to search for a function, f theta. By the way, for the rest of the talk, whenever you see theta, these denotes parameters in a fixed hypothesis class bounded in some norm. So, fine, I can just train a map from n to n, n is the dimension of my data, and in fact. Of my data, and in fact, if you tell me the forward operator, I can also benefit from it and then already project back onto the span of A and sort of just learn their residual, if you will. And this has worked really well. Other people have done this, particularly in the early years of using deep learning and signal processing and inverse problems. Now, there's another thing that you can do, which is a little bit perhaps closer to our original framework, which is Original framework, which is to say, well, I still want to do this sort of variational type problem, but now I want the regularizer to be data-dependent. I want it to learn from data. Once you have it, you can plug it back here and then run your favorite optimization algorithm and then be done with it. Now, there are many ways to do this, and people have proposed many different approaches. One of them is to say, well, I like this because it just conveys what the idea of these regularizers should be. What the idea of these regularizers should be. You want R to distinguish good things from bad things, natural things from not natural things. And so, literally, you can train a network to do that. You can, let's say, optimize. Sorry, this should be F, doesn't matter. But basically, what you want is the risk of a predictor whenever you see samples from the P of X, from the correct data distribution, you want this value to be low. And if you observe samples from a different distribution, Q, you want the value of the regular. Q, you want the value of the regularizer to be higher. And so you can train this sort of critic that will tell you, yes, this is likely minimal prior. No, this is not likely minimum prior. And you can do many other things. You can restrict your, you know, you can literally be this, an indicator function so that you are always in the span of a generator, like DeMakis and many of his collaborators have done. You can do regularization by denoising and many other approaches. Okay, there's a third. Okay, there's a third group that I call sort of implicit priors, which is the following. Let's assume for a moment that we knew what R was. If we knew what R was, and let's say that we're not going to assume that it is smooth. In fact, we're not going to assume that it is convex at all for the moment. Then a reasonable thing to do, if you give it to any grad student nowadays, is they would run something like Is they would run something like a proximal gradient step where you take a gradient with respect to the smooth part of the loss and you compute the proximal of your function r. Okay, and just as a reminder, this proximal operator is nothing else than computing the solution of this little simpler optimization problem, where if you give me a vector u, I'm going to return to you a vector x that is close to u, but also minimize. You but also minimizes R to some extent. So there are a couple things to note. The first one is that this problem is a simpler problem from the one above, right? So here there is no linear operator A. And second, and perhaps most important, this is simply a denoising problem, right? Because the four operators just an identity. So the only thing that I'm observing is a noise corrupted version of my. noise corrupted version of my input x. And so under this measurement model, this denoiser is simply computing, like we said before, the map estimate of the unknown given our measurement. Okay. Great. So about 10 years ago, people made the observation that, hey, if this proximal step that was here in PGD, and by the way, when I say PGD, I'm And by the way, when I say PGD, I'm really just using Proxima Gradient Sent as a running example. But if you like other optimization algorithms, if you like have quadratic splitting or ADMMs, these proximal operators appear in those settings as well. So all of the things that I will present apply to those algorithms also. So, okay, if this proximal step was playing the role of a denoiser, great. I can just go and go to GitHub and grab any of the latest denoising. Grab any of the latest denoising algorithms, and they keep improving because, likely, people keep working on denoising algorithms. And so, just grab the latest and greatest neural network model or diffusion model, perhaps, and then plug it in here. And then you get now for free a solver for the original inverse problem that you wanted to solve. And this goes by the name of plug and play and has been very, very, very successful in a number of scenarios. And by and large, the And by and large, the better the denoiser people use generally improves the solution of the more general inverse problem that you wanted to solve. Okay. So the main question here is that if I go to GitHub and I put something, there's really little to no guarantee. In fact, there's no guarantee that any noiseless that I grab will actually be computing the approximate. That I grabbed will actually be computing the approximate operator, which is what I needed, for me to compute the actual map estimate for my inverse problem. And even if there was one, well, for what regularizer? And so that's exactly what I want to study in the next few minutes that we have here. So the first question that we want to ask is: if you give me a function f that is acting as a denoiser, when will it come? Noise there, when will I compute a proximal operator? And for a function r, and in particular, I want to know what that function is. Ideally, I want to be able to query it. I want to be able to analyze it. And this is not enough because, fine, even if I can compute a proximal operator via a neural network, I don't care about any proximal operator. So the picture here is that we have all functions from Rn to Rn, and I care about the subset that only computes. About the subset that only computes proximal operators. What I also very much would like is to recover this correct prox, which, like we said, is the negative log p of x. If I could choose, if someone gave me an oracle, I would pick this as my regularizer because this is the right thing to do in this Bayesian type way of looking at the problem. Okay. So, if I write, let's say that I'm trying to estimate the maximum of my posterior, and I decompose this via base rule, I will end up with p of x in one of my terms, right? If I apply logs, then this p of x turns into this regularizer. This is a little snippet that we had a few slides ago. And so the regularizer should be proportional or inversely proportional to. Inversely proportional to x. The more likely something is, the lower the value of the regularizer should be, and vice versa. And the reason I'm saying correct is because we'll get there, we'll get there. This is a wish list. Any other questions? Okay. Okay. Okay, so the first question actually turned out to be a lot simpler than we initially thought in light of some recent results. So there's this very nice paper by Remy and Mila Nicolova. And this is a simplification of their results. But in a nutshell, what they show is that if you want your function f to be a proximal operator, a complete characterization of these functions is actually functions f that are the gradient. That are the gradient or in the sub-differential of a convex function. So, this picture is the one that you should keep in mind. We care about finding a function f from Rn to Rn, which is our denoiser. We also want this denoiser to be a prox for some function R, from Rn to R. Excuse me, from Rn to R. This function F will be a prox if and only F, it is also the gradient. It is also the gradient of a convex function. Now, here's the nice thing. And this is perhaps why this is slightly different from characterizations of proximal operators that you've seen before, due to Moreau. Moreau's result is a bit more constrained than this because it holds for convex functions R. This does not. So So, this is true even for even when R of X is non-convective, which, as I will argue in a moment, is going to be very nice because it's really going to allow me to characterize prior distributions that are not log concave. Okay, so basically, the answer to our first question is pretty simple. All we need to do is pick a function psi. Psi convex, and we're going to make it differentiable as well. And just take the gradient map of that function. And by construction, this function will always compute a proximal operator. Okay, so how do you do this? How can you find a neural network that computes a convex function? Well, lucky for us, there's a lot of people have been working on this problem already. This is one of these networks by Zico Coulter's group. They go by the name of input convex neural networks. input convex neural networks. In fact, these are so these timely functions can only generate convex functions. And in fact, they're universally approximators for convex functions. If you are interested in the details of the architecture, it doesn't really matter too much. What you care mostly is some of these weights have to have non-negative entries and the activation functions have to be non-decreasing. And if you do this, Decreasing, and if you do this, you end up with a function from Rn to R, which is always compared. Okay, great. Now, great, we have understood how to obtain neural networks that give us proximal operators. They should just be the gradients of convex functions. Great. What if I now want to know for what R, what is a regularizer for which they are proximal? Well, there are. Well, there is an expression that relates all three things. So the regularizer, the f, and the psi function. And the details of this, they're not important. Don't worry about it. The only thing that I'm stressing here is that in order to compute R, if you were to give me F, is that you need to invert your procs. You need to invert this neural network that computes this proc. And that sounds like it's a scary thing, but actually, because of all the structure that we have, it's actually a very simple problem. A very simple problem, and it turns out to be just a convex optimization, in fact, strongly convex optimization problem. So, if you write out the optimality conditions for this problem, you realize that just solving this convex optimization problem allows you to compute the inverse of your props. So, great. If you now give me an F that is a proximal operator, you can invert it, just run conjugate gradients fast and And with by running this thing, you can compute the value of the regularizer. And I will show you examples of this in a moment. Okay. The second question. Let's see if I can answer Dustin's question. So, great, we want a function that computes a prox, but we also want one where this prox is. One where this prox is actually computing the maximum of this posterior, just like our ideal map denoiser would do. And of course, we don't know P of X. So I'm going to take a la machine learning approach and sample. And so I'm going to construct myself measurements Y from data X and Gaussian noise D. Assume that I can sample from this. I'm going to assume that I can sample from this. And then I'm going to say, well, I've now understood what this class of functions that only give me proximal operators. I'm going to search over that family of functions. And I'm going to minimize the risk, which is just the expected loss of how well can my network, given measurements wide, reconstruct my ground truth peaks. It's just a denoising loss, a denoising problem. All right, great. The question is: what loss function are you going to use? And these seem, at least in the beginning, we thought that this was really a sort of almost a trivial question, but it ended up being very important. So if you were to use an L2 norm, this is really the wrong thing to do, because this L2 converges to the MMSC estimator, which is the mean of that posterior. But we want a prox that computes the maximum of the posterior, not the mean of the posterior. So the L2 is the wrong thing to do. You can try with an L1, which is very popular on sort of these plug-and-play type methods, but also you're going to approach something that resembles a median in high dimension. Okay, so sort of the main result of this work is to propose a family of loss functions. This is just one instantiation of them. And again, don't worry about the expression. Them and again, don't worry about the expression, it's just one minus a Gaussian, right? This Gaussian is controlled by a parameter gamma, which tells me how spread it is. And so the result says the following. Minimize your function, or search for the function that minimizes a sequence of problems where the parameter gamma. Where the parameter gamma for these loss functions goes to zero. So basically, as you shrink this progressively to zero, and what you are converting to is basically a now zero norm. The solution to that problem is this F hat star. And the result is that that F hat star precisely does what you wanted it to do. You recover the maximum of the posterior, which is Posterior, which is the prox of the minus log p scaled by sigma squared, which is the standard deviation that you used to train it, that you used to sample from. Does that make sense? Yes. No, no, no, no. So this is only true for this loss function that I'm showing. It's a very different minimizer from the one that minimizes the L2, and I'm going to show you comparisons between them in a moment. Yeah. Okay. All right. Okay. So after we have this, we basically did what? We basically did what arguably anyone in this room would do. Let's take data from a Laplacian. So I'm going to sample X from a Laplacian in 1D. And I like a Laplacian because the log gives me the L1 and I know what the prox of the L1 is. It's just a shrinkage function. And I'm going to corrupt it with V, with Gaussian noise. And I'm going to train this denoiser. And again, nowhere in this process, I'm using the fact that the data. I'm using the fact that the data came from a Laplacian. All I'm doing is just training a denoiser with this specific loss function. And the class of functions that I'm using are only computing proximal operators always. All right. So in dotted line, you have the shrinkage function, which is the correct prox. In red and blue, you get the solutions, which are also proximal operators. Which are also proximal operators by construction, but these are the ones that you would get if you were to minimize the L2 loss or the L1 loss. And you see that they don't basically don't recover, they don't approximate, they shouldn't approximate the correct proximal. And if you switch to this PM proximal matching loss that I just presented, then you start converging to the correct proximal as gamma goes to zero. So, because this is precisely the comparison. The comparison. So, indeed, there's a big difference of whether you use the minimizer of the L2 versus this. I mean, one recovers the mean of posterior, the other one approximates the mode. Yeah. Yes. Excellent. Excellent. We don't know. So the result that I presented before, I'm sure you noticed. I'm sure you noticed, it's not a finite sample result, right? So the very relevant question here is precisely that. How should my samples scale with gamma? Even if I use a finite gamma, can I still say something? These are relevant questions that we don't know. So I promise to you that I can invert the procs. The procs. So here it is. In the dotted line, you have the L1. And so I just take my compute this expression that I showed you before, and you start very much approximating the correct L1. Whereas in the other cases, if you just minimize an L2, you basically converge to something like the Huber law. Okay, so this works in toy problems. Here is more toy problems, but I promise that this will show something cool. So we train adenoiser on digits. On digits. And then, because I can query the regularizer, I can now learn a lot about the data distribution. So, for example, if you take a digit, a nice-looking six, and you push it through this thing and compute the value of R, you see that it is 2.2, whatever it is, not normalized, doesn't matter. But then, if you increase the amount of noise, the value of the regularizer goes up exactly as it should, because these samples are not likely under the distribution. In fact, you can do some more cool stuff. I can take two good look at. I can take two good looking digits, a four and a one, and the value of the regularizer for both is kind of small. And then I can take a convex combination of these two points. And for any point in between, this is not a likely number. And indeed, the regularizer goes up. And this only happens because we're able to model regularizers that are non-convex. This wouldn't happen otherwise. You can do more. You can do more cool stuff. You can inspect your regularizer. I'm running out of time, so I really want to just tell you something to end. Because we've trained a denoiser that is a proximal operator exactly, there's a lot that you can say about optimization as well. So when people care about claiming optimization results for these type of solutions of problems, typically what they do is actually just care about convergence of this algorithm that is parametrized by some denoise. And in general, you And in general, you can't really say much about this problem because you don't know what this R is, except in some cases. In our case, that link is completely clear. And so what we can show is that if you use proximal gradient descent or ADMM, you will converge to a fixed point of that problem. And remember, it's a fixed point because the regularizer is non-convex. Lastly, Lastly, of course, the goal was to have a sort of all-purpose denoiser. So, indeed, you can plug it back in to whatever inverse problem you have. This is for de-blaring and denoising. And it performs basically about the same as any of the other plug-and-play stuff. But you have the added benefit that you can query R, you know that you're approaching the map solution. These are also results for SparseView CT. sparse view CT and for compress sensing. And there's really a big difference in these cases if you actually compute the map. Okay, that's basically everything that I want to say. I want to go back to this sort of my first slide where I think there's a whole bunch of problems that are really interesting at the interface of sort of using functions as black boxes and then putting minimal assumptions on them so that you can still say something about what solutions they are computing. They are computing. And like I said, and to Mauro's question, there's a lot of subsequent questions that we have about this type of learned proximal networks. That's all. This is work, by the way, of my PhD student, Shen Chan Fang, and Sam Buchanan, who's a research assistant professor at TTIC. Thank you. Excellent. Yes. So the question was whether we can use this to do generative modeling. And the answer is yes. In fact, so you will have noticed that out of the body of work that has come out of diffusion models, for example, the take-home message is that if you can estimate the score. If you can estimate the score well, which is the gradient of log p, then you can sample. So, here we're not estimating that, we're estimating something different. We're estimating a prox of log p. And so, the question is, what can you do with this? And when is it useful and when is not useful? And we're working on this, and I'll be happy to say more offline as well. Yeah, it's really interesting to actually talk about it. When you said that, I was actually I was I thought you were going to go a different direction with that. So can I ask why? Sure. Let's see. Maybe I didn't think of the direction. So the way you start to talk is you want to perhaps find a good regularizer, but you don't want to bake the data in right now. Don't want to bake it in, right? And then you have this story about those great problems, which are always going to use these by doing range of sense proxy regulators. So why train your proxy regulators on synthetic data? Why not use the actual label data, XIYI, where like each iteration, you have an XIYI, you're going to update your ICNN based on how well you're going to process. I think so. So, what you're suggesting is if I just want to understand what you mean by label, let's say that label is the actual observations that I have of some ingress problem. Is that what you mean? Okay, yes, you can do this. And people in that, in fact, have done this. So you can supervise this problem in some sense by taking into account the forward operator that you will use at the coordinate. You can definitely do that. In general, you do better in those specific problems. Do better in those specific problems. The reason we didn't do that, though, and there's a very good reason for it, in my view, is first, in all these questions, if you really want to compute the map, what you should compute is minor log p of x and the prox of that. And so y has no role there. And so I want to have something that really estimates the prox of minus log g of x that doesn't require y. That's one. That's one reason. The second reason is: if I train a denoiser for some distribution, then this is a general tool for any inverse problem. You just have a new problem. In fact, it might be non-linear. Fine. In the end of the day, when you run your algorithm, ADMM or whatnot, you will have to compute a prox. This will be your plugging thing for that. But in practice, what you're suggesting does work better. Yeah. That's a good question. Thank you.