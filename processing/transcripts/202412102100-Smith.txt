I want to tell you today about some ideas I've been working on together with Paula Belzig, Li Gao, and Tishu Wu. It's on the archive just in the last week or two on reverse-type data processing inequalities. Equalities. So, what we're interested in here is looking at how the distinguishability of two states is preserved or not when we apply some noise process to those states. So, we know actually that data processing tells us that if I do some processing on a pair of states, the distinguishability of the two states is going to decrease. States is going to decrease. And the goal of this talk and this work is to kind of say: well, if noise decreases distinguishability, how much distinguishability can persist? And we mean this in sort of a And we mean this in sort of a fairly specific and quantitative way. And the motivation, at least on my side, is that we can use answers or partial answers to some of these questions of this style to tell us about questions in quantum Shannon theory and specifically to help. And specifically to help us understand and construct examples of sort of well-behaved channels that we can evaluate certain capacities for. So we can quantify a kind of a couple of types of answers to the, or one type of answer to this question that I'm not really going to look too much. I'm not really going to look too much at today, and maybe the next speaker will is we can look at contraction coefficients. So the contraction coefficient of some channel N is given by just the supremum over states, pairs of states that are not equal of the final distinguishability divided by the initial distinguishability. Distinguishability. And this is nice because if you have such a contraction coefficient that's strictly less than one, then this means asymptotically, if you keep applying the channel over and over and over again, eventually they become indistinguishable. And this is the sort of thing that has been looked at by Girge and Tomi Michel, and also Hia and Ruskay, among others. Um, among others, and you can kind of sandwich this guy in between like the trace version of this contraction coefficient, um, which is just given by this. You take supreme now over basically differences of states of the image of the thing under the channel divided by the initial trace norm of the thing. Initial trace norm of the thing. And you better have trace zero. And you can sandwich this guy in between the trace, oops, the trace version of the contraction coefficient, and its square. What we want to look at, and this is sort of like, well, look at the And this is sort of like, well, look at the pairs of states and find the ones that preserve the information kind of the best or preserve the distinguishability the best. And that's why it's related to the asymptotic development of indistinguishability and it lets you get sort of nice balance on multiple applications of the channel. What we're going to look at, or what I'm going to tell you about today, is kind of the Well, if this is called the reverse of this, this is called the contraction coefficient. I want to tell you about something called the expansion coefficients instead. So we're still going to look at the relative entropy, but instead of taking a supremum, we're going to take an interim. Hello, Ingram. Yeah. So in the right bottom corner of your board, there is still another line. Of your board, there is still another light that is reflecting off it. So, okay, just a moment. Now, there is another problem that might arise. But can you see okay? Yeah, so everything has become a little. Yeah, so everything has become a little dark. Yeah, we can barely see you. Okay, okay. Well, I think we may have to live with the one on the bottom right then, because I'm in a room with all the lights off, and then the lights just coming in through the wall over here. I think this is just very. This is good. Oh, I see. All I can do, look, I'll. All I can do, look, all I can do is either have these blinds up or not. Sorry. Yeah. It's okay. It's okay. Okay. Go ahead. Yeah. Sorry. Yeah. Okay. So we want. Just avoid the patch where you wrote Supremum last time. Yeah. Like here? That's correct. Perfect. Okay. Okay. I'm sorry, not only did I am I, well, anyway, I couldn't find my pen for my iPad, so I'm doing, I'm improvising again today. Okay, so we want to look at expansion, what we call expansion coefficients. And if I have some channel n. And if I have some channel n, the expansion coefficient or we call it eta n check is just the infimum over all pairs of input states that are not the same of the relative entropy between the images of the thing, the states, under the action of the channel compared to the relative entropy between the two initial things. The two initial things. And if this thing is positive, it means that the channel preserves at least some constant fraction, some fraction of the initial distinguishability. And one thing I should say about this is this is no longer related to the trace version of the same question. So there's some eta-check trace that could be positive even when this thing is zero. Even when this thing is zero. And what we're going to find basically is this thing is zero an awful lot of the time. And as a result, if we want to capture the sort of feeling that, hey, if you start off with a certain amount of correlation or a certain amount of distinguishability in a pair of states, you should preserve at least some constant fraction of it. We're going to have to kind of change what we look at. We're going to have to let go of the idea that probably we can hold on. That probably we can hold on to a constant fraction of the initial distinguishability. We can only do that if we introduce what we call relative expansion coefficients. So basically, one of the results I wanted to tell you about is most of the time, like if you have channels that are not unitary channels, so they're not well, well, they're not unitaries, but they have equal input and output dimension, then unless they're Then, unless they're just a replacer channel or a perfect unitary channel, this eta check is gonna be equal to zero. So it's somehow too crude a measure of how much distinguishability you preserve under the action of the channel. And in order to get around this, in order to sort of soften the fact that this is always either zero or one, when the dimensions are equal at least, we introduce what we call relative. We introduce what we call relative expansion coefficients and relative contraction coefficients. So now, instead of a single channel, we have a pair of channels, now N and M. And typically, what we're going to think is M and N are in some semi-group, and M is sort of M, N. M, n rather, is a less, n is a more noisy version of m. So, like, this might be a depolarizing channel, and this could be a depolarizing channel, and this is just like more of a depolarizing channel. Then we can kind of look at how much distinguishability does N preserve compared to the amount preserved by M. So we can introduce the relative contraction coefficient or relative. Or relative expansion coefficient in this case, as the infimum over pairs of states of the relative entropy between the image of the pair under the action of N and the image of the pair under the action of M. And you could also introduce relative contraction coefficients with just a supremum. Coefficients with just a supremum. And this turns out to be kind of the right thing to look at if you're interested in the applications we have in mind for quantum Shannon theory. So let me just summarize what the main results are, and then I'll tell you just a touch about the techniques that we use. Yeah. So the D on top and the relative entropy D are the same thing, or? No, no. No, no. This is sorry, they're not. This one has little curls on the end, but let's call it something else. This is a CPTP map that can degrade M to N. So we'll call it big E, curly E. And this is just the relative entropy distance between these two states. These two states, but so the point is not mentioned explicitly on the left, in the either as a subscript or something. No, that's right. It's not because basically, sometimes if n and m are very, very different, you can find states where like where sort of this goes to zero and this doesn't, or this goes to zero and that doesn't, and it just becomes either. And it just becomes either infinity or not. So, this really is only a useful thing to look at when you can reasonably sort of imagine that the noise processes are similar. So, M is like a less noisy depolarizing channel, and N is like a more noisy depolarizing channel. Are we fixing the E in this definition? Are we fixing? No, we're not. Are we fixing? No, we're not. We're not even. This is kind of an aside: like, these are the channels where I can process this to get that are the ones where we're going to expect this to behave in some reasonable way, like in particular, be between zero and one. Got it. Yeah. Yeah. And we don't have a full handle on what the, like, we're pretty sure that it needs to be like N is a more noisy version. N is a more noisy version of M for this thing to be like not zero and not infinity, but probably you need more than that. And we're not fully sure what it is you need more, but it's probably something like they live in the same semi-group. Okay, so what are our main results about these things? Well, main results. Main results. One, we have sort of estimates for relative expansion and contraction coefficients. We call if Oh, I erased it. We call sort of a positive value of this ADA check a reverse type data processing inequality because it tells you kind of instead of the relative entropy must go down under the action of the channel, it tells you the relative entropy, at least some amount of it is preserved. So it doesn't go down too much. So we have criteria, some criteria for which. Some criteria for which this thing is going to be strictly positive. I'm sorry, I just lost my spot. Oh, good. Criteria for positivity of the relative expansion coefficients. We have some explicit. We have some explicit calculations, explicit calculations for three kinds of channels, for the relative contraction and expansion coefficients, for depolarizing, dephasing, and amplitude damping. And we can use we can use these expansion you can use these these these calculations and use the our knowledge of relative expansion and contraction coefficients to find a state A state, sorry, a channel, psi, that is what we call less noisy, less noisy, but not degradable. And I'll tell you in a moment what those things mean. And finally, we have sort of a conjecture. There's a conjecture. There's a conjecture if we could do similar calculations for the sort of the complete version of these contraction coefficients and expansion coefficients, by which I mean, yeah, you just here you have identity tensor N, and identity tensor N, identity tensor M, identity tensor M, and you do the infimum and the supremum over states, including some. States including some reference system. If we could show positivity of certain pairs of channels, well, certain amplitude damping channels relative expansion coefficients, then that would also give us, well, we have conjectures for what the values are. And if we could show positivity of this thing at least, that would give us what we call informationally degradable channels. Degradable channels, channels that are not degradable. And that's important because informational degradability is a, well, degradability, which I'll tell you what it is in a minute, is a criterion that allows us to calculate quantum capacities of channels. And essentially, all the channels we know how to calculate the quantum capacity for are degradable. Informationally degradable channels. Informationally degradable channels are a class of channels that are at least a priori broader than degradable channels, but we don't know of examples of informationally degradable channels that are not degradable. But if we had them, they also have additive coherent information, so we can also calculate the quantum capacity of those channels. So it would give us new examples of channels that are not in the old class, the degradable class, that we can still calculate the quantum capacity for. So that's So that's a little bit the motivation down here, the motivation are the motivations for looking at these coefficients. And then we kind of ran off and had a lot of fun calculating what we could about them and trying to identify potential conjectures that are maybe possible to attack that can really tell us interesting things about additivity in quantum Shannon theory. So, are there any questions about this so far? About this so far. Also, are you all sitting in a room somewhere, or is everybody online? No, no, we are in a room. There are more than 14 of us. Yeah. Oh, there are 14 online and maybe about 30 in a room. Okay. Oh, by the way, I have a question. But does anybody last time I went to India? I came home with a bunch. Went to India. I came home with a bunch of paper money, and then they canceled all the paper money. Did they uncancel it yet, or is it just garbage? Yeah, yeah. I mean, you can keep it and sell it after 10 years, and it'll be worth more. Okay, okay. All right, the degradability and less noise. Let's look. So, degradability and less noisy channels. So, if I have a channel N, I can always express it as Express it as an isometry followed by a partial trace. And the complement of the channel is just the environment's view of what happened. So I trace out B instead of E. And degradability is about the relationship between the channel and its complement. We say a channel is degradable. Degradable if there exists what we call a degrading map, D, that's completely positive, trace-preserving, so it's a channel, such that at the output of the channel, if I apply the degrading map, then I can get the complement. So it means that the output of the channel has strictly more information or has all the information that the complement gets. And this is of interest because this implies the quantum. Because this implies the quantum capacity satisfies a very simple formula. It's what we call the coherent information. So you just maximize some formula over a single use of the channel. So a channel is degradable if there's a degrading map. A channel is less noisy if for any CQ state, For any CQ state, row XA, and you act on A with the channel, the mutual information between X and B is always bigger than the mutual information between X and E. And data processing of mutual information tells you that all degradable channels are less noisy. Classically, you have a similar kind of definition. Classically, you have a similar kind of definition, and you can find examples that are less noisy but not degradable. And that's much harder to do in the quantum case. And one of the things that we find using these reverse contraction coefficients is an example of channels that are less noisy, but not degradable. And yes, please. Yeah, so it looks like you have many subsystems. So there is an A, there is a B, and there is an E. So they all And there is an E, so they all coexist. Oh, sorry. The channel takes input A and it gives out output B and the environment of the channel is E. So A does not coexist with B and E. You just, A is the input, and then B are the outputs. And here I have an X that keeps track, that sort of acts as a reference system. So I have a pair of systems, X and A, and I Pair of systems X and A, and I act on A with the channel to get rho X B E. I use the isometry, and now I measure the mutual information that X has with B and X has with E, and it's less noisy if the mutual information with B is always higher. So just to complete the picture above, there is a horizontal line labeled X or Yeah, and I'll make two horizontal lines because it's supposed to be. horizontal lines because it's supposed to be a it's supposed to be a classical system here um and then finally the if that system is not classical but instead could be quantum we say that the thing the channel is informationally degradable informationally degradable that means for every row va the mutual information between v and b needs to be bigger than or equal to the mutual information between v equal to the mutual information between V and E. And this is an interesting example. This is an interesting situation because this is sufficient to show additivity of coherent information and then also get a simple formula for the quantum capacity. But we don't know of examples that fall into this category informationally degradable that aren't also degradable. We have examples or we have conjectures. Or we have conjectures about where to look for these. We have constructions that should work, should work, as long as we can show certain complete relative contraction coefficients and expansion coefficients satisfy the right values. So, what I would like to do next is just tell you a little about what we've calculated about. What we've calculated about these relative contraction coefficients and why we know that the sort of non-relative versions of them kind of misbehave and also give you a little bit of a flavor for the construction of these less noisy and conjecturally these informationally degradable but non-degradable channels, what they look like. I guess. Maybe, yeah, let me argue first that let's say if I have n. Okay, here's one of the theorems. Let n have input dimension dA bigger than or equal to input dimension dB, and it's not purity preserving. That means That means it has at least one input that is pure that gets mapped to a mixed state. Then the expansion coefficient of that channel is equal to zero. And you can see. And you can see that if it is purity preserving, actually, the expansion coefficient is going to be equal to one. But that's not really the juicy part of this. The juicier part of this is showing that basically all non-trivial channels with inputs at least as big as the output don't satisfy a reverse type data processing inequality. Reverse type data processing inequality. So let me just remind you: this eta check of n is infimum of rho that's not equal to sigma, the relative entropy between the image of rho and sigma under the action of n and the initial relative entropy. So this tells you that there are pairs of states for which having some initial Having some initial, basically for which you never preserve a constant fraction of the initial distinguishability. And what you do typically to show that you have an expansion coefficient that is equal to zero is you find a pair of sort of a sequence of states that are approaching each other where this thing goes, this thing rather goes. This thing rather goes to zero much faster than this thing goes to zero. Or put it another way, you find one state and you perturb away from it, and you find that these two things have different behaviors under that perturbation. So the proof looks like this. You find a projecture, you can find a projecture projector. PA on the input, that is not the full space. So the dimension of the projector is less than the total input space. And you can find another state psi that's orthogonal to the projector. And it has the property that the support of, well, the projector. Well, the projector, I'll call it K0. That's the dimension of, that's this dimension. The support of the image of that projector, actually, let me write that differently. Rho naught equals the projector over its dimension and the support of the state that you get by feeding Rhone. State that you get by feeding row naught into the channel is actually equal to the support of the state you get by feeding row naught plus this new state into the projector. So you can see that at least in terms of supports, rho naught and rho naught plus psi, they look pretty darn similar at the output of the channel. And what you do is you let rho, you What you do is you let rho equal rho naught, sigma epsilon equal one minus epsilon rho naught plus epsilon projector onto psi. And when you do some calculus more or less, and you'll find that in this ratio, you get zero over zero, but you take some derivatives, you take the ratio of the derivatives, and you'll find that this derivative goes to zero. This derivative goes to zero faster than that. So the whole thing goes to zero. I believe I have like two minutes left. Is that right? Or do we? Yeah, let's say five. Let's say five. Okay. Great. So this is why I guess we sort of need to move to relative contraction. To relative contraction, sorry, relative expansion coefficients, if we want to say something non-trivial about the infimum of this ratio. And the reason we want to say something non-trivial about the infrimum of the ratio is that it tells us a lot about a family of channels that I want to just show you. They're just flagged mixtures of fairly simple channels. So let me write N P. It depends on P. Oh, shoot, I shouldn't call it N. I'll call it F, but it's still a channel. Look at that F. And it depends on P, N, and M. And it's just the flag mixture of. Flag mixture of N and M and M complement. And we'll choose N and M to be degradable. And furthermore, we'll have the degrading maps D1 and D1. We'll have the degrading maps D1 and D2 for those channels. And basically, first let me argue that this is a channel. Yeah, what's M complement? Ah, M complement is the map to the environment of M. So M is trace over E U M rho U M and Um and m complement is you trace over B of rho. And the idea here is: well, if n is degradable, this term, you know, you're going to be able to degrade. When you get this term, you can degrade to the environment of this term. And furthermore, if P is very big, And furthermore, if P is very big, because any degradable channel is less noisy, the mutual information of this term will kind of dominate the bad behavior of this term, the sort of low mutual information of this term. And furthermore, even so, if you have any non-zero part of this term here, then this kind of terms. Then this kind of breaks the degradability that exists just for that. So, this term is introduced to break the degradability because M complement, the complement of a degradable channel, as long as the degrading map is sort of strictly contractive, then that is sort of not a degradable channel. So the whole thing is not degradable. But the idea is if most of the time I get this, that sort of dominates the That dominates the value of the mutual information that you can generate with the overall channel. And in fact, turning that intuition into an actual proof is basically what leads you to these relative contraction coefficients. So if the relative contraction coefficient of n with respect to m is positive and the degrading map degrading map and the degrading map d1 has strictly has contraction coefficients strictly less than one then then there's a p there's a p such that this channel here is it's less noisy but it's not degradable And with just my last moment, I want to tell you about a sort of a conjecture for making this even more interesting by making the relative contraction coefficients complete. So if we use the same thing, but we can have, we introduce now the complete relative contraction coefficients. So The complete relative contraction coefficients of n and m that's just equal to the supremum over all states rho va of the mutual information between v and v1 and mutual information between v and b2, where these are the outputs of the channels. We can also define this complete relative expansion coefficient. It's the infrimum over again. It's the infimum over again all states rho VA of the mutual information between V and B1 and mutual information between V and B2. Then the same construction, as long as this thing is strictly positive and the contraction coefficients of the degrading map, the contraction coefficient, the complete contraction coefficient of the degrading map is strictly less than one, then again, there exists a p that may There exists a P that makes this channel not degradable but informationally degradable, which is a sufficient condition for additivity of coherent information, which lets us calculate the capacity, the quantum capacity of the channel. So specifically, if we have N and M be N and M be amplitude damping channels. This gives very sort of concrete and specific types of construction for a channel that is not degradable, but certainly looks like it is informationally degradable. The challenge for us is to calculate these complete expansion coefficients because this infimum is indeed an infimum. We have to worry about having an arbitrarily large dimension. An arbitrarily large dimension on this V system, the numerics we do suggest that it doesn't matter if you have a bigger and bigger system. But in order to prove that, well, we have to be able to do things a little bit smarter than what we know how to do at the moment. I think it's sort of, we have reasonable conjecture for what this value is going to be, but we don't yet have a proof that we can get a cardinality bound on the V. And for me, that's. Get a cardinality bound on the V. And for me, that's, I think, the most interesting open question that we've left in this work. And I'd be happy to answer questions if you have any.