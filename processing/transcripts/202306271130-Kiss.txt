Which you can read here. It's 2D tag, a large 2D expandable, trainable, experimental computer tomography data collection for machine learning. Well, a lot of words, a lot of adjectives. Why are they here? Let's look at some motivation for this. Well, large number of research in computational imaging focuses on employing model-driven or model-based data. Or model-based data-driven methods. So, basically, artificial intelligence and all of that needs a lot of data. Experimental, realistic data is very, very scarce, but very, very important to develop and train deep learning algorithms. We have some examples in the field. So there's, for example, the fast MRI data set that many of you know, which is actually one of the only ones that actually give away raw experiments. Actually, give away raw experimental data. But we also have others like the low-dose CT Grand Challenge from the Mayo Clinic, which was at the beginning a very small sample space. Now they are very big with 300 patients, but they still are based on simulated data only. And we have the LodoPup from Bremen that is known for many of you as well. Again, large size, I think 40,000, 50,000 slices at least. At least, and again, only simulated data. So, no real data. And that is hard if you want to use that for model development. We also developed one at the Central Viscount Informatica, which is for walnuts, the Walnut CT data set for Combium CT artifacts. But again, there are always limitations with these data sets. So either the sample size is small, we only have simulated projection data, or there are limited application possibilities. Or there are limited application possibilities because of the modality, or because it's only like for one type of artifact reduction or similar. So, to summarize, we have a lack of data set that is in 2D CT slices and can be used for versatile deep learning applications. If you want to acquire such a thing, solve that problem for people, you will run into some challenges. So, the three major challenges that we would run into as like Would run into as like a community is most research group do not have a scanning facility readily available. And that is extremely necessary if you want to acquire time-extensive data collection processes. And second is most available scanners that you have as research groups, they don't allow you to access raw experimental data, or they are very limited in the way that you can change acquisition parameters. Change acquisition parameters or geometry, and simpler. That also limits the possibility to serve different applications if you cannot do that. And the third one is that most available scanners do not have the ability to automize your processes. So if you want to acquire a huge data set and you would have to be in there in the lab all the time clicking on the machine, that is very cumbersome and limits again the number of CT slices that you could acquire. Of CT slices that you could acquire if you don't have that. Luckily, at the Central Miscundent Informatica, we can solve that with our FlexRay scanner, which is a highly flexible CT, micro CT scanner. You can program that in all these parameters that you can read here. But also, most importantly, it is scriptable. So we developed an in-house software where we can get scripts, where we can run experiments without human intervention for multiple hours and acquire a lot of CT slices. And acquire a lot of city slices. Okay, if we fix this, now let's note down some aims that we want to serve with this kind of data collection. So, first one, we want to provide raw experimental data. So that must be available, and we don't want to rely on artificial data simulated with varying degrees of freedom. Then, there we want to have real-world applications. We were thinking especially about medical CT scans or like Medical CT scans, or like trying to resemble to some extent medical abdominal CTs with our measurements. We want it to be expandable, so a reproducible setup that at some point there might be an interest of the research community or of ourselves that we want to expand the data set with either additional slices with different mixes, or we want to expand the like level of classifications or segmentations that are available within the data set. So that should be possible as well. And we want to serve. Possible as well. And we want to serve versatile application areas. So you can see we want to say, like, we want to use it for denoising, for sparse and limited angle scanning. We want to have it for beam hardening reduction, super resolution, region of interest tomography, segmentation. I think some dream of yours might be fulfilled in one of this. So how did we do that? We have to look into three key features of the data set for the data acquisition, basically. One is we need a scanning object and the scanning. Is we need a scanning object and a scanning sample mix. Second, we want a good scanning setup and experimental design. And we will look into the semi-automatic feature of the data acquisition such that you can understand how that to detect data collection was developed. Well, let's look into the scanning object and the sample mix. First of all, our aim was to produce images that had similar image features and contrast as abdominals in medical cities. As abdominals in medical CT scans. And we wanted to have like a diverse enough sample mix that has a high natural variability in both the inter- and intra-sample shape and density, and is also stable over a long time, because if you scan that with a lot of radiation over hours and hours, and they are not temperature stable, you will run into problems. So, what did we do? We took a cylindrical copper tube 34 centimeters high and a diameter of 10 centimeters, which Of 10 centimeters, which represents the body. We took centimeter-scale objects mimicking organs and bones and submersed all of that in a background medium resembling the connective tissue. What did we pick exactly? Like after a lot of testing throughout the months, we finally picked dried fruits, banana chips, dried figs and raisins, nuts, almonds and walnuts, and coffee beans because of their And coffee beans because of their all of their temperature stability. But we also introduced lava stones for bee margining artifacts, and everything was immersed, as you can see in this picture on this side, in a cereal-based coffee powder. We tested various different objects, but also various filler materials, and that proved to be the most effective. It feels very healthy. Yes, it also looks like cooking, but don't feel cooking. Also, challenges that are not noted down is: well, if you scan something like that and you have beaming artifacts, lava stones in the whole sample tube and radiate that with 90 kV, 90 watts, we basically also melted the cereal-based coffee powder once. That was a very fun thing to do and clean up the tube afterwards. And there are things like detectors breaking down and like changing them in between. Nothing that you had to be concerned of, but in the end, it works. But in the end, it worked out. Let's look into the second thing: the scanning setup and the experimental design. Well, I already told you the different application areas, and now I will mark a few of them, red and green. And the green ones, they are basically available for free if you get the scans with a high enough resolution and a high enough number of angle projections. So we were thinking, okay, what is the problem? So we were thinking, okay, what is the minimal requirement of our angle projections that we need to have? We used the Nicholas-Chanin sampling theorem, applied to CT scanning, look for the number of projections for the scanned pixel sizes of the detector times pi half, so 1912 available pixels on our detector, and we get to roughly like 3,000 projections. So with 3,601 projections per slice, we are way oversampled and can. We are way oversampled and can definitely have enough angular sampling. Second one is resolution. So, what do we have to look into there? Well, we have to choose for three objectives, sufficient photon flux, high resolution, and a good detector coverage. So, if we have like this setup, let's say, we look into two parameters, especially, the source to detector distance and the source to object distance. And if we increase the source to detector distance, our beam geometry. Sort of detector distance, our beam geometry gets more parallel, but we also decrease the photon flux and therefore we increase the noise. If we want to mitigate the increased noise, we would have to increase the exposure time, the scanning time. So the data acquisition time increases a lot. So we did not want to do that. And for that, we found basically in the end a resolution with this formula where the magnification factor is Where the magnification factor is determined through the ratio of the source to detector distance and the source object distance, we basically want to maximize the spatial resolution that we have in this setting without exceeding the size of the detector. And if we plug in some numbers there, what we found, like 529 has a source to detector distance and 431 millimeters as a source object distance, we have 60.95 micrometer resolution within the object. Resolution within the object. So, like one pixel represents a size of 61 micrometer within the object. Well, if we found that, we definitely know now we have oversampling in both the angular and the projection domain. And it's a little light today. And therefore, we have to acquire at least a noisy data, a clean data, and a beam hardening or flicked data. And a beam hardening or flicked data to serve all of these applications areas in the end because the green ones we satisfied with this oversampling already. Well, how do you acquire a noisy, a clean, a beam-hardening afflicted data? Well, let's look into it. We need to filter and optimize our beam spectrum. So, the interplay between like a broad beam spectrum and energy-dependent absorption creates beam modeling artifacts. So, if we optimize our beam spectrum, Artifacts. So, if we optimize our beam spectrum and filter out like the low-energy portion of our beam spectrum, we will have limited or no beam-hardening artifacts present in our reconstructions. So, I will give you a visual example, like empirically computed beam spectra with the TASMAP software. So you can see in blue, like a standard 90 KV, no added filtration spectrum with 60 kV and no added filtration, you have the. KV and no added filtration, you have the orange line. And then, with the filtration, Torreus filter, namely, which is like a compound filter of tin, copper, and aluminum of varying thicknesses, we get this green spectrum. And we see that there is mostly excluded everything below 40 kV in the spectrum. And that is very beneficial because 40 kV was found the minimal penetration energy of our. Penetration energy of our photons to go through the sample cylinder. In the end, we have therefore one mode, which is like the noisy mode, where we have like the 90 kV with the Tiraius filter to not have like double artifacts, the beam hardening artifact and noise. But we chose like a very low power, three watt, and the clean data is then just like the 30th amount of power, so 30 times the current. So, 30 times the current that we used for that. And mode 3, the artifact mode, is the 60 kV, no filtration, and therefore severe beam hardening artifacts present within our reconstructions. Well, let's look into the experimental design in like a bigger fashion, and I will walk you through what you can see there. This is like a simulation of our scanner from the inside. We have one. We have one the X-ray source, two the Torreus beam filter cell, because we cannot fix that onto the tube, otherwise everything will be acquired with a filter. So we wanted to place it within the beam line in a way that we can have it in there in the frontal position, as you can see here. But if we move the whole setup to the mid position, there will be no filtration happening there. Three is our sample stage, four is our sample cylinder, and five is our detector. So, again, what you have to take away from this side is there is one frontal position that we determined and one mid position. They are indistinguishable in their reconstructions if we use the same parameters. But if you then place this filter sail in there, you can basically have mode one and mode two, the clean and noisy acquired in the front position. Noisy acquired in the front position with the filter sail. You move the whole setup to the mid and you acquire mode three, the artifact inflicted mode. Well, let's look into the way that we finally scanned the whole thing. We have the sample tube. We scanned it slice by slice, in a one millimeter separation. We used a fan beam acquisition setting by only reading out the middle detector line. out the middle detector line from a comb beam setup and each slice was scanned as said before in these three modes like one that was artifact free and high noise artifact free and low noise and the mode three beam hardening artifacts but also low noise how did we do that because well there's a lot of theory what was the actual data acquisition process Data acquisition process. We look into that in more detail, but it's hard to understand this graphic with just a single look. So let's look first at the legend. The legend just tells us that this symbolizes the acquisition of one dark field, this acquisition of one flat field, and this the acquisition of line projections. So let's look at the top part of our acquisition process. And that would be this. And that would be this. Oops. We first start with the front position and mode one. We acquire a pre-batch dark field, a pre-batch flat field. Then there are happening 3601 line projections while the object is rotating. Then both the detector and the tube move down by one millimeter, and we acquire the next slice with, again, 3601 lamp projections. They move down again by one millimeter. They move down again by one millimeter until we reach the 10th slice. And then we acquire a post-batch flat field. Then we change the acquisition parameters and go to mode two. So we increase our power, our tube current, and do the same thing again. Pre-batch, dark field, pre-batch, flat field, line projections for slice one, two, to 10. And then after we acquired another post-batch flat field, we move the whole setup to our mid-project. Move the whole setup to our mid position and acquire mode three, the artifact mode, again with a pre-batch dark field, a pre-batch flat field, the 3600 line projections for slice one till 10. Well, why did we do that in like 10 slices, batches? If we would move after every slice into the different modes and change the parameters, that costs a lot of time. So we said, okay, we want to keep it stable enough. Okay, we want to keep it stable enough, like not scan the whole tube of like, let's say, 50-50 slices in eight hours, and then switch the mode. And then we run into some error and we don't have matching pairs or like things sack down or something like that. So we set like 10 slices each and then we switch and we switch and then we move on. So we move on to the 11th slice. So we go back from the mid position to the front position, acquire again a pre-petged. Acquire again a pre-pitched dark field, flat field, the line projections for slice 11, and move onwards to till slice 20, and then the different modes. What do you end up with? The to detect data set structure, let's say. We had 111 scanning sessions, more than 850 hours over the duration of five months to acquire that data set. We had additional 750 out-of-distribution slices that we acquired where we Slices that we acquired, where we scanned sample mix with only one sample or with samples that were not part of the mix before, like fresh figs, grapes, hazelnuts, pistachios, to get like robustness tats or like detect them for an object that was not part of the mix before, that is also possible with this data set. And a fun application again with the medical sector. We had a friend from a startup that develops prosthesis screws. Develops prosthesis crews, and we could include them also in our acquisition. And then the whole thing was pre-processed because we, in the end, had 540 million files. Because of course, if you read out 3601 times the middle detector line, which has a shape of like one times 1912, and you do that for 5000 sizes in three modes, you end up with a lot of data. With a lot of data. To make that more easily processable for people who download the data set, we combine them into sinograms. So we have like 3601 times 1912 as the shape of the sinogram. And yeah, you can pre-process all these sinograms with the pre-batch dark and flat fields and the post-batch flat field with this formula to get beam intensity loss images. And then we also provide reference reconstructions and segmentations for the people. Instructions and segmentations for the people that may have not high computing facilities available and just want to work with image-to-image training, basically. We used a Nasdaq gradient algorithm with 100 iterations to solve a non-negative least square problem. And for our segmentation, four-class segmentation, we used some multi-oto thresholding and mask matching with the Nellomat algorithm. So you get from us. So, you get from us the projection data. You get from us a description of the scanning geometry. You get all the pre-processing scripts and the reconstruction and segmentation scripts. You get the iterative image reconstructions for reference and also the reference segmentation. And with all that, you can do different kind of applications. And that's the last thing that we're going to look into after viewing what does actually such a slice look like. So, here you can see, for example, the noisy slice. I hope that is. I hope that is visible from your angle. That you see like a lot of dots there. There's like a lot of noise in the image. Then, other slice, which is the clean slice, where we have no artifacts in there and low noise. And the artifact slice, where you can clearly see between the different like high density objects, you see shadowing and streaking, which is our mode 3 referencer. mode 3 reference reconstruction. Finally, the segmented slice, we based this on the A2 reconstruction, so of the clean mode to have as little artifacts, neither noise nor beam hardening in there, and segmented it between the background, which I removed, you don't need to see a black box, then the tube wall, all the objects inside, and the filler material. Well, what are the versatile deep learning applications that you can use that for? Deep learning applications that you can use that for. We had three main objectives with that, and you already saw the application areas previously. So, one option would be reconstruction segmentation. So, both from limited or sparse angle sinograms to target reconstructions or segmentations. So, let me make that a bit bigger for you. So, you have the cynogram, you maybe only take a few lines of that, or you only take the first third of it, and you want to reach that clean reconstruction. To reach that clean reconstruction or even directly that reference segmentation, another option would be to do a low-dose CT reconstruction challenge. So pick your noisy cynogram or your noisy reconstruction and try to go to the clean slice. So get from this noisy slice to this clean slice from an image to image training within a supervised learning setting. Training within a supervised learning setting. Or you could do the removal of beam hardening artifacts, which would be using the acquired artifact afflicted and the artifact-free pair. So here again, the artifact slice and the artifact-free corresponding slice, where you see there is not that shattering present and also not that streaking. Well, to summarize all that, you can look up the paper on archive. The paper on archive under this QR code. The data set, this is the first 1000 slices, and everything else is linked in there as well. Can be found on Zenodo and on GitHub. You can find the 2D tag codes. And I'm open to your questions. Thank you very much. Thank you. When you acquired just the central line, did you also collimate the beam to just the central line? So the reduced comb beam scatter, or would this data have a fair amount of comb beam scattering? There will be no comb beam scattering because if you just look at the middle and see the setup that we used with the source to object and source to detect to distance, we are way far away from where you have comb artifacts. And you mentioned pre-processing, taking the logarithm. I'm sure you had to do that for your own reconstructions, but are you making the raw data available for people who want to deal with the plus on the data? So you get the very, very raw photome counts, which is amazing. And then you can either rely on the way that we moved onwards, or you can just use the pre-patch dark flat field and post-pad flat field to do it yourself. And you did this, if you don't mind me asking one more, this interesting batch of 10 slices at a time. Is the mechanical reproducibility good enough that somebody who's interested in sort of 3D object models could stack the slices together and learn from that? Or is there some gaps or issues? No, so I would say it is reliable enough. So we see that the matching of the different sizes is very well across modes. And then, of course, if modes and then of course if you have usually we used 50 slice batches so that took eight hours and 30 minutes to acquire all that so you have like five centimeters 51 millimeter separated slices that you could combine also like the cynograms and say well let's do that in in the 3d fashion or do that temporarily resolve that was also an idea that one group member one week ago um um yeah Yeah, thought of and said, well, we could like go through the slices in a temporal fashion and use that as well. That is also possible. And then I don't have an answer for this, but I'm curious. You know, there are people who are training machine learning models using whatever, natural images, even though they plan to apply it to, say, medical imaging, right? Now, you've done your best to emulate. You've done your best to emulate the, you said abdominal, but whatever, some part of the body, right? And so, what do you think is the, as far as the object part of this, you, the sensing system, of course, you've done extremely well, the object itself, what do you think are the pros and cons of using your images for training then applying to medical versus using natural images? Neither one of them are exactly abdomens or whatever, right? So, what are your thoughts about this? Yes, that is actually a very, very fun application. So, currently, my third project will be the investigation. Project will be the investigation of when does noise, experimental noise, matter and when it's good enough to use only simulated noise. So we take our real noisy data and see whether like clean data with simulated noise actually works for machine learning algorithms. And yeah, one of the fun application areas that I was also thinking of was saying, well, I train my denoiser on my data set. The noiser on my data set with raw experimental data, and then take simulated noisy data from, for example, the Maya clinic and apply that to. But that is just like a fun thing for me to maybe have as a outlook, because I'm talking more about the fundamental question: is it necessary to have real data or can you use simulated data? To what extent, what's the threshold for that? So, if you want to look into the training at the noise. Into training a denoiser and applying it to medical data, feel free to do so and tell me afterwards what their curious question the answer is to. You let me know. Thanks very much. Any other questions? If not, I'm looking forward to see many people use that data set. If you know somebody in the field that could make use of that, please let them know. And yeah, thanks again for your.