Thank you so much, Fan, and thank you to you and to Alejandra, to all the organizers, for putting together such a great workshop. Unfortunately, I cannot be there in person, hopefully the next time. And it's really interesting because all the talks of this session will be on causal inference and also on air pollution. However, these, I really appreciated Georgia's talk. She said this. Appreciated Georgia Stall. She set the stage for causal inference in a really great way. However, here we will use very similar data, but we will kind of not really go into the interference and spatial compounding part, which, however, we should kind of use as a potential scope for future work. So this is something that we don't account for that, but we do a work here on proposing a BNP model, which we call the confounder-dependent Bayesian mixture model. Founder-dependent Bayesian mixture model in which we try to characterize heterogeneity of causal effects. And this is a joint work with Daphne, who is going to present right after me, Antonio from the University of Padua and Francesca Dominic. So there is plenty of epidemiological studies that are focused on the causal link between air pollution and public health. And we know from this plethora of studies, the exposure. From this plethora of style, the exposure to higher levels of air pollution. In this case, we will focus on a particular type of air pollution, which is called fine particular matter, which are these particles which are very, very small with a diameter smaller than 2.5 micrometers, which can travel very, very deeply in the human body. And so they pose very high risk for human health. And for instance, there is these studies by Wuan Kauter and Dominici at Cautor that actually show these are just two studies, but it's really These are just two studies, but it is really a huge literature on that: how exposure to higher levels of air pollution are causally linked to higher mortality risk and not just mortality, but there is an increase in cardiovascular, respiratory, and neurodevelopmental diseases as well. And in the US, we also know that low-income racial and ethnic minorities are often exposed to higher levels of air pollution. There is a so-called There is a so-called structural environmental inequalities that make so that, like, these communities and this subgroup of people are actually structurally exposed to higher risk because they are structurally exposed to higher level of air pollution. And recently, the Environmental Protection Agency has set the stage to reach what is so-called environmental justice, which is to say that no group of community of people should disproportionately bear. People should disproportionately bear the risk of air pollution and any environmental contaminant. And understanding the heterogeneity of the treatment effects, so understanding who are the people and the communities that actually are most affected when exposed to a higher level of air pollution is key in order to guide policy and to really understand who are the communities that are at high risk. So, in this case, we will use data. In this case, we will use data that we're using in a previous literature about Zhao Wu and his co-hosters in a paper that was published in Science Advance, in which we look at how the long-term exposure to PM2.5 actually affects the mortality rate. And here, when we look at the long-term exposure to PM2.5, we look in particular at levels of exposure that are above the threshold of 10 micrograms per cubic meter. This is an important This is an important threshold because on January 27, 2023, the Environmental Protection Agency actually decided to decrease the level of air pollution, in particular PM2.5, down below levels that are between 9 and 10 micrograms per cubic meter. And so understanding whether people exposed to higher level or a lower level, who are the people that suffer the most is particularly important from. Particularly important from a policy perspective as well. And clearly, here we do a huge simplification by treating exposure to PM as a binary variable. But again, we try to look at this threshold from a policy-relevant perspective, since this is like the threshold that is set by the Environmental Protection Agency. And of course, as George was saying, there is all sorts of variables that are in between the exposure, the treatment, so exposure to a higher level of PM. Exposure to a higher level of PM and the mortality that can affect both simultaneously. In this case, we chose demographic information, but there is really a large number of variables that can confound this association between these two. And the goal of our paper, which by the way is available on biometrics, was published last April, is actually to identify groups of people that are at higher risk with respect to With respect to the average person. And so, here, what we want to do, for instance, is like to characterize three different subgroups. For instance, like group one, which is the one on the left, when the causal effect is equal or very close to zero. So these people are not directly affected when exposed to higher level of air pollution versus two other subgroups of people that are actually more vulnerable. So, the subgroup here when the housing effect is larger than zero. So, these are people. Effect is larger than zero. So these are people that are actually at higher risk when exposed to air pollution versus people that are at lower risk of air pollution when exposed to a higher level of air pollution. And the way this is usually done in the literature, in the paleomological literature, in particular, to basically run a regression with that interaction effect between the treatment, meaning the levels of air pollution. So the treatment in this case could be a binary variable, setting the threshold at 10 micron per cubic meter, and looking at various. And looking at various characteristics of the people or the communities and looking at which are the losers or winners. However, the problem with that particular approach is that in the first, first and foremost, is the researcher themselves choosing which are the variables to interact. So we might actually not look for important factors that are critical for understanding which are deteriorating, because they could just be overlooked. And so here. Overlooked. And so here we actually propose and try to propose an approach that is completely data-driven. So the way we define this group is not by using a method which would be a linear regression with some interaction term between the treatment and other covariates, but we'll actually develop a model which will let the data speak and will let us identify which are the subgroups that are higher, lower, or neutral risk when exposed to higher. Risk when exposed to a higher level of air pollution. And to do so, we will employ a Bayesian non-parametric model. So, in this case, we will use the Bayesian paradigm because we'll allow for a straightforward imputation of missing data. And as everyone that is familiar with causal inference, causal inference is inherently a missing data problem because once we observe one of the two potential outcomes, the realized one, we cannot observe the other one, which becomes common. Cannot observe the other one, which becomes counterfactual. And so, using Bayesian approach here really allows us to have a straightforward way of imputing this missing data. And also, the non-parametric component will also enable us to exploit rich information of the dependence given the confounders, as well as I was saying before, capturing the heterogeneity of the causal effect in a completely data-driven way. So, we'll not have to pose any. Way. So we will not have to pose any structure on the heterogeneity of the effect, but we will let the data speak and tell us which are the subgroups that are at higher or lower risk. And here is a very, very large literature on and a growing literature on the application of Bayesian methodologies and Bayesian parametric methodology in causal inference. The seminal works dating back to the contribution of Hill, which introduced the Bayesian advanced regression tree. Introduced the Bayesian administrative regression tree for the estimation of heterogeneous causal effects, as well as the contributions of Schwartz, Lee, and Menali, which propose a dependent Richlet process mixture in the context of principal stratification. And actually, this is a little assets to Daphne's talk, which actually will provide an extension of this model using a dependent Richlet process. So, in the next talk, you'll see an extension of that. But there are more recent contributions. But there are more recent contributions, such as the one of Kim Roying and Arman, as well. And there is a very nice literature review of BNP application in causal inference by Linaire and Antonelli that got out last year. So our contribution with respect to the research that I have just introduced is to propose a Bayesian non-parametric model that exploits the dependent Dirichlet process for the imputation of the For the imputation of the missing potential outcome in the context of causal inference, which will simultaneously allow us to estimate the individual treatment effects. So for each individual in our study, we'll have an estimation given the set of covariates of that individual of their causal effect, as well as identify the subgroups that are defined by similar treatment effects. So we will also identify the heterogeneity in the subgroups as well. Heterogeneity in the subgroups as well. And also, we will be able to characterize the heterogeneity in the effects in a precise and interpretable manner. So, often it is the case that when we go back to our collaborators, which may be medical doctors, having also an intuitive and interpretable characterization of the heterogeneity might be very important for them to understand what is going on and why some people or some communities are more affected. Some people or communities are more affected than others when exposed to any environmental pollutant. So, just to quickly set the stage, the potential outcome framework, here we'll have a set of N units indices by E. We have a set of pre-treatment variables, which are background characteristics or confounders, a binary indicator T, which is our indicator to tell us as whether an individual or a community was exposed to higher. Individual or community was exposed to higher level with respect to the new national ambient air quality standard, meaning like higher or lower level of air pollution. And then outcome, in our case, this outcome will be the mortality rate per 100,000 individuals. And the causal effect is going to be a function of these two potential outcomes. So for each individual, with the causal effect per individual, it's simply the difference between the potential outcome under treatment. The potential outcome under treatment manage the potential outcome under control. However, unfortunately, as I was saying before, this will never be able to get this quantity as once we observe one of the two quantities, the other one will be counterfactual. So we will not be able to actually see it in the data. In this case, we are particularly interested in two different quantities. So the first one is the conditional average treatment effect, which is the average treatment effect. Sorry, there was an E there. Sorry, there was an E there that is not showing, but that would be the average treatment effect of the difference between the potential outcome under the treatment and under control conditional on the characteristics, on certain characteristics. And in particular, here we'll target the Hausal estimate, which is called the group average treatment effect, which is again the average of the two potential outcomes conditional on the individual being part of the certain. The individual being part of a certain subgroup, and here you can think of a subgroup, for instance, of being part of the high-income versus the low-income people. So, this you can think about like the effect of the exposure of being exposed to higher or lower level of air pollution on the people with high income. So, this is what the group average treatment effects means in this context. And the model that we propose, which we call the confounder-dependent Bayesian mixture model. Founder-dependent Bayesian mixture model. Basically, we define the marginal distribution of our outcome, conditional on the real treatment T, which again can assume value 0, 1, as part of two components. The first one is a continuous density function k, and then we have this g, which is a random probability measure that depends on the confounders associated with a certain observation i. And the way we actually And the way we actually model this is through the single ethon DDP model, where we have this set of infinite random weights w as well as random Kerner parameter psi. And the way that we actually model our random weights is basically through a stick-breaking representation that dates back to Seturaman in 1994. Seturaman in 1994, where the view L of X here are zero, one value-independent stochastic processes, while the psi are IID from a base measure G. And here the idea, and actually in the next talk, Daphne will expand on that. I will show you in more details how this works, is that given the discrete nature of G of X, this will allow us to identify the latent. To identify the latent categorical variable S, which will actually tell us to which of the subgroup each unit will belong to. And so here we'll have out of this confounder-dependent Bayesian model, we have both an estimation of the potential outcome at an individual level for each individual with characteristics with a set of covered Xi, as well as we'll have an indicator of which is the group to which the individual. Is the group to which the individual belongs to, and the group will also represent the heterogeneity. So, just to wrap up this, our proposed model will enable for the inclusion of pre-treatment variables in the weights of the non-parametrics measure that defines the potential outcome distribution. Will enable for the grouping of units into the different subgroups, as well as will allow to estimate the causal effect in the subgroup. Estimate the Hausdorff effect in the subgroups and at an individual level. And just to kind of show how the model works, let's go back to our motivating application where we want to assess what is the effect of being exposed to higher level of PM 2.5 on the mortality rate controlling for a set of information. And so, here we will focus on text as well. Here you can see the distribution. Texas, where here you can see the distribution of the population of Texas as well as the distribution of PM2.5 in the state. And these are the heterogeneous subgroups that were found. So here we have six subgroups ranging from subgroup A, which tells us there is a slightly protective effect of being exposed to air pollution, two subgroups D, E, and F, which actually show a large negative effect of being exposed in terms of an increase of the mortality rate. In terms of an increase of the mortality rate. And also, we are able to characterize these subgroups. Unfortunately, here the captions are very small, but the idea is that the subgroups with a higher percentage of black and other people are the ones that are more vulnerable. While we find that in this case, for instance, like the subgroup with larger Hispanic population might have. Might have a protective effect. And this could have sorts all sorts of potential. This could be due to potentially not controlling properly for confounding the type of things that also Georgia was talking about before, as well as there are some ideas in the epidemiological literature of the fact that this is a selective subgroup here. Sorry, I should have mentioned we are focusing on the Medheir population, which is in the The Medier population, which is individuals older than 65 years of age. So it could potentially be that the most vulnerable individuals for these subgroups have already died before entering. So there is a so-called selective survival, which would potentially explain why we see this protective effect. But this is again something that we want to investigate more, and that could also potentially due to some potential bias. This is the distribution of the vulnerability across Texas. So we can also. Across Texas. So we can also see how this is distributed geographically in the map of Texas. And just to conclude, since I think I'm running out of time, here we exploited BNP model and we tailored them for the specific use in the context of causal inference and in particular to understand the heterogeneity in the causal effects. The proposed model are data-driven methodology that discover and characterize the heterogeneity in the treatment. The heterogeneity in the treatment effect, and the natural flexibility of the BNP model actually allows us to impute the missing variable and estimate the Household effect as well as find these relevant subgroups. And this is the literature that I've cited. And thank you again to the organizer for your attention. Thank you so much, Poko, for the great talk. So let's see if we have the questions from the audience. Let's see if we have questions from the audience. Nice talk, thank you. Towards the end in your example, you identified six interesting subgroups. Related question, there's something, there's two awkward things about the DDP model. One is that you have these stochastically ordered clusters that Clusters that it produces, which is kind of not what you want, right? You actually want, you probably want kind of the opposite. You want probably comparable-sized clusters. And you probably want clusters that look interpretably different. Like it would, boy, wouldn't it be nice if we had like small green people and a tall yellow people? Then you could tell a spinner story. And that's not part of the DDP model. So I think you might want to look at something like repulsive model. Something like revisive multipliers and maybe also not allow for infinitely many items because they are impossible to interpret just if you remove them. Sorry, I exit. My connection is already working, but I think I got your question. And I think these are all interesting points. So the first one on the cluster size, we actually are not, in this case, like pruning clusters are. This case, like pruning clusters are too small. And for instance, we see that at the extreme, for instance, the cluster with the protective effect, I think there is 4% of the population that goes there. Ideally, as you mentioned, it would be very interesting to have like equally sized clusters. And so this is something that we should definitely take into account. Indeed, it could also be that these very large effects are just driven by the fact that we have this very, very small subgroup for which This is a very, very small subgroup, or which maybe you are not even like controlling properly for confounding. And then the second point, which is on the having subgroups that are interpretably different, that's also a well-taken point in the sense that we see that some of the subgroups that we have presented overlap in a sense in the terms of the characteristics that they have. So even inducing a sort of like different in the cluster would be something. Different in the cluster would be something interesting. So, I think this is something that would definitely be interesting, like looking more and looking into that. So, I'm happy to keep the conversation going on that. But these are for sure you have highlighted limitation of our model, which was a beautiful story. Yeah, thank you. That triggers me to think about it as a subgroup analysis problem. Subgroup analysis problem, and then you would probably think about like different lists again. Yeah, no, that's for sure. Also, I think there is a large term, like there's a lot to be done in terms of also understanding where is the heterogeneity coming from, right? Once you see kind of which are the subgroups that you have found, we really want to understand whether this is kind of a spurious heterogeneity or which are the types of, there is a beautiful paper that was published by. That was published by Jamie Robbins on epidemiology in 2008, in which they show how there could be very different drivers when we talk about heterogeneity. And so I think here the idea is like a starting point, seeing which are the groups that are heterogeneous, and then kind of investigate where there is a spurious heterogeneity, which are the drivers of that. And then once we have found that, even kind of by design, say, okay, we want these subgroups to be. Say, okay, we want these subgroups to be in this way. Here we were completely, I would say, we let the data speak by themselves and say, okay, we don't want to have any constraint. But again, like this would be something interesting to be done, as well as a post-processing analysis to understand what we have found, if this is something that is really true or not. Do we have another question in the audience? Or someone online. Right, so again, we can definitely continue the conversation offline. Thank you again. Thank you, everyone.