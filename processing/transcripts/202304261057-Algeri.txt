And that's great because it gives a little bit of perspective into these apparent larger discrepancies, for instance, with WMS. But there are many situations where we can actually go back to results that we can assess because we know the truth now. After many years of measurements of some physical quantities, we come up with a very Of some physical quantities, we come up with a result that is very, very close to error bars, has shrunk considerably. And so, in principle, you could apply your technology to estimate this episode value globally for a community. This kind of thing has been done by Mats Ross in his 1975 paper, where he looked at the kaon and other particle decays and branching ratios, and he found that. And he found that the quality uncertainties showed significant tails in their residuals with respect to the true value that was now known. So you could do the same thing, I think, and look at the systematic uncertainties in particular and focus and derive a col a field wide uh kind of uh inflation, right? Uh so I wonder whether that is the next step. Whether that is the next step. This is a very interesting suggestion. And I just want to make a caveat. Well, I'm not stating that systematics of these experiments are wrong. I'm just saying that they may have uncertainties on top of them. And as Frank outlined yesterday his talk, for example, serial systematics where you do scale variations can have 50% uncertainty on top of that. And they are just not considered. see better yeah i mean we don't need to blame anybody by doing doing the by doing the exercise on a full set of results on several very well-known quantities you can you can derive an overall scale factor if you want. Maybe just directly to this, so a similar code. I mean, we're now publishing these likelihoods. So all of these are openly available with a bunch of Atlas likelihoods where you see all of Patterns like here, where you see all of these constraint terms. And I think it would be interesting. I mean, even without knowing what the precise true value is, you could just kind of use it as much. How sensitive are you? If you would basically expand the constraint terms, and I mean, they're out there, and you know, there's many examples, and you could just see, like, for those likelihoods. How does play with epsilon change the reports? Yeah, exactly. But presumably, you need to do that from inside the experiment. Need to do that from inside the experiment for it to be meaningful because not all arrows are equally sort of susceptible to how you set epsilon is then a separate question. Bob is on the same topic? Okay, then there was chat. Can I say something on the previous topic? Can I say something on the previous topic? Yes. This was a long time ago, Tomas. We've gone a lot quite. I mean, do you doubt that we have a lot of things in our situation? They might not be as small. For as long as they use it already. EDS still has one. Is it on the same topic? I was just going to ask when you were sort of mentioning the mass of the W and you were sort of inflating the errors, you chose which experiment to inflate the errors on, or were doing it for all the systematic experiments. Okay. Well, I've done uh three different examples uh to explore like different kind of assumptions that can be done since I don't have the expert knowledge to like actually assess which one is the most precise and which one is the less precise. So in this first example, both the Atlas and the CDF systematic uncertainties, so they're all systematic uncertainties, have the same assigned error on them. And so I plot everything as a function of this. I plot everything as a function of this epsilon parameter. And this is the first example. The second one is that let's set the epsilon of the CDF. So the uncertainties on the systematics of CDF to, well, almost basically to 0. 0.01 is just for numerical stability of my code. And let's just vary the Atlas one. And these two are very, very similar. This is because the Atlas uncertainty is the largest and so dominated. Uncertainty is the largest and so dominates. And this is also an important point because if you have many terms, if you actually do a breakdown of your different terms inside your systematic uncertainties, well, you don't need all of them to be big to see such those effects. You just need one of them, the larger one maybe, or the second larger one, to have a significant error on top of it to observe similar effects. And the last example is that is when the ultimate one is switched off. That is when the octost one is switched off, and so now the CDF one can show its effects, and this time we have the opposite trend. I mean, do you end up with asymmetric uncertainties on the other flag? If you go back to slide 60, that's 25. Well, this is a confidence in the value. It just oh sorry, go ahead. So does it matter? For instance, here the outlier is above if it was below the gamma distribution is asymmetric and would behave differently. The answer is yes. For large act for large errors on errors, confidence intervals are slightly asymmetric, as you as may be easy al also a little bit. Maybe visible also a little bit. But yeah, for simplicity of just even the like outsize confidence interval, but this confidence interval is actually symmetrical. Okay, yeah, I just wanted to ask about the choice of the gamma distribution here. At one point, I felt like it was going to be like a hierarchical Bayesian model. It's like prior on these parameters, have you looked at something like that? Have you looked at something like that? And is there a way of assessing that choice? What the impact is? No, we have some backup slides on this point. Okay. Well, first of all, we choose this kind of setup for the likelihood because we think that this reflects a way We think that this reflects the way our measurements are taken. So we assume that we have some Y measurements, some contour measurements, and then we take the best estimate on our systematic uncertainty to be another measurement. And we take the gamma distribution to describe the distribution of these measurements. And the reason why we choose a gamma distribution, well, there are some, well, first of all, the gamma distributions are. First of all, the gamma distributions are quite general description of positive defined stochastic variables. But we've also tried, I think Len tried also other distributions and similar properties of the model were observed. But the gamma distribution, among other positive defined, other distributions for positive defined variables has a nice properties that allows us to profile in close form over our new. Over our new Louisiana parameters. But also, a more like mathematical-based argument is the fact that if gamma distributions are, if our estimate on the systematic error comes from some real data set of control measurements, so we have some we ask, we try to estimate some Luis parameters, so we have a data set and we estimate. set and we estimate the we estimate the systematic uncertainty this way. Well, then this quantity will follow a chi-square distribution and if this quantity follows a chi-squared distribution then this variable is gamma distributed. So our model includes a case where our systematic uncertainties are inferred from a real data set of control measurements. Measurements. And also, this motivates our relation between what we call an effective sample size, which is related to what I'm calling here. This is what I've called everywhere else epsilon. It's just an order notation. So it's like when we increase epsilon, it's like we add a smaller sample size. So we know less about our parameter. Belain, do you want to add follow-up? Can I just follow up on that? So he said it right. The latter half of your questions had to do with the Bayesian version of this. And I looked at that many years ago. And in fact, there is a model by Giulio D'Agostini and some other people who were cited in the 2019 paper. But anyway, there's a very similar Fraysian-style model that effectively assigns errors on errors and Errors on errors and qualitative results are very simple. Can I just follow up on that? So the Bayesian model. And the other thing is that because you have multiple sources of information, it does seem like you should be able to learn about your systematics. So the difference when I'm talking about pragmatic versus fully Bayesian, here it really seems like because you have these multiple observations that disagree, you should have real latitude. You should have real latitude in a Bayesian model, or maybe you're multiplying micro, you're still learning about your systematics. I would say that's especially true. So, the Bayesian model we built in astronomy, multiple instruments looking at multiple sources in kind of a two-way setup. So you can learn about both the effect, the systematics, because you're looking at multiple sources with the same instruments. So, I think one thing I was, all of that leads to the question. Here, you're always looking at this, you're estimating the same quantity with multiple. Same quantity with multiple instruments. Whoever estimates other quantities with those same systematics. So you could also learn about the system. If you put it all together in one analysis, you could then learn about, you'd have more information to see patterns in how your systematics seem to vary from your measurements. That's not the use case we've really had in mind. We're thinking about something like estimating the mass of the W boson. W goes on, there are systematic uncertainties you enter into that, parton uncertainties and so forth. Those would be very different uncertainties if you considered some other observable. So by learning about the parton uncertainties in the W boson measurement, you would maybe learn some general lessons that you could apply to other measurements, but you couldn't map the errors on one error zone to the other. The systematic effect is different for different measurements. Can you tell me your market dress more? Can you tell me your microphone? Well, what's sitting on the board here is the center of the t-distribution, which you measure. And I were summarizing the results of the control measurements. I would write down and bolt on the likelihood, which shorn of everything is a t-distribution for u bar minus mu over s over root n. And a gamma distribution with a particular number of degrees of freedom, which is computed from n, just the way you did. I mean, and write down those likelihoods and glue them onto whatever other model you're trying to build these things into. Like, you actually summarize with the correct likelihood function the output of the control measure, which is that the t-statistic has a t-distribution. has a t distribution and n minus 1s squared over sigma squared has a chi-squared distribution on n minus 1 degrees of freedom. I mean one of the things you always do, you write down theta hat plus or minus plus or minus plus or minus and you pretend those numbers in the plus or minuses are constants known to you exactly right. This is what this is about, right? When they're not exactly right, what do we do? We adjust degree We adjust degrees of freedom, we do t statistics, and it becomes a big problem when you try to add together things with t, you know, the pivots you don't have, x-bars. You have to try to understand the distribution of the sum of several normals with different standard errors, estimated standard errors, and different degrees of freedom. And it's not convenient, it's not nice, but it's the true. It's not nice, but it's the truth about the distribution of the data, if you really believe the Gaussian card here. Sounds like the landing once you're a sound. I think that's exactly what we're doing. Except what we're doing, that is in a situation where the n is a fictitious n. We don't really have a sample of size n for the control measurements. We're talking about trying to estimate an uncertainty on the basis of some other. Other highly some ad hoc recipe like what Frank was describing yesterday. So in some sense, what this model does is it takes a student's tea-like model and applies it to a case. It connects this effective sample size to this error on the error parameter that I could maybe extract from someone like Frank. Is that what you want to ask? Yes, just connected to a bit of this. Yesterday someone asked about asymptotics, how our asymptotics define. So as Anthony mentioned, usually we think of n of the sample size, which is some sort of proxy of the amount of information that we get from data, but that's not the only possibility. So another example, and I think Anthony mentioned it in the past days about Poisson distributions. Here, the pin Poisson. Here, the pinpoint distribution of our approximation lies with the expected value, at least near the if the expected values go to infinity, then our approximation comes pretty well. And a third possibility, which actually wasn't highly explored in the statistics, there's actually one paper back by Juergensen in 1987, is about small dispersions of trophies, which actually means the results get means the results get the better on our approximation the smaller the uncertainty is in measurements or the particles and this is the situation here so the an i is some sort of factor some size blank coordinate which depends on these four eye terms I think there's a title and it should be an epsilon eye term as well the original original version was an art exactly Yeah, yeah, yeah. Exactly. So, the more precise the measurements are, the more information we have for those. Yeah, the whole point is that we're thinking about applying this model to a situation where we don't have this picture, right? This is just, we want our model to be able to include also this situation, but that's not the main example we are thinking about to apply our technology to. Hopkin, you want to ask that please? No, then let's go. No, no, no. Let's go all up in ten minutes. Not actually uh uh two comments. But one is in principle, very simple one. But just in general about the tactics, I'm slightly afraid that this might create some biases. So if we know, I mean, we know in Europe, you're outdoors at PDG there are this curve at the beginning where you see all the measurements do lock over time and then there's some jump. And then you come and say, now we have to have an error on error. But oh, it's just, you know, some hysteresis effect. And I'm just glad you were. I'm just slightly worried about that here. But I think you just need a reasonable judgment of what's to properly. The other thing I just realized that 17 minutes ago here, Piers Berger Brubel was giving a talk about systematic uncertainties with least square and maximum likelihood, and he was advocating the method of MS base to be used for outlier. The case they are outliers. The case they are outliers. Now, this might be a different case about outliers. This is something actually you must say you you're utterly confused with the notion of outlier. For me, outliers are like a detector where you get the noise sheet, which can be just like a spat distribution, it's not correlated with the track position or something. So this is what I learned about as an outlier. What do you call an outlier? It's not really the format of an outlier, it's just a measurement with underestimated uncertainties, right? It's a measurement, the term corresponding. The the the term corresponding to that measurement makes a large contribution to the log likelihood. I think that is my service for curiosity it would be I would be really curious to see how your method compares to the method of M estimates. The M estimates boil down to, but you say you're applying a measurement point which is like for instance four sigma away from the fit result from the averaging, then you just give it the constant wave, not the spodatic wave. Bob, you wanted to say something? So of course averaging discrepant data has been around forever and will be around forever. And there's at least two organizations who have to do it officially. So the organization that gets the official values of physical constants, like floats constant, charge of the electron, and all that, they have, I think, kind of a Bayesian mentality, actually. Then there's our own particle data group, which does throw out some outliers of its old data, and then this famous scale factor. So suppose this gets adopted. I mean, these are all conventions, of course. Suppose PDG, if you have some influence there, you know, adopts this as a new replacement for the scope. Replacement for the scale factor or an addendum to the scale factor, whatnot. Then you're gonna have to choose epsilon from the data. What they do now is they just scale up all the errors by the same factor until a pi-square figure would be a few equals one. And that's the recipe. Will you do something similar, or won't you do it? And at what point, I mean, this gets off on the whole philosophy, but at what point do you throw out the outliers? So this type of a model is maybe more appropriate not for the particle data group, but for the things like the heavy flavor averaging group and the Higgs averaging group and some collection of experts who would be better placed to assign meaningful values to these epsilon parameters. The thing with the PDG is that they have to be democratic, they have a bunch of measurements done by the desk. They have a bunch of measurements done by the desk, and then they have to come up with an average. And by that point, the expert knowledge as to what is the appropriate error on the error factor is not available. So maybe the DDG could have some kind of a philosophy that says all theory errors get epsilon of at least 0.3 or 4 or something, I don't know, something like that. That sounds difficult to engineer, but I would be willing to have the discussion with the. Have a discussion with the so just to follow up on this. Is it actually possible to estimate the epsilon? Like, can you treat the epsilon as an initial parameter? So, it's quite a literature on trying to estimate the number of degrees of freedom of a student's tea. And that's known to be difficult, the likelihood becomes an algorithm that doesn't work. And there might be some circumstances where you could estimate a global epsilon from a collection of. Load from a collection of measurements, it wouldn't work in general that was confirmed. So, I like this notion of experts going assign an epsilon. And it's very easy for experimentalists to get together and say, TB just gets 0.4. But what would you think the CPF people would assign as an epsilon to their own uncertainties? I'm supposed to go give them a seminar. That's a con that's a conversation that has to take place amongst the experts. And where you don't know, you do what epsilon just showed. And that is to say, you show how the result would change if you were to assign different values to epsilon. So I did a similar thing for the B on T minus 2 thing a year or two ago, and it was very similar to what Enzo just showed for the W minus. Put on your favorite. So there, the dominant uncertainty was. The the dominant uncertainty was this hydraulic vacuum polarization thing and so I talked to some of the experts there and yes they they concurred that something like 20, 30, 40 percent was perfectly plausible. But that's again a theory, I started. Yes, exactly. Yes, but that's not what I was asking. I was asking if you go to let me imagine I go to Atlas or I go to CMS and I ask what do you think the epsilon on your experimental measurement. I was thinking theory uncertainties because even within the experimental measurements gets The experimental measurements, for example, the uncertainties connected to parton density and the uncertainties connected with theoretical corrections that have to enter. They're the ones that have the largest epsilon factors. Most of the other systematics are going to be estimated with control measurements that are real measurements with a certain sample size, and that's going to tell you what the correct systematic error to quote is. Not always, but it's so fine. It's still fun. I mean, if I come back, what do I you have to go talk to the experts? Say, look, you assigned a certain error on the basis of some arbitrary assumptions. If you had made those arbitrary assumptions a little bit differently, how much do you think your estimate of the uncertainty would have varied? I don't know. It's back to this elicitation of expert knowledge where you strap the guy to the chair and you interrogate him. You can also show the different distributions for the choice. Like the different distributions for the choice of epsilon, and ask what your opinion is the best distribution describing your knowledge of this systematic uncertainty. If I'm the Bayesian in the room, if you could just really get, rather than that, if you could have multiple, again, if you're the same groups, we're estimating different quantities, you should at least get some sense of how good these groups from the group. How good these groups from the data instead of tying people to chairs, about how good their estimates of their systematic errors are. You're not going to get information from the data easily as to the size of these theory uncertainties. These theory uncertainties are mixed together with a gazillion other ingredients to give the final result. And that result that tells you whether the theory error was assigned or not. Not the individual errors on the, you know, so the individual systematic. For the individual systematic effects, but how good this group is at measuring their systematic effects? You should be able to. Okay, so that's what Tomaso was suggesting. I think, as a group, if you go back and you look at historical information, you could then know how to put it in a model. Okay, right, right. So it would somehow provide information on how well physicists at a group can assign systematic uncertainties. systematic uncertainties. Like a multiplier effect for this group. It would be of limited individual cases are going to be very different types of systematic uncertainties. I would argue time people the chairs are getting numbers. So I think Perenca really wants to say something about curious I mean, yeah, actually I wanted to ask somebody to summarize that is what a few of the small items have. But can I ask you whether maybe it's a good naive question? And I think of, you know, when you say epsilon is 0.5, right, that my naive thinking is, oh yeah, uncertainty is, you know, 50%, should be 50% not sure, but that's my very naive thinking. Of course, you have a gamma t distribution, whatever, which is a distribution. So epsilon is point. Distribution. So if epsilon is 0.5, it could even be, you know, the actual uncertainty could be even large one. This is maybe why you get the thing is perfect. So it would be nice in your plots to see as an operance, just for my own mental thinking, right, what if I were to literally just take, let's say, the CDF measurement and literally just increase that error by 50%? Because that's what I somehow have in mind, say F1 equals 0.5, ideally, you know, if I see that. Naively, you know, if I see that thing, I say, oh, that's that mm is a small model, this is different percent large or not. So you just you're just taking inflate taking the error and inflating it. That's obviously not what we're doing. I'm just saying it would be nice to see it as a comparison. Because in that particular case, if I increase it by 50%, CDF would still be smaller than than Atlas, so it would still dominate. So it would still pull it. You know, pull it. The average would simply be closer to the CDF measurement. Whereas in your plot, if I saw that right, at F0N was 0.5, CDF was the complete outlier, and the result was basically Laplace 1. So there's a big effect somehow of the fact that when you say epsilon 0.5 is that epsilon 0.5 significant means it could have been a long by a factor of 100% with some probability behind it. So I don't know if that's So, I don't know if that's the type of information, but. What to reply or anything? Well, you have such a strong effect also because you are assigning an uncertainty to the whole CDF uncertainty, right? So, if you do, if you assign it to just one, I don't know, the PDF uncertainty in the CDF in the CDF systematic uncertainty. Well, you get a similar trend, but not such a strong trend, nevertheless. So, also is that because of the tails? Is that because of the tails of the T distribution? Exactly. Okay. So since the tails are heavy, you can pull without the angle or tree quite soon. Exactly, yes. So this explains the sharp phase transition in slight plan to six. And then it's actually very exact. It first tries to, you know, I like this experiment, and then it's like, oh, I try to combine them so I make a very large uncertainty. And then okay, I forget about C D F. Well you're not really forgetting about C D F because still instead of having eight MeV an eight MeV confidence interval you have a 20 MeV confidence interval. So the inflation in the confidence interval is still quite significant. No, no, but you get 0.35 which is somehow saying the tension between the two are the tension and they have like, I don't know, more or less the same weight or something like that. And then it's like you forget somehow about the CDF. About the CDF value as you move down, as mu goes down, and then the uncertainty becomes smaller again because it's like, oh, let's forget about the CDF value. Yeah, but it is slightly reduced, but it's still very unique. You don't forget completely about the CDF measurement. No, no, sure. Never forget completely. I don't want to dissect the cusp there, but can I go back to this issue of what if I just said, all right, my assigned systematic uncertainty is. Systematic uncertainty is itself uncertain to 20%. Why don't I just inflate my uncertainty to 20% and see what that looks like? And that exercise was done in that V1G-2 thing from a year or two ago. And what the problem with that is that you don't then get any of the features that Enzo described. That is to say, you do not have a model then, which is where outliers are desensitized and where the size of the confidence that it holds is sensitive to... The confidence that it holds is sensitive to the compatibility of the data. So you don't get that. It's true that obviously by increasing the size of the systematic uncertainty, the tension, the number of sigmas decreases, but not nearly as much. Yeah, but this is probably my question. It would be nice to see how big the effect of sort of doing it in mobile lab as opposed to my naive plot. Okay, so that that what was in the that was plot was in the VR GRNS two paper. That was plot was in the new one G1 studio. Okay. Because, you know, so the reason I'm asking some scripts is to say that PR and COVID is a lot of things code, right? You know, in some sense, I would be happy to say that our PR answer is maybe underestimated by 50%. But I don't think they are underestimated by 100 or 200%. And given that I now see that your T order by Gamma distribution has this very long tail, allowing like really, really crazy Like, really, really crazy. I'm wondering whether that's the right model for theory and sort of behavioral. I mean, don't some systematic uncertainties come along with their own uncertainty? Like if you're doing sort of OPAP, where you change the value of a nuisance parameter, see how much the answer changes by, there's an uncertainty on that shift. So that I would have thought would pretty. That I would have thought would have told you what epsilon was for that particular case. The one thing that I'm slightly worried about in that situation is that the way you're dealing with things, that would increase the overall uncertainty. Take into account the uncertainty of the shift, it would increase the uncertainty. Whereas, in the usual way of dealing with things, if you see a shift when you change a new A shift when you change a nuisance parameter by 4 plus or minus 6, you would say, oh, that's consistent with 0, we'll forget about it. So it's sort of two things. Is that sort of the uncertainty on OPAC? Is that the sort of thing where you would know epsilon? And why would it shift the answer away in the opposite sign from the way I would have liked to have done it? Well, yeah, you can consider. Yeah, that w you can you can consider that uncertainty as uh your uncertainty on the certainty, of course. And uh well the answer you would get would be dependently on uh how good is your goodness of fit of your analysis or your combination. So if you have a good uh goodness of fit. So your result would be uh you would see only slight modification on your final, for example, confidence interval. Instead, if you have some kind of big tension in your analysis, then you would see significant effects. Analysis, then you would see significant effects kicking. And also to reply to Frank: well, you said a 50% uncertainty yesterday, and you're now saying that this may not now the gamma distribution may not completely reflect what you had in mind at the fi at the fifty percent uncertainty. But that exactly is the point also of like showing to the experts like different gamma distributions. So if you think that 0.5, so this one doesn't re reflect your One does it reflect what you had in mind for your distribution? Then you can go back and see: well, maybe is another one, maybe it's an orange one or red one. So you can, instead of assigning an epsilon parameter equal to 0.5, you can look at the distribution and say, okay, this is the one that for me reflects the most how I imagine my uncertainty on the uncertainty to look like. Okay, so unfortunately, in the internet, So unfortunately, in the interest of time, I have to have the discussion here. You're very welcome to carry out on the slide or the real first floor. But thank you very much, Etzegin.