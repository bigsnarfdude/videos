Okay, I'm gonna start if you guys are ready. Okay, so this is about so this is related. So Min was the first person in this direction, and then there's me, and there's gonna be more people, so it's about close. There's going to be more people, so it's about closing the loop. So, how do you use sensor, other types of data to make decisions about what treatments to provide when and where? And compared to yesterday's talk, this is more, yesterday's talks, this is more about problems that I'm struggling with. Okay, so what does my lab do? What we do is we do experimental design development. Design development. So we're involved in quite a few different clinical trials, all in mobile health, and they all involve sequential randomization. So what does it mean sequential randomization? How might you think about that? Well, often some algorithm is doing this randomization. And the algorithm may be trying to ensure that you have high quality data after the study is over. After the study is over, that may be the purpose of that algorithm. Or the algorithm might have the goal of both trying to improve treatment selection during the study as well as after the study's over, you have high-quality data. So this type of algorithm, there's a compromise going on. And here the focus is just on after-study data analysis. And these are all in really heavily constrained clinical trial settings. So I'm usually Trial settings. So I'm usually working with people who are interested in individuals who are struggling with chronic disorders. So they don't tend, you know, it's not like we're going to have a sample of 500 or anything, a clinical trial of 500. That would be rare. So what is this sequential randomization? So I said already, each user is randomized many times. So there's randomized, the kinds of online algorithms are either Either employ online predictions and they may employ reinforcement learning, which is a way to attempt to learn how to choose the best action as you're experimenting. In both cases, though, it doesn't matter which type of online algorithm you use, there's always some probabilistic budget. So, what do I mean by that? So, you might say, you might have a budget ahead of time that on Ahead of time, that on average you should only send 2.5 messages per day to this individual. That's a probabilistic budget. On average, only send 2.5 messages. And these are all intended to manage treatment budget burden. Actually, there's often a lower treatment budget as well. There's a lower budget as well. And that has to do with the fact that one often thinks of these as these are non-stationary. Things as these are non-stationary problems, and you want to have a low level of experimentation, even if there's no evidence of the treatment effect. So, there's usually an upper bound and a lower bound. And the other thing that constrains any kind of algorithm that you might develop is this desire to be able to conduct causal inferences after the study is over. And that's a big issue because it excludes a whole vast array. A whole vast array of common algorithms. In particular, algorithms that use internal sources of variance, i.e., variance in the user's responsibility to try and experiment and understand. These get excluded by this desire. So, if you're used to UCB upper confidence bound exploration, that's the type of algorithm that would get excluded. So I'm going to talk about just briefly about two of these algorithms. So one is we just call it a sequential risk time sampling algorithm. And here the idea is you have certain times that, let's just focus on a day as opposed to a week. It could be a week. But within the day there are certain times at which an individual is supposedly or detected to be at high risk. To be at high risk. And you want to uniformly, and you can either send a message versus not. You can also experiment with different kinds of messages, but I'm going to ignore that right now. You can either send a message or not. So essentially, it's the same. It's your sampling from all those risk times to decide how many to send a message to. They're equivalent problems. And the second is the reinforcement learning algorithm. And here, the goal: you have some horizon you're interested in. In my world, that's In. In my world, that's determined by the domain science. And you want to select treatments that will lead to a maximal, some good outcome over that or is it the goal? So I'm going to talk about the first one first. So I'm going to talk about this in the context of hard steps. If you've heard me give a talk, I often use hard steps because it's one of the easiest studies to explain all of these ideas. Even though we have. Even though we have other studies of this type. So, this is with anti-sedentary messages. And so, here's an example of one. They appear on your lock screen. Your phone pings. It lights up. And you can see the message. I think y'all can read it. It's not a big ask. Like, just stand up, roll your neck. And the decision times, the times at which you might either deliver or not deliver a message. Either deliver or not deliver a message, it's every five minutes. So every five minutes the system looks to see if the user is currently sedentary. If they are currently, if the user is currently sedentary, the algorithm has to decide whether or not to deliver or not deliver this message. And the reward, that is, what is this message trying to impact? It's trying to just get you to stand up. And so a natural thing would be, did the heart rate, individual's heart rate increase or not? That's it. That's the goal, the primary. It. That's the goal, the primary goal of this message: just disrupt sedentary behavior. And in HardSteps, we have an algorithm running that's attempting to sample these sedentary times uniformly. So I'm just going to talk about the algorithm from a high level right now. So suppose ahead of time you knew that Joe in front of you is going to be sedentary 20 times today. 20 times today. You knew that. And you want to provide your budget is an average of one and a half messages per day at sedentary times. So you would just randomize Joe with one and a half divided by 20 at each one of those 20 times, and you'd be done with it. It would be great. The problem, of course, is you don't know how many times Joe is going to be sedentary. So you have to both learn about that, continually learn about that, as you're attempting to unify. About that, as you're attempting to uniformly sample the sedentary times. So it becomes an online algorithm. And again, the goal is to uniformly sample the sedentary times. And so it turns out, if I told you, if an oracle came down and told you, if this G were the number of times remaining, I'm at a particular sedentary time. G tells me the number of remaining times that day that the individual is going to be sedentary. I have an oracle. Sedentary. I have an oracle that comes and tells me that. So I have one plus that, because I'm out of sedentary time right now. And then above that, I just, the numerator is the number of times I want, one and a half in the case of hard steps, minus the sum of the probabilities with which I've randomized up to that time. That would give me a uniform distribution. If I had an oracle that would tell me how many more times How many more times Joe is going to be sedentary? I don't. And so, with this, so what we did was we construct a prediction and input it here. Now, of course, all of this gets trained. I mean, there's two. We actually, in fact, in the real study, even though our target was one and a half, because we tuned this on prior data, we ended up with a different number here. Very close to one and a half, actually, by happenstance. But because in the trialing of the algorithm on existing data, we saw that we could get closer to an average of one and a half per day if we changed that number. But the target, this is usually one, it would have been one and a half, this is the sum of the randomization progress up to that time. This would be one plus the remaining sedentary times. So what we're doing now is that the algorithm that's running in this study, what it does is it looks at the fact that if you're sedentary, At the fact that if you're sedentary for the prior five minutes, you're probably going to be sedentary for the next five minutes. So it looks at runs and it predicts and it uses older data, pre-existing data on similar users to predict how long your run is going to last, how many more runs you're going to have in the future, and we just substitute in those numbers here. And we do it at each time, at every single time of the day, we get a different number we substitute in here. Number we substitute in here. How did it work? And so this study is in the field now, but in mid-September, our goal was one and a half messages per day, and we were getting an average of 1.3 messages per day, which makes you feel okay. Maybe our prediction was not so bad. But here's the caveat to that. One of the users, a couple of the users had. One of the users, a couple of the users had like an average of a half a message a day. Some of the other users had an average of two messages per day. So the point I'm trying to make is the prediction algorithm probably needs personalization. It probably needs some improvement. Because some people are just more sedentary than others, and we're not using a prediction algorithm which is as personalized as it should be. So, one of the ideas I have, one of the collaborative ideas I have, is how could you improve this idea? So, you want to have, you want, you have some measure of distance from a uniform sampling. It could be called back lightning or could be some other measure of distance. And you have to also involve a second online algorithm, which is a prediction algorithm. How can you improve this? How can you prove regret bounds? So, like if you Regret bounds, so like if you would compare it to an oracle that would uniformly sample the risk times, what kind of rates could you achieve? I'm really interested in this problem. I'm interested in how, and the reason why I'm interested in this is my suspicion is that we're going to see these kinds of algorithms even if you're not interested in randomization. You're just trying to spread out. You decided you want to give people a budget? You want to give people a budget of one and a half messages on average per day, and you're implementing in real life, and you want to spread your budget out over all the risk times. Okay. We're also using this in other studies, so I think it has a lot of legs. There's a feature here. Okay, so that was one component: that's the anti-sanitary messages. The second one is contextually tailored activity suggestions. And here is, so what happens? This could happen. And here is, so what happens? This could happen at any, you could get delivered or not delivered. There were other things being experimented with, but I'm going to focus today on deliver versus not deliver. At each of five time points a day, these are all related to your work schedule. So Eric's schedule would be different from mine. He'd have different five points, but they'd still be five points related to your work schedule. Your phone would light up, it pings, and you get a message. And now this ask is a little bit more stressful. It's like a three-minute walk. A three-minute walk or so. So it's not just stand up anymore and roll your arms. And the reward is the 30-minute step count following each of the five decision points. At each of the five decision points, you either deliver a message or not, and you see what's their subsequent activity following that. And all these messages are, the content of the message is tailored to where you are right now, like that was right before work. Like that was right before work. So they're intended to be actionable in the next 30 minutes. And they're tailored also to the weather as well. So the algorithm here that we're running, I put bandits in quotes because it's not quite a bandit algorithm. But I'm just going to go over it. I'm sure a lot of you know about bandit algorithms, but if you don't, I want to just help us be on the same page. Same page. So you have some model for your mean reward. Remember, it's the mean 30-minute step count. Given the person's current vector of features, like where they are, what the weather's like, how inherent they were in the past, and so on. So that's S. And then the action is binary. You deliver a knot. So you have a mean reward given the feature and action. So as the trial is going through, as the heartstips trial is progressing, at each time Is progressing at each time point you observe your features, you have an experimentation strategy that selects the binary action at each time point, you observe the subsequent 30-minute step counts, and then you have the algorithm that updates your model for the reward. Do I have to stop now? I didn't even start till five. I don't think so. Anyway, I'll just continue. So the next speaker is not presenting, so you have to give the war. Is not presenting, so you have to give a warm time for him. Yeah, and I started really late, so I'll just, until you come and stand next to me. So the learning algorithm, so you have two algorithms. You have an experimentation strategy and you have a learning algorithm. The learning algorithm, what it does is it keeps updating your model for the mean 30-minute step count given the person's current state and current features and actions. And this is just the way it goes through time, right? Each time, this is the way it's. Each time, this is the way it's proceeding. So, what we're actually again, I put this all in quotes because it's not quite right. So, our learning algorithm is a Bayesian linear regression. It's a simple version of a Gaussian process model. So, it lends itself to being updated because of the Bayesian crank. And the experimentation strategy is called what we now call Thompson's. What we now call Thompson introduced this experimentation strategy, and now it's called posterior sampling. So, the idea is: with you select the treatment here, A equal 1, we have binary treatment. The probability with which you select that treatment is the posterior probability that the reward, if you send an action, minus, I'll ignore this part right here. The reward if you send an action minus the reward if you don't send an action is greater than zero. Makes perfect sense. Now, the issue here is that we don't want to just pay attention to the immediate outcome, the immediate 30-minute step count, because you're always worried about people becoming overburdened by all these interruptions in their life, all these notifications. So you want to, if there's signs of burden, if you detect signs of burden, you may forego the chance to send a message now, even though they'd be highly responsive. Even though they'd be highly responsive, just to be able to preserve their responsivity into the future. And that's what the purpose of this adjustment is. It's intuitively, it's the difference in a prediction of the delayed burden effects between treating now and not treating now. Given how much you've been treating recently, that's what current dose means. And this is coming out of a MDP, a Markov decision process, as if you're in this area, you know all about that. If you're in this area, you know all about that. But it was built completely off of an existing set of data from hard steps. This is hard stips V1. It was built off of the data from hard steps V1. This is V2. It was built off the data from Hard SIPS V1. The posterior probability, this is all approximate, because we put in this prediction and then we ignore any confidence in that prediction from then on. In that prediction from then on, it's just like a constant that gets put in. No, it's constantly being updated, but we don't take into account the posterior probability is not more spread out because we have a bad prediction or it's not more concentrated because we have a good prediction. It ignores that confidence entirely. But this is the algorithm we're using. Okay, so I'm going to show you just with one user here. And this is one of the users. I just came from Colombia. I just came from Colombia, they saw this. And from San Diego, they saw it. I went to them straight one after another. So, this is one of the users. This is the treatment effect in log step count. So, that's the y-axis. And here's the day. So, this was the summer into this fall. And this is the posterior mean. So, what these are are predictions. The posterior mean should not be thought of as an estimate of the treatment effect in this setting. Estimate of the treatment effect in this setting. It should be thought of as a prediction for what the treatment effect would be if you were to provide an intervention at the next time. So each of these dots is like a prediction of what the treatment effect would be. You combine these treatment effects with the posterior covariance matrix and that proxy for the delayed rewards, and that determines the randomization probabilities. So on the next slide, I'll show you the randomization probabilities. Slide, I'll show you the randomization probabilities. Are we all? So here's the randomization probabilities. They start off lower, they get higher, and then it starts to decrease. And it's starting to decrease because the user became very responsive, and then for whatever reason, their responsivity decreased. So the randomization backed off. We learned a lot. We actually, at the time of this dashed line, we changed the algorithm. Time of this dashed line, we changed the algorithm a whole lot. That's the reason why I put it here, just so that I don't have any one point I want to make is these posterior probabilities are clipped. So this is also men referred to this yesterday as well, this issue of you're always concerned about non-stationary. So we don't allow the randomization to get above point A or below, in this case, point two. And all of these algorithms are like that. They have this class. Are like that. They have this clipping. Okay, so here's my collaborative idea, too. This is my second to last slide. So we saw two components of hard steps just now. There's actually six. And we're micro-randomizing all of them. But these are the two that have the algorithm. The others are all constant randomizations. So here I'm just going to review the anti-sedentary messages. The anti-sedentary messages have decision times every five minutes, and the reward is whether or not your heart rate increased, right? Because we just want you to stand up. The activity suggestions have decision times approximately every two and a half hours, and the reward is 30 minutes, because we're trying to get you to go for a little walk. There's another component, which is every morning, which is about trying to help. These are for people who have hypertension, who are sedentary. Sedentary, and we're trying to help them reframe physical activity so that they begin to perceive physical activity as enjoyable. And that has a binary outcome, and it's every day. Yeah, Chiprian? So people, when they start reacting to a message and they enter activity, they react extremely differently in terms of their heart rates. Especially people who have serious heart conditions, you can have somebody send a message, they stand up for just Message, they stand up for just a second and their heart rate goes through the roof. How do you deal with that? Yeah, so it's a binary outcome. So, did your heart rate increase? So, it's not the degree, and also it's for these are stage one hypertensive patients who have stage one hypertension. So, they're not in heart failure. But nonetheless, I'm sure there's enormous variability from one person to the next in terms of how much their heart rate might go up. We just want to know: did they stand up? We just want to know, did they stand up? Did they react at all? Now, I guess sorry to interrupt, but if the question is, if they stand up, that's different from if they hard drain a lot. Yeah, this is how we're encoding it, because the tracker we have right now is not sensitive enough to detect. The ideal thing would be to have a tracker, it's a wristband tracker, that would detect the fact that they moved up, right? But it's not that sensitive. No tracker isn't at all. Excuse me? It's not going to work. Excuse me? It's not going to work. No, so we can't. So, what we do is we're just looking to see if their heart rate increases. Now, I guess their heart rate could increase because they're excited about the message. Is that what you're worried about? I mean, I'm trying to think, why would their heart rate increase and have nothing to do with the message? I think there is a lot of viability in heart rate that can be often, you know, it would lead to a lot of false. Right, but the issue is that sort of measures heart rate versus measures heart rate and blood pressure over time, and it's super big. Oh, so okay, so here's the issue here. What I hear you telling me is that it's going to be very hard to get a signal-to-noise ratio. Remember, you're comparing times at which you got a message with times you didn't get a message. And you're, so if someone, if there's a lot of variance, both arms, it's like a little Arms, it's like a little two-arm trial at that five-minute time point. Both arms are going to be highly variable. And there's going to be a lot of those ones in the binary outcome, which are going to be one sort of for no good reason, and a lot of potentially zeros, which are going to be zeros for no good reason. So I'm going to have a lot more variance. So, what I hear you saying is that the chance of us seeing an impact after the study is over may not be. I don't know though, because May not be. I don't know though, because this experience, this is going on for this study, is a 90-day study. So there's many, many chances to deliver this intervention. Remember, you're comparing two instances. You're not looking at what happens when the heart rate, when you get a message. What you're looking at is the contrast between what happens when you get a message and what happens when you don't get a message. Certainly, I agree, the noise is. I agree. The noise is it. I think this is, I'm dealing with the noise, could be very high. We don't need like a null message to test the effect of the message itself, somebody picking up their phone. I think if you were interested in more of the bench research, you might just have a notification that said like a positive control, like, have a happy day. You know what I mean? That would be a natural thing.