So, in many ways, I will be way more pragmatic than I think most of you, because in the end, I'm just I want a number. So, most of the time, I don't really care so much about like the mathematical structure, just want to get to a result. And this is work that is done with in collaboration. That is done with in collaboration with Fernando February-Cudero, Gustavo Figurado, which you probably have met in Amplitus conference, Ben Page and Laura Reiner. So, why do we want to compute TT by Hicks production at the to-do level? TT-Bar Hicks, for example, allows us to. To yeah, I just used mine. Allows us to extract the topcock-you cover couple directly from measurements. And currently, if you're saying like at the one loop level, then the theoretical accuracy of ET by Higgs production is of the order of 10 to 20 percent. But as you can see here on the left, the anticipated precision goal of the high luminosity LHC will be around. That the LHC will be around 2%, and the dominant contribution to the uncertainty is coming actually from the theory side. So, we really have to increase precision of our theoretical predictions to be able to make use out of these data sets. We already have, since 2022, an approximate next-to-next to leading order, and you can see that they are getting close to like 3% uncertainties. Uncertainties. The approximation that they did is these two-loop virtual amplitudes have been computed in a soft expansion. So they essentially take the Higgs to be massless. And then things simplify quite significantly. So what we are after is the full full virtual amplitudes without any kinematic approximations. So. So on the side of next to next to leading order calculations for five point amplitudes with top quarks, there's actually not too much in the literature so far. There's mainly the effort for TTBA JET, where Hector has been contributing quite a lot. So they have essentially now the leading colour to loop master integrals. And I guess they start now computing the amplitude. Amplitude for TT by Higgs, there has been a lot of approximations at the beginning. People computed directly like infrared divergency as an input for the full amplitude. But then mainly we have results in soft ticks or boosted limits. And these are at the moment actually, these are the ones that have been employed in the approximate next-to-next deleting order calculation. And before Christmas, we Then before Christmas, we published the first set of planar master integrals that are sufficient to compute the amplitude, including like walk loops. And I think this triggered quite some panic shortly before Christmas, because I don't think that people had us on the radar. So like directly same month came the one Luke Luto TT by Hicks from Lorenzo out. Lorenzo out afterwards, also the boosted limit calculation. And earlier this year, we have the two-loop qqar to Tt by Higgs amplitudes for closed fermion contributions, including heavy quark loops. But this is like a fully numerical approach, while we are pursuing a more analytical approach to this problem. If you are interested in computing TT by Higgs at the TT by Higgs at the leading color level. Then you have to deal with these three topologies plus their mirror images. So we have the first planar amplitude has already like 111 master integrals, and we find a new analytic structure, which is the appearance of a nested root that gave us a lot of headache. And then the other two topologies are even more complicated because Even more complicated because we have like 160 or 190 master integrals, we get more and more nested roots into the game, and we become, we find more and more elliptic sectors here. So, in the first part of the talk, I will essentially talk about the closed fermion loops, everything that is at least not apparently elliptic. And then, in the second part, I will give an outlook of what is What is awaiting us in the other two topologies? Okay, so for the closed Fermi loops, we essentially only have to consider these two topologies. The most complicated thing we have to deal with is this nested root. Like questions so far? No? Oops. Okay, so just a word on the scattering kinematic. So we're looking at glue. So we are looking at glue gluto tt by Higgs. We have like two on-shell top quarks. We work in a generally off-shell Higgs kinematic. This can be then also used as the partial these integrals will be also partially relevant for T T by Z production, for example. And of course we have two massless external particles. The kinematic space is parameterized by seven invariant Mandelstum Invariant Mandelstum variables. And in the problem, we have 11 square roots. Six of those are Gram determinants, two are modified Cayley determinants, and three simply originate from Maxima-Cut computation. But we get two nested roots. So we have like plus-minus solutions here. And this nested root looks quite horribly because it's like a degree three polynomial and five variable. Degree, three polynomial and five variables. So, this, the analytic structure of these differential equation entries was really like a headache for us. It's just in the way of when we try to canonicalize things, then yes, so we in this Yes, so we in this case we did not look at the maxima cutter. So we just brute force Magnus Magnus expansion of the I will talk about this in a minute. We were hopeful because it's by accident that I chose a basis that That I chose a basis that kind of made things nicer. And I can comment on this a little bit more. So our general strategy is, of course, we are going for differential equations and we are trying to explicitly construct a basis transformation that brings us into epsilon factorized form. And once we have the epsilon factorized form, we would like to express this as a linear combination of one forms. Combination of one forms. And for now, we just obtain a numerical solution for this differential equation via a series expansion as implemented in diffx. But we will actually try to construct like pentagon functions for these things. I think I'm way too fast. So please ask a lot of questions. So how do we start generally? But generally, we try to find an initial choice of basis where the differential equation kernel is just linear and epsilon. And then we just try to construct explicitly a basis transformation of this basis. And if you can essentially, the basis transformation is the solution to the homogeneous part of the four-dimensional part of the differential equation. Dimensional part of the differential equation. And by this, you can algorithmically remove the constant pieces of your differential equation. So, if necessary, we have to compute off-shared correction terms to our master integrals. And this is done by explicit integration of these entries. As you can imagine, with seven scales, just looking at these differential equation matrices, this becomes quite horrible. Quite horrible. Like, in order to actually compute anything, we are using finite fields techniques from the start. Everything is numerically, and we are just trying to reconstruct the functional dependence of these entries that we really need. So, in the end, we try to push numerics until the end and only try to fit a rational function or even like an algebraic function. If necessary. So they're like easy things in our differential equation that have which has this lower triangular form. And in these cases, things become rather easy because you can first work out the solutions to the diagonal, which becomes then just the normalization of these integrals. And typically, what happens is once you normalize. Typically, what happens is once you normalize your diagonal properly, then the lower things here become integrable in terms of like rational functions. So you can just integrate essentially the homogeneous solution and use this as a sub to kill the on-shell block. But then off-diagonally, you just integrate out one by one the off-shared content. One the off-where contribution. And for example, in this bubble triangle, you will see that you get rather simple results. So this is actually what we've done for most things. We have tried not to compute maximal cuts if possible, like explicitly by bycorp representation or so, because I'm not very good at it. But the fun stuff is our activity. Fun stuff is our kite. And this integral topology has seven master integrals. And there's no way you can put in like tensor integrals. So you just have to like crazily dot everything. Just by randomly choosing integrals, we were able to actually find four of these. And we found a particular form where we have only a couple three by threes. We have only a coupled three by three system that has a very particular form, and this differential equation here has already an algebraic d log form where these integrals are the same. So that is what made us realize that we can actually Magnus expanses. Because if you have a form of this two by two blocks like this, then it's clear that after the first commutator that the Magnus expansion will chunk. Yes? At least on shell it's not like I'm what what you no I mean I don't know if like if we high orders an epsilon because of the complicated this might become elliptic but at least we Might become elliptic, but at least we never had to use elliptics to normalize anything. What do you mean by dimension? This is all four-dimensional. So if you Magnus expand this guy, this will. guy this will then the the magma expansion will introduce these nested roots and the reason is that this thing here is already uh a plus square root b and this will introduce exactly these guys so this is like a new analytic structure that hadn't been observed in any other as far as i know in any other two loop differential equation but for example tt budget Example, TT budget also has these nested roots. And this, I think, seems to be like a thing that we have to care from now on. I will later on, in the other two topologies, it's more clear where these nested rules come from, and I will point out this later on. Like the whole basis looks quite horrible. looks quite horrible because you now have like the you have sums of propagators and the differences and they are normalized by different nested square roots and this will only make the um the on-shell block canonical the last guy here is just a horrible thing that we had to brute force through because there was nothing meaningful anymore to pick but uh to come back to the the form how we even The form: how we even try to attempt to attempt exactly on shells. On-shell means we only put all five propagators on shell. That means I don't have any of these subtraction terms. Yes. Like for the maxima cut, this is what we have done here. This is like the maxima cut solution. And once we have implemented this, we have to look into if offshare we have to subtract something. It is just for the last guy here, because there's no good choice anymore. We just had to brute force our way through. And you see that this thing is like, if I write it out, it's like a page of subtraction terms. So, how the funny thing is, how did we get here the first two zeros in the first place? And what happened is I knew that because you have these two massive propagators and you have the Higgs line here, there will be like a square root, the square root C1, which is like the Higgs mass minus four times the top mass. This one will pop out everywhere where you have the Higgs. Everywhere where you have the Higgs in one corner. You already see this like in the one-loop massive bubble where you have the Higgs exactly on as the momentum flowing slope. So what I tried is you increase propagator powers here and you then have the plus and minus because in the minus case the threshold should be cancelled. And that's exactly also what we see here that the minus combination did not Combination did not generate the square root. And this led to the fact that we, these three canonical bases on the diagonal, you had to normalize by a fourth root. And the fourth root ended up to be this guy here. And then after Magnus expansion, the fourth root magically cancels out, and you're just left with the nested one. Yeah? Nested one. Yeah? There's C one here. Like the so I don't think that. So I don't think that the nested root by itself has this interpretation, but it's my choice of pre-canonical basis that I already chose the plus and minus solution. Like if you don't have that, then it becomes quite impossible to see what could even happen because then the Magnus expansion even has to produce these linear combinations. I mean, even integrate the Magnus expansion of this five variable integral or Integral already becomes awful. And we were just lucky to get through, but there's no systematic approach. We were just lucky that I found in the beginning the initial base. Okay, so like this was really the most complicated topology that we had to deal with. And with these two techniques, Magnus expansion. Two techniques, Magnus expansion, and just trying to integrate out the homogeneous solution. We were able to go through all of the 111 integrals. And now we try to actually construct the D-log differential equation, but simply from numeric, because we don't want to reconstruct the entire metric. So what we do is we are following this, what's called the random directional. What's called the random directional differential equation approach. You just take a vector of random numbers, random but fixed, and you just take the linear combination of all, a random linear combination of all differential equations. And that means that the integration kernel becomes the becomes a random linear combination of these d log forms. But that means you don't have to look now into. You don't have to look now into seven individual differential equations, but you have one differential equation that depends on every single D-log form that you can have, like on every linear independent one form, let's say like this. So now simply by constructing these metrics for n different phase-based points completely numerical, we can now ask what is the rank of this metric on different phase-based points. And this rank. Base points. And this rank is bound either by the number of points you have computed, or at some point it will be the dimension of your one form. So just by increasing the number of points, we found that we have 152 linear independent one forms to compute. And so you have these 152 one forms you have to construct really like analytically, but that means I have to. But that means I have to reconstruct 152 entries of the differential equation, but not like the entire of seven 111 by 111 differential equation matrices. And then we can just fit essentially on this basis, and from this you get the coefficient matrices, which are just rational numbers. Yeah. The sampling of uh r vectors. Of our vectors play out with the sampling of numerical forms? We are not sampling here. This is like a fixed but random number, once and for all. Because if this R is different from, if you would change this over time, then you could not invert this. Okay, so running it through with a single choice of R is enough that you can do the reconstruction that you described. Yes, because it's like in the end, this is just Because it's like in the end, this is just the like the we compute this thing here fully analytically, this thing is fixed, and you just sample now kinematic points. And just by inverting this matrix, you get what you want. You get these rational matrices here. And everything is done in finite field arithmetic, so computing this inverse applies. Other questions? Other questions? This is like the one, the first pentabox. So as I already mentioned, we have 152 one forms, letters. I'm aware that you would not call a letter if it's elliptic or whatever, but for now, let's assume one form is letter. This is much more than you would. This is much more than you would find, for example, in W plus jet production for single family. So there's quite an increase in the complexity of these integrals. We also find that nine letters of those are irrelevant if you are just interested in the solution of the differential equation up to epsilon to the four, which unfortunately are not our nested root friends. It's more like It's more like trivial things. And from these, we find like we have 122 relevant letters that do not involve the nested root. Then 43 are polynomial letters and that go up to like master mention six. We have 42 single odd letters and we have 37 double odd letters. These expressions are already quite big, but we were able to actually But we were able to actually, if you express them not in the basis of the Mandelstam variables that we have chosen, but just in any kind of Mandelstem variables, you can make the expressions more compact. But we have 21 letters that actually depend on the nest square root. And they might not be as terrible as one could imagine because few of most of them can be just put directly into. Put directly into so most of them can be directly put into like D log form. But we have found four cases where we are not able to integrate this to like a D log. And these four cases are actually corresponding to the maximal cut of the kite. But they have this particular But they have this particular form, and they are just related to symmetries. So, in the end, the question is just if you can delock this expression. What we have verified is that this expression, once you expand it around all of the poles of the denominator, only generates single poles. So, I think by this case, we can at least say that we can still. We can still talk about later about the weight of an integral. Because, as far as I'm aware, the second you have like double pole, that there might actually be like total derivatives hidden in there. So, at the moment, we are kind of lacking a clear decision procedure if this thing can be put into a D-log form or not. So, that's actually the main problematic point here at the moment. But, except for the Moment. But except for these four one forms, we, yeah. Yeah, so they belong to the maximal cut of the kite. So they will sit in this on-shell seven by seven. Yes, probably. Just that the bycoff representation for this thing is not nice. Like, I mean, I was not able to get like a one-fold integral representation of the maximum curve. That's the main problem here. Yeah, so this n plus here has the nested guy, and yeah, because the numerator is polynomial. The numerator is the polynomial. Other questions? Okay, so then going on to numerical solutions, we use AM flow to generate a 100-digit physical boundary point. And then we want to use this to obtain results using this general series expansion. But the problem. Expansion, but the problem was that these nested roots, if X does at the moment, don't know how to deal with them. So, actually, for the analytic, for the numerical implementation, we actually changed to an auxiliary basis where we simply removed the linear combinations of these kite integrals. And by that, all of the nested roots from the differential equation will vanish, and then you transport to the point. And then you transport to the point, and then you transform into the canonical basis again. That's our current approach to get numbers out of these differential equations. Here, for example, just one randomly chosen point, we were able to get really high precision results for up to like the Penta box, which I think was above 90 digits. Was above 90 digits accuracy while providing a 100-digit boundary point. You can ask yourself if all of this was necessary if you anyway inflow the boundary. Can you aim flow everywhere? And what we found is that solving the differential equation is approximately two orders of magnitude faster than computing with AM flow on a particular point. Computing with AM flow on a particular point. AM flow takes roughly like six hours on one point for 100 digits. And the main bottleneck there is actually the rational IBP reduction that is being done in the background. So this is so far everything that we have published before Chris. Before Christmas, this is like the first set of master integrals that allows you to do the scattering amplitude for closed fermion loop if this year is like light quark loops. So now since then we started to look into the full leading color integrals and you have to take into account those things. And in the following, I would like to just To just not give like a proper outlook, but more like where do we see more and more complexity creeping into the problem. I want to state that everything is preliminary and we haven't solved the problem yet. This is more like the problem that we are struggling with at the moment. And for now, we are just focusing on maximal cuts. So the elliptic sectors are all box triangles, now our problem. Box triangles now problem. We have like two topologies that have like an elliptic curve where like one pole sits at zero. And then they also have three masters. And then we have two topologies that have the full degree four elliptic curve with four masters. Plus, you get crossings of these things. So at the moment, I would like to focus on one of these three masters. Like to focus on one of these three master things because this is where I've thrown my head at the most time. And the main problem that we have is our starting point of the differential equation. We do not find a very good integral basis where on the maximal cut the differential equation is on-share and all the off-shared terms, the possible subtractions that we have to compute are also linear and epsilon. So it boils down to a fact. So it boils down to effectively two choices that you can do. You can have linear in epsilon off-shell, but then you always end up with a particular form of non-linear epsilon dependence on shell. Or you can choose that your off-shell entries are rational in epsilon, but at least on-shell, you get like a nice form that already tells you which one is. Tells you which one is the couple two by two block. So we have decided to go for the latter one at the moment because I just don't know how to deal here with the one over epsilon whole. So in the following we will look at this topology and what seemed to be very beneficial for us is to put like a tensor insertion on the triangle side. Think like for a regular Like for regular polylogarithmic things, you would never try to do this because this breaks power counting. But in this elliptic case, where you're anyway forced to break power counting, these things look way nicer than, for example, the tens on the box side, just square. So then the unshared differential equation or the maximum cut has this particular form and we would like to study this now in And we would like to study this now in Bykov representation. And the idea that we have is to follow the period metrics approach, meaning you compute the maximal cut of your three master integrals on three independent on tours, and you essentially take linear combinations of these integrals. Then, by demanding that this period matrix becomes proportional to the identity, you can fix the unit. You can fix the unknown coefficient fik, and they will tell you how your basis transformation actually should look like to canonicalize the maximal cut. So in the end, what we have to do is we have to find, first of all, a one-fold integral representation of the maximal cut of these box triangles, and then compute this transformation matrix. So, how do we do this? First of all, we start. This. First of all, we start with the triangle side, like we follow loop-by-loop Bykov representation. I extended the triangle side with an additional propagator to account for the tensor integral. And you get a Bykov representation that is essentially the Gram determinant. And now you have an arbitrary power of alpha of rho 10, which will be either alpha equals zero, will bring you back to the Will bring you back to the triangle, and alpha to the power of minus one will be the tensor on the triangle side. So, in the scalar triangle, you get a square root that is like row nine and four and t minus row nine, where row nine is exactly the tensor on the box side. And for the tensor on the triangular side, you get a correction factor which looks Correction factor, which looks a little bit more cumbersome. So now you can use this row 9 and the remaining propagators to build again like a box diagram for which you then construct again the bykoff representation. And now you essentially just stitch them together for specific values of alpha and beta. And what you end up with is that, for example, our That, for example, our scalar integral will just become the integral over the elliptic curve. Then we have the tensor insertion on the triangle side, which is a linear combination of a tensor and a scalar, but also has like a higher pole in one of the ports of the elliptic curve. So, in principle, here you have to IBP reduce this to get rid of the pole to do the integration. In this case, we were actually lucky and we did not have to do. We were actually lucky and we did not have to do this. And the last one is the tensor on the box side that just becomes lambda in the denominator, in the numerator. So now you have to integrate those forms. And we chose like three independent contours between the branch points of the elliptic curve. And one is the residue at infinity. And for the first contour, And for the first contour, you will see that you get like a linear combination of elliptic k, elliptic e, or elliptic pi. The second contour is very similar, it just has like more complicated linear combinations. And the third one essentially is 0, 0 and some 1 over z square root for the third one. So, currently, we're checking if these maximum cuts are actually a solution to our differential equation because we have. Equation because we have been a little bit floppy about like pre-factors. So there might be some corrections that we have to do. But in the end, this will allow us to compute the basis transformation to canonicalize at least on-shell on the maximum cut these elliptics. So clearly, I have to learn more. So, clearly, I have to learn more about elliptics. That's why I'm here. So, like, what I don't know at the moment is how we even attempt to compute these off-shared subtractions if they are rational in epsilon. I have no idea if it's known how to choose like a better basis from the start, that you don't have to, that you find a linear epsilon basis. Okay, this is all that I wanted to say. Okay, this is all that I wanted to say for the elliptics. Now coming back to additional nested square roots that creep in, and they are actually all related to penta triangles with internal masses. And if you compute the maximal cut, you will find that we do this again loop by loop. If you the triangle side now typically already generate a square root, while the maximum cut. While the maximum cut of the pentagon side only gives you a denominator with a quadratic denominator in the variable. So these poles of the pentagon are already linear, have already like the square root of the phase five, of the gram five point gram determinant in there. But if we now start to take residues in these points, what will happen? These points, what will happen is that essentially lambda one or lambda two is propagated into the gram determinant of the triangle. So, since lambda one and lambda two is already a square root, this thing here turns out to be a nested square root again. So, this is another source of these nested roots that only enter in the second and third topology. However, you will think that nested roots are kind of awful, but these gram determinants actually have a lower polynomial degree than what we have in the kite. They actually might be nicer to deal with than the kite itself. The main problem is we have one penta triangle that has a nested square root with the pipe point gram determinant that will pinch. That will pinch onto our kite that has a different nested square root. So we expect already like quite interesting differential one forms here. Of course, we have not really checked if they actually talk to each other, I have to say, but the experience from the first topology was everything that the nested root essentially touches becomes a linear independent letter. Becomes a linear independent letter. So, in this case, I would be not surprised that we get a lot of very complicated analytic structure. What makes things even more complicated is that this thing also talks to an elliptic sector. So I think, yeah, at the moment, I have absolutely no idea how to do this. I just want to point out very interesting problems on the way that we have solved. And this essentially. And this essentially allows me to conclude. So far, we have computed the first set of master integrals for Tt by Higgs, then allows you to do the closed Fermi-look amplitudes. We have provided differential equation in epsilon factorized form. We don't know yet if this is polylogarithmic or elliptic because of these four unknown one forms. We have found a new analytic structure, which is this nested square root that will, I think, show. Nested square root that will, I think, show up everywhere now in five-point top quark processes. We are currently working on a computation of one-fold integral solution to these differential equation in forms of pentagon functions. So I think here we will revisit our four unknown one forms. And we are also working on solving these remaining elliptic pentagox family. Thank you. Thank you. Thanks, Nanique, for the very interesting talk. I mean, this is one of the most challenging to loop integrals that one can look at currently. And yeah, it's not surprising that with so many variables, when you start doing analytics, you have all of these ugly things popping out. But do you have reasons to believe that this nested square roots cannot be get? We cannot get rid of them, even if we start with somehow more motivated choices. Okay, that's now a good question. Probably I'm not the right person to answer this. In most cases, actually, if we end up these delog forms, you could make a variable change where you just take the nested root as your integration variable. And then things you could just analytically integrate. And then things you could just analytically integrate in Mathematica. But once the thing talks to more complicated square roots and everything, then this did not work anymore. So I think for like certain cases, you could do... We did not try to rationalize this thing. That is the question. I agree that whenever you do this first, but they decide to look out which type of epsilon basis is better and so in our look by look on our analysis. Plug Market analysis a few years ago, we toiled really hard to figure out why a buying spaces is better in some sense, better looking for some reason this basis where you look at the elliptic curve and you go like dx of y and then the precedative of dx of y. These type of bases seem to land, uh have differential equations in which the entries of the matrix are the type of functions that Pauli was writing down. That Polly was writing down earlier today, and then we know how to integrate them into a function space that is sort of mathematically under control. Whereas, as far as I can understand, when you formalize theory stuff, you give up these elliptic E's, lipid states, and i's that actually don't combine nicely into these nice integration problems. That's all we talked about. What a number, it doesn't look better. So, you say I should take the derivative of the scalar. Like what was the basis, what it's like in climate. But if you take that and re-extended it to what it looks like, That and re-extended it to what it looked like the final integral. And I have no idea how to translate how to start with those finite integrals and write down some crazy linear combination that produces this nice state on the function function. Yeah, I so at the beginning I played around a little bit with this. So because we have an application in mind, that's Because we have an application in mind, that's why we decided against the derivative. Because, like in unitarity, this will be your numerator insertions. So, we don't want large numerators. That's one thing. So, we try to find something simpler. What is not clear to me in the multivariate case is if I can just take any derivative in any variable, or do I have to take how do I choose which variable if I have seven? Happened. Yeah. The other problem that I had, what like I've not pushed it to the end. The problem is if I want to take, I have to compute then analytically the derivative of the scalar. But this integral, they are talking to 80 different integrals below. And have to analytically reconstruct everything. I mean, of course, you can do this. Of course, you can do this. I just we tried to find something new about this comparison. Yes, I know the paper. I'm not an expert, but it feels to me like the derivative is like a brute force solution because. Because getting the derivative of these integrals that are sitting very high in the hierarchy of integrals becomes quite a mess. But maybe this is the way to go and just accept that fact. I think Ekta might know more on this. I mean, technically, basically, like a technical issue, right? So, when you want to do the derivatives, you already have for the just deriving the differential equation analysis. Just deriving the differential equation analytically, you have to do this IDPs on so many dots and numerators. And then, when you want to do more derivatives, the problem becomes worse and worse. So, it seems like a technical problem, like technological problem. Yeah. And there was a question. Ah, that's now a good question. So we have four here. So we have four here, and then trivial kinematic crossings. They are. So we have an elliptic curve. But as far as I know, they will never talk to each other. Well, they're all box triangles, isn't it? Triangles, isn't it? So, I like one elliptic curve could only talk to another elliptic curve if they are like sitting above it, if there's a propagator more or so. At least that's my understanding. Correct me if I'm wrong. I think because we don't have like an elliptic sunrise or anything, this is the very first elliptic factor. I don't think that they can talk to each other. Yes, that is true. That is true. Probably that will be then. So in the processor, probably there was a Yeah, above it everything talks there there's a a seven master double box that talks to all of this, I think. That talks to all of this, I think. May yeah, did not think about that. Thank you. And one thing, actually, I was a bit confused because in the middle summary, you talked about this epsilon polynomial, and then you would choose not that form, but the other one. But the offshore contributions were a bit off. Yeah, here. But then at the end, you said you have an epsilon factorized for the different on the maximum cut. Max McCartney looks like we have not yet pushed it through to actually do the transformation because I'm still figuring out some pre-factors that they actually become a solution to the maximum cut because I was a bit sloppy in deriving the bycoff. But in the end, they will lead to the fact that this should become epsilon factorized. But then I have no idea what happens below. More questions? If not, then let's. If not, then let's thank Manius.