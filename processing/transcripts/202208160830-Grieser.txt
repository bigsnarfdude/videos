It's my pleasure to introduce my dear colleague, Daniel Greaser, who is also responsible for making B calculus of Nels accessible for a generation. So yeah, I'm very interested in the talk. Okay, thanks, Bar. Okay, thanks, Boris. That's very nice. So I'll try also to make this talk accessible. Let's see if I succeed. And please ask questions. I'm not sure if I can follow the chat, so just ask them acoustically if any questions come up. Well, first of all, I'm grateful to the organizers that they gave me the opportunity to give one of these few talks here. I think this topic may be of interest to some of the people here, although it does not directly involve spectral theory. So I'm not going to talk about spectral theory at all, but I think the main result will be quite useful or can be quite useful for spectral theory, for the spectral theory of the Dirty Neumann operator, meaning this Steklov problem that we have heard about. Okay, so what I'm going to talk about is joint work with Karsten Fritsch and Elma Schroer. Elma is a professor in Hanover and Karsten Fritsch was a postdoc there and before that he was a great student of mine, but now he actually is doing other things. In any case, so let's just get started. So I'll start with the table of commons. Table of contents. I actually do think that this is a useful thing to do, unlike Alejandro yesterday. Now, I think it sort of sets. Well, when I listen to a talk, it sets my mind to what to expect. So I'll start with sort of geometry. What is this thing in the title called fibered cusps? So I'll spend maybe 10 minutes or something on this, and then I'll explain sort of the main analytic words. You know, words in the title: the Kaltur-Wand projector and the Dirsch-Neumann operator. So I'll explain what these are in the classical setting, and then it's also pretty clear what they are in our singular setting. And so the cold-de-Rund predictor will be called C, and the Neumann, the original Neumann will be called N. And so the main result will say that these are pseudo-differential operators of a certain kind, so-called fiber-cusp pseudo-differential operators. And I will not. Operators, and I will not define sort of in full generality what these are, but I will try to explain in an accessible way what this means and what this sort of buys you. So this will be the next, the last thought. Okay, so let me start maybe with a little puzzle for you here. Namely, the question, I have four pictures here, and the question is, how are they related? Maybe I should be a little bit more explicit. Maybe I should be a little bit more explicit what they are. So, the leftmost here, I'm talking about the domain here, which has a singularity like this. So, the singularity is such that these two boundary curves are tangent to first order at that red point. Yeah, so this is what's sometimes called a cusp singularity or sometimes also called a horn singularity. So, this is the first geometry, it's this domain. Now, the second one. Now, the second one, well, you see two balls that touch each other in the red point here. And the domain I'm considering is actually the exterior of that. So it's sort of the three-metal space minus these two balls. And I'm just interested in the neighborhood of that red point. So that's the second geometry. Now there's a third one, which is if you are in the upper half plane of the complex plane and use the hyperbolic metric there. And use the hyperbolic metric there, then you all know this kind of picture here. It's the fundamental domain of SL2Z, and you know, it has all kinds of meanings and so on. And it has this cusp at infinity. And metrically, these two things actually sort of come together at infinity. You don't see it in the picture, but it's in the metric. And so, kind of the geometry I'm looking at here is the geometry of this thing at infinity. And actually, this is not. Actually, this is not exactly what you usually know as this fundamental domain, but I'm thinking of these black lines as boundaries. So I'm not identifying them. This is really this strip with boundary. And I'm just looking at kind of this other part. I'm not interested in these corners down here. They're different. I'm looking at them. Now, the fourth geometry that I want to look at is this one on the right here. And what is it? I mean, I tried to draw it. I don't know if it's clear. Tried to draw it. I don't know if it's clear. You take a cone in three-dimensional space and you kind of look at its infinite end. Sort of, you're not really interested in what happens down here. I'm looking sort of interested in what happens up here. But I'm not just taking the cone, I'm fattening the cone a little bit by a constant amount. So think of fattening it up by, let's say, thickness one or two thickness one. Now, if you're like me, then you would probably say, well, these two four pictures are all completely different. They are completely unrelated geometries. But I want to convince you in these first few minutes that they are actually very much related. And the relation is that the same, well, the first two is not so difficult. The second one is sort of a generalization of the first one. If you rotate the first one around this axis, then you This axis, then you sort of get this one. So that's easy. But the relation to the other two is not so simple, and it's by a conformal factor. So let me explain this in a little more detail and in a way that will be useful for the rest of the talk. So I'll look at this first thing first. And so let me just take sort of a neighborhood of that singularity. And so I want to introduce some coordinates. I want to introduce some coordinates, and the way I'll do this is as follows. I'll sort of this vertical coordinate, I'll call x. And then for a given x, I can look at all the points on the horizontal line here, and I'll parameterize them by a variable z, but this distance will not be z, it will be x squared times z. No? So, why do I put the x square here? Well, this is exactly what first-order tangency means: namely, that now z will vary on a fixed interval, which is essentially independent of x. So you should think of this as z, for example, varying on the interval from minus one to one. Yeah, and then x squared z, then you just get exactly this picture. This picture. So that's the kind of variables I want to use. And there's sort of a way, another important picture related to this, which is as follows. I just want to draw the domain of these two variables. So z is in x minus 1 to 1, and x is greater than or equal to 0, right? So what does that mean? I draw sort of the x-axis, and then I draw the z-axis, and then I draw the x-axis again. And then I draw the x-axis again, I mean the vertical. And these vertical things correspond to these black lines. And so now, here again, I have the z direction, and here I have the x direction. Right. So that's where my two variables x and z live. And parameterizing my original domain with x and z just means I have a map here which I which Which, yeah, you know, it's often called beta. It doesn't matter really what it's called. And the way people talk about this kind of construction, introducing these variables in such a sort of special way, and then looking at the domain like this, is they call it blow-up. Yeah, so this right thing here is called the blow-up of this thing, and what I have blown up is. And what I have blown up is that point because, as you can see, this point corresponds to the full bottom line here: x equals zero. You can take any z, you always get that point. But outside of that point, you have a diffeomorphism. All right, so for the rest of the talk, I will always think of this space on the right here, because in a sense, you have to think in terms of. Because, in a sense, you have to think in terms of points. And this space will always be called X. And as you can see, this space X has corners. Here is a corner, here's a corner. And then it has boundaries, these vertical boundaries and the horizontal boundaries. And these boundaries, the two kinds of boundaries have different meanings. So for example, there is the red boundary at the bottom, and I called it ds of x. S stands for singular. It's the singular boundary because it corresponds to the singularity down. It corresponds to the singularity down here. And then the rest of the boundary here is I call dbc of x. So what does bc mean? Bc means a boundary condition. And you know, it's a part of the boundary where I can impose boundary conditions in my original problem, which is just these black, two black lines. Yeah, so there are two parts to the DC boundary. Okay, so. Okay, so now we have already seen kind of the main object here. We will have a manifold that corners x, and it has boundary hypersurfaces of two kinds, the singular ones and then these BC ones. And they will occur all over the talk. Okay, now what else is important about this? Well, I will be interested in Laplacian, for example. Laplacian is related to a metric, and the metric I'm looking at in my original domain is. I'm looking at in my original domain is the Euclidean metric. So let's just write down what it is: the metric. So I have to take dx squared, and then I have to take d of the other variable. The other variable is x squared z squared. Yeah? And I, so if you multiply out that second term a little bit, then you get dx squared. Squared and then you get x squared dz squared and then you get a few other terms like you get one which is like x times dx and so on, but they are in a sense lower order than these and or of a similar type. You know, they are, I mean, these are the main terms. It's easy to see. And so here we already see also another important structure that we have because we are always interested in the neighborhood of the singularity. The singularity is always at x. The singularity is always at x equal to zero. And so you see that our metric, when written in these coordinates, well, the x square is a very common thing, but here it has an x square factor. So when x is equal to zero, this is not positive definite anymore, this whole thing. Yeah, so the blow-up here means that on the space capital X, my metric is degenerate at the singular boundary and degenerate in this way here. Okay, so. Okay, so this is kind of the main things that I'm going to use. And so let me just say a little bit: well, are there any questions so far? Okay, that's very good. Or maybe not, I don't know. So let me look at the second picture. Now, this is kind of the near the touching point of these two spheres, right? If you remember the previous page. And so I can do a similar thing there. Let me actually, I prepared that picture because it would take too long to draw. Picture because it will take too long to draw it. And so, all right. So, so the analogous picture in this case would be would be this. Well, it got a little bit too big or whatever. And here are your variables again. Well, let me just say sort of shortly what they are. The X variable is now the variable is the now it's the um it's not just the vertical but it's kind of the the distance to the the distance to the red point but in the in the touching plane within the touching plane i introduce polar coordinates and x is the radius and y is the angle in the touching plane you know um the tangential plane to both uh spheres at the Spheres at the right point. Right. And then I have the z coordinate just like before. And now, if you do a similar thing, you think about where do these variables live. And then it's a kind of much more complicated picture. Like the z variable still, you know, it gets sort of drawn out like this. And the y variable lives in a circle, which is not surprising for an angle. And the x variable sort of goes out from that cylinder, that y and z form. And now if you And now, if you so now this is again a manifold with corners, just like before, with two kinds of boundary hypersurfaces. And now, again, if you look at the metric, then, well, what do you have? You have the well, first you have the Euclidean metric in the touching plane where I write it in polar coordinates, which are x and y, and then that's x dy squared. Ey squared. And then I have this other thing again, which I had before. Right? So now you see the structure of the metric is like this. You have the dx and you have the x squared dz that we had before, but you have an x dy. So the y variable sort of degenerates when x goes to zero, like in a different power of x. And that's the general structure that we include. That we encounter. Actually, for most part of the talk, I will just focus on this part, but I sort of want to mention for those who are interested in the general setting that this is kind of the all results apply to the general setting. So I've sort of analyzed what the metrics are on these two examples here. Remember, these first two examples. I'll actually leave out the second example that's an exercise to show what I claim. Exercise to show what I claim later, but let me have a quick look at this third example. Um, and um, I call this the fat cone the infinity. And so what is metric there? Well, infinity, now I call the radial variable I call r and the angular variable I call y as before, and then this fatness variable I call z. And now I always want to put the sort of interesting point to. put the sort of the interesting point to x equal to zero. Here the interesting part is infinity. So I said x equal 1 over r. So r equals infinity corresponds to x equal to 0. So that's this. Now what's the metric on this on this thing? Well I take the Euclidean metric again, which is polar coordinates in R and Y. It's that. And then you have the usual Euclidean metric in the transversal direction, which is dz squared. And if I now do this change of variables here, then d. This change of variables here, then dr becomes dx divided by x squared with a minus sign, that doesn't matter here, and here r becomes 1 over x. So you get this structure. So that's the metric in this case. And you can see it sort of looks similar to the previous, but different. I mean, here the x square is in the denominator, before it was in the numerator here, but it was in a numerator of dz. So, you know, if you look at these precisely. You know, if you look at these precisely, well, let me actually put everything next to each other here. So here's the metric that I just derived. And this is called the phi metric, G phi. And now let's look at the thing that we had before, namely this. And as you can see, this is just x to the minus four times g phi. And as I say, a little exercise shows that the hyperbolic metric is x to the minus two times g ph. Is x to the minus two times g ph. Well, there happen to be no y variables, just like in this case, there are no y variables in the very left picture. Here also, there aren't any. And I'll actually focus on that case later on. And as you can see, y, well, if it is there, here it was on a circle and also here, it's always on a closed manifold, while z is on a on a manifold boundary, compact manifold with boundary. So here this was an interval. So, here this was an interval, z interval, and here also. Well, so this is the answer to the question that I asked in the very beginning: how these geometries are related. They're all conformal to each other, and this is kind of the structure of the metrics. All right, so this is the geometry part of the talk. We want to understand. So, what's our goal now? We want to understand differential operators associated with these kinds of metrics, like the Laplacian. Like the Laplacian, so also the Rug operators, or you know, whatever. But I'll talk about the Laplacian and then the Diocese-Neumann operator. So, and I always have this structure, power of x times times this kind of metric. Okay, so let me just introduce a general definition for this. You know, this thing is called a fiber cast manifold or a phi manifold, and these metrics are called phi metrics. I'll go through the definition rather. Metrics. I'll go through the definition rather quickly. If you don't follow all details, it really doesn't matter because I'll sort of stick to a more special case later. Well, in just a minute, where not all details matter. So the general setting is this. You have a phi manifold. It's just a compact manifold with boundary. And the boundary is equipped with a vibration. So the base of the vibration is called B, and the fiber is F. And then when I introduce local coordinates, I will. When I introduce local coordinates, I will call the coordinates in B I call Y, coordinates on F I call Z. And then, so a neighborhood of the boundary is always product times the boundary. So I have variables y and z here. But then I also have this variable x. So the boundary is x equal to 0. And then, as I said before, a few metric is one which looks exactly like this. And you can write it in some more invariant form like this, plus some. like this plus some you know i mean there are always some lower order terms which you know lower order in the sense when x goes to zero and and maybe mixed terms but they're not so important um well i mean this is the feminifa but at this point we did not have this bc boundary so this is actually the classical definition given by mazio and nerros in their 98 paper and um now we have this additional boundary that the fiber has a Boundary that the fiber has a non-empty boundary. This is our setting that we're interested in. And so let me just remind you of what that picture was. You know, it was this kind of thing here. This was my X and this was the B C X and this was D S X. And these are these two kinds of boundary. And so I will follow. And so I will focus here on the special case of this, which is called the cusp manifold, which corresponds to B being a point. And because the variables on B are Y, it just means there are no Y variables. And this is like in this example here, if you remember, this example just had an X coordinate and a Z coordinate. And one thing to observe for the rest of the talk is the following. Suppose I have a manifold, a female. A phi manifold with B C boundary. And now I take its B C boundary. Then what you obtain is a phi manifold, but one without B C boundary. So let's look at this example. Maybe I'll call this thing Y. Yeah, so Y is the union of these two black lines. That's the B C boundary of X. Well, in this case, it's totally. Well, in this case, it's totally simple. It's just two black lines. I mean, it's just two lines. And that's a BC manifold because the reduction of the metric for that is well, sorry, it's not a BC. It's a phi manifold. Which itself does not have a BC boundary. It does have a singular boundary, namely the points down here, of course. Right. And this just, of course, corresponds to the fact that if we'll go back. Corresponds to the fact that if we'll go back to the very first picture here, if I take the boundary of this thing, it's just these black lines and they both enter that singularity. Okay, so this is just this notion of few manifolds. Now we want to know something about the operators, like Laplacian. So let's get there. So that's called, well, So that's called. Well, from now on, I will stick to this cask case when there are no y variables, but everything actually generalizes to the phi case. And so, as you recall, this was what's called a phi metric, or in this case, it's a cusp metric. Well, now it's very easy to see that the Laplacian looks like this. So, here for the dx over x squared, the x squared is in the denominator, and then you get with the part. You get with the partial derivatives, you get the x squared as a factor, not as a denominator. Yeah, so the capital dx are just as usual: one over i d by dx. And then the dz just turns into the partial derivative in z. And then there are some lower order terms, which I don't really care much about at this point. Or whatever. Now, as you remember, we are not just interested. You as you remember, we are not just interested in these metrics, but we are actually interested in multiples of this metric by some power of x. Like there was x to the minus 2, x to the minus 4. So in general, it's x to the minus 2a times such a metric. So g tilde is now this thing. And then, you know, it's again very simple, very easy to see that in the classian just gets an extra factor here, plus some lower terms. Okay, so here's the main definition now. Just a general setting, what includes this Laplacian is a differential operator, you know, which near x equal to zero, so near the boundary, has this structure, smooth coefficients. And then you have these, you know, what you have to focus on is these combinations, the x squared times dx. And the dz doesn't have an x with it. So that's a custom fresh operator, for example. So that's a custom fresh operator, for example, Laplace and SVC up here. Now, if you want to do like investigate things like ellipticity for these, I mean, like Freton property and so on, you want to have a suitable notion of ellipticity. And the correct one turns out to be as follows. I mean, usual for ellipticity, you would replace dx by some symbol variable. But here we do something different. We will replace x squared dx by a symbol. x squared dx by a simple variable which I call tau. So x squared dx I replace by tau and dz I replace by zeta. So that would be called the c, the cusp symbol. And if I only take the leading terms, which means here I have only the maximal order equal to m instead of less than equal to m. And that's the cusp principal symbol, which is invariantly defined. Okay. Okay, so that's the cusp principle symbol. Like for the Laplacian, it's just this function of tor and zeta. And then you have this notion of ellipticity sort of adapted to the cusp geometry. I'll just call it C elliptic for cusp elliptic, just meaning that this thing is invertible whenever these variables are non-yeah, this pair of variables is non-zero. And the Laplace and obviously. And Enter Laplace obviously satisfies this from here. Now, it turns out if you do want to investigate things like the Freshhorn property for such operators, it is not enough that they are C elliptic. It turns out you do need another condition. And this is kind of well, let me just give a very kind of quick motivation for this. For this, because I mean, let's look at this domain here, and our operator is kind of a different operator on this domain. And what does the principal symbol do? The principal symbol tells you at each point, what does the operator look at this point when you freeze the coefficients at that point? And if you want to invert it up to arrows, then kind of invert it at every point. You glue these things together and you get a parameter. Now, turns out that this point here, this boundary, this red boundary point, is very special. I mean, well, maybe I should say something. I like to think of these freezing coefficients as local models of my general operator P. So you have these constant coefficient local models around each point. And you understand P if you understand all the local models. So, but this also tells me that, well, I should probably also have a local model at the tip. Probably also have a local model at the tip. And then, what might that be? And there's some work involved in trying to figure that out. That's, of course, what Maxio and Menros did. And that extra model at the tip is called the normal family. And you can already see here's the formula. And so, what you do, you take this operator and first of all, you set x equal to zero. You remember x equal to zero means the singularity. x equals zero means the singularity, so this is exactly saying I'm at the singularity, and then I have to replace this x squared dx by tor, but I do not replace dz. Dz stays as it is, and so this thing is not a number, like this here. This would be a number, but that's a differential operator for each torque. So it's a family of differential operators on that. That's kind of a typical thing that happens in singular problems, that you have this second, sort of the second. Second, sort of the second symbol in this case, the word that people have are using is normal family, right? So, in the case of Laplacian, well, it's just this. You know, I have just in my operator, I've just replaced the x squared dx by a tau and then we retain the other. So, it's a family of operators on that interval z and then z. So, that's what customized operators are. What are customized venture operators and what their kind of essential structures are. So, remember, the first essential structure is the principal symbol, the second one is this normal term. Okay, so now what we have to do with this. Now, this was the first part of my talk. So now I come sort of to more analytic things. So I want to explain the So, I want to explain the other two main notions in the title: Calderon, Project Engineering, Norman Operator. Well, okay, maybe I should stop for a second and ask if there's any question at this point. Okay, let's go on. Okay, so here, what's the classical sense? Okay, so here, what's the classical setting for the Carlin-1 projector? I take a compact, smooth manifold with boundary. And in our previous context, this boundary would be the BC boundary. So the boundary where I can impose boundary conditions and not the singular boundary. So there's no singularities here. Yeah, no singularity. Just everything is. And so, and I take a elliptic differential operator, and elliptic means uniformly. And elliptic means uniformly elliptic up to the boundary. And if you like, you know, you can include sections, then you can do things like the rock operators and so on. But I'm not going to put them in the notation. Okay, once you have such an amp-order operator, you can define its boundary data space. And what is it? Well, you take any smooth function on the manifold, smooth up to the boundary, which satisfies the homogeneous equation. Which satisfies the homogeneous equation pu equal to zero. And then I take its boundary values. So the boundary value of u, the boundary value of its normal derivative, you know, up to the m minus first normal derivative. The order is m, and so these are exactly m things. So for the Laplacian, you would check the digitally and the normal data. Okay, so this is called the boundary data space. Now, of course, you know. Now, of course, you know from your experience with the Laplacian that this is far from the whole space here. You cannot prescribe both the original Neumann data. You can only prescribe one of the two things. So kind of the dimension of this thing is, you know, it's something like half or maybe square root, whatever. Well, they're both infinity. But this is much smaller than this. And this is. This and it's a subspace, so I can look at projections, and such projections that's what is called a Kylter-Wong projector. Any projector. This is not strictly true. There's actually not an official definition of this, but there are actually different sorts of projections which are all called Cald1 projections. As you know, when you have a subspace, there are many projections to that subspace. One could be possible. One could be more specific here, but it's not important for the talk. Okay, so what was Calderon's idea? So, I mean, what is kind of the main problem about sort of classically about boundary value problems is you have an operator P and you have some sort of boundary operators and you want to know that this boundary value problem is threatened and you want to understand smoothness and so on. And he proposed an approach to this. Approach to this problem. And he said, Well, once we have such a projector and we understand its properties sufficiently well, then we can do all these other things. So I'm not going to talk about all the other things. I'm just talking about this a little bit. So what's his theorem? His theorem was, or his claim was, that you can choose this projection in such a way that it's a pseudo-differential operator. That's what this sign means. It's your first operator, of course, on the boundary, on m tuples of functions. On m tuples of functions. And he also gave a formula for its principal symbol. Now, why is this of interest, especially here for our Steklov eigenvalue community? Well, because the Dirich-Neumann operator can be calculated using the Kalgoron projector. And the way this works is as follows. Well, first, let me recall what it is. So you have the Laplacian for some metric on your manifold, and then the Dirty-Nover operator. And then the Dirty Lumber operator maps functions on the boundary to functions on the boundary. So you take a function on the boundary, you solve this problem, our Dirichlet problem, this boundary value is f, harmonic function u. You take the normal derivative of that function, and that's by definition n of f. And right, and now it turns out that you can write n in terms of the Carl duron projector for the Laplacian. Projector for the Laplacian. So remember, the Kalon projector is a map in this space. Now, m is equal to 2, so it's actually a 2 by 2 matrix of pseudo differential operators. If I number them like this, then there's some formula here, which is sort of a simple exercise to derive. Well, you have to think a little bit, but it's not very difficult. And this thing is always invertible, so it all makes sense. So what this tells you is that if you understand that C is a zero differential operator, That C is a zero differential operator, then all of these are. And if you understand their principal symbols, then you understand the principal symbol of n. And this implies this well-known result that was mentioned yesterday several times already, that n is a zero-functional operator of order one, whose principal symbol is this C else value. So, this is sort of Carl de Ron's approach to boundary value problems and the Diotian-Reiman operator. Okay, so so far, Okay, so so far X had no singular boundary, so our goal now is to extend this to the case with singular boundary. So I'm going to do this now and I'm going to just state the main theorems. So remember, we have this notion of cast manifold with B C boundary, which was this thing where you have kind of the B C boundaries in black and this singular boundary in red. And the whole thing is called X. Eggs and um right, and uh, we also had this factor in front which came from you know the conformal factors of the metric. So you can forget it, it's not really important, it just means that you can do this in general for any A. Okay, the main thing is here you have a C elliptic cusp operator, and then the theorem is that you know things work as expected as they work in the smooth. As they work in the smooth setting, namely that the Calder1 projector is a pseudo-defension operator in some adapted calculus, and I'll explain in a second what that is. And you can calculate its principal symbol by the usual formula, which I didn't give, it's not important. And you can calculate its normal operator. It turns out to be all very, very natural, namely, normal operator of the Cali-Rond projector is equal to the Caldurant projector for the normal operator for the normal family. Well, Um, well, the proof of this theorem is actually quite involved. I mean, you really essentially follow kind of the classical route and combine it with this cascal, but there are many technical details, so it's a rather long paper, actually. Okay, and once you have that, you can you know you can come to the Steel-Neumann operator theorem. So, let's say I have a metric which is x to a times the cusp metric. X into A times the cusp metric, like the examples I had before. Then the Dirgy-Neumann operator is in this calculus times some X factor. And its principal symbol is as usual. And it's sort of the normal operator of this, when I put the X to the A over here, is exactly the Dirigenormal operator of some associated thinking, of the fiber Laplace and plus tau squared. Laplace and plus Tulsque. So, and in the rest, let's see how much time you have, maybe around 10 minutes or something. So, I want to explain a little bit what this means. I'm not going to say anything about the proof beyond what I did, but I want to say something about what this means. Well, are there any questions otherwise at this point? Yeah, so I had a question, Daniel. By saying C elliptic, you only assume invertibility of the cusp principal symbol, but not of the normal operator, normal family. Oh, yes, yes, indeed. Yes, indeed. And I yes, yes, indeed. For the first theorem, oh, yeah. Oh, yeah, that's right. I forgot to. Sorry, for the first theorem, this is good enough for the Carlborne projector. But for the DH Neumann, you do have to assume that the boundary value problems, yeah, I have to add this. Assume that the boundary value problems for n of p of tau are invertible. Yeah, so that's kind of replaces the full electricity that you know from the fecal tools. All right, thanks. Yeah, thanks for asking. Yeah, but for the DHA problem of the Laplacian on this casp domain, this is satisfied, for example, because you know this is just the second derivative of an interval and that's invertible. And that's invertible with the Auschi boundary condition. So, okay, so we have to let me just recall a little bit about the pseudo-ventural calculus. So what are psi dO's? And let me start sort of in the case, a very classical case, X, sort of a closed manifold. Closed X. And I'll just take a very simple, just so. Just take a very simple, just so X is just a circle, yeah. Um, just one-dimensional. So, um, and then I want to just explain in this example what the what superferential operators are in the circle. And so, what you do is, well, first of all, you write operators using their Schwartz kernels. So, in other words, you write them like this, where k is a distribution on x times x. So both of these variables x and x prime are on my space capital X. My integral goes over my space capital X. So that's the kind of the first thing. Of course, there are different ways to introduce your. Of course, there are different ways to introduce your flexible operators, and I'll use the way which is most useful for us here, and which has been advocated by Richard Merrows in all the singular settings very much. And I think it is actually very suitable for this. So I think of operators as being given by their Schwarzkernels, which are distributions on the double space, x times x. Now, what's the double space? Well, let me just draw kind of the like this. Like this. So I have the x variable, which is on a line, what's locally aligned. It should be a circle globally, but it doesn't matter. And there's the x prime variable. And so let's just recall what the differential operator, if you have a differential operator, and let's just look at a very simple one, let's say d by dx to the power k, then you know what the Schwartz kernel is, right? And then you know what this Schradz kernel is, right? It's just delta to the case derivative of x minus x prime. And so what you can see is, well, what is this? This is the delta, a derivative of the delta distribution on the line where x is equal to x prime, which is the diagonal. Yeah, so this is the diagonal. x is equal to x prime and um now if you have a general differential operator then you know it's it's some like this and here also some a alpha so um you know it remains the same it's it's only singular on the diagonal but with some variable coefficients now uh what you do now is you um i take the fourier transform I take the Fourier transform. My order here is not very nice. Fourier transform. And I think of x minus x prime as a new variable, and its Fourier transform variable I call c and then I get some a k of x times c to the k, which of course you all recognize as being the symbol of the operator. And so, okay, so starting with any differential operator. Starting with any differential operator, I can look at the Schratz kernel, and the Fourier transform gives you the symbol. Now, what pseudo-differential calculus does, it replaces this by any function sigma of xc, which has somewhat similar but more general properties. You recognize this is a polynomial in C. Well, this doesn't have to be a polynomial in C, but it should still be sort of homogeneous or almost like a sum of. Or almost like a sum of homogeneous terms, you can see, and I'm not going into details, of course. Okay, so now for zero-differential operators, I take these symbols and I get my Schwarz kernels. And these Schwarzkernels define an operator, and this is how the supermatch operators. And I have to recall what is the main, what are the main things? What are the main things, Daniel? Yes. Is the top line missing a u of x prime? P U of x. Oh, thanks a lot. Yes. Yeah, sorry. Thanks, David. Of course, there's a u of x prime. Otherwise, it doesn't make any sense at all. Thanks. So what are the main properties of this Stratz kernel? Well, it's that it's smooth for x0 equal to x prime. equal to x prime meaning outside of the diagonal and it has a specific kind of singularity at the diagonal and this is called the conormal singularity at xx equals to x squared and you should think of this as some natural generalization of these delta things corresponding to generalizing polynomials to these other functions and um so okay so this is what superferential operators are uh for a closed manifold but now But now we are interested in not a closed manifold, but we have a boundary. And so I have to go to the next, oops. No, I don't know. I guess I have some scratch paper here. I have to get how do I get to the very end? Just a second. No, okay. Now, okay. Okay, let's go on here. Okay, so now let's do another example with boundary, and I'll just do a very simple one: R plus just the positive half line. So here's zero and here's x. And now for my differential and serial differential operators, what matters will be also to know what happens when x goes to zero. So here I write my double space again. Um at this at this oops, what happened here? Oh, okay. I'm sorry, I don't know what happened here. X, X prime. This is the heat sometimes. Well, remember, this X actually, well, let me not call it X, let me call it this space actually occurred as the As the BC boundary of my other object, yeah, right. This was this is one of the two components of this, you know, of this thing that arises from my class. So this is actually the very thing you might be interested in if you're looking at singular domains in the plane. And so, right, so what I want to Um, right, so what I want to know is how does my operator behave? And what's interesting in particular is what happens when x or x prime goes to zero. So let's look at what happens. So well, let's look at what happens when x goes to zero, but x prime does not go to zero. Let me just say x prime is greater than epsilon or something. So this means I So, this means I'm kind of approaching this left face here. Oh no, what's this? You see all this? Yeah, you do. And then, well, what I'm doing here is I'm writing down the definition of the cusp pseudo differential operators. So, so that you can understand what our main theorem says. The main theorem says that the Dioschi-Meiner operator is a cusp pseudo differential operator. And so it means its integral kernel has the properties that I'm writing down now. Yeah, and so the properties are as follows: namely, that this is actually rapidly vanishing when x goes to zero. Yeah. To any order, faster than any power. Okay, so this is the first information. The second information is, well, what if both x and x prime go to zero? But let's say they. But let's say they go to zero along a diagonal. x prime divided by x is equal to t, but which is not, well, it's not a diagonal, along a line, which is not equal to the diagonal. So what I want is that x prime over x minus one is bigger than epsilon. Yeah, I'm staying away from the diagonal. And then again, my condition will be that. I my condition will be that k is big O of x degree. And then finally, now what's really interesting, of course, we do know already that along the diagonal, this thing is singular. So, you know, similar statement as before couldn't happen. But how do we kind of approach the diagonal? And this is described by sort of parabolas like this. And this is the most interesting regime. This is the most interesting regime, and it says, well, when x goes to zero and x prime over x minus one goes to zero linearly, like t times x, then, well, what it says is that k is smooth in x greater than or equal to zero for t not equal to zero and has co-normal singularity. Co-normal singularity edge t equals zero. So you realize t equals zero is exactly the diagonal. Right. And so this is the precise description. And those of you who know about blowup, of course, they will recognize that this is nothing else but saying that the pullback to the phi double space is vanishing to infinite order here. Is vanishing to infinite order here, infinite order here, smoothed here, infinite order here, and has a conormal singularity uniformly to the boundary along the diagonal. Right, so this is that thing. And just to finish, well, let's see, I started five minutes late. So I'm, yeah, I guess I'm at 50 minutes. So maybe I'll just take two minutes for. Two minutes for sort of one example which is higher dimensional. Namely, let's say my original omega, so this was omega in this case. Now let me take omega equal to the cusp in three dimensions. Oh no, this my iPad doesn't like the heat, I think. I have had this before. So you kind of rotate. So, you kind of rotate this thing. Yeah, you have to bear with these funny lines here. I cannot do anything about it. So, this is the case where the fiber is two-dimensional. And so the boundary, so the y here would be just a circle times, you know, times the x direction. So x goes up and z goes here, but it's this. And so the way this looks then, So, the way this looks then, just to tell you that I can also say something in this case, is: well, I'll just express it in terms of this space here. And so my, well, my double space corresponding to this thing here, now it has an x variable and an x prime variable, but it also has a z variable, kind of a z variable and a z prime variable. kind of a z variable and a z prime variable so they're extra directions here and uh so what i do is i blow up this this corner line and then i blow up sort of the center of that again and let me just say result is the result is just the same as above except you take the product with z and z prime yeah so Yeah, so it's kind of the z and z prime directions. And then the diagonal sits somewhere here, and you have all the same kind of statements. And maybe the one very last thing I want to say is if you do want to do things like spectral theory and so on, you want as much as possible to have as much as possible information about this inloop kernel. So I told you already. Kernel. So I told you already the information that it's conormal along the diagonal and what its symbol is. And I told you it vanishes to infinite order at here, at these phases here. And I told you it is smooth at this phase. So, you know, the natural question is, so if it's smooth, can you calculate what that smooth function on that phase is? Well, the answer is yes, you can. And that smooth function is actually exactly. That smooth function is actually exactly the integral kernel of the normal operator. Yeah, well, whatever, of p. So, and I had a formula if p is happens to be the Deja Neuma operator. I did give you a formula for the normal operator. And so you can, from there, you can actually find out what that smooth function is. Okay, and now I'll finish here and. So I will finish here and hope that somebody will make good use of this result maybe. And thank you for your attention. Thank you very much. Let's proceed with the questions. I have a question. Please. Thanks, Daniel, for a very interesting talk. So, I'm wondering, since I'm interested in applications to spectral theory, Applications to spectral theory. So, is there a well-understood spectral theory of this apparatus, like the model apparatus that you discussed? This customer? Actually, I'm not really aware of it. I mean, you see the thing, when you do spectral theory, this number little a starts to matter a lot. Remember, we had this conformal factor x to the power a. And if a is zero. And if a is zero, then you have a sort of complete manifold, something you know, your singularity is at infinity, and of course, you have continuous spectrum. And well, plus potentially embedded eigenvalues, whatever, but you certainly would expect continuous spectrum. Well, if A is positive, like in your cusp, it would be two, then the main theorem said that the And the Dirac-Lambda operator is actually x to the power minus 2 times this Smooth function. And well, I have to admit, I should have done this some time ago, but I haven't yet. And I'm not aware of other people who have actually studied the spectrum. So, for instance, it would be very interesting if one can sort of apply this machinery to study, well, first focus. To study, well, first of all, classification of spectra, but also spectral asymptotics. And recently we did work with Michael Karpukhin and Jean-Lagasse, which in particular covered the case of planar domains with cusps. And it turned out that the speed of the cusp is important. So for certain speeds, you get essential spectrum, but for slow cusps, you actually get discrete spectrum, and you can just prove while law. Sorry, by speed, you mean that. Sorry, by speed, you mean the power of like the order of tangency? Yeah, exactly. Yeah, right. So, for a finite order of tangency, what do you get? Well, it depends actually. You have to have an order of tangency which is very, like, very small in order to. Okay. Okay. Thank you. Yeah. Yeah. Yeah. Of course, I should have mentioned this also. I said in the very beginning, my example was a cusp where you have first autotained. Of a class where you have first-order tangency. I mean, we didn't do this, but if you have higher-order tangency, the technique, of course, I'm sure applies to treat that as well, but you know, hasn't been worked out in detail. But I'm sure you get similar information. Okay, thank you. Other questions? Okay, we had some. Okay, we had some questions during the talk already. So let's thank the speaker again for this very interesting talk. Okay, well, thanks for your attention.