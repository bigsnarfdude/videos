I just want to make a little bit of comments on these two. So why is the ST boost? A lot of people already use this, another is the cut of gold. I was thinking our theme kind of for the in general at the data science, it's very important to participate in a big program. These days there are a lot of these online kind of open source programs. But what the two big things I was seeing to learn from here in the IC Boost, the coding styles is so important. I know Professor Jing Yan for united me come here, and also Professor Jing Yan have a short course and Have a short course and also the teaching this kind of coding style, how to use GitHub and a different coding style. And he also educated me, he also going to publish a book. And he's really looking forward to these books. So today I'm going to talk about the generality of AI on a smooth manifold. So before I talk about it here, I just give a little bit of bigger background why this is so important. I'm coming from Eli Lee and the company and I'm kind of leading the daily AI machine learning on iPhone. The daily AI machine learning effort. So, people probably know, may not know the media and the company. So, right now, it becomes the largest pharmaceutical company in terms of market attack, and it's bigger than the combined with the Pfizer Mark and the Sun Fee. So, our team really have long histories of making medication during the past 500 years, and people talk about the most important medication to change the human life. Among 10 medications, three of them are from Lee. Exactly, 500 years ago, Lee collaborated with the university. Years ago, really collaborated with the University of Toronto and commercialized the first insulin product that saved, if not billions, hundreds of millions of people's lives in the insulin. The second product released by Lee commercialized is panicinium. And so that is antibiotic just because of panicinium. And the humans improve their life, average lifespan from 14 years old to 65 years old. So the third one is making the changing the history also the project. The history also the product, and that's the first drug to change the kind of the neuroscience on the depression. So, our team is: I'm currently leading the data science machine learning AI group in Lynn Company. In my group, I have like 30 PhDs from multiple different disciplines. And for the past couple years, we did hundreds of products from different parts of the machine learning AI. And the topic I talked about today is that I truly believe the most important product among all of these products, our team working. Of this project, our team working on that have both theoretical implications for our field and statistics, and also have many of the real implications. Just because of the methodology we talked about before, last year we made the lead first denominal design, the design of large molecule, the design molecule at the help in the patient. And this molecule is tested in the lab and verified in the lab. So I feel like this should be very cool methodology and very cool areas. And this methodology. Areas. And this methodology where I'm going to talk about also have a big implication for our statistics. You probably will see why this is so important. This picture, oh, by the way, yesterday I observed a lot of people do the presentations and their slides had the ratios in the 4 to 3. I just changed my, like from the 16 to 9 and to fill in this kind of this screen. And if there are something kind of I've had a little problem with. I've had video problems, but let me know. This is my first time to present these slides. So, here is the kind of my outline of today's presentation. I started to talk about the kind of giving the general introduction of generative AI. And in particular, I want to emphasize why it is so important of statistics. When the people mention the general AI, some people just are thinking about some people thinking about these chat GPT or just kind of the generative image. But the fundamental thing for here is generating. For here is generating AI along us, actually, maybe the first time along us to understand high-dimensional PDF. The high-dimensional PDF is I give you the thousands of a picture of a dog. After that, I give you another picture. I ask you, what's the probability of this dog in the hydrogen? What's the p-value of this dog? During the past, there's no statistical methodology to address this. But the fundamentally, the general AI is able to understand this. I've seen this one of the fundamental problems. Is one of the fundamental problems of statistics. I will talk about why this is so important. Or maybe just give a little bit of background of this picture. So people are probably very familiar with this picture, and they put a pong and ask the daddies to generate the image, to generate the astronaut riding a horse on the space. You can consider this generation in a conditional generation. We also talk about this kind of why this is so important in later slides. And my slide, I'm also talking about. My slide I'm also talking about right now in the very good moment in terms of motion learning, from a technology to the science. What I mean by the technology, the human to not already know the labor principle, probably 5,000 years ago, people know how to use the labor. Until most recent 500 years, people know the labor principle. And under the labor principle, when the machine is heightened and made a lot of difference for our field, for the field, for the science. These are from technology to the science. Technology to the science. I also, in our statistical field, there also very significant moment also happened from technology to the science. I still remember probably 30 years ago, like 27 years ago, that benefit computer scientists first come out with the idea of the majority voting. In the majority voting came with the idea in 1986, 1987. Probably for the 10 years, there's no good methodology for the majority voting. And he was in the 1996, correct me if I'm wrong. And the AWS, the method comes out. Other ADA boosts come out, this comes out from better computer scientists. Nobody knows why this works well, why it doesn't work, why it doesn't work, and a certain extreme situation is unstable. Until the annual discussion paper published in 2000, 2001, 56 few people come out to better understand the AIDA boost is equivalent to the exponential loss. its potential loss and the exponential loss is unstable. It's unstable to the outer layers. They change this to the kind of gradient boosting algorithms. And the gradient boosting algorithm becomes very popular for the next 15 years until the computer scientists like ICU and King GTL people improve this in high-performance computing environment. So I think this is another good story from technology to the science and the And by theory, and improved methodology. What I'm going to talk about for here, I also believe there are significant opportunities for statisticians and for our theoretical people to get into these areas and to make a significant contribution in the field. I also feel like I'm going to talk about three things. The first part is the generality of AI under the same framework of stochastic differential equation. And these tours can help people to better understand the theoretical property. Many of the theoretical. The theoretical property, many of the statistical properties can be better understand today compared with the last year this time. The second part of this is that I also think it's important that during the past, they have so many ad hoc approaches and even in the deep learning, if I ask a person, so in the image recognition, what's the one trick that can significantly improve the image recognition? People probably call me like the data augmentation. The data augmentation is rotated these pictures. Data augmentation, they rotate these pictures, this image. But think about if they move in the three-dimensional space, not only to incorporate the rotations, they also need to incorporate the translation. There are so many different possibilities. The brutal force of the data augmentation doesn't work anymore. But on the other side, this rotation, this is symmetric translate, this symmetric and it relates to the group theories. And by the line with group theory, so we have. align with group theory so we have kind of incorporated this into the into deep linear significantly improved the deep linear accuracies that are one of the areas that for example people probably familiar with alpha photo too that's one of the most important things alpha photo 2 get to get improved their performance alpha photo 2 use these ice3 groups and have these uh uh kind of the ipa invariant point attention mechanisms and improved algorithms Can I ask a question? So I have a whole bunch of forces to change the color and argument in my sample size. Yeah, change the colour, change the contrast. So what you comment about that? So this time we can outperform this I will talk about but uh to answer this question we have thinked about But to answer this question, we have think about, I'm thinking of this kind of lot of these bring the different perspectives. Think about the traditional CNN. The traditional CNN is the picture is fixed. My filters is moving around. But they can also look at another perspective. My filter is fixed. My picture is moving around. So these two things are equivalent. But even my picture is moving around. It doesn't consume. My picture only moves in this translation. My picture can be rotated, can be changed, can be changed. Be rotate, can be changed, kind of changed in a different color, but by filtering the six. By different perspectives, they can independent in the different types of the group. I'm going to talk about for here in the primary, in the SU3 group, in the dosation and the translation, because that is important for our molecule to molecule design, but certainly they can depend on other different types of groups. Yeah, because some people say this coloning thing is also data augmentation idea is also general. Idea is also generative, but I think that's probably different. Okay, good, good. So there's a kind of different, a very popular framework for generative AI starting from the 10 years old, the Bay GAT models, the flow-based solutions, kind of the auto-regression models, and the ChatGPT and the BERT are different attrition mechanisms based on this. And the most recently on the uh diffusion models and the score-based generative AI. Models and the score-based generative AI. My first part of the talk, I'm going to primarily focus on these two approaches, and we will kind of introduce these two approaches and why they're so important. And after that, unify these altogether and the stochastic differential equation. And I will talk about why we need to unify this stochastic differential equation framework and what kind of good properties we can generate. And very interestingly, and we all have a different framework. So, handling the foundation model. Work so handling the foundation models, and I'm seeing this another highlight. With the people talking about the foundation model, not the people only familiar with the choice for learning, but right now, with a better theoretical framework, they have much, first I feel, is a much better framework on the foundation model. And because of that, actually we really build this, test this, and generate the models. What I'm talking about for here is not only theoretically important, most importantly, it works. It works. We generate a lot of kind of good results on top of this. I also think that some of the older models, because people are probably familiar with the GAP model, populated already can outdated the GAM model. It has very bad performance, it's very difficult to treat the mode collapse, and probably in the very near future, not many people will use the GAM model. Now, let's get started. So, why is it so important for the generality AI? And people look at the generality AI from different angles. I'm seeing the most important foundation inside generative AI is helping us to understand. Generative AI is helping us to understand the high-dimensional PDF. Understand the high-dimensional PDF, that's the holy grief of statistics. That's a very, very challenging problem. Give the rhyme the idea. So suppose we have 1,000 picture dogs, and I ask you, kind of, what is the high probability? Any of the pictures with the high probability and low probability, I believe that's known methodologies can handle the high dimension of the data. And other things, can I get a two-picture dot by the fact that things about this, and not only This and also, not only we don't understand the probability, we also cannot do any of the operation on top of this. That's a big limitation. I give it a big fat dog a picture, another slim dog. So I want to kind of get average, these two pictures. So why would I mention average to this operation? I want to generate the the dog, probably not fat and not a slim somewhere in the middle. But if you simply do the pixel by pixel average, you you will generate nothing. You will generate nothing. But the question is: can we, if we can do some operation on this, can we better understand the distributions? So that's the fundamental of the generative AI. Once we can better understand the high dimensional densities, then we can generate any of the new dots, the new pictures. And also, in this space, we can do a lot of calculation operations. Another thing is important is kind of, once we know these densities, we know everything. We know everything. Not only we can also do the conditional generations. The conditional generation, we can generate a dog, wear the sunglasses. So that's the conditional information. At the very beginning of my presentation, want to generate, I put the pump and generate an astronaut riding a horse in the space. That language, that sentence, is a conditional information. We can put this into the generations. So that's the power of the generation. You can look at it from this way. Many things can be highlighted. Way many things can be hyped. For example, in our general data, people do written data imputations and copy many of these techniques. You can imagine. I was probably thinking that this is maybe the most important contribution for the field of AI to copying the statistics for the past three or four years. So, how can we generate the image? And the important thing. And the important thing is, how can we map this in my laptop? I have thousands of pictures. I want to map this to another distribution. Another distribution, the key thing of this distribution is the sample-friendly. So this is the keyword. The sample-friendly is the keyword. Because in the original space, it saw the picture doubt. I cannot do the sampling. I cannot generate the new pictures. But for here, this is some more friendly, it's more easy to generate. How can we do that? How can we do that? The two steps is the forward and backward. The forward is kind of starting from the original pictures and gradually adding the noise from one step after another, gradually adding noise and shrink this kind of signal a little bit until these pictures are matched to the complete noise. So even put on the formulas, it's like this. So this is kind of the forward process gradually. This is a previous image and general analysis. Condition of previous image and candidate another image, and this image is just keep adding the noise. So that's no problem. The important thing is to kind of we learn from a condition of this image, we learn backwards. So coming on my previous image, what's my previous one? So the reason I put the theta for here, it is learned from a neural network. They can learn backwards. So this is kind of the general approach called DDPM stable diffusion models. So, as I mentioned before, I'm going to talk about two methods: stable diffusion model and a score-based solution. And in the end, we are going to unify this in the stochastic differential equilibrium framework and talk about the good component of statistics. Now, putting into this little bit of kind of formulas, these are kind of just described in the previous slide. So, conditional of the previous one, we added the kind of, we generated a new one. And these generations and This generations and so the reason I put kind of we put the beta from 0 to 1 in general the small numbers. The reader I put for here, you know, we want to keep the variance in the same. You can simply do the calculation. The variance will keep the same. We don't want our kind of variance blowed up. So this kind of the kind of the generation kind of forward process. After doing the forward process, we want to build a deep neural network to learn the backward. To learn the backward, there are different ways to learn the backward, but a simple way to learn. Learn the backward, but the simple way to learn to learn this point. The importance for this equation is this gives you this error curve, and this error term is exactly the same error terms. And that's many of the tutorials, papers online. I'm not going to talk about this kind of detail, the formulas, but just to give the intuition is from the backward, continue to learn this and give the noise and through the neural network to predict, to learn, kind of to do the noise. Once we build the neural network, we can learn kind of what's the kind of the kind of what's the kind of the what kind of the noise and we can do the sample we can do the generation the generation is conditioned on the complete noise because gradually going back to generate the image gradually going back to generate the image so this is how the image generation so which means I give you the noise and from the noise we have gradually to generate the image and also this the interesting part of this kind of people may ask kind of uh previous in the computer neural networks In the convergence neural networks, when the subset is small, only have one picture or two pictures, the algorithm doesn't converge. But for the kind of generating the DPPM, those methodology, even you have one image, you can still do the generations. But certainly the generation, the performance is not that good. So which means if I only put my pictures in the training model, all the pictures generated is the exact same as my picture. But on the other side, if you put a thousand of dogs pictures, who can generate the new type of dog? The new type of dots, the new dots. So, this is how the DBPM, this is how the DDPM works. So, that's another kind of a popular approach is called a score-based score-based question on the previous slide. So, the last equation, is there a mechanism, I assume the variance will keep on shrinking, right? So you keep that tape on. Yeah, so the forward process, the kind of the noise in the kind of the kind of the noise in the kind of the total variance you can keep the same because you can keep single kind of because this is true the value will keep the same and but the the signals keep become smaller and smaller after keep a time you become after that the signal becomes complete noise and we go back to worse but after doing this kind of gradually make the signal become bigger and bigger this equation just looks worse over this one I'm not doing the introduction, the popular DDPM. You saw the paper on DDPM, they require a lot of detailed information. But I'm thinking very interesting. I found the original paper on DDPM. Original paper or DDPM, the proof is incorrect. You can adjust some of the expectation, we should not have the expectation, but in general, it doesn't matter, and this still works. So the second approach is the score-based generating model. I'm thinking this is also very interesting. And so without a lot of generalities, all the PDFs can be written this way. But the challenging part is that the normalizing constant. So this is the internet. Constant, so this is kind of evolved with high-dimensional integrals, and in general, this is intractable. So, these are kind of the problems. But the question is, to solve this problem, some of the previous works, we just want to take the derivative of these functions with respect to x, not with respect to theta. And people call this like a standard score functions, not feature score functions. Feature score functions, people will all fire. score function we people we're all very familiar with is take the derivative love of love likelihood take the derivative with theta but for here take the derivative with love with love likelihood take the derivative with x so why kind of the general question why why we are using this guy so intuitively because we know the kind of the first part is once we take the log likelihood and take the derivative in fact x the c theta will go away because this is this Will go away because this guy is nothing related with X, they already integrate out. But once we have this one, and we also know the information of this kind of know what's the vectors, kind of vectors at any of the, you know, on the field given for any of x and the derivative in the space. Whereas we know this one, and we also know the derivative, kind of the integral, the whole thing equal to y. And we can just use this one to generate the samples. And for example, Generate the samples, and for example, the Leiden dynamic. And people may be familiar with the Latin dynamic work on the MCMC. MCMC have different types of algorithms. For example, the algorithm is the most popular one. The Latin dynamic is another algorithm. So, the short answer is, once we know this one, we can generate the samples. So, this is kind of the but the question, but the question is, how can we estimate this one? How can we estimate this one? This is kind of interesting part. This is kind of an interesting part. How can we estimate? To estimate this guy, and in a very intuitive, we can use the least square estimation. So, this one is what we are going to estimate. The log less food table derivative. We want to estimate this one. But to estimate this one, we can just propose another neural network. So the x, x, kind of theta. We can just use this one, use this square estimation based on our data to do this one. But the problem for here is I don't know this guy because this way I know. Because this one is unknown. I want to estimate this one, this one is unknown. So I cannot build my loss of functions. But the beauty of this, based on the previous work, the beauty on this one is you guys imagine this way our models and the setting the parameters want to kind of take the minimum of the setup. And to make this one happen, we can, the most important thing is this one is the square and also the kind of the kind of the interaction part. And so, but the interesting point. So, but the interesting point for here is because it takes the derivative and also takes the expectation of the theta by the integrated by parts. And we can do a little bit of algebra, the above equations will be exactly the same as the bottom equation. We look at the bottom equation, the bottom equation only evolves with our real data and S. S is our models. So these are no parameters go away. These are no parameters go away. So we can just build these models. So we can just build these models to estimate, we can just minimize this equation as our loss functions. After we follow this S, and that's kind of our unknown, kind of our unknown component. Once we know this unknown component, we can use a later dynamic to generate the samples. This proof is very simple by just by the integral by part. But the problem for here is in the high dimension, in the low dimension, when I directly use these equations, I think it is very magical. Equations, I'm thinking this very magic sequence. But in the higher dimension, so with the data, I'm just doing the kind of because of the curse-dimensionality in the higher dimension, we can just make this illustration as a simple. And suppose our data is here, and in the middle part, we don't have data. If we don't have data, and the data score cannot, we don't have data not accurate, the estimation is also not accurate. So, how can we do that? So, but we know if So, but we know if we continue to add the noise and diffuse these signals, make it bigger and bigger. And so, out of perturb these data, they can be estimated accurately. So, we can, from the center, we can continue to add the noise and make it bigger and bigger. We can estimate accurately. But it's still not solved the problem. We still want to estimate in this situation. How can we do this? The beauty of doing this is using this kind of the kind of we can We can continuously add the noise in the sigma 1, sigma 2, sigma 3. Think about instead of one piece of data, we continue to add the data and to diffuse this. A little bit like a simulation is tabulation. Continue to add this. And so we have got this kind of this part accurately estimate. But this kind of we cannot accurately estimate it. But what is your sigma one is the error schedule error? Errors schedule variance, or that's the random noise? So, this is the kind of scheduled noise. Scheduled noise. So, noise variance, right? Noise variance. Standard deviation. Standard deviation, right, right, right. So, think about I have one of the kind of these pictures. With the center of each original picture, I can continue to add the noise. So, this is the original data. These are sparse in the higher dimension. But I continue to add the noise, that will become more Become more. Yeah, but the beauty of this is this noise added, they can be more like a sigma variance in the continuous added up. The key idea for here is I only do the one neural network and I put this sigma and our parameters in the neural network. So, which means my neural network instead of ha only have this data, we added this sigma, continuously added one and a grade and also have a much much more data. Also, have much, much more data to diffuse this whole process. By doing this, and we can do the good estimation. Of course, this equation can be re-translated in my previous equations and only calculate the trees of the matrix. The methodology of general is called a neodom dynamic general use. But my important point, now I have finished kind of the two popular methodologies. Two popular methodologies. And this methodology have been proven to be better than the GAM models. In particular, generally, the sample is more diverse and covered the whole space. A lot of kind of demonstrations are into the two popular codes. The next part of the slide is the more important. I was thinking right now we have tools to put this in the unified framework. Can I solve the problem? What is the unified framework? So this is kind of popular stochastic differential equations. The stochastic x is our image. The dx Is our image, the dx is kind of from a thing about a continuous time, and the x is the kind of keep changing. So, and this is the three terms, the mean, and this is kind of standard deviation, and this is the kind of brown emotions. We also call the Wiener process. So, these are kind of regular forms of the regular form of this kind of stochastic differential equation. Stochastic differential equation. And this, I put it in the vector format, I can come in high in the Format that can come up in a higher dimension. For each one, it can be a sensor, can be a pictures, can be a vector. And important things, what we talk about for here, this act can be really, really, really large. Can be in the pictures, kind of thousands by thousands, or ten thousand by ten thousand. This methodology still works. And I'm seeing that this is truly talked about the high-level data. And the methodology we talk about here can also handle, the old right-hand framework, they can handle really, really large data. This is truly large data. They are improved. Truly larger data. They haven't proven the works methodologies. So I'm going to talk about the previous two methodologies can be under the same unified framework under Stoches and differential equation and what's the benefit to put on this. Now let's first see we can put on the same framework, the DDPM. So the first part is this is very simple. So the previous kind of normal generations can be written in this way. The C, we can random noise. With a little bit kind of the algebra, we kind of The kind of algebras and we can easily prove. So the DDPM method can be written this way. Which means that these equations, the DDPM, is a special case, the DDPM can be a special case as these stochastic differential equation generations. So this is a dx and this is a mu part of the sigma generation. Now the second the second part we talk about the score based solutions. And the score based solution you remember. And a score-based solution, you remember original picture, and diffuse a little bit, add a noise, kind of add a noise, and do the single single unified kind of uh uh kind of the loss function. But in general, the scope-based generation models can be written this way, and this can be also read down. And this kind of without the drift without the drift terms, it can be written this way. So, that kind of makes the point is both the kind of both the kind of So both the TDPM and the score-based generation model can be under the same framework and the stock has different conclusions. So G here is like when you add more noises, the time index is like how much noise there is. Right, right, right, exactly. Yeah, when the time goes up and added more noise. So that you can imagine this is where this diffusion process is coming from. But how can you after writing this? After reading this, I think a lot of people in the rooms already got very excited because these equations can exactly describe the probability path. And there are so many of the statistical properties. And we can do. So far, I didn't see any of the papers talking about kind of the convergence variability. I understand the kind of many of the statistical properties. I do believe, maybe five years from today, a lot of very good statistical work can be come out from this remote. From this framework. So, starting with the question: so, why we want to put it in a unified framework? And people already start getting some of the feelings that's really important. But I'm just talking about kind of this kind of line by line. And the next slide, I'm going to talk about the equations. The first part is the way bypass normalized constant. This is very difficult to handle in the traditional code, but right now we can get one with this with the general A. So, the second part, then soon. So the second part, so many, I think the statistical definition of equation topic kind of people making the life in this area for the past 100 years, there are so many rich of the result. And we can leverage statistical methodology, we can handle this, and also they are very rich, so the mathematical machine, they can handle this work. And now, I feel like this is truly the moment from the, I call it from technology to the science. The technology, the DDPM, because the diffusion model during the past, it's very. Diffusion models do not pass. It's a very ad hoc approach and very intuitive. They even prove in the original papers some errors, but it doesn't matter, it works. But now, this becomes kind of from technology to the science. We are better tools, much better to understand this generative AI. So these are kind of the first component. The s the second part I also think is important is that this proved demonstrated improved kind of kind of comparing the previous holy grilled GAM models. The previous holy grilled GATT models, this methodology has improved the sum of qualities. And we also know, if the stochastic differential equations, the marginal, they're also equivalent to the ordinary differential equation called the flow ODE. In the flow ODE, in the ODE, there are so many ODE solvers, they can solve this very, very fast. And also, they have a kind of different, another type of shape mentioned, knowledge distillations. And we can also apply the knowledge distillation. Apply the knowledge distillation. The knowledge distillation is, we can just give this, we'll continue to treat the neural network without really solving this, use different ODE solvers. We can just use the neural network, give the initial condition, they can automatically predict what's the final result. That makes the generation much, much faster compared with the previous people criticized because the DPM generation is so slow. Now, with the data tools, we can generate thousands of times faster because this might go the mathematical. Because this makes good mathematical properties to the both with the flow ODE. And also now it's distribution. Another part that also makes me really excited about precise probability evaluations. And we want to give you the probability. We have known what's the p-values of this probability. There are lots of implications. Currently, we are working. We applied the whole methodology in the generation, the generator AI generated molecules. We always suggest in the last year. I know some people work on the Azama disease. I'm kind of working on another problem in our team in the Azama disease in the evaluation. So, the general course is we know the longitudinal data for the image, for the pictures. But even given another new person's kind of image, I would want to know how extreme of these values. The the kind of methodology generated AI can help him to understand this probability. Previous, there are no such tools can do this. No such tools can do this. Another component is commissioner generation. And I'm also going to talk about in my next slides. And also, another is kind of different type of foundation model that makes me really excited. Now go to my next slide and talk about three important things. The first important thing is that we talk about the general framework, the general framework, and the forward process. In the forward process, that is the kind of stochastic. Stochastic, the stochastic differential equations. Important based on the previous result, kind of 40 years result. And the backward percent is also exactly the stochastic differential equation. That's why we need to estimate these goal functions. So once we have the gender, kind of the forward process can be formulized in this way, once we figure out what's the mu and sigma, and this really depends on different matter. And it really depends on different methodologies to do the forward process in this way. The backward process will be exactly written this way. That's the magic. So because once we know this, we know a lot, we can have a lot of good statistical properties can be derived from here. We can do inference. We can understand the variability. So far, I didn't see any statistical papers based on this framework to handle this. But I truly believe in the next few years, this will happen. Few years, this will happen. The second component is also very, very important. We talk about the backward process in the stuff has a differential equation, but it's also equivalent to the flow ODE. So there's no random terms for here. These are deterministic. Give it original and deterministic. That's also very important. So why this is so important? They can map one distribution, the pictures, to another distribution. There are one-to-one mappings. There are one-to-one mapings, because the people work on the mathematics, the homomyphism, homomyphosis, because that's the kind of one-to-one corresponding. And in the word space, in the picture, in the original pictures, the space, you cannot define the operation what the plus or the minus sign means. You cannot do that. But in another space, by doing this equation, the one-to-one mappings, you can do a lot of calculations of this space. Of each space, and after doing the space, you can map back with the original space, take a general mathematical result. I feel like this is another component. But this is very interesting. If this is actually missing one part, you probably won't be able to go back. So what would be the uh the one-to-one mapping is uh built into the normalizing flow. You can go from one way to another. You can go from one way to another. This can generate it, but you close the distribution hardware to be able to program. I mean, so the one to one max thing is the dominant flow. Right, right. Yeah. Yeah, yeah, but this. This is the determinant flow is the probability based. But after we learn this, these are kind of the deterministic flows. But if you look at the say distribution-wise, you probably won't be able to establish what it's like. Able that they establish what is what value. So the initial value, so the probability of the initial value. And then if you want to map that back to whatever the one is generally, the process is not reversible in that regard. You mean this approach or the normalized flow? No normalized can do that. But it has a one button in the normalized flow in the kind of bit space. I'm not quite sure there's exactly one to one mapping, but for here, this is exactly one-to-one mapping from the somewhat. From the the somewhat friendly space back to the original picture. Yeah, relative distribution, supposedly you your goal is to to estimate the uh distribution of the emission value. Yeah. And that's this is an innovative given. So this is an innovative process sense, it's probably one to one, but in terms of distribution sense, it probably would not be able to change or something like that. See what I mean? It's very subtle. That's the subtle difference between. Saddle, that's the subtle difference between these two approaches. The normal flow is actually doing the reverse thinking of this one. Yeah, but maybe I just kind of kind of we can talk about more. I'm not fully convinced at this moment. So I'm thinking the nice part is that which made me so excited for the whole week. So many of our generation is a conditional generation, is a conditional generation. We are the generator molecules to increase the binding affinities, we are generating Increase the binding affinities. We want to generate a dog, wear the sunglasses. So, think about the case, it can make it very simple. I want to generate a dog, wear the sunglasses. So, this, I want to check the X, my picture, the Y information, the dog, wearing the glasses. So, if we talk about the foundation model, foundation model, a lot of people only think about the transfer linear, only talk about the transfer for linear. But we are not worthy of understanding the theoretical framework, we have better tools to do this. This. So, but by doing very simple Bayesian formulas, we got this x kind of conditional y equals to y conditional x t times p s t divided by p y. So, these are very simple Bayesian formulas. And other calculus scores, the generation models is like this one. This increasing is so profound. So, why? So, we want to generate the conditional information of why. That only one component, only this component concerns why. All the other parts. All the other part, this left-hand side, just without Y. Only this component with Y, all the other components without Y. So that means I can use a lot, if I generate the dog wear the sunglasses, I can collect millions of dog pictures and tune this guy without worry about any of this conditional information. So that can be formed my foundation models to generate this component. To generate this component. After that, if I'm interested in the general dog wearing sunglasses, I only add this component. But the problem for here is this guy is high-dimensional. This is very difficult to train. ST is high dimensional. It's very difficult to use our data to train these models to a huge amount of data. But this guy is much, much easier. Because a lot of times the Y is scalars in the lower dimension. The dog wearing sunglasses or not. The dog wearing sunglasses or not wearing sunglasses, data classification problems. They can use a much smaller data set later on to treat this component. So they can decouple the trainings. I truly believe this is a better framework to kind of many of the transfer learning frameworks. So we can add a building model building state. I'm not worried about the future properties. I just estimate this component. Later on, if we need to add a conditional information, I can add this conditional information. So that is the foundation model that we built on the antibody generation space. There's one group called the OAS. The OAS, the observed antibody space, there are 2 billion antibody information. We use that as a foundation model to generate this one. And once we generate the humans kind of include the poverty, we try to put our poverty builder classification models on top of this and generate our computer. Yeah, so um so uh if I'm basically answering. Uh it's a Bayesian, so I'm thinking this from a Bayesian perspective, right? So what you're saying is you're trying to generate sample a lot of variants. So I'm I'm viewing Xt as the plant for Y is the quite even Xt still like this. So you're saying you're generating a lot of samples from the priors, right? You can do as many as you want. You can get a very good sense of the prior. I think when I'm not saying this is an issue. I'm not saying this is an issue, but potentially the issue is that if the likelihood is very sharp, then most of your priors are going to be very of very little value to the likelihood. So you generate let's say a million XTs, right? But maybe only a hundred of them is relevant to the likelihood. The rest of them doesn't need falling anything on the lightning. I kind of agree. The thing about this case is that if we have Great thing about this case is that if we have a whole distribution, the convenient distribution is only one line, in a connected distribution, but on the other side it is one line, that's so many information that could potentially problem. So that's kind of, I want to separate out of this, one the data issues, another in the methodology themselves. I put a belief under the conclusion of the data, the kind of framework is the best, the most elegant framework that I have received, the most efficient use of our data. But if, of course, if our data is not that data, If of course, if our data is not that good, no magic method can stop the situation. So now we talk about the generation models, and I have like maybe only 10 minutes left. And so the generation the generation the generation models in the what we talk about in the Euclid actually talk about in the Euclid space. About in a Euclidean space, and so the plus and the minor side in a traditional Euclidean manner matters. And the question is: imagine our real application. That's because the data is not from Euclidean space. I'm going to talk about from technological science and the second part is from geometry point of view, we needed something extension. From algebra point of view, we also needed extension. So, we needed a true extension. Now, let's start from a geometry perspective. Many of our work is on the kind of a lot of people know that. A lot of people know that kind of each changing geometry. Each changed geometry is kind of referred to the ambient space. But in reality, many of these depend on the intrinsic geometry space. Now, one example in the universe that we are living, we know we are living in kind of the four-dimensional space. We don't know what's the higher dimension, we don't know what's the kind of physical meanings of the higher inviting. We need to understand from an intrinsic perspective. That's an intrinsic geometry. So why is it relevant? So, what is the relevant? So, for example, the navigation system, I always play this film with my kids. So, at a certain point on Earth, if you go north at 1,000 miles, and going west, I'm going to 5,000 miles, and go to the north under 1,000 miles. Can you come back to the same location? The answer is yes, if you are in the north pole of this on the same location. So, the point for here is kind of in the on top of the surface, many near the things can happen. That's why this is important. That's why this is important. So currently, you know, GI Lead and company, we're also very large company, large company work on Azama disease. And we have quite a lot of these images, on the train image. But some of the generation on the image is not generated in the Euclidean space. And the curvature and generated curvature. And that's why we need this kind of put into this manifold the generation. Another very important area is we design the molecule. I'm trying to figure out how to make this cartoon. trying to figure out how to make this cartoon with my PDF files in the viewer. So this kind of this my molecule in the rotation and the translation, that will keep the energy level keep the same. We will treat this as the same molecules. How can we put all of this symmetric information all together? This is the group. And each element of this group, all is the continuous moon, that forms this group and this theory. And we're going to do the this theory. And so here I think this slide is more interpretable, but I they sacrifice some accuracy. But the general idea for here is if a lot of people are very familiar with the pictures on the data notation, but in the three dimensions it doesn't work. And with very rough ideas, they need 500 times more of this data, use a brutal force to do this symmetric information. Compare, we can leverage this in a group theory, we can leverage the steam. And so just in a stable time. Personally, I really patiently love symmetric. That's so beautiful in the physics, in mathematics, even in the statistics. And also in the robotics, and we divide kind of there are some image recognition. And if this is my iPhone, and they can recognize the iPhone, regardless how I rotated this, what's the position of the space of this iPhone? This is still the same iPhone. And I just want to train one model. They can recognize all. One model, they can recognize all the positions in the space. That also needs to leverage this symmetric group of information. But to make this happen, not that trivial, so kind of we doing the two-dimensional, three-dimensional space, put into this out of this surface, for example, on the surface. So all these kind of calculations kind of we need to use the different tools to do this, and the people may be familiar with the different geometries and be touched on some information. So the key. So, the key of I see this slides talk about some key. The key ideas have two things. One part is that we need to kind of leverage this metric information both from a geometry and also from algebra from real. This translation and notation, this is a special group. The special Euclidean group called SE3. This group is a SO3. This is SO3 is the notation, kind of special orthogonal kind of group. This is kind of the rotation. Plus, I'm some of the translation. Rotation plus on some of the translation, the direct class on some of the translation. To make this happen, we need to define like a two operators. So, previously, you remember in our stochastic differential equation, we depend on two things. Why is the plus and the minus keep either noise? So, what is the plus and minus in the kind of in the high-dimensional indices manifold? This is what you need to define. So, what is the winner process on this surface? We need to do these two things. To make this high density, and we need to kind of sum out the tool. We need some of the tools doing this. In the left-hand side, it's a geometry definition, geometry tools that we can leverage. In the right-hand side, it's a kind of algebra tools. The most beauty things we can combine them together. But although they look very expensive, the idea is extremely simple. I'm seeing all the ideas we talk about live in this kind of graph. The reason I have this unit ball for here is you can imagine from the kind of from the from the algebra from the algebra kind of point of view any of these rotations any of these rotations can be included at one point on this uh on top of on this uh sphere on the surface of the sphere so regarding how the rotated rotator is one of the rotations can be point can be corresponding to one point on this sphere our goal to add it the kind of plus or the minus sign to doing this but a challenge but a challenge for him but a challenge for here is this is very difficult Calendar for here is very difficult. We don't know how to do the calculus on the sphere. But how can we do this? We just add a tangent space. On this tangent space, in the tangent space, we know how to do the vector plus vectors. We know how to do the calculus in the tangent space. The important thing is that any point from this tangent space, from this tangent space, they can have a one-to-one corresponding to this obvious sphere. So the point from... So the point from this tendency space to this sphere is called its potential map. From this sphere, go back to this surface. You can imagine this is called a log map. Just because this kind of the translation, we can do a lot of calculus, we can do any of the calculus. We familiarize from the temperature space and map kind of to this sphere. And once it maps this sphere, we can. And when the match is sphere, we can define the plus and the minus sign. And we can define this plus and minus sign. And also, we can define what's the winner's process on this sphere. So that's kind of the, we talk about plus and minus sign and also this winner process and on the other spheres and then we can use this. But after doing this, we can combine them all together with the generative AI to generate the molecules. Generate the molecule, we can wear the protein target. The protein target, and we use a diffuse model to diffuse this noise and go back. But all of these residues, amino acid residues in the three-dimensional space, we need to leverage kind of this symmetric information and put what we just talked about for here. We generate really first to know that lot about it. Actually, what we talk about for here, I truly believe not just theoretical importance, also, if it works, make the impact of how. Works make the impact and how we help the human beings. In the end, I also want to talk about from the technology to the science. The technology to the science is kind of the previous DDPM, the different methodology. We can enter the same framework with a stochastic differential equation. We can open the field, have a lot of statistical problems we can do. And we also talk about kind of we can leverage this Lie group and Lie theory to better from a geometry and also from. From a geometry and also from algebra point of view, to better learn information, that's another moment from technology to the science. But to the science themselves, not enough at our discipline. I feel like there are a bigger gap for our training. I believe everybody in the room knows these formulas. But on the other side, I believe everybody in the room, almost 100% everybody in the room, don't know how to build. Don't know how to build atomic bombs. So, the point I want to make here is to the science itself is not enough. And we also need to think from science to the engineering. That's our training system, it's a very important kind of, we need to participate in the big, at the very beginning of my presentation, participant in the big kind of open source programs. Myself, attended this Katago in the Deep Reinforcement Millennials. And I read a lot of the book on the Reinforcement Millennials. Reinforcement millennials, I still kind of, once I really do the codings in the same class class, I can much better to understand the details, how to do the coding, do the parallel compute, do the parallel computing, understand the concurrent, and how to build up on these computations. There's a lot of detail, a lot of demos in the details. And these are so important for our team plan. And also, we need to think from engineering to the science. And we need to build these end-to-end solutions at our T-splan. Yesterday, I mentioned that we are so spoiled, and we are only working. We are so spoiled, we only work on the small component. People give me the data, I push the button. We are not that age anymore. We need to be more hands-on and have a focus on ends, big social impact for our works. I truly feel that is so important for our discipline. Among all of this, to make this ourselves become a food scientist, the curiosity is so important. We need to continue to reinvent myself. In my career, at the very beginning, I do the basic methodology, reinventing myself. Methodology renting myself not doing the beta methodology. For the past probably 20 years, I collaborated with many people in the room working on the recommendations, ITR, individualized treatment recommendations. Now, starting from after the pandemic, I put a lot of my effort in the general discount of generality AI, in particular for the most one year in the AI space. I'm thinking continuously, it's important to keep ourselves, be curious, continue to push in the downloads. So, here is my presentation and summarize this. And summarize this, I truly believe that's substantial evidence in the opportunity in the areas. This is a very exciting time for our discipline. Now, a lot of computer scientists give ad-house approach. We have, these days, a lot of smart people come here. We have much better tools. We can translate the technology to the science. And in the whole area, I believe in the next few years, we will grow. And so, then we talk about general AI in the same framework. So, case differential equation is kind of in a manifold. In the manifold, and this is the moment I call it from technology to the science, and also we need a prior 24 ourselves to improve ourselves skill set from science to engineering. So, with that, and Junior deal with the task, I'm going to finish five minutes already to have some QA times. So, is there any questions, comments, criticisms? I don't know. I have a question for you. I have a question about also representation. I think you're very familiar with making a picture of the outline on the beach, outline grassland, so they make the pigs very. So how does that play a role in this? So first part is not related, not that related, so it's kind of what I talk about, and also kind of the kind of what they talk about have a different component. Have a different component why it kind of relates to the kind of wide area. Yesterday we had some discussion on the adversary attack on the image. Think about the case, kind of think about the case in the past, we do the classification, these are cat and dog, and we can understand the classification better, really well. But now after we're learning this, we give another data point, maybe it's a alligators. So because they never see in the training data side, and clearly they are not in the classification bundle, so they may make this as a track. So, they may make this as a cat or the dog. So, that is the kind of continuum to create some problems. I'm thinking to improve our methodologies not only to understand the classification boundary and also understand what's the boundaries of a training data without going outside of this. So, that's the kind of one comments I feel like. But aspect, another aspect is a causal understanding, but that is another challenge problem. How we define the causal still a lot of philosophy debate. Okay. I think the problem is the algorithm works like compounding factor of the background instead of each each of the count. Yeah, it depends on what kind of algorithm they use. Yeah, so I'm asking this algorithm that you saw something like MW. I mean it has potential, I think. But it hasn't touched the causal part yet. Yeah, there is potential you can use that technology to deal with. That's a future. So that's a future direction. Why does Lee care about generating AI? So think about this question. For all of these analytical problems, what's the most important analytical problem for the pharmaceutical company? You can do the classification you can do the trial design, chemical sample science, you can do the carbon identification, use the genetic information. All of this was the most important. All of this was the most important point. I think about two things. The first thing is in the decision-making, the goal or no-go decision. The second one is to generate the molecule. We today have the stock like $330, become the largest pharmaceutical company. It's not because we have better calculating sample size. It is because we are better molecules to helping patients. The better molecule is dependent on the generating AI to generate the molecule. So you look at it, most recently, for example, people are probably familiar with in Seattle, the David Baker's. In Seattle, the David Baker's group. The David Baker group focused on this result and they just sold their lab for $1.8 billion. You are searching his unlike the David Bakers in Seattle. That's the most principle argument to sold out in all the histories. But that's why, among all of these analytical problems, I truly believe what I talk about for here is the most important one. So that's why we didn't care about this, because general is important. Let me ask you a quick question. So you were talking about Thank you. Quick question. So you're talking about the manifold. So you know the manifold is now you try to identify the manifold and the second time you just stay on the top of it. Right, leverage the manifold because the manifold comes from two perspectives. One perspective is from a geometry perspective. In the geometry perspective, I may not know the exact manifold. For example, the human brain, I need to know the curvature. But on the other side, we also very importantly to kind of from algebra perspective, thinking the algebra. perspective. In algebra perspective, the rotation and the translation. Each of the rotation and the translation, one of the actions, is corresponding to one point in this manifold. So each of this rotation corresponds to one point of this manifold. So that rotation, each of the rotation is correspond to one point of this sphere on the rotation. So that this rotation is already known, this manifold is already known. No, this is manifolding already. No, this is a unit of all of all of the translations. That's how we can, this is how we leverage this information from another perspective. So you able to develop a methodology this time, but at the same time identify the latent, let's say, latent manifold. Sense of all the distribution or whatever. Okay, so this kind of probably you mentioned these are different problems. probably you mentioned these are different problems so the kind of so so for some of the cases we we may not know the manifold like the from a geometry perspective on the brain image and we want to learn this I think that's a very interesting topic suppose we have a thousand pictures and what's the most representative brain manifold structure I think that's the kind of little bit of different problem in the totation and how we can learn the totation from the Lie group Li theory on this kind of the this because in this case the manifold is already known The mind defaulting already known exactly on these spheres. Okay. Do only have time for one more question because we have to get all the computers set up in our next speaker. Also, I think if there's some people online, if anyone online have a question, otherwise we can do one more thing. Yeah, sure, go. So we're probably a little bit fundamental because I think stochastic. Because uh I think uh stochastic differential equation is not new, right? Not new, I think. Yeah. So what's the what's the magic here? Is it because there are a lot of more data? They are put in the higher dimensions and in the AIS remote. They build in the anti-Linear structures. They can use the automatic differential equation. They can use the platforms to make this high dimension. The key is the mapping from the simple complicated distribution to a simple distribution and then to reverse back to generate And then to reverse back to generating data, that is in a high-dimensional case. Using differential cases. Using stochastic differential equation approach to accomplish that. So imagine that, just put a single function. How do you generate high-dimensional data? Statistically, I don't think we know. Because at one dimensional put a CDF, when do you get it? The high-dimensional, there's no analog of CDF. So how do you do it? The idea is very simple. You map some complicated distribution to a simple distribution that you Distribution to a simple distribution that, in this case, a gaussing, you can say you are using all the flag to uniform, whatever. You're mapping that, you learn that process, for process, mapping and reverse process, learn it, and generate going back. That's the beauty of this thing. In this process, utilizing stochastic different equation, in a non-delaying floor, I can construct a multi-layer composite function to copy the same form. So, that is the beauty. I don't think that thing has ever done in. I don't think that thing has ever done statistics in the class. Okay, okay, thank you. Thank you for listening and get up earlier for my treatment. I'm sure more people can ask questions about that speaker at lunch.