Yesterday we we saw an excellent uh oh yeah hey good the slides work yeah yeah yesterday we got a good introduction to to rank metric codes by by Alberto so this is this is good we have some background and now Philippe today can talk to us about how to use rankmetric in cryptography so In cryptography, so thank you, Philippe. The floor is yours. Okay, can you hear me? Yes. Okay, good. Thank you. So thank you for inviting me to this very nice conference. So the goal of my talk is to present what we can do with rank metric for cryptography. So I will first give a So, I will first give a general vision on this metric, and then I will also present some recent results that we have. So, it is our result with Carlos Aguilar, Nicolas Ragon, Victor Dissrant, and myself, and Jules Zemor. So, the paper is not available, but it will be on reprint very soon. Okay, so first, I will say. Okay, so first I will survey, then I will mention this very newest interesting result. So here is a summary of my talk. So first some motivation about post-consumer cryptography. So for cryptography, we need some different difficult problems. So of course factorization, discrete log, short vector problem for lattices or symptom decoding problem and decoding problem and all the and some of these problems in fact are can be attacked by a quantum computer and in fact for some problem like factorization or on discrete log there is a logarithm improvement so that in practice the the best attack become polynomial so it cannot be suitable anymore for cryptography. Some other problems like SVP or syndrome declared. Like SVP or serum decoding problem or MQ problem for multivariate are related to NP-complete problem. And at least for the moment, it seems that if a problem is NP-hard in classical, it seems to remain also difficult in control, which is not the case for factorization and discriminator problem. And discriminatory problem. Okay, so first, how can we link everything like that? What can we do, and what can we do in general? There are two main problems that we can do easily. It's solve a linear system and the factorization of polynomials. But for instance, start from a problem of solving a simple system. Suppose that you are given x. x and h, which is random, and x. And for a given s, you want to find an x such that h times x is equal to s. So in practice, this is a very easy problem, since to do that, it's exactly equivalent to solving a system. For instance, you can fix n minus k colon of h and then you get n minus k times n minus k. minus k times n minus k is the matrix a of h and you can construct a solution x which is a minus 1 times s so this problem is very easy to solve basically what what we can do now we will add a constraint to x now not only we want that x satisfy h times x is equal to s but moreover we want x to be of small weight for a particular metric so Particular metric. So by doing just adding this small constraint, in fact, if the metric is Hamming distance, then you obtain code-based cryptography. If you consider a Klidana distance, then you obtain lattice-based cryptography. And if you do that with hang distance, you obtain hang-based cryptography. Also, Anna Lena mentioned Limetric. If you also do that with Limitric, you would obtain. Do that with LeMetric, you would obtain Li-based cryptography. So, now what is the difference? The difference comes from what we know about the different distance. So, each distance has its advantage and a drawback, but typically the Klingon distance is more precise than other distance. And usually, people consider run base and code base are the same, but in fact, Base are the same, but in fact, it's only the difference comes only from the metric. And this difference implies different progresses in terms of cryptography. Now, starting from this simple problem, which is solving a simple linear system, so a possibility is to is to so add a constraint on the rates and all these problems are proven NP complete or with a poistic reduction. You can also consider rather than solving a linear system you can also solve a multivariable system in which case you attain the MQ problem for instance. So you can see that simply starting from a simple system of equations System of equations and just adding some constraints and some variation, then you can find some of the main problems which are used for post-quantum cryptography. There is no link yet with hesogeny, but maybe one day. Okay, so the general interest of post-quantum cryptography is, of course, to be resistant to a quantum computer, but usually it's faster than number theory-based cryptography. It can be easier to It can be easier to protect against side channel attacks because of intricate linearity. Usually, the problem comes from the size of the key, which may be larger. Okay, then for each of these metrics, there are some again advantages and drawbacks. For instance, for lattice-based cryptography, one of the main problems is the LLL algorithm, whose complexity is difficult to evaluate because Difficult to evaluate because you can always try to find some particular improvement. So at some point, we can expect that the situation is stable, but then we saw recently that there are also in some cases some type of new attacks, and specialists sometimes disagree on the exact attacks. So this is one of the particularities of LLED, which is a Allelide, which is a which can be whose security is difficult to estimate. Okay, but then there are advantages. For instance, you can do something like FSE that you cannot do with other metrics. Then for cold-based cryptography, so we heard about that. So one of the very good points is that there is a close formula for evaluating the security, and in particular for some type of system, Analena. A system and Elena mentioned HQC or back earlier for this type of parameter where the way that you want to attack is in a square root of n. For instance, the best attack known for the exponents, the best attack have not been proved for six years. So this is not the case for mechanist-like parameters, but for parameters like a bike or a like bike or fqc uh the original branch algorithm uh all all the all other algorithms asymptotically give the same type of formula and then there is a close formula so this is really a good point for for code-based crypto then there is multivariate cryptography then there is equivalent of um of LLL which is a kind of attack by Grodar by this again then you need a you need a have a some You need to have some type of equation, and then if you find some other equation, maybe it can maybe it can improve attack. So, again, there is a notion of stability in the attack, which makes things difficult to evaluate. We saw that with rainbow, this was also the case for honk metric, where sometimes you can have some new attacks that you don't know. And maybe you can always think that if I change a little bit my equation, then I will gain. Questions, and I will gain 10 bits or 20 bits or something like that. And of course, you are never sure that it cannot happen, of course. So, again, you rely on stability. Okay, now what about home codes? So, we go deeper into home-based cryptography. So, first, some definition and basic properties. So, there was a talk yesterday, so I come back on just to give some notation. Just to give some notation. So you consider GFQ to the M, an extension of degree M of GFQ. Then you consider B, a basis B1BM of GFQ to the M over GFQ. So it means that GFQ to the M can be seen as a vector space over GFQ. Then say a linear code over GFQ to the M is exactly as usual with a dimension and a length, with a generator matrix, you have a parity check, so everything is. You have a parity check, so everything is the same about that. So you have a notion of syndrome, everything works the same, except that rather than being on FQ, you are on an extension. So what is the difference then? So if you consider V an element of DFQ to the M so to the N, so every coordinate Vi can be deployed as a as a, you can write any element Vji. So you can write any element v j as a in the basis of the bi. So every v j in the extension can be written as a sum of v i j b i with v i j in the small in the small field. So it means that to this vector v you can associate a matrix over the small base field. And of course what we will call the weight of the vector would be the rank of the matrix. matrix hence the name rank metric so if v as rank uh if v has rank r which is the rank of the matrix what is important is that the determinant of v does not depend on the basis so if you can change the basis you won't change the weight of course we want that of course and then you have a definition of frank distance which was defined in 1947 if i remember well that if you continue I remember well that if you consider two elements x and y of gfq to the n to the n, the rank distance between x and y is defined as the rank of the difference of the two vectors. Okay, then you have a notion, so then once you use that, it's a metric, so the same object as usual, so a vector space with a particular metric. Polar metric. Then we can define the minimum rank distance of a code. There is also the notion of unit decoding, which are only related to the metric and not a particular metric. So for instance, if you have a weight of the error which is less than d minus one divided by two, then you have the Divided by two. Then you have the classical theorem that encoding theory that there exists a unique element C dash in C such that the distance between Y and C dash is equal to R. So this is exactly the same type of property that you have in Hami. So now we go deeper into what is interesting. So something that I like to call a kind of code book. To call a kind of code book. So, what is important, I think, is to understand the kind of analogy that you can do between rank metric and Hamming metric. When I first went into rank metric, of course, I had a Hamming background. And then at first, you need to say, well, this metric is different. How can I see how it works? How can I get some insights that I talked? Some insights that I take some insights from Hamming and then I use them for metric. So this is important to see how an ID is adapted in the other metric. For instance, an important point is the notion of isometry. So isometry, of course, you have weight preservation. So in Hamming distance, an isometry for F2 is a set of permutation matrices which, of course, preserves the weight. Then the equivalent of this. Then the equivalent of this is an operation for a matrix which preserves the weight. But the weight is the rank of a matrix. So in that case, the isometry becomes the set of invertible matrices over GFQ, which in fact preserves the weight. Okay, so you need to another analogy that we can use is a kind of support analogy. What is a support? What is the support of a word of RGFQ to the end in Hamming metric? So usually for Hamming metric, so you have a set vector x, and then what you call the support is a set of position xi which are non-zero position. Of course, you can do exactly the same for rank metrics, and it was something that was done in some paper at the beginning of rank metric. Some paper at the beginning of Frankmetry. But then, if you think a little bit, it doesn't make sense. Of course, you can do that, but it doesn't clearly make sense. Because, in fact, when you put the Arrang metric, for each coordinate, in fact, each coordinate is mean that lives in a particular vector space. Okay, so at the difference of Hamming metric, the notion of support for Hamming metric is means that it's not related to coordinate. It's not related to coordinate. The support of the, in fact, will be the subspace over GFQ generated by all the coordinates. So somehow it means that the support for Hankmetric is something which is outside the world. It hurts a little bit when you compare with the Hamming metric, where in fact for Hamming metric you can position and the support is a set of coordinates. Port is a set of coordinates, it is a set of positions, and it's not at all the case for rank metric. So, of course, you can see that why it works, because suppose that you know the space of the error, and if you want to solve s is equal to h times x, then if you have the set, if you know where lives the x, The X. In fact, then it means that you only have a system to solve. So this is exactly the same case than for Hamming metric. When if you know the set of positions, then you have a system to solve. And of course, if the number is small, then you solve a system and then you recover the exact value of the error. A third analogy that we can do is that's, of course, a very important point. Of course, a very important point is that when you count the number of given weights, so for hamming, the number of given weights is the number of sets with still elements. So it's T choose N. So typically it counts the number of words of red t. So for rank, so this is Newton binomial. So for rank, you want to count the number of subspecies of dimension T over GFQ. So you have this notion of Gaussian. So, you have this notion of Gaussian binomial. So, an important point is that when you count vector, so here you can see that the n choose t, whenever t is less than 2 to the n. When Forenc matrix, and this is an important point to notice, is that here, if you consider t, which is small, it's something which is more or less q to the t times n minus t. So, it means that the exponent is quadratic when. Exponent is quadratic when in Hamming, it's only a linear in. So, this is an important point to mention. Okay, then you have this notion of spherical packing bond. So, this is a co-hand to count the number of elements in a bowl. So, you have the notion of sigleton bond, and you have also the notion of Frankish beer-Bashmar bond. So, the value for which, if you consider an elemental syndrome, there is a statistical. Statistically, on the average, a pre-image of the weight of Gibbs rational one. So it's interesting to notice that in the case where M is equal to 24 for angle, there are four parameters. So of course M, K, minimum distance D, and then there is this notion of extension. So the typical case when it's harder to attack is a case when N is equal to M. So when M is equal to M, in fact, if you compute In fact, if you compute the Gilbert Vashmov model, the Rank Gilbert-Varshmar model, you obtain something which is n times 1 minus square root of k divided by n. So it gives you an idea. Okay, so now what can we do in rank metric? So there exists, so at the difference of Hamming, so Hamming typically, there is typically one big One big family of codes, and then many families in terms of algebraic codings derives from this. So there is a hit-roman codes, and then if you take sub-code and you obtain many, many codes, not all of them. Read Muller code, for instance, can be considered different, but somehow read Muller codes, they can be seen as Rich Roman code, but in a multivariable setting somehow. Okay, and then many. Okay, and then many of the codes we know, BCA to GoPAC, etc., at some points are related to RIT-SOMEN codes. And for Runkmetric, it's not the case. So there are Gablin codes, which are, as Alena mentioned, the analog of Fritzuman codes with Runkmetric and Kupunium-Ls. And then if you want to do what you do in Hamming, then subfield coexistence, in fact, if you do, there are a result. Codex are in fact, if you did, there are a result by Gabi Duin and Pierre Raneau, which shows that if you consider subpiece subcode of Gabeelin codes, then you obtain again a direct sum of Gabeon codes. So you don't find new code, you just find some other Gabeon codes. So it means that there is not the diversity that you obtain in Hamming. Then there are also some simple codes. So simple codes then were more like a code which are somehow equal to a Equal to an MDPC, LDPC somehow, which are code which reach the rank supervision bond, but in a probabilistic way. And then there are also, but these codes are very good, they are probabilistic, but for cryptography, they cannot be used because the structure is too important. I don't know how to write the structure. And then LRPC codes. So what is the So, what is that we introduced in 2013? What is nice with LRPC codes is that they can be somehow a kind of generation of simple codes. The point is that they are easier to hide. They are somehow the equivalent of NDPC or what you can find with NTU. Okay, so all these codes are different properties, and in particular, they are so used for network code. Particulars are so used for network coding, also for soaking of information. So, now let's see about LRPC code, which will be really used for cryptography. So, consider, in fact, Lorentz parity checks. So, the idea comes directly from LDPC. So, the idea that you consider matrix H, which is a form where each element form where each element, so H I L G belongs to a small vector. So this is the equivalent of a sparse matrix. What does it mean sparse for rank? It means small weight. So having small weight means a lot of zero. For hank it means that every coordinate lives in a small space. Okay, so you consider H a matrix which is whose all coordinates live in a space F which is a small F which is a small of dimension D. So in other terms, every coordinate HIG belongs to F and with basis F1 up to F. Okay, so how can you decode that? In fact, it's very simple. So you can do as usual. You want to recover the support and then deduce the coordinates w. So we consider E1. So we consider E1 up to N an error vector of rate R and then so you receive the so you get your error then you multiply by H so overall so it permits to vanish the the coding part of the word and then you want to solve H times E you don't know E is equal to S so here you can see that E Here you can see that E belongs to the error vector, the error space capital H. So it means that every coordinate EI belongs to a space generated by capital 1 up to capital R. So when you write SK, it means that HK is H times E. So it means that all element SK lives in the product space E times F. Okay? Okay, so usually, so this space is generated by the vector EI times Fj. Okay, so usually the dimension of this space is R times D. Then it means that if L minus chi is large enough, so you are given some element of this space. So if L minus chi is large enough, usually you can recover all the space. Okay, so how does it work? So you are given this. It works so you are given the syndrome. The syndrome means that you are given some element of the product space. Okay, so if you are given sufficiently many, I mean, a number which is greater than R times D, it seems that typically you are given, you can recover the old space. Okay. Then now you want from S which is equal to E times F, you want to recover E. So how can you do that? Very simple. So the problem. So the problem is kind of factorization. So you know the product space E times F, you know F, and then you want to recover E. No, you want to do that, but with a linear space. So then you consider Si, which is equal to Fi minus one times S. So since S is equal to E times F, you can write E times F like that. So it means that if you multiply If you multiply S by F i minus one, it means that you will vanish all the F i here, and it means that E1, E2, up to ER belong to Si. Okay, so it means that since you know different Fi, you know that E belongs to every SI, so you can you can recover E as the intersection of all the SI. Of course, there is a Of all the SI. Of course, there is authority that doesn't work, etc., but it can be computed easily. Okay, but typically, if you consider the intersection of the SI, then you can recover. Then you obtain a general decoding algorithm, which is simple. So from the syndrome, you recover the syndrome space. From the syndrome space, you compute intersection. intersection of product support, then you recover the support of the error, and once you have the support of the error, then you have a system to serve to recover all the coordinates and then so on. So of course there are some conditions of success. So the algorithm, this is a very important point, is probabilistic. So it means that you need that Rd is less than n minus k. What is the probability figure that's what What is the probability figure that's when in fact you are given when you are given s you are given in fact element of the product space so you need sufficiently many in order to recover them so it means that typically if you consider the case of d is equal to two you can decode up to n minus k divided by two errors. Okay, so this is the same thing that you can do with Gabriel encode. Um, Gabe encode. The difference is that you can even decode up to n minus q n minus k data vector, even in the case where m is smaller than n, which is not the case for Gabrieline, but then you have a probability because when you are given the element of the product space, you are not sure that requires everything. Okay, so this was improved in Improve in 2018. So we provide that in the case where the dimension is not typically when you don't recover everything, a space of dimension R D, but a space of dimension R times D minus C. So even if you don't have the whole space, in fact, there is so much constraint that still you can do something. In fact, if you use this, you promise to have a better decoding so that the pointer face. decoding so that the operator failure is not n minus k minus r d but n minus k minus 2 r plus d so it means that here you have a quadratic term and you replace the quadratic term by a linear term so it means that you gain in term of failure of decoding which means that you can also gain in term of parameter okay now what about complexity issues so decoding random run codes Random run codes. Okay, so first we can define the notion of Frank's number decoding problem, which is exactly the same that for Haming, except that you have rank metric rather than Hamming metric. Okay, so this is the same. You change rank by Hamming by rank. So now what is the complexity of this problem? So in two So in twenty fourteen with Gilles Zemor we had a reduction. So in fact the idea of the reduction is very simple. So suppose that you have an algorithm which solves the RSD problem and then you want to use it to solve the SD problem. So the idea is simple. You stand from your vector over GFQ, so Hamming. And then, so the idea is how can I The idea is how can I make a connection between the Hamming weight and the rank weight? So if you consider a Hamming vector, there are only 0 and 1, so the rank is 1. So you cannot use that. So the idea to do reduction is simple. You multiply the column by a random element of the extension. So if you do that, it means that if you have a one, then you will multiply the one by something. And if you do that, that in general, If you do that, that in general, the characterization with the embedding means that if you can solve the new problem with this embedding, then you have a kind of correlation between the hamming weight and the rank weights of the embedding. So, if you die and you do some computation, and you do that with a very strong probability, if you have a If you have an algorithm to solve the rank metric, the RASD problem, then you can solve the RSD problem. Okay, so typically, if you start by this and you will obtain this vector, and you see that the hamming weight here is three, and then the run weight here. In general, of course, you can have bad luck and then that there is a connection between alpha one and alpha four, but in general, statistically, it won't be the case. Okay, so it means that you have a poistic reduction. That you have a poistic reduction, which shows the difficulty of the RSD problem in general. Now, in terms of attacks, there are two types of attacks. So, there are communal attacks and algebraic attacks. So, depending on type of parameters, some attacks are better than others. So, the oldest attacks are communatorial attacks. So, it was done by Shabbo. So it was done by Schabo and Stern. So then it was only basic enumeration. So it's the worst that you can do. Then there were improvements by Roiski and Johnson in 2002. Then we did some improvements in 2016, which was interesting that what we did in twenty sixteen was the typically resist notion of support was the equivalent of the ESD print algorithm. The French algorithm, which had not been done before, because for hamming there is a difference if you consider a syndrome part, syndrome point of view or a MACLIS point of view. And in practice, in the parameter that you have here, the first attacks, you didn't have the notion of M divided by M. And then we did some recent improvements in twenty eighteen which can uh win uh uh uh a little bit A little bit. Then, of course, there is a quantum speedup. If you use Grosor algorithm like in Hamming, you divide the exponent by two. Not that for lattices, it's not the case. You cannot do that directly, but for codes in general, with a Grover algorithm, you can have the devaluable, at least for the basic ESD, for the other, where you use when you use birthday paradox, it's not exactly the same. Paradox is not exactly the same. So, these are the first type of attacks. And for a long time, there was also the notion of algebraic attack for electric. So, the first were there attacks for an EVA pere. So, there are some error in the equation setting. But Silk, they were the first to propose some attack like this, and they were kidness Shanier. So, we also did some stuff in 2016 with Annulator polynomial. With an Eulator polynomial. And all these attacks were not very efficient. So in 2020, there were some new attacks which were used on this parameter. So these attacks are based on the max miner approach, which corresponds to a minorization from optimum minor. And this attacks, in particular, in the case when you want to attack. When you want to attack air which is in a gigo of square root of n, which was the case for what we propose for NIST, it has a dramatic impact on the parameters. So typically it divided the security by two or maybe 40% or 60%. It doesn't mean at all that a rank metric is dead or anything. It's just that. Is dead or anything, it's just that there are some attacks for some type of parameter which are efficient. In practice, when we propose for the second round, we propose some new parameters. Maybe it was something like more 50% on the size of parameters. And since there are no new attacks, but clearly this attack made some demand, but it's very interesting because it permitted to improve the knowledge of. improve the knowledge of the best thing that we can do. It's also important to notice that algebraic attacks, in fact, cannot be, I mean for the moment, cannot be improved by a quantum computer. So it means that if rather than considering the attacks, you consider the the complexity in term of quantum resistance, it means that here wa when for uh handing you When for a communatorial attack, you can divide by two. Here, you cannot divide by two. So, it's important to do this, but for the NIST, it's not the type of metric which is used. So, now, what can we do in Rockmetric? So, as Alena mentioned, there is a notion of Macillis. So, basically, there are some you can adapt the Macillis. Macillis is not a system, it's a set. A system setting like is the director. You can take your favorite code and then you replace it. So, if you replace a Gopa code or Ritzuman code by Gabriel Cox, then you obtain a new system, so exactly the same. The main problem is the notion of hiding. So, hiding, so Anna Lena talked about hiding and the attacks. Hiding and attacks, and what is called so structural attacks. So, if you take, so there are many hiding have been proposed: so, AG plus X or H and A and then wrong reducible. And all this attack, in fact, all these masking have been attacked. So, you you have to understand that uh Gabian code are the equivalent of uh retro code and the structure is very, very, very strong. So, it means that hiding them is very difficult. Hiding them is very difficult. So, in practice, also, it's very difficult to hide the Red Summon code. More or less, all the variations of hiding based on the Red Summon code have been attacked. So, there is one type of masking which is resisting, is when, in fact, you mask the Gabi unicode by a kind of LRPC. So, if you multiply by a homogeneous matrix of weight. Of weight three or four, then in fact, you kind of it's difficult to hide. If the weight of the LRPC is rather two, then you can do something, but if it's three or four, it's more difficult to attack. Of course, it means that you have to multiply by three or four the the other the weight of the attacks that or the weight of the error that you want to um that you want to decode, so it means it has a strong impact on parameter, but Had a strong impact on parameter, but at least the interest of this is that at least it's system based on Gabrielian code, which is not which is still a resistance. So this is a system proposed by Pierre Radrot in 2017. But it still leads to a large size of key. Public key is still about 40,000 bytes. Okay. Okay, typically the main attack, it was mentioned also by Analena earlier. So, this is the overback attack, which is a kind of equivalent of the scroll attack, even if it was done before. I don't come back on that. Okay, so now in terms of cryptography, so there is an untrue-like family. And true, it means that you consider a double-circular matrix A and B, and then, so A and B and small, so you can. So A and B and small, so multiply by A minus one, and then you have an E and H. So you can do that with NDPC, and you can also do that with RRPC. So the idea is that you want to use the fact that the matrix is circulant to have something which is small. So then you can use LRPC code for cryptography. So we saw that LRPC code can have a type of decodable code. Okay, so, but what is nice with LRPC? But what is nice with a LRP sequence? So AB will be LRPC matrix. But if the weight is sufficiently high, then you will be based on the indistinguishability of this matrix. Typically, if the weight here is seven or eight, then the best attack is then the problem is difficult to attack. So it means that if you consider, for instance, simple code that I mentioned earlier, That I mentioned earlier, the size you can decode better, but then the minimum weight is one, so you cannot use this. But in the case of LRPC code, you can have matrices here where the weight typically will be in square root of n. Okay, so then it's exactly a type of Mackellis-like crypto system. So it kind of generalizes Mackellis in the sense where you don't hide your code with you don't hide your code with You don't have your code with an isometry, you just have a kind of hiding. So you give a public matrix. In your public matrix, there is a trade door which permits to decode and you don't do necessarily a masking. Okay, then this work in general, even if the metric is not cyclic, but the very good point of and true in that simplicity permit to To decrease the weight of the code. So, a double-circular matrix is a matrix where if you are given the first rows, then you can deduce the whole matrix by simplicity. Of course, you can do that, but it's more efficient to consider ideal matrices in which, in fact, you take f and you take modulo p, where p is a polynomial shelter by fx, where By fx, where all the coordinates live in the small field. So, if you do that, it means that you don't change typically, you do not change the weight. So, then you can have very matrices which are easily described with only one wall. Okay, then you can use this for cryptography. So, it was that's what we proposed to NIST. So, at first, it was proposed as a lake. As a lake and a locker, and then for the second run, we all mix together in a system called Rolo. So, there are some specific attacks which use the notion of which are based on LRPC, but these attacks are easily controlled. And so, IPAMI to have a very nice set of parameters. So, you can see the parameter of ROLO. So, this parameter are. So these parameters are resistant to algebraic attacks. They are not original parameters. So you can see that the size of the key are very small. So it's in bytes. So it's very smaller than lattices and of course all the hamming weight. But the main problem here is that the FR is 2 to the minus 228. Okay, so as a conclusion to a classical LRPC before I LRPC before I try to discuss some new results that we obtained, is that it's very simple, very efficient. There is a good DFR analysis, but there are some problems. So the first problem, which is a limitation, is that the DFR is in 2 to the n minus RD. So it means that if you want to obtain a very small DFR, not something like 2 minus 30, which is very nice. two minus 30 which is very nice if you if you want to do some uh key exchange two minus 30 is okay for key exchange but it's not okay if you want to do some encryption so it's kind of limitation of course you can do you you can go to a to 100 and minus 128 but it means that you want to have a large n but if you have a large n it has a big impact on everything and so then the system becomes less interesting okay so this was a very strong limitation Strong limitation that, in fact, if you want to decrease this, you have to increase the n and increase the zen. You don't want to increase the n because it has a 2D impact. Another problem is that we needed some constant time implementation. Our implementation were nice, somewhere in constant time, but still there was room for improvement, clearly. And also a limitation in that the parameter R in square root of n. And R in square root of n, we saw that it was a bad error. And we saw that it was a bad error for algebraic attack, where algebraic attack was sufficiently efficient. Okay, then we have this new approach. So the paper will be on the printer very soon, probably next week. So the idea is very simple, something which could have been seen many, many years ago. It's a pity that it hadn't been done before. So the idea is very simple. You want to do LRPC with multiple. simple you want to do lrpc with multiple signals so rather than the idea simple you you take you consider your lrpc cut like we saw earlier and then the idea is that rather than sending so one syndrome in fact you will send several syndrome okay so it means that when you consider when you consider LRPC so here So here your public key, your encryption is what? Is Mg plus E. Then you will send a different syndrome. But if you do that, you see what you are saying. If you do that, if you send several syndrome, it means that you will obtain several elements of the product space E times F. But what is nice is that if you But what is nice is that if you obtain several elements of E times S, it means that it's a way to improve the capacity to decode without improving the N. Okay, so it means that the problem that we have when we were obliged to increase the N if we want to lower the if you want, yeah, here we are we had to increase this N if we want this to be smaller. In fact, the idea. Smaller. In fact, the idea here is that we keep the same n but we send multiple senders. Of course, if you do that, you increase the size of your ciphertext, but at no cost on end. Okay, so foreign caps, it's exactly the same. So you will send your H times C. You will send your H times EI, so the CI. But from the SI, you will recover different elements of the vector space. And so from this, you can recover different elements of the vector path, and then you increase your probability of recovering the whole vector space. Okay, so this is exactly what I say. So rather than adding S is equal to E times E, then is equal to E times E, then you have capital S is equal to U times a capital V. Okay, so this is exactly the same. Then it's easy, so we have in the paper an analysis of the DFR. So if you do that, the DFR remains so Rd minus, so N minus K times L, when L is the number of new syndrome that you add. So this is very nice because it means that the limitation that we add The limitation that we add on n, then you just add some syndrome, and then you don't have any limitation of n anymore. So it has a strong impact. It means that now you can decode not only you are not obliged anymore to have a r which is in square root of n. Where does the square root of n come? It comes from the fact that when you want to decode, you want a r times d is less than n minus k. Okay, so it means that typically you have to have r is equal to d. you have to have R is equal to D more or less. So it means that you have that R square has to be smaller than N minus K. So it means that R square is smaller than N. So it means that R is in square root. Now, because of this L, in fact, it means that you can have R in D as high as you want. So it means that in particular, it means that you can take R and D on Gibber-Vashmov bond. And this is very nice. And this is very nice because when you are on geographic program, usually the algebraic attacks are less efficient. It means that we are in a case where algebraic attack and commutarial attacks are about the same complexity. When r is equal to square root of n, algebraic attacks are far better. Okay, so what does it change? So to decode, we need element of the product space. So when you only increase the number of When you only increase the number of given syndrome to decrease the DFR or improve the decoding capacity. Now, what changed is that the security is based on the RSL problem, and it permits to risk the rank fever version of higher. So, here is what we obtained. So, when it was difficult to risk DFR this, and this now it's easy to obtain. And here is the size that we obtained. So, we obtained a public key, which is a very A public key which is very small. For instance, we can see that the size of the public key plus the secret key, the public key plus the ciphertext is about 2.4 kilobytes. So this is very small and clearly smaller than what we have before. And more than that, we have also something which is very, very interesting is that with this notion of multiple syndrome, we have the notion that We have the notion that we can also decide to that we do not want cyclicity anymore. So why wouldn't we want cyclicity anymore? Cyclicity potentially, of course, we don't know what is the impact of cyclicity on a quantum computer. So maybe there may be some attacks that could use cyclicity. So in the case of lattices, in the case when you have one generator, there are attacks which One generator, there are attacks which are better. In the case of two generators, for the ones we don't know how to do that, but still it opens some doors about that. So it makes sense to have a public key which is small and without without simplicity. And our new technique permits to reach that. In particular, with a LRPC with multiple synonyms, we can have a public key. Public key, so it's always public key plus ciphertext. Mustodal size is about seven kilobytes. So, seven kilobytes is for public key without ideal structure plus the ciphertext. So, it's very small. So, in particular, it's a third, about a third of FRODOCAN, which is about the same. And of course, if you compare to a Luadro system, which that I mentioned earlier, it's also a factor five. And I don't compare. factor five and I don't compare with classic mechanists then it's a factor of more than 40. Okay so in fact this new approach with the LRPC with a multiple syndrome gives in fact the smaller the smaller parameter without ideal structure. Okay and in the case when you have ideal structure then if you compare what to obtain of course resisting to a new algebraic attacks To algebraic attacks, you obtain something which is better than bikes and better than less 40% with what we had before and also better than HQC. Okay, so our approach is specific to rank metric because you have this notion of you have this notion of RSL. So we have this notion of RSL. So what is RSL? So the RSL. So what is RSL? So the RSL is simply the fact that when you give H1C, you give several syndrome with the same support. So this problem was defined in a paper that we did in 2017 at Crypto, and then it was further studied by Miguel Barde and Pierre Briot in 2021. So the idea is when issue the number of syndrome that you give, somehow it's more like LPN or LWGE, but then in our case, the number In our case, if the number, I mean, you don't have this notion. This is a particular notion where the support is fixed and then you change the element of the vector. If the number of syndrome that you give is better than Rn, then the problem is easy. You can do it polynomial. When L is equal to 1, this is exactly equal to RSD. Then there is something in between. So there are some formula given by Magadhi, and also in that Magadhi and Pierre. And also, in that Magadi and Pierre, and also in that paper, which shows that typically, if the number of syndrome that you give is small enough, less than a K times R typically, something like that, then you cannot do better than attacking a classical arrest. Okay, so typically it seems to be a resistance. So then there is a RQC scheme. So RQC is like a HQC. Then there is Zander is all bored, so I don't want to go into that detail. Just to finish, I would like to mention about authentication and signature. So, authentication, so there was a stand protocol, chain protocol was done in the spirit of STEM protocol, but we completely broke it because the notion at that time of support was not well understood. And we propose some kind of stern adaptation. Which is a with a good notion. And so now, what about signature with a Reich metric? So there are different types of signature. You can do signature by induction, like a unique induction by RSA and CFS. You can have several induction like entropy sign. Here I forgot to mention wave. You can also signature by proof of knowledge. So by construction, like Schnorr. So, by construction, like Schnorr or Lubachesky, phantom booty was used by Indy Litum, and also you can do it generically with Schletchering, Paris. So, Stechering or Rankmatrix, so there are two types of signature. So, there is Journdal, so Journdal that we published in 2019, which is a variation on Schnorr-Lubacheski with a plus, because if you don't do the plus, it doesn't work, so there is an idea. It doesn't work, so there is an ID more so that it works. Typically, this type of ID doesn't work in Amin, but the new ID is also based on the Aristyl problem. So the parameters in 2019 were about 20 kilobytes and signature 5. So now we will soon propose some new parameters, which would be about 6 kilobytes for the sum of the weight of the. Some of the weight of the public key and the signature. And also, there is a really interesting new approach. I heard that Annelena early explained that it was difficult for with the proof of knowledge signature to have a small signature. But I don't think I really believe that for years and years, but it seems that it's not true anymore. So we did a sequence of paper and typically on. Of paper and typically on a rank metric. So, typically, we are about 10 to 10 kilobytes forces with the signature, and even sometimes less. And typically, there are some new approach with MPC in the head, which completely changed the approach for proof of knowledge. And there are also some results I duarded some stuff also in Hamming. It completely changed all that we know. All that we know before, and typically you can go down to something like 10 or 12 kilobytes. And which is very nice is that here you have no masking anymore. You are just based on the plane problem. There are also paper from a recent paper, not at all external. So we did some stuff. So I mentioned the stuff that we did. Typically, these are. Typically, these are preprints, but there is a strong, strong, strong potential with this, and typically 10 kilobytes. This is what we can expect to obtain. And then again, which is very nice, it's based on the plane problem without any structure. Of course, you can do simplicity if you want, but you do not even need to have that. So to finish about to conclude about distance is interesting. Distance is interesting because it gives some small parameters, so we saw parameters and with a strong distance. So, our new multi-syndrome approach completely changed the perspective on an RPC scheme in terms of reaching very low DFR. This before we cannot do that. When we repeat syndrome, then you can really decrease what you have content, and it's easily and at almost no cost. Now, the type of parameter security: now you can reach rank. Now you can reach a rank feature of virtual bound when before it was not possible. And also it's also possible to have very small size of key, about a few kilobytes, without any ring of ideal structure. So you still have the RSL problem, but then we saw that RSL problem is to give only a few syndrome. It's not a problem. We don't know absolutely how to act in that case. And a point that I would like to And a point that I would like to mention. When I saw the paper, I was really happy about that. We tried to do some stuff for the NIST. We did some good stuff, I think. But then there was a paper by Tung Shu and Jing Anyu, which clearly improved what we did and which proposed very efficient constant time implementation. And I was really, really happy about that paper because it's really a strong, strong improvement in terms of implementation. Time of implementation. And for instance, there are some parameters for this 30,000 cycles. So it's very, very fast. So clearly, I really do consider that this paper is also a breakthrough in terms of implementation for Frankmetric. And of course, so we did some stuff on our side. So there is a run-based cryptography library for those who are interested. So it's free. Everybody can use that. Everybody can use that, and we are still working on that. Then the algebraic attack, of course, algebraic attack did it was a problem. We had some to change some parameter, but still they have not been proved since two or three years by now. And I think that eventually it's more a proof that the Rank metric is efficient and resistant because of that. It's better to have this because, of course, you need to go into algebraic attacks. To go into algebraic attacks, and I think that improving algebraic attacks now really the CC set some really new ideas. Okay, so from future work, so we are still working on improving journal parameters. So the multiple simul approach, we are working on this for AQC, it was the same. So I mentioned MPC in the head, really for those who are interested, it's really, really an interesting subject with really strong, strong potentiality. Main challenge for Hankmetric, I would say it's advanced encryption. Hank metric, I would say it's advanced encryption. So, is it possible to do functional encryption, witness encryption, FSC, etc.? This is a problem for Hank, but also for HAMI. So, overall, Reichmetric could not get to the third round, even if the NIST clearly supports Reichmatrix, as it explained in its report for the search round. Because algebra attack, but now we have some more, we know more about algebraic attacks, but my message that. But my message that Frank Patrick and cryptography on Frank Patrick is still improving and still clicking because we are releasing new results and more results are arriving. Okay, I stop here. Thank you for your attention. So, I have never heard. So, I have never heard before about simple cuts. So, simple cuts, you can find the reference in the RSL paper. It's in our crypto paper of 2017. So, crypto paper and IBE 2017 are defined in that paper. In that paper, there are also some of the rank paper scheme. So, there is IBE, which was broken because it was based on a rank sign which was broken. And because it was hiding, and which is not the case of John Dahl, and everything is explained in that paper. I don't hear anything, so yeah. Yeah, I guess Edoardo was supposed to share, but I don't know. Maybe they don't. There is a problem, maybe. I see that their microphone is off. So I don't know if anyone else has question that they want to ask. Thank you for a very nice talk, by the way. We didn't clap because I was waiting. I think we were all waiting for Edoado to. Sorry, Aniso. Sorry, I did realize that we have a double microphone. That we have a double microphone situation. We have a physical microphone, and then we have the Zoom microphone. And the physical microphone was on, and I thought I was actually speaking. No worries. So please, please. So you didn't hear the clap, but can we please clap again for Philippe?