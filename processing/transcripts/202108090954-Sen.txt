All right, 11:55. So, our next final speaker today, as you can see, is Supervat Sen from Harvard. And okay. Thanks so much to the organizers for this wonderful workshop and for the invitation. Yeah, so today I'll be talking about large deviation. Today, I'll be talking about large deviations for some models of tense random graphs. This is based on two projects: one with Shobhik Dhara and a later project with Christian Bergs, Jennifer Chase, Julia Gaudio, and Samantha Fetti. So I'll start with a very basic question about large deviations for subgraph counts on random graphs. So, for example, if we start with the Erdogan random graph, and if t is the number of triangles, the very well-known The very well-known question about upper tails for triangle counts basically says: okay, what is the probability that, let's say, I have twice the expected number of triangles in my graph? And from the probabilistic perspective, this falls squarely in the area of large deviations. And our interest in understanding the precise probability of this event essentially stems from the desire to understand the structure of the random graph. Of the random graph conditioned on the rare event. So, for example, you could have a lot of triangles simply because you have many more edges than expected, or you could have sort of smallish structures that contribute a lot of triangles. So, from a purely technical perspective, if I set Aij to be the adjacency matrix of the graph, then up to a scaling by a constant, the number of triangles is basically a degree 3. Triangles is basically a degree three polynomial in the entries of the adjacency matrix. So, this is a non-linear function. And therefore, large deviations for this statistic do not follow directly from traditional sort of large deviation machinery that we have known. Further, for a long time, you know, this phenomenon of localization where very small structures in the graph contributing a lot to the red event, this has been expected for this problem. So, in terms of phenomena. Problem. So, in terms of phenomena, this was expected to be sort of pretty exotic and involved. And this problem has been studied very extensively for a long time in probabilistic combinatories. It has been referred often as the infamous upper tail problem. And I should add the very preface say that the past five years have witnessed kind of remarkable progress on this problem, starting from the work of Amir Dembo and Shaurab Chatterjee on non-linear large deviation. Chattergy on non-linear large deviations. So, right at this point in time, we do understand much more about it than we did, let's say, a decade ago. So, today I'll actually look at large deviation questions for subgraph counts, but I'll restrict myself to dense random graphs. You know, we expect denser graphs to be simpler, but hopefully I'll convince you that even in the dense world, there are certain things happening. World, there are certain things happening that we don't really understand very well yet. As I already remarked, there has been sort of very fast progress on the sparse random graph end of the world as well, but the techniques and the phenomena there are sort of very different, so I won't really have time to get into it. Okay, so the seminal result, at least for dense random graphs, is the result. Is the result by Chatterjee and Varadhan, who basically established a large deviation principle for the dense Erdoshreni graph by embedding it onto the space of graphons. So at a very high level, if I have a graph, you know, embedding it into the space of graphons, basically, you know, you can think of it as follows. So you take your adjacency matrix and you embed it into the unit square as follows. You know, you divide up your square. Folllows. You know, you divide up your square into subsquares of size 1 over n, and then you set the value of a function to be 1 on the ijth square if there is an edge connecting i and j and 0 otherwise, right? So at a very high level, that's what it does. There is an appropriate metric that we can put on this that is induced by the cut metric. And upon doing this, this space becomes a nice compact metric space. Now you can take your favorite. Now, you can take your favorite Edoshrani graph, you can embed it into this space. This gives rise to a certain sequence of probability measures. And Chatterjee and Varadhan essentially prove that this sequence of probability measures satisfies a large deviation principle with a speed n squared and a very well-defined rate function. So what is the rate function in this case? So it's essentially the integral of a relative entropy functional. This IP of H and P here indicates. P of H and P here indicates the KL divergence between two Bernoulli distributions, the base measure having probability P and the second argument, you know, taking in an argument H of X Y. Now, let me remark, you know, why is this relevant for the question of subgraph large deviations that I started with? So, one very nice feature about the cut topology is that The cut topology is that scaled subgraph densities or homomorphism densities are continuous functionals with respect to this topology. So, if I have this large deviation principle, I can project it down and derive large deviation principles for every subgraph density. So, at least at the level of large deviations, this answers the question for Erdős and random graphs. And moreover, this topology of graph limits turns out Topology of graph limits turns out to be the right conceptual ingredient to think about the structure of the random graph conditioned on the rare event. So in particular, Chatterjee and Varathan established that if I condition on the rare event that my graph is actually in some closed set F, effectively the random graph is supported around the optimizers of my rate function. Optimizers of my rate function subject to this constraint. And therefore, in special cases, if there is a unique optimizer subject to this constraint, then my graph basically looks like this optimizing graph one. Now, what this does essentially is that it reduces the probabilistic problem to an analytic one. Now, basically, one has to understand what are the optimizers for the rate function. For the rate function subject to this constraint. And in particular, the question we are interested in is: okay, suppose I have a certain high number of triangles in my graph. So this is sort of the homomorphism density for triangles. What can I say about the minimizers? This is a fairly involved optimization problem. We are minimizing a convex function, but the constraint set is definitely not convex. Constraint set is definitely not convex. And therefore, solving this explicitly turns out to be hard. And therefore, people sort of ask more sort of basic questions. At a basic level, let's say, you know, we want to understand that if I have lots of triangles, I could have it simply because I have a higher edge density. And therefore, the graph would look then like an Erdogeny with a higher edge density. Or it could be some non-constant solution, right? Constant solution, right? And we effectively want to understand whether it is, let's say, one or you know whether it is one of these two possibilities. The regime where there is a constant solution to the variational problem, we say there is symmetry, whereas if there are non-constant solutions, we will say there is symmetry breaking. So, this fail diagram, at least for the Ayrdo-Schrény case, is quite well understood now. So, this picture that I have. So, this picture that I have is from work by Lubetsky and Zhao in 2015, who identified the exact boundary between the symmetry and the symmetry breaking phases for the upper tail of triangles and other regular subgraphs. So, to explain the figure a bit more, so I have on the x-axis the quantity p, which is the edge probability of the base graph. And on the y-axis, I have r. So, r is affected. have R. So R is effectively the effective edge density if I fix a certain triangle density. So you should think of R as the value of the edge density if I wanted to simulate an Erdos Reni random graph, which would have my target triangle density. And we are interested in upper tail large deviation, so we look at the area above the diagonal. So in this figure, if you look at the region in blue, then it is known that here the solution is always Solution is always a constant solution. Whereas in the region here in white, our solution is non-constant. But we don't know whether there's a unique solution. We don't know anything about the solution. That's the current state of the art. But even this result already kind of shows a very intriguing phenomenon. It says effectively that if you start with some probability that's small enough, if you start at the diagonal and then if you start cranking up the target. Start cranking up the target density of triangles, then initially you have a constant solution, then you kind of pass to the region of non-constant solutions, and then at the very end, you kind of re-enter this phase of constant solutions. So such phase transitions in physics and chemistry, apparently, they are known as re-entrant phase transitions. So I might refer to that terminology. Okay, so this is sort of our current understanding regarding. Current understanding regarding the phase space for the Erdushren graph. But today, I want to talk about models beyond Erdogan, right? So what can we say about large deviation problems for these other random graph models? So two essential classes of graphs are very commonly used for applications and for modeling. These includes, you know, graphs with inhomogeneities, things like block models and stochastic block models, or random. Block models or random graphs with constraints. So, for example, random deregular graphs. Maybe the simplest possible model, for example, for random graphs with constraints that one can consider is the GNM model, right? So, Erdogany, but with exactly a given M number of edges. Now, large deviations for these graphs are of natural interest, and it's not just a direct extension of what we know for red or shrimp. Know for Erdogan. Because once you have constraints, for example, certain things that happen for Erdogan can no longer happen. For example, if you look at upper tail large deviations for triangles in the random deregular graph, you cannot have a symmetric regime simply because the number of edges has already been constrained. So, understanding these phenomena even qualitatively is very different from the Erdősian case. So, my goal today, let me just see how. Goal today. Uh, let me just see how I'm doing. Okay, so my goal today is to basically talk about two sets of results. So, we have some results about block models and some results about random graphs with given degrees. I'll try to describe what the results are and kind of distinguish, you know, what happens that is different from addition. Okay, so let me first talk about block models. So, I'll consider the case where, you know, I'll consider a simplified model. Consider a simplified model for sort of graphs with inhomogeneities. So I'll construct a sequence of graphs on Km vertices. K will be the number of blocks, and I'll just consider K to be some order one quantity. I'll have some matrix of connection probabilities. So PAB represents the connection probability between a vertex of type A and a vertex of type B. And my edges will be added independently. And so at each step, you know, I. And so at each step, you know, I take n vertices, I put n vertices of each type. And then, depending on the type, I add these edges independently. This gives rise to my graphs. And these graphs can be naturally embedded into the space of graphons. This induces a certain sequence of laws. So let me skip over this. This is just to say that when I try to state the LDP result, depending on the block model, for example, if I have zero or one blocks in my base. Have zero or one blocks in my base graph on my measure is not supported on the entire space of graphons, but is rather restricted on a certain subspace. So if I have to derive some LDP, I have to actually restrict my support. But that's just a purely technical point. The more interesting question sort of concerns the candidate for the rate function, right? So at a very basic level, you know, the first guess that we would have for the rate function is to say, okay, let's change the rate function for error of trendy. Change the rate function for Erdos Reni, right? Let's take the relative entropy functional and just integrate it over the unit square. In this case, the base measure becomes w0 of xy instead of a constant p. There are two issues with this, essentially. The first one being that, as you can see, unless w0 is a constant, this object is not invariant under vertex relabelings. And therefore, this is really not well defined on the space of graphons. The second issue is that for The second issue is that for something to be a proper rate function, it has to be lower semi-continuous, and lower semi-continuity, at least for this functional, is not immediate and is probably not even correct, unless w naught is a constant. So what turns out to work is that you can essentially take this function and you can do sort of the obvious modification, which is that you replace this function by its lower semi-continuous envelope. And once you do that, And once you do that, it turns out this is the right candidate. So we have a large deviation principle for the sequence of inhomogenous random graphs. It still happens with speed n square. And the rate function in this case is exactly this lower semi-continuous envelope that we introduced in the previous slide. So following our work, there were results by Markering and Picorco and Grebic, who actually simply Who actually simplified the form of the rate function? And the result by Picorco and Grebic, they kind of extended our large deviation principle. So, for example, you know, even if we consider the entries, the connection probabilities within blocks to be constants potentially, we can, you know, our result as such can only do block models where blocks have rational lengths. But the result of Picoco and Grebic extends that, and you can do any general K-blocks. K-blocks. Now, once you have the large deviation principle, the rest of the probabilistic story kind of stays the same. So you can again talk about, you can again reduce yourself to a similar variational problem. And if you understand the optimizers of this variational problem, then you can understand what is the structure of the random graph conditioned on the rare event. That's okay. Okay, but in this case, our rate function is much more involved. So, getting sort of useful information out of this variational problem turns out to be more work. Now, the question is: okay, what is even the right question in this setting? And the question we sort of ask is as follows: right, we say, okay, for the Erdos Reni case, we were our basic question. Case, we were our basic question was: okay, is our solution a constant versus not a constant? And the question was essentially motivated by the following consideration. So, I started with an Erdos-Reni graph, so that was supposed to look like a constant. Now, I want to know that once I enforce this rare event constraint, have I broken the constant symmetry? Or is my resulting graph still approximately constant? That was the essential motivation. So, we asked sort of the analogous question here. Of the analogous question here. We started with a graph that looks like a K-block graphon with a given size, you know, set of block sizes. Condition on the rare event, does the resulting graphon still retain that symmetry or does it lose it? So do I have minimizers that have the same block structures? When that's the case, we say that there is symmetry. And if I don't have k-block, Don't have k-block constant graphons with blocks of the same size, then we'll say there is symmetry breaking, right? And here, our first result essentially is that sort of we uncover in quite broad generality regimes of symmetry, right? So we say that something similar is true as for the Erdos-Reni case. So if you're looking at target homomorphism densities that are close to the expected value, then you Expected value, then you will get symmetry. And similarly, if you're kind of at the very, you know, very close to the maximum possible value of the homomorphism density, you will also have a symmetric solution. So if you recall the picture that we had for the Erdoshreni case, this result essentially recovers the symmetry regime near the diagonal and near the maximum possible value. And for certain specific models, For certain specific models of inhomogeneous block graphons, we are actually able to establish symmetry breaking in intermediate phases. So for these examples, you know, we have the same kind of re-entrant phase transition. So symmetry, symmetry, breaking, symmetry again. I should say here that this part of our argument, symmetry breaking, is kind of case specific. I believe there should be a more general argument, but so far we can do it only. Argument, but so far we can do it only for certain specific cases. So, there might be something to do here. So, that's sort of our understanding in terms of the variational problem for the upper tail for subgraph densities on block constant graphons. Let me spend the last five minutes maybe on large deviations for constrained models, right? So, the simple So, the simplest model, as I remarked before, for constrained random graphs is the GNM model, right? And here we are working sort of in the dense regime, so m will basically scale like n squared. So large deviations for the GNM model were looked upon by Dembo and Lubetsky a few years back. So, and they do derive the large deviation principle in this case. So, how do they do this? So, their idea essentially is that, okay, I can realize. Essentially, is that, okay, I can realize the GNM graph as the unrestricted GNP model conditioned to have exactly M edges, right? So, and of course, if I choose P appropriately scaling in M and N, then this event that I have exactly M many edges, this is actually only polynomially small. So, this is actually much larger than the large deviations event. And therefore, And therefore, you know, you would naturally believe that since the conditioning event is actually much less rarer than the LDP scale, I should still be able to use large deviations for the unrestricted model to derive large deviations in this case. And that is sort of at a high level what happens. There are certain complications, but this is sort of the big takeaway message. So, in joint work with Shovig Dhara, we actually looked at a slightly We actually looked at a slightly more sort of complicated example where you know we have a few more constraints, right? So we look at the case of random graphs with exactly a given degree sequence. So in this case, you know, we say that d1 to dn is a graphical degree sequence. Each degree sort of is scaling linearly in n. And we say that, okay, let's draw a simple random graph uniformly at random given this degree sequence. And so we want to understand large details. And so we want to understand large deviation properties for this particular class of models. The issue in this case is that my number of constraints in this case is scaling linearly with the size of the graph. So we have many more constraints. And at the very outset, if you really want to understand even the typical properties of the graph, even that is not obvious. For example, in cases where the degrees are order one, so in the regime of sparse graphs, one can use Of sparse graphs, one can use the configuration model, for example, to study typical properties of these graphs. But the issue here is that when your degrees are scaling linearly in n, these two models are definitely not contiguous. So you cannot appeal to the configuration model heuristic. So what we draw upon is this really nice result by Shorov, Percy, and Alan, where they actually derived the typical behavior. The typical behavior of this family of random graphs. So they essentially say that: okay, if I look at my sorted degrees, imagine if these scaled degrees are actually converging to a limiting function, then this family of random graphs actually converges to a deterministic graph on limit, which is specified, which has this particular structure in terms of a function beta. And the function beta here is actually specified. function beta here is actually specified in terms of the limiting degree function d. So you give me a degree function d, I can solve for the function beta as a fixed point equation, and then I plug it back in, and this is my limit for the graph. So they have some minor regularity conditions on the degree sequence D, it kind of has to be well-behaved. But apart from that, this is sort of remarkably general. Now, why Now, why do they have this particular form for the limiting graphon? The idea is that, subject to these constraints, they say, okay, I'll find the maximum entropy distribution which satisfies these degree constraints. And this maximum entropy distribution essentially turns out to be the so-called beta model. So it's an inhomogeneous graph where I have edges independently between vertices i and j with probability. between vertices i and j with probability pij, where pij essentially has this particular form. So it's e to the beta i plus beta j by one plus e to the beta i plus beta j. And the sequence beta i is essentially chosen so that it satisfies the degree constraint, the expected degree constraint, right? And so of course now once you see this, you know, the expression for the function beta in the last slide becomes sort of clear. You get that by taking the limit, right? And a very crucial And a very crucial ingredient that drives the result of Chatter G diacones and Sly is that if you choose your beta parameters appropriately, then the probability that the graph G has exactly this degree sequence di, this is still lower bounded by e to the negative n to the sum 7 over 4 exponent, right? So the important point to note here is that this is a smaller bound, but this is still much larger than e to the negative n squared, which is the large deviation scale. Squared, which is the large deviation scale. So, morally, one should be able to use the same sort of heuristic to derive also the large deviation behavior in this case. And this is sort of what we can do. So, modulo some technical complications. Under the same assumptions as Shattering G diaconizing Sly, we can also derive the large deviation principle for this sequence of random graphs. The rate function in this case is. The rate function in this case is sort of what you would expect. It's the one for the inhomogeneous model, where the base graphon is the typical limiting graphon derived by Chattyji Daikon's slide. And the constraint here essentially is one which basically says that I have some non-trivial rate function value only at graphons that satisfy the same degree constraint everywhere else. It's infinite. Okay. Right, okay, so let me skip over the proofs. I think I'm kind of running over my time. So there's one, you know, probabilistic and analytic part. There's some things there. So there has been quite some follow-up work after our work. The same kind of results have been derived by weak convergence methods. Certain other variational problems have also been explored. And there are now sort of similar large deviation principles for dynamic edition nanographs. For dynamic additional graphs. Let me finish with some open problems. So, I would like to say that we still don't understand anything much about the symmetry breaking phase. I think that would be very interesting if we can do anything there. And unfortunately, you know, even though I can tell you what is the large deviation principle for, let's say, the deregular random graph, I cannot tell you anything about the variational problem. So, none of the existing tools seem to work there. So, understanding this would be definitely very So, understanding this would be definitely very interesting. And I think, more generally, sort of trying to see how much we can push or sort of how many more constraints we can have before we start seeing the effect of the constraints on the large deviation problem, I think at a high level, that would be very interesting as well. So, thank you so much for your attention.