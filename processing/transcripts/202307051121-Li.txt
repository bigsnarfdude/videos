which genes are differentially expressed, which are multiple two sample tests problems. We do one hypothesis testing for a gene. If we reject the null hypothesis, we call the gene a D gene. So moving to single cell RNA-seq data, we have heard about this workflow multiple times. A crucial step is to cluster cells to define potential cell types and then do D-gene analysis to identify potential cell markers. But the issue here is that we use the same. issue here is that we use the same data for toys. So this problem is commonly referred to as double dipping. In this case we do clustering and DE on the same data. So in this toy example you can see that even though we only have two genes and the cells look like one homogeneous cluster, we can use k-means clustering to force the cells into two clusters. Then both genes will become differentially expressed between the clusters. But this is not biologically meaningful because we don't actually have two cells. Because we don't actually have two cell types here. So, to formalize the discussion, I will introduce the notations. We refer to Yij as the expression, which is usually a count of gene J in cell I. So, here I have N cells and J genes. And I consider the N genes as M variables. So, therefore, every cell I is an N dimensional vector. And this is aligned with MS formulation. And we consider every cell as an observation from some positive. As an observation from some population. So, this is often organized into a count matrix where the rows are cells, columns are genes. And I refer to the set of cells as this math cal Y because I need the set notation. So, therefore, I have some latent membership for every cell. And for simplicity, I only consider two cell types. So, therefore, every cell I has a latent variable indicator 0 or 1. But I don't observe this. So, therefore, a clustering out. So, therefore, a clustering algorithm must be applied to the set of cells to define cell clusters. And here, I use notation G to refer to a clustering algorithm like k-means. And then, with this algorithm applied to the data set, MathPALY, I can get a map from every cell in the set line to a 0-1. So, I still consider two cell clusters. So, with this, every cell I will have a cluster membership denoted as ZI-hat. So, the idea. So, the idealized multiple testing we need for de-analysis is to compare every GJ's mean under Z equals 0 versus Z equals 1. So here for simplicity, I dropped the cell I sub-index just to make the notation simpler because I consider given the ZI and YI, they together are IID copies of Y and D. So mu0j is the conditional mean of gene j in Z under Z equals 0, and mu 1 J is z equals 0 and mu1j is the conditional mean under z equals 1. So the null hypothesis we really want is to compare mu0j and mu1j and if this holds we call gene j as a true non-DG. But in reality what we did is double dipping. So the conditional means we're comparing our given z hat instead of z. So we call this mu0 JDD double dipping versus mu1 JDD. So this is the model. JDD. So this is the modified null hypothesis. And so you can see we will have conceptual false discoveries because it's possible that the null hypothesis DD holds, but does not hold, which means that we want to reject this null, but the real null we are interested in holds. So we will have conceptual false discoveries, even though we don't do any tests. Okay, so there are some existing methods that tackle this issue. The most recent one is called CANSplit, and it has the Split and it has this interesting idea to split the count yij into two counts by binomial sampling. So the assumption for the theory to hold is that yij itself follows a Poisson distribution. And there is an extension to formalize the discussion on archive. And in this paper, it mentioned that the traditional approach, like in cross-validation, that we split the cells into two halves, clustering on one half of the cell. Clustering on one half of the cells, DE on the other half of the cells wouldn't work. This is called cell split. And gene split means that we only use a subset of genes to do clustering and only do DE tests on the remaining genes. Wouldn't work. So they are discussing this paper. And then earlier paper is called TN test, which does cell split, and then they develop their own way to calculate the p-value using the truncated normal test. But the assumption is that after But the assumption is that after some log transformation, Yij should follow Gaussian distribution. So, our finding is that these existing methods wouldn't give us valid p-values with genes are correlated. And this is our finding. So, we actually have our simulation with gene-gene correlations, and the p-value distributions are under the null. So, all the genes are non-D genes. We can see that CANSplit has p-values concentrated around zero. Values concentrated around zero instead of being uniform. And gene split has the same issue, even though both will actually give us quite uniform distributions when genes are independent. What's the reason? Our intuition is that gene correlation is the culprit. A non-D gene, gene J, may actually be correlated with the cell cluster Z hat. That's the reason. So when this correlation exists, then this double-dipping null hypothesis doesn't hold, even though the idealized null hypothesis. Even though the idealized null hypothesis code, then of course the p-values from testing this double-dipped null would not follow uniform distribution under the real null. And we can also explain why the gene split method wouldn't work, because even though you may not use gene J for clustering, but gene J could still be correlated with some genes used for clustering. So therefore, gene J will still be correlated with Zhat. That's the reason. So here's our proposal. So here's our proposal. We propose our method as cluster DE, and the motivation actually came from biological experiments. That is, we set up some negative control in the beginning, and we run the same experimental procedure throughout and compare the results in the end. So our idea is to generate some synthetic null data that reflects our null hypothesis, that is, there's only one cell type, so that all the genes are non-VG. So with this data that mimic real data, This data that mimics real data, but satisfying the null hypothesis, we apply whatever pipeline clustering algorithm in combination with the DE test to both data sets, and then we do the comparison of their discoveries in the last step. So the first core component in our cluster DE method is to simulate synthetic null data. And for this, we use our recently published simulator SA Design 3. And I want to note that regarding the simulation of discrete Regarding the simulation of discrete cell types, the special case is SCDLAN2, the previous version. So, in which we consider every gene as a feature or variable, and we consider cells to be a random sample from a multi-gene distribution for every cell type. So, specifically, we consider in one cell type, the null model, every gene follows a negative binomial distribution. And back to the previous question about zero equation, we actually had a paper that discussed that. Paper that discussed that when the data are from the UMI-based single-cell RNA-C, the negative binomial distribution can disrupt it up very well. And I think Shang has a similar paper about this discussion. And the issue with the many zeros can actually be explained by having a negative binomial with a very small. And regarding the joint distribution, so marginally, everything is a negative binomial. So we need a multivariate negative binomial distribution. Negative binomial distribution, but it's difficult to fit as Dee said already. So we approximate this distribution using Gaussian copula. So, in other words, we want to marginally transform every gene into a standard Gaussian variable, and then we fit a multivariate Gaussian distribution. So, what's left for us is to estimate this Gaussian correlation matrix. So, we fit this model, and then we do sampling from this model as our synthetic model. So, how this works? When the real data has only one cell type, When the real data has only one cell type, our synthetic null mimics real data very well. We can see the very good agreement in terms of gene mean, gene variance across all genes, and gene-gene correlations are also quite similar in the real data and synthetic null data. But when the real data has two cell types, this is what our synthetic null data will look like. We will fill in the gap to make the cells look like one type, and this wouldn't be achieved if we just permute the cell type labels, as in Permute the cell type labels as in the permutation analysis. The reason is that the cells topology will still be preserved, even though you don't know the labels. But our synthetic now will give you what the cells would look like if they're at one time. So, okay, so the second part is for FDR control. I'll just quickly go through the idea. It's motivated by the knockoff method in statistics, but we didn't generate the knockoff as they did. But instead, we used the synthetic knob. But the contrast idea is very similar. Idea is very similar. So, our idea is that if a gene is non-D gene, then, so going back here, so the DE scores from real data versus the null data, they will be equally likely whichever one is bigger. So, it will be symmetric about zero. But if a gene is very likely a D gene, then the D score from real data will be much bigger than the D score from null data. So, we will see it in the right tail. So, the idea is to find the threshold on the control. Idea is to find a threshold on the contrast scores so that we can call Degents. So, this is what we have in the simulated data. So, the interesting thing is that as the effect size goes up, that is the two cell types are better, better separated, the double dipping issue diminishes. So, the double dipping issue is most severe when the cell types are not so distinguishable. So, the clustering doesn't agree with the real cell type. But we can see that compared to existing methods, we can control the FPR much better. We can control the FBR much better under the target and we have very good power. And on the real data, I will quickly say that when the data are one cell line in each row, we have very few discoveries as expected, but existing methods can give us thousands of D genes, which are a lot. And when there are two cell types, we still have discoveries, while existing methods also have thousands of D genes. So you see, they always have thousands of D genes regardless of the truth. Of the truth. And this is something biologically quite interesting. Looking at the top DE genes, we find they do have very distinct distributions in the two cell types, but the top DE genes found by existing or the SURAT approach, Naive double dipping approach, they may find genes that are hardly distinguishable in the two cell types. And this is due to the clustering effect. And here, this is a cell type marker gene CD16. This is a housekeeping gene. We see that Surat put them both on the top. Surat put them both on the top in the D genes, but we put the housekeeping gene in the bottom. And this is also confirmed by the gene set enrichment analysis. In our DE genes, the housekeeping genes are not in the top, while the cell type barper genes are knowledge, are enriching the top. While using the na√Øve Surat approach, they are both in the top. So, finally, I want to say that we propose this approach to circumvent double dipping, and it uses our simulator for synthetic null and to do the p-value-free FDR. So, I want to talk about. Free FDR. So, I want to acknowledge my students Dongyuan for leading this work and also Xin for assistance work and Xinjiou for that FDR control part. And I want to say that Dongyuan will be on the job market this year. I think he will be a fantastic candidate. Thank you. I'll ask you the last uh last talk, and then we'll have a full discussion. To have a floor discussion or optimization. So, the first speaker is Mark Robinson from University of Burgundy. He's going to talk about benchmarking methods using symbols and data and also to this paper.  There we go. Got it. Okay. Right. So, hello, everybody. Huge thank you to the organizers for organizing an exceptional meeting. I've been really enjoying the conversations. I'm going to go in a fairly different direction to the talks so far, and that's kind of maybe a bridging talk, talking about benchmarking in general and more. In general, and more posing questions and sharing some anecdotes of our experience doing benchmarking, kind of generally. Just first want to highlight just kind of the spectrum. We have very diverse interests in our group, and a lot of these topics have been discussed so far. So, for example, Muscat is our kind of our pseudo-bulk approach. Pre-processing, doublet finding, differential distribution analysis, similar to what Jessica talked about. Sell and excess for. Cell mix S for looking at batch mixing, even sample QC for doing more model-based QC analyses and various other approaches. I guess another big theme of our lab, as you may know, is all about benchmarking and also a bit of infrastructure. And so, yeah, I guess what I want to get to today is a little bit about this kind of new platform we're building on the benchmark. And so, what I thought I would do to start this off is to. To start this off, is to pose a few questions of what I think about when I see all these talks and read all these papers. And I think a lot of these have also been mentioned so far in the discussions. But what are the metrics for success of a method? And I think in many cases it's dead easy, right? So if it's a statistical method and you have a ground truth, then yes, it's statistical power, but also statistical power conditional on maybe controlling. Conditional on maybe controlling errors. So I would say that's kind of the easy approach. If you're clustering, well, there's things like adjusted rand index and F1 scores. That's simple enough, but there are aspects where there's actually a variation of these scores that penalize you for different aspects of misclustering and so on. So it's a little more difficult than just that. But then there's things like batch integration. And I'll come back to this point later. That's actually much more difficult to evaluate. To evaluate. So I always think about what are the metrics for success and also are they good metrics for success? Because there are cases, and I'll give you one anecdote later, about having an excess of metrics, and it's hard to know which metrics are useful to you. Are simulations reasonable? So I guess it's always difficult, right? Because when you see a talk, even when you read a paper, you see kind of the end results of all of this. So what I often like to do is dig in. What I often like to do is dig into the details of the simulation to understand that there's kind of reasonable assumptions in there. And I have an anecdote later about this: about, well, even simulations that, based on some calculations, mimic properties in the real data, if you run a benchmark based on those simulations, they don't mimic what you would get from the real data. So I'll give you an example of this. Reproducibility, so here I think about computational reproducibility. Computational reproducibility. So, would I be able to reproduce the benchmarks that you do? Because there's actually a lot to unpack there. There's kind of software stacks and so on. So, that's a topic. And then I guess the whole thing about benchmarks is, you know, the results of a benchmark, what you hope that those apply to in the future. What data sets, future data sets, do the future data sets represent, the current data sets that you are presenting? Presenting your results on. And the other problem in this field is that there's a lot of activity. So, how long are these results valid? Because there's new methods coming out every week. So, that's just kind of a practical problem. Okay, so maybe something a little bit abstract, but something that really struck me a few years ago. So this was a talk from Marcel Salose in Switzerland. And he was talking, actually, he read this book, and he was talking about, well, how do you distinguish between knowledge and opinion? And in this book, there's this quote. And in this book, there's this quote: checking of each by each through public criticism. And this is kind of my, it kind of forms my thinking for how we should go about benchmarking. And the idea also proposed in this book is that no one gets the final say and no one has personal authority. And so part of this is leading you to the idea that when we do benchmarks, it's a snapshot in time, but of course things can change. The data can change, new methods come out, new metrics come out. Come out, new metrics come out that we maybe know better are telling us how well these methods are working. And then there is a little bit about personal authority. I guess it's also you want to do this in maybe a community-based way as opposed to kind of individuals claiming that methods are performing well. So I find this a nice way of framing this, at least a little bit. Okay, so let me jump through some anecdotes, and I'm going to go through this pretty quick. Quick, just some experiences that I've had over the years. So, if multiple people compare a method, they'll get the same results, right? And so, I'm going to go through this fairly quick. And what I want you to focus on, so people will probably, people that work on spatial data will know this data set. And I want you to focus on Bayespace, just because it's in every figure that I'm going to present. And just to get a sense of, so every block or every page. Every block or every page that I'm going to show you here is a different paper that's proposing a spatial domain detection method, and they all compared against Bayespace. So, here Bayespace is bottom left, doing pretty well. Bayespace is here not doing so well, but maybe some of the other methods are not doing so well.