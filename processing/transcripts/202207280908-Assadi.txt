  Let's just think of the numbers one to n. There is nothing unknown about the vertices, they're just numbers one to n. The edges are completely unknown, they are coming in a stream, and really, let's not think of it as a stream, these edges are written. A stream, these edges are written in some external memory that we can access in a sequential manner. Okay, so my graph is on some slow memory written somewhere. I can read it sequentially. I can read it once. When it's finished, maybe I can read it another time or a couple of more times and then solve the problem. Okay. So in this setting, if I have n vertices, if I give you n square memory, the problem is completely trivial. You just store the entire input. Now you have Store the entire input now, you have an offline computation. Okay, so there's nothing left to. So, the interesting case is that when the memory is much smaller than n square. And number of passes, the ideal number is really one. Like if you can get a one-pass algorithm, perfect, that's the best thing. If not, maybe two, maybe three, four, five. Even if not that, maybe log n. If you want to be very generous, let's say n to the one over 10, a square root n. Okay. Over 10, a square root 10. After that, the problems become very different. We really don't have an n-pass streaming algorithm. That's now more or less a bounded space computation. The challenge is everything changes a lot when your number of passes become that. By the way, stop me at any point, ask questions. So let's talk more about memory first. There are two interesting regimes of memory. One, when your memory is One vendor, your memory is roughly from poly log n, or maybe constant, all the way to sublinear in number of vertices. Here, you want to solve this type of problems. What is the size of maximum matching of your input? Or if you've been to Madu's talk, what is the value of a CSP? Estimate the number of triangles, things like that. Or some sort of property testing. Given a graph, is it connected or is it very far from being connected? Far from being connected. These are the type of problems you usually target to solve when your memory is that restricted. Okay. But as I told you, your memory can be much larger than it can be proportional to number of vertices, but smaller than number of pages. Here you have more memory. You can actually maintain a spanning for us, maintain a shortest factory, or maintain a maximum matching. So here you want to find a solution to something, or just check a property exactly. Check a property exactly. Is my graph connected or not? Is vertex s, can vertex s reach a vertex t in a directed graph or not? So, in fact, these two regimes are somewhat very, very different from each other. In some sense, even the people who are like working on these two sets of regimes are somewhat disjoint. Although, I mean, if we take a representative, if we use like attendance of this workshop as representative, this sentence is completely wrong. Center is completely wrong, but in general, that's the case. Okay, good. So, so we are going to talk about these two regimes. That's about the memory. Let's see. What about number of passes? I want to make this remark. In a streaming, really every constant in number of passes matters. It's somewhat different from what you see in models, I don't know, like conscious, distributed, local. Like, we don't have order one pass streaming algorithm. Of order one pass streaming algorithms. One pass is way, way more interesting than two pass. Two pass is way, way more interesting than 10 pass. And just give you some sort of a personal perspective to like put this in mind. Let me make this analogy. Think of number of passes of a streaming algorithm, the way the techniques, everything works for it, as exponent of runtime of a classical algorithm. So a one-pass algorithm is as good. A one-pass algorithm is as good as a near-linear time algorithm. We really like it. We'll try to get this. A two-pass algorithm, think of it as quadratic time algorithm. Order one pass, it's polynomial time. Don't know log n pass, think of it as quasi-polynomial time algorithm. And n to the alpha as some sub-exponential. And the same way that these problems, like the type of problems in the classical setting that you study across these different regimes, changes a lot. These different regimes changes a lot. In a streaming, also, it changes a lot. Like the type of problems you look at in single pass compared to the type of problems that you want to allow, I don't know, a square root and passes for. Okay. Are you saying this in like a like applied perspective or informal? Yeah, extremely informal. If you want to just compare, this is a comment I had to make at some point because someone was telling me that all this paper is doing is reducing. All this paper is doing is reducing the number of passes from log n to one. I think of that comment is the same as saying that all some algorithm is doing is to take a quasi-polynomial time algorithm and take it into near linear time. This is like a practice perspective. You're saying that they really like... No, even in theory, I mean, quasi-polynomial time is an entirely different thing than nearly linear time, no? In the type of problems that it captures, this is what you're saying, that there's like a You're saying that there's like uh classes of problems, and this is how you would classify. Okay, just something along that, okay? Good. So, we talked about passes, memory, these things, this is our background. Now, let's talk about the problems. So, what do we want to do here? We have this space. There are these two things that we care about, a space and number of passes. If a space is more than n, we have this OT low n memory regime. This OT log n memory regime and above. If it is less than that, we have this polylog n memory regime. I give you a problem. You want to tell me what is the space-pass trade-off for solving this problem. Okay, so maybe, for instance, some problems, like the easiest problem here, even in one pass, we can solve them in order one space. These are the easiest, these are the best problems in a streaming. We can solve them very easily. There's nothing that much going on. Nothing that much going on. Then we have these hardest problems. Roughly, they behave like this. If I give you p passes, you need ns squared divided by p space to solve the problem. Now there's this question, why is there any trade-off here? It depends on exactly what you mean by streaming algorithm. For the purpose of lower bonds, I think I'm cheating. When I talk about the streaming algorithms, I mainly mean end part. Mainly mean n-party or n-square-party communication complexity model, each party holding one part of the input. So think of it like that. There you'll always have a trade-off. You can never have a like after NSQ over PS space. So if you have NS score over PS space and you have P passes, you can ship all your input to the last player. That player now has everything, can solve the problem. So we are going to work in this regime. Are you going to work in this strategy? Sounds good? Good. So, and given a problem, we want to figure out how it looks like. Where does it land in this Space? So, let me go briefly talk about earlier results here. Let me pat ourselves on the back, say that single-pass lower bonds, we know them very well. We are in a relatively very We are in a relatively very good place now with respect to proving single pass lower bounds. Now we're in the following sense basically. Now we are at a stage that really you wake up one day, you'll try to prove a lower bound for a problem. By night, you have some lower bound instance. Then you figure out, oh no, this is not hard enough. You turn it into an algorithm. Then the next day you're working on the algorithm. And then you see again, no, this is not an algorithm. It's a lower bound. These are so close to each other. Lower bound, these are so close to each other. A good example was Madhu's talk on a streaming CSD. You see, this is like this dichotomy: that it's either an algorithm or lower bound, they're that close with each other. I think that's the ultimate goal. So, in single pass stream, we are getting close to being able to prove those type of results. And don't get me wrong, there are tons of open problems here still, but at least we are like our lower bound techniques are very, very strong. Bound techniques are very, very strong. Okay, so single pass, I will not get into that here. There is another regime that we are relatively very good at proving lower bound. This higher curve that I showed you, this NS square divided by PS space type of trade-offs, we are relatively very good at proving those type of lower bounds. Some examples, these are this type of problems that we knew for a very long time how to prove those type of lower bounds for. Prove those types of lower bounds for one example is testing triangle freeness, and you may ask, I mean, this result is 20 years before even there was a notion of graph streaming. Look, how forward-looking these people were. And the answer is a lot. But the thing is that these results can be proven without knowing anything about graph streaming. These results are proven by proving communication complexity lower bounds without any restriction. Without any restriction, you just take your problem, you partition it between two players, and you prove that: look, solving this problem, for instance, testing whether your graph has a triangle or not, requires omega and a square communication. If you can prove something like that, you get your streaming lower one for free. There's nothing else you have to do. Okay. And given the audience in this workshop, I will not go through this reduction. Either you've seen it or take my word for it. If you can prove a communication complex. If you can prove a communication complexity lower bound, you immediately get NS squared divided by PS space is needed for P-Pass algorithms. Good? How do you prove these lower bounds? Most of the time, you just reduce it to disjointness, gap hamming distance, sometimes some direct sum version of them, like tribes function. Even if the reductions do not work, I mean, communication complexity, we know it well enough. There are tons of techniques we can use. There are tons of techniques we can use to prove these lower bounds. So, these types of things we can prove. The problem is that communication complexity is a much stronger model than streaming. Just because you have a problem that you can solve in the communication complexity model does not mean you can get also good streaming algorithms for it. So, it means that there are lots of problems that we actually cannot prove communication complexity lower bound for them. Complexity lower bound for them, even though in a streaming, there are some lower bounds. And some examples: reachability, shorter span matching, network flow, if you have capacity on vertices, all of these problems have a small communication complexity protocols. And from now on, I want to call these problems like not too hard problems. You cannot just lower boundar communication complexity and get a lower bound. Complexity and get a lower bound for a strength. Does this make sense? Good. So what did we say? We can prove lower bounds for single pass algorithms. There are this hot trade-off here. We can prove lower bounds there. There is one other trade-off that people have been proving lower bound for for a while, and that's this type of lower bounds. In P passes, you need n. In p passes, you need n to the one plus one over ps space, and these are lower bounds for problems such as how do you construct the BFS tree? How do you solve shortest path problem, for instance, dominating set edge cover? There are lots of problems that all of them have this type of trade-off. If I give you p passes, you need n to the one plus one over ps space to solve a problem. Solve a problem. How do you get such a trade-off? These results, all of them follow from direct sum versions of pointer chasing. I will talk about pointer chasing toward the end of the talk, but basically there are various versions of this problem. It's a well-known problem in communication complexity that captures the role of interaction in communication. And there are direct sum versions of this problem, like these ones that I mentioned here, you can just use. ones that I mentioned here, you can just use them and prove lower bond in the streaming model as well. Now, the problem with this approach is that pointer chasing instances create random graphs. They are hard on instances where the edges are chosen randomly. And if you look at these instances, these lower ones that I mentioned to you, the graph that they create is really a random graph, either a random graph or a layered random graph. Graph or a layered random graph. And these problems that I'm talking about: reachability, matching, shortest, but most problem of interest, random graph are somewhat the easiest instances of the problem for them, not the hardest. So because of that, and these trade-offs never give you something beyond log and access. And this is not only informally speaking. There's a nice paper by Amit, Andrew, Prantor, and Sophia that actually. And Sophia, that actually for reachability, they're showing on this family of random graphs, you can even solve the problem in log n passes or non-trivial algorithm in sublinear space. So if you were just going to use this random graph and this type of pointer chasing, this log n seems to be a barrier, really. We cannot go after. And an important note I wanted to add is this n to the one plus one over p has. To the one plus one over p has a meaning in streaming because we have lots of upper bound techniques that also give you this type of trade-off. Okay, and most of these upper bounds come from some sketching techniques, such as the one for MST, for approximate matching, or more recently for weighted mean cut. There are some generic techniques that give you this type of trade-off. But the catch is that these upper and lower bonds do not intersect. Upper and lower bonds do not intersect on the same problems. So, for instance, the upper bonds work for the approximate matching, the lower bonds work for the exact matching. Okay, good. So, so here we were basically. There is this natural trade-off that we can prove. It was just probably not for the right problem. An example again is that we can use it to prove log n pass lower bound for reachability or matching, but the upper bound for these problems are very. The upper bound for these problems are way off. They are like a square root n or n to the three-fourth. And a couple of other single paths we knew very well, and this hardest problem we could prove lower on. Sounds good? All right, so this is this. So now this leaves us with these areas basically that we didn't know much about. And I want to spend the rest of this talk talking about these different areas. For instance, For instance, are there some of these not too hard problems that require quadratic space even in two or more passes? Okay. Or what are some other trade-off curves that we can prove a lower bound for? Here, I think this is my most favorite question in this regime or this space: is that what happens beyond log and passes? I told you, algorithmically, it might not be the most interesting regime, but for the same Interesting regime, but for the same reason, if we can prove lower bounds in that regime, it's very interesting because now we are ruling out almost any interesting algorithm for a problem. And then finally, at the bottom is like this CSP type of result that we are seeing again, like think of Madhu's talk. What can we do for them in multiple passes? And this is what I'm going to try to talk for the rest of this talk. But any questions so far? But any questions so far? Good. All right, so let's move on. Oh, by the way, I want to mention something. I don't want to take credit for asking this question. This question, all of them in one way or another have been asked in the literature before. Okay, so these are some references. Good. So let's talk about now more recent. Let's talk about now more recent progress. Let me start with this region. So now we know at least one problem, which is like quote unquote not too hard, that we can prove a polynomial path lower bound for it. This is this problem. With UCHAN and Sanjeev Kono, we prove the following result. If you have a PPAS streaming algorithm for the following problem, like zoographically first MIS. The first MIS, then it needs to have something like ns squared divided by p to the 5 space. What is this problem? It's basically the output of the following algorithm. Pick the vertex that has the smallest ID and add it to your solution. Remove all its neighbors. Pick the next vertex that has the smallest ID. Remove all its neighbors and continue. The output of this algorithm is like Of this algorithm is like it's an MIS because none of these vertices are neighbor and it's dominating all the other vertices. And among all of them, it has the lexographically first order. So there's one problem. This is a key complete problem. I know people have looked at it in the context of parallel algorithms before. Here we show that, look, this problem actually requires a lot of space, even in p-passes. And the reason that I call this is not a too hard problem. This is not a two-hard problem. Is you can, if you think about it in a two-communication, two-party communication model, you can solve this problem with n-log and bits of communication. No? The players, you first pick the vertex that has the lowest ID, then Alice communicate all the neighbors of this vertex to Bob. Bob communicate all the neighbors of this vertex to Alice. You mark off those vertices. Then you pick the next vertex, you mark off those. Pick the next vertex, you mark off those remaining vertices. Any vertex will be marked at most once. You need log n bit to communicate it, so n log n bits of communication. So this is a problem that these previous techniques couldn't prove a lower bound for. Now we can prove a lower bound. I want to spend like two minutes or maybe five minutes telling you how to prove this lower bound. And it's somewhat very straightforward. Sorry, sorry to interrupt. Just a quick question about the previous thing that you said. Thing that you said. Like, are you saying it's easy in one-way communication? Like, I mean, when we think of okay, oh, because it's multiple passes, you're okay to like have interaction. Is that yes, absolutely. This is definitely not easy in one-way communication. Okay. But in two way, you can solve it with very many rounds of interaction, like maybe n rounds of interaction. Right. So, if I just, I think I was also like thinking about constant passes, then. Also, like thinking about constant passes, then it's not like trivial, no? No, not at all, no. But remember, the goal here is to prove some polynomial number of passes lower back. Ah, okay. Okay, so the game here is to prove, I don't know, this requires n to the one over five passes. Fair enough. Okay, sounds good. Thanks. All right, so how to prove this lower bond? We couldn't do a reduction directly from set this. Do a reduction directly from set distrointness. Why? Because set distroitness has a high communication complexity without limiting the number of rounds. This problem doesn't. And I tell you, pointer chasing will give you random instances. They tend to not, these previous techniques won't go beyond log and passes. So what do you do if neither of these two things work? Let's just merge them together and get something that is merged of both of them. That one actually works. So in some sense, you just follow your nose when you want to prove this. Just follow your notes when you want to prove this result. So, let me define a new problem: this hidden pointer chasing. It's the following problem: I have Alice who has M sets S1 to SM. By the way, do you see my pointer if I move it? Yes. Thank you. So, I have Alice who has M sets S1 to SM. Bob has M sets T1 to EM from a universe M. And Si and Ti intersect in exactly one element. Think of it as an instance of set intersection prompt. So this is an example of the sets, for instance. Now, think of the intersecting element of each pair, call it a pointer. Okay, so S1 and T1 intersect in two, call it pointer of element one. Go to that pointer. Go to that pointer, it will pointer to another instance, then go to that instance, find its pointer, move to the next instance, do it k times, output the last pointer. In other words, start from here. S1 and T1 intersect in two, go to instance two, they intersect at four, go to instance four, they intersect at three, output three. Okay, so in this problem, you have this parameter k, you want to follow. Have this parameter k, you want to follow this thing for k step and solve a problem. And think of k as being much, much smaller than so. The natural strategy to solve this problem is that you spend one round, you do order m communication, solve the set intersection, you figure out what is the pointer, you move to the next one, you spend another round, solve set intersection, find the pointer, you solve it. Pointer, you solve it in K rounds. You can solve it with MK communication. And remember, K was a small, so this is a low communication for us. And if you have less than K rounds, if you think about it, for the same reason, pointer chasing arguments like these problems are hard. This problem should be hard also. You cannot follow this strategy. So you should try to shortcut one of the pointers. Pointers, and that's hard. We prove that if you have k minus one round, then you require ms square over k square communication to solve this problem. And again, k is a small, m is large, so this is a high communication lower bat force. And yeah, I should mention that if you look at the original paper, it uses four players. Paper, it uses four players. Now it can be proved even for two players in like the standard communication model. And the proof strategy is just the following. I mean, set intersection, we know finding each pointer requires omega and communication. This is just set disjointness problem. And the rest, we want to do some sort of pointer chasing type argument, plus some direct sum argument because we have many, many set intersection. So all of that sounds good. There is just one. So, all of that sounds good. There is just one tiny problem here. When you want to solve this problem, even algorithmically, there is no reason for you to exactly find the pointer. If I tell you that this pointer belongs to a set of, I don't know, 100 other elements, then you just have to in the next round solve the set intersection for that 100 other instances instead of all of them. So, I don't necessarily have to find a pointer to solve the problem. The lower one should. Solve the problem, the lower one should be stronger. The lower one should say that if you are doing a small communication, the next pointer, you don't know anything about it. Because if you can limit the range of that pointer, if you can change its distribution, then you can focus on solving those instances and subsequent rounds. And that gives you a lot of power. You can start a shortcut. And so the type of result that we prove are something like this. Are something like this. If you, in set intersection problem, if you have a small communication, let's say epsilon squared times m, either communication or information protocol. Then if I look at Alice's view of the pointer, the distribution of pointer based on what Alice thinks of it, after I do the communication, these two distributions are almost the same. So, originally, just Alice, by looking at her input, has a guess that okay, the intersecting element is one of the elements in my input. Even after the communication, after the transcript of the protocol, entirely given to Alice, still Alice doesn't know which one is the correct answer. She's like almost completely oblivious when which one is the correct target. And that's the type of lower bounds we prove here. So, with a small communication. Yeah. So, with a small communication or with a small information protocol, the following type of things work. Sounds good? And if you have this, then you can go through, just adapt the proof of pointer chasing to get a lower bound for this HPC problem. And by the way, I think there is a noise communication complexity lower bound here. I wanted to ask: it might be that this is the case that this. Might be that this is the case that these bounds are not tight. I think there might be some distribution that if you do epsilon times m communication or information on that distribution, so not epsilon squared times m communication give you such a bond, but in fact, much larger, even epsilon times m communication, you cannot change the distribution by much. And the typical distribution for set distrainers won't have such a property. Won't have such a property, but there's a distribution that Mark and Ankor use for proving a lower bound per se jointness. I'm not sure, maybe that one will give you some property like this. And if you use that, you can immediately improve the bonds in HPC also that P to the five, I think, from P to the three or something. Okay. So now that we have this lower bound, how do we use it to prove a lower bound for this MIS? Prove a lower bound for this MIS problem. Think of the following layered graph. I'm going to put the vertices of my graph here. Remember, in this HPC, I had M sets. Each player had M sets from a universe of size M, and I wanted to follow a pointer for KS. I create this graph that has K plus one layer. I put these vertices one to M here. IDs are decreasing from left to right. Increasing from left to right. And vertex I at each level is connected to the next level by Alice using the edges that are not in SI and by Bob using the edges that are not in TI. Now, let's look at what happens. Look at the first vertex. Alice has edges to a bunch of vertices on the other level. Bob has edges to a bunch of other vertices on the other level. Vertices on the other level. Si and Ti intersect in exactly one element. So it means that there's just one element in the second layer that has no edges from the first vertex. If the first vertex has to go to this LFMIS, this vertex is the only vertex that will not join this LFMIS. So this is the next vertex that will not be part of. Sorry. This is the only vertex that can be part of L F Mis. Vertex that can be part of LFMIS. And by this ordering of vertices, you have to pick this vertex inside the MIS. Now that you picked it, the same game is played. There is only one vertex on the next level that will join this MIS. You add that, you just continue doing this thing. And basically, the set of vertices that you're adding to your MIS is exactly the set of pointers in that HPC problem. That HPC problem. So these problems were almost the same, really. If you can solve LFMIS on this graph, you can solve HPC also. And this will give you a lower bound for the problem. Can I ask a question? Yes, Stephanie. Good. Very good point. So you can make this. Okay, that's an excellent question. Question is, these are creating more. Question: Is these are creating multi-graphs? There is a simple way of creating this graph to be simple. You can just copy vertices twice here, they'll go to one vertex there. Like in the, if you look at the paper, actually, we proved it for simple graph. That's not always possible, but in this particular instance, you don't need to worry about multi-graphs. You can add a bit more vertices to it and make it simple. That's a very good question. Question. Any other questions? Yeah, I should add this thing also. Early on, I think us and many other researchers, we've been a bit too cavalier with using multi-graph. It seemed like most of these lower bounds, we can just prove them for multi-graph, and it seemed okay because we thought multi-graph and simple graphs are the same. More and more recently, we see some surprising. And more recently, we see some surprising examples that some problems suddenly become very hard just because it's a multi-graph. So it's a good idea to try to prove lower bounds directly for simple graphs. Okay, good. So let me move over this animation. And so, if you do the math, basically, now we have a graph. math basically now we have a graph with this should be m times k vertices not n n is the number of vertices so we have m times k vertices the lower bond was ms square over k square becomes ns square over k to the four the numbers do not matter that much the important thing is that now the lower bond is quadratic if you have a p-pass algorithm you have to pay another p in this communication or a space lower bound so this gives nsquare over p to the five a space lower Over p to the five, a space lower bound for p pass streaming algorithms. Okay, this is a space, this is not pass. And simple question here is I don't think and like p to the five was just some number just to make everything work. I think one should be able to reduce it all the way to p square. It will be nice to do it, but not too interesting. It's a good question. I don't think it's a hard question. It's a hard question, but let me just wait a minute, say something. I mean, you have been nice, nobody really asked, but I think the actual question here is, why do you even care about this problem? Okay, it's not really the type of problems we ever wanted to solve in a streaming. And the honest answer is that we really do not care that much about this problem. The goal was to just have a lower bond technique for proving a lower bond for these problems. A lower bond for these problems that are not too hard. Okay, this lower for problems that their communication complexity without restriction on the number of rounds is small, but you can, if you restrict the number of rounds, even to some polynomial, they still require a lot of communication. And this HPC seemed like a technique at that point, at least, to prove lower bound in this regime. And we later on managed to use it to prove lower bonds for some other problems that I find. For some other problems that I find more interesting. Let's say weighted ST minimum card or capacitated bipartite B matching. The catch is that now we have to use very, very large capacities or weights, something sub-exponential to get these lower bounds. So the problem I think here is that even though I talked about this, we have this technique, I feel like we didn't manage to prove interesting. didn't manage to prove interesting lower bound use using these techniques. Okay, there are like there are more interesting problems that lie in this regime that we still do not know how to prove lower bound for. So let me pose some like question. What are some other problems that we can prove a lower bound for? In particular, can we prove lower bound for problems like reachability, shortest path, or exact matching beyond log and passes? Beyond log and passes. I think that would be very interesting. And really honestly, that was what we tried to prove when we were getting this lower bound for LFMIS, and it didn't work at that point. I don't know. I don't actually want to even conjecture that these problems definitely have such a lower bound. I'm not sure. But at least the current algorithms are way off. Like they have a square root n or n to the 3/4 passes to solve the same problems in n polylog n as. Problems in polylog n space. So it will be very interesting to see if we can prove lower bounds that go beyond log n pass for these problems. Sounds good? Okay, so this was this regime. This is the regime that we were trying to prove lower bounds beyond log and passes, and we can prove it for some problems. Not the most interesting problems, but at least for some problems. So let me talk about. So, is there a question? No, good. Okay, let me talk about another regime that we have been way more successful. And let's just start talking about two pass algorithm. Okay, so we had one pass algorithm, we understand them fairly well. Now, let's just give them one more pass. And already things go very crazy here. Here, they're really more powerful. Two-pass algorithms are really, really more powerful than single-pass. My favorite example is this global mean cut problem. In single pass, you need omega ns squared to solve a problem. In two passes, you can find an exact global mean cut in n log n space. Okay. And this is, in fact, follows from a code query algorithm of Ruberstein, Schrein, and Weinberg that you can now. That you can now adapt it to the streaming and get this result. I find it very surprising, really. This is an exact problem. You can find an exact minimum cut of a graph in just two passes and n log n a space. And for the same reason that the algorithms suddenly become much more powerful in two passes, proving lower bonds become also much more challenging. And some of the challenges of proving multi-pass lower bonds already manifest themselves when you just want to prove two-pass lower bonds. That's where we are starting. Slower ones. That's where we started to like target and prove results. And here, one of these early results was we drawn that we proved that any two-path algorithm for problems such as directed reachability, undirected shortest path, or exact bipartite matching requires roughly quadratic space. And a square divided by 2 to the square root log n. So ignore this as. So ignore this small of one term, or there is this small n to the small of one term here. And remember, n squared is the trivial space. So this result says that modulo did this n to the small of one term. There are no non-trivial solutions for these problems, even in two passes. And previously, we only knew how to prove this n to the one plus one over p type of trade-offs, which give you n to the seven over. Which give you n to the seven over six space. Good. Let me see if I can show you briefly how this lower bound works. Yeah, I'll go through it very quickly. So the idea behind the lower bond is just as follows. I show you a random directed bipartite graph. Edges are going from left to right. Okay, so this is a random directed bipartite graph. Then we are going to pick this gadget. Then we are going to pick this gadget that now, in hindsight, we can call it vertex hiding gadget. And it's something like this. So there are these two graphs that come first, the one on the left, the vertex S that you want to solve the ST reachability for belongs to that. Vertex T belongs to the other one. And what is their property? Their property is that in this gadget, this blue regime, this random bipartite. Regime, this random bipartite graph, only one vertex can reach, is reachable from S. Okay. And in the other one, only one vertex can reach T. Good? And the main property of this gadget is that a single pass streaming algorithm with a smaller space have no idea what is this vertex. This pair of vertices SS star and TS star remain entirely hidden to the algorithm. Entirely hidden to the algorithm, or the distribution remains almost the same as it was at the beginning of the algorithm, even after the single pass of the algorithm. Why is this a good thing? Think of it like this. If only S star can reach is reachable from S and T S star can reach T, the only way S can reach T depends on this single edge, this single black edge from S S star to T S star. star to ts star okay now how does the how does the instance look like first i first show you this random bipartite graph you don't know the endpoint of this pair you cannot remember all its edges it has too many edges so you're most likely not going to remember whether this black edge existed or not then i'll show you these green gadgets their property is that even after the pass finishes you will not figure out what is You will not figure out what is SS star and TS star. So at the beginning of the second pass, you still don't know what is this important pair. You cannot store its edges. Okay. So you will miss this black edge even in the second pass. Now, during the second pass, you figure out what are S star and T S star, but that's too late already, no? Because the edges in the middle have passed. So you have no idea whether S can reach T or not at this point. At this point. And that's how the lower bound will go through, really. And the main part is to create these vertex hiding gadgets. They use some combinatorial construction using these Rosa-Samerady graphs. And that's why we get this type of NS square minus a small of one term. And then they'll go through some set intersection arguments similar to the ones that I showed you for that HPC problem. Good. And if you do this thing, you can prove the lower bound for. You can prove the lower bound for reachability, and similar reduction will work for matching and shortest. Now, here there have been lots of exciting improvement. There's a very nice work by Gilot, Rawbon, Shoach, Cheng that are here. I think the other authors are not. Good. So, this is a very nice result that take this approach and really extended it dramatically. Some sort of recursive extension. Sort of recursive extension, maybe you can call it, that takes these things that I was calling vertex hiding gadgets and turn them into things that can hide larger sets, not just pair of vertices, but very, very big sets of vertices. Once they had that, they managed to recursively build on this thing to create hard instances, not only for two passes, but for a very large number of passes. In fact, they proved that for the same problems, you need roughly any square. Problems, you need roughly an hour space, even if you have up to a square root log n passes. Okay. And in fact, that result, you can even think of it, it will hold all the way to log n passes. If certain dense Rosa-Samerity graphs exist, we don't know whether they exist or not, but we also, the current combinatorial techniques do not rule them out. So you can think of it as a barrier result. If you get a better than log n pass algorithm for any of these problems. Algorithm for any of these problems in streaming, you are proving that certain dense RS graphs do not exist, which seems like a very challenging question. And in fact, the same authors have some result for directed random walks in two passes. I'm not that familiar with it, so I will not talk about that. Another follow-up here, I had a result that says not only finding exact matching, but some constant approximation of maximum matching in two paths. Of maximum matching in two passes, and this n polylog and a space is also hard. And here it was good because you can now prove even constant factor approximation now roughly in the ballpark current algorithm. The current algorithms rule out, sorry, the current algorithm achieve things which are roughly 0.6 approximation. The lower bound is way off. It says that you cannot get 0.98 approximation, but at least now there. Eight approximation, but at least now they're constant, somewhat close. And here I should say again, the actual constant depends on this unproven, or not unproven conjecture, depends on the density of RS graphs. Okay, so you can think of them as conditional lower bound or barrier results. Good. So, in this regime of small pass streaming algorithms, now we are starting to prove lower bounds that are getting qualitative. Bounds that are getting qualitatively close to upper bound. It's still a long way to go. I don't think we are anywhere close to the power of lower bound techniques we have for single-pass algorithm, but maybe we are getting there. And so I think the main progress for multi-pass algorithms in the last couple of years was in this regime. Very nice question here is: can we get lower bounds for number of passes if I want to apply? Number of passes, if I want to approximate maximum matching to one minus epsilon in n polylogana space, can we prove any lower bound on how many passes we need? These two pass lower bounds that I showed you just show you one point of this trade-off. It says that in two passes, you need, I don't know, 0.98 approximation, or better than 0.98 cannot be done in two passes. But if in general I want one minus epsilon approximation, can we prove lower bounds here? Prove lower bounds here. And I feel like using current techniques, it shouldn't be even hard to prove log one over epsilon passes are needed. But more interesting would be, can we prove some polynomial lower bond in one over epsilon, a square root one over epsilon passes are needed? And here, the upper bonds are not too far. The upper bonds are one over epsilon square passes or log n over epsilon passes. So if we can prove this type of lower bonds, I think it will be very, very interesting. Very, very interesting. All right, so questions? Good. All right, so let me talk in the last seven minutes or something about this other regime at the. Oh, sorry, there was this thing I wanted to talk about. I think. There was this thing I wanted to talk. I think I won't be able to talk about it, so I'll skip it. But because I just showed this one, we can prove lower bound for a different type of trade-offs. Now, it only holds for a restricted family of sketching algorithms. And this is also not a made-up trade-off. Like it holds for MIS that the current algorithms achieve the same trade-off, but the current algorithms do not belong to the family of sketching algorithms that reprove the lower bound for. We prove the lower bound for it so now that I said it's good, but I will skip this part. Let me briefly talk about this CSV part also, the one at the bottom that we didn't talk about. And here, let's switch gear. We don't want to solve reachability, maximum matching. Now, the problem is, or like the problem or somewhat simpler, you want to just estimate the size of maximum matching size, or you want to property test whether the graph is connected or not. Test whether the graph is connected or not. And here there is a very general technique for proving lower bounds. There is this problem called gap cycle counting, which is the following. I give you a two-regular graph. It's either a vertex disjoint union of k cycles or two k cycles. And k is a small constant, right? 100, let's say. You want to distinguish between these two cases. Two cases. So either I show you the graph is which is like a bunch of triangles or a bunch of six cycles. They are coming in the stream. Which case was this? Verbin and Yu introduced this problem. They proved that this requires roughly n to the one minus one over k space. It's a very nice lower bound using this problem called Boolean hidden hypermatching. And this problem itself is based on this BHM problem of Govinsky at all. Problem of Govinsky et al. It's a very, very nice result. I think at this point, it might be perhaps the strongest tool we have for proving single pass lower bounds in this regime. And also, this bound is tight. You can just sample these many vertices and store their edges and solve the problem. So there's this GCC problem that you can use. There's this lower bound by Verbin and U that get this result. And it turns out this problem captures difficult. This problem captures the difficulty of lots of other problems. These are some examples of them that just reduction from this problem itself will give you lower bound for all these other problems. What is the catch? Oh, by the way, like you may remember from Michael's talk, it wasn't exactly about counting, it was about finding cycles. But you can see again, these problems appear a lot in this context. lot in this context. The catch is that all these lower ones are for single pass algorithms. And the reason is that these two problems that they build on, hidden match, Boolean hidden matching and Boolean hidden hypermatching are inherently for single pass or one way communication. If you allow two way, both of them can be solved with log n. And that's a bit weird because GCC is not at all a problem which is like restricted to single pass. It's like restricted to single passes. It's not at all clear how you can solve this problem in two passes or three passes. And we wanted to understand what is happening for this problem. And it turns out that for the same problem, you can start proving multi-pass lower bounds. The first was with Guillot, Rohanch, and Ho-Chang that we proved that a p-pass algorithm requires this type of a trade-off. Without going into that, let's just say that if you want polylaw organic, Let's just say that if you want polylog NS space, you need to spend omega log k passes. So, logarithmic in the length of the cycles, you need polylog ns space. Then in follow-up work with Vishwajit, we show that actually you need to spend passes linear in the length of the cycle. And the second one is now tight, you know, because you can just sample a vertex and do a BFS from that vertex for K steps, and you figure out whether you were in a K cycle or in a 2K cycle. A K cycle or in a two K cycle. Comment that the first one is a stronger than what I said. It holds even for distinguishing between Hamiltonian cycles and short cycles. And the second one is a bit weaker than what I said. It requires you to add some noise to your input. In both cases, you always have a bunch of length k paths as well. But we can prove this type of lower bounds. Looking at the time, I don't think I have time to go through. Time, I don't think I have time to go through much detail about it. Actually, how much time do I have? I don't know when I start. Um, I think about five minutes. Okay, so then let me not go try to talk about how to prove this result. It was fun, but we will skip this. Let's come here. So, so you can prove lower bond for this. If you are interested, I'll be very happy to talk about the offline video. About the offline video, but you can prove this type of lower bound. So we have tight points for GCC in multiple passes. This gives you some tight lower bound for property testing problems, for instance, and lots of non-tight lower bounds. For instance, it tells you that non-tight, I mean, I don't know whether they are tight. I highly doubt they are tight, but it's not like me, no, they are not tight. For instance, it tells you that if you want to get a one process. Want to get a one plus epsilon approximation of max cot or matching size in polylogana space, you need one over epsilon passes. And here, I think the main question that remains here or always remain here is what can we do for problems like MaxCot? For instance, can we prove you cannot get better than two approximations in just two passes even or more? Or more, or alternatively, instead of one plus epsilon for fixed epsilon, 10 to the minus 100, I don't care what is that constant, just for some fixed constant, is it the case that you cannot get one plus epsilon approximation in a small number of passes? And here, I think Roarwange mentioned also Guillot, Robert, and Huachen, and maybe others have been working on this. I don't know exactly. I leave it to them. If you have questions about. Leave it to them. If you have questions about this, you can ask them. So, good. So, this is this question here. And this is basically lay of the lands. Now, if you look at here, in this regime, this was the regime beyond log n passes. I think the most interesting regime for lower bounds. We can prove something below log n passes. We can prove lots of things. Now, it's still very far from what. Now it's still very far from what we should be able to prove, what we are proving thing. And at the bottom, again, we are proving interesting results, really. So good. Let me conclude. I think, yeah, as I said, the main question to me is that prove lower bond beyond log and passes in omega and a space regime. Reachability, maybe some other problem, it will be amazing to prove those results. Prove those results. What one thing I wanted to say is like a general shortcoming of all these techniques. Any lower bound that we have currently for multipass lower bounds, its hardness comes from difficulty of local exploration. You embed some sort of pointer chasing, a soil argument inside your input. Usually, like these are correlated pointer chasing, you have to chase, I don't know, teto and pointers at the same time. Teta and pointers at the same time, those types of lower bounds, but still you have to chase pointers. It's a local exploration hardness. But the thing is that most of the current upper bounds in the literature are not based on local exploration. There are, for instance, iterative optimization methods. You run multiplicative weight update or you run gradient descent for shorter span, multiplicative weight for matching or gradient descent for shorter span. For Teridian descent for shortest, but that's how you solve the problem. They don't really look like local exploration type approaches to me. Either it's the case that they are not the right algorithmic techniques or current lower bound techniques are not proving the lower bound for this type of result. Is there another lower bound technique that can capture the difficulty of doing, I don't know, monitoring? Difficulty of doing, I don't know, multiple rounds of multiplicative weight, for instance. I think that would be amazing if we can start proving lower bounds that come closer to the techniques we are using in our upper bounds as well. And that's all I wanted to say, really. Thank you. Any recent examples of interesting things? give some examples of interesting applications of this yes good so multiplicative weight update separate can you repeat the question sure okay yeah let me say even if i have the question correctly thing is there any interesting application of this iterative optimization methods in multi-pass streaming so we now know how to solve both packing Solve both packing covering or mixed packing covering LPs using multiplicative weight in log n pass or even fewer number of passes in a streaming. People have used it to solve approximation to bipartite matching, approximation to shortest path, load balancing, this type of problems. So you can do it in a streaming, but these are again upper bound. But these are again upper bound-wise, we can use them. Lower bound-wise, I don't know. In fact, you can use even more like fancier stuff. There are some recent work that use interior point methods in a streaming. Those requires too many passes. So that, I don't know, but multiplicative weight or gradient descent, people are doing it in log n passes, sometimes even constant that depends on epsilon number of passes. Sure. So very nice talk, Separ. Can you just go back a slide and I want to ask you about this picture? Just the one. Yeah, that one. Right. So this upper diagonal that you have, and you say, well, you know, that's really the upper bound on the lower bounds that we should seek. But in principle, we can go beyond, right? Go beyond, right? Is it so? And it seems that the big difference is that we are using communication complexity, but we are not distinguishing total communication per, or the fact that, you know, after the first message is sent to Bob and he responds, he has to forget what was received. Exactly. Exactly. Yes, exactly. So it's exactly this diagonal term is basically saying that Bob remembers the thing that he has heard in previous classes. That he has heard in previous passes, he does not have to pay for that in the space. Yet, in principle, we can prove lower bounds above here. I feel like those are bounded as space computation, lower bound. In the limit, you are proving lower bounds for log space computation, for instance. I have a feeling the techniques, as far as I've seen so far, are so different from the techniques we are using currently that it basically are two different things we are now talking. There are two different things we are now talking about. I don't know if it's right to even call those algorithms a streaming, for instance. Okay, in fact, it's a question. Do you call an algorithm that uses a square root n passes a streaming? Maybe so, but and if the number of passes is really small, if it's like n to the small of one, then there's not much difference between these two. After that, I completely. After that, I completely agree with you. I just say that talking about that regime is completely beyond the scope of current techniques. At least I know. Maybe one more standard center again. Sorry, posting this morning, but I don't know what that's up in my mind. Yeah, I mean I said I'm doing it because people don't know how to access it. I can log into it. I mean okay all right. That's the true way to do it. That's the way to get out of it. I know. Yeah.