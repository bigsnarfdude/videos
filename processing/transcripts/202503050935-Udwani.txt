From Berkeley, talking about optimality of non-adaptivity in allies and varietal welfare with stochastic updates. Thank you, Lewa. Before I start, I just want to thank the organizers for this wonderful workshop. I'm really glad to be here having a lot of fun conversations. Okay, so I'd like to tell you about a generalization that captures a bunch of resource allocation problems where there are some stochastic outcomes. So you choose actions, and then actions are stochastic outcomes. And then actions of stochastic outcomes. And perhaps the main or the surprising thing that we find is that you don't need to adapt to these outcomes. So you can be completely oblivious to the stochastic outcomes and still get the best possible opportunities. Okay, so what I want to do is first tell you about this classic problem of online supportular welfare. So it'll be a bit of a review to just make sure that we are all on the same page. We are interested in different kinds of We are interested in different kinds of arrival models, so I'll first just give a review of this and then I'll try to convince you that in contemporary settings, settings that we've been looking at in the last decade or so, we really need a new model. We need a generalization of OSW because OSW fails to capture many of these settings. And then I'll talk about the main results and tell you about this new formulation and hopefully time permitting give you through an example an illustration of how the proof An illustration of how the proof technique works. Okay, and hopefully, leave you with some interesting open problems at the ready. Alright, so we are going to be interested in set functions here. So, let me start just by defining a set function and the two key properties that we'll assume. So, a set function, capital F, will map subsets of a ground set A to non-negative real boundaries. You see non-negative functions. Now, a function f, set function f is monotone if I give you a set S. If I give you a set S and a superset S plus, the value for S plus is at least as much as S. More is better. The function is submodular. If I give you an element A that's outside, I add A to the big set S plus, I get less bump in value than if I added A to the small set. So decreasing marginal gains. More I have, the less new value I get. I want to bring this notation to attention because I'll keep using it in the later slides. So this is marginal value. So, this is marginal value of an element A given that I already have said S plus. So, it's the difference. So, hopefully, this refreshed your memory about polar 2C and submodularity. So, let me start now with the submodular welfare problem. It's well studied. These are some of the, let's say, papers that showed new results over time. I'm going to talk about the formulation that is, for example, considered by this last paper group finder. It's a generalization of the classic formulation for the experience. Of the classic formulation for the experience. So, okay, in the online submodular welfare problem, I have a monotone submodular function f over a ground set of actions A, and I have a partition of this A. From A1 through AT, these are the parts of the partition. Now, initially, this set of actions is hidden. That's why I've played it out. And it's going to be revealed to me sequentially. So, the first step, I see the first part, D1. And now I'll be able to see all the values. And now I'll be able to see all the values for all subsets of A1. I don't really know anything else. And I have to choose one action from A1, exactly one. So I choose an action, let's say small a1, and then my reward so far is f of a1. Next step, I'll see a new part. 2. I repeat the same. I want to choose exactly one action. Let's say I choose A2 and I keep going. Last arrival, I see the final set, part AT, and now I have access to all the Now, I have access to all the values and I choose one action and the game stops. The action that you pick, like say at APE, has to be part of the recently revealed action. That's right. So A2, small A2 is part of pick A2 and so on. So from each part, I choose one. So you can think of it as a partition matrix. Each partition is revealed to us. You know the whole set of each days? I'll talk about it just on the next slide. So yeah, you're an element or you can add. Here an aluminum or you can have any system. So, capital is um a part of uh the partition. So, I have a partition of capital A, right? So, anything that you see as capital, this is a part of this big set A. And small A is a particular element inside of B. A is a set. Yes, all the capitals are sets, and all the smalls are elements. And the action you take can be set or just a elements. It's one action. One element. Like A2 can overlap with A1. Without loss of generality, for monotonous and modular functions, we can just create copies and assume they are disjoint. So let's just assume they are disjoint. But that will be without loss of generality. So this is chance to be confused. So if it's disjoint, then that prevents me from picking an accident that was part of game one. Whereas if it was overlapping, I have a largest set of choice. No, so this is what happens. Maybe I'll take this in detail offline. For today's talk, just assume it's disjoint, but let me keep it with a high-level. Assume it's disjoint, but let's leave it with a high-level story. So, let's say I want to repeat an action in A2. I can just create a copy of an action in A1 and call it a new action. And I can do this always without lots of generality and maintain monotonous. I can give you more. I haven't specified how the instance is generated if there is randomness. But when there is randomness, yes, there will be a stochastic time. All right. So, how is the instance actually generated? If we look at three different models, so the first model, this whole instance is just generated by an adversary. So, the instance includes the set A, the partition, the sequence, the function, everything is adversarial. The adversary has to make sure that the function is monotonous or what. Okay, so let's just use G to denote the instance. And in the adversarial model, my goal will be to design an algorithm with. Goal will be to design an algorithm with the best possible conduct ratio, which is really the worst case ratio of the online algorithm with the offline algorithm that knows everything, knows the full instance. That's adversarial, very pessimistic. I can consider something less pessimistic. So in the random order model, adversary generates an instance, same as before, but the nature randomly reshuffles the partition sequence. So now, when I look at the compared ratio, the online algorithm will see a Compared ratio, the online algorithm will see a permutation of the sequence. So it'll maybe do a bit better. It doesn't see an utter seven sequence, it's randomly reshuffled. To the offline, it doesn't make any difference. Offline knows the full instance anyway. So that's a relaxed border. I hope you get better guarantees in this. I can relax it further and consider an unknown IID model. So here, again, adversary generates the instance, and then nature gives me arrivals where. Nature gives me arrivals where, at each arrival, it'll sample one of these T sets with probability one word. Just a quick animation. So now, related to what Yash was asking, some of these sets may be repeated in the unknown ID model. So to get some intuition, you can think of it as sampling with replacement from this set of keeprames, whereas the random order was sampling without replacement. Whereas the random water was sampling without replacement. So, this is a further relaxation. So, I hope to do even better in this unknown idea. To just clarify, so the classic ones are actually the dish of cover is always a special case because you look at the ground that's a set of all the edges. I'll say more about it when I give an example. The gigantic sub-boss is a summation of all the justice. You're on the right track. Okay, so in this case, my compared ratio is comparing justice. Ratio is comparing justice. My compare ratio is comparing the expected performance of the online with the expected performance of the offline because offline also sees a random instance. So offline knows the exact random sequence of arrivals and I take expectation. Question? So in the unknown ID case, you let's just say for the algorithms we'll look at today, we don't even know the horizon. I don't even know how many are algorithms. You can say that I know the horizon and then ask the question, can I do better? Ask the question: Can I do better? But we'll actually work in this oblivious model. So, so release test with them now or could keep going. Yeah. Are these disjoint in this model as well? So, this one is disjoint, but remember, like what I was saying, you can always make them not disjoint. It's without loss of geographic. Thus, the often can choose two from the same set. Yeah, given that we can make it non-disjoint by creating coffees, it could end up happening. No, no, no, but I'm saying that for the No, no, no, but I'm saying that for the offline, is it possible to select two from? No, no, no, no, no. Yeah, good. So to clarify, offline and online both choose one action from each part. So we are fair about that, and the gap is just information. And do we know that at the end of the day, all of the sets will happen in our partition, but in the random order as well? In the random order, yes, it's randomly reshuffled. In this one, that may not help because we are taking IP samples. Ah, yes, yes, yes, yes. So, if we do the random struggle, then why don't you just solve the optimal offline and then whenever that set happens, show it? Like, it is a set function after all, like at the end of the day. So, you're saying in this case? Yeah. So, in this case, the online algorithm still doesn't really know what it's going to see, right? So, all it knows is that an adversary is going to sample some instance, and then nature will randomly reshuffle the instance and then send it to me one by one. So, when I receive something, I still don't know what's going to happen next. I still don't know what's really important. I see. So, it's possible that at the end of the day, you didn't see all of the partitions? In the running model model, you'll see everything. You don't have the partitions as the only algorithm. The other tip is a set of partitions, and the nature is shuffle formal. So, the question is, is it a shuffle or is it like a random pick and throwing it? So, this is random pick without replacement, right? So, as a result, you will end up picking it, which is the same as random reshuffle. So, and if you are, if the uppoint is also supposed to be. You are, if the offline is also supposed to choose one from each set, then why don't you solve the offline? And then, whenever, since you know that at some point you will happen, and then the function is like a set function, which means that the order doesn't matter, whenever that partition happens, just show the element from the offline. So, I cannot solve the offline because I don't clarify the partition. I can even clarify offline. Okay. All right. So, if I give you a second, Alright, so if I give you a second to think about it, which I won't be able to, this is probably the first algorithm you'll think of. Online greedy algorithm will say that I'm at arrival D, I need to pick one action. So I'll pick these actions, small a1 through small a t minus 1 in the past. What action should I pick? Just be greedy at the action with the maximum marginal value. So pick the action A that maximizes this marginal value. All right. And this is valid in all the array forms. I can always be greedy. We can also look at a greeny-like algorithm that is greeny but with respect to. Algorithm that is greedy but with respect to some other function f tilde. So I'm going to be a bit nebulous about this because of time constraints, but I can tell you more of one. So to give you some insight, this f tilde is a perturbation of f. It can be a randomized function or it can depend on the past actions that you have chosen. For the experts, this could correspond to inventory balancing algorithms or perturbed PD algorithms. So I'd be happy to tell you more offline about how this F tilde might look like. Okay, so let me give you a quick Like. Okay, so let me give you a quick example. Let's look at this classic problem of bipartite matching. Radh already warmed us up. So we have the set of resources. These numbers C1 through Cn are capacities. When I get an arrival, I see edges on the arrival, and I have to pick one edge. It's a matching problem. Now, if I want to put this in the context of OSW, the set of actions revealed to me at the first arrival is just a set of edges. And I pick one edge. So let's say I pick this edge. I'm going to get a reward of R2. Edge, I'm going to get a reward of R2 and I lose one unit of capacity of resource. So set of actions is just set of edges, but is the objective monotone submodulate? So at the very end, I claim that the objective just looks like this curve. So this is more formalized, but I want to bring your attention just to this curve. So x is the number of times I match a resource, n. So the reward that I'll get will be proportional to x up until I hit the capacity, at which point my reward is saturated. At which point my reward saturates. So this is clearly monopoly. It's also submogen. And my overall reward is just the sum of reward over all the resources. So it's also a monopoly project. So this is a simple example of an online subordinate benefit project. So, sorry, Rush on. Jeff, on the previous slide, I had the impression that you have rewards. The rewards cannot be edge-dependent here, right? No, they cannot. Okay, okay. But they can be like free disposal, like they aren't edge. Disposal like an art mention. Yeah, but yes, we don't have H dependent between these. So, this OSW is really a reference model in the sense that it captures many classic settings, including ad words, display ads, which we were just talking about, and concave reports, etc. So, that by itself doesn't tell me anything because I can always construct a magic giant problem that captures a lot of settings. OSW is really attractive because it also has this nice fact that if I take this. The fact that if I take this PD algorithm, it is 0.5 competitive in the adverse L case, better in the random order model, and 1 minus 1 only in the unknown RID model. So I get these lines in nice improving constant factor for a simple algorithm. Moreover, if I take the DD algorithm, let's say for the ad words problem, I will do no better. I will end up with 0.5 in the adversarial and 1 minus 1 only in the universe. So it's really a reference model in the sense that it unifies and generalizes results across different settings. Settings. Another thing I want to stress on a little bit here is that for the OSW problem, this result and this result are optimal. So even if you try to be clever and look at some fancier polynomial time algorithms, you won't be able to do better than 0.5 and 1 minus 1 only in the UI in Percel and UII market. So these are best possible results under complexity assumptions. So this is great. If you give me a new setting, I'll try to see if it's a special. I'll try to see if it's a special case of OSW, and I'll get these nice baseline results. So, what's missing is that in many of the settings that we have been interested in the last several years, we, for one thing, have stochastic outcomes. So, I choose an action, and that action has some random outcome. To give you one example, let's say I'm thinking of online-by-bundered matching, where the arrivals are customers. So, customer comes, I propose a match. With some probability, they accept the match, with some remaining probability, they leave the system. Some remaining probability, they leave the system. So I decide what edge I'm picking, and then I see this realization. I could do something more sophisticated. I could pick a set of options, I show this to the customer, and then they randomly choose from it. Or I could show one option, the customer declines, I show another, maybe they decline, and I keep doing it until the customer leaves some problem. So I can have all of these different actions. The common thread is that you choose an action, and then after you choose the action, there's a random output. The online algorithm. The online algorithm, before picking the action, only knows the probability of how an action maps to outcomes. It doesn't really know the outcomes. And it may be able to see the outcomes of past actions if it's adaptive. So I showed some action to customer T minus one, I saw what they did, and I can adapt to this at customer. That's an adaptive algorithm, but it may be non-adaptive. What the prior customer did may not be revealed to me at all. So I may have to be oblivious to realize. So, I may have to be oblivious to realizations of stochastic outcomes, and I may have to operate in the world of expectations. Okay. So, these are not captured by OSW because of this adaptivity, non-adaptivity. The other thing that OSW does not capture is the fact that, well, sometimes objectives may not be supportive. So, to give you one quick example, if I think about online matching, but not for a reusable resource. So, what does that mean? So, I have this arrival, I match it, there's one unit of capacity, so I'll get. It there's one unit of capacity, so I'll get this one dollar reward. And then this is a reusable resource, which means that I'll now have zero capacity, but only for the next D duration. And after this D units, the resource will come back. So if I get an arrival in this duration of D and I try to match it, I get no reward because I'm out of capacity. But if I get an arrival after this D duration and I match it, I again get one comma. So it's easy to see that this is not a submodular objective because when I add arrival 3 to the set 1,2, 3 to the set 1, 2, that gives me 1 number. But when I add 3 to just the set 2, 2 and 3 are very close together. So I get 0. Nonetheless, if I look at a Green algorithm, it's still going to be point-fly compared. So this indicates that submodularity is not the end-all-be-all. There's something broader than this where PDE continues to have some of these technologies. These are the two reasons why we want to try to journalize Westman. Give you an ID and the user resource tool? I don't know. Good question. I can say more about it in the end. Alright. Okay, so our main goals are going to be to look at a unified model and then try to analyze simple algorithms across different arrival models, hopefully without doing too much work. And most importantly, perhaps, to understand the merits of adaptivity. If I look at adaptive algorithms, do they buy me anything in terms of compare ratio across these different models? These different models. Okay, so here are the results informally stated. For a general formulation that I will hopefully be able to tell you about shortly, which has stochastic outcomes, if I want to analyze the compare ratio of a non-adaptive 3D-like algorithm, I can just look at the compare ratio for the simpler setting of deterministic outcomes. So, if you have already done work for deterministic outcomes, you can just lift the result for the stochastic use. So, something I want to highlight here is that we are comparing non- One thing I want to highlight here is that we are comparing non-adaptive online algorithms against adaptive offline algorithms, which is what makes this interesting. Okay, so what is this bias? Fine. If I want to analyze comparative ratio, I can look at a simpler deterministic setting, but so what? So for one thing, if I look at online submodules, I have its stochastic outcomes, a setting that I'll define shortly. An over-up-defined VD algorithm gets all the competitive guarantees using priorities also. So it's 0.5 competitive in adversarial case, 1 minus one every in the UI. 1 minus 1 over E in the UID case. And furthermore, I know that nothing can beat this 0.51 minus 1 over E already for the deterministic problem. So that means that no adaptive polynomial time algorithm can beat these categories that you get by non-adaptive. So non-adaptive algorithm is, in some sense, in general, the best that you can do. So the user research is a stochastic assumption would be the duration of the settings? So that is a setting that we cannot quite accept. That we cannot quite get to. That was my thing. Yeah, yeah. I'll definitely say more about it. Questions? Yeah? So, can you repeat what adaptivity means again? So, I'll give you more intuition in the suit, but briefly, adaptivity is that I'm adapting to the realizations of past actions. Like I showed a customer an assortment, they chose something. So, when I go to the next customer, I adapt to this uh choice of the customer. But I already know the distribution, right? So, why is this adapted to help? So, why is this unaptified? So, for example, a customer chose something, I know that I actually lost one unit of a particular resource. If I don't look at what the customer chose, I only know that, well, with half property they chose this resource, with half property they chose the other resource. It's a great question, and I do intend to tell you more concretely on how it is. You're adapting to the state of the system, that's changing. Yeah, okay. One more question. So, you said that in the three cases, the left and the right case, you're optimal. What were the The left and the right case were optimal. What about the middle case? The random model model? Good question. So the random model model 0.5796 is just the state of the art. We don't know what the best is, even for the simpler OSW problem. So if you improve something for the OSW problem, it will automatically translate to this more general problem. The optimal thing node and special cases belong in matching, or you have a list of special cases. I think maybe the optimal, don't quote me out, but as far as I know, the optimal is what going in. But as far as the optimal is not going in special cases, like it's still open to try to improve the constant factors. But yeah, I fascinating that you know under IIPA MySmania and it proof is like five lines, but other random things. So I'm still a little confused about corollary two, but maybe it's the idea that an adaptive greedy-like algorithm in the random case would also give you a competitive rate. Would also give you a competitive ratio of the deterministic setting? The corollary two is just saying that for this and this result, you cannot beat it even if you are looking at the space of adaptive algorithms, not just greedy-like, but any adaptive algorithm. So, really, this simple non-adaptive greedy algorithm gets you the best possible guarantee for adversarial and URL 81. Adaptivity cannot buy you anything more as long as you're looking at polynomial time algorithms. So you said the left and right case, these are optimal, right? So is it? I'm a little confused. When you say this is optimal, that means there exists an instance where you cannot beat this, right? It means that no matter what polynomial time algorithm you take, there will be some instance where your ratio will be 0.5 points. Then my question is in those special cases, is it conceivable that we can read this? Yeah, absolutely. Absolutely. Okay. The lower boundary is weird, right? It's not purely information dirty. It's actually information 30 can have computation. Yeah, yeah. There's a boundary. And clearly it's a problem. Oh, I see. Well, if it's weird, I mean, I have never seen anything with that as well. Yeah, yeah. Maybe trivial alternatives. Does corollary 2 come from the fact that deterministic instances are stochastic instances? Yes. Alright. So we can say something beyond greedy. So for OSW, greedy is kind of So, for OSW, 3D is kind of what gets the state-of-the-art guarantees, but for special cases of OSW, 3D-like algorithms do better, which is what Siva was asking about. So, we can show using the same technique that in the adversarial model, if you give me a resource allocation setting, which includes some of the examples that I just flashed a few slides ago, and I look at large inventory, the kind of thing that Rav was talking about, I can get a 1-1 only non-adaptive offer. Just by the fact that somebody showed some results very early on for details. Some results very early on for deterministic algorithm, I can just lift them to non-adaptive. So, to give you some context, this result was already known for adaptive algorithms. If we show that you can get the exact same for non-adaptive algorithms, so adaptivity again is in binary. Okay. We can also actually, so this so far I've been saying if you do something for deterministic, I can do something for stochastic for free. But we can also go in the reverse direction. So, there are some upper bounds now for adaptive algorithms in stochastic outcome settings. I can take them and show. I can take them and show new upper terms in deterministic settings. So, in particular, for this whole page ad optimization problem where you want to figure out all the ads, display ads on a particular web page. So, there can be many 5,6 ads, it's a whole configuration. No algorithm is known that beats 0.5 in general. So, we can rule out by using this technique a bunch of natural algorithms which are randomized perturbed 3D algorithms. We can say for sure that they will not beat 0.5. So, we can get upper bounds for detailed. So we can get upper bounds for deterministic problem by using some existing upper bounds for stochastic because of this back and forth equality. But this is a small capacity match. Yes, this is the general case, unit capacity. The whole basic optimization of unit capacity is just a DF problem. Well, it's a match generalization of weighted matching with free disposal, right? So this comes from the name cold page optimization and this comes from the application, that application. It's like as if you have an advertiser that requires one. Have an advertiser that requires one in person? No, no, no. It would still mean that you have these different advertisers. You have one unit of each, and you can display a configuration, a subset of them. That's true, but the advertisers' demand for the impression is much. That's much. Otherwise, you can't get online from the I I'm not disagreeing. Alright, so let me just quickly run through this table. What I hope to just Through this table, what I hope to just bring attention to here is that in many of the special cases, a lot of the results were just for adaptive algorithms. So, wherever you see an A, that means there was no result for a non-adaptive algorithm known. So, now we can get new results for non-adaptive algorithms. And some of the star marked cases here are ones where we actually do better than the best known prior results. There's a lot of work on this, and I know I'm doing injustice to the prior work, so I hope you'll forgive me. Work, so I hope you'll forgive me about it and I can tell you more about it offline. Okay, so in the remaining time, I'll do my best to first tell you about the formulation and then give you a gist of the technique. Alright, so the formulation is pretty natural. We'll now say that each action has a set of potential outcomes. So action A has a set of potential outcomes n of A, and the action will have a single realized outcome from this set N of A. So A might have outcome E with property P E and the sum of all the property. E with probability PE, and the sum of all the probabilities is exactly one. So every action has a set of potential outcomes, only one outcome is realized, and the realizations are independent across actions. So more generally, I have an action set, I have an outcome set, and there is a one-to-one mapping from A to B. It's a random mapping. Let's assume everything is disjoint. And my objective is now on the outcome set, the final outcome. So I have a monotonous subordinate function small f on the set of outcomes. The set of outcomes. Okay, so this naturally induces an objective on the set of actions. Because if I give you a set of actions s, it has some random outcome x. So the value of this set of actions s is just the expected value of its outcomes. Expectation is over the random part of the random. So small f is my objective or set of outcomes. This induces a set function capital F of a set of actions, which is the expected reward. Alright, so what exactly happened? World. Alright, so what exactly happens in the problem? So I just gave you the high-level changes. What happens at an arrival T. So if you are in the adaptive world, at arrival T, I will see the new set of actions AT. My goal is still the same. I want to select one action from AT. But now I also see potential outcomes of AT. That's the set NT, and I see the probabilities. In the adaptive world, I also know what the outcomes of previous actions are. So this E1 through ET minus 1 are all the realized outcomes of prior actions that I've selected. Of prior actions that I've selected. And I want to select one action for AT. Once I select the action, then I'll know the outcome. So the adaptive 3D algorithm will just select the action that maximizes the expected new reward. Expected new reward is if I select an action A, it will have an outcome E with property B E, and then I'll get reward F of E marginal value of E1 through ET minus 1. I want to maximize this. It's adaptive because it knows the outcomes. Once I select the action, I actually see the outcome TT. Actually, I see the outcome TT outcomes are revealed after. Questions about this? In the non-adaptive world, I never see any outcomes. So everything is just an expectation. So I'm really focused on this function capital F. So I select an action AT, I know the probabilities, but I don't really see any of the outcomes. And my reward is just an expectation. Those outcomes are never repeated. So the non-adaptive greedy algorithm will look a lot like the greedy algorithm. I'll just pick the action that maximizes the marginal value. That maximizes the marginal value with respect to capital F. So, small f is the adaptive word, capital F is the expectation non-adaptive word. Alright, so as you might notice, this non-adaptive problem is really quite, it's basically the same as the classic deterministic problem because I don't really see any of the outcomes. So, it's easy to compare non-adaptive greedy with the non-adaptive offline, right? That's the deterministic thing. But we want to compare non-adaptive greedy with adaptive offline, which is the main challenge. Any questions? Any questions? All right, good. So I have enough time to give you the gist of the technique. So, how do we analyze a non-rotative algorithm for a setting with stochastic outcomes? So, let's say that we have some large enough, and I can give you more sense of what I mean by large enough, but let's say we have a large enough set of instances, omega. These are instances with stochastic outcomes. And we have a subset of this, omega t, which are all the instances that have deterministic outcomes. So, now I want to. Outcomes. So now I want to analyze the worst case ratio for a non-oriented algorithm. So give me an arbitrary instance G with stochastic outcomes. I will map it to some instance GD that has deterministic outcomes. And I'll map it in such a way that greedy is full. Non-adaptive 3D on G and Greedy on the deterministic instance do the exact same thing. Greedy doesn't see the difference. But when I look at the adaptive offline and compare it with the offline solution for the new instance GD, the offline solution is actually GD, the offline solution is actually better even though this is a deterministic instance. So I can embed the adaptive offline inside GD such that I have full Greedy. And what does this do? So I want to analyze this ratio. The first property of invariance basically tells us that the numerator is just the value of Greed on Gd. And then the denominator, due to the dominance property, I get that this ratio is lower bounded by a ratio inside of this omega t, which is the simpler set of determinants. Which is a simpler set of deterministic problems. So, if I have a compare ratio for deterministic problems, that gives me a compare ratio for the stochastic set. So, since I'm out of time, the numerical example is something that I can walk you through offline if you are really curious. There's one more thing that I'll just quickly flash. I said that we also want to go beyond submodularity. So, to capture things like reversibility, we can do it with this generalization of submodularity called submodular order functions. Modularity calls a modular order functions. They're at very high level, we have submodularity just in one direction. So if you think of the arrival direction, the order in which elements are revealed, I just impose a kind of submodularity in this very specific direction. And our results for adversarial model go through. For the other models, it turns out that this submodular order property ends up becoming submodularity because you're randomly reshuffling the order. So you're asking for submodularity in every one. Okay. So I. Okay, so I'll summarize. Sorry for getting a little over time. We introduce a new formulation, the online summary of the order welfare problem and stochastic outcomes, and we give a black box technique that lifts results from deterministic settings to the stochastic setting. This shows that in general, adaptivity on the space of polynomial time algorithms does not offer a benefit in compared ratio. There are also some results for non-monotone functions using adaptive algorithms. Okay, so some interesting open questions. Questions. Well, what settings do adaptive algorithms actually outperform? So, very specific setting. Is there an adaptive online algorithm with compare ratio even slightly better than 0.5 for the basic problem of online matching with stochastic? So, people have been looking at special cases of this problem, and I've shown better than 0.5 with adaptive algorithms, but in general, 0.5 remains the state of simple generalization of bi-point matching, where we don't know what we can do with the template. You can also think about non-monotone functions, which I Monotone functions, which I didn't have the chance to touch upon, but are there guarantees with non-adaptive algorithms for non-monotone functions? In the paper, we only show it for adaptive algorithms. Thank you very much. I'll be happy to take questions if you're starting. So, in the interest of time, let's take the questions offline so we can have the coffee break. But we'll resume my time today. Thank you. Thanks for having me. What's your thing? Doesn't it imply that if you have an algorithm for the internet between the time? So when you look at a stochastic image, if I get the objective function and then the expectation of what it is, that is still some of the objective functions. So the results implied that if I non-adaptive situations, the non-adaptive TP is half against the non-adaptive. Half against the non-attractive off point, but the non-adaptive off point is with respect to just the difference. But when you look at the fact that I have this colleague, can be the obvious portal or welfare of the big self-pieces or it can be a special purpose. So you need a sufficiently large special piece because you want the Sufficiently large specialties because you want the instance GD to still be inside online. So, for example, if I take online matching and stochastic rewards and I try to convert an instance of stochastic reports to online matching such that I get those properties, that will not be so. In the paper, I define this resource allocation with vector output, which is inspired by the home page allocation with configuration functions. And if you give me that space of problems, I will show a result and usability, usability, convince an already for that space of problems. I can definitely use it. And I guess that is the equivalent of the version where it's a really model, but I just are not talking about some magic action.  Yeah, it's kind of like it's rather than having one allocation on getting targets. I allocate if I want it's gonna be broken into pieces and then each piece I come back at some point is administration and these times are different. Yeah, basically the world municipality is true. Yeah. So that one will also be somewhat different. Right. And so and is is that somehow like kind of problem that if you Somehow like Terraform managed to also say in the department then you can't get an awkward and put the support. So I I believe so the only thing that makes me say believe instead of yes is that in the paper where we showed the result, we didn't look at configurations, we looked at assortment. So I would just need to modify the approval and make sure that it goes even if I make remote configurations. Oh yeah, there will be a right. So I haven't done that. I also believe it should be not too hard to do it. And then once it's ready, that's going to be hard.