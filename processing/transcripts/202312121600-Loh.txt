All right. So this work is going to be about privacy, differential privacy, which is a topic that I've started working on in the last three or four years. And it's mainly spearheaded by a collaboration with my collaborator, Marco Vela Medina, who's at Columbia. There's a student, Casey Bradshaw, who's finishing up at Columbia, and a former student of mine, Zheng Liu, who was a PhD student at Madison. So, I'm going to start with a few slides just about differential privacy in general. Hopefully, you'll forgive me if you already know this. So, privacy is a property of a randomized algorithm, and the property is differential in the sense that if you apply your algorithm to a different data set that differs in exactly one sample, you don't want the output to be too different. So, usually, the standard definition of differential privacy has two parameters. Differential privacy has two parameters, epsilon and delta. And if epsilon and delta are close to zero, then that says that if you change one data point, then you don't change the output very much. So it's extremely private. And usually people think about ways to make an existing algorithm private. And it generally consists of adding some sort of noise, some kind of perturbation. So one example is to add Gaussian noise to the output of an algorithm. If you haven't seen this before, the most basic If you haven't seen this before, the most basic example is: let's say you want to create a private mean of a data set. And assume that for starters, your data set is bounded. Then if you look at the empirical mean of your data, the sensitivity has to be bounded because each of the data points is itself bounded. And then that tells you that if you add a certain amount of Gaussian noise with variance scaling like the sensitivity, the bound on your data, then the output of this. On your data, then the output of this will be private. Of course, and this comes into play in what I'll discuss in this work. If your data aren't bounded, then you need to do something a little bit more clever to bound your data set or truncate it or something else. And you may also have heard of the Laplacian mechanism. And if you're wondering when do you add one type of noise or the other, a simple way to put it is that originally people were interested in what they call just epsilon. In what they call just epsilon differential privacy, where delta is zero. And in that case, the right noise to add is Laplacian noise. Okay, but actually, so to make things a bit more complicated, although the work that we do could be analyzed under this usual epsilon delta differential privacy, for reasons that are a bit more technical, we'll actually be working in a slightly different framework of privacy. Because you might think, well, it's sort of arbitrary that the amount of change I'm allowed for the output of my algorithm depends in this. Algorithm depends in this exponential multiplicative way on epsilon and additively on delta. So we'll work in the framework of so-called Gaussian differential privacy. And I won't go into this in much detail, but the basic idea is that there's a different way of viewing differential privacy in terms of hypothesis testing. And this is something that was first written about in a paper by Larry Wasserman and Zhu Hangzhou in 2010. And it's not too hard to see that. And it's not too hard to see that if you have a differentially private algorithm, then it basically corresponds to a test. And at a significance level of alpha, the power has to be upper bounded by e to the epsilon times alpha plus delta. And more recently, so yeah, so Dong, Roth, and Su sort of piggybacked on this idea, and they considered other types of trade-off functions that aren't parameter. Off functions that aren't parametrized so much by two parameters, epsilon and delta, but different sorts of parameters. And this leads to a notion of Gaussian differential privacy. And why is it called Gaussian? Well, the relevant part to our talk is that, in fact, if you add Gaussian noise to a function that has the same sort of, think of sensitivity bound as like, you know, you have bounded data and you take an empirical mean, then if you add Gaussian noise, Then if you add Gaussian noise, this time with the variance depending not on epsilon and delta, but on this single parameter mu that tells you how private you want to be, then the output of the algorithm is Gaussian differential private. Okay, so that's the important part that we're going to use as the building block to make things private. And the reason why, or one reason why people prefer to look at Gaussian differential privacy is that it satisfies these tighter composition theorems. So generally in differential privacy, Generally, in differential privacy, you have various little building blocks like adding Gaussian noise to means of things, and you apply them multiple times in an adaptive fashion, and you need your final output to satisfy privacy as well. Now, with the original epsilon delta differential privacy, there are different types of composition theorems that one can prove. But it turns out that you get some very nice tight composition if you look at things from the point of view of Gaussian different. If you look at things from the point of view of Gaussian differential privacy. And as you'll see in a moment, in the work that we do, we have to apply these private algorithms many times in sequence, which is why we look at things from the point of view of Gaussian differential privacy. But okay, so if that went by a little bit fast, then maybe the way to summarize it is we looked at things using Gaussian differential privacy because our algorithms consist of a concatenation of many separately private algorithms. If you wanted to prove that things were epsilon delta differentially private, Prove that things were epsilon delta differentially private, you could also prove it the same way because ultimately we end up adding Gaussian noise on each step. It's just that the accumulation kind of blows up a bit faster. All right, so what is our talk about? Well, I think fundamentally, from so differential privacy comes from the computer science side. And generally, the focus there was on just making sure that the output of an algorithm was private. More recently, there has been an interest in doing There has been an interest in doing parameter estimation in a private sense. So, this is very, very much the sort of framework that a statistician would look at it. The idea is that you have data, and they come from some model that's parametrized by theta naught, and you want to be able to estimate it in some differentially private way. Now, this perspective, I think if you look in the literature, was in fact taken by the computer scientists in 2018, but it was popularized more recently in an annual Popularized more recently in an annals paper by Tony Tai and co-authors. And in this paper, which they called The Cost of Privacy, the question was: in fact, what are the minimax rates of parameter estimation for different problems? And what they found was that, for instance, for mean estimation in P dimensions, the cost of privacy is the second term. So you can't, if you want to find an estimator that's private, you can't. Find an estimator that's private, you can't do better than a non-private estimator. So that's this square root p over n. But then because you impose that the algorithm is also epsilon delta differentially private, then you get an extra term here. And you can see that, you know, if epsilon is treated like a constant, then the p over n term is going to be smaller than the first term, but this is true in general for different values of epsilon as well. So there might be cases in which the cost of privacy dominates, and there might be cases in which the dominates and there might be cases in which the usual parametric rate dominates. Okay, and then the estimators that they looked at were through some sort of gradient descent algorithm, which I'll go through in much more detail, plus a truncation step. Okay, so this work sort of, or this talk sort of spans two different papers. One is in a low-dimensional setting and one is in the high-dimensional setting. And in the low-dimensional setting, actually in both settings, what we do is we sort of Is we sort of look at methods that were useful for robust statistics. And although it might not be obvious at first at all why robustness has anything to do with privacy, it turns out that using methods from robust statistics can actually be quite nice. And I'll try to illustrate here. So I won't say much at all about the background of robust statistics, but the one thing you do need to know is that the idea of M-estimation comes from robust statistics, and the idea is that And the idea is that instead of using a squared loss, minimizing a sum of squared errors, what you do is you minimize some other function that's applied to these errors. And usually the types of losses that are popular are the Huber loss, which is quadratic and then becomes linear, or an absolute value loss, which also sort of grows linearly rather than quadratically, but it has some non-differentiability. And then maybe some non-convex losses, such as the two gel. Some non-convex losses, such as the 2G loss. So, usually in robust statistics, what is done is that if you want to do some parametric estimation problem, then you would minimize the average, the empirical average of some appropriate loss function applied to your data. And the nice property of the loss function is that the derivative is bounded. And this leads, although I won't go into this, this leads to so-called bounded influence of your estimator, which is a good thing. Estimator, which is a good thing. And it turns out that that is one justification for why people use something like a Huber loss, because whereas the derivative of a quadratic grows linearly, the Huber loss is like taking that quadratic and then truncating it so it's actually bounded. So now what's the connection to privacy? Again, maybe not obvious at first. So in differential privacy, there was interest in looking at these sorts of In looking at these sorts of M-estimators. And of course, in computer science, they call it empirical risk minimization. So the idea was: how do you turn empirical risk minimization into something private? And what they wanted to do was, as I said, in privacy, there are often these little building blocks that you put together at different places to make the overall algorithm private. So what was done is on each iteration, in order to make that iterate private, what you do is you add some noise to the gradient, thereby making that gradient. Gradient, thereby making that gradient calculation private, and then you proceed to the next step. And I should also say that in the past, computer scientists were more interested in so-called excess risk bounds rather than bounds in the parameter error. So even though these ideas were sort of floating around out there, it wasn't until much more recently, for instance, in Tony Tai and co-author's work, that people started actually trying to characterize the error of the parameter. But so it's not, and it might look But so it's not, I mean, it might look like it's a simple algorithm, but it's not quite that easy because if you remember, this idea of adding Gaussian noise, it only works if your estimator has bounded sensitivity. And what that corresponds to is, you know, in the context of M estimators, you need the derivative of your loss function to be bounded. And now this starts to look a lot like robust statistics, right? Because what I just said is that in robust statistics, you want to look at loss functions with the derivative bounded. Functions with the derivative bounded. But originally, when people looked at this sort of interit, the idea was that you should instead truncate the loss function. So rather than using a specific loss function with a bounded derivative, in order to make these gradients have bounded derivative, you just have bounded sensitivity. You take the derivative of your, let's say, quadratic loss, you truncate it at some parameter that you need to choose appropriately, and then you add Gaussian noise. And in fact, that's the method that was suggested in. That's the method that was suggested in the Tsai et al. cost of privacy paper. Okay, so having seen, well, having thought about these ideas sort of contemporaneously and seeing this use of noisy gradient descent and truncation, we were coming at this from a robustness perspective. So we said, well, what if we just use a loss function that has a bounded derivative? Like, what kind of mileage can we gain from this? Okay, so in the first paper that I'm talking about here, we looked at this low-dimensional setting. This low-dimensional setting, and we started looking at what are conditions we need for various rates of convergence. So, I mean, outside of privacy, right, in statistics, with the interface with optimization, you might wonder what are some properties of the data set, of the loss function, and so on, so that I can get certain conditions on my overall objective, which gives fast convergence. And actually, this is And actually, this is quite useful from the point of view of privacy because unlike in usual statistics where you can just optimize as long as you want, you can run onto convergence and then say something about the final output. In privacy, it becomes very important to actually tell you how many iterations capital T you need. And why is that? Well, that's because the capital T, it tells you how many times you need to apply this private mechanism. This private mechanism, the Gaussian noise mechanism. Because if you remember from several slides ago, right, the overall privacy, it composes in this way, right? So if overall you want a bound of mu on your privacy, then that means that you need to budget out mu over square root of t for each time you add noise. And in general, making smaller mus means adding more noise, so you get less accuracy. So it becomes very important to actually carefully analyze the optimization trajectories. The optimization trajectories. So, what we did was in our first paper, we sort of looked at first-order algorithms and we also looked at second-order algorithms. And then you might ask, well, I mean, overall, the rates that we get look somewhat similar to the ones from the Ty et al. paper. You probably have forgotten what those rates look like, but let me just remind you. So, in that case, they looked at epsilon delta privacy. You could do the same thing in our case, but you essentially get a square root of p over n in the Get a square root of p over n in the first term, p over n in the second term. And before there was an inverse dependence on epsilon, now it's an inverse dependence on mu. So it's more or less the same thing. But then you might ask, well, then why do you want to do this M estimation strategy instead? And the main answers are twofold. So first of all, instead of having to actually tune this truncation parameter, you can more easily see how it depends on properties of the covariates and the errors. Of the covariates and the errors. So the tuning parameter, the truncation parameter before was the truncation parameter required for bounding these gradients. And now it's just sort of cooked into your problem. You choose your loss function parameter appropriately, like a Huber loss. You choose that parameter. And then on top of that, robust statistics gives us a whole toolkit of various things that we can do if we want to deal with various types of outliers. So, in fact, something else that we So, in fact, something else that we leverage quite heavily when we move to the high-dimensional case is that, in fact, so the previous work, including the work by Tsai et al. and I think almost all of the previous papers also in theoretical computer science, they assumed that their X variables were bounded. And this led to bounded sensitivity on gradients. This might not always be a good assumption. So, in robust statistics, we care about different kinds of outliers. Different kinds of outliers. So the outliers in the X direction are called leverage points. And then they're also outliers in the Y's. And sometimes you care about one or both, or both of them simultaneously, one or the other or both simultaneously. And one idea from robust statistics is that if, oops, what is going on here? Sorry, I think I don't know if that was just on my side or on both. Okay, so in robust statistics, the idea is that you can. Statistics, the idea is that you can add a weight function, and this sort of squashes down any high leverage points. And we can cook this all together. Okay, and another thing we were able to do through our M-estimation procedure was to build asymptotically valid confidence intervals. Okay, so now let me move to the high-dimensional setting. So, in a high-dimensional setting, you know, it's fairly simple to think about what you should probably do, right? You should probably add a penalty, and then you should. Add a penalty, and then you should try to do some of the usual manipulations. Okay, so what we did was we looked at a penalized sort of objective, and we looked at a method of composite gradient descent. So for those of you who, I think many of you are probably familiar with these ideas from high-dimensional statistics. And the idea is, you know, if you have a penalty, well, you like to do gradient descent, but your penalty is not smooth. And so what you can do in And so, what you can do instead is to sort of approximate the smooth part of your curve with a quadratic and then minimize that altogether. So, sorry, is my screen flickering? Or is it fine? Is it just me? Yes. It's fine. Okay, sorry. Yeah, I did an update and there's some error thing that I can't get rid of, but as long as you're all fine. Okay, great. Fine. Okay, great. So, what we want to do in the private setting is we want to do the same sort of thing, but we need to add noise whenever we add these gradients, because these gradients themselves might not be private. So the idea is that we need to understand, well, what's the amount of noise that I need to add in order to overall make things private? And the amount of noise that we add is going to scale like the sensitivity. And the sensitivity. Sensitivity and the sensitivity is related to the gradient of the loss function, right? And so, at a high level, you know, everything proceeds just as before, but of course, the devil is in the details. So, okay, so if we have this upper bound on the gradient, right, this capital B, and this is where we need to start being careful in high dimensions, then it's pretty easy to see that the amount of noise that we should add should scale as capital B. Should scale as capital B. And then we also need to worry about budgeting things with capital T, the number of iterations. So it's kind of just as before in the low-dimensional case. Okay, I'm going to maybe quickly go through sort of the technical details and get to the sort of the hard part of it. So the technical details are that, of course, we need to have some conditions on the loss function that allow this to work. And generally in high dimensions, what it And generally, in high dimensions, what it's called is something like restricted strong convexities, restricted smoothness. And so we need the same sort of stuff to hold here. And of course, we, well, I mean, we also assume some sort of sparsity so that this will work in high dimensions. And the theorem maybe is not too surprising either. So it says, you know, if you have restricted strong convexity and restricted smoothness conditions, and your regularization parameters are chosen appropriately. Parameters are chosen appropriately. Then, as long as you initialize close enough, then this noisy composite gradient descent algorithm should, in fact, converge to a certain radius of the truth. And the sorts of, without really looking at this too carefully, the sorts of expressions that you see in here are what you would expect. So you have like, you know, k log p's, you have your restricted eigenvalue parameters, and so on. And then the overall. And then the overall estimation error will have the usual parametric rate, and then it will have the cost of privacy part. So, just to sort of emphasize this, right, if you put together the private error part and the usual non-private error, then you end up with something like this. Okay, and also I'll comment that the theorems that we found overall just give this sort of local rate of convergence. So, if you are rate of convergence so if you are sufficiently close to to the truth um then you will you will converge um quickly to uh to this parameter theta t okay so there are two comments maybe one i'm not going to talk about too much because it's it's sort of technical right so so there are reasons to think that this first bound that um that we gave is not quite good enough um we don't nobody currently knows what what lower bounds there are so it's not really obvious There are, so it's not really obvious what your lower, what the correct rate should be. But based on our low-dimensional analysis, we sort of highly suspected that the rate that we should get should actually be better. So rather than getting what I called R' on in the previous slide, what we should actually get, or what we think we should actually get, is R prime squared. So we worked really hard to try to get this rate, but then we ended up having some trade-offs that we didn't particularly like. Trade-offs that we didn't particularly like. So, what ends up happening is that, well, I won't go back to the previous slide, but in the previous slide, we needed n to scale like capital B times, actually, let me maybe I will go back to that. So, we need to go like B times square root of K. And over here, we needed to go like B squared times K. And you might think, okay, well, K, you know, I don't really care, root K or K are fine. But the problem, which I'm going to get to in a moment, is that this capital B might actually include some dimension depending on. might actually include some dimension dependent factors, in which case it actually does matter a lot whether it's whether it's capital B or capital B squared. Okay, so let me get to sort of the heart of the problem. So we got these theorems, they're plausible, they seem reasonable, then we try to apply it to an actual setting. So if you start to now apply it to the case of linear regression, then you see that, okay, well, you need this bound capital B to hold on the gradients of the loss. To hold on the gradients of the loss function, you calculate your gradient and then you come to this point. So you calculate your gradient and you realize that even if your loss function has a bounded derivative, that will chop off any deviations in the y direction. But you still have an x popping out. And this is the point. And what you need is this capital B to be the sensitivity bound on your derivatives. So if your x's are bound. So, if your X's are bounded, which I should say was an assumption in basically every paper that we've seen on this topic, then it's fine, right? So, you say, okay, well, I assume a global upper bound and the L2 norm of all of my coverts is fine. But since we were coming at this from a robust statistics perspective, we said, okay, well, that's not good enough for us. We want to be able to deal also with perturbations in your X's, right? Because differential privacy is that you can change your data points in X or in Y to whatever you want. points in X or in Y to whatever you want we want to deal with with unbounded spaces as well okay so so then we we thought all right well um we know how to deal with um with with having with getting away getting away from boundedness and covariates let's let's take a page out of robust statistics and use these weights and I'll show you the objective in a moment and then another thing we did was we said okay well actually in in robust statistics you know you would you would actually be instead of having to sort of Instead of having to sort of make assumptions on this Huber parameter gamma, what you can do is you can, if you don't know the scale of your errors, you can actually do some sort of joint optimization between beta and the standard deviation of your errors. All right, so sort of cutting to the chase, the loss function that we looked at was like this. And if you come at it from a robust statistics perspective, then this is not really anything surprising to look at. This is sort of a loss function you would look at. This is sort of a loss function you would look at when you are trying to deal with possibly unbounded covariates and also not having to assume anything about the scale of your errors. So just to emphasize this, instead of having to actually tune a Huber parameter, you would choose your Huber parameter to be something fixed. And then because you're jointly optimizing over sigma, then you get rid of the difficulty with having to estimate a Huber parameter. Difficulty with having to estimate a Huber parameter because you're jointly learning everything. And then the other great thing on top of this is that it ends up being a convex function. It's convex jointly in beta and sigma. Okay, so everything sounds great. And then you start to choose your weights, right? So in standard robust statistics, the choice of weights, it ends up scaling like one over the norm of the x's. So the idea is that if your x's are sort of too far out, then you should scale them down. Are sort of too far out, then you should scale them down. And usually, p, the dimensionality, is treated as a constant. So you don't really think about that. And you just think, okay, well, I need to scale my weights like one over the norm of x, or a constant over the norm of x. But then you move to high dimensions, and it starts being a little bit problematic. And basically, you know, if you're working in high dimensions, then the norm should be like square root of p. So how you can't really multiply, you can't really just take your weights to be like a constant over root p. It needs to a constant over the norm of x. It needs to be like root p. The norm of x, it needs to be like root p over the norm of x, and this is where things start getting a bit complicated. So, we actually calculate our gradients of the loss function, which we then want to use to bound the parameter capital B. We see that the parameter capital B has a scale at root P. And we were not able to get around this with various sort of rescaling of different aspects of our objective. So, when you sum it all up, right, so yeah, so when you sum it all up, so. Right, so yeah, so when you sum it all up, so we were able to show that restricted strong convexity and restricted smoothness held. Let me sort of skip this part. But at the end of the day, what happens is that because our parameter capital B depends like square root of p, the overall rates that we have look like this. So I should say that, you know, we were hoping for a high dimensional statement. And I guess what we ended up with was sort of a moderate dimensional statement. A moderate dimensional statement. So, in order to make this error completely disappear, then we need n to scale like k times square root of p. Right? So, there are certainly settings in which the assumption on n is that n doesn't necessarily have to be bigger than p, but you know, it's not going like log p either. Okay, and then I guess I'm running out of time, so I think the last part that I want to talk about briefly is that, well, since we had, to be honest, we had in mind, you know, privately. We had in mind private linear regression, but we figured that since we had come up with this framework of M-estimation, then maybe we should use it for other types of estimation problems as well. So we looked also at the problem of mean estimation. And here, you know, you're looking at this multi-dimensional vector with a common mean beta naught. And epsilon i will come from some multivariate scale param scale family. Let me skip that technical detail. So we needed some assumptions on epsilon. We didn't want to just assume sub-got. On epsilon, we didn't want to just assume sub-Gaussianity, so we assumed something a little bit different. And the M-estimator that we looked at was taken again from robust statistics. So there was a method that was proposed by Timothy Mathieu for robust multivariate mean estimation. And the basic idea is that we want to apply some sort of Huber loss to the errors, but Huber loss is a univariate function. So we looked at the L2 error, and then we also did this joint estimation over. The disjoint estimation over the parameter beta and the noise, the noise scale, which we call psi. So then we can again show that RSC and RSM hold. It takes quite a bit of work. Our parameter, our bound capital B, again, is not dimension independent, which we weren't super happy about, but I'm not sure if it's possible to actually get around that, although I don't have a proof of it. And the other thing to highlight is that. And the other thing to highlight is that although our theory pretty much does go through in this setting, we did have to be a bit careful because of non-differentiability of our objective function at certain points. And this is because of the type of loss function we use. Okay, so then we ended up with rates that actually are, I think they look, the way it's stated here, they look exactly the same as the ones for linear regression. So the unfortunate part is that, you know, and again, Is that n again has to scale something like to make this, I think earlier I said that n did not have to scale as p, but according to this bound here, so n does actually have to scale on the order of p. And but yeah, so this is the assumption that we need on sample complexity. Again, I should say that as far as I know, there are very few other people who have. There are very few other people who have looked at the setting of high-dimensional linear regression or mean regression, sorry, linear regression or mean estimation, assuming that the x's don't have a bound on them. So it's unclear whether or not it is actually possible to do better than this rate. Okay, I'll skip over the details. I mean, for high-dimensional people, it's sort of it's not too hard to see what the general Not too hard to see what the general argument should go through. Basically, you look at analysis for in high-dimensional statistics for analyzing the trajectories of gradient descent, sorry, composite gradient descent. There's some noise, so you need to worry about controlling the amount of noise. Maybe the one trick, though, is that in order to go from slow rates to fast rates, there's something else we need to do. And the argument ended up being fairly technical. Argument ended up being fairly technical. So you sort of look at the iterates in chunks, and once you get close enough, you can actually show that the support of your later iterates is order of K, where K is the true sparsity. And that then allows you to bound things more tightly than the overall upper bound on the error, which we use for the initial set of iterates. Okay, so let me wrap up. So there's sort of two parts to this. So the low-dimensional work I talked about lightly, that's the complete. About lightly, that's the complete completed work where we had a first look at private M-estimation with noisy optimization in high dimensions. And this is the work that is sort of still in the final stages of being wrapped up. So we looked at noisy composite gradient descent, and we studied the consequences of our theory in the case of linear regression and mean estimation. And I didn't have time to talk about the asymptotically valid confidence intervals, but basically it's sort of some variant of Lasso de. Of some variant of Lasso debiasing applied to these noisy estimators. In terms of future work, so we sort of gave up after the local analysis. We haven't yet figured out how to attack the global case. So global convergence means that you initialize from anywhere and you want to say how the initial iterates behave before you get to the basin of attraction. Lower bounds are still sort of wide open. And also, since in the low-dimensional case, we looked at In the low-dimensional case, we looked at cases of Newton's method. We'd also be very interested in studying second-order optimization algorithms in the high-dimensional case, with the caveat that things have to be also computationally tractable. So that's it.