Place with this beautiful workshop, with this fascinating talks. Now it's a little bit less fascinating because I need to apologize that it seems that the jet lag kicked in, and I'm in a half zombie mode. So let's see where we go. Okay, so what I would like to talk about is actually a transversality method with iterated function systems. So what's the setup we have? So let us have a finite collection of iterated function systems, a finite collection. Derived functions is a finite collection of contractions on R D, which we usually call an IFS. And then there is this well-known, unique, non-empty compact set, which is called the attractor of this iterated function system, and it's invariant with respect to these maps in this sense. Okay, so what can we do with such kind of object? This guy can be, every element in the attractor can be associated by an infinite sequence of symbols. By an infinite sequence of symbols formed by as many symbols, as many functions you had. So I have here n many functions, so I define a symbolic space which is formed by all infinite sequences by this finite set of symbols. And this is a very, very nice space. It's equipped with the left shift operator, which is a beautiful, well-understood dynamics. There are plenty of ergodic measures on them. And what we can do. And what we can do is that we can define a projection from this shift space, the symbolic space, to my attractor that for an infinite sequence I concatenate, I iterate my functions in the given order what I is, and then this is the projection. So every element on my attractor has such kind of coding, but this coding is not necessarily unique. And what do we do? Well, we grab a left-shift inverse. Grab a left-shift invariant every probability measure and push it forward via this natural projection. And we are asking that what's the dimension of such kind of measure? Well, which in which sense the Hausdorff dimension and in the upper packing dimension. If you don't know what these are, I'm pretty sure that you do. But here is some. Recall what are the upper packing dimension and the lower house door dimension of a mesh. Lower Hausdorff dimension of a measure is. So, if you want to consider general contractions, then the life is pretty hard. So, let's make our life, which is already hard enough, a little bit easier by considering conformal networks. So, from now on, whatever I will say, I will assume that my maps in my iterated function systems are contracting C1 plus R for maps. Okay? Alpha max. Okay? And in this case, Feng and Hu showed that no matter what, which ergonomic measure you consider, then these two quantities, the lower Hausdorff and the upper Paking dimension, will coincide. What does it in particular mean? This means that this quantity, which is here the lower local dimension and this one is the upper local dimension, so these quantities exist as a limit and it's an almost surely constant function. Almost surely constant function. This is what Feng and Hu proved, but it was the upper bound that you can have a natural upper bound, which is the minimum of the dimension of your state space here in Rd, and the ratio of the entropy over Yakonov exponent, of course, was known much earlier. This is always an upper bound. Here is my entropy, and here is my Yakunov exponent, which is the average contraction at a typical point. Okay, when do Okay, when do we know that there is actually an equality here? Well, if there are no overlaps in the business, for example, if you assume that the strong open set condition holds, so you can find some nice U-bounded open set who is mapped into itself by all the maps, the images of this open set are disjoint. Then you know that actually for every ergodic measure, For every ergodic measure, the dimension, and from now on, I ignore the other notation, so I always use this Hausdorff dimension. It's nothing else than the entropy over the upon the next point. So if you have no overlaps, then the life is nice. So the question is, what can you say in overlapping cases? Again, let us consider an even more special system, namely then mine maps. Namely, then my maps are similarities, and now I restrict myself to the real line. So all my maps are similarity transformations. Okay, so what do we know in this case? So Mike Hochmann introduced the exponential separation condition, which basically means that in some, okay, whatever this very disgusting formula is, this means that there is some nice matrix. Is some nice metric on the word of the similarities, and if with respect to this nice metric, the iterates of these maths in the nth iterate cannot converge together, so cannot go together on some certain scales faster than exponential. The life is nice, and what you know that for every Bernoulli measure, no matter what, its push forward is the minimum of one and the entropy over the other. Then, publish measure. Then Pablo Schmerkin did a step forward. Pablo Schmerkin did a step forward, and he also showed under the same condition that the RQ dimension also does not drop with respect to its natural upper bound. So this TQ over Q minus 1, where TQ can be calculated by this formula. This is the LQ dimension. Ignore this. If you don't know what it is, but I'm pretty sure that you. You don't know what it is, but I'm pretty sure that you do. And TQ itself is called the air tube spectrum. Okay, and then relying on Pablo's result, Thomas Jordan and Ariel Rapapoch showed that again, that no matter what, if you have exponential separation condition, then again, the push forward of every ergonomic measure is nothing else than the entropy over Yakunov exponent, minimum of one, of course. Okay, what can we say in a slightly more general case? Well, we can consider another special family, namely rational maps. And let's just assume that these rational maps are dice, so you can find some compact interval. These functions are mapping this compact interval into itself. There, these functions are C1 plus alpha. There, so it avoids the singularity. Also, they are strictly. So they are strictly contracting. And Hochmann and Solmia generalize for this case the meaning of the exponential separation condition. And again, if this holds, then for every bare Nouley measure, the dimension of the push-forward does not draw. And again, we are in the world of Bernoulli measures. So as far as I know, in this situation, the LQ dimension and the dimension of every analytic measure. And the dimension of every aerobic measure is still open under this exponential separation. Is it? Yes. Yes, exactly. Okay, so then, but what is the good part about the similarities and these rational maps? Basically, the semi-group has quite a nice, reasonably friendly structure compared to general C1 plus optones. So, if you want to say something about general So, if you want to say something about general single-class alpha maps, it's a natural idea to instead of studying one particular IFS, you study a whole parameterized family of iterative function systems. So, what we do is that we define a parameter space, it's a relatively nice parameter space, locally compact, separable, actively the portable, probability measure, blah blah blah. Okay, anything. Okay, anything. But what you do is, then you define a parametrized family using u of conformal mappings in such a way that each of these maps are depending on the parameter continuously in the sense that from u to the c1 plus alpha r, okay, with the natural metric of that. Okay, and you have this continuity, and of course, then And of course, then your natural projection will again depend on your parameter. But you need also to assume that a transversality condition works. What is the transversality? What do we like? We like if there are no overlaps. What does this mean? If I have two infinite sequences, i and j, whose immediately the first symbols are different, then they should be false. Then they should be far away on the space on the other option, right? Now, the transference that we condition in this sense says that the set of those parameters where these guys are close to each other, closer than they should be, is small in the sense of measures. So they are at most R, and the measure is at most R D. You can recall that we are in R D. In this situation, Shinon, Solomiak, and Shimon, Soloviak, and Urbansky showed that for every ergodic measure, your almost for your almost every parameter choice, okay, this is just inky ignored that, I didn't draw that detail, okay, then for almost every parameter, the dimension of your measure does not draw entropy over Japanese state. And now you see the difference between this theorem and the previous ones. The previous ones giving a condition and saying, Giving a condition and saying every algorithmic measure. Here, a priori, your full measure set on U might depend on your algorithmic measurement. So you don't have this simultaneous statement. And this is what we would like to do, having a simultaneous statement, namely having a common full measure set here for each every evaluative measure grade. Reach every erudite measurement. Okay, but we also want to include not only conformal systems, but very wider family of systems which are mimicking to be a conformal IFS. What does this mean? So, what we introduce is a kind of generalized projections. So, what do we have? Projections. So, what do we have? Still the same nice parameter family. And then what we do is that we introduce a map, a rho lambda, a parameterized terminal of maps, from the finite length words to the positive reals. And feel free to think about this, that this rho of lambda is nothing else than a parametrized family of ultrametrics on my symbolic space. And what Space. And what do I claim? Well, rho lambda of i for a finite word, this shrinks uniformly exponentially with respect to the length of my finite word. I also require that there is some quasi-multiplicativity property of rho lambda in ij in the concatenation. Why do we need that? This is the property which mimics that I'm conformal. This is the property which says that, okay, my system says so. Says that, okay, my system sensor is more or less conformed. And what we also need is that this ultrametric family is depending in some nice sense continuously on my parameters. Okay, this is one thing, what we need. The other thing is a transversality condition. So, in this sense, we have now a general still-code natural projection map, a parametric family of maps, from my symbol. Of maps from my symbolic space to Rd in such a way that it's lip sheets with respect to my woodharmetric. But also, if I see two words who, okay, these kind of the images of these infinite sequences can be at most that close and they will be at most that close. But if there wants to be closer, then its measure is against U. This is what generalized projection scheme turns result. Let me know. Let me note that this generalized projection scheme is a little bit different than the earlier one which was defined by Borisomia, also for the generalized projection scheme. Okay, so now we live with that, but the statement is, okay, examples. What kind of examples do we have? Everything what we want. For example, self-conformal systems. Self-conformal systems. So, whatever Schimon, Solomiak, and Urbanski were considering fits into this term. Okay, with the nice ultra-metric, which is just the derivative of 1x. But also, non-autonomous systems are fitting in the business. So, for example, if you consider instead of the usual Bernoulli convolution, some kind of modification of that, this also satisfies this kind of generalized projection scheme. This was shown where. Projection scheme. This was shown very recently by Nakaji. And also, it satisfies the statistical similar sets, fits into the business. So what are the self-similar sets? Of course, if you have self-similar IFS, then you can write your natural projection as an infinite series without that x. What is the statistical simula? For each finite word, for each iterate, you Word for each iterate, you introduce an IID error term on a compact set, tiny, big, but from a compact IID, and you add it to each iterate, independently. And these guys are random, the image of the symbolic space under these maps is a random set, which is statistically similar. This one also fits into the business. This system is considered, for example, by Jordan polycottage. And the state. And the statement is the following: under these assumptions, whatever we have before, almost every choice of parameters, every ergonomic measure has no dimension drop. It is the dimension of the push-forward is the entropy over the Yapunov exponent. What is the Yapunov exponent? So, again, thanks to the good properties of this row, this mutametric, you can define something. Can define something which can play the role of the Yakundax. Okay, so basically, that's it. Now, it might be the time that I will tell you some details about the proof, but I won't. Instead, I would like to introduce another concept, which is really called exponential distance from the enemy. Oh, yeah, there's one thing that, of course, Oh, yeah. There is one thing that, of course, under these assumptions and under everything, you can also calculate what is the dimension of the push-forward of the whole symbolic space, right? The usual quantity, the matrix per bound, what you would guess, name. And now I would like to introduce this other concept, exponential distance from the enemy. What does this mean? Who are our enemies? Our enemies are the overs. These are the overs. So, a guy, an eye, an infinite sequence, has this exponential separation from the enemy if that guy, this point here, has distance from every other n-length cylinder who is not the first n symbol of i, so just someone who is different. This distance is bigger than rho lambda. Than rho lambda of i n. Of course, this distance is at most that, approximately up to a constant, but this must be greater than e to the power minus epsilon n for every epsilon. Okay, so asymptotically, you can fit here every epsilon. Why is it a nice concept? Well, first of all, we can say something about that. Namely, again, we can have some uniformity here. some uniformity here. Namely, if you assume that the upper bound, if you assume that the upper bound, the natural upper bound on your dimension of your set is at most V, where we are living, then for every choice, for a typical choice of parameters, then every ergodic measure, every point typically to this ergodic measure. To this ergodic measure has an exponential distance from the end. It means two things at least. One thing is that this natural projection becomes invertible if you restrict it to a full measure set with respect to my ergodic measure. And not only invertible, its inverse V further continuous. That's one nice property. That's one nice property. The other nice property is that you can also calculate the local dimension of measures with respect to other measures. And again, simultaneously. What does this mean? Again, for a typical choice of parameters, whatever quasi-Bernoulli algorithm measure you take, if you don't know what's quasi-Bernoulli, ignore it, some nice family of algorithmic measures. And for every other algorithmic measure, if you look at the measure, Every other energy measure, if you look at the local dimension of with respect to mu typical point, is the relative entropy of with respect to mu over the Yapunov exponent with respect to mu. Well, a positive energy is needed to ensure that this guy exists positive. Okay? Why is this property interesting? Because it has also some nice application for the modifier. For the multifractal analysis. If you're also asking why is the multifractal analysis interesting, I really cannot help you. So, what does the multifractal analysis asking? You have a measure, whatever measure in Rd, then describe the map, which asking that for a given alpha, how large is the set of those points in the sense of Hausdorff dimension, where the local dimension is equals to this given. Is equals to this given value of. So again, if you don't have overlaps, so we have strong open set condition, then Arbacher and Pachke determine equal. So what happens, this is this DQ, what we have already seen, the LQ spectrum. The Hausdorff dimension of those points for a band who we measure, where the local dimension is alpha, is given by Is given by the Legend transform of this aircraft spectrum. Okay, so this guy is nice, analytic, very has nine nice properties. Here, what you can see is a typical picture of that. There are two notable points here for alpha. One is the entropy over the eponymous exponent. The entropy of the eponymous exponent here is spectrum, so the graph of this guy in alpha is tangential. Alpha is tangential to the identity. Okay? Another notable point is where the maximum is achieved. The maximum is always achieved with respect to the measure who maximizes the dimension of myself simulation. Okay? And the endpoints, the endpoint might have zero dimension or positive, whatever. Okay? So, what can we say again under in the Under in the case of overnights. And there is a result by Barr and Fenn, again going back to similar systems on the line, that if you have this exponential separation condition and you also assume that the Hausdorff dimension of your self-similar set is strictly smaller than one, then you can have this smooth diffraction spectrum, but only Spectrum, but only in the increasing part. Only in the increasing part. Why? Because what are the ingredients for this multifractal image? Well, the upper bound, which is given by Pablo, which is the accurate dimension of my Ber Mouli measure, mu, and also the accurate dimension of my dimension maximizing measure. This gives you the upper bound. And for the lower bound, you just guess from this formula and basically from this. From this formula, that who is the measure where the largest dimension wants to be achieved. And since the accu-dimension behaves nicely, we have a so-called asymptotic weak separation condition, which says that basically you can understand in this increasing regime the local dimension by just looking at your main cylinder, your main cylinder, which should be there. But if you're willing to sacrifice If you're willing to sacrifice the exponential separation condition for a Lebactypical choice of translations, then you can have by the application of the exponential distance of the enemy to the full spectrum. Okay? And that's all what I wanted to tell you. Thank you very much for your attention. So, do you know anything about the dimension of the set, the set of exponential distance to the MME for typical lambda? For typical lambda, the size? No. No, you haven't looked at it. Whether it is zero-dimensional or is that too much to expect? I think that's too much because you really need that here we have ergodic measures. So, most of the points are very, very irregular, they have full dimension. So, we really need to have the direction. Full dimension. So we really need that we are working with ergodic measures because the trick is that we approximate these ergodic measures with k-step Markov measures in a reasonable good sense with using entropy. And if you are in the general scheme, that we are no idea. More questions? Modern assignments we have. 