And today, I'm going to talk about my recent research on non-asym toilet theory to for two-layer neural networks. And the subtitle is Beyond the Bias Vienna Trade-Off. So this is joint work with my previous PhD student, Hui Yung Wang. He is now a postdoc at UPenn. So let's start with some introductions. Let's start with some introduction to the background and the motivation of the study. So, we all know that in recent years, deep learning has achieved great success, especially in industry. So, we have seen a lot of applications and successful stories in industry, including especially those in computer vision, like face recognition and large language models recently. But some researchers, many researchers have questioned the reliability of the deep learning methods currently used, heavily used. And some posed the concern that we need to develop some more theory for the deep learning methodology. So, what do we want for such a For such a theory for deep learning. So, in statistics, usually we are interested in deriving some statistical guarantees for neural networks. So, for some statistical methods, especially those non-parametric methods, we want to derive some generalization bounds. So, here, generalization error is also known as prediction or out-of-sample or test error. Or test error. So there are several different names for this. But generally, we mean we need to measure the accuracy of the algorithm that the algorithm predicts an outcome on previous unseen data. So we train the algorithm on some training sets, and then we want to test its performance on new data. So this is called a generation error. So we all know that the machine. So, we all know that in machine learning, we generally decompose this generation error into several parts. The three parts of this decomposition, the first one is approximation error. This is also known as the bias. And the second is estimation error, which is also known as variance. And also, we want to consider optimization error. So, the first part, first component. The first part, first component approximation error is usually a mathematical question. We need to, so this is in this part, there is no statistical error. So generally, some many researchers in computational mathematics are interested in this part, deriving some approximation bounds for best approximating some function within some class of functions. Class of functions. And we statisticians usually concern with the estimation error. And finally, we want to combine these three parts into a generation bound. So the picture is a well-known one from the book, Elements of Statistical Learning. So this is the composition of several components of generation error. So now let's recall some. Now, let's recall some important features for deep neural networks. So, this is an example of deep neural networks. This is the one with the simplest structure. So, just the fully connected multi-layer perceptron. So, some of the important features of neural networks include: first, it has a compositional structure. So, the input So, the input goes through the network from the first layer, input layer, to the hidden layers, then to the output layer. So, each layer has a composition to apply composition to the previous layers output. And each unit has an activation function. And this activation function is usually non-linear. And the most popular one is the value, the so-called radio. Is the radio, the so-called radio function, which is just the rectify the linear unit. So this is just like take the maximum of x and zero. Here I show you some deep neural networks. But however, in this work, I only consider two-layer neural networks because this is the simplest one. I need to first make sure that I understand two-layer neural networks. understand two-layer new networks then try to extend the result to deeper ones so um so uh we may be interested in the question so deep neural networks can be seen as just another non-parametric approximator a non-parametric method has been we have been we have seen many uh non-parametric method in statistics including uh those uh fully non-parametric method like uh Fully non-prime method, like nearest neighbor and additive models, single-index models, multi-index models, and so many non-permission models and methods in statistics. But what are the difference, the main difference between neural networks and those classical non-parametric methods? So, we want to know why DNs are better. Are better has better performance than those classical ones. So we want to show in theory how this is possible and how this is achieved. So let's now write down the formal description of the two-layer radio networks. So because we only consider two-layer neural networks, so it's simple to write down as a precise form. So this g function of the X is This is our g function. So x is the input. Okay, theta is a parameter. And now we have m units, m hidden units, and each unit has weight vk and a bias vk. And go through the activation function sigma. Here, sigma is the radio function. That is the take the maximum of z and zero. And each unit has a weight ak. Then we sum up those output from. Those output from the M's unit. Okay, so now the unknown parameter theta includes A1 through AM, V1 through AVM, and also the bias B1 through BM. Okay, so why is the theory a non-trivial? So this is, although this is not so complex, but we have lots of difficulties in this simple form. The first one is that we do not have the Do not have the identifiability of the parameters. So, this is simple to observe. Okay, so for example, if we multiply VK by some constant, then divide AK by some constant, then we will assume that we have the same output. So, there are lots of unidentifiability, and this is a simple form. So, this is the first. This is a simple form. So, this is the first difficulty. And also, non-convexity. So, if we try to maximize some loss function for this neural networks, then the resulting optimization problem will be non-convex. So, if the problem is non-convex, what are we going to talk about the global optimum? The global optimum or the local optimum, if the local optimum, then which optimum? So, this is a question, and also the over-parametration problem. So, here we know that in logic models, like a large language models, usually the parameters can be very many. So, can be larger than the number of samples. Number of samples. So sometimes this overparameterization has a conflict with the classical machine learning theory, especially the bias-variant trade-off, because we know that to achieve best performance, we need to balance the bias and the variance. But some practical results show that, suggest that. Show that suggests that some over-optimization may not be, may not be, may be also good. So do we have a theory to explain this phenomenon? So this is also the question we want to answer this work. There are some related work, but because of time constraints, I will not go. I will not go through the details of this related work. Basically, there are some asymptotic studies for two-layer neural networks, especially mean field approximation and the neurotension kernel. But the point of the major limitations of this theory is that this theory cannot distinguish in general, in my opinion. In my opinion, do not distinguish neural networks from some classical methods. And also, some theories do not lead to immuted applicable generation bounds. So for example, in mean field approximation theory, you derive, you obtain some PDE as a limiting, describing the limiting behavior of neural networks, but it does not immediately. But it does not immediately lead to a usable generation bound. So we want to directly answer this question: what is the generation bound for neural networks and how is it different from those for classical methods? There are also some L2 risk bounds derived in the literature for two-layer neural networks, usually with some explicit regularization. I list three of this here. Three of this here. And the first one is a very classical one. It has this decomposition. So the first one, the first term is the approximation error. And the second term is the estimation error. And because M, the width of the neural networks appears in the denominator of the first term and the numerator of the second term. So it shows a classical bias event trade-off. So this is no surprise. This is not no surprise. And also, the recent work by Paheer and the Norweg also show a similar result, but with improved rates. So, this is by matching the bias in various terms, you get the optimal bounds. So, this is the optimal bound. And they also show that this bounds, I cannot use this pointer. This one. This bound is minimax, optimal within some certain function class. Function class. So, this is no surprise, as it's very similar to some typical result in non-parametric estimation. But this one is non-classical. This one, this result is non-classical. So, here, this work actually derives a bound like this. So, the width of the network appears in the first numerator of the first term. Okay, this is as usual. But the second term, this But the second term, this is supposed to be the variance term, but M does not appear in the second term. So this shows that if the width, network width goes to infinity, then we get the optimal bounds, right? So this has some explains some benefit of over parametrization. We can increase this M and get optimal generation error. So here there is no trade-off. Here, there is no trade-off. So, this surprised me at that time when I read this, see this result. But this work actually do not work on, do not impose unusual assumptions like we impose in statistics. So we want to try to investigate in depth this phenomenon. So, of course, there are also some recent results on deeper new. Recent results on deeper neural networks. So, because we are only concerned with two-layer neural networks, so I'm not going to talk about these details. So, this is a well-known picture showing the double descent phenomenon. So, double descent phenomenon is very closely related to the so-called bias band trade-off. Okay, so as you can see from this picture, the left side is the usual classical bias band trade-off. Classical bias event trade-off, okay, curve. Okay, so the risk curve first goes down and then rise. Okay, so this is the first valley and the first peak. And then, but in practice, some researchers have observed the phenomenon when the complexity of the model here, the complexity of model is just the network width, the number of parameters. As the As the complexity increase continues to increase, then we will observe the second descent of the risk curve. So, this is called the double descent phenomenon. So, this was first suggested in Belkin et al. in 2019. And there has been several work trying to explain this phenomenon and proposing some theory for explaining this. But in this work, we are going to In this work, we are going to answer two questions related to this double descent phenomenon. So, the first question is: How does the network perform in the over-parametrized regime? That is the right side, the over-parametrized regime, differently from in the under-parametric regime. So, what are the differences? And the second question is: how does the over-parameter minimum risk? So, that is the risk achieved at the infinimum. Infinimum, infinity. Okay, so overparameterize minimum risk compared with underparmature counterpart, and how far is it from optimal? So we want to compare these two values. So which one is better? So if the infinity one is better, over-aparnetized one is better, then this shows some benefit of over-aparnitized neural networks. That's consistent with. Uh, that's consistent with the practical observation. So, to summarize, I want to in this work, we want to develop a translation theory for two-layer radio networks with explicit regularization. So, in this work, we have to impose some implicit regularization. But this regularization does not require sparsity. If we require sparsity, we all know that sparsity is a very We all know that sposity is a very important regulation method in statistics, especially in high dimensions, because even the ambient dimension is high. If you have a sposity, then you can reduce the dimension of parameters down to under the smaller than the sample size, then there is no problem. This is the familiar framework. We have done lots of work in this line. But in practice, especially in deep learning. Practice, especially in deep learning practice, actually, we are using very large models, and those models have lots of parameters that does not have sparsity. So, we do not want to impose a regularization that leads to sparsity. So, we have to impose some regularization, but this regulation is not a sparsity-inducing regularization. And also, our work is algorithm independent. And also, our work is algorithm independent. So, this is a downside of our work. So, actually, we are going to extend our work to algorithm dependence methods. But for now, let's simplify the setting. And we are only concerned with the algorithm independent theory. So, we are going to talk about regularized optimization problem and talk about its global solution. We want to derive some non-asymptotic bound for any finite n and m. So, this is a non-asymptotex, it's intrinsically non-asymptotic. And also, we want to find some minimax lower bound to match our upper bound. And finally, we'll compare our result with the random feature model and show that random feature model in some sense will be suboptimal and suffer from the curse of dimensionality. Okay, so let's. Okay, so let's first define our function class. Okay, so actually, this is the first key point we need to consider relevant but wide enough function class for this neural network approximation problem. So as you can see, this class of functions mimics the limits of two-layer neural networks. So this is the why do we have the integration here? Why do we have the integration here? Because it can be viewed as the infinite limit of the approximation limit of two-layer neural networks. So if we let M go to infinity, then we have this integration. So you sum up lots of infinitely many units, then finally you get the integration respect to this measure alpha. That measure alpha. And we also need to define a nuan for this function class. So this is our data generation model. So this is just typical as typical in the non-parametric statistics. So our output yi equals f star, the true and null function, and xi is the input and the error. And the error epsilon i. So we impose some assumptions for this model. So, first, this fstart unknown function, the target unknown function, must belong to the class, function class we previously defined, and with its norm, S norm, the so-called S norm is less than or equal to M for some constant M. And then we assume. And then we assume that this xi follows from some distribution mu independently, and the mu is supported in this unit ball. And epsilon i is Gaussian, okay, independently and independent of xi. Okay, so we are going. The first step is to impose some regularization. So, what kind of regulation do we want to use? So, I mentioned use. So I mentioned we do not need to, we do not want to impose sposity. So we are going to use this one. Okay, so we call this a scale variation regularizer. So this is just a weighted sum of the weight. Okay, so the weight of WK is the first layer weights. Okay, so we take the second one of the weights of the case. The weights of the case units, then weighted by AK, and then sum up. And then we impose this regularizer to the empirical risk minimization problem. So we use the L2 loss, the usual L2 loss, okay, the first term as the usual L2 loss, then plus this regularization parameter, lambda, times our Lambda times our scale variation penalty. So I will later talk about, you may be asking why do we need to consider this form of penalty. So I will mention this later. But first, I mentioned briefly what approximation bounds is available for this problem. So this classical result from Bach results. Results. Okay, so this is the improved result from Barron's more classical result. So this is actually the best known approximation result for this class. I will just use this one. Actually, our function class is slightly different from that considered by previous researchers, but Researchers, but basically, this with some slight modification, this bound is still usable. So I will answer the question, why do we consider this regulation penalty? So the point is that the scale variation regulation penalty is equivalent in some sense to reach. In some sense, to reach regression, the rich penalty, L2 penalty. So by a reparametrization, by reparametrization, we can write. So basically, if we, because AK is time is multiplied by WK. So if we multiply the AK and the WK, multiply AK by a constant, divide the WK by the corresponding constant, then we can match their magnitudes. Then we can write the Magnitudes, then we can write this in this form. So under reparametrization, this becomes the ridge regression. So by this observation, we can prove that some equivalence between our regularized problem solution to the ridge regression. Okay. So this is described by this proposition. And also in alphabetism. In alphorismic perspective, we also can show that when we initialize this parameter at certain points, at the same points, and follow the gradient descent algorithm, then the trajectory of the two gradient flows coincides. So, because of the equivalence to ridge regression, we can be assured that the sparsity is not here. So, there's no sposity in this regularized. No sparsity in this regularized problem. But if we do not have sparsity, then how do we control the complexity? The control complexity of the model. So fortunately, we can also prove another connection to group LASO. So this is the most important part of our theory. So this is the important observation. Because the basic structure of the two layers. The basic structure of the two-layer neural networks is like this. The first layer actually is just a weighting. So VK is the weight of the case unit. And then, so this is the weighting and plus bias term. Okay. So because our activation function is non-linear, but it's just a rectify. Just rectify the linear unit. So if we can determine the sign of xi transpose v plus v, then we can make it into a linear unit. Because it only depends on the sign of the weighted input. So the key point is to determine how many to divide the whole parameter space in. The whole parameter space into many regions such on which the sign of this weighted input is a constant. So how many different regions do we divide the parameters space into? So there is a very classical result from the 1960s. So here is the result. The n hypersplanes The n hypersplanes, this one, divides the parameter space into finitely many regions, R1 through RP, so that this sign stays constant over each region. The number of these regions have this bound. So this bound, actually, you have us. So this one is a sharp actually, the first upper bound shop when X has a rank. So I think this. Um so I think this is uh the most important results. So why do we uh how do we achieve the complexity control for this uh for this uh model if the number of parameters can go to infinity? Um this is the this is for how the consideration of the the weighted input, but still we have AK beside each before each unit. Okay, so taking into account Units. So taking into account the sign of A, we can partition the parameter space into two P regions. Okay, so AK can be positive or negative. And another important observation is that the linearity of value over each Rj and optimity of theta hat entails that we have some collinearity within the same region. So we can show that if the weights of two units The weights of two units, two neurons fall in the same region, the cone QJ we previously defined, then they must be parallel. They must be parallel. So within each region, we can combine those parameters. Although, so essentially, if we you can think about this, although we can have infinitely many parameters, the number of parameters can go to infinity, but we only have finite. We only have finitely many regions. And within each region, if we have the optimal solution, then this optimal solution, the parameters must be parallel or collinear within the same region. So actually, this is automatic combination of those redundant parameters. So this shows why over-parametric is not harmful because there has some intrinsic mechanism that's Intrinsic mechanism that automatically combines those similar parameters that fall into the same region. If we impose the connection, then it should be an optimal solution. So using this connection, we can show that we can reformulate our regulation problem into a form of a good lasso. Okay, so within each region, because those parameters must be Those parameters must be parallel. So we can combine them into a group. So we can write a problem into a group, less or regularized problem. So this is a familiar group lasso. Using this idea, we can finally show generation bound for derived generation bound for our neural network. Okay, so of course we have several detailed results, but this is one is most Of detailed results, but this is one is the most important one. So, the first term is the approximation bias, approximation error, and the second term is the estimation error. So, here, this is a little complex because we have two terms and have to take the minimum. So, let me show you some picture. Okay, so this one is the most important result from our work. So, here, so you can see the double descent curve. Okay, so actually, if you Actually, if you consider the generalization, so this one, the red line, the red line is the approximation error. So the first one, first term, okay. The first term continue to decrease as M goes to infinity. So the approximation error always decreases. This is reasonable. And the estimation error, the red line, the red dashed line. The red dashed line actually goes up. Okay, so first increase, okay, first increase, and then until it reaches some point, this is a point that we achieved the maximum complexity. So because there is an upper limit for the complexity that is related to the number of regions I previously described. So at this point, achieve maximum complexity, then stay constant. Then stay constant even if m can continue to increase. So if we add up this two line, then we get this double descent curve. So this is a very simple mechanism, but not a previous observed. So actually, the right picture is the same as the left one, but I just draw this picture for larger M. You can see more clearly the You can see more clearly the double descent curve. Okay. And you can see actually, because in this picture, it's hard to compare this value and this value because it does not, it can extend to the infinity. So from this picture, you can see the second value can go lower than the first one. So this show over permanent neural network can be, maybe, not always, but in some times, under some conditions, can be better than the Can be better than the under-parametrized one. Okay, so actually, we derise a condition for under what condition the second value is lower than the first one. Actually, we have an explicit condition for this. And I will also prove some because time constraint, I will just skip this. And we also approve some minimax lower bound to explain why this is almost the best possible. And finally, we come. And finally, we compare this result with random feature model. So, random feature model actually is similar in some sense equivalent to kernel methods. So, NTK, neurotangent kernel type results, always sometimes compare the performance two-layer neural network, infinitely wide neural networks with random feature models. So, this is a benchmark. Benchmark for examining the performance of two layer neural networks. So we show actually random feature model still suffer from the cursor dimensionality and has a substantial performance gap from our two-layer neural networks. So, this is the explicit lower bound. So, it showed that because the dimensionality appears here. Because the dimensionality appears here, so it's still sucker from the optimal dimensionality, and it's a suboptimal over our defined function class. So to summarize, here I present some results. And from this results, we actually build up a theory for explaining the double descent phenomenon. In my opinion, the In my opinion, the most important advantage of our theory is that we do not build a completely new theory. Actually, it is consistent with the classical bias event trade-off. So, because in classical bias event trade-off, we only require that the bias, the approximation error, always decreases, and the estimation error should be increasing. But so, in our But so in our picture, actually, the estimation, the variance actually first go down and then go up, then stay constant. So in this sense, okay, so in some sense, this is compatible with the classical theory, does not violate the bias event trade-off. Okay, so this is a recent study by Shimi Heber and his students on bias event trade-off. So in this sense, So, in this sense, our theory does not violate the classical vice-event trade-off. Just it can be seen as an extension to the overpuncture regime. Of course, we have done only this theory on two-layer neural networks. So the next step will be try to extend this result to deeper neural networks. So, the main difficulty lies in how to calculate the number of regions if you have. Regions, if you have more than two layers, that will be much more difficult than the current case. And also, we need to consider the algorithm perspective. So, we need to consider implicit regularization like those usually adopted in practice, like noise injection, early stopping, and the gradient descent, and SGD, like many practical algorithms. We need to incorporate this algorithm into our theory. And also, it will be interesting. And also, it will be interesting to consider classification problem and more architectures like a CNR and RESTNet and section. So, this is my talk. So, I only show you a two-layer theory here, very simple. If you are interested, you can try to explore this line and develop a more complete story for the deep neural networks. Thank you. 