So we need to switch the link, but I'm seeing JSA well. Are you seeing my slides in full mode? Yes, perfect. Thank you so much. So our next speaker is Jose Hugh, who's Jeremy Albert Lawrence Box all the time. A lot of research in genetic equities and genetics. Or, well, she's been doing a lot of research in genetic ethics. A lot of literature, please. Thanks, Joseph. Thank you. My slides advancing. Yes. Perfect. So, thank you so much for the invitation. I wish I was here with you. Please enjoy the beautiful scenery. I've been to Benf many times and it's a great opportunity. So, today I'm going to talk about approaches to exploit family history, and it's especially relevant to biology. Especially relevant to biobanks. So, I'll start with a little bit of a motivation, which I may not need to with this audience. I'll talk about our proposed approach to family history aggregated-based tests, which we call FAT. And then I'll end with an application to the UK Biobank looking at Alzheimer's disease and dementia and hypertension. So, as many of you know, genome-wide association study has led to the association Study have led to the association of many variants associated with disease. However, most of these variants have really small effect size, and the power depends on the number of cases that are available in the particular sample that you're analyzing. Rare variants are crucial to understanding the unexplained heritability, but because of low power, association is often evaluated with the group of rare variants, not with each variant themselves. Variant themselves. And that's where family history may come from. It's the disease status information on relatives, and it can be incorporated to enhance statistical evidence. And that family history is often collected in Biobank. So the challenges with GWAS, we all know, stringent threshold to declare significant, replication often sought to validate findings. Most genetics, as I mentioned, small effect. You need really huge sample size. Really huge sample size are required, which is where biobanks are great, and very low power to detect association with rare genetic variants. So, the model that I'm going to be using for some of that work is very similar to a single nucleotide variant model that was presented earlier today. It's a generalized linear model that includes the covariates or phenotype and then a single, for the moment, this is a single gene. For the moment, this is a single genetic variant. So, G is a vector of size N, where N is the number of individuals in my study. And then, depending on whether the phenotype of interest is quantitative or qualitative, then either a linear or logistic regression would be used to obtain an estimate of the effect size of that genetic effect data had with its standard error and the p-value from that test for each study. From that test for each. So, this is usually done one single variant at a time, and then the p-value for each of these millions of tests is visualized using a Manhattan plot. And then here, Z is some kind of covariates of interest. For example, if you're studying pulmonary disease, Z might be smoking history, whether somebody is a current smoker, could be age, sex, DMI, but those are all the non-genetic covariants. Covariance. So, and then it's called a Manhattan plot because that's what a plot would look like. You plot minus log 10 of the p-value, so higher is better. And usually when you find an association, then you see a big peak. So, this is a GWAS of fasting glucose in a consortium. So, there were 21 studies that were together. When analyzing a single cohort, the genes in purple were the one that came up as significant, nothing else. Came up as significant, nothing else. But when combining 21 studies, all of the red genes came up, and then when combining an additional 27 studies, then the black, the two black genes came up. So you really need a large sample size to detect associations. Because of the low power, oh, and I forgot to say, these were restricted to variants with minority frequency greater than 1% in the population. So if the variant was rare, then it was excluded. Variant was rare, then it was excluded from the analysis. So, because of low power to detect association with rare SNPs, they're often excluded from GWAS or they're analyzed jointly within genes or region. And the some commonly used approach to jointly analyze rare variants is what we call the Burton test. You look at the sum, how many for a number of genetic variants in a region, in a gene, or of interest. You sum the number. Of interest, you sum the number of rare anneels that a person carries and you test the association with that sum. This is most powerful when all of the rare anels have the same direction of effect. They're all beneficial or detrimental. If you have some beneficial and some detrimental, they will cancel out each other and the power will not be as high. So, to deal with that effect that some variants could be detrimental or beneficial, then somebody would Somebody, Wu and I'll propose the sequence kernel association test. It's aggregate the score statistics of each SNP into a variance component. And then the SCAT O is kind of a linear combination of the burden and the sequence that optimized for maximum power. Because when all the rare variants have their effect in the same direction, the burden is more powerful. So the scab always kind of to make up for that loss of power. And the model is exactly the same for the rare variants and the For the rare variants and the single variant. The only difference is now G is a matrix. It contains K genotypes. And the genotypes are usually the number of rare ideals. Or if you're dealing with imputation, it could be the expected number of rare ideals in the form of dosage. And the test now, instead of testing a single beta equal to zero, it's to sum that all the betas are equal to zero. So none of the SNPs affect the phenotype. And the specific form of the And the specific form of the null hypothesis will vary according to the approach. So, for example, for the scat test, the assumption is that those beta follow a distribution. The distribution is not specified, but it follows some distribution with a mean of zero and a variance WK tau. And the WK are known prestified weights, and they can be your belief as how much prior information based on the annotation of Based on annotation of how likely a particular variant is to affect the phenotype. Often it's just based on ideal frequency, believing that rarer ideal will have a larger effect. And testing that all of the betas are equal to zero is equivalent to testing this variance component parameter is equal to zero. And the beauty of this approach is that the test statistics is very intuitive. So y minus mu hat is just the residual of a regression. Residual of a regression of your phenotype Y on the non-genetic covariance. So, this is the residuals, this is your genotype matrix, this is your diagonal matrix of weight that you believe, and this becomes a chi-square statistics. And this, because of LT between the variant, this as the distribution of this statistics is close. You can compute it without simulation, and it's a mixed sum of chi-square one. The sum of chi-square one distribution. So it's very computationally efficient. Okay, now on to the next. Now that I've given you a five-minute introduction that many of you didn't need, let's go move on to what we've done differently. So we want to exploit family history. The power of TWAS depends on sample size and number of cases, and some H-related disease may have very low prevalence in biobanks. You may have very few cases of dementia in Alzheimer's. Of dementia and Alzheimer's, if the average age of your biobank is not high. So, the disease status of parents may provide information on the future phenotype of a particular biobank participant. And incorporating family history, the disease status of relatives may improve power when the number of available cases is low. So, the Goshen all and Peter Kraft was an author on this. Your craft was an author on this paper, notice that you can decompose the likelihood of the phenotype of the relatives and the phenotype of the participant in this way. So if I let YR the phenotype of the relatives, so these people may not be participant in the particular study, but I've asked a particular participant, did your mom have Alzheimer's disease? Did your dad have Alzheimer's disease? And this is recorded in the data. And this is recorded in the data. So, this information is often collected. And then, YP is the phenotype of the participant. Okay, I do not have the genotype of the relatives. I've just asked my participant, did your mom have such and such disease? So I have the genotype of the participant, I have the covariance of the participant, and I have the phenotype of the participant, but all I have on the relative is their phenotype steps. And probably with a little Steps and probably with a little noise, too, which I'm not dealing with at the moment, but that's something that we should look at as well. So, you can write the likelihood of the phenotype of the participant and the phenotype of the relative given the genotype of the participant, the covariates of the participant, and the covariates of the relative. And you can separate it in two independent likelihood: the phenotype of the participant given its genotype and incovariates, and the phenotype of the relative given. And the phenotype of the relative given its covariates genotype. And what's important here, I need to condition on the phenotype of the participant in this likelihood. So here we assume that the phenotype of the participant is independent of the phenotype and covariates of the relatives after conditioning on the participant genotypes. So if I know someone's genotype, knowing their parent status is independent, which is another assumption. Independent, which is another assumption that may need to be verified. So the full likelihood is the sum of these two. So I can evaluate each of these pieces separately. So I can do a regression for finding the effect of the genotype of the participant on its phenotype. And then I do another regression looking, but this time I still have the genotype of the participant, but I'm looking at the phenotype of the relative. At the phenotype of the relatives, conditioning on and putting in the model the phenotype of the participants. And you can show that this beta r, which is the effect size that I get of the effect of the genotype participant on the phenotype of the relative, is a discounted. Of course, you're one relationship away, but you can show that the discount depends on the relatives, the relationship between the relatives. The relationship between the relative and the participant. And for fourth-degree relative, this would be: you expect the effect of the relative to be half of that of the participant. And moreover, these are independent. So you can do two independent score statistics to test the association from that likelihood aggregated test. You know there are. You know the relationship between the participants, so you can estimate it. And then what you would do is you would downweight the weight that you got from the relatives by this two times t factor. And then you could then combine these into using a meta-analysis. And the beauty of it, again, the distribution of this FAT statistics follow a weighted sum of chi-squared distribution. Distribution. And you can, similar to SCAT-O, you can have an optimized version that is a linear combination of the FAT statistics and a FAT-burden statistics that will be most powerful if all of the rare variants have an effect in the same direction. And we can also adjust for case control unbalance ratio, the same way the SAGE software has done in this particular. So we compared this method, this new method, the fat and fat whole, to other aggregation tests, the burden tests. We compared it to ACAP, which is an aggregated Cauchy association test, which is based on single variant association statistics of these rare variants. We also compared it to a liability threshold family history. So there's another way of incorporating family history. Is another way of incorporating family history, and I'll describe on the next slide. And we've applied this to the UK Biobank. So the LTFH, the Liability Threshold Family History, computes a posterior mean of genetic liability for cases in control. So the more first-degree relative parents you have, the higher your liability. So in this particular example, the control on the top left has, this is a control when no parents have factored. Control with no parents affected. So its liability is close to zero or maybe a little discounted to 0.1. A control with one affected then would have a higher genetic predisposition to the disease. So maybe its genetic liability would be 0.4. And then a case with two affected parents would be thought to be highly genetically predisposed and would have, in this particular example, a phenotype of 1.56. 1.56. And then these quantitative phenotypes can then be incorporated into your analysis. So instead of using the case control status, you would then use this quantitative treat in your analysis. So on to the application for the UK Biobank. So we applied it to the exome array on about 130,000 individuals of European descent or covariates that we adjusted for with sex. That we adjusted for with sex, H, BMI, and PCs, and we looked at dementia and hypertension. So, in the middle line here, you see our participants' age was 57, ranged from 38 to 72. So, not a lot of dementia in this participant. The parents, though, were average age of 78, ranged from 60 to 105 for mothers, and 60 to 102 for fathers. So, more opportunity, especially for a phenotype like dementia, to Dementia to be present. We have 27 participants with dementia only. And then, but we had more mothers and fathers who had dementia, 4.4% of the fathers and 8.2% of the mothers. And we wanted to contrast with a phenotype that was really higher prevalence to see whether when it's low prevalence, that's when we expect the method to be incorporating feministry to be the most. Reading family history to be the most beneficial. And we didn't expect much gain for hypertension, where the prevalence in the participant was around 25% and similar in mothers and fathers. So we started with four genes where rare variants have been implicated in the literature with dementia. And we compared these two approaches here, do not take family history into consideration. These two approach, which is the liability threshold and then or approach in blue. And then, or approach in blue, do take into consideration the family history. And as you can see for dementia, these known variants, the evidence for association was much higher in the, or the p-value was much lower in the two approaches that take family history into consideration. When looking at hypertension, again, we picked five SNPs where we had evidence in the literature that rare variants were implicated. Parents were implicated, we didn't see that much of a gain because already hypertension is very common. So adding family history did not really add much, maybe a little bit with the LTFH here, but when you're looking at the other, it's very similar to what we found with this is only analyzing the participant, this is adding family history. So if liability threshold family history did so well, why do we need? Family history did so well. Why do we need a new test of that? Well, our method is more computationally intense efficient. This is the computing time in minutes for testing a thousand regions compared from the FAT to the LTFH. So you great gain in efficiency, and that might not sound like a big deal, but when you're analyzing thousands of genes or regions, that can make a big difference. Make a big difference. Finally, we applied these methods genome-wide to dementia, not only on genes that had rare variants associated with dementia. And these are our top six findings. And interestingly, three of the six genes had common variant associated with dementia. So it looks like rare variation might be also associated. The genes in black will need to be replicated and followed up in other studies. And followed up in other studies. And similarly, for hypertension, we found three genes at the exome-lite level, and two of these had common variants associated with already in the literature. So in conclusion, inclusion of family history improves power. I didn't even show you the simulation results. Lower p-values doesn't really necessarily mean improve power. Really necessarily means improved power, but our simulation did show improved power for the two methods that added family history. When including first relatives, FHAT and this liability threshold family histories led to similar results. Both had greater power than not incorporating family history, but FAT is much more computationally efficient. One advantage, maybe, to our approach is that the liability It the liability threshold only incorporates first-degree relatives. We have applied our data to family-based studies where we can incorporate distant cousins and all of that to improve power. So that's a small advantage. And as I showed you, we observed improved significance for gene known to be associated with the phenotype when incorporating family history in the UK biobank. I just want to end by acknowledging all the people who have worked on this. On this work and the support for the work. Thank you very much.