Because I had expected maybe that there would be somebody who knew less about the subject than I do. And I'm looking at the list of participants here. I'm thinking, oh dear, what I've got to say is going to be utterly banal. So if you want to drop out, please do and come back in an hour. Or alternatively, interject when I say silly things because I'm sure it will be more much. Because I'm sure it will be much more interesting for everybody if you pick me up on my mistakes. Okay, so is that somebody saying yes already? No. Just wanted to say the floor is yours and we're really happy to have you give this lecture. So here's the, oops, let me just get rid of these things, these here. This is a bit irrelevant to everything. Okay, so motivation. I put this slide here together probably. Slide here together, probably two or three years ago, but I could have done the same in any recent year. It's summer 2018, and a whole variety of extreme events of various sorts, North America, Europe, Africa, Asia, and so on, massive variations in global sea surface and global surface temperatures. I could have picked last week's Guardian, which had a cover about extreme events. About extreme events. Any time recently, one could have made a similar picture about bad things, rare events happening in large numbers. Why are we interested in them? Well, basically for extrapolation. We want to extrapolate to the tales of distributions beyond previous events, and we want to be able to make inferences to predict what new conditions might be. What new conditions might be in a warmer and perhaps more variable, well, certainly more variable world. And the issue, in some sense, is summarized in this picture here. As you see, the original world, as it were, is this black thing, and we have some observations that you can see at the bottom here. Whoops, that's not what I wanted. It's this one. The black observations you can see at the bottom here. And then, if we are interested in the red ones, some of which are massive. Red ones, some of which are massive, clearly the probability of being above this point here is much higher under the red curve than under the black curve. And we want to be able to say something about such probabilities. So we want to be able to estimate changes in extremes, perhaps for better forecasting. We may want to do risk assessment at a single important site, power station or something. We might want to do risk estimation for particular compound events. For particular compound events, possibly risk of crop failure due to drought over a large region, drought followed by massive rainstorms, so leading to bad things of all sorts, trying to estimate total insurance payouts over spatial regions. And something that I expect will be talked about later in the workshop is attribution of events to causes. To what extent is a heat wave caused? Is a heat wave caused in quotes by climate change. And what we want to do in all of those cases is accurate interpolation, possibly spatial, and extrapolation into the rarer world. And what that's going to involve is accurate modelling of individual variables and also of their joint distributions with particular reference to the extremes. And this can involve. And this can involve what might be called bold or perhaps foolish extrapolation, such as the prediction of the 10,000-year event from 80 years of data. I've, for my sins, attempted to estimate 10 million year events from 27 years of data, which is clearly an idiotic thing to try and do, except that somebody has to do it by law in Switzerland if you want to assess. Want to assess the possibility of rare event high winds for nuclear power stations, and perhaps it's better if somebody who knows that you shouldn't do this does it than if somebody who thinks that it might be feasible does it. Of course, the basic problem is the events are or at least used to be rare, so there may be little or even no directly relevant data. Why do we want specialized models? Well, because the task Specialized models? Well, because the task is extrapolation, and the multivariate normal distribution or univariate normal distribution are just too inflexible for accurate modeling of the distributional tails. There could be regime change in the tails, which might not be captured by a fit to the entire distribution. Different fits to the bulk may give you different tail estimates. Here, just to illustrate this point, I've plotted three distributions, a Gaussian. Distributions, a Gaussian, a Cauchy, and a T20. The Gaussian is black, the T20 is blue, and the Cauchy is red. They all have the same probability, 0.05, for events greater than 1.96, but the ratios of the probabilities for when we go further out into the tail can be massively different. So here we've got the ratio of the Cauchy probability to the Gaussian and the ratio of the T20 probability. And the ratio of the T20 probability to the Gaussian. And you can see they can be massively different. And we might not be able to detect that in the middle of the distribution. So the extrapolation here could be wildly off if we fit using this portion of the data. Things get worse if you look at joint Gaussian tails for rare events. This picture here shows the situation of what's called asymptotic. The situation of what's called asymptotic dependence, where we have simulated data with, in this case, Gauss, I don't know, gumbel tails, I think, gumball distribution. And here we have a asymptotic dependence, which you see by this stabilization of the conditional probability here. Here, we're fitting a bivariate normal distribution in the data, and you see here that the probability of the distribution. And you see here that the probability doesn't stabilize. And indeed, if we go far enough out, it's going to tend towards zero. So, joint probabilities there depend on the sort of joint distributions that you fit. And again, it may be difficult to tell what to do from the center of the data. So, the extremal paradigm is that we need a basis for extrapolation outside the sample, maybe based on a small subset of the total data. Subset of the total data available. The uncertainty is going to be large because we're basing the inference typically on a small sample and it's going to have to be taken into account. And that standard methods and models, by standard, I mean sort of classical, are just going to be too limiting. They can't deal with heavy tails. The joint tail probabilities are likely to be too inflexibility. And they may involve misunderstanding. May involve misunderestimating probabilities for joint events, to use George W. Bush's phrase. So, what we're going to try and do is to fit asymptotically justified models to do the extrapolation, but then of course check the adequacy of those models carefully. So, to move on to talk about extremes of single variables, the basic notion is one of max stability, and that can be expressed. And that can be expressed in words saying that the maximum of 100 consecutive years of data equals the maximum of the 10 decadal maximum. And what that implies is that if we rescale a maximum of n observations by subtracting off some normalizing constant, some location constant, dividing by some scale factor, then if this Then, if this thing has a limiting distribution g, that's non-degenerate, then that g has got to satisfy what Maurice Frechet called the stability postulate, this expression here. For any t greater than zero, g has got to satisfy this equation here. There must be some functions a and b such that this is true, which is just another way of saying this in the limiting case and replacing 100 by t here. By t here. What Fisher and Tippett proved, at least in the modern expression of what they proved, is that the only solution to the stability postulate is the generalized extreme value distribution, which can be written like this, e to the minus this thing lambda, where lambda is this expression here. Okay, we'll talk a bit more about that in a minute. Okay, there are three parameters here. There's this Um, there's this uh eta and tor, which essentially correspond to the b n and the an. So, if you've got the wrong b n and an, you would have to include an eta and a tor. And of course, that means we always have to do that in practice because we never know the underlying b n and a n in applications. And then there's this parameter ψ, a shape parameter, which we will need to look at in a bit more detail later. And this, the GEV, is a kind of universal law analog. Of universal law analogous to the use of the Gaussian, our habitual use of the Gaussian for averages. Here are the three types that actually Fisher and Tippett found, the Gumbel, Frochet and Weibel distributions. They're determined essentially by this parameter ψ, which determines the rate of the tail decay. Positive xi gives the heavy-tailed or frechet type 2 distribution, so-called, which is Distribution, so-called, which is bounded below at this point here. It's the red curve. The gumball is given by xi equals zero, the type one distribution, support on the entire real line, and negative psi gives this reverse Weibull distribution, the type three so-called, which is got is bounded above. And so you can think of as psi goes to zero, this point here goes off to minus infinity, it reaches there when psi hits zero. Reaches there when psi hits zero. And then the top, when cosi starts to become negative, the upper terminal, which was at plus infinity, starts to move downwards. And we therefore have a finite value here. Minima, the limiting distribution of minima is one minus g of minus y, so there's no need to discuss them separately. Why do we want to do this? We want to extrapolate. So what we typically will do is if we were to fit this distribution to maximum. were to fit this distribution to maxima would be say to fit this to annual maxima in typically in an environmental setting and then we just use the stability postulate we just rewrite it in this form here so so saying g to the t for independent t maxima from t years with parameters e to tau and xi we rewrite those and i'm sorry there's a typo here um this is going to be g This is going to be G of Y eta Tor Ta, sorry, eta T, Taur T and Psi. And the eta T and the Taur T have got a close relationship to the original parameters. So essentially, what we're going to do is just change the location and scale parameters and thereby come up with our new distribution. And that's what you see here. Here I've done this with t equals 50. If we imagine that this is the distribution we fitted to Imagine that this is the distribution we fitted to our original data observations down here, then this would be the distribution we would extrapolate to if we were interested in a 50-year maximum. A different way of thinking about this, but a closely related way, and a way that's important when it comes to more general settings, is to think about this in terms of a Poisson process. Poisson process is just a random point pattern in some state space. Pattern in some state space that's determined by the properties of the counts in sets and that satisfy two properties. The counts have got to be independent for disjoint sets, and they've got to have Poisson distributions given by this expression here, mu of A, where mu is a measure and is non-atomic. And often in applications, it's going to have some intensity which I'll call mu dot. So, why is this useful? Well, it's partly useful because it gives us general constructions for extremal distributions, particularly in the multivariate and the spatial settings, but it also gives us another way of handling distributions in the scalar case, partly because of what there are two results that are important. One is the mapping theorem, which says that if we take a mapping of a Poisson process, Of a Poisson process, then we get another Poisson process provided under mild conditions on the mapping. So if a mapping doesn't create atoms, points of mass, doesn't map several points to one point, then we get another Poisson process. And we can easily compute the measure of the new Poisson process in terms of the old. And the second point is that if we restrict the Poisson process to some subset. Process to some subset of our original set, we're also going to get a Poisson process with the same measure. So what we do is we say, well, let's take observations. So X1 up to Xn, IID according to distribution F. Rescale the individual observations in exactly the same way as we did the maxima previously. Take this. Take this set here to be the real line, and then think of rescaling the. So, what's going to happen now? n is going to tend to infinity. We're going to get a sequence of these point processes. Let me show you a sequence of point processes. I've put them like, so here there are 10, I think, 100,000, 10,000 points. BN is increasing and it's dragging the points down towards minus. Down towards minus infinity. An, I think, in this case, is just a constant. So, all that's happening here is that we're getting more and more points, and we're interested in what happens in the region up here. Okay, and what you see is these patterns here are changing, and they are converging to a Poisson process with the measure given by this expression here. So, we have the measure of the number of points that falls into. The number of points that falls into some rectangle that looks like this: right, infinite up to the top between t1 here, t2 here. That's going to converge to a Poisson process with the measure given by this expression here. So the mean number of points in such a set will be this, and the number of points itself will have a Poisson distribution. What that implies is that That implies is that if we think about take a series of IID data and think about rescaling them and look at the exceedances over a threshold like this, then they will be distributed according to a Poisson process with this as the measure. And we can use that also to make inferences and to describe the behavior of those extremes. Okay, and in fact, it turns out that because of this of the structure here, those This is of the structure here, the times are independent of the sizes, and the sizes for exceedances are given by the generalized Pareto distribution. The times are just a homogeneous Poisson process. And that gives us another tool for modelling, because what we could do, for example, here would be to say in applications, well, let's suppose that the rate here in time varies perhaps according to modeling. Perhaps modeling that with a generalized additive model or some such thing, and try and model the sizes, possibly also depending upon time, as we would have if we did that, possibly allowing them to depend on some other covariates, possibly allowing them just to be stationary. So we can, therefore, we can, we, this sort of leads to three different ways of modelling. We could take the maxima, we could take the Poisson process. Take the Poisson process, we could just look at the threshold exceedances and fit them using the generalized extreme value, generalized Pareto distribution. And do extrapolation. In principle, we will have the same answers for any of those approaches. In practice, they may be slightly different because of the data we may be considering may be somewhat different. Generalized Pareto distribution is a nice flexible distribution. And here are some. And here are some examples of it. It's exponential when the parameter ψ, the same parameter as for the GeV, is zero. It can have an upper terminal if the parameter xi is negative, and is a type 2 Pareto distribution if the psi is positive. But if negative, we can have a variety of other distributions. We can have the uniform, the triangular distribution, and then various other distributions with upper terminals. Distributions with upper terminals of the forms that you see here. And one can modify this distribution, as has been done, for example, by Raphael and Philippe, to try and accommodate what goes on not only at the upper tail, but also at the lower tail. Okay, the shape parameter is crucial, as I said. It tends to have characteristic ranges for different types of data. Ranges for different types of data. For rainfall, typically 0.1 or so. For extreme hot or cold temperatures, typically negative, typically minus 0.2 or something. Wind speeds, typically negative, and so on. It's difficult to estimate and its uncertainty dominates the extrapolation. So what we often might want to do is to combine data from is to combine data from different sources to reduce uncertainty for estimating the ψ. In a spatial context, that often means choosing the same ψ at all points of a spatial process. And there's often a reason for an added reason for that, or argument for that, is that we're dealing with the same sort of phenomenon. So we would expect on grounds of physical consistency to have the same shape parameter everywhere. Or we can maybe use Bayesian methods. Or we can maybe use Bayesian methods, combining the data by some kind of empirical Bayes approach, or imposing some kind of constraints on Maxi, as is done, for example, in hydrology, typically. That will do that. I'm going to skip mostly the statistics elements of this because there just isn't time to fit them all in in a All in a talk of this length on so many topics. But just a few comments. Obviously, the GEV and the GPD are limiting models, but they're used as approximations for maxima for finite, a finite block size. N doesn't tend to infinity. We take some finite m and use that value. So 365 if we have daily. 365 if we have daily data, 20 if we're considering monthly data in the context of sorry, monthly maximum in the context of financial time series or something. And we are going to otherwise we might be considering threshold exceedances, but not of a threshold that's effectively infinite, but of a finite threshold. So we're going to have to trade off taking too small an M or Small an M or too low a threshold, that'll give more data, but estimation is going to then be more biased. It's going to inevitably be biased. The question really is, is the bias so much that extrapolation is dangerous? We could otherwise increase M or U, increase the block size or the threshold. That's going to reduce the bias because presumably Reduce the bias because, presumably, the asymptotic approximation will be better approached, but that may give too little data for a useful assessment of uncertainty. And in principle, one could trade those two off. In practice, as far as I can tell, mostly what people tend to do is just to take as a lower threshold as they can, or as M typically taken to eliminate certain things on. Eliminate certain things on eliminate seasonality or whatever, and then hope for the best essentially. So, there are methods for automatic choice of these things, but they may perform not very badly. So, generally, graphical methods are used for these purposes. Quantile regression, I think, is increasingly used to choose you in big data sets. And there's been quite a lot of more recent work on such things. Work on such things. Crucial element is sensitivity analysis. The conclusions should not depend too heavily on the asymptotic approximation. If they do, then the whole extremal paradigm is starting to look shaky. And just as a general comment, there's been a lot of work in the literature on different sorts of estimation for these models, but usually there's some complication. Usually, there's some complication. And rather than prove new theorems, we just tend to use likelihood or base methods, which are flexible and general and readily accommodate, for example, some forms of selection or censoring or any other lots of other complications you could think of. Quantiles and return levels. So, what do we want to estimate? What are going to be our measures of risk? Of risk. One standard thing is to try and estimate quantiles. And the p-quantile of a generalized extreme value distribution, it would be given by this expression here, where we would, to estimate this, we will plug in estimates of eta, torr, and psi. And presumably, we want to do this for some fixed p, corresponding, for example, to 10 to the minus 7. 10 to the minus 7, well, rather, 1 minus 10 to the minus 7, looking at the largest, in quotes, the largest value that might be seen in 10 million years at the site of a nuclear power station. So YP is called the return level associated with the return period of 10 million years. And the obvious question, of course, is: is this at all useful in a non-stationary setting? non-stationary setting. Does it make any sense at all to talk about this? Or should we be talking about the largest value we might see in the next 10 million years rather than some fixed parameter of a model? And so one aspect of this is if we're interested in future events, for example, the largest floods to be seen in the next two years, even if we knew the data generation mechanism. Data generation mechanism exactly, whatever it was, we would still think of that largest flood as random until those next few years have passed. So the question then arises, well, should we actually think about predicting future events rather than thinking about these parameters, which is essentially what a return level is, and so is a probability. In a Bayesian context, prediction would be, in principle, at least, straightforward. Principle, at least, straightforward. We would just compute the posterior predictive density of whatever we're interested in conditioned on the observed data using this expression here, standard expression, some sort of prior density. And we could, if we wanted one, so this would be a full density function, of course, we could compute summaries of that of some sort, quantiles of this distribution, or the mean, or whatever. In the frequentiest. In the frequentist setting, we might estimate properties of this thing, and that will be taking us back to something probably rather similar to the return level. But I just want to point out that in many applications, we might be more interested in thinking about this as a prediction problem rather than an estimation problem. Here's an example. So, maybe I should just stop. I've been talking for 20-something minutes. Just stop. I've been talking for 20-something minutes. Are there any questions? Talking very fast for 20 minutes. No questions. Everyone's fallen asleep. Oh, good. No, we are enjoying it. All right. Thank you, Gabby. Okay, so here's a set of data: annual maximum for Vargas. This relates to a Set of rainfall data, which Leo knows well. These are rainfall data, there are daily data from 1961 through to the end of 1999. There are annual maxima for 10 years before that, this part here. And one advanced, and the red blobs that you see here are the annual maxima for this series. So, what we would want to do, and We would want to do, and what you see here is this last one was a massive event which took place in December 1999, if I remember correctly. I can't remember the exact dates. What you see here is that this event, of course, is much, much bigger than anything previously. And this led to a really massive rainfall of, I think, something like 700. 700 centimeters, sorry, 700 millimeters of rain fell over 24 hours or so. Because of the landscape, there, which I might have here, no I don't, that led to massive mudslides and killed who knows how many people, 20,000, 30,000? They don't really know. It's the Vargas tragedy. If we try fitting distributions to those data, so the data, just the Just the daily, so the annual maxima over this period. You see here the estimated location scale and shape parameters. So this is over the whole period. And you see the shape parameter is 0.36 with standard error 0.15. Very wide possible range of tail behaviors. And that's exactly what you would expect looking at the observations here. There's with such massive variation in the maxima, this observation is clearly going to have a very large effect on any fitting. If we leave that out, then we see these estimates here. The location and the scale barely change. So what I'm doing here is fitting the generalized extreme value distribution. Notice the shape parameter has changed by a lot. And indeed, in this case, the shape parameter The shape parameter, zero, the Gumbel distribution, is well within the range of the standard error, the 95% confidence interval in that case. It's clearly outside it in this case. So one observation in this particular case has made a huge difference to the fit, not so much to these two parameters, but to that one. So that's obviously a worry. So that's obviously a worry in practice. There's also a second issue here, which is that this is the last, this is the data stop here. And we've not been able to get the data subsequently. So there's some sort of stopping rule that could be being applied here. We get the data up to the largest event and then we stop. And this is not taken account into straightforward maximum likelihood fitting. Straightforward maximum likelihood fitting here. And Leo has done some work to try and take this into account. And I think I'm not quite sure if our paper has yet appeared in the Annals of Line Statistics, but if it hasn't, it will shortly, I'm sure. Okay, in this case, for the predictive densities for the annual daily maximum, which you see here, the black curve, and the 39-year daily maximum, red, this curve here. This curve here, this is the extrapolation. Now, you see that this event that we see in 1999, these are based on the data without that last observation. You see that this event is unlikely even regarded as the 39-year maximum, but it's not impossible that the density here is not zero. Is not zero, whereas if we think of it as an annual maximum, it does appear to be essentially impossible. So, this is just coming back to this issue about predictive inference again. Maybe we would be better off thinking of this event as being the largest thing we could observe in 39 years. Okay, if we look at seasonality here, here are the monthly maxima, and what I've done is just to plot them against gumbo. Plot them against dumbbell plotting positions for each individual month. And you can see that some of the maxima are actually zero in some months. So for example, these ones here, I think. There are certain months down here. So clearly, it's not going to be very wise to be fitting gumball distributions, which don't have a point mass zero to data like this. We would need to think about Think about accommodating that somehow, for example, by fitting a censored gumball distribution, or possibly not even a gumbel distribution at all, or possibly allowing for the fact that in different months, there are a different number of positive observations, and therefore the gumbel approximation may need to be modified in some way. And maybe we should also be thinking about mixtures here based on some kind of climatological considerations. Climatological considerations, because we can see that certain observations lie well off the lines for the rest, and that may suggest that something is going on, particularly exceptionally warm sea surface temperatures or something, tornadoes, rather hurricanes that are leading to these very large unusual values. If we do a peak server thresholds analysis of the data, you can see that we're The data, you can see that we're collecting, instead of getting this one observation at the very end, we get several others. Here I've taken the threshold to be 40, it looks like. Yes, 40, here we are. We can also see this clustering. Okay, and we can use properties of the generalized Pareto distribution to try and choose the threshold because this idea of stability, if we have If we have, if the threshold is, if the distribution is stable above the threshold, the parameters estimates should not change. And therefore, we can plot those estimates against the threshold and look for where they start to stabilize. And we see those in these three plots for the eta, the tor, and the xi. And I don't know quite what we should learn from. We should learn from this, but it looks like 40 is certainly stable above that point. Now, Jenny, who I think is probably snoozing, has done and others have done work that would aim to improve pictures like this. The issue being that all the observations, only sorry, all the observations here above threshold. Above threshold 100, are of course also they're a subset of the observations above threshold 20 or any lower threshold. And that's going to mean that these points here are all correlated. And so some form of decorrelation seems like a good idea. Otherwise, the eye risks being misled by such plots. But anyway, in this case, taking 40 seems a perfectly reasonable threshold. Seems a perfectly reasonable threshold in this case. If we look at the degree of clustering here, we can assess that using something called the extremogram. This is simply the probability, if you like, is an autocorrelation function for extremes. We look for each lag, h equals one, two, three, and so on. We look conditional on the observation at place t being above u is what's the probability that the observation. The probability that the observation at t plus h is above u for some threshold. And of course, if there's no serial dependence, we should expect to see that this thing should be a constant because the fact of this conditioning should not affect the probability of this event. It's not quite, but it's very close to being the autocorrelation function for the time series of the indicators of these events. And we can estimate it by almost taking the correlation function. Estimate it by almost taking the corresponding correlogram. And here you see that for these data. Okay. And what we appear to have here is some autocorrelation, some positive probabilities here within for about five days, possibly. At longer ranges, there appears to be no correlation, except, of course, we might, if there was an annual cycle, we might expect there to be. We might expect there to be some correlation at relatively long ranges because if there was an annual cycle in these sizes of the extremes, we might expect to see re-emergence of correlation over here. We can clearly see that there's some correlation at short term. So that's leading to clustering of the extremes. And that you might think would be a worry. On the other, the good news. The good news is that, under reasonable conditions on the long-range dependence, block maxima from a stationary process still have a limiting generalized extreme value distribution. Given by this expression here, lambda is the same lambda as before. This new thing, theta, is the extremal index. That's a number between zero and one. And that determines the behavior of the clusters. So you can see that that is somehow, it's like the t. Well, it's like the t that I inserted before here. If we took the t year maximum, theta would be replaced, we would have t there, right? But of course, t is going to be greater than one. Theta is less than one. So we see that this is going to reduce the extremes, reduce the size of the extremes relative to those for the corresponding IIG data. That's good news. That means that if we want to fit, if we have local Want to fit if we have local correlation as we do in the Vargas data, and if we have, and it's really local there, we can expect that the GEV will provide a good approximation to annual maximum, even though the underlying data are dependent. And it turns out that the basic Poisson process approximation that I described earlier extends to allow clusters of Extends to allow clusters of extremes, they have mean size one upon theta, but otherwise an arbitrary configuration, essentially, when the data are long-range independent of extremes, but short-range correlation, potential for short-range correlation. Now, seasonality in extremes here could stem from variation in the numbers of large range. Variation in the numbers of large rainfall days, or the sizes of the amounts, or both. And by playing with the parameters of the lambda or a theta here, we can, the Poisson rate, the rate at which exceedances might occur, we can formulate regression models to try and distinguish between these different possibilities. So, what happens is that if we have t. If we have t lambda of y, something times lambda of y, when this goes in, that influences the location and the scale parameters. And how it influences those location and scale parameters can be used to distinguish different forms of dependence, whether of the sizes of the maxima or of the rate of them at which the maxima arise. Okay, and then the final comment that I want. Okay, and then the final comment that one I made before is the data set there ends with the largest event, so there's a stopping rule being applied implicitly. And if we ignore that in general, that's going to bias estimates of the risk upwards. Because really what we should be doing here is saying there is some level. I stopped, say, here, right? I'm waiting essentially until I stop. The last observation should be conditioned to be. Observation should be conditioned to be greater than this point here. It should be conditioned to lie in this set. All the previous observations should be conditioned to lie in this lower set. And that should be somehow built into the analysis. It's relatively easy with likelihood methods, and it does influence the estimates. But if we don't allow for that, when we should, that's going to clearly bias the estimates, the risk upwards, because what we're saying about the largest. Because what we're saying about the largest, the observation at the end is it could be anywhere here when actually we've somehow implicitly conditioning on it being above a certain level or an equivalent stopping rule. Maybe it's the largest of all the others and it's got to be five times larger than the previous one or something. Okay, let's stop and ask if there are any questions or comments. And there are no questions from the chat, but if people remember. The chat, but if people would like to raise their hands or ask their questions, they're welcome to. There is one question here. Hi, Anthony. This is Sebastian. I just have a question. So there is some sort of selection bias that you mentioned that you kind of fit a GEV once you have seen a large observation, right? Or you tend to look at data sets where you have. To look at data sets where you have a large observation, but isn't there also a spatial effect where you kind of select these stations where something extreme has happened and then you fit a distribution there? So can you account for this kind of selection bias in the spatial setting or do you see any challenges with that? I think it's generally difficult to allow for selection bias anyway because there's an implicit stopping rule. There's an implicit stopping rule being applied of some sort, implicit, and it's not, it's sort of thing, well, I know it when I see it, but before I see it, I don't really have a rule for seeing it. So if you could specify in advance, well, this is how I'm going to get my data, then you could maybe do a precise analysis of the effect of the rule. But since we rarely have such a rule, it's more a question, I think, of doing a sensitivity. More a question, I think, of doing a sensitivity analysis and saying, well, if we had such and such a rule, this is how it would have changed our conclusions or our estimates. Or if we'd had some other type of rule, this is how it would have changed our conclusions. Maybe Gabby wants to comment on this. Thank you. Oh, I was going to ask a question as well in the similar also about the stopping rule. Is this a good time? Yeah, well, we should. Good time? Yeah, well, we're talking about we have a project on, and that, and one of the postdocs in the project is looking at the Lytton heat wave or the Canadian, the Northwest US and Canadian Northwest heat wave, which broke the GEV, as we know, right? But as you say, that there is kind of an implicit selection in picking, first of picking an area where you had a walking extreme. And I wonder what we do, what we learn. What we do, what we learn from these really record-breaking extremes, and how we then deal with event attribution if the GEV is a bit risky to apply, because, as you say, there is an implicit rule that we go for a place where we have an extreme. So, has anybody figured out a solution to this? I don't know. Perhaps Chris Perhaps Kirsten wants to tell us about it. So, Smoo also asking a follow-up question. So, how would you take this into account for predictions? So, you emphasize that prediction is basically the ultimate goal. And with this challenging situation that you presented, with this last observation being very influential, how would you take that into account? Well, you would. Well, you I guess I mean what the what the allowing for selection does is it is it gives you more, in quotes, more reasonable estimates of the a more reasonable model fit. Because you're somehow saying, well, the earlier observations have to fall into this set, say, down here, and then there is at least one up here. Right, so that gives you it that gives you a different likelihood than if. Likelihood than if you had just refitted the observations as IID. And then you would feed those estimates forward into the prediction mechanism. So you would end up, so coming back to the Bayesian, this expression here, the f of y for the existing data would correspond to the likelihood after adjusting for the selection. The selection. So it would not be the same likelihood as it would not just be the IID likelihood. Does that make sense? Okay, yeah. And then, of course, because that gives you a different posterior distribution for theta, that's then going to give you a different predictive distribution for yt. Okay, so that would feed there into the predictive distribution as well, basically. So yeah. Okay. Thanks. Okay, thank you. Have you written this up? I would really like to share this with the panicking people. There's a paper. Yeah, so Leo's put in the chat, there's a link to a paper in applied statistics, which I'm not sure I'm allowed to say this. I think there's a mistake in some of the simulations. But I hope, oh, dear, I'm being recorded. Oh dear, I'm being recorded. Well, anyway, we did ask them. Okay, I'll better carry on quickly. I guess only you still have left around, let's say, seven minutes. So how many minutes? 97? Yeah. Seven? Yeah, around seven. Yeah. That's all right with you. Yeah. Gosh, okay. And 64 slides. Okay, multivariate extremes. Let's skip structure. Multivariate extremes, let's skip structure. So, yes, okay, so there's two motivations for thinking about multivariate extremes. Many extremal, first is that many extremal problems are intrinsically multivariate or spatial or both. We might be interested in flooding of river system in many places, heat waves, many successive hot days, spatial area and such like. There's a substantive motivation. There's also a statistical motivation. There's also a statistical motivation because we may want to reduce uncertainty by combining information from different sources. We have to think in several dimensions about what is extreme. And I'm going to skip the structure variable. In two dimensions, for example, we can imagine different, in quotes, extremal sets corresponding to these grey areas here, where, for example, the maximum of the two variables is greater than some. Two variables is greater than some threshold u, or the minimum is greater than u. Is that right? Yes, that's right. Or where their sum or some other function of them is greater than u, or whether only one of them is greater than u. And different, this particular setting here has been considered in particular by Heffernan and Torn in a paper I won't have time to talk about, unfortunately, but which can handle many. Which can handle many, in principle, gives you a flexible model for dealing with what happens when one particular variable is taken as large and we condition on that event. Okay, so what do we do? We basically transform the margins. So you imagine conceptually you fit an ordinary generalized extreme value or generalized Pareto distribution to the margin. Pareto distribution to the margin. You transform the margins to be a standard form. And the standard form that you can take any standard form in principle, but it's actually convenient to take the unit Frochet distribution given by this expression here. And then you watch what happens as the limit when you stabilize the margins and you have this as the limiting margin. Okay, and what that means, I'm Okay, and what that means, I'm sorry, I'm going to have to skip a lot of stuff here, is that we have, so here we are. Ah, this one. The limiting distribution of the maxima suitably rescaled is of this form here. So Z1 is the maximum for variable one up to Z D is the maximum for variable D. We're going to have the same form of distribution as before, e to the minus something. Before e to the minus something, the something is again a Poisson measure. Now I'm going to call it mu, and that can be written in one of three equivalent ways. Az, so this bizarre set here is this set here. This is A of Z, where Z is this point here. Okay, so we're interested in what goes on in this red set here going out. Red set here going out to infinity, right? And then the probability, we have this probability here corresponding to this. There's this function v. There's d times an expectation of this expression here, maximum. This variable, everything can be summarized in terms of this angular variable, w. This lies in the simplex, it sums to one. Sums to one, right? It's got distribution nu, which I call the angular distribution, and that satisfies the marginal constraints. Expected value of Wd is one upon D, and that is what gives it the correct mark the right margins, these margins, the e to the minus one upon z. And one way to think about this is to say, well, under what circumstances would the maximum Would the maximum be less than some value z on every margin? If we go back to this picture here, that means if this point here is z, that corresponds to this set here being void. And therefore, we can say, well, if there's a limiting Poisson process with lots of points down here, we want there to be nothing up here and points down here. And in that case, this set here is void. The probability of set here is void, the probability of that will be e to the minus mu of that set, where mu is the Poisson measure. Okay, so we again have this link between the maxima and a set being empty in terms of an underlying Poisson process. Another way to think about this is in terms of what could be called extremal functions, which are the individual points that I did. Okay, on that I that I did, that I put, sorry to keep skipping backwards and forwards like this. If you imagine the individual points here, right, these are individual Qj's, there's going to be an infinite number of them in the limit, and their structure is this, Rj times Wj, where Rj is positive, and the Wj is this thing that falls in the simplex. Okay, and these. Okay, and these are the points of a Poisson process with some given intensity. They're independent of the angles, and we get a Poisson process in that set with intensity, a given intensity. So these QJ's can represent individual extreme events, storms, heat waves, and so on. And what we can do is to simulate those events by starting off with the largest of the RJs and then Of the RJs and then working downwards. And here's an example of that. Here I've done some Husserler Rice simulations, Husserl-Rice in a particular model, in this case a bivariate model. Okay, what I've done is to take, so this is Q1, if you like, here. One of these is Q2, Q3, etc. And I think I simulated thousands, the top thousand points here. And this is the maximum here. This is the maximum here. Okay, and there, the maximum is the largest observation on the first margin down here. That's this value, and as well as being the largest observation on the second value. In this case here, where I've taken a different dependence parameter, I again simulated a thousand points. This rather strange shape here is because of the log axes. Okay, and this time the two maxima correspond to this point and this point. Correspond to this point and this point. And they correspond, so that we would observe a point here, they would correspond to two different events. So the largest wind at Vancouver is not the largest wind in Toronto, whereas maybe the largest wind in Toronto is also the largest wind in Hamilton in some given year. Okay, so the dependence here might be spatial, it might depend on where you are. Where you are. And the Hussler-Reis distribution, I just mentioned that briefly, that's a natural analogue of the normal distribution in multivariate extremal contexts. It's got a scalar parameter in the bivariate case, got a rather easily manipulated distribution that's expressible in terms of the standard normal distribution. It can give you total independence and total independence as the limits as this parameter goes to. Limits as this parameter goes to infinity or zero. And it's actually rather easily simulated. In this particular case, the w can be generated like this by this rather odd-looking expression here. Epsilon is a standard normal and i is just a binary variable, plus or minus one. And the general expressions of this sort are available for other distributions, for other simulation problems. I'm going to skip this. Yep. Yep. Stop. Yep. Yeah, I think it might be a good point where to stop on this very nice and easy-to-use Hussler Reich distribution so that we can gather some questions. There are maybe a few from the audience or so there are no questions yet on the chat, but again, people on Zoom can raise their hands or people in the room can ask their questions. We still have a few minutes for questions. Okay.