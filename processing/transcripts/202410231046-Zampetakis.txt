So I'm going to talk today about some steps that we have taken towards the right understanding of extrapolation in machine learning and data science. So extrapolation is a very intuitive notion, but just to identify what I will be talking about today, we say that the model extrapolates if it has a small error even when it's tested outside the support of the training. Okay, so we'll have a terminology. Okay, so we have a training distribution that has some support and want to understand how we will behave if we test this model outside the support of this training distribution. And there are many people from many different perspectives looking at extrapolation. We have even seen some presentations here. Yes, we did with Lafrito, and now towards the end, Basilis also mentioned some extrapolation properties. So, this is an example where Where some network is trying to solve some mazes in like 9x9 grids, and then it's applied into larger grids and they check how much the cover behaves. Or this is another example where we train regularly with NMIST, but then we start uh rotating rotating the digits and slowly we make this go outside the the supporter distribution where we use. Support of the distribution with the support. So, in most of these examples, what we see is that neural networks do not have very good interpolation properties. So, they have very good interpolation properties, but they're not so good in extrapolating, at least in a one million way. I'm gonna get in more into that. And there's a lot of uh research on how you can design methods that extrapolate better and Methods that extrapolate better and under which conditions we can extrapolate better. So, most of the work that has been done in extrapolation remains starting from a practical perspective, designing careful experiments and methods to test the properties of the extrapolation properties of machine-like methods. What we're trying to do is start from very simple theoretical models and try to identify what are the important properties that lead to extrapolation. So, this is an example that will come up later. So, we have some almost periodic function. This is the targeted blue function. It's a simple dimension, it's a very simple setting. You use a neural network. It's a realizable setting, there's no noise. You use a neural network to train on this regression problem, and inside the interpolation region, as expected, the neural network has almost zero. Almost zero error. Outside the interpolation region, once we go beyond the support of the distribution that we use for the training, the neural network behaves very badly. So this is one very simple model, this very simple regression, realizable regression problem that we will be looking at today. And towards the end, if I have time, I'll talk about a slightly different problem in statistics, which is instead of Statistics, which is instead of observing realizable data from a regression problem, I assume that we have a density estimation problem and you observe some data, but now the data are truncated, and your goal is to figure out what is the underlying distribution, let's say the mean of the distribution, although you observe only truncated data. Okay, so this my talk will have two parts. My talk will have two parts. I will mostly focus on this part: on how this design transfer in the qualities for regression problems. And if I have time, I want to mention a cool learning theory problem that comes up in truncating statistics with a non-truncation. So more concretely, what is the setting we consider? We have some realizable learning problem where we observe data x, f of x, where x are drawn from some chain. where x are drawn from some training distribution p and f is contained in some function plus f. And then we want to find an f hat that minimizes sorry p should be f star and that minimizes the average error that we get to be f? Yeah, so if if these two are the same, yeah, then These two are the same, yeah, then f hats will be f. Otherwise, or if this is not a population loss, but like it's a Birker loss, then you want to find F hat, then you require F hat to be a member of F? Not necessarily, yeah. But we'll see an example of that later. On this slide. Yeah. The problem is that we don't have access to data from P, we have access to data from some other distribution Q. To data from some other distribution Q. So we have some distribution shift. And what we want to minimize the error with respect to Q, but we only have data with respect to V. So we have this simply distribution shift problem. So we want to minimize this error with respect to Q, but the only thing we can do is minimize the error with respect to P. Question function is the same? Yeah, the imaginary change is the same. Yeah, yeah, only the distribution changes. So, what is the simplest way to deal with this problem? It's a problem that comes up all the time. The simplest way is to write the expectation with respect to Q, to multiply with the density with respect to Q, divide with this density with respect to P, and then transform it into denominator. No, no, no, yeah, yeah, I'm not. I'm not even suggesting though, I said, yeah. I'm saying what people do. So, what people do is like they write down this change of measure inequality, they receive the change of measure inequality. And then, one trick that this relation comes up with, mathematicians call it radon-nicotin derivative, but like in many settings, this is called like the propensity story. Okay, and then we can. Okay, and then we can do like the method that Vassili said of trying to estimate the propensity score. And instead of optimizing the error without this re-weighting, this propensity score equating, we estimate the propensity score, we re-weight the error, and then we optimize with respect to the distribution q. So this is, I mean, in order to make all of this concrete, there are like hundreds of papers that work on that, but like this is a general idea, a general method to deal with this. To deal with this distribution shift problem. Another thing that you can notice is that if this density ratio is bounded, then you don't really need to estimate the proprietary scores, because you can write another bound, this transfer inequality, this transfer measure inequality, that says that I'm gonna pay the maximum ratio between the two densities and I'm gonna just minimize this. And if this has some rate that goes to zero, then To zero, then this would go to zero as well, because if this is finite. It would go to zero, then this factor makes a big difference. Yes. And this factor is sometimes called transfer coefficient. And even if so, this factor, the Ellen Finti normal, the ratio of the density sometimes is a very current upper bound because it could be that point-wise the densities are very far from each other. So you can just apply. So, you can just apply a little bit more elaborate argument, is to apply holders inequality and have here some divergence between Q and P. That could be, for example, for S equals 2, this could be the cache problem. So, you're trying to solve a problem that's initially not solved the information theoretically. Yes. You're just changing the formulation. I haven't done anything yet. I'm just describing, but it's related work still. Okay, so you. So you can apply Holger's inequality, and then you get this transfer coefficient, and instead of taking the only thing we know, we pay this transfer coefficient. And this slide is like supposed to serve as a summary of how people approach this distribution system. Like many papers approach this distribution system from PTQ. Okay. So, the nice thing about this method is that it applies for any function class. We don't make any assumptions before any class. The problem is that it requires that Q is absolutely, at least absolutely, continuous with respect to P. In a figure, what this method says, this change of Mesoamic world says, is that I have here the true data, the data that I'm interested in. It's this red probability distribution, and P is the probability. And P is the probability distribution that I have. And the transfer coefficient is essentially saying how much I have to inflate P so that it contains Q. So this is how much is the error that I'm going to pay. And it's not a very surprising extrapolation. Whenever you can do that, it's not a very surprising extrapolation phenomenon because you still have access to the whole support of the data. It's just that the weights are different. Okay, you have to pay. The weights are different. Okay, you have to be careful to pay more attention to some data as opposed to other data, but the support of the two densities are the same. What I will be interesting is cases where this is not true. So have a distribution Q and some distribution P that has smaller support. Okay, if if P has larger support, it's still good for example, like for a distribution shift. But I don't have some some P that has smaller support. P that has smaller support, so that now there is no way to inflate P so that contains Q. So, this is a case where the duration densities is not finite. And there are some observations, both theoretical and bigger, that say that in some cases, in fact, extrapolation is possible even when this ratio is infinite. Okay, so this is, we shouldn't be hopeless when these ratios are. We shouldn't be hopeless when this relationship goes to infinity. That's the first observation. The second is that now. But but those results are under realizability assumptions. Function f can get to zero. Functions are otherwise. I'll come to that. Another important thing is that now the function class matters if the ratio density goes to infinity. Okay, so an example is if you have here even a threshold function, and the threshold function is function. A threshold function and the threshold appears outside the support of p, it's impossible for me to know to distinguish between these two distributions. So now I cannot have a result that is as general as the change of major inequality because the function class matters. Okay, and then I want to show inequality for a class of functions that is outside this regime, so I want to understand which function class is extrapolate. Function classes extrapolate, have good extrapolic properties. And for that, the notion that appears to be very important is the notion of anti-concentration. Okay, sorry? Anti-concentration. So for that, let me remind you, obviously you all know that. The concentration of meso is usually based on Markov's equality. So the probability that post-drug homerony is greater than terminal is underbounded by the expectation of is undervalued by the expectation over gamma. Now we say that the random variable is anti-concentrated with degree k if we have an inver an upper bound on the probability that x is smaller than gamma. And actually the upper bound looks very much like Markov, except it's the inverse, it's gamma under the expected value of x. And obviously just this race is too much to ask. So we allow for some one over k on the right hand side. And when we're On the right-hand side. And whenever a random variable satisfies this condition, we say that it's anti-concentrated with degree k. Okay? So why I'm saying that I'm going to call a function class anti-concentrated with this degree k with respect to some distribution family t if the error that I observe, the probability that the error is smaller than gamma, is underbounded, so essentially the error So the essentially the error is an anti-concentrated algorithm value. Okay? This is what I want to say. So the probability that the error is smaller than gamma is upper bounded by gamma divided by the expected error over the whole distribution. Okay, so this is the same as going here and saying that this random variable x is the error between f hat and f star at a given point x. So when this error So when this error is anti-concentrated with respect to some sorry with respect to some probability distribution function yeah so these are the classes of functions that I will be considering and it appears that for extrapolation this is this is the the important property due to the functional class. Yeah. It's in some sense yeah I'm not gonna have time to talk about Yeah, I'm not gonna have time to talk about in some sense it's an if and only if not absolutely, but like almost no if and only if I'm saying it it allows to do analysis. It doesn't mean that it is practical. Yes, it doesn't mean that it's practical, but it if this does not hold for the distributions that you're distinguishing, then extrapolation should be possible. And so what are some families that satisfy this? So what are some families that satisfy this? So this is general neural networks, probably. Yes. So what is there some families that satisfy this anti-concentration condition? So polynomials or general left polynomials of the D-at-most gain, they satisfy these conditions, respect this condition with respect to log concave measures. And this is in the formal so some of these uh statements have some details, but like also signs and consigns And consigns with frequency at most K also around concentrated. You added an assumption that the underlying distribution is not okay. That is doing prediction of labeling, then you assume that each label has its own mode and it's not local. No, I'm not gonna do that. I haven't reached the theorem yet. I'm just saying this is a useful property. This is a useful program. Yeah, since equal signs and also polynomials of sines equalsines also satisfy the Randy Concentration. So there are general class of functions, yeah. Is it related to this multiple property in general? Yeah, for polynomials, yes. For ecosystems ecosystems is a little bit different, but yeah, for polynomials it's related. In general, these theorems are very difficult theorems to show. Okay, they're not theorems from most. Their theorems from mathematicians, they use complex analysis, etc., etc. They're very, very technical results to show such theorems. So these inequalities are completely non-trivial. Okay, and obviously threshold functions are not multi-concentrated. So what is the result that we have? Is that if we have some multi-concentrated family of detail k with respect to some family Family, dist distribution family B. Then we know how to transfer the error with respect to P to the error with respect to Q, and we play this transfer coefficient. And what is the transfer coefficient? It has some dependence on K. And then it has these two products. The first is the ratio between Q and μ. μ is a density that belongs to this function class. And this is the ratio of P to μ to the ratio. The ratio of p to mu to the raised to the power k. So I don't assume that p or q are long k. Neither of them. They can be multimodal and fine without. But do you assume that the base optimal belongs to f? Function class f? I do. Yeah, which is a very strong agreement. Yeah, yeah, I agree. That's sort of kind of a good question. And I don't that I don't assume that uh at least I don't I don't assume that each one of them belong to this function class. belong to this function class. So what this inequality says is that if you have such a nice anti-concentrated family and you have this probability distribution P, this probability distribution Q, then all you need to do is to find some probability distribution in this function class that you can inflate it and cover both of the distributions. This is a very surprising design. So it's like, at least on a technical level, I can have good error. I can have good error in this distribution, in this region that I haven't observed anything from, only from data from this distribution, as long as there exists, for example, a log and k distribution that covers both. Yeah, so if the functions are polynomial, the mu here would be log concave, then yeah, what is what would be k? What would be this transfer coefficient? K is the decrease of polynomial. I know, K will be the degree of simple. Yeah, so in particular for polynomials, we have this inequality that like the error of any function f with respect to f star with respect to q will be upper bounded by the error with respect to p times this transfer curpus. And also this product of these densities. Of these densities defines an interesting distance between P and Q. That is not bounded at the race. It's not like some Remy divergence, some chi-squared, or even total variation distance. It has some geometry inside. It's an interesting new notion of distance between chi q that allows you to extrapolate. No, the total version distance here is one. What looks like a hypothesis dependent, social variation, or something like total variation with respect when you measure with respect to degree K polynomials, or something like that? No, I don't think it's more close to Baselstein distance, if anything, but still it's not exactly Basenstein. It has it has geometry inside, because the local cavity uh logo cavity captures in some way captures distances. In some way, it captures distances. It's not only about the weights only. Yeah, it's about the distance. Yeah, it has some interesting geometry and science. Yeah, I'm going to skip the proof. Okay, the next question is, as Jamie asked, can we have a similar result on the records? So we did a very simple experiment which went beyond our theorem, so f star minus not a polynomial. So f star lines on the polynomial. We have this periodic, like almost parity function, periodic parity function. And the distribution, this whole right up is the distribution q, but we add. And we only observe data from this distribution P inside this. And we train polynomials to approximate this. And what we end up is this, okay? We don't have zero error because we cannot apply error result because a star was not. A star was not polynomial, so we're not going to have a zero error outside, but we still see some interval. So, still, this concentration of progressive polynomials gives us some modes that there's no reason to predict with the training data. So, now let's see how neural networks will do. So, neural networks do this. So, in some sense, neural networks that do not satisfy this anti-concentration property. This anti-concentration property, but correctly, they do not satisfy this anti-concentration property because they are not able to extrapolate and see what happens outside. Yes? The method was just running VRM over the polymer. Yes. No, no, no. That's not, yeah, not very important. Yeah. We don't so far, at this part, we don't change the R. Yeah. But what I want to say is that, like, whether some function class, it seems, like, at least in this single sentence. At least in this simple setting, it seems like one important property of a function class to be able to have some non-trivial extra relation properties is this anti-concentration property. Now, one problem that I mentioned is that like this theorem only applies if this error with respect to p can only can go to zero, then we imply that the error with respect to q goes to zero. But if we have noise, for example, If we have noise, for example, then the error with respect to B goes to some constant, then what this inequality says is that, okay, the error with respect to Q will be bounded, but not small, if we just do the n. I want to say that although this is not what we want, it's still a non-trivial problem to prove. Okay? And this leads me to the second result, which I'll go through very quickly, where we have a Where we have, as I said, the truncated density. We want to estimate the mean of this truncated density. And this is exactly a problem where if we look at, this is the, so the distribution from which we observe data is targeted Gaussian distribution with respect to some set S. And the error is not sorry. The error is not going to go to zero. It's going to go to the variance of uh uh of the of the Gaussian distribution. Of the Gaussian distribution, of the truncated Gaussian distribution. But what we want is to minimize the variance of the untruncated Gaussian distribution. But we don't have so much. So a theorem that we solved, so now you cannot simply do your energy, you have to do something more clever. So one theorem that we solved in 2018 is that actually there is an algorithm for this Gaussian problem that extrapolates and lengths the mean of the Gaussian distribution. And interestingly, And interestingly, an integral component of this was some version of the inequality that I showed you, the anti-concentration property. So still, this anti-concentration property is very important, even if you need to change the objective function to do something. Very quickly, I'm going to go to this recent unalterration problem, which we have explored also back in 19, but we visited this year. But we visited this year. And the goal is first to learn the tragation set, the survival set S, and then use the algorithm that I mentioned if you know the tragation set. And the interesting learning theory problem that arises is that we try to learn some set S, so we get data that are targeted on some set S. We want to learn S, but we only observe some P spin S. So we only get. So we only get, there is no way to get negative samples. We only get positive samples from this. And in general, learning from positive samples, it's like a challenging problem. So Madhavazan in 1977 saw that most interesting classes will not be reliable from positive samples. Like even circles are not reliable. You can see that. Dynophyll, you can see that we observe this positive data, but we have no idea if the underline distribution was, if the underlying concept was that, and these are the data that we're missing, or it's this, and these are the data that we're missing. It's impossible to learn, to identify between the two. Okay, so what you do, one very popular alternative is to use some unlabeled samples. If I have Labeled samples. If I have some positive samples and some samples that are unlabeled, then I can do the following trick. I can label all the unlabeled samples as negative, and now I can solve the corresponding agnostic learning problem. Because I know that in the regions that the samples are positive, the density of the positive samples is much higher. The samples is much higher than the density of the negative samples. And outside this region, I get the negative samples that I was missing. So now it becomes a learning problem that is feasible. And again, very high level, of course, we don't have unlabeled samples because this is what we're trying to estimate, the underlying Gaussian distribution, but this is where our transfer inequality is now important because we can generate some samples that are not exact. Are not exactly from the Gaussian that we're looking for, but this first, this application of this transfer inequality gives us a very good warm start. It gets some unlabeled samples that have some non-trivial connection with the two data. It's labeled from positive and unleveled samples. You need to know the ratio of positive to negative data. To negative indicator. Yes. And that's the information that you got there. Yes, so what we solved, you only need an LOL bound on that. So we assume that if you only observe a measure zero set, then you cannot do anything. But if your set is at least 1%, then I don't want to know more about that. And this was not even appeared in the original Bengal's paper. But we saw some negative related examples in order to estimate. In this particular case, I know that the distribution is Gaussian. I apply the original transfer inequality and get some estimation. It's not a good one, but it's not also very random. And I get the risk of that. So we prove a stronger version of Densis theorem that says that even if the label data are not from the true From the true distribution, but there are from some distribution that is somehow bound, it's related to the true distribution, then that is enough to solve the modification of the agnostic learning problem. Yes. Okay. Yeah, that's what all I want to talk about, and more details about the same. I want to talk about and more details about the second part you can find till the presentation next week at Fox. Thank you for that.