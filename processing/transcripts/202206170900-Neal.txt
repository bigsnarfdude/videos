It's a pleasure to be invited to talk here. I should say that this is joint work with Frank Ball, who's also logged in via Zoom. So what I want to talk about today, sorry, just get is two modelling questions that we've been looking at inspired by the COVID outbreak, and in particular, sort of looking forward. Sort of looking forward as this workshop is to the next pandemic. So, the two questions: first one is: well, fairly simple one in one sense to state, how many people are infected? Well, if we have complete observation of an epidemic process, then we know exactly how many people are infected. But what can we say about how many people are affected, or more particular, about the distribution of the number of people infected? Of the number of people infected, given just partial information, and sort of the initial starting point for this was, well, when should we impose or lift control measures? So a very simple sort of feature to mention about epidemic models, which I'm sure most of us are aware of, is we've heard a lot of talk about the R number, certainly in the UK, and I think globally, you know. Globally, is the basic reproduction number, reproduction number greater than one? If so, the epidemic is growing. If it's less than one, the epidemic on average is decreasing. But as we, or many of us will be aware, that you can have an R number just greater than one. And if you have just a single infective, then it's quite likely that that infective won't affect anybody else. So we want to make use of the information early on, control media. Use of the information early on, control measures are expensive. So, we want to know that the epidemic is likely perhaps to take off if we don't do anything before we intervene. And also, you know, when should we lift control measures? When do we feel that we've got a disease under control? So that's the starting point for the first question. The second question to look at is in the UK and across a lot of the world, restrictions will. Um, restrictions were put on the size of groups that could mix. So, in the UK, the rule of six was that up to six individuals could meet in a group initially outside, but no group meetings greater than that. So, what we thought of there is: well, actually, if we think about most epidemic models, it's discussed in terms of pairwise interaction. Pairwise interaction. We have a susceptible and we have an infective, and we say, well, at what rate is an infectious individual making contact with a susceptible? And then there's typically some chance of infection. But if we look at outbreaks, and certainly if we look in the UK, going back to say March 2020, one of the reasons for One of the reasons for an initial spike in Liverpool is has often been attributed to a Champions League match between Liverpool and Atletico Madrid with quite a few Spanish fans coming over and in a sense creating a super spreader event where we've got a lot of susceptibles in contact with a lot of infected. So how does actually allowing mixing to be based upon groups rather than On groups rather than just these pairwise interactions. So, where maybe typically a lot of the interactions will just be pairwise, but we allow for different size of groups. So, I'll talk more about the first question and say a little bit about the second here. So, before I talk about that, just sort of talk about the model that we'll look at in both cases. So, we can give a general description. We can give a general description of the model, and I will stick with the Markovian case in particular to keep things simple. Sometimes I will make the restriction that I'm assuming it's time homogeneous, that the parameters are not changing over time. But for a lot of what I'm talking about, it's fairly straightforward and we'll see general results in the time in a homogeneous case. So we can allow rates to change with time. So, I'm going to start off by assuming I have a population size n, and this might or may not be a closed population. Certainly, when we talk about the rule of six, we'll sort of discuss it in the case where it's closed, but it'd be fairly straightforward to extend it to an open population. And certainly, when we're looking at the distribution of the number affected, again, allowing for an open population rather than a closed one would be a fairly straightforward extension. Be a fairly straightforward extension. So, the first thing I'm going to do is talk about the infection process. And often, when we're talking about epidemic models, we talk about infectious contact rates. What I want to do here, and in particular related to the second question about the rule of six, is sort of break that down. So, I'm going to describe events, and events are where individuals can gather together. I'm assuming they're going to. Together. I'm assuming they're going to occur at some rate lambda, which could depend upon time. And I'm just going to, for convenience, scale that by the number of individuals in the population. When we have an event, when people gather together, we're going to assume that these are distributed according to some probability distribution C, where C is at least 2 and is an integer. Because if we're thinking about Integer. Because if we're thinking about events as opportunities for infections to pass between two individuals, we need at least one infective and one susceptible for anything of interest to happen. And then given we've got an event, I'm going to assume that within an event, an infectious individual has probability pi C of infecting somebody else who's a susceptible individual. Now, this could depend upon. Now, this could depend upon the group size. If I'm involved in a group of size two, just a pairwise interaction, I probably have more close contact than if I attend a football match and there's thousands of individuals there. But just to keep things simple for this talk, I'm going to assume a constant probability high. So, as talks about, if you like, the infection side of things and the sort of the special case, if we just allow The sort of the special case: if we just allow C to always be equal to two, all groups of size two, then we have, if you like, a common standard infection rate beta t, which will just be two times lambda t times pi. So we can always just reduce this to the pairwise interaction model. And the other side of things, we're again going to have our recovery rates, removal rate, gamma t. And again, Gamma t. And again, as with the infection process, we're going to allow this to vary with time. Okay, so going to come back, if you like, to my first question. So what can we say about the distribution of how many people are infected if we just observe part of the information? So here I'm going to just assume that we've got a sort of standard pairwise interaction. So I'm going to substitute. Interaction. So, I'm going to subsume all those infection and interaction properties into a single infection rate beta t. We've got our removal recovery rate gamma t. And I'm going to assume that when somebody is removed from the population, that there's a probability they're detected DT. And again, this can depend upon time. The most of what I'll talk about will be in the I'll talk about will be in the special case where we assume all removals are detected, but a lot of the results we generate extend over to where we just have partial detection of the process. So we can easily define sort of quantities of interest. The R number here will be time varying and it'll just be the ratio of the infection rate to the removal rate. And if, for example, we might alter. And if, for example, we might alternatively want to focus on the growth rate. So the difference between beta t and gamma t. So say what's the situation we're going to specifically look at is, well, what can we say about the population, and in particular, how many people are infected if we just observe the removal times of detected individuals. So, how we're going to go about this is, as I mentioned at the start, sort of our starting point or sort of main interest was, well, when should we introduce control measures? When should we lift them? And these are perhaps more pertinent questions early on in the epidemic. Once the disease has taken off, you can act, but it's probably going to take a lot of effort compared with doing that when you just see a few cases. When you just see a few cases. So that would suggest that we're in a regime where we might look at a branching process approximation, where we take that each infectious contact that an individual makes is successful. So we're modelling infections and coupling them. We're linking them with births in a birth-death process, and then naturally deaths corresponding to removals. Responding to removals. So, this assumption that all infectious contacts are successful. Well, we can make the sort of usual branching process type approximation argument. This is going to be pretty good if we've got a large, mainly susceptible population and under the assumption of homogeneous mixing. But because we're allowing the infection rate, The infection rate and in our approximation, the birth rate to vary with time, we can actually use this model a bit more generally. If we know, for example, that a certain proportion of the population only are susceptible, either because they've already had the disease or been vaccinated, we can essentially replace our birth rate beta t by beta t tilde, which is Tilde, which is the infection rate times the proportion of contacts which are going to be successful, i.e., with susceptibles. We typically won't know the number of susceptibles, but at least we could perhaps approximate it. So I'm going to look at this time inhomogeneous birth-death process, birth rate beta t, death rate gamma t. So here I'm taking the case. So, here I'm taking the case where all the deaths are detected. So, in particular, what I'm going to look at here is: given I've just observed a set of removal into removal times, inter-death times. So, I'm going to observe the first death. I'm just going to set the clock equal to zero at that first death. Then we'll wait some period of time and the second death will occur. And that interval. Will occur and that inter-arrival time will be t2 and then the time from the second to the third death t3 and so on. And I'm just going to let s k just denote the cumulative time at which the kth death occurs. So the question we ask is, well, let's look at xk, where xk is the number of individuals or the distribution of the number of individuals alive, I should say. Individuals alive, I should say, immediately after the case death. I'm going to look at how this depends upon these inter-removal times of the deaths. We can look more generally at this process, so we can look at the process in between death times, but for the sake of brevity, I'm just going to look at what happens at the death times and what we can say about the size of the population or the number. Size of the population or the number of individuals alive. So I'm going to make one assumption to start with, and that is to say the disease will be introduced into a population, and then how many people are alive when the first death occurs. And I'm going to assume that this follows a geometric distribution. So immediately after the first death, if the first individual alive is not given birth, Individual alive has not given birth to anybody, the population will have died out. We'll have nobody alive. But if they've given birth to individuals, we'll have a certain number of individuals alive. Now, note here, we're making no assumption about when the initial individual was born, entered the population. So, I'm making this assumption that we've got a geometric number of individuals. Number of individuals. This will be certainly the case if we start off with a time-homogeneous process. And this might be quite reasonable for many diseases until we actually observe a case. You know, behaviour in society is going to probably carry on as normal. We might, you know, hear rumours of disease as with COVID from elsewhere. But until we actually have cases, perhaps in the town, Perhaps in the town, district, country we're living, behaviours are not likely to change. So it's not an unreasonable assumption. More generally, with a lot of epidemic models, birth-death process type behaviour, we often see that the number of individuals alive at the first detection or first death does follow geometric distribution. Geometric distribution. And we can make some changes to the arguments in other cases where this doesn't hold. So, assuming we start off at the first death with a geometric number of individuals, what can we say about the number of individuals alive at subsequent deaths? And nice thing we can say is that actually the number of individuals alive, given we've observed. Number of individuals alive, given we've observed k deaths and given the inter-arrival times between these k deaths, just follows a mixture of negative binomial distributions. So we can express this as a negative binomial, which has its two parameters, rk and pi sk. So what are these terms? So rk is just going to be a random variable which has support on 2 to k. On 2 to k. So, for example, when k equals 2, r of k is just going to take the value of 2. And each time we observe the death, the range of rk, so the number of components which can form part of our mixture, just increases by one. And the probability mass function of Rk will just be determined by the inter-arrival times of the deaths. The depth. And whilst I'm not going to sort of pick up on what that distribution is, what we can do is write it down in quite a nice, simple sort of matrix form. The other part of the negative binomial is the success probability, if you like, the pi pi sk. And this will just be given by pi t, which solves the ODE, which is given at the bottom of the slide. Which is given at the bottom of the slide. And you can write this ODE in a number of ways. And I've written in a particular way that in the square bracket here, we've got the ratio of gamma t over beta t plus gamma t. So this is the probability that if we have an event at time t, that that will be a death minus pi of t. So for example, if we have a time. For example, if we have a time homogeneous case, then we'll have a constant pi t, and that will just be equal to the probability that an event will be a death. So, how do we go about proving the theorem? Well, most of the, if you like, the hard work is looking actually what Like the hard work is looking actually what happens between the first and second death. So, what we're going to do is say, well, what can we say about the number of individuals alive or the distribution, given that it's now time tau? So remember, we're taking time zero to be the time at which the initial individual that we observe dies. What can we say about? What can we say about the number of individuals alive, given we've not seen any death from zero to tau? And then the second part is saying, well, what about if we observe a death exactly at time tau? What does that tell us about the number of individuals who are alive? And once we've got that, then it's actually quite straightforward to essentially iteratively look forward from if we know the distribution. From if we know the distribution at the k minus first f to describe how many individuals are alive at the kth f. So what I'm going to do just to start with is suppose that we have a single individual at time zero and look at what happens in that case. So I've just defined a bunch of things here, but But we only need a few of them. So, the first thing is we're just going to keep track of how many individuals are alive, why one tau. As I said, I'm interested in what happens given there's been no death up to time tau. So I'm just going to define an event EE tau, just to denote no deaths have occurred. I'm going to define a quantity phi. Define a quantity phi, which is a function of t and tau. And basically, that is just going to be the probability if I have an individual that they have no births and they don't die over an interval from t to t plus tau. Nothing happens with that individual. And for psi, well, I'll describe how that arises in a minute. So, given this information, what we can do is What we can do is show that actually the number of individuals alive at time tau, given we've not seen any deaths, will just be one plus the geometric. So our initial individual plus a geometric which has a parameter which depends upon the sign. I thought it'd be just good to see a little bit how this arises. Arises. So we start off with one individual. So here I've got time zero at the bottom, time tau at the top, and my initial individual has lives throughout this time period. We've not seen any deaths. So what I'm going to do is start at time tau and work backwards. And I'm going to see if my initial individual has any offspring. So I go back a few units at a time and I find that they have a child. Time and I find that they have a child. Now, I don't want any deaths, so I want to calculate the probability that they also live from the time of their birth up till time town. So, we've seen one birth, but now we're back in a situation similar to before. So, we can take the red lines as denoting the life histories of people that I've explored, and the black lines as being life histories. Black lines as being live histories I've not explored. So I can then look at this child and again work back in time and see if they have any children. So I find sort of halfway through their lifetime so far, they had a child and do I want their child to survive to time tau. And we can repeat this process. We start at time tau and we essentially And we essentially work back until we find somebody who's given birth. So now I look at this last individual to be born, they have no children. I then look back at their parent, see if they've got any more children. Then move back even further to the initial individual and find that they have another offspring. Again, we want Again, we want this person to side to time tau. And now finally, we have a situation where if we look at this last individual, they have no children. And also, our initial individual had no children. So what we can see is each time we observe a birth, we basically reset things. And this leads to the geometric distribution that we had. That we had. So, in particular, the key things we need to put together are the probability the initial individual survives to Taiwan Tao, which will just be an integral of their survivor function. The probability they don't have any offspring. And then this Pasai term is simply the probability of an individual having at least. An individual having at least one offspring as we work back. So we look at their most recent offspring from time to, and that that offspring manages to survive to the point of interest to time tau. So we can put this together to get the joint distribution of the population having reached n individuals alive and seeing no deaths. And then once we condition upon And then, once we condition upon no deaths, we get one plus this geometric. But the situation I described at the start is saying, well, actually, at that first death, we don't have just one individual in the population. We have a geometric number. We might have nobody left alive, as the initial individual didn't have any births. Or we could have lots of individuals alive. So, what can we say about So, what can we say about how many people, the distribution of how many people are alive, if we started off with not one, but a geometric number of individuals? So I can now just define y of tau, and I've now put a subscript g naught, just in note we're now starting off rather than with one individual, a geometric number of individuals. And then I've got a function h, which depends upon t and tau. I'm not too bothering. I'm not too bothered about its particular form, but just to say it's going to be a probability. So it's going to range between zero and one. When tau equals zero, it'll be one. And as tau tends to infinity, it will turn down to zero. And then what we can nicely show is that actually the distribution of the number of individuals alive, given we've not seen any death since the first time. Death since the first death now just follows a mixture distribution. And that mixture is a geometric. And that the success probability of that geometric is pi tau. So the solution to the ODE we've just given with probability determined by H. And with the remaining probability, we've got a point mass at zero. So one way of thinking about this. So, one way of thinking about this is we're getting more and more weight on point mass at zero the longer we wait for death. So, if I know that I've got a random number of individuals alive at time zero, the longer I wait, the more likely it is that I start off with fewer individuals. So, I had lots of individuals, then they'd either give birth to lots more people, but there'd also be an increased chance of death. So, the longer I'm waiting, the more and more likely it becomes. The more and more likely it becomes that actually what I started off with was nobody alive in the population. So now let's actually have a look at the second death. So if we look at the second death, so just before time T2 at which the second death occurs, what can we say about the distribution of how many people are alive? Distribution of how many people are alive. Well, it's a mixture of a geometric g tau and a point mass at zero. But the moment I actually observe a second death, essentially what I'm doing is ruling out that there was nobody alive. There must have been at least one person alive. So that's telling me, essentially, that there was a geometric. But next, I'm learning about, you know, if I've got one person alive, If I've got one person alive, the probability or the rate of death is going to be gamma t. If I've got n people alive, the infinitesimal rate of a death is n times gamma t. So I'm more likely to observe a death if I've got more people alive. And so what we end up with is size biasing or size bias sampling from this geometric. So actually, rather than end up with the geometric, we're With the geometric, we end up with a negative binomial. So a negative binomial two and the same success probability, pi tau two. And this negative binomial could just simply be written as the sum of two independent copies of the g terms, of the g round of variables. In particular, if we go back. In particular, if we go back to where we started off with talking about the number of individuals alive as a mixture of negative binomials, this is the case where we've just got R2 equals two. So suppose now that we've arrive at the time of the cave. At the time of the k minus first death, or just after it, and we've got a negative binomial number of individuals alive. Well, that's the distribution given the data we have. And suppose that this archive rk minus one is equal to j. So obviously we have a distribution of it, so let's actually have a look at what's the case when it's equal to j. Now, what we can do. Now, what we can do is think about the population if it's made up of so the negative binomial with parameters j and some success probability pi t, will just be the sum of j geometric random variables. So what I'm going to think of or describe it as is that we've got j family groups, and each family group contains a random number of individuals given by a geometric. So this geometric A geometric. So, this geometric might have zero, so we might have that a family group is empty. More generally, there's going to be a random number of individuals. Now, each of those are going to evolve independently over time. So, within each of these family groups, there'll be births and deaths. So, we have that the next death occurs at TK. Now, that death. Now, that death will be associated with one of these family groups. And in that family group, we know that if we start off with a geometric number of individuals, we can essentially extend the argument we had on the previous slide to show that immediately after the next death, that the size of that family group will now form a negative binomial with parameters 2 and pi sk. So we can think about that as one family group. That as one family group splitting into B2, just the sum of two geometric kind of levels. So, perhaps a slightly counter-intuitive thing, that each time we see a death, our process is going up. But the counter that is, well, the more deaths you observe, the more it tells you that the epidemic, or in this case, the birth-death process has actually taken off. There's lots of people alive, increases the chance of seeing death. Increases the chance of seeing death. So now let's have a look at the other J minus one family groups. So these ones don't experience a death between the time of the last death, SK minus one, and the current point. So what's happened with each of those family groups? Their distribution is going to be a mixture of this geometric, i.e., we can think of that as being a single family group and a point mass at zero. A point mass at zero. We can think of that as that family group having then died out. So, putting this together, what we have is that if we know the value of rk minus one, the size of this negative binomial parameter, then rk will just be two, and that two comes from this group which gave the death, plus a binomial, which Plus a binomial, which will just have parameter J minus one, the other J minus one family groups, and the probability essentially of survival of each of those family groups, which will be given by this probability H. So what does this thing give us? So I've talked a bit about the details, and now what I want to do is say a bit about what it might give us. So it allows So it allows us to compute the distribution of the number of effectives at time t, given just the removal data. So interchangeably using removal in the epidemic context with death in the birth-death process. It allows for changes in infection rate, and that might be due to implementation of control measures. It might be due to the deployment. Due to the depletion of susceptibles. So, going back to the scenario I started off with. And it also allows for changing removal rates. So, if we have increased surveillance, increased testing of individuals, we might be able to pick up those with the disease generally quicker. So, that would lead to an increased removal rate. And also, if we're incorporating the idea of detection, The idea of detection, then often with it in the initial stage of the disease, many cases might go undetected because people might attribute a disease to something else. Once we start looking for disease, we will naturally like see more cases. So all of these features could be incorporated. I've talked very much about sort of the probabilistic side of things, but from a statistical perspective, But from a statistical perspective, a nice property of this is it's very easy to write down the likelihood of observing a particular set of removal times. And we can do this without having to impute, do data augmentation for the infection types. So from a statistical inference perspective, this model has nice. Has nice setup. As I say, we can allow for partial observation of the death process, and we can allow that in a time-varying manner. We can't allow it in the full generality that I've described here, but certainly we can write down corresponding theorem to the one that I gave. Again, in terms of a mixture of negative binomial, if we assume, for example, Binomial: if we assume, for example, the parameters are piecewise constant between each detection. The final thing to say is we can write down the distribution of the number of infectives in the general stochastic epidemic model, sort of corresponding to this birth, you know, the birth-death process approximation. Death process approximation. We can do that exactly, but it ends up being more computationally intensive to calculate. Rather than end up with this nice mixture of negative binomials, what we end up with instead is essentially comes down to matrix algebra. We're looking at transitions of a Markov chain, straightforward enough to write down. Enough to write down, not so easy to sort of get an to analyze and get a full understanding of. For those of you worried about my timekeeping, I note that I'm moving perhaps a little bit more slowly than I thought. So I'm going to just focus on this first question and just go through, finish off with a few examples to sort of show what we've learnt. Show what we've learned in practice and actually implementation. So, the simplest thing we can do is just look at: okay, let's suppose we have a time homogeneous model. Here I've just taken the birth rate to be 1.5 and the death rate to be 1. So, we could think from an epidemic context, we're just taking our unit of time to be the mean length of the infectious period. So, here. Infectious period. So here the black solid line just represents the number of individuals alive over time. And what we have in the shaded area is just the 10% to 90% quantiles given by an estimate of the number of individuals alive over time. And the dashed curve, sorry, the dashed grey line in the middle of. Dashed gray line in the middle just represents the median. So, all we can see is that we have quite good tracking of the actual growth of the number of individuals alive in the population. We can extend this to the case where we have only partial detection of deaths. So, here I've looked at exactly the same data, but only detecting on average Be detecting on average one in four deaths. So each death, basically I toss a biased coin and with probability of quarter, I observe it. So we have a similar path. One thing we can note is that my estimation starts at about time one rather than time zero, because the first couple of deaths we don't observe. And what we can see is we again get good coverage of the Good coverage of the growth of the population, but we have wider quantiles, and that comes down to we've observing less of the process. The second example I want to look at is sort of going back to my starting thing, starting point. Well, what about if we have control measures? So, we'll look at the previous sort of case. I'm going to let alpha n be my birth rate without. My birth rate without any control measures. I'm going to set that equal to 1.5. When we introduce control measures, we reduce the birth rate to 0.5. And again, we're going to just take constant death rate equal to one. So we're super critical without control measures, subcritical with control measures. And what I'm just going to do is introduce control measures once my estimate. Once my estimate of the number of infectives hits four, hopefully then the disease will come under control. And once my estimate of the expected number, infected or alive, gets down to one, I will lift control pressures. So we can introduce this. So the black line, solid black line, shows the true number of individuals alive. Number of individuals alive, and the black dashed line shows my estimate of the expected number of mean number alive. Well, what we can see here is that it will jump up at times where we observe deaths, and in between death times, it will be decreasing. So, we'll introduce control measures if we do, immediately after a death, and we'll lift them when we've not observed. Them when we've not observed a death for a sufficiently long period of time. And we can also, in this case, look at how pi t varies as we first of all impose control measures and then lift them. So the black dashed lines represent are at two thirds and two fifths, and those are the probability of an event being a death in the case. A death in the case of control measures in place for two-thirds and control measures not being in place for two-fifths. So the final, very final example I want to look at is, well, actually, how good are we at, for example, capturing the behaviour of an actual epidemic outbreak? So I've taken a population here of 2000, an epidemic which infects 829. Which infects 829. This was generated with I've reduced beta a little bit from what we had before to 1.25, and gamma equals one. So look at three estimates of the number of individuals alive. So the first one is calculated exactly and is using the general stochastic epidemic model based on the fact that we only observe the removal times. Observe the removal types. The second approximate, and then use an approximation, just the time-homogeneous birth-bath process. So I just take beta t equals 1.25, gamma t equals 1. The final thing I do is just to use a time in homogeneous birth-death process. So where I allow the birth rate to be modified by an estimate of the number of individuals who remain susceptible. And so, this is well to finish. So, here we have black dots which show the epidemic over time from the initial case or the initial removal till its full completion, sort of 35 time units later. The red line is the exact calculations using the GSE. Is in the GSE. The green dashed line, which is indistinguishable till about time 10, is the birth-death process approximation, which tends to overestimate the number of infectives. And the blue one line is based upon just using a quick, simple estimate of how many people are still susceptible. And this gives a very good, close pattern following the actual. Pattern following the actual true number or the exact distribution given by the GSE for the numbers infected. Okay, so I'll finish there and very happy to take any questions. Thanks, Pete. Questions, comments for Pete? Yes. Hi, Pete. This is Greg Rompala. I have a question regarding the sort of quality of the birth and death approximation. So generally speaking, the birth and death approximation will work up to certain time, depending on the population size, that will be like a square root of the total size of your initial population. So how does this factor into your approximation? Does this factor into your approximation, or doesn't it? So that fact, that's a good question. So that sort of factors in here. So the green dashed line is based upon that standard branching process approximation. So it is sort of departing approximately. So what have we got here? 2,000 individuals. So, you know, 40, 50. You know, 40, 50 infections in, we're starting to see a departure. For this time-inhomogeneous approximation, we're essentially using a second approximation for how many individuals are still susceptible. So unfortunately, I've not been able to sort of quantitatively actually see how good an approximation it produces. It produces, but it certainly looks to give a good approximation. And when I looked at the difference at no point in time over the whole trajectory did it differ in the mean estimate by more than one, in fact. Hi, Pete. This is Oda Dikman. I'm wondering whether you did any experiments. Wondering whether you did any experiments where you generate data with an epidemic model that doesn't fit your assumptions and then do the estimation the way you do it. In particular, I'm thinking of an infectious period that is like constant of duration rather than anything exponential. That's a really good, really good question, and something very much looking to come on to. So, here, and what I've talked And what I've talked about, and actually, most of the work we've done so far has been very much on the model. But we have, as I mentioned, the likelihood, wanting to do inference and see how it works for real data. And that is something I want to test out. One thing I should say is we don't get such nice results, but you can show that Show that for more general infectious period distributions, you again get this mixture of negative binomials, and the copy and the constant one is one where you can actually make progress. There's essentially, let's say the infectious period is just one unit of time, then you know everything at time t, you know everything about the epidemic up to time t minus one. Has anybody infected before then will have recovered? Would have recovered. And yeah, it's a very good point. That's something we do want to explore further. Any other questions or comments for Pete? Oh, there's one online. Please go ahead. Hi. Francesco. I'd like to know if you thought about showing the results for when the rates rather than being And the rates, rather than being functions of times, are like random variables themselves. So that if you have a distribution on the rates that you maybe are not aware of, if you can generalize your work to that case. That's something I've not considered at all. I think it's a good point. So yeah, here we've just taken them. Yeah, taking them as being known. If they were themselves a stochastic process, yeah, that'd be an interesting thing to look into. Thank you. Thanks. So one, any last question? I don't know. Okay. This is Eben Kana. This is a really interesting. This is a really interesting presentation. One of the things I noticed is the examples that you showed all have relatively small numbers. Could these calculations be done with hundreds of people or thousands of people or anything like that? Or would there be an approximation that would be useful? That is something I have looked at. So I mentioned, you know, we can do exact calculations for the general stochastic epidemic. So for the plot I'm showing here, for the exact general stochastic epidemic, it took me in our, just on my laptop, about three or four hours to actually do the calculations. For the birth-death process approximation, it took me a couple of minutes to plot things. So, certainly, you know, on that level, a discussion scale. Have also looked at a sort of further approximation of the birth-death process which does scale or potentially scales linearly with the number of effective. So, here I mentioned about the distribution of RK. Basically, each time you observe another death, time you observe another death to get that distribution you're you're timesing a matrix of length k minus one so effective length k minus one by a matrix which is k by k minus one or k minus one by k whichever way around so you you know it gets slower the more the more individuals you have um so certainly the best death approximation is is Is plausible for bit bigger populations, but also looking at and have done some calculations, certainly from an inference point of view, using a further approximation which is scalable. Okay, thank you. Okay, so thanks, Feed, for the wonderful talk. Let's now. 