This wonderful workshop. It is my first time in the years. Not the first time in Bang, but I visited this in summer a few years ago. So a completely different scenario. Trying to enjoy tomorrow some of the scenario that we are going to go to some hiking. Okay, so I'd like to talk today about two projects that we have been involved. The first one, both of them relate to internal sensor covariance. And by the time that I started And by the time that I started working on that, it was with Professor Steve LaGracos, who passed away unfortunately at a very young age. And this was in relation to this HIV AIDS who we had traveled that I will tell you in a minute. And this was back in 2003. And we balanced this with him and with my very friend and love colleague Anna Spinarco, also passed very, very soon, too soon. To SON, and we published this in the Statistics and Medicine. And then one of my PhD students, Vereka Todd, we continued in this work. But it seemed that nothing was moving after that. And of course, we had other things to do. And then, you know, 10 years later, we received a mail from a person that was interested in our code. But the code we had in 2003 was, okay, I can only remail from here. And so I had this colleague, Klaus Landworth, my I had this colleague Klaus Lambert, my very, you know, one of my most closest colleagues, Klaus. And then we said, when don't we just put anything in R is the master in R. And so we did that and we complete some of the simulations that work on that. And we published this in biometrical journal. But this was 2014, 10 years ago. And again, nothing happened. Meaning that we couldn't find other persons interested in having these co-parates. In having these co-parades that were internal sensor. And so, you know, we had other things in between, we were engaged in other projects, and life was on, and this was the end. Until two or three years ago, that this person, Maria Marguenta, she was doing the PhD in the nutrition department in the University of Barcelona, next to us. And then she had this problem with limit of detection, limit of quantification, and I'll tell you a lot. She was looking, she didn't know how. She was looking, she didn't know how to solve, how to analyze, and then a guy that was in our group drew her and said, Oh, why don't you talk to Lupe? Oh, she's doing this and that. And then she appeared with that, and the difference, one of the main differences that her covariates are not tied to events, are just karotenauts. I'm going to tell you about that. And then we thought, okay, can we do that? I mean, can we just adjust the same methods? What is the difference? And that's where we thought that, you know, the linear model. Thought that the linear model was not the right one, but we had to just move to a general linear model. And that's what you need. And so I'm going to try to explain these two parts. The first part is sort of close. Of course, there are always things up here. So, you know, happy to share other things. But the second part is the one that we are working on. Okay, so with the first part, it refers to this ACTJ, H Clinical Trial Group, Protocol 359. Title Group Protocol 359 that was at Harvard University. And the problem, okay, I have to look at that. Okay, so this was so at that time, because this was a few years ago, they were still platting to have a good treatment for people that had HMV. And they had this group of people that had had many different antiretroviral treatments and they were failing. And in particular, there was one treatment which was protehas behind. Proteas inhibitor, proteas inhibitor, what is called Indi Labir, that some people were also famous. So they need to rescue them with other treatments. And so the study population they had was people that had failed to different treatments and in particular to individual failure. And then the clinical trial would consist in following up these persons and continue doing the treatment that. The treatment, the new treatments that they had at the time. But this was prior basically to starting the new treatment, and they were really interested in knowing so that with this information they could adjust the following analysis, knowing whether those that had a waiting time since they failed in Intina V until they were recruited, if this had an influence in the baseline coverage. You know, in the baseline covirus, in particular, in the viral load that they had at the normal. Okay, so as you can see here, in this here, so this would be the viral load, this is time, okay? And so the viral load is supposed to be below 500, but for some people, because they were in different treatments, but they were failing, and failing to indin a gig meant that at some point these individuals crossed this threshold and were above 500. This threshold and were above 500. So they were not doing good in terms of viral load. Okay, so this would be, and why here is going to be our response, which is the viral load. So this is going to be time at enrollment. And so what we want to sort of associate is the time from indignative failure, which is this point here, up to the enrollment, and relate this to the level of viral loss. Okay? So Okay? So what happened is that by a lot is only check uh not periodically. And so for these persons, you had, you know, this is 500 and so on, that's an instance. So our random variable is going to be in between L and R, okay? And so, but we don't know where, and that's because, you know, this person was okay in this checkup. It was still okay because it was below 500 in this checkup, but then in the next. This check up, but then in the next time that they were analyzed, it was above 500. So, for this person, you have this value, but it would be C ta sub L, and this that would be C ta sub R, which are the ones where the failure happened. And so the time to enrollment is going to be the time from this that you can see here, the CL, this is going to be the lower limit, and this part over here is going to be the upper limit. Okay? And so the model that we're presenting at the beginning. And so the model that we're presenting at the beginning, and it was very standard, it was just a linear model, okay, with normal approach. Okay, so that's too busy. I'm sorry about this slide. I don't know how familiar are you with the use of non-informativeness in interval sensor. I cannot extend on that. We have a lot of work done with my colleague Ramon Miguer and Manu Calle. And the first one is in Canadian Genome Statistics, but there are many others that are a little bit. But there are many others that are a little bit older. And we continue doing this. It's quite very theoretical stuff, which are the non-informative conditions that allow you to split the livelihood and to use the livelihood, what we call the simplified livelihood. But anyway, this would be the contribution of an individual, the probability that the Y say the value lot is a certain value, Y to Y, and another covariance have a. And another covariance have another set of values, and zeta, which is this waiting time, is in between zeta l subine and zeta r subine. So this probability can be written as the integral of this joint probability where here zeta is equal to s. And so by just taking the conditional density of y given x and zeta, you can write down the likelihood for all the individuals as this product of this integrals of this conditional density. And so what we have here. And so, what we have here is two types of things that interest us. We are interested in the W, which is the distribution of the waiting time, and we are interested, of course, in the parameters theta, which are the parameters of the linear mode. Okay, so once we arrive here, and you will see what we're doing later, once we are here, say, okay, what do we do? Because these intervals, I mean, how do we know what is the I mean, how do we know what is the value between that? And so we decide, okay, let's discretize. And this was to take the fast, easy one. Okay, so let's assume that we discretize the different points where the covet can have values. And I call this as 1, SM, which corresponding mass is W1, WM, and then I can write down this likelihood, or better the log likelihood, as the sum of the log. And here, what you have to introduce is this indicator eta. This indicator eta that, as you can see here, is telling you whether the individual i, the corresponding sj, belongs to this interval. Is it clear so far? Okay. So double U, what? Yes? Yeah, that's the density of the waiting time. So that's of the situation. W is the distribution of sita. W is a distribution of zeta. So zeta and x are independent. Zeta and x don't have to be independent, but we are assuming that they are independent. So we are assuming that they are assuming that the covariate zeta is not dependent on the other covariates. Yeah, in fact, when I wrote this here, I should have add x, and I didn't. Yeah, you're right. Okay, thank you for the comment. Okay, so once you are. So, once you are here, you have to maximize that. And of course, it's going to be subject to some restrictions, like that the sum of the W has to be equal to one, and these are weights, so they have to be positive. And this is what we did with Anna and Steve, and we published in the Statistics and Medicine of Historical Methodists. Okay, and so what we propose is what we call this algorithm: GEL means Gomez Espinal Lagos. And so, in the first step, so it is a typical two-step algorithm, in which in the first step you have sort of an expected maximization algorithm. So, here what we have are the self-consistency equations for W. So, basically, what you're assuming is that you have given a value theta, what you are doing is just reproduce that the mean, you know, the probability that Ci is equal to given what you have observed. What you have observed, and you take the average, this should be equal to wj. Okay, and so this, of course, is a sample, and this conditional probability can be written using the density and the sum of all the densities. So, this is what you solve for a given value of theta. So, once you have all the w's for this value of theta, then you just maximize this. It is important, and I'm not showing this here, but you can check at the paper. Here, but we can check at the paper. When you go to maximize these equations, you obtain, the estimating equations that you obtain are the normal equations, the ones that you would obtain when the thetas are exact. Only the difference is that where you're going to be the theta, you will have, you have the expected value of theta. Okay? So you can just reproduce the estimating equations of the normal equations. Exactly, they do exactly the same, but wherever you see a theta inside of this exact value of theta. Inside of this exact value of zeta that you don't know, you're going to have the expected value of zeta. Okay, so this is implemented in R, and you have in this paper there is also a tax supplementary material with all the code. Okay, so that's what we did. And so when we try to apply to this data, we were just using the log of the vinyl log. And you can see here the seats were in days, and so you have the value. So you have the values, for instance, the values of the days that they had to wait until they were between 1 and 87 days. And for the CETA R, 6 and 90. And we also have, you know, the width of the interval was in between 1 day and 44 days. So it was quite large. So the intervals were quite large. And these persons were between 24 and 64. So they were with a median of 40. So it was the young people. So that's what we were. So that's what we were fixing or fitting here. And I don't have time to just be involved in all the analysis of that, but what we already saw is that beta, which is the corresponding coefficient for the waiting time, is significantly associated to the viral log. Okay, so that's something. And so basically, the faster you put these persons into the Into the enrollment, the better for them, the better for everyone. They need that because it was not clear, it was not obvious that it had to be like that. However, once we were there, I say, okay, but how can we check whether this linear model was right? So how do we validate elasticity and normality? And so we take a little approach and say, okay, if you would have the normal resilience. Residuals in a normal model, you would have y minus y hat, right? And y hat would be alpha hat, would be beta hat times zeta i, but we don't know zeta i. So, what do we do? Okay, let's say, okay, let's try two couple of things. Since theta i is in between these two values, if I assume that beta is positive and the same if beta is negative, I can define y minus alpha hat plus beta hat times the upper. The upper limit and be with the lower limit. And so, what we see is that the residuals are as well interval sensor. And now the residuals can act as an outcome, if you want. And many things you can do with that. For instance, you can decide that your residuals are going to be the midpoint of these two values. That's one possible point. You can also decide that the residuals take into taking. Take advantage of the normality of the error, and then you can just calculate the expected value of the error given these receivers. And that's the formula, you know, with phi being phi being the kinetic distribution function for the normal, and this being the density. Okay, so these are two possibilities. And then there's a third possibility that we call gel residuals, in which instead of doing this A and B, we are going to replace CI by To replace Ci by the expected value of Seta given the information that you have on the intervals. So, by doing that, I'm using Turbo's estimator. So, by doing that, you are not using any of the other information, just information given by the residuals. And we thought that that was a nice approach. And with Rebecca Top, we just, in fact, I mean, in fact, these residuals were already mentioned in the first paper. Already mentioned in the first paper with Stefanana, but then with Rebecca, we developed this once and we did a simulation study with that. And I don't really want to spend much time with the simulation. I mean, it was not a huge simulation study. We had two sample sizes and the distribution. And for instance, to check normality, we were checking normality again shifted to created exponential. And to check homelasticity, we just took another value for the values. So it was not a huge. For the values. So it was not a huge, but the results were that if you call delta super the mean of the interval widths, then it's clear that the smaller this mean, the better is the behavior of the residuals, since there is less uncertainty introduced by replacing the residuals by a single value. And so, in terms of this mean, what we can see is that if n is bigger than If N is big enough, 215, then all the residuals work okay, except for checking normality, except the Rebecca, the top and homeless residuals. However, for checking homoce elasticity, the best residuals were given by top and office. And in this part of our research, there are more things to do, but Remeka, after the PhD, quit and started a new job and a new thing, so this was a little bit stopp after there. This was a little bit stopped after there. But the recommendations that we can give is that to check normality, you can just use the GL residuals because they capture normality very well, but sometimes don't reject this exponentiality as often as they should. On the other hand, top and normal residuals reject normality satisfactorily, but sometimes they also do that with normality data. And for homo-selasticity, the three of them work quite well. The three of them work quite well for homocelasticity, and perhaps top and bonus are doing better for hematostasis scenarios. Okay, so in our residuals, in our ACTJ 3569 clinical triangle, we choke all the residuals and they were doing okay, so the model was fulfilled and so we have it. Okay, so since you are going to get all the slides, here you have all the links and everything, and please. Leads and everything, and please, anything that you want to discuss after that, anything that can be proved, please, you know, let me know. Okay, so now I go to the second part, in which what we are going to have is interval sensor covariance, which are not time to event. Okay, and so let's, I'm going to explain this plot in a minute. But let me just introduce you the problem. This is a trial that is called pretty made. Tried that is called Pready Made Plus. It's a trial that has been for many, many years, that has involved thousands of persons, and they are trying to see what were the effects of a very strict Mediterranean diet, together with physical activity, with behavioral support, so it was something very complex. And they wanted to see if this was able to prevent cardiovascular diseases. So among all the All the goals that they had, that was, you know, it was many. One of them if the concentration of carotenoids in plasma was associated or related to anthropometric clinical and biomedical parameters that they know they are related to cardiovascular risk. Okay, okay, so what is a carotenoid? Okay, carotenoid is a pigment that we have in many fruits and vegetables, and it's the one that produces the orange color of pumpkins, carrots, corn. I mean, sometimes not that orange, like corn, but that's a carotenoid. There are many carotenoids. And it's already known, and I think you all know, that the better, you know, we have to eat carrots, right? I mean, to be have a good. Okay, so it's true, we have to eat carrots. Okay. Character. Okay, so what they do, they use this liquid chromatography mass spectrometry. They have promised me that they will take me when they to the lab to see that, but since I didn't do, I couldn't do it, I couldn't take my pictures, so I had to do live on Wikipedia. Okay, so a machine like that is the one that computes the concentration of carotenoids. And for each one of these, there is a limit of quantification, so the lowest quantity. The lowest quantity that can be accurately quantified, and there is also a limit of detection, meaning the lowest quantity that can be distinguished from its absence. So that's a compound. Let's call S, this compound. So some compounds have a value S, no problem, it's an exact value, but some compounds are below the limit of quantification and above the limit of detection. So it would be in this interesting. So, it would be in this interval, and some of them you cannot distinguish. You don't know whether this carotenite is in your plasma or not. No idea. Okay? And so, this S, which is our covariate of interest, it's a mixture of interval, sensor, and complete data. But if we only have one compound, I mean, these limits are fixed, are not random. So, there is not much we can do about it. But the thing is that they are not interested in one compound. Thing is, that they are not interested in one compound. Also, let me show you this first. So, for instance, this is the alpha-carotene with this group of 104 women, the ones that were used to check that. And so, you have that we have around 40 or 41 women that they were below the limit of detection and above the limit. No, so 20-something below the limit of detection, and around 15 between the limit of detection and quantification, and all these same. At all these, say, 60 women, so all these dots corresponds to the level of the alpha-carotene in mule cans by neutron. Okay? I'm sure. So, on the previous slide, I'm kind of trying to wrap my head around what this would look like coming from the machine itself. So, if you saw a zero reading, would that tell you it's somewhere between zero and L O D? Pro. I am not sure whether they know a zero. You can get like zero. You can get like zero. You can get zero. I think it depends on the machine, but basically at some point, I don't know which is the value that the machine says. I don't know whether the value that it says is this LOD, 20. I know this not with this chronometer because I have been working with viral node with HIV. And so with viral node, for many years, it was 50. And so below 50, there is no answer. There is no number. Answer. There's no number. You already know that it's below 50. Now I think it's below 20. And of course, depends on the machine. But here depends not on the machine, because the machine was the same, depends on the corrosion. Why? If you ask me this, I don't have the idea. Why is different? So you'll know whether it follows or falls between zero and LOG, or you'll know if it falls between LOG and LOQ. Like there are separate. Either one or the other. Or it's above the LOQ. Oh, it's above, and then you have the D. You have 13 complete data. Thank you. Okay, so that's the information that we have for just for one. But as I was saying, we have a sum of carotenoids. And as you can see here, you know, this is for carotenoids, but this is LOD1, it's here, but for another one, it's there. So, you know, this number is, say, I invent three, and this is seven, and perhaps this is five, and this is eight. Okay, okay, so this, so now. So now our covert is going to be this Î¸ equals the sum of all these concentrations of carotenoids, and claim is an interval sensor random variable. So I'm just in that random. And in fact, imagine that we have only three compounds, S1, S2, and S3. So imagine that S1 is between the limit of detection and the limit of quantification. The S2 is before the limit of detection, and S3 is a second. And S3 is a success. So for the sum of this, what you know is that it's in between LOD1 plus S3 and LOQ1 plus LOQ2 plus S3. So these are the two limits. And although this could be thought as fixed, S3 is not fixed anymore. So I think that it pays to think that this is a random variable, and then it makes sense to talk about the distribution function of this random variable. Okay, so this is the way it looks for this 100 for women and the eight carotenoids that, as you can see here, we have many different carotenoids, alpha-carotene, beta, glycopins, and there are two groups. These are carotenoids and these are xantophils, okay, which are also four different ones. And so you can see here for the 104 women, and this is the value of the total sum of the charotic ones, okay? And for most of them, you Okay, and for most of them, you see the interval because that's the information I was telling you. But some of them, you know, it's exact. So, you know, for instance, if this is exact, what it represents is that for this woman, woman 90-something, the eight values of carotenoid were exact. And so you have an exact value. So here we are going to have a situation with interval sensor that also includes exact observations. So by doing so, if we assume that these intervals are close. These intervals are closed both ways, you know, at the left and at the right, then we don't have any problem dealing with this exact distribution. Okay? Exact value, sorry. Okay. So now our generalized Neil model is the useful one. So you are going to have Y, which is, for instance, is going to be glucose, or it's going to be glucose in one of our examples. That could be discrete or continuous, but we're assuming that. Be discrete or continuous, but we're assuming that belongs to the exponential family of distributions. And then we can have a set of covariates x, and zeta is the covariate that we are interested in. I mean, interested. We might be also interested in the x1s, but that's the one that has the internal sensor product. And that's the one that we assume that has distribution value u, and it's in between zeta al and theta r. I should have saved it from double y. And g is the name function, you can just speak. function you can just pick you don't ever have the one that is biggest and you have the near pair so what is our goal our goal will be to estimate alpha beta gamma plus all the parameters that involve the distribution of y okay including the dispersion of the model okay all right so this slide is very similar to the one that we had before okay the only difference I mean in fact it's the same so you In fact, it's the same. So you can decide that you have a discrete support and you can just work with that, and you're going to do exactly the same, but the only difference is that here the F will corresponds to the conditional density and the degeneralizing a model. And before it was just the normal model. Other than that, it's more or less the same. And that is what we published with Klaus and Klaus Langor and Carrie Baruenta in twenty twenty three. in twenty twenty three. And then s how how is the algorithm? Yes, and then. Do you suppose that's a distribution of the distribution of W? No. Okay. No, no, no, no, no. That's what I'm going now to the algorithm. Yeah. And so what we assume again here is that this is the shape the form of the joint, sorry, how was it joint of the conditional density, which is the form of our generalized fluorobolo. And then, in fact, I was gonna, I mean, when I was preparing the slides, I was hesitating whether to put this expression here, but then I thought it was a little bit too complicated. But basically, this expression should be here, right? And so you have some bit the same as we had before, right? So you have one step in which you start assuming that theta is known, and then you solve this, and these again are the self-consistent equations, and then you get the value for theta. And then you get the value for the weights w, and then with this, you maximize this. Okay, and that's you know the same. So we are just repeating the same with a different type of covariance. Go over again why step one should estimate the weights? There's like some intuition behind that formula. Oh my god. Yeah, because I don't know whether this was before. Yeah, because this could if you remember you have. If you remember you'll have to remember, but here this is the self-consistent equation, okay? So the self-consistent. Ah, okay. So that's something that Ephraim developed back in March. And so basically he's telling you that Is telling you that WJ is equal to the expected value of WJ given what you have observed. Okay? So it's like the first part in the expectation and maximization algorithm. You're more familiar with the EM algorithm? So it's like the first step. Okay? And so what you are saying here, I mean, let's because what I do later is the same. So if you probability of Ci equal SJ is equal SJ, it would be we have the condition WJ, right? So, but now I am just asking at this probability given what they have observed for this individual. And this is something I can write in this way. But this depends on the W. So now if you look at this, say, okay, I can do this for every individual. So I, okay, let me start. Fix J. Okay? So fix J, so fix what? So fixed j, so fix one mass point s j. Okay, what is the probability that ci is equal to s j, given that I have observed that? This can be written this way. Okay, but now if you just go back to this, say, okay, if I average off all these values, what I am getting is an estimator of the probability that C I is C J. So I'm getting an estimator of W j. And so this is a sample. Okay, maybe we have to spend more time on that, but that's, you know, I could have written. But I could have written instead of this particular in terms of the spectrated value. I didn't do that. But is that more or less intuitive? Yeah. So it's like this is going to be a good proxy of approximation of WJ, but itself depends on WJ. So it's like loop, okay? And so you start with initial value, one over n, for instance, or one over n, or whatever. Over n or whatever, you start with the initial value, and then you just get the first solution. Once you get the first solution with this w, then you put this here and then you maximize. Okay, and then that's easy, okay? Because you are maximizing a normal or generalizing your model, and then this provides you with a theta hat, okay? And then you go back with this theta hat and go again, and then you stop until convergence. So you stop until the difference. You stop until the difference in the double U, double UJ, or the thetas are close enough so that you decide to stop. So, for this to work, the X and the C have to be independent? The what? X and Z The X and the C. They have to be independent. Yeah. Yeah. They are disconditionally dependent in this process. Yeah. But I think I mentioned this on right now. I had a similar question, but I'm trying to think about how to phrase it because that was how I was reading it too. It's only subscripted by J, so it feels like it's doing some sort of, it's like non-parametrically estimating that density piece. And so I guess I wondered if that choice was more computational. Like if you were to do something like, like I acknowledge, like non-parametric conditional density estimation is really complex. So I wondered if you thought about doing something like a method of. Doing something like a method of sieves or something that kind of smooths that conditional density, or if that would fit into this framework. I admittedly, I haven't done the self-consistent equation piece, but I've set up similar EM algorithms, so I wondered about that. We didn't do it. I mean, we just thought that, you know, the self-consistency equation, I mean, just being from the, I mean, I was in interval sensor from the beginning in the response, and so the way you get, I mean, I'll show you. So the way you get, I'll show you this to me: the way you get to find the values, you know, the maximum likelihood estimators is to this expected output, or the one self-consistency out there. And so, you know, when we arrived there, we thought that this was the natural way of doing that, okay? And it worked. And of course, yes, X are acting independently, okay? So they are not influenced. Okay. Five minutes? Oh my god. Okay, so Okay, sorry. Sorry, go. Sorry, may I wonder? Yes, I'm wondering how complicated the step two. I cannot remember very well, but I noticed what's not complicated anymore. This was many years ago. No, no, it was, it was, it was, it was good enough. Okay, so let's. Okay, so let me continue because now I have to be fast. Okay, so I decide for that, that, that, that, that. Okay, and so now we are going to do this here. And so, again, the idea of the algorithm is going to be exactly the same, so it will represent the same idea. And so, in this particular case, with the glucose, let me see. This is the descriptive data. So, these were people like 68. Like 68, because yeah, in this cohort of people, they were older than 65 people. And so you have here the energy intake that the median was 2159, and the carotenoids lower limit was in between 1.42 and 10, and the upper limit between 2 and 10. So you could see there was a large difference. And so here you have also the code if you want to look at it, and if you want to improve it, then it's small. And so, just as an example, before I continue, you know, with glucose, you know, so that's the way that the conditional density is going to look like. So, that's the one that you're going to fit in that step one and then step two. And what we found out, as you can see, it's not that I'm very fan of p-values, but okay, here the p-values are. And so, the only one that was significant was the daily energy intake, okay? But so, you know. But so, you know, glucose concentrations were related to this daily energy intake. With the sum of carotenoids, what we could see is that they were inversely related to glucose, but it made sense, although the confidence interval in the zero, so it's not that we could have been confident in saying that, but it was in the right direction, at least for a prospective and future studies that they will do, and slide. Okay, the same was done with obesity, with the risk of obesity. Of obesity. And again, but here none of the cobins were significant. But again, we also saw that the sum of carotenoids was conversionally related to risk. So again, in the same direction. Okay, so what is the work in progress? So we have four minutes? Go ahead. Okay. So let me just tell you a little bit. So one of the questions that we have in mind is the extent of CETA to continue, so we're going to have Another, of course, is everything has to be implemented. And then also to work a little bit more with the recipes. Okay, so this is Troubles idea, but I think I'm going to skip that. So are you familiar with this? You are not, I'm so sorry. Okay, basically, the great idea of Trubo is that if you have interval sensor observation, okay, so you have. Observation. Okay, so if you have this C L I and C L I, you're going to do the following. You put all the left in one class, all the this calligraphic L, all the right in another class, R, and then you build these total intervals that are called Q, P, in such a way that the Q has to belong to the L's, the P has to belong to the R's, and in between, you cannot have anything else small in between. Okay? So it's like. In between. So it's like if you cannot have another LR in between, right? So this is the way you build these intervals. And what he proved, Peter was also doing the same. So this was far in the that the maximum, okay, and then wj is the mass assigned to this interval, okay? And so the distribution function of the zeta, which is the one that you have to. Which is the one that you have the observation between C tau and C ta, is a non-parametric maximum molecule estimator of this. It's going to be a step function that is flat everywhere except on these intervals, but it's undefined on the intervals. So you don't know what's going on. It's not identified within the integrals. So then the likelihood function that we had already seen a few times, it can be written in this way. It can be written in this way. Remember that we already had that, okay? But now we don't want to just, what do we do with that? Because now, you know, the S is in between QJ and PJ. And so the difference from before is that before we had a discrete partition, okay? And so the intervals were given a priori by a discrete partition. However, now, the partition is given by Turver's intervals, okay? Given by turbos interlocks. So these are the only places where there is real identification of the model. So it's really a step further because sometimes if you do just a discrete, maybe you are just assigning values in points which are not identified in general. Okay, so the idea. But then again, you have this problem that if you want to solve this integral, you cannot solve it because w, you don't know what value. You don't know what value is not identified. But of course, you think that Y is going to be related to Donald. Okay, so it's not that you want to act naively. So we just think in two possible approaches. So this is, I didn't say that, obviously. Oh my god. Okay. Some place should say the name of Andrea because that's Andrea. So this part of Andrea's thesis. So, this part of Unreal Sisters. That's good job. So, I'm going to skip that, but basically, let me go first here. So, you see, the naive approach would be, okay, just take the middle value between Pj and QJ, okay? And since you know that all the mass is concentrated, so just act, take the middle value as the representative of this interval, and solve all the masses of the UJ. Okay, so you have that. That's sort of the naive approach. This is like assuming. Naive approach. This is like assuming that all the different values of the carotenoids don't change the value of glucose within Q and B. But perhaps this is not true. And so we want to take into consideration that Y might depend on the values that she takes inside. And so what she did, I'm not going to go through that, but basically what she realized is that these are the conditional densities, okay? The conditional densities, okay, and the Q and P can be either on the side, it could take the maximum. So, what she proved is that there is always a global maximum at this value, okay? And so, once we have that, we can have lower and upper bounds of these values of this density. And I'm going too fast, I know. But with these lower and upper bounds that we have, we can check the scale difference so that we have a measure that is in between zero and zero. A measure that is in between 0 and 1, an invariant of change of units. And then we can, and so what is the meaning of this? This is basically measuring how far the integraph is of actually being constant. And so you can say, okay, give me a value, epsilon, and if this distance, if this ratio is less than epsilon, I believe that it's constant. Okay, so that's the idea of doing that. And so if I believe that this integral, so if this Integral. So, if this for these values is constant, then this integral can be replaced by the, you know, adding up and then by doing the lower and the upper limit, and then you have this part. And this, the mass here is wj. And so you can replace this interval by this part. So you don't have the interrupt anymore. And if you do that, you can go and do the extension. And in fact, this has not been programmed yet. Okay, so hopefully it works. Yet, okay, so hopefully it works. But you will start: okay, derive the QP turbo intervals. That's the first thing. Okay, they are going to be the same no matter what, because these are what you have got from the carotenoids information. Then set a value epsilon in such a way that you believe that this, you know, this constancy, okay, that you're thinking. Okay, and say, once you have that, you go with step A is just for every fixed theta and for every individual, check. And for every individual, check if this scale difference is less than epsilon. If it's less than epsilon, replace this interval by this other quantity. If not, refine the intervals until you find that this is smaller than epsilon. Okay, so it's like now it's not going to be qj pj, but it's going to be qj and something else, something else and something else. So maybe you have two, three, or four integrals. But this thing is the same. And then once you have this, the step B. This, the step B is like before, it was our step A. So, again, the self-consistent equation. And so, here it's clear: here is the expected value, you see, that you were asking. So, this is the expected value of C given W, the value W, of indicator that C belongs to QJPJ, given all information. Since this is an indicator, that's a product. But basically, that's the expected value. So, this is what you expect. You expect, okay, and so what you're assuming is that if you average this, this is the solution of the estimator of WJ. That's what the self-consistent equation says. And say self-consistent because it's self-consistent. Okay, but now here with this information, now isn't a bit easier because you can just replace by this lower and upper bounds and you get that. Okay, and in the step B, again, you have. And in the step B, again, you have the same, you have to maximize. Now the maximization is that. So you get the first set of solutions, theta and w. But then you have to go back to step B. Because now you have, remember, step A, sorry, because in step A, you were fixing theta, and you were checking this for this even theta. Okay, so now you have a new theta, so you still have to check again. Okay? If it works, it doesn't work. Okay, and so here you will say www. Here it was saying, W does not depend on covariance. It came too late into the. I knew that they had put this on there. Okay, so this has to converge, and hopefully it will. And this would be, you know, the new and the new workshop that we have, perhaps in the short. Okay, so let me just finish only with this model diagnosis. Just the idea. So the idea is to use commode single C in DLM, like Debian. In GLM, like Devians, Pearson, Anscombe, Quentile. And what is going to be the idea? Since all of these residuals are always in terms of y, x, and zeta i plus the estimators, idea, just replace the theta i by expected value of the zeta i given the estimator given by total. Okay? And so once you have that, then I'm going to skip this. Let's see, for instance, with Pearson's kind of square statistic. Pearson's chi-square statistic, the would be y minus t minus one, alpha hat plus beta hat, x plus gamma hat. And here, this theta i hat is the expected value of the theta i. So you can use all these residuals, at least, and so the problem, or many of the problems, is, I mean, under general situations, these residuals behave as a kind of square with n minus 2. of the square with n minus 2 by square. Is this going to be valid? Perhaps not. Most likely not, but we don't know. So work for everyone if you want. So as a final remark, sorry for being late. So what we have seen is that we can be working with covariates that are interval sensor, but they are not time to event. We hope, or we think that this is a relevant topic because in nutrition and in many other of this, you know, there is a limit of quantification and limit of detection. And in-depth detection, it's something that happens often. So, in those situations in where there is just the sum of some of these concentrations, this could play a role. And as a future research, you have seen many things. But one that I would really like to work soon is what happens if you have sensor outcomes, because until now, the moment we have both wise were complete, were not sensor. Okay, so let me acknowledge my colleague Klaus Langor, and my student at the Atloba, and this group, Jeda Rio, which is part of our life, and BioStudNet, which is also a network of biostatisticians. There are more than 200 in Spain, and we have many young people, and we have fun together. And the grants, and also thank you to all of you, and thank you very special to Tanya for the invitation. Thank you. And we're going to take a break. Now, if you'd like to ask Luffy more questions, please feel free to do so.