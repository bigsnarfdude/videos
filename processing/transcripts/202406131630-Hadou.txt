Robust and rolled networks. All right, thank you. Hello, everyone. So, my talk today is about robust and rolled network, which is like a joint work with Navid Nadiri and Alejandro Ripero. Yes, so the goal of unrolling is that we are trying to solve an optimization problem, and for me. Optimization problem. And for many decades now, we have developed tons of iterative algorithms that solve optimization problems. Maybe the simplest and the most famous one is gradient descent. But all of them, all in all, have the same concept. We start with some random initialization, y0, for example, and you try to update this estimate step by step, taking a descending direction, right? And so this is like the whole idea of iterative. The whole idea of iterative algorithms. And Rolling is trying to do the same thing, but by replacing every update, sorry, by replacing every update function with an URL layer, w1, w2, wl, and so on and so forth. So like the goal is that to imitate an iterative algorithm by learning the parameters of this iterative algorithms. And the goal is to learn how to reach the optimal, not to learn the optimal itself. To learn the optimal itself. And there has been a lot of work using unrolling in many applications where there are huge success in it in the sense that we get the same performance of the iterative algorithms and in much fewer steps. That's why, because you fix the number of layers during training and you can reach this optimal in this fixed number of layers. It could be 10, 15, 20. And this is unlike the iterative. And this is unlike the iterative algorithms that usually take hundreds of iterations, thousands of iterations. So, this is the main success of unrolling. But I'm not here to talk about how great unrolling is. I'm here to talk about like what is the downside of it. Okay, and what we observed actually is that even though these unrolling architectures imitate iterative algorithms that should be descending, they do not really descend. So, for a very simple example, a convex function. Simple example: a convex function. I have an optimal, and I'm trying to find this optimal, just one unique optimal. So, in the gradient descent algorithm, I'm going to find, like, initialize my estimate and follow the gradient direction until I find my optimal. If I do the same thing by unrolling the gradient descent, so I'm just learning some parameters in order to find the gradient direction. I get this result, which I start at the same point. Result, which I start at the same point, I go all over the place, just completely random, and then converge to the optimal at the last layer. Okay? And so I reach this non-descending parametrization, even if I am imitating an iterative algorithm that should be the same. Okay, so how can I fix it? This is what I'm going to talk about today. How can we fix it? How can I fix it? I'm going to add requirements during training to during training to find this parameterization this descending parameterization I need to learn some no I'm still sorry yeah how I'm going to find my parameterization so you still have my descending parameterization so I'm going to add some requirements during training to make sure that I'm going to descend if I have like my layer L and I give it y L minus one I'm And I give it yl minus one. I'm trying to make sure that whatever this layer is going to predict, it actually decreases my function f, and therefore, over the layers, I can converge to my optimal solution. Okay, and how I'm going to do this by forcing constraints during training, and this could be in the form of reducing the gradient norm or the distance to the optimum. Okay, and when we do this constraint learning approach for unrolling, Train learning approach for unrolling, we managed to get, for the same example, we managed to get a descending parameterization. Of course, it's different from the gradient descent because in the gradient descent, I'm following the gradient direction. Here I am not. I'm learning a descending direction that's just minimizing my function. But of course, it's completely different from the standard I'm running that was completely random and found like the optimal at the last place. Okay, so you can tell me right now. No, you can write. You can tell me right now, okay, summer, you had a hammer, constrained learning, you wanted to use it on unrolling. You succeeded, yes, you found a descending parameterization that is much, that looks completely different from the unrolling perspective, unrolling approach. But what I see is that you get actually the same performance in both situations, you get the same performance at the last layer, and this is all what I care about. So, why should I care about principal learning or your approach? Been learning our approach. And the answer to this question is that gradient descent is robust to perturbations because it's descending. Because if you are going to perturb the direction, the path of gradient descent, it's going to keep descending after that. It doesn't matter how many times you are going to perturb it, it's going to continue descending and final and find the optimal at the end. Maybe it will take more iteration, yes, but it's going to reach the optimal afterwards. Optimal afterwards, like at the end. And when in unrolling, when you fail to descend, you just lose this property completely. You're no longer more able to be robust against perturbations. And there's this very simple example. So this is an inventing problem. I have this image with a black box, and I'm trying to fill in this box. If I try, I'm going to show. I'm going to show exactly how this experiment works towards the end, but like to give you just a very simple overview, I trained and enrolled the network and I was able to fill in this box with something like it's very reasonable to our eyes. But when I add perturbation to my input data set, I started like the performance of the unroad networks started to deteriorate until I get something that is. Until I get something that is yes, the nose, the eyes, stuff like that, but it's not pleasant to our eyes. So, of course, we were not robust enough against perturbations. But when I trained the same exam, is the same exact unrolled network with my constraints, well, the performance barely deteriorated over the perturbations, no matter how much I increase the perturbation size, which means that my descending constraints were able to. Constraints was able to fight against these perturbations, and therefore I can generalize to out-of-distribution images in the sense of that shift, I have a distribution shift, sorry, in the form of added perturbations. And I was able to fight against these kinds of perturbations. With that in mind, let's dive deeper in with mass in a former definition, with the formal definition of what I have done in order. Of what I have done in order to get these results. So, as I said, my goal is to solve this optimization problem. I'm trying to optimize f, that is a function and variable y, and this is the variable I am optimizing over, and I have x, which is another variable that could be my input. Okay, and I can solve this problem, and I have no assumptions about what is that. It could be anything, as long as there is an iterative algorithm that can solve this problem, I am okay with that, okay. Problem. I am okay with that. Okay. And I know that I can solve this problem with a gradient descent, which is I'm going to initialize y zero with whatever, and I'm trying to, I'm going to take the gradient direction in order to reach the optimum. This is the gradient descent, but I'm not necessarily have to unroll gradient descent. I could use any iterative algorithm. So in general, I can write these up this rules as a function g's that takes one. function g that takes yl minus one the previous estimate x and produce to me like the new estimate y and i know that uh these iterative algorithms yes work very well they compete we proved all of that but unfortunately they some often the case it is very slow to compare and this is why we do uh unrolling and in unrolling i'm going to use the same update the same uh the same update role g's that take one Same update rule G that take y L minus one take X, but it's now parametrized by a neural layer W L. And W L could be the parameters of the three divide algorithm itself. So it could be the step size. It could be some parameters I use to compute the gradient. It could be some parameter. If I have a proximal gradient scent, for example, it could be the losing my voice. It could be like the parameter of this proximal. Like the parameter of this proximal operate, okay. And the idea of that is to learn how to reach the optimal instead of learning the optimal itself, as we said. And how I'm going to train this optimization problem, supervised learning. I'm going to construct my data set by sampling some problems. And I sample the problem by choosing sampling some x. And for every x, I'm going to use my iterative algorithm in order to find white stuff. Y star. And then I have this supervised learning on trying to match y L, my detection is the layer capital L and Y star. Okay, and I have this expectation over X, which is the problems, and Y 0, which is the initial values that I have here, because it's random. It could be from normal distribution or whatever. And I'm solving this supervised problem. Thank you. Probably that's what I need. So yeah, this is my supervised learning problem that I'm trying to solve. It's non-convex, it has different local minima. Some of them are descending parameterization, some are non-descending parameterization, and it's open the case that we end with. In the case that we end with the non-descending parametric, I start from whatever random value and then try to run my round across. I round the optimal and then I find the optimal last layer. And I know that, as I said before, when I do that, I just lose key features in iterative algorithms, which was robust to perturbations. So as I So, as I said, in gradient descent, if you just perturb one of these estimates, gradient descent is going to go towards the optimal, no matter what. But when you do this for unrolling, you perturb the same third, I guess, that was the third estimate. The unrolled network has no idea what to do after that. Okay, and this is why it's not robust to perturbations because it lost the idea of the. Because it lost the idea of the sending. Okay, I perturb it. It's just like any neural network. It's just like any neural network, it's going to be to protect whatever some nonsense when it's how to say this. When there is some perturbation happen to it. Okay. So, okay, so this is the problem right now. I have unrolled networks that isn't robust to perturbation. How can I solve it? And as I told you. How can I solve it? And as I told you, I can solve it with my hammer constrained learning. How I'm going to do this, as I said, I need to add something in my training approach, in my training paradigm that makes sure that my function f is going to decrease over the days. So how I can do that? I can do this in two ways. We propose two ways. The first thing is to reduce the first ingredient, don't to decrease over the n. So here I compute. And f. So here I compute the derivative or the gradient of f at a layer l minus one, and I make sure that wl I'm going to predict will make sure that this inequality holds, that the gradient at layer L is lower than it is in layer L minus one with this parameter epsilon. And epsilon is just a hyperparameter I'm going to choose during training that make sure if it's like big enough, this value. Like big enough, then this value is small, and it means that my constraint is very aggressive. I'm trying to reach the optimal in a much lower, a fewer steps as possible. So epsilon is somehow control how aggressive this constraint. So this is the first constraint I could use. The second one is the distance to the optimal. Same thing. I'm trying to compute the distance at layer L minus one. I'm trying to find WL that makes this distance decreases over the layer. Distance decreases over the length. Okay, and now I can write my constraint problem, which is to reduce the supervised loss. It's the MAC loss between my prediction and the optimal value and expectation subject to the constraints I'm talking about. And I'm definitely taking the average over the data set. And this happens for every layer in my architecture. My architecture. But this one small thing that I didn't say is that I'm going to change the architecture a little bit by adding some noise before I process every layer. And this is to make sure that my constraints are statistically independent. Like what happens in one layer doesn't really affect what happens next. Okay, and then my expectation is going to be with respect to this noise as well. Okay, so this is my. Okay, so this is my training problem. I made it a little bit harder, of course. It has now constraints and I need to solve it. How I'm going to do this? As usual, we do in constraint learning in our group. I'm going to solve it in the dual domain. So I'm going to construct my Lagrangian function, which is nothing but my objective from before. And here, my constraints multiplied by some Lagrangian multipliers. This is the Lagrangian. This is the Lagrangian, and now I'm trying to find the saddle point of this Lagrangian function, w star and lambda star. But of course, I don't know, I don't have access to this expectation, the distribution of the problems. So I'm going to replace this with the empirical Lagrangian function. So I'm replacing the expectation with some over n samples. I'm going to sample from the distribution. It's just n problems with n solutions. And now I can form my And now I can form my dual problem, which is to minimize the Lagrangian with respect to W and then maximize it with respect to R. Okay? And I'm going to solve this. So this is now unconstrained, somehow simple compared to the original problem I was trying to solve. I'm going to solve this with primal dual algorithm. But by now, you should be concerned because I can use duality. I can shift to the dual domain if I have. the dual domain if I have strong duality, but do I have it? Of course not, because my original problem, original problem, my original problem was non-convex, which means that I don't have strong duality. Okay, so how we are going to go around this issue? Well, my former colleagues actually proved that for neural networks, if you are what you are trying to do is to estimate, like if your problem, sorry, if your problem is to If your problem is to admit some parametrization of a neural network, the duality actually holds, not holds, but there will be a very small duality gap. And this is because modern neural networks that you are trying to estimate or to find is a universal optimized approximators, are universal approximators. And under that assumption of modern neural networks, the duality gap between the constraint problem and the dual problem is kind of And the DWOT problem is kind of small. It depends, and the game depends on n, which is ellipsis constant, nu, which is the universality parameter, lambda star, and this is L2 norm, and this is the sensitivity of the constraints, which are like how hard the constraints, how hard or easy is the constraints. So the higher it is, the harder is the constraint to be satisfied and the sample complexity as usual. Okay, so now we. As usual, okay. So now, with this series that they proved, I know that if I solve with the dual problem, I will get near optimal solution and near feasible solution. So my constraints are going to be satisfied up to some error, which is the sample completion. So the higher and the smaller these values, the more feasible you are. Okay, you have questions? Yes. Why the constraints are the right set of constraints? Constraints are the right set of constraints. Specifically, what we wanted at the beginning was for the new gradient to be smaller than the previous grade. Yes. But we want that to happen for every x. So here you're enforcing that the average gradient, expected gradient, smaller than the. Right. We can do this for every example in the data set. Like, yeah, instead of using this connection. Using this constraints on average, we can use it for every single point in the data set, but it's going to be harder to compute. So it's much easier. Yes, it's not perfect. I agree with you. This is like a limitation. It's not perfect, but it's much easier to handle this problem compared to if you handle the constraint for every single data point in the data set. Well, but I think in the paper of Juan, he considered one Lagrange multiplier for every Lagrange multiplier for every constraint. Yes. And then when you go to the dual, you just get the integral. And he had a way to handle that for related problems. Then I need to talk with him about that because we tried myself, I don't know, tried and find it very hard. But if he did it, I should talk to him. You mean one surviving? Okay, I will talk with him because then, yeah, it's much better. Because, yeah, I know that this is a limitation. I'm doing all of this in the next video. Limitation: I'm doing all of this on expectation, which means that I'm not really sure that for every single problem, this is going to be hot to hold. So, you are true about that. So, yeah, we should consider that for sure. Thank you. Yes. So, just wondering, do you have shortcuts in the blocks, like in going from L minus like one? Yeah, I mean, like, these blocks repeat itself. Like, I have capital L of them. Yeah, I mean, we have to. Yeah, I mean, we have the residual connection. Yes, we have residual connection as well. Yes, it's not here, like, this is a very simplified version, but yeah, we have residual connection from the input to the output. So every output is just the, like, every Y L is going to be the output of this layer plus a residual connection. That's that's true. I see. Then it's kind of surprising that the Y are they are jumping around, right? Because if it's the residual and then if the residual is small enough. If the residual is small enough, then the process should be relatively stable, right? It shouldn't be really jumping. But why are you assuming that this is going to be small enough? That's the whole spirit of the residual connection, right? So you make sure that the residual part is small enough so that you don't really jump too far away from the input. Maybe I do not really understand what you mean by residual connections then. Basically, what you said, right? So the YL is computed from the summation of YL minus one and the Of yl minus one and the output of the else log. Yes. And hence, if you think if your WL is small enough, so that's how you, yeah, that's my question. How you're going to enforce that WL is small enough, then that like the step you take is actually small enough. That's not guaranteed at all. So, usually, like that works sufficient to set the parameters to be small enough at the initialization point. So, there is this zero point. Okay, so there is a residual paper basically at the initialization point you make the residual connection to be small enough. Okay, but yeah, but like when I like as I'm going to show you in the experiments, when I did the same exact experiments that people in literature did, we didn't have these smallest steps. We still were able to get like very long steps, and therefore we were. Steps and therefore, we were jumping around. I'm going to show you the experiments of real data because, like the example I showed before, it's a very simple problem. I'm going to show you like real problems, and I'm going to somehow convince you, hopefully, that no, it's not. In this case, it's not, it wasn't the case, like that wasn't the case. I was jumping around and I'm not doing much progress towards the optimal. I'm going to show you the experiment, and still you have a concern. We can discuss it then. I see. I mean, just I think maybe just one thing. Just uh I think maybe just one simple solution is just to verify the residual where you control the the scale of the reserve. Yeah, maybe, maybe, yeah. Question? Okay, so yeah, we were here, we formed the dual problem, we pulled this result from Luis Chamon and as my other colleagues. As my other colleagues, in order to show that if I solve the zero problem, I'm going to get near-optimal solution and near-feasible solution, which means that I am not really sure that I'm going to satisfy the constraint with zeros. So near-feasible solution could be a problem. Maybe all my layers are going to violate this constraint because this constraint, because this constraint was supposed to be zero, now it's not. So maybe if all of them are violated. If all of them are violating the constraints, then we are, we didn't actually find the standing parameterization we are hoping for. And this is where our result, mean result in this paper came to solve this problem. So the problem, again, to summarize, near physical parameterization that we're able to find, not necessarily descend at each layer. So I might have some errors and some of Some errors in some of these layers. But what we show in our theorem that I'm going to show in a minute, we saw that these errors don't really accumulate. With every layer, we start fresh. So even if one of these layers made an error, the next layers are going to correct this error and try keep descending exactly like what happens in stochastic gradient descent, for example. And how we were able to prove that, we just hit. That we just get, we yeah, we analyze the basic gradient norm in L layers. We construct it as a stochastic process, and we showed that it's actually going to converge to a very small area that depends on the sample complexity and the constraint parameters that we talked about before. And as I said, I told you, when we keep epsilon high enough, then this region is going to be small, which means that my constraints were very aggressive. were very aggressive and made me converge, sorry, made me converge to my optimum. Okay, so this is that was my main theorem that said that even though you are the only thing that you can get is near feasible solutions, they were enough to make sure that you are going to descend to the output. Okay, and when you go back to this same symbol problem example that we had before, yeah. Examples that we had before, yeah. Now we have like this kind of descending uh parametrization that finds the optimal, and it's robust. Definitely for perturbation, when you perturb, like I guess it was the third estimate, we were able to find the optimal regardless of the perturbation. So we had this robustness towards perturbation just because we kept descending towards the optimal. Okay, and to okay, this is a very simple example to like convince you more with more interesting problems. You more with more interesting problems. I'm going to consider two applications. The first one is sparse coding. I'm sure that all of you know what sparse encoding is, but just to summarize, I will have some data, like X, let's say it's an image, and I'm trying to find the sparse representation. So the optimization problem I'm trying to solve is to minimize this reconstruction error with some regularization, which is the L1 norm, in order to make sure that we have sparse representations. Parse representations. Okay, and we can solve this with ISTA, very well-studied and established iterative algorithm, which basically does gradient step that minimizes the reconstruction error, and then approximate operators that make sure that we reduce the L1 norm. Okay, and when you look at this specific, yeah, and that answer actually your question is no reset wall connection. Okay, and when you look at this, iterative. This iterative, yeah, when we look at this, I update whatever code, you can see that this is linear in the inputs, yl minus one and x. And I have here some non-linear operator, which is much, yeah, resembles a one-layer fully connected network. So I can actually learn the parameters of this update rule by multiplying x by wl, y, y, l minus l by. Minus L by Y L minus one with V L. I'm trying to train our learn WL and VL. And I have some parameter for this non-linear activation function as well. So I'm learning the three parameters of them. And this is very well known enrolled architecture called Lista and it came out in 2010. So it's like a little bit old. And it doesn't have the residual connection. And maybe this is why. So you have a very good point. We should try to see if like the residual connection might have. See if, like, the residual connection might help with the sending without the constraints. Thank you for that. Yeah, and then we trained this model on CIFR 10. And I used for this problem the distance. Yeah, like my constraints was to decrease the distance to the optimal. Okay. And as you can see in these figures, I have here the distance to the optimal. I here have here the objective function S itself. And we can see and like. And we can see, and like our constraint list is a blue one, we are descending, we are making progress through the layers. Unlike the unrolled standard lista, which didn't make any progress over all these intermediate layers and just jumped it to the optimal as the last layer. Okay, and same for the objective function, but as I said before, the difference between the performance is nothing. This is like log scale, so this difference is. Like log scale, so this difference is nothing, and you can see that in this plot. This is the histogram of the L1 norm for constrained and unconstrained lista, they're almost the same, there is no difference. So, like the whole thing is that we're just sending, but we didn't really make a huge progress in the performance. But as I said, yes, that's true. But, like, was a gain we get with these constraints is the robustness toward perturbation. So, this is the figure without perturbations that ends up. Without perturbations in the previous slide, and we started to add perturbation to the CIFR10 images and try to see what happens to the L1 norm. As you can tell, when I increase the perturbation size, both of them aren't perfect, both of them are moving away from ICTA, the iterative algorithms. But you can tell that the blue one, the constrained one, is more robust towards the perturbations compared to the case when we didn't have. To the case when we didn't have any constraints, okay, when we weren't descending. So that's for the first example. The second example is in penting with generative models. This is the example I showed in the introduction. So in penting problem very quickly, we have a clean data, then we add some mask to it in order to get x. And now I'm interested in the inverse problem. Given x, I need to predict y. And in order to do this, I can minimize like. do this i can minimize like the optimization problem over there i'm trying to minimize the reconstruction error and at the same time i'm maximizing the likelihood of the image one okay um how we can yes but the problem is that i don't know i don't have access to this p the distribution of images and it's here 2024 so of course i'm going to learn it with uh generative model i'm using here in generative model i'm going to use normalized flow models which are invertible so Which are invertible, so I can go back and forth between the image and the latent sphere. And my latent space, we assume that it's going to be a normal standard Gaussian distribution, has a standard normal distribution. Then, which means that I can transform the problem into the reconstruction error. And instead of the log p of y, it's going to be log like I'm going. So, like, I'm going to confirm from Y to Z, and I'm going to end up with Z to norm instead. And I'm trying to optimize this optimization problem instead. How I'm going to do this? With work that came out in 2023, that we can actually solve this with proximal gradient descent, which where I'm going to make a gradient step in order to optimize a gradient step to minimize this reconstruction error, and then I'm going to. Error, and then I'm going to shift to the Z domain, minimize the eliton norm of Z, and then go back to the damage domain again. And this is going to be like one iterative step in my iterative algorithm. And at the same time, it's going to be my unrolled layer and my unrolled step. Where in each unrolled layer, I'm trying to learn here the mu L, which is the step size. I'm trying to learn this alpha L, which is like a factor. L, which is like a factor that minimizes or not minimizes that shrink the value of zl. And I'm trying to learn the generative model gw and I will have a different generative model in each layer. So and yes, so this is three. So this is going to be my unrolled layer. I'm going to train this architecture, once with constraints, another without constraints. Another without constraints, and I'm going to analyze the difference if there will be any difference. And for that, I'm going to use the constraints. My constraints are going to decrease the gradient norm and the gradient here for x, for sorry, for y and z together, not y only or z only on both variables. And when you compare the constraint and unconstrained, you will barely see any difference. So here is the input with the mask. This is the clean image that I'm hoping to predict. And in between, I have... To predict, and in between, I have the output of my five layers. Both constraints and unconstrained work very well. We are decreasing and going towards the optimal. So, does it mean that we didn't really need the constraints? I guess from the introduction, you know the answer? Of course not. Here we add the perturbations. This is, as I said, that was prediction when we didn't have any perturbations and it was perfect without constraints. But once I start to add perturbations, Once I start to add perturbations, like any neural networks, I'm not doing well, but with the constraints, I had perfect performance, not perfect, but like the performance didn't really change that much with the perturbations, which means that we have some kind of robustness toward perturbations. So with that, let's conclude. Unrolling is perfect. It can solve optimization problems in much fewer steps, but unfortunately, it doesn't descend. If you want to make it If you want to make it descending, then you need to use this kind of constraint, descending constraints. And that's important. Yeah, you will get the same performance, of course. I'm not claiming anything else. But in return, you will have this robustness toward perturbations. And we had this experiment in two different applications to show the powerful of using constraints in this problem. And with that, I conclude. Thank you. Questions? You were right. There was no residual connections. Very interesting talk. So thank you. But now I want to ask is: can you comment on your prototype and rolling with weight height across different layers? You basically use the same weight. Basically, use the same weight across different layers. Okay, yeah, we tried this actually in all these problems, and it doesn't really work well compared to like when you change the weights across the layers. And this is because when you go back, maybe no, like you can here. Here the distribution of the input to each layer is actually ch changing. To each layer is actually changing. So, if you look at this mask, like it's like the shape of this mask actually changing over the layers, which means that the distribution to each input is actually different. So, if you are going to train one model that works for all these distributions, yeah, good for you. But it's much easier if you have different layers that can handle different distributions for this input to the layer, if that makes sense. Yes, and no, but I mean. Yes and no, but I mean, I mean, like toward what I mean is that toward the output, you'll have values that is very, very close to the optimal already. So you don't really need the same layers that you need when you were very far away from the output. If that makes sense. Because if you think about gradient descent, it's pretty much saying that you in each step, you are using the same wave. No, no, you are using different gradient and they. Are using different gradients, and the gradients at the beginning are large, and toward the optimal is going to be very small. If you go to like, let me show you, like here, and this gradient steps here, the steps at the beginning was large. And when we approach the optimal, the gradient is very, very small. So, we are actually making very, very small changes from one point to another. The gradient itself, the To another, the gradient itself changes. When we are far away, the gradient is large. When you are close to the optimal, the gradient is small. So, no, it's you are not taking the same step size every time. Because the size isn't just the mu, it's the mu gradient of the function f, right? And this is different when you are far away from the optimal compared to when you are close to the optimal, if that makes sense. No, not convexity. I'm just like the only assumption I made is that f is Lepsch continuous. Apart from that, it could be any function that can be solved by an iterative algorithm, and I'm going to unravel the iterative algorithm. Iterative algorithms. What do you mean by unique? No, I have a set of minimizers because I have different x. I will have for every x, I will have y star. So it's like it could be a family of functions. Yes, I'm not saying that I can solve all functions. I'm going to consider a family of functions. Let's say, for example, the least square. So I'm going to consider all functions that have this 4. Consider all functions that has this 4, x minus ay, for example. This is going to be like all functions that has this form, a is fixed for me, and I'm going to predict y star for every x. So I'm just like focused on this kind of functions, not any functions at all. Yeah, that's just an example. It could be a non-common. Just an example, it could be a non-convex problem, whatever, but it has some structure. Like you have some okay, so we can write it. I will consider some function of class f of x for specific for all x, for example. This is like my going to be my function class, and I'm going to solve my play my unrolled network to solve this. Doesn't need to have to be convex. And in the case of non-convex, I'm going to reach just a local optimum, a local optimum, not necessarily the good. Local optimal, not necessarily the global optimum. Yeah, great. So this is why we have two different constraints because each one of them is going to be different. So when it was yeah, you are correct. So in my data set, like why what is my data set like why what is y star like here when i was considering y star this y star is in my data set so i constructed my data set so even if i have non-convex problem i just run an iterative algorithm and picked one of these solutions and i'm trying to mimic it so i'm not trying to go beyond the iterative algorithm i'm trying to imitate it so whatever y star is it that you are going to compute this is what i'm going to generate i'm not going to generate another solution to Another solution to this problem. I'm going to generate the one that was in the data set, if that makes sense. So, in terms of the constraint, why does it make sense to want decreasing gradient norms rather than when I was preparing this slide? I had the same thought. We didn't know at all about the function itself, but I guess it made sense to us to use this instead of the function itself because these are going. State of the function itself because these are going to be non-negative values. So I know that when I compute this as the last layer, it's going to be zero across all the data points. So the average is going to be zero. So it's much easier from empirical perspective. But I guess, yeah, if we choose F and instead, it should be fine as well. But we didn't thought about it when we were developing this this year. You're okay. Any other questions? All right, that's thank you.