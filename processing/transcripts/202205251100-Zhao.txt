Thank you, Grace, for the introduction. And thank you for the Xu, for Peng, for Grace, for David, and for Chen Yin for the organizing of this fantastic workshop. I choose to talk about this research mainly because of the theme of this workshop is missing data memory errors in high dimensionality. And later on, you will see this is pretty much a work that shares the same. Work that shares the same data structure as missiness, and it has a lot of ingredients about high-dimensionality. So, the title is Statistical Exploitation of Unlabeled Data Under High Dimensionality. And I give almost all the credits go to my co-authors. I have a longtime friend and collaborator, Dr. Ning. He's currently a factory member at Cornell and my mentor, post-dom. Mentor, post-doc mentor, Huping, is a faculty member at Yale. And the first daughter is Xi Yi, and she is a PhD student under Yang's supervision at Cornell. So the story begins in a lot of biomedical studies with EHR data electron and health records. In this type of studies, people usually can have fully observed and usually high-dimensional attributes, and you can understand as the And you can understand as the covariates X. However, there's a lack of gold standard health outcomes or phenotypes, which we call the response or outcome or why. Later on, you will see in my notation. And, you know, there are a lot of things, issues that people can discuss about EHR data. But the work I will present today is mainly on the high-dimensionality of EHR data. And also nowadays, when people talk And also nowadays, when people talk about integration, especially you want to incorporate real-world data, RWD, into a traditional RCT setting, randomized controller trial, we will also have a similar problem as what I will present today. So the data structure, as you can see from this graph, it's very simple and we just assume there's a outcome y variable and we have a p-dimensional covariance or attributes x1 to xp. Attributes, x1 to xp. And we will assume all of the x variables are fully observed. And however, there's only small n subjects that have the corresponding y values. And we call this is the labeled data. And also the subjects without the y values are the unlabeled data. So the sample size for the unlabeled data is capital N. Okay, so for people who are familiar with missing data literature, this is in Data literature, this is indeed a missing data structure. And actually, this is a missing response problem. We call it semi-supervised learning as opposed to missing data because there are mainly two reasons. The first reason is we are going to allow this n over big n plus little n. This ratio is like missing this probability in missing data literature can equal to, I can go to zero in the end when. Goes to zero in the end when under some settings. And also in the asthmatics, we are going to allow n and p, both of the small n and the number of covariates x can go to infinity. When we develop asthma later on, you will see we are going to somehow let this capital N to be fixed, and we are going to discuss the effect of the capital N in our asthmatic theory. Okay, so So, this slide, I will mainly outline what is the framework we are considering. We consider there is a true data generating model. We call that is y equals to fx plus epsilon error. And this error just very simple has mean zero and variance is a sigma squared, which is a big O1 term, a constant term. And we call this is true, which means we know that this is unrealistic in practice. Unrealistic in practice most of the time. So, this model, of course, is accurate, right? Because it's a true model, underlying true model. But however, it lacks the interpretability for practical use. So, under this model, we can consider a lot of misspecified working models for the response y. For example, this model here, we have this y equals to x transpose theta plus error. Transpose theta plus error. And under this true model, and this model, most of the time, and it is a misspecified or wrong model, but nevertheless, there is a good interpretation for this parameter theta. And we can interpret the theta as a minimizer of this loss function. This is a distance square, very commonly used the distance square loss. And also after a little bit more algebra, you can see because we know, suppose we know the fx. know suppose we know the fx and this say the say the star can be written as capital omega multiplies a capital c this capital omega we already call that is a precision matrix that is exclusively on x and the c is expectation of x multiplies y okay so in the literature sometimes people call this is an assumption lean framework in the sense that we assume there is an underlying true model that true model is as non-permanent Their true model is as non-parametric as possible. But we are going to interest in some other parameters or estimates that can be derived from a maybe a misspecified raw model from the true data generating process. And particularly in this work, we are going to mainly estimate or study the estimation problem of the CD star. And later on, we will assume this star is sparse and there's a high dimensionality. And then when we move on. And then when we move on to conducting statistical inference, we're going to mainly talk about a linear combination of the pseudo-star. We assume there is a V vector V, and we are interested in the statistical inference, or basically the inference, the asthmatic variance of the V transpose zeta star. Okay, so this is the basic setting of this problem. And what we can do, given this setting and also given the data structure, I introduced. And also given the data structure I introduced two slides ago. Firstly, if we only have labeled data, this is a very simple semi-supervised learning problem. There are rich literature on this topic. Basically, there is no unlabeled data. So this is just a very simple traditional high-dimensional problem. People already proposed a lot of methods, like Lasso, Danzinger Selector, both of which are mainly for estimation. And later on, there are a lot of literature on high-dimensional. There are a lot of literature on high-dimensional inference. Okay, so the main statistical interest in this work is we are going to think about what is the benefits of the unlabeled data. One key observation is if we have a misspecified model, because the CDA star, our permit of interest, is derived from a misspecified model, we realize that the axiom unable data may improve both estimation and the inference of the CDA star. While this may Well, this may not be true if your model is correct. So, that is why this is a very key component or key observation in our work. And some key ingredients other than high-dimensional techniques, we are going to use a lot of semi-parametric ideas. Basically, especially in the inference part, we mainly play with the semi-parametric efficiency influence functions, especially under the missed specified models. Okay. By the models. Okay, and I will first give a very high-level overview for both estimation and inference. For the estimation part, this is a more interesting problem in terms of under the high-dimensionality setting, because we have a benchmark supervised estimator CT star. Suppose it is derived from LASO or Danzig selector. And then our statistical question is whether the enabled data could be incorporated to construct an estimator with a faster. Estimator with a faster convergence rate. And later on, I will present what is the upper bound for supervised estimator and what is the minimax lower bound for a semi-supervised estimator and how we can achieve that. Okay, so this is a mainly list the strategies we use and the results we obtain. So we first establish a minimum lower bound and then we show that the supervised estimator cannot attend. Supervised estimator cannot attend its lower bound. And then we propose a semi-supervised optimal semi-supervised estimator, CD D hat. And we are going to show this estimator will achieve this minimax lower bound. And the implementation of this estimator will need a f hat x, which is basically the true conditional mean function of y give x. So this, well, I will give some discussion later, but this is reasonable because if you want to. Because if you want to achieve something that has the optimality, then you probably will need to estimate everything in the data generating process. For inference part, it's relevant, but the questions are different. So for the inference part, the statistical question is whether the unlabeled data can be beneficial towards an estimator of V-transport statistical star. And this estimator should not only be osmotic and normal and should also be more efficient. And should also be more efficient than the supervised estimators. And we mainly propose two approaches to solve this problem, to answer this question. We first propose a so-called safe semi-supervised estimator. This is something that I feel the most excited in this work. We call this, say, the hat, subscript S and the superscript D. And this estimator can be guaranteed to be always more efficient. To be always more efficient than the supervised device estimator, no matter how you estimated your f hat x, which is a conditional mean of y x. And fortunately, if this f hat x is available, like in the inference part, like in the estimation part, and we are going to show this estimator will become to a seed hat D without the subscript S. And we are going to show this estimator, seed head superscript D will attain the same. Superscript D will attain the semi-parametric efficiency bound when capital N, the sample size of the unlabeled data, is sufficient. Therefore, we call this an efficient or semi-supervised estimator. So basically, for inference part, I will mainly present the two estimators. Okay, there are tons of literature on this topic, especially in recent years. And on this slide, I will only highlight four of them. And the first one is chakra body and the Thai in 2018. They only consider a low-dimensional problem, a fixed P. And their idea is they're going to use some kind of imputation technique. This is Anna's paper. And another paper in 2019, they considered a high-dimensional problem, but their approach can only estimate the mean of. Can only estimate the mean of response Y, and their high dimensionality has a lot of restrictions. And correspondingly, there's a 2020 GRSS CB paper, they consider a correctly specified linear model and they estimate an estimand called the explained variance. And also, in more recent paper, John and Braddick, they also studied a similar problem, but again, their approach. A similar problem, but again, their approach can only estimate the population mean and the population variance, and they have some restrictions on their sample size of the labeled data and unlabeled data. So, basically, the idea is people study this problem using different approaches, but each type of approaches will have their own strengths and the limitations. So, I think this will be the outline of this. I mean, the introduction of this. Of this, I mean, the introduction of this talk. Maybe I will just pause like 10 seconds for everyone to have a quick mental snapshot, and whether you have any questions, you can just point out or any clarifications. I didn't see any questions, so maybe you can continue. Okay, thank you, Chris. Okay, so next I will the talk will have three parts. I will first talk about the ST. I will first talk about the estimation results and the inference results, and also I will show some numerical results. So, for estimation, well, my slides will have some technical details, but I will try my best to explain using Lehman terms to show each of the terms or each of the formulas is going to, what are each of the terms going to talk about or what are their meanings. What are their meanings? So, for the estimation part, we will have to start with a supervised estimator under a model misspecification setting. And for people who work on high-dimensional data, Ibico 2009 paper is a very important part, a very important literature for our work. However, their paper, they didn't consider the model mis-specification. So, our work will start with Biko's paper, but under a With Bico's paper, but under a misspecified model. Nonetheless, we can somehow follow the details of their proof. We call this C that hat L is the labeled data only based densing selector estimator. And we can show the lower bound for this estimator. Basically, the L1 norm has this order, where the S is the sparsity level. Usually, people just assume this S is much smaller than P, and most of the time it's also much smaller. And most of the time, it's also much smaller than the simple size n. And of course, root of log p divided by n. This is a very commonly seen term in high-dimensional literature. And I will mainly explain these two terms. This is sigma, probably you already seen before, that is the variance of the error in the true model. And most, and we are going to assume this part is going to be the big O1 term that is a randomness error. And also, randomness error. And also we are going to, we have another one is this phi term. This phi is an error due to the model missing specification. This is the definition of phi. It is the upper bound for the diff for the distance between the true condition fx and the linear combination in the x transpose theta star in the MISI specified model. And a very important assumption we make and we think is also Make and we think is also reasonable. Is we assume this phi square can go to infinity. Well, that means under high dimensionality, and basically, one cannot control how your misspecified linear combined conditional mean model, X transpose to the star, how wrong it is. So, we assume this part, the distance, this part of the distance with the true conditional mean can go to infinity. And in the paper, we can easily. And in the paper, we can easily demonstrate or have some examples where this phi has the order of root of the sparsity level s. So, of course, it goes to infinity as a symbol size small n goes to infinity. Okay, so well, basically, this slide just to show this is the most important message is the upper bound of a supervised estimator. This is basically you can regard as from the current literature with a little bit of modification. With a little bit of modification. And then the other side is we want to understand with our setting, the semi-supervised data setting, what is the optimal bound that one can achieve using whatever method. And this is the so-called minimax lower bound. The order I highlight on this slide, it has two components. This is the minimax lower bound, basically the optimal bound that one can achieve for any possible estimator state hat. And it has two components. Two components. The second component, I can take a ratio of the two components, and the ratio is basically this term in the red box. That is phi divided by sigma multiplies a ratio between the sample size. Well, the idea is we call this part here is something like irreducible. So basically, because the sigma is an error, is a randomness error from the true model. I mean, from the assumption of the true model. The assumption of the true model. So, this somehow one cannot further reduce. But somehow, if this term, you know, if you compare this term with the constant, with big O1 term, it's possible that the first order, the first term in the order, basically this term, can be reduced to the second term in this order. So, let me explain in a graph. So, in this graph, I have x-axis is basically the rate. x-axis is basically the ratio. I mean, the inverse of the ratio, capital N plus small n divided by small n. And it starts from one. When it adds one, it means there's no unlabeled data. So when there's no unlabeled data, the order is the upper bound of the supervised estimator. This is what we see from two slides ago. And then if this ratio, phi squared divided by sigma squared, and I said. square divided by sigma square as I said before we assume this ratio can go to infinity. If this ratio somehow equals this capital this capital N plus small n divided by capital divided by small n, that basically means this one will equal to a constant. And then we can show that this order here will be the minimax lower bound. That means once you increase You increase your analytical data sample size to this part, such that it equals to phi squared divided by sigma squared. And then the corresponding order can be reduced from the order of the supervised estimator to an order which is somehow the optimal. Okay, so two messages. First one is if there's no enable data, well, you can see that this is the order from the This is the order from the supervised estimator, which of course is not optimal. And then, if you have more and more unlabeled data, it means your capital N is getting bigger and bigger at some point. And one should be able to obtain, obtain an order which is going to be optimal, right? And also, after that point, if you further increase the capital N, and this other cannot be further reduced. So that is why we. For the reduced, so that's why we call this order is going to be the irreducible order. This is basically the meaning of this graph, and also it motivates us to propose a new estimator, which will use the information from the unlabeled data such that the corresponding estimator will achieve this order. Okay, so this is the basic idea so far. In the estimation part, we first show the current order of a supervised estimator. We show that it. Supervised estimator, we show that it is not optimal. And then from the other side, we show that minimax lower bound. We show what is the possible optimal order. And then we, from this graph, we demonstrate that under what type of situations it's possible that we can achieve this order. Okay. And next slide is on the construction of an optimal semi-supervised estimator. Again, here I mean optimal is I want to make sure that the I want to make sure that the constructed estimate will achieve the minimax lower bound. Actually, the creation of this estimate is very simple. We only need one simple step. Basically, you know, we are going to work on this score function for the estimation of sieve star. And if there's no unlabeled data involved, we will have a term that is the average across all the labeled data part, across all the small n. And we are going to replace this. We are going to replace this term with the average of all the data small n plus capital N terms because you know this is this term only has X in it and actually we assume I didn't say specifically but all of our work we assume the so-called MCAR mechanism basically the distribution of X in labeled data and unlabeled data are the same so there's a missing completely at random situation so this one this one So, this one, this one, these two will have the same expectation. And when we do their final sample version, we can replace the average of the small n to the average of the small n plus capital N. But later on, you will see this will play a very important role, the key role in the construction of this optimal semi-supervised estimator. Why is that? Because we can demonstrate, once we replace that step, we can estimate our precision matrix using all the information. Using all the information from labeled and unlabeled and also unlabeled data. And also for this cassie part, we basically have a term that is the average. There's a subtract, right? Average of x multiplies fx. And also there's a plus and average of n plus capital N X multiplies FX. So imagine if there's if the capital N equals to zero and then you did not use any information, you don't have the unable data for the Kasi bar part. For the KC bar part. But if you do this kind of modification, you of course will use the information contained in the Anablo data for the estimation of KC bar. Okay, that's it. This is just the one key step for the construction of this estimator. And once we have that, we can propose a dancing selector type estimator. And we are going to estimate both this matrix and also Cassibar. But one more thing is: you know, in this case bar, we'll have to estimate this fx, right? That is a Effects, right? That is a little bit of complexity. The main message is: we'll have to adopt the cross-fitting approach. This is very popularly used as early as 1980s in semi-parametric literature, but in recent years, it was very popularly advocated by double machine learning literature. The basic idea is we will have to estimate something that we call it's a nuisance when you produce. In when you produce an estimator, and then there's some kind of intercorrelation between the nuisance you estimated versus all other data you use. So, once you use this curl splitting or sample splitting technique, you can avoid this type of intercorrelation in the theoretical proof. So, other than this approach, we'll have to talk about how to estimate this f hat. Remember, f hat is the true conditional mean function in our model. Well, this is Function in our model. Well, this is going to be very hard in general, right? Because under a non-parametrical setting, I don't think we will have a very unified good approach to estimate f hat. So our theory is based on some conditions on how good we can estimate this f hat. So this slide is on some preparations for theoretical derivation. Because of the time, I think I can skip this. Well, but the message is everything is very standard. Is very standard, um, is very standard compared to the high-dimensional literature. And one thing I maybe need to point out is: we are going to assume this Cedar star is as sparse because, again, we are considering a missile-specified model. So under high dimensionality, how you can guarantee your Cedar star is also sparse under a missile-specified model, which is not as straightforward, right? So, there is some literature, Peter Roman and Is some literature on Peter Broma and Sarah Van De Gier did talk about this issue in the previous literature. So somehow we also borrow their idea for this kind of theoretical proof. So this is the condition I mentioned for how good we are going to estimate this F hat. Basically, we are going to need this order B and is going to be small O1 term. Well, this can be achieved in some literature that as far as we know, in a sparse additive model, that was. Sparse additive model that was proposed in 2010 and this paper, they can achieve this type of order. So I think there's a lot of complexities that I can talk about, but I think I will just mention that to achieve the optimality of this D hat, we will have to have a very good estimator, or just roughly speaking, we will have to estimate this F hat correctly under the L2 norm, under the L2 norm. So once you can ensure So, once you can ensure that, and we can show this D head will achieve this minimax lower bound up to a log of S factor, but that is going to be ignorable. Okay, I think that's the result for the estimation part. And any questions? I didn't see any so far. Okay, great. Sounds good. Great. Sounds good. But I want to emphasize: don't be scared about all of the techniques. So, this is a once you follow some of the high-dimensional literature, this is feasible to do. And also for the estimation part, I also need to clarify, maybe it's more interesting work under the high dimensionality. Maybe under the low dimensionality, we won't have this kind of complexity. However, for inference, it's going to be different. Inference, it's going to be different. Remember, our problem is for inference, we have a lot of unlabeled data. We want to make sure that how this unlabeled data can be beneficial for estimating the theta star. Well, this problem is not a trivial problem, even under low dimensionality. So we approach this problem from the efficient influence function. We consider, we compare two different efficiency influence functions. The first one is under the setting that if we only have labeled data. If we only have labeled data, which is we only have the one to small n data. And then under this situation, you know, the true model generating process is non-parametric, purely non-parametric. But we can define our parameter of introduction to the star, right? So this is also a semi-parametrical problem, and the corresponding efficiency influence function is in this form. Actually, this is very easy to understand. This is just if you use the least square, and maybe there are some adjustment for this. Maybe there's some adjustment for this omega matrix. While the other efficient informs function is, we are going to consider this model where the p-axis is known. While we consider this model, this is going to be an approximate model when you have sufficient unlabeled data. Well, the intuition is if your capital N is large enough, you can almost assume the distribution of X is completely known. Well, once you assume this model, you derive its This model, if you derive its corresponding efficient influence function, it's going to be in the form of equation one. Well, this efficient influence function is not friendly because the parameter C star does not contain in this efficient influence function. In other words, if you only have this efficient influence function, you do not have a way to estimate the C the star, right? So normally people just solve the empirical version of the efficient IFs equals to zero to get an estimate of the C the star. To zero to get the estimate of zeta star, but this one doesn't have z star in it. However, it gives us the idea of if you assume the distribution of x is known, what is the minima, what is the lower bond, what is the semi-parametric efficiency bond that one can achieve for estimating C the star. Okay. And also, you know, there is a decomposition for this, I mean, for the difference of these two terms. Our idea is we somehow get going to play with. We somehow get going to play with, or another very important message is: if you want to implement the equation one, this efficient influence function, you will have to make sure you can correctly estimate fx. Okay, so in other words, once you want to implement equation one, you will have to make sure that you have a good estimator for fx. However, under high dimensionality, again, we cannot ensure that. So our idea is we're going to somehow play with this two efficient influence. With these two efficient influence functions, we're going to make sure no matter your estimator of FX is true or wrong, is true or not true, and you will always have a better estimator in terms of more efficient estimator than this one, than the labeled data only based estimator. Okay, so what do I mean? I mean, you know, take a look at the equation two here. I have this T1. Forget about the. Forget about the big this omega matrix. The T1 is basically the efficiency influence function for labeled data, based on the label data only. And then I will create a T2. I will make sure that this T2 is going to be mean zero. This is basically, I write this form for the easier understanding of the notation, but somehow this T2 is just a mean zero equation of any function of X. The Mx here. function of x the mx here is going to be arbitrary the idea is i'm going to do a projection or you can understand you do you regard this t1 as a response as a double coated y and t2 is a double coated x and you do a regression of t1 on t2 so basically you do a projection of t1 on t2 and then this b matrix is basically the coefficient in that projection or in that regression okay and what we can get from And what we can get from doing this projection, we can get this t1 minus b transpose t2, this vector or this influence function is going to be always more efficient than t1 only. This is just from the orthogonality in the projection approach, right? And also, you can examine if you fortunately choose this fx as a difference between As a difference between the true, sorry, if you fortunately choose this mx as a difference between the true f and the linear combination x transpose c star, this two will become one. Okay, so these two boxes give us a very good story about why we are going to use this equation two as a fullness function to create our estimator. The first message is just give me arbitrary mx, any function of mx, using this equation. Max, using this equation too, I can guarantee that corresponding estimator is going to be more efficient than T1 based estimator. T1, again, it's labeled data only, right? We call this is safe because MX could be anything, but it's always going to be more efficient than labeled data only estimator, right? And also, the second thing is if the MX is fortunately chosen as some kind of optimal. chosen as some kind of optimal Mx, which is Fx minus X transpose theta star. And this equation two will automatically become the efficient influence function under this model, under this model, here, under the equation one. So it means if you use equation two, it will indicate if you choose your MX very wisely, it will achieve the efficiency. Okay, so this is the So, this is the basic idea of how we play with this influence functions. And the next two slides, we will have to talk about how to estimate this matrix B and also matrix omega. Both of them are high-dimensional matrices. So they are not straightforward. Somehow, this type of problem are already discussed in the previous literature. So, there's nothing new here. Literature. So there's nothing new here. So I think I would just skip these two slides. This one is just the estimation of B, and this one is on the estimation of omega. Okay. And then while I'm going to come back to this safe influence function, before we can construct the final estimator, we will have to make sure that all of the terms are well taken. So for this one here, xy minus b transpose t2, we are going to V transpose T2, we are going to propose a corresponding estimator for it. We call this Cassie S hat. Okay, and this one is basically the matrix sigma. So we are going to use the sigma n hat to estimate. And then combined with the idea of the one-step estimation, or it's also the de-biased estimation, especially for high-dimensional data, this estimator here is the final estimator we propose. Is the final estimator we propose, the safe semi-supervised estimator. Again, this omega hat is something that I skipped from the previous slide. See the L hat is a labeled data-based, labeled labeled data only-based estimator. This L represents labeled data. It's at the very beginning when I talk about the estimation. And the Cassius S hat is just this equation here. And the sigma n hat is a very regular estimator for the precision matrix. Estimator for the precision matrix. Okay, and then on some more technical details. I will skip the assumptions and the discussion of the assumptions. I will mainly present what this astrophotographic normality can tell us. Again, our framework is we allow both small n and p go to infinity. And in the final estimator, you see that we will have something like this. We will have something like this capital N there. So, well, this equation here is not super accurate because, I mean, we want to let this ratio in the final gamma matrix to talk about the efficiency gain. So, basically, the story is on the next slide. Firstly, if your model is correctly specified, it means your conditional mean is indeed. Your conditional mean is indeed the linear combination. And then this gamma matrix in the previous slide will equal to zero. So basically, this one will equal to zero. And then the astronautic variance will reduce to sigma square v transpose omega v. Well, I think I have a typo here. It's not, this should not be right. It should be the this demo may. This gamma matrix will reduce to a sigma square, so then the corresponding estimator will be sigma square v transpose omega v. So this is a pipeline, this is not zero. And then it means if your model is correctly specified, and then no benefits from the enable data, because this osmotic variance is the same as the supervised device estimator, and it also matches a semi-parametric efficiency bound. It means there's no way to improve your efficiency. This is Improve your efficiency. This is very reasonable because if your conditional mean model is correctly specified, it basically there's nothing the unable data can help. Okay, but more interesting is the next few items. First one, if this limit equals to one, it means there's no unlabeled data, right? It's a capital N equals to zero. And then we can show the previous slide as negative variance will reduce to this term. It means To this term. It means, well, this one will be the same as the supervised estimator, supervised devised estimator. This conclusion is also very reasonable because if there is no end of the data, the estimator we construct will be same as the supervised estimator. And if the ratio equals to rho, that is something between zero and one, we can show this asthmatic variance is strictly smaller than this component. It means there. Component. It means there is going to be some variance reduction. And the extreme case is if this limit equals to zero, that means you have sufficient many capital N over the data. And then the astronomical variance will achieve the maximum reduction. Okay. Sorry, I remind you five minutes left. Okay. Okay. Thank you. So as we can see from these three items, the corresponding estimator will The corresponding estimator will always be smaller than this estimator, right? So that means our estimator is safe, it's going to be guaranteed to be more efficient than the supervised estimator. Okay, because of the time, I think I will just somehow skip this optimal semi-supervised inference. And the idea is the same because this is the one we want to create an optimal estimate that can achieve the efficiency bound. Achieve the efficiency bound and it shares the same story as the previous safe estimator. It also has the similar stories here, but the difference is here, if this limit goes to zero, and we can show this estimator will attend the semi-parametrical efficiency bound. Okay, so I will skip all of this because for the numerical studies, you know, simulations, you can create mods, some models that. Can create mods, some models that you can correctly specify, some others you cannot correctly specify. And basically, everything is going to be, although it's very complicated, everything is going to be matched with our theory. I will skip all of this and we'll just use one minute to review our real data application. This is a data from the Mimic 3 database. This is a very large-scale EHR database. And what we do is we have And what we do is, we have some kind of outcome of interest: this album level in the blood sample. In this database, there are around 5,000 patients, which has this Y value. And we do a lot of pre-processing, you know, because in the real data, there are a lot of missingness in the covariance and also correlation, collinearity in the covariates. So we do all of those pre-processing. And finally, we focus on sample with capital equals to 900. capital n equals to 900 and small n is 100 and the p is 160. We compare three methods. I mean this application is mainly for the inference part. We didn't show, I didn't prepare the slides for the estimation part. So the first estimator D La Sau 1 is a very traditional Lascelle based estimator using labeled data only. So you can see this column, red column here is a half of the length of the confidence. A half of the length of the confidence interval. And then the second column, this SSSL, is a safe, semi-supervised estimator. And the corresponding half of the length of the confidence interval, it is smaller than the D-La-So estimator. Okay. And the optimal one is somehow we use a more refined method to estimate the F-hat, and you can see it is going to be more efficient than the safe estimator. To be more efficient than the safety estimator. And scientifically, you know, our SSSL can identify three more biomarkers besides the D-Lasso method. And we find that there are some evidence from the medical literature. For example, the albumin level, it associated with hemoglobin, with lymphocytes, with white blood cell counts. And also, we also found there's in the literature, there's so-called HALP score. Literature, there's so-called HAUP score, which there's a lean is a kind of combination of the hemoglobin, albumine, lymphocytes, and platelet, which can indicate that there's some kind of clinical evidence to show the high correlations among of the biomarkers. Okay, some take-home messages. We investigate the benefits of antibodata under a model message specification. I think I put too much material. I think I put too much materials on this talk. We focus on both estimation and also inference. And other than high-dimensional techniques, we think that the semi-parametric efficiency influence function plays a very important role for the improved inference. We have some ongoing work along this line of research. We consider unlabeled and labeled data from the different populations. We also consider some other different estimates. I think I will just stop here. And we have a QR code for the archive version of our paper. We are doing a revision right now. So if you have any questions, just let me know. Thank you very much.