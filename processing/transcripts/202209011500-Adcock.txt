Great. Thank you very much. And let me, before I start, extend my thanks to the organizers for putting together this workshop. I've really enjoyed it so far. It's been a really interesting week of talks. So let me get started just by mentioning my collaborators. So this is joint work with two of my PhD students at SA. Of my PhD students at SFU, Juan Cardenas and Sebastian Moraga, and also two of my former postdocs, so Simoni Brugier Paglia, who's now at Concordia in Montreal, and Nick Dexter, who's just very recently started a faculty job at Florida State. And before I get started, also a shameless plug for my recent book. So if you're interested in polynomial approximation in Heidegger, In polynomial approximation in high dimensions, you might want to check out this book, co-authored with myself, Simone, and Clayton-Webster. It was published by SIAM earlier this year. So please check it out if you're interested. Okay, so the title of the talk is, Is Monte Carlo a bad sampling strategy for approximating smooth functions in high dimensions? And to give away the answer, before we go any further, the answer is. Go any further, the answer is, or what I'm going to try and convince you in this talk is: the answer is no. And the short, very short proof of this actually is you can just refer to what's known as Betteridge's law of newspaper headlines, which says that any newspaper headline that ends in a question mark can be answered by the word no. So the same is true for this talk as well. So I'm going to try and convince you that Monte Carlo sampling is actually a good sampling strategy in high dimensions. high dimensions. Okay, but let me start by introducing the general topic of the talk. So the problem that we're interested in in this talk is to approximate a smooth function f on some domain u, which we're going to take to be a cube in d dimensions from sample values. Okay, so very standard problem. Sample values, sample points are going to be denoted by yi throughout this talk, and at various points, Talk and at various points we'll allow the samples to be noisy as well. The focus of this talk is high dimensions, so d much much bigger than one. Actually, when we get to the theory, we'll talk about functions of infinitely many variables. So why am I, where do the motivations for the particular case of this problem come from? In my case, they come from problems in parametric. Problems in parametric modeling, so parametric PDEs, and its applications to uncertainty quantification and construction of surrogate models and that kind of thing. So here, think of the function, if you like, for the rest of the talk, f as some quantity of interest of a parametric or stochastic differential equation, where the variable y is a vector of parameters. One key thing to emphasize for Emphasizing for this problem, besides from it being high-dimensional, because there are lots of parameters in sophisticated models, generating the data is expensive. So in a parametric PDE, a stochastic PDE, every time we want to sample our function f, we have to solve a PDE. So that's computationally intensive. So we're interested in using as few samples as possible. Okay, so. Okay, so I said the function is smooth. I will later in the talk get to the precise definition. But what I mean by smoothness in this talk is that the function is holomorphic. So it's a holomorphic function in the variable y. Why do we make this assumption? Well, there's a long line of work starting in the mid-2000s that is established for very broad classes of parametric ODEs and PDEs, that the solution depends on the parameters in a holomorphic way. So there's a long series of papers by various people, including Chris Schwab, Albert Cohen, Rowan DeVore, and many others that have shown these kind of results for parametric Ds. If you'd like a nice, or if you'd like a survey of these kinds of results, we have a chapter on. Kind of results. We have a chapter on it in the book that I mentioned at the start of the talk. There's also a very nice actor numerical article by Cohen DeVore, which I think people will be familiar with as well. Okay, so since our function is holomorphic, this is going to motivate us to look at polynomial approximation methods. And in particular, two types of classes of methods that have received a lot of attention are methods based on least squares and also more recently. And also, more recently, methods based on compressed sensing. So, we'll talk about those later in the talk. Okay, so let me elaborate a bit more on the purpose of this talk. So what I want to focus on in this talk is how we choose the sample points. So these, remember, these are the points y1 through to ym. Okay, so first of all, let me be clear what I mean by Monte Carlo sampling. By Monte Carlo sampling, I mean that we draw the sample point. That we draw the sample point from some probability measure rho over our domain. So, this is very standard. This is in the parametric DE, stochastic DE community. This is kind of what you do as the sort of first thing to do. However, although this is very simple, it's known to be theoretically suboptimal. I'll talk about what I mean by that. I'll talk about what I mean by that precisely a little later in the talk. But what I want to emphasize now is this observation that Monte Carlo sampling is theoretically suboptimal has led to a lot of research on improved sampling strategies, including sampling strategies that are theoretically near optimal. So optimal up to log factors and constants. So obviously we've heard Obviously, we've heard a number of talks along these lines this week. There have been very nice talks by Mario Ulrich, Tino Ulrich, Vladimir Temnikov, and several others about essentially about sampling strategies that are near optimal or optimal for approximation in certain function classes. I should point out because I'm coming from this as a slightly different I'm coming from this in a slightly different angle. So I'm coming more from the parametric DEs and UQ side of things. I also want to point out as well that there are a lot of sort of improved sampling strategies that people have developed as well that are not necessarily theoretically near-optimal. They might even not have theoretical guarantees, but they all sort of improve over Monte Carlo sampling. So I've listed a bunch of papers here, all of which introduce different types of sampling strategies and all of which gave Strategies and all of which gained some improvements over Monte Carlo sampling. So, there's been this long line of work to try and do better than Monte Carlo sampling. But if you kind of go and dig through the literature and you look at the numerical results in the papers that have numerical results, there's a phenomenon that you see sort of consistently throughout all these papers. And that's the following. So, in low dimensions, In low dimensions, any kind of improved sampling strategy does tons better than Monte Carlo sampling. I'll show an example in a second. However, as the dimension increases and you start trying to approximate higher dimensional functions, the benefits almost always diminish. By the time you get to sort of high dimensions, whatever you mean by high dimensions, there may be very marginal gains or no gains whatsoever over these kind of improved strategies. These kind of improved strategies. So, this is really what the talk is about. The talk is aiming to explain why this phenomenon occurs. Let me, before I start getting into any more technical details, let me just start with an example to explain what I mean. So here's a function of d variables. And what we're going to do is just change the dimension. So, what I've plotted here, this is a least squares approximation scale. Is a least squares approximation scheme. Okay, so it's actually an adaptive least squares scheme. I'll explain that more a little later in the talk. And we're using two sampling strategies. So one is Monte Carlo sampling. So in this case, I'm working on the unit cube in D dimensions, and my measure is just a uniform measure. And then the other sampling strategy is one of these near-optimal schemes. In fact, it's the scheme of Of Cohen and Miglerati, this one here, which I think we've seen earlier in this workshop numerous times being mentioned. So what I'm plotting here is the, on the left, I'm plotting the error versus the number of samples, M. And on the right here, I'm plotting the condition number of the least squares matrix. So what you see here is that Monte Carlo does very bad, okay, badly. So initially, the error decreases, but then it's The error decreases, but then it starts blowing up exponentially fast. And the reason for this is essentially related to the condition number. So the condition number is also blowing up essentially exponentially fast as well. And by the time we have 1500 samples, we have a condition number that's sort of on the order of 10 to the 14. Okay, so horrendously ill-conditioned and unstable. Conditioned and unstable. So, this is in dimension one, and this is very well known. Now, as we start increasing the dimension, the picture starts to change. So, in two dimensions, already the approximation error looks a lot better, but the condition number is still growing for Monte Carlo sampling. And but if we increase to four dimensions, the things get closer between the two schemes, and as we keep increasing the dimension. And as we keep increasing the dimension to 8, to 16, and then finally, if we go to 32 dimensions, we'll see there's virtually no difference between the two strategies. So they produce essentially the same error and they have roughly the same condition number. Okay, so this phenomenon is exactly what the talk is about. Okay, so let me move on with the next part of the talk. So I want to. The talk. So, I want to discuss least squares, polynomial approximation by least squares first. So, okay, let me clarify exactly the setup we're going to consider. So, for the rest of this talk, u is going to be the unit cube in D dimensions, and where we're going to allow d to be infinity as well. Rho is going to be the uniform measure on u. Rho is going to be the uniform measure on U. This is primarily for simplicity. One could consider other measures. In the paper, we also look at the Chebyshev measure, but we'll just keep things straightforward in the talk and think about the uniform measure. And so given you and the probability measure, I'd like to construct the orthonormal polynomial basis on this domain. So this is. So, this is the orthonormal Legendre polynomial basis, and I form this just by taking tensor products of one-dimensional Legendre polynomials. The only thing we need to be careful about in infinite dimensions is how we index things. So, in infinite dimensions, we consider multi-indices that only have finitely many non-zero entries. So, this is what this set F is. So, in finite dimension, it's just the set of all multiple. It's just the set of all multi-indices with non-negative integer values. In infinite dimensions, you only consider the multi-indices that have finitely many non-zeros. Anyway, this sort of technical thing aside, once you construct your basis, you have an orthonormal basis of Legendre polynomials. Okay, now we have an orthonormal basis. So if we want to do some kind of polynomial approximation. Kind of polynomial approximation. What we're going to do is we're going to pick a set of multi-indices S. This is a finite set of cardinality n throughout the talk. N is going to be some number less than or equal to m, which is our number of samples. And then we consider the subspace spanned by our Legendre polynomials with multi-indices in S. Throughout the talk, I'll call this P subscript S. So this is the space in which we want to. So this is the space in which we want to approximate our function and the simplest thing that we can do, arguably, is to do some kind of least squares approximation. So we approximate f by f hat, which is the best fit polynomial fit from our subspace to our data. And we could also do a weighted least squares approximation as well. I'm not going to go delve into the details of that. So we could do least squares or weighted least levels. Of that. So we could do least squares or weighted least squares. Okay, before I move further forwards, as a way to sort of motivate something, I'll talk to you towards the end of the talk. I just wanted to say a couple of things about the choice of the set S. So we want to construct an approximation in PS. So obviously, the first thing we have to do is decide what set of multi-indices to consider. And of course, this is fairly standard stuff. There are standard ways that you might choose this set S. So, if you work in low dimensions, you might use a tensor product index set. So, tensor product, you just take the tensor product of 1D indices up to some finite maximum. Of course, this has cardinality that grows very badly with dimension. So you might use a total degree index set. And in some sense, this is perhaps more natural when you're doing. This is perhaps more natural when you're doing polynomial approximation. Of course, the size of this index set still also scales poorly with dimension. So, in moderate dimensions, you might use a hyperbolic cross, of course, which is, you know, we've seen the hyperbolic cross come up numerous times this week. Or if you want to go to sort of higher dimensions, you might start looking at anisotropic versions of these index sets. So, an anisotropic tensor product index set would be a rectangle. Be a rectangle in 2D. You can define anisotropic total degree index sets and also anisotropic hyperbolic cross-index sets. One thing to note here is that these are all examples of lower sets or downward closed sets or monotone sets, however you wish to refer to them. So they all have the property that if you have an index in your set, then all the other indices Then, all the other indices that live in the rectangle with this index being one of its vertices, they also belong to the set. Okay, so the reason for sort of having this short discussion about index sets was to motivate what I like to refer to as unknown anisotropy. So, we know if we want to do some type of high-dimensional proxy. Want to do some type of high-dimensional approximation, we need some kind of anisotropy assumption on our functions. It should depend more strongly on some variables than others. Otherwise, we're going to generally succumb to some cursive dimensionality. So, in practice, you might not know the nature of this anisotropy. So, what I mean by that is you might not know which variables. You might not know which variables, which coordinate directions your function is varying most strongly in, and you might not know the relative strengths of these interactions. So, if you want to set up an index set like an anisotropic tensor product or a total degree index set or a hyperbolic cross, in this case, where you're putting more indices in the new two direction than the new one direction, you need to know that your function is varying more strongly or with respect. Or with respect to its second variable rather than its first variable. So in practice, you often might not know this behavior in advance. So what can you do in practice? There are various ways you might try and treat this. Perhaps the simplest is to do some kind of adaptive approximation scheme. For instance, an adaptive least square scheme. instance an adaptive least square scheme. So here in adaptive least squares scheme, you would generate a nested sequence of lower sets S1, S2, et cetera, et cetera, and the corresponding approximations. And you would do this in a greedy manner. So given the ith approximation f hat i here, you would then use this to estimate the coefficients of the function lying on the margin of your index set s i. Of your index set SI, and then add those that were most important in some appropriate way. So, this would be a greedy method. Okay, so this is being quite widely considered. There's a bunch of references on this here. We'll see this in the numerical experiments in a minute. However, later in the talk, if I get time, I'm going to also talk about a different approach to handling this situation, which is based on compressed sensing. This is something Sensing. This is something that's been used quite a bit, especially in parametric PDE problems. And we'll see this a bit towards the end of the talk. Okay, so putting that aside for now, let's get back to the question of sampling strategies. So let's suppose now that we have chosen an index set S. Let's assume it's a lower set. And this is the set in which we want to. And this is the set in which we want to affect our, or to construct our polynomial approximation. Okay, so Monte Carlo sampling, as I said, very standard. Draw my sample points IID from rho, which is the uniform measure I'm assuming from now on. Unfortunately, it's known that this has poor sample complexity in general. So what I mean by this is that there are choices of S, choices of lower sets S, for which the sample complexity or the best boundary. Complexity, or the best bound on the sample complexity that can be shown is log quadratic in n. So, in other words, you need something like m being a constant times n squared times log n samples in order to construct this approximation. And in fact, if you don't assume that s is lower, you can make the scaling essentially arbitrarily bad. If your set S contains multi-indices that are sort of S contains multi-indices that are sort of arbitrarily high or far out in your index space, then this sample complexity will scale with those. So I should point out here that this log quadratic sample complexity is not just some limitation of the proof techniques or something like that. This is actually something that you see in practice. So we already saw this briefly in the motivating example, but let me repeat. The motivating example, but let me repeat a similar experiment here to emphasize the point. So, if you scale your m more slowly than log quadratically, then the least squares approximation is going to be ill-conditioned and it's going to be potentially divergent. So, in particular, if you scale your number of samples m log linearly with n, then it will be exponentially your conviction. And here's just an example. Conviction. And here's just an example showing that this is definitely the case. And up to the fact that these are random samples, I mean, this exponential ill-conditioning and divergence has essentially been proved. So it's been proved for exactly equispace points, but here we're considering points drawn randomly from the uniform measure. So they're roughly equispaced. Okay, so because of this kind of phenomenon, as I was saying at the start of the talk, there's been a lot of work on designing improved sampling strategies for polynomial least squares and also polynomial compressed sensing. Perhaps one of the first was this preconditioning approach of Holger Rauhood and Rachel Ward. Then Aliraza Dustin and his collaborators had a bunch. And his collaborators had a bunch of different strategies. Then there are things such as randomly subsample quadratures, while points, low discrepancy points, et cetera, et cetera, et cetera. There's a bunch of techniques as well, all empirical that rely on tools such as optimal design of experiments. So all these techniques are designed to get better results than Monte Carlo sampling. So most of these here. So, most of these here are not theoretically optimal or near-optimal, and many of these don't come with any kind of theoretical guarantees. But as we've seen in this workshop, there's in the last few years, what's emerged is essentially near-optimal and optimal sampling strategies. So this sort of started, I believe, with the paper of Albert Cohen and Giovanni Miguelati. Albert Cohen and Giovanni Miguelati. In fact, I think it's fair to mention as well that some of these ideas were actually found in this paper of Hampton and Douston in 2015. So what they call coherence optimal sampling is essentially a special case of what Cohen and Miglerati did a couple of years later. So what was introduced in this paper by Cohen and Miglerati was a random sampling scheme that possessed Scheme that possesses near-optimal log-linear sample complexity. So your number of samples m should scale like a constant times n times log n. And what's particularly powerful about these ideas is they can use, you can do this for any n-dimensional subspace. It doesn't need to be a space of polynomials or anything like that. It could be a space of wavelets or trigonometric polynomials or whatever you like. And in fact, you don't need your And in fact, you don't need your initial measure to be a probability measure either. So you can start with sort of any kind of measure you like, and you can do this construction. As we've seen, the sort of key idea here is to generate a probability measure based on the Christopher function of your subspace, and then draw samples randomly from that. So this gives you near-optimal sample complexity. And then what we've seen in the last couple of years is ways. Last couple of years is ways to refine this using these ideas from these papers of Spielman and Sri Bostava with Batson in 2014 and Marcus in 2015. And so we've seen a couple of talks on this already. Essentially, by subsampling from a set of n-log n samples drawn this way, you can reduce your sample complexity to linear in n. Okay, so let's get back to the main sort of premise of this talk. So, is Monte Carlo sampling a bad sampling strategy? So, let me show a few more experiments before we delve into some theory. So, what I'm doing here, we're going to show a few more experiments along the lines of what I showed at the start of the talk. So we're going to compare Monte Carlo with the near-optimal scheme of Cohen and Miglia. Near optimal scheme of Cohen and Migliarity. We're actually going to use adaptive least squares, so using this greedy procedure. And throughout, we use the scaling m is essentially n log n. Okay, so here's another example. So again, in one dimension, we see the same effect. Monte Carlo sampling is terrible. It's very unstable, and you get poor results. As we increase the dimensions, As we increase the dimension, though, the picture changes, and as we go to 16 dimensions or 32 dimensions, the results are indistinguishable between the two sampling strategies. Similar example here, so different function, but essentially the same phenomenon. So, again, once we go to higher dimensions, we see basically the same results. Finally, here's a parametric PDE. So, this is an PDE. So this is an elliptic diffusion equation. The function f that we're approximating is a scalar quantity of interest of it. It's just the solution evaluated at a point in the spatial domain. And again, we see similar effects here. So Monte Carlo is very bad in one dimension. But as we go to higher dimensions, we see the same phenomenon. Okay, so. Okay, so let's now I want to try and analyze this theoretically and explain why this occurs. So in order to do this, we need to specify the class of functions that we're going to be working with. And I'm going to work with the class of b epsilon holomorphic functions. So let me define what this class is for those of you who are not familiar with it. First of all, recall the one-dimensional Bernstein ellipse E rho. So, the first thing we do is we take Bernstein ellipses in 1D and we tensorize them. This gives us a Bernstein poly ellipse. So, this is defined by a sequence of parameters rho here. Then, what we do is we take a sequence b with non-negative values, and we take an epsilon greater than zero, and zero and we look at the complex region rb comma epsilon which is the union of all bernstein ellipses for which this condition holds so ace the sum over the parameters rho i or this quantity involving the parameters rho i rho i times the b i is less than or equal to epsilon. Then we say that a function is b epsilon holomorphic if it's holomorphic. B epsilon holomorphic if it's holomorphic in this region. And just to make life simple, so save me writing this norm throughout what's going to follow, I'm going to consider the class of such functions that are bounded by one on this region. Okay, so why consider this class of functions? Well, so I should say that this class of functions comes from work of Albert Cohen. Of Albert Cohen, Ronda Baugh, Chris Schwab, various others. And the reason for considering this class of functions is because it arises in parametric deeds. So in these parametric PDE problems, you can show under assumptions on your PDE that the solution map is a B epsilon holomorphic function for a suitable choice of B. Okay, so that's the class we're going to consider. Going to consider, and the first thing we need to know about this class is rates of best end term polynomial approximation. So, suppose we take a function f in this class and let's write fn for its best end term polynomial approximation, where the n term is with respect to our Legendre polynomial basis here. Then, one of the sort of fundamental results that was proved about this class by Cohen. By Cohen, DeVore, Schwab, and others, was that the best end term approximation error decays algebraically fast in n. So, to be precise, if b, this sequence is in LP, then this error decays like n to the one half minus one over p. And it turns out this rate is essentially sharp. Okay, so if your sequence b is not or fails to be weakly L. Or fails to be weakly LR summable for some R less than P, then you can find infinitely many functions for which this best end term approximation error won't decay or must decay slower than n to the one-half minus one over r. So this rate is essentially optimal. Okay, so if we want to explain what what we've seen in this Explain what we've seen in the numerical experiments, then one way to answer this is to ask: can we achieve this rate? So n to the one-half minus one over p, using Monte Carlo samples? So in other words, can I achieve the same rate, but in terms of the number of samples m? And this is exactly what Simone and I were able to prove earlier this year, and this is in the paper related to this talk. Paper related to this talk. So here's the main result, or the first main result. So suppose we have our B and let's suppose it's L P summable. We draw our M Monte Carlo samples. Then there's a set S, which depends on B and epsilon. And it has cardinality basically M divided by log M. m divided by log m such that for each fixed function f that's in our in our class, if we construct the least squares approximation, then with high probability it's unique and it satisfies the following error bound. So the error goes down like m divided by log m to the one-half minus one over b. And we're also stable to noise as well. So, what does this mean? Why does this address the question? So, what I said earlier in the talk is that Monte Carlo sampling is theoretically suboptimal because it can have a log quadratic sample complexity. So, from that earlier discussion, you might be inclined to conclude that the best rate you can achieve with Monte Carlo sampling for this class. With Monte Carlo sampling for this class of functions is something like m divided by log m to the one-half times one-half minus one over p, because you've got a quadratic scaling between n, the dimension of your polynomial space, and m, the number of samples. However, what I'm saying is that for this class of functions, we can actually achieve the rate m divided by log m to the one-half minus one over p. So in other words, Minus one over p. So, in other words, this is optimal up to up to constants and the log term here. Okay, so I don't want to go into too many details of the proof, but let me just say a couple of things about it. So, basically, the key ingredients to this proof are to show that there are for any function f in this class, there's a There's a set S with the following properties. So, first of all, it's of size M divided by log M, modulus of constants. And the S term, the approximation error for the best approximation to F in the polynomial space defined by S decays like n to the one-half minus one over p. Over p. Okay, so that's the first two properties. And then finally, the Christoffel function, or the technically the reciprocal of the Christoffel function of the subspace Ps is linear in n. And this is really the key thing. So in general, you might expect this to be quadratic in n, which would give you log-quadratic sample complexity. Here, what we're saying is that we can actually find a set S where this is linear in N. Linear in n. So, the sort of more intuitive way that I like to think about this is the following. If you think about when Monte Carlo sampling gives you quadratic sample complexity, it's when you have large multi-indices. So, an extreme case is when your set S just contains indices in one coordinate direction. So, for instance, these indices here. Okay, so the first S. Indices here. Okay, so the first n multi-indices in one coordinate direction. Okay, this is the worst case possible for Monte Carlo sampling. What we're saying here is that for the class of functions that we're considering, we actually don't need to go too high in any coordinate direction in terms of the multi-indices, but it's important that we cover all the dimensions, essentially. And it's essentially that that allows us to get this algebraic rate here. Right here. Okay, so that's the result for least squares. Let me see, I have a few couple of minutes left. I now want to go back to what I was saying earlier in the talk and discuss this unknown anisotropy problem. So let me just go back to the definition of this class of functions. Okay, so here's the definition of the class of functions. It's a bit unpleasant. Uh, it's a bit unpleasant to define, which is why I've come back to the definition. And what you notice here is that this sequence B is essentially what defines the anisotropic behavior of the functions in this class. So if B i here is very large, then this only holds for rho i being very close to one, which means your function doesn't have Have is less smooth in the variable yi okay because it doesn't have as large an analytic continuation at all. On the other hand, if bi is very small, or in the extreme case, if bi is zero, then this holds for large row i's, which means your function is very smooth with respect to the variable yi. So to summarize this, To summarize this, the sequence B is telling you about the anisotropy of your function. Smaller BI means smoother with respect to YI. So the previous results that we proved here, this is in some senses, is generally impractical because the set here that you have to construct depends on this sequence B. Depends on this sequence B, which you may not know. In fact, technically, we only say that there exists a set. We actually don't give a constructive way of finding it, even if you knew B. Although I think that could be done. So the question is, okay, well, what can we do if we don't know B? So in practice, you might do some kind of adaptive least squares. That's what I showed in the numerical experiment. That's what I showed in the numerical experiments so far. You might do some other sort of adaptive things as well. But the problem with these kinds of adaptive procedures is they generally don't have theoretical guarantees for these polynomial approximation problems, at least. So what I want to, I'm going to use instead is compressed sensing ideas. So in order to do this, though, I do have to make an additional assumption. So I'm going to assume now that B. So I'm going to assume now that B is not just L P summable, but it's actually an element of the monotone L P space. So in other words, its monotone majorant is L P summable. Okay, so the monotone majorant is just the, at the ith index, I just take the supremum of the entries of the sequence with indices greater than equal to i. Okay, so just a little bit of technical setup to set up the compressed sensing problem. Set up the compressed sensing problem. So, what I do now, I define a big set lambda. This is essentially a hyperbolic cross-index set, but it only considers multi-indices that are non-zero in their first n entries. So it's an n-dimensional hyperbolic cross embedded in this infinite-dimensional space of multi-indices. Once we've done that, what we do is we write down our matrix. We write down our matrix A, which is just our Legendre polynomials with indices from lambda evaluated at the sample points, which are Monte Carlo samples. We write down our right-hand side, which is just our vector of sample values. And what we're going to do now is we're going to solve the weighted L1 minimization problem. And I don't want to dwell too much on the details of this. The weighted L1 norm is going to be a sum over. norm is going to be a sum over the entries of the vector. Sorry, there's a little bit of inconsistency in the notation here. And the weights are going to be the infinity norms of the Legendre polynomials. And now what we do is we solve a particular weighted L1 minimization problem. This is a weighted square root lasso minimization problem, and I'm happy to talk offline about why we solve this particular problem. About why we solve this particular problem and not something else. And then, if we do that, now we can prove the following. So, suppose we draw our Monte Carlo samples. Here's the lambda is the parameter in our square root lasso problem here. And n was the order of this set here. Okay, so notice everything here is explicit in terms of n. Everything here is explicit in terms of m and epsilon. Then suppose that your b is in the monotone LP space, then the following holds for every fixed f in this class of functions. Every minimizer of this problem leads to an approximation f hat for which the error goes down like m to m divided by log, this logarithm. divide it by log this logarithmic factor to the one-half minus one over p okay and again we have the noise term here as well so what we're saying here is we can get essentially the optimal rate up to this polylogarithmic factor which is like log m to the four essentially but we don't assume any knowledge of b so if you look at the construction on this page here I realize there's there's a incons some inconsistency with the notation Some inconsistency with the notation. We don't use the sequence B. Sorry, this should not be called B. Obviously, this is just a vector. So what we're saying is that, yeah, we can deal with this unknown anisotropy regime and we can still get near optimal rates of approximation. Let me, I'm running a couple of minutes late. I'll skip the two. Running a couple of minutes later, I'll skip the two other discussion points here. Let me show just a couple of examples. I just want to illustrate that this does indeed work pretty well and it works pretty nicely in comparison to adaptive least squares. So here the compressed sensing approximations are in red and in yellow. In the red, we use Monte Carlo samples. So it's not so good in low dimensions, but much better in higher dimensions. In the yellow, we use a different sampling strategy. Use a different sampling strategy. It's not theoretically optimal in any sense, but it does improve things in low dimensions, as one might expect. So here's just another function. Here we get a significant improvement over adaptively squared schemes. And here's a parametric PDE example as well. So again, the compressor and signal approximations do very nicely for this as well. Okay, so let me conclude. Conclude, I'm a couple of minutes over. So, what we've shown in this talk is that essentially Monte Carlo sampling for polynomial approximations is near optimal for this class of b epsilon holomorphic functions. To put it another way, any kind of improved sampling strategy is going to only have benefits in lower dimensional settings. One thing you might ask is why bother? Why bother doing all of this? Why bother doing all of this when we know now how to sample in a near-optimal way, regardless of the dimension? I think it's good to point out here that Monte Carlo samples that you can generate them offline. So they are completely independent of the space you want to approximate in. They also arise in legacy data. You don't need any kind of interface between your approximation scheme and the numerical methods computing your data. And in this unknown anisotropy setting, we just don't know how to develop a near-optimal sampling strategies. Let me say that there are a couple of caveats. So our theory doesn't explain anything about how high is high-dimensional before you start to see these effects. Although, in all the examples I showed, by the time D is 32, you struggle to see a difference. I've only looked at I've only looked at polynomial approximation on cubes in d-dimensions. I don't claim to say anything here about Hermite polynomial approximation on Rd or Laguerre polynomial approximation on zero infinity to the D. Here, Monte Carlo really does very, very badly in low dimensions. So I don't know what will happen in high dimensions. And of course, I should say that, yeah, I'm not claiming that Monte Carlo sampling works. Claiming that Monte Carlo sampling works well for every problem. If you have a low-dimensional problem or you have discontinuous functions or highly localized functions, clearly changing the sampling strategy is going to be beneficial. Okay, I will let me skip the details of the ongoing work because I've run over. And let me end by thanking you all. And if you're interested, we have a paper on the archive on this, which came out a couple of weeks ago. Uh, which came out a couple of weeks ago. Okay, thank you for your uh your attention. Let's thank Ben. Does anybody have any questions? Uh, yes, please. May I ask? It's Boris Kashin. Yes, please. Uh, you see, I'd like to say that in the 80s of last century in Russia, it was a couple of paper by Varonian with Kawhar, and they criticized. Author, and they criticize Monte Carlo, and they have a problem with people from PDE who use it for many purposes. And the critic was connected with the fact that Monte Carlo is probabilistic and the standard method gives definite results. So they introduce measure. So they introduce measure following Banach and Wiener measure on the class and then the answer, the opinion, the result was that Monte Carlo is worse than more standard method. Have you analyzed their paper or maybe never heard about it? I know that this is new to me, so I'd be very interested to read this. Maybe it makes sense to look. To look at what they did. They considered multi-dimensional classes and they introduced measure on these classes and then compare the same procedure. Okay, this sounds very interesting. I may email you to get the reference. Thank you. Any other questions? Okay, if not, let's thank Ben again. 