It's okay. My apologies. Go to it. As you know, I always miss my talk. But luckily there's no karaoke establishment in Ben, so I made it after it. I think I met Timo in 2004 for the first time. There was a party like this for a pitchemo, as you all know. As you all know. And Pablo was there, Peo was there, and I think Greg Lawler was also there. And at some stage, I had the duty to drive the guests to some restaurant. And I had this scrappy of a car, and Greg and Timo were driving with me. And Greg was like, Well, he sure is safe. And he was looking at the whole thing, he really didn't like it. And Timo said, Well, I think for a young assistant professor, this is a very respectable car. Fashion, this is a very respectable car, and then it just got so. I was very happy with this. I'm not sure if we would still do it now when then we did it. So, and also in this conference, we discussed some Hampshire ideas, and it was inspiring. I think that's why I stuck to it also. So, thanks a lot. Today, we're going to talk. I'm going to talk about some familiar things. I mean, so most of you know, it should be a laid-back talk and sit back. Back talk, and then you can sit back and relax, and there won't be any hard things going on. So, what is the periodic PNG model? Just very quickly, so you have particles on a circle, from 0 to L. You have the positive particles that move to the right, the speed 1, and negative particles that move to the left with speed 1. And then when the positive and negative meet, they annihilate, and then they're gone. And at And at intensity two in time, so on this cylinder, new pairs are born of positive and negative particles that move away from each other. And that's the model. And it will look something like this. So in this case, I start with the empty configuration, and you get, you know, particles are born, and they move, and they annihilate again, and And they annihilate again. And what is a little bit special about this example is that, as you can see, these parts are formed, these are rings on the cylinder. And that's because you have precisely as many positive as negative particles always. Because they're born together and they die together, so the difference is maintained. This picture will come back a couple of times. And then you can ask what is a stationary model. We know that the beam. What is a stationary model? We know that the PNG model on the line, stationarity is just two independent Poisson processes and they have to have some relation in the intensity. In fact, I think if you have intensity two on top, the product of the intensity should be one for the Poisson process on the line. So this is well known. And in fact, here it's basically the same. So, and that's because this stationarity is local, so it doesn't really feel the pre-load. The periodicity. So you can take some process on zero L with tends to be lambda for the x particles, the positive particles, and one of the lambda for the negative particles, and then this will be stationary for the PNG, the periodic PNG. But there is a slight issue with this model that somehow I don't think it's a fellow process. And also the subtlety with the generator going on. So I'm not sure if anybody recognizes this, but I would like to talk to them about it if that's possible. If that's possible. But you don't need it. You can do all kinds of calculations. I'm not sure if I will repeat this one. So, what I'm doing here is you just calculate the probability of finding a certain configuration at time h, and then you, you know, at time zero you have this Poisson process, and then you calculate everything up to order h. Things cancel and it works out. But that's not surprising. So let's skip this. And now there is a special property, of course, in this periodic case that you have these angolic components. Case that you have these angle components because we maintain the difference of the positive and negative particles. So that means that if you condition on this, if you fix the difference, then again you get a stationary measure. And so these conditioned processes are also stationary for this model. So that's kind of nice. And especially one that we'll be looking at is when they have the same number of positive and negative particles. And then, well, what you get is What you get is two Poisson Poisson with the same intensity, and you condition them to be equal, and then you get this kind of square Poisson distribution that will be relevant in the talk. So that's nice. You get this, well, like a lot larger value of relationary matches, but it's periodic case. And as Paul was mentioning, you can also. And as Pavel was mentioning, you can also look at dual points, also in the spirit case. So the the black points are the Poisson points and then the where they annihilate those are the green points and well you would expect and some kind of reversibility and something like a like a Burke theorem I think I would call it and this is also true of course and you you can do that again for the whole Poisson thing you don't need to first condition so if you So if you start with lambda times L, L of lambda as a Poisson process for the positive and active particles, and then look at the dual process, then it's again a Poisson process that takes the two, and it's independent of the top part. So you can kind of really start at the top and then go down with the dual process, just like you were saying in your model. So that's also very nice. And one way to do this, like, is you You look at time t minus h, you condition at time t what is the configuration, and then you ask, what's the probability of finding a dual point in the little strip that you're left? So let's say a times b times h, this, and you calculate it, and so this you got to this conditional probability, and and then it turns out, of course, that this in fact does not depend on the configuration at the top, and that's how you can see see that it's independent. And that's how you can see it's independent. And also, it gets, you know, you get the right intensity for a possible process with intensity two. This is all quite nice. Right? And then I guess I would want to mention this that so here you have this one-dimensional family, if you look at Poisson processes, but if you condition, so you conditional the difference to be either zero or whatever, then of course these lambda. Then, of course, these lambdas disappear. The conditioned probability do not depend on this lambda. That's a small calculation. It also makes sense because if you would start, let's say, with the empty configuration, there will only be one stationary dimension. So there's no one-dimensional family anymore. So it has to disappear. And that happens with the condition. Lambda goes out. Okay. That's nice. Um yeah. As I said, these are these you have these rings. As I said, these are these, you have these rings when you have the same number of particles. So here's the whole coloring. So this will be a path. And then this positive particle will be on a different path, but it will meet up with this negative one. So they're on the same path. And this negative one will meet up somehow with this one. It could be that there are some points here, but it doesn't matter. It will always match up with this one. But all these three are in the same path. And this is, again, a new path that's linked with this one. And then this one goes like this. And then finally, the last one goes like this. The last one goes like this. So, on this time, you will have three rings that are crossing the horizontal line. Three separate rings. It will be like a bit like this. You draw a horizontal line, you will cross several rings at that stage. And then, so what you have to realize in this model is that this is somewhat interesting direction. Interesting direction, because that's if you have the instant model, you would get like the KPZ stuff if you go up in this direction. Now, of course, here, if you fix L, you won't get any KPZ, it's just a finite thing, and there's nothing interesting going on. If you scale, if you scale L and time, then you should be able to find back these easy things. So that also means that if you, for example, look at this process here, the crossings with a vertical line, that should be a non-trivial process. It shouldn't be something like Poisson or something. It shouldn't be something like Poisson or something. That's that's not possible. Because you need to get back these special rings. So it is somehow interesting to see how many rings you would have. Let me say something about that. Now, before I go to that, I want to make a little observation because it turns out to be useful in our research. Maybe it's not so relevant now, but it was a way how we discovered something. It was a way how we discovered something, so I want to explain this. We have this square Poisson distribution, and so they had these squares of the factorials, and then it turns out you need these kind of Basel functions to see what is the normalizing constant, and you can write down the expectation. And it turns out that the expectation of the square is actually equal to L squared, and that's because if you have this factor, If you have this factor, you multiply by k squared, then you just get k minus factorial, so it's somehow easier. You have to thank Yuri for this, of course, for remarking this. So, thank you very much. And that is something that we should remember. It is a little combat, this fact. It's somehow it hints towards a certain direction. Okay, so what can we say about the expected number of rings through a vertical line? Well, it's And, well, it's not so hard. What you do is, so this is here is the vertical line, and then you take a small piece of it, and then you want to see whether you see a ring or not. And this can happen in two ways. Either there's a positive particle somewhere here, and it crosses it, or there's a negative particle here, and it crosses it like this. And they both count as separate rings, right? Because a ring can only cross a vertical line in one position. So these would be different rings, positive and negative ones. It wasn't an active ones. And then you can just write it up because you have an expected number of particles. So the probability of finding one here is just this expected number divided by the length of the circle. And then you multiply it by two, because it can be positive or negative. And then by the total length, and this will be the expected number of rings. So that's nice. You can explicitly calculate this thing. It's the expected number of rings that you. This thing, this expected number of rings that you see. And then there is an observation that now we want to look at the rings. So these rings have a certain structure. And for example, you can wonder how many minima does a ring have. That's somehow an interesting number. A ring can have one minimum, at least has one, but it can have several before it meets up again. And you can wonder what is the distribution of the number of minima of these rings. These rings. So then there's this observation that if you look at long time, then each of the minima corresponds to the Poisson point of this path. And so that means that if you take the expected number of minima and you multiply by the expected number of rings, you would just get the total number for some points. Therefore, you can immediately write out that the expected number of minima is this thing here. And here's an L squared comes out, and then I just note that it's the same. And then I just know that it's the same as the expectation of w squared. And this gave me some hint: what could be the distribution of this n compared to the w. And I'm not quite sure. I think maybe it's because of my statistics background, maybe you have some other reason why you should link this w and this n, but this expectation corresponds to the size bias distribution of w. Of W. The size bias means that you take the probability, you multiply it by the value of this thing, right? And then you get this expectation. So, in my youthful enthusiasm, I just kind of assumed that it should be true. So, you know, I write down the size bias distribution, and then I do a big simulation. So, I did like 100,000 rings, and I counted just the number of minima that you get, and you see it like this. It's sort of a bit interesting. And then I Started a bit interesting. And then I plugged in the theoretical values from this size-bias distribution. I'm not sure if you can see it, but these red things here appear and they're just spot on, basically. So I was very happy that this should be the right thing. But then you would like to prove it also, of course. Okay. So for that we need to do a little bit more work. And uh did I did I mention it was work with you or not? Did I forget? I forget. You're right here. So when I say we are in power, I am. Good lord. Okay, so what I'm going to do is I'm going to make the state space of these rings and try and describe the full distribution of this picture here. So not just the number of minima, but also how they're shaped. And And so the state space consists of, well, actually, x particles and y particles, just like the original particles problem. And the ring is encoded like this. So I start here at zero, and then I go out to the first y, then to the next x, the next y, the next x, and to the top, and then all the way up to there. And this point and this point, they meet up again. And then you get the ring. So that's the state space of rings. And then you can map them to the cylinder. Rings, and then you can map them to the cylinder where they actually live, where the Markov process is going on, by just taking this green point here and put it to zero, let's say also at time zero, and then you can kind of rotate this picture 45 degrees and you get this thing. So these rings, they will always have a minimum in this point here. Okay, and I call this map uh phi. Okay, so these rings they have a couple of features. They have Let's have a couple of features. They have a number of minima, let's say, in the cylinder, which is one more than the number of x and y points that we need. And, you know, so if you have more than two minima, then you need these x's and y's to encode where the minima are. And if you have only one minimum, that's just the path that goes up and to the right. So you don't need any x or y for that. And then there's this phi sigma that's on the cylinder now. So that's on the cylinder now. That ring will start with a minimum at 0, 0. And then the other minima, you can just write up a formula where they are. That's not so hard. And it's a little relevant that this phi adds a Jacobian. Because you need to be kind of precise in what follows. So you need to keep track of everything. So there's in the cylinder we will have this intensity 2 Poisson process that on this space E, This space E, the space of rings, you know, you need to make sure that this mapping corresponds. So that's why you need to keep track of the Jacobian as well. Okay. And now we're going to introduce a Markov chain on the space of rings. So you have a ring, and then there's a new ring, and a new one, a new one. Of course, it should be somehow linked to the process that's happening on the cylinder. So what do we do? So you have some ring, and then you map it to the cylinder. Map it to the cylinder. So that's this red part here, same ring as we had before. So now it's on the cylinder. And then I create the Poisson process above this ring of identity two independent. And I make the next ring. I just kind of run my process, and then there will be a birth here and a birth here, and it will my leg, and they will form a new ring. So this blue thing is a new ring. So this blue thing is a new ring on the cylinder now. And then I need to get get it back to the space E, the space of rings. I don't want to be on the cylinder. So what do I do? I pick one of the minima of this new ring, which could have more or less minima than the first one. I don't know, one, whatever. You pick one at random, you translate it to zero, zero. And once you've done that, you can just invert the map phi that we have. The map phi that we had. So this is like a random map a little bit because you need to randomly choose a minimum. And that's what I call this phi star of R1. So R1 is the one you get through the process. Then you map it back to the ring space, and you get a new ring. And this is a market check. The minimum decade equivalent. Yeah, yeah, yeah. So if you're three, you each have probably one for. Where you're three, you each have probability one-third. It doesn't depend on the position of the ring. So this was, you know, a bit of a guess. I didn't really know if it would work out. Well, I wouldn't be telling you if it didn't work out, of course. Okay, so now I need to prove well or find some stationary measure uh on this space of ring for this particular Markov check. Markov check. And somehow we kind of knew because of the simulation that this size bias square plus one thing should play a role in this. Okay, so I'll come back to that. So first we set the stage, let's say. So we have this space of rings. It consists of copies of... So you have a number of x's and y's. And so you have a number of x's and y's that you need, and they're the same number, and then you have just a little back dominating measure, and I just kind of make sure that, you know, each this this dominating measure has measure one for each of the levels of n. That's just to fix the whole thing. And then we need to find a transition kernel for this, you know, with the spectrum is new, and see if we can do that, if we can write it up. Okay, so what turns out to be important. What turns out to be important is if you have two rings on the cylinder, R0 and R1, then you can define the area between R0 and R1. And of course you can only do this if R1 is strictly above R0. Then you can see what is the area in between. And then it's convenient to say that if they're not above, then you just call this area infinity, which is a little bit counterintuitive. Instead of zero, you call it infinity. But the reason is that we will look at e to the minus We will look at e to the minus this area as some probability, and then if it's infinity, that's zero. So that's why it's useful to do it like that. If they don't match, you get infinity, and if they match, you just measure the area in between. Okay, so now we fix some R0, and then the next ring, the random ring, is called R1, and we're interested to see is what is the probability that this R1 gets somehow close to a ring Q? And that's the question. And that's that's the question uh that that would give you like the transition kernel, the whole thing. Okay, so let's make this a little bit more explicit. We have a sigma and a tau. We start at sigma, we want to end up at tau and write up the transition kernel. So there's a dz here, which is like a neighborhood of tau in this space, but this is in the ring space still. Then you define R0 to be phi of sigma, and R1 is the next random ring. And what needs to happen is that, you know, if you take the phi star of this R1, it should be tau. That's the idea, given that you started with sigma. Okay, so what needs to happen? Well, maybe I should make a picture. So you have this real. So you have this ring here, and then you need to see, you know, so this tor is the shape of a ring, but it doesn't tell you where it is on the cylinder. It doesn't matter, as long as it appears somewhere, then you map it back, and then you get this toll. So there's a certain shape given by this toll, and then you put it on the cylinder. So let's say it's somewhere here, and then you need to know what is the probability that. Then you need to know what is the probability that my next ring in my original process looks like this. So what needs to happen? Well, you need to find Poisson points here. That's important. And this part here needs to be empty. And when that's the case, that's it. Because once you fix the minima, the whole ring is fixed. So that's all you need. So you need two possible points here, and the rest should be empty. So, what do we do? We take phi of tor, sorry, tor is a shape, phi of tau is now a ring on a cylinder, but it's somewhere here, right? It's not above this one. Then I move it around with an action in a t-direction. So I move it around in all places. When I go down, of course, nothing happens because it's not above. So then this term here becomes zero. But as soon as you're Becomes zero, but as soon as you're above this thing, then you get a sort of probability of actually getting there. And that's given by this. The area should be empty. And then there's this. And that's just the fact that you have to have Poisson points at those places. So there's an intensity two that you get there. And there's this Jacobian, because here is in the space of rings, and here it's actually on the silver. And here it's actually on the cylinder. So the Jacobin also plays a role. Okay, so then I have explained almost every term except for this one over tau n here. Why is that there? Well, you have the shape, you put it in the ring, and you have it defined with the marker process, but then by mapping it back, you pick a middle at random. And you have to pick the right one in order to get the same shape. Otherwise, it would look different. Different. So the probability of picking the right one is one over twenty. That's what you get. Okay, so that's cool. We have a formula for this transition kernel. And then you have to somehow divide by the density of this mu, and you get this formula here. So that's the transition curl now on E, so there's no cylinder anymore to go from sigma to tall and it looks like this. Looks like this. Okay. And now we have to find a stationary diffusion for this, for this transition curve. What do you do? Well, we just use a like a detailed balance approach, which is the following idea. That if you can find a transition kernel, like a dual transition kernel. A transition kernel, like a dual transition kernel or a reverse transition kernel, K star. So it's a transition kernel, and it has the property that for a particular F, sorry, an F that we have to choose, for a particular F, but any sigma and tor, if you go from sigma to tor times F sigma, or if you go from tor to sigma according to K star times F tor, these two things should be the same. So this is like a detailed balance. Like a detailed balance condition. And if you have this, then you prove that the density f is in fact a stationary measure, not just for k, but also for k star. So this will be a way to check whether our guess is indeed the stationary measure. And it's very difficult to find the stationary measure from this, but if you have a guess, then you can do that. And we have a guess. Right? We. Right? We um we kind of know that the distribution of the total number of minima should be this size-biased uh distribution. And then, you know, when these points are, well, let's just pick them uniform. What else do you do? So the density that we guess is just the total probability of finding that many minima. And then the location is as uniform according to this new basis. And of course we also need to pick our k star and what we do is we do the time reversed version of k and then things should work out. That's basically the idea. So let's see what is the reversed process, what I mean by that. So now I actually have sigma n tau in the other. Sigma and tall in the other way around. So I mean by that. So what I mean is that if I have phi sigma here, then my tau, I will put it below, right? That's true, but I've also switched the two things here. So in fact, the picture stays the same, it says that we fix this one, this top one now, this is torque, and we move the sigma. Tor, and we move the sigma, we move it around. Yes, that's good. That's good. So we go from tor to sigma, but we go down in time. But that's what you have here. So we have five sigma, you move it down so that you go below tau. And then you wonder what is the probability that with the original Markov chain, you start with sigma and you end up with this tau here. Taw here. Now, of course, the key is that, you know, if you take fixed sigma here and you press, sorry, you fix tau, you go downward sigma, then this area is the same as fixing sigma and tau going up. It doesn't matter. So that means that here there's a little switch. You can put this on the other side in this area. And now these vectors are also the same as they had before, but that's the one of the sigma, because you need to pick the right. They have the one of the sigma because you need to pick the right minimum. And these are just come from because your density respected is new. Okay. So this is k star. And now all we have to do now is check detail balance for this. And if that's correct, well then we're in business. So Well, here's k from sigma to tau. Here's k star from torto sigma. What needs to happen if you do from sigma to tau, you divide by from torture sigma, you need to end up with f tau over f sigma. That will be your uh your detail balance. And what you can see in well here you can see it in k star, but for k uh the regular k that's the same thing. The regular K that's the same thing. It's that here there is this is the square plus one probability, and here you multiply with sigma n. So that's the size biased squared plus one probability. That's right here. So that's why you get the p star of sigma n here. And for the other case, it's the same, although I didn't write it like that. So maybe I'll just, you know, the other case is just that you put it. The other case is just that you put tor here everywhere, so you get the same thing. So you get p star of tor on the top. And there's a factor L minus two, but it will cancel. And this is exactly, you know, by definition, density at tor, and this is density at sigma, and therefore you get the toe balance. What we know in this picture, find it back again here. What we know is that if you take a look at the picture, That if you take one of these rings, we know the distribution of the minima, we know you just put them down in some kind of uniform fashion, and then that will be the distribution of these things. So, yeah, that's interesting, I think. What we didn't do, and I'm also not quite sure that that's possible, but I call this solvability as a model, right? Because you can exactly calculate all kinds of Yeah, you you can exactly calculate all kinds of uh things. And it might be that if you were able to uh for example, if you were able to say more about this process here on this finite case, then you can take some limits and you can actually uh prove something for the more interesting case, for the infinite case. But the fact that this is already has this kind of solvability is somehow a stronger statement than it just holds for the for the infinite case, right? Okay, I think that's a result. Okay, I think that's a little long I wanted to say. Thanks. Okay, Joe, any questions? I would ask that question already. Well, so that's relevant, and that's kind of true for the solvable model. And but I think, for example, the fact that we know the distribution of these things in this final case is somehow a little bit stronger than this stationary measure. Also, and the stationary measure was for when you look at one horizontal line, whereas these paths, they live in an entire similar space, let's say. So it's yeah, there's a little bit more complication to it, but we were surprised by the fact that we could actually you know write down what is the distribution of these things. Write down what is the distribution of these things. We started with this, we didn't expect that. I would expect so. But the system that we've done with the matching is kind of amazing. Much computations. You mean with the positive negative parts that you can match? Yeah, and there's also periodic cases, these topics. So that would be interesting to have a look at that. But I didn't mention if, for example, if you have one positive particle more, then this picture changes, and you would just have like one winding path that goes up like this. And if you have two more particles, then there are two paths. And so we didn't think about what is the distribution of those things, which are also, you know, because they're kind of interlaced. Kind of they're kind of interlaced, it's kind of more difficult to describe, I think. It would also be interesting to look at if you look at like a discrete version, so like geometric LTP or exponential, you know, you have these connections to these discrete or continuous taste setups. So then in the periodic setting, JiPeng Liu, Jinho Bike have done a number of things, probably related to your question about the number of lines that cross as you go up as a topic, you know, the fluctuation around block members. But there's also But there's also then stationary measures for the particle systems for like the periodic pace-up. And I wonder if, from that perspective, the calculation about the number of minima can somehow be thought of as the question of like, you know, you have a half-pole set of particles of n, you have n over two particles, and you ask how many transitions from ones to zeros you have. Yeah, so I'm not sure about that. So we tried to look at the discrete case, because that was actually the original question that we looked at, and we were not able to find any. And we were not able to find any stationaries. I think what maybe is the difference is that it kind of depends how you define periodicity. So the PNG model is like a rotated, I don't know, Hemersley type thing, and so you can make it periodic in that sense. But if you turn it 45 degrees and then you make it periodic, you get something else. So they're not immediately linked anymore, these two things. And I think that might be the reason why we could not find. That might be the reason why we could not find for discrete case any stationarity. But this, for like LPP, this would be equivalent to going in a diagonal strip. And that's the same type of periodicity when you kind of map the LPP to the periodic case of model that you have. But anyway. Well, so I will be interested to see if you can send me a link to these papers. Nice. Oh, one more question. There is a member of the Misantrov family. Member of the misantroke family with minus one, zero and one number of particles per site, which looks a bit familiar. So this is like a discrete version. Yeah, yeah, so it's just a bit more complicated than exclusion. We have minus one zero one for straight. Hunch! Hi! It's good to give a talk here. All right, let's thank Eric again. Drink coffee, and we meet at a quarter two.