Okay, I was muted apparently. Thank you. All right, so let's start. Oh, I hear myself. Yeah, it's your computer. Okay. Okay, so if you want to model the marginal modeling of extremes, the classical The classical framework essentially is to assume that you have a sequence of IID random variables y1, y2, y3, and so on. And what you do is to actually look at the maximum of a block size n. Okay, so mn here denotes the maximum between y1 and yn. Now you can actually figure out that if y is distributed according to f and they are iid, then the distribution of the maximum is f to the power n. Now in practice, you don't know f. Now, in practice, you don't know f, right? And so, if you try to estimate it, you might make some small mistakes. And then, when you raise that to the power n, it might end up with a big error. And so, instead, extreme value theory tries to seek for approximations when n is large, good approximations when n is large. And this is essentially the extreme value times theorem, which was proven by Fisher and Tiplet and then refined by Kittlet Denko. And then refined by Gitenko later. And it says that if you can find sequences of constants and bn such that the maximum we normalize with b n and a n in this way has a non-degenerate limit distribution g, then this g has to be a member of the GEV family generalized X and Value distribution. And so the GEV is this expression here, and you see it has three parameters, location, scale, and shape, and the shape is really. And the shape is really important, determines in particular the support of the distribution and the tail behavior. So, heavy tail, light-tailed, and short-tailed. One important property that this GEV distribution has is the so-called max stability property. And so it's essentially written here, and it means that if you take IID copies of Z here, and you look at the maximum of Z1 up to Zk, let's say, then this will actually. Of 2zk, let's say, then this will actually remain within the same location scale family. So, maxima from the limits remain within the same distribution. And this property will be important when we talk about the extension to spatial processes. Now, you can have a different view on extremes or a different way to define extremes through high threshold exceedancies. And so, now looking at the same why, the Looking at the same y, the same observation y, but looking at it when it exceeds some high threshold u, then you can actually show that the excess above u, y minus u, given that y is greater than u, can be well approximated by the GP generalized Pareto distribution when this u actually goes to the apparent point under similar conditions. This is usually called the Piquens-Dehan-Balkema theorem. And so, this GP distribution, which is the only possible limit for Which is the only possible limit for high threshold excellencies, has this form. It has two parameters: a scale which is different from before, and a shape, ψ, which is the same as before. Now, this GP distribution doesn't have the max stability property, but it also has a certain stability property, which is called threshold stability. So, if y minus u, given that y is greater than u, is a GP with some scale and shape parameters. Scale and shape parameters, then by conditioning above a higher quantile v or higher level v, so y minus v given that y is greater than v, remains within the same location scale family. So again, it will be GP, right? But with potentially different scale parameters. And again, this will be helpful to understand also what is going on later when we talk about spatial processes. Okay, so they seem very different from each other, the G V and the TV. From each other, the GV and the GP, but they can actually be related to each other through a point process representation that I won't have time to explain here. So you can see here some of the GB densities for different values of the parameters, especially the shape parameters and shape parameter. And here this is the same for the GP density. You can see you can get the exponential as a special case in black here when the shape parameter is equal to zero. Is equal to zero. You can get bounded densities here when the shape is negative, or you can get heavy-tailed perito-like densities when the shape is positive. So in terms of statistics, the common approach when you have IID data would be to, if you take the block maximum approach based on the GV distribution, would be to you have your time series here of observations, you split it up into blocks, and then you pick them. And then you pick the maximum in each block. Okay, so indicated here by the red point, the red dots. And then you just use the external time theorem and assume that this maxima, this observed maxima in red, follow the G V distribution with some location, scale, and shape. And then you can try to estimate these parameters by maximum likelihood or moments, methods, and so on. Right? So this would be the block maximum approach. When you want to take the threshold exhibit, You want to take the threshold exceedance approach. Instead, what you would be doing is to set a high threshold u, this blue line here, and then identify the points that exceed that level in red here. You see the sample of extremes is different from here. I mean, some of the points are the same, but not all of them. You may actually have more points potentially by the way you define your threshold. And then, once you have identified these extreme values, you model the excesses. Model the excesses y minus u using the generalized parameter distribution. And again, you can use maximum likelihood techniques, Bayesian methods, moments methods, and so on to estimate those parameters. Now, usually life is not as simple, so you usually don't have IID data, or the data might be spatially located, right? And so, what I want to do now is to showcase an application where we have. Application where we have space-time data and we're interested in estimating return levels, so high quantiles of the response variable over both space and time. I just want to show you how this actually works in practice, what kind of models you can build, and with the goal of estimating marginal distributions of extremes at every point in space and time. So, here the problem we're interested in is to model UK. Model UK peak river flows. And actually, so this comes from that paper down there. You probably don't see it very well. It's with collaborators. So Arnie Johanneson, Stefan Sigurd, Hochunbaca, and Berger Rafunkelsson. And this is published in the Analog Line Statistics. So what we want is to model or to build a model. To build a model to understand extreme floods, right? So, extreme floods cause casualties, widespread property damage, and of course, civil infrastructure damage. So, it's very important to understand the occurrence of these floods. And one key element for risk assessment or flood mitigation is to estimate these return levels. Now, the data that we Now, the data that we collected or that we downloaded were annual maximum of these river flows from 554 catchments across the UK. And the time span actually varied from 25 years to about 125 years. So, this was downloaded from this website here. And what we did here was to adopt a And what we did here was to adopt a Bayesian framework. So we built a Bayesian hierarchical model because it's a very natural framework for incorporating expert knowledge, accounting for all kinds of random effects, and also deriving predictive distributions to predict at unobserved locations. So, here in this work, really focused on estimating the marginal distribution of extremes, I really want to emphasize this, not trying to understand the spatial dependence. Trying to understand the spatial dependence, right, but rather estimating return levels at every point in space and time. And so, for simplicity, we will assume here conditional independence at the data level given the process level. Okay, so this is the map, and you can see all the points here correspond to a different location, and the color indicates the number of years. Indicates the number of years that the data were available. So you see from about 20 years of data at some locations, so not a lot to actually estimate those three parameters, location, scale, and shape, up to about 125 years of data here, close to London. And so what we're interested in is to build a model that is able to borrow strength across location for better estimating where we have a small sample size, but also Small sample size, but also use the model for predicting where we don't have data at all. So the data that we have are annual maximum flow, we denote it by Z I T, I for the location, T for time. And so we built, as I said, a Bayesian hierarchical model that comprises a data level, a latent level, and a hyperparameter level. Sometimes the latent level is called the process level. Latent level is called the process level, and the hyperparameter level is called the prior level. There are different names, but they mean the same thing. So, the data level here, because we have annual maxima, we just assume the GEV distribution. So, we assume that the data come from a GEV distribution and it has a location, scale, and shape, parameter. We assume the location to vary across space and time, so indexed by I and T, but the scale only changes with respect to location, and the shape may also. And the shape may also change with respect to location, but not time. We assume that the data are conditionally independent given the parameters, right? And that's a key assumption here. And this makes computations much, much easier. Now, the location here is modeled in this way. Okay, so we have a kind of background location parameter mui. Of background location parameter μi that varies across space, and we have another parameter delta i that actually indicates the trend at each location. So, in fact, we have two parameters inside the location parameter. We have this mu i and the trend, the time trend, delta i. Now, what we did was to transform these parameters, which have certain constraints, onto a different scale so that we can assume. Onto a different scale so that we can assume a Gaussian process for each of those parameters. So we used a log link for the location because we thought, okay, ensuring positivity would be like a natural assumption here. We modeled the scale and the shape jointly to account for the correlation that we see in these estimates of mu i and sigma i in this way. So we transform them using the log of sigma i divided by mu i. We have a We have a very particular transformation for the shape ψ. It's written here. That's this formula here. The reason why we chose this formula is that it ensures that the shape will remain within the interval minus 0.5 to 0.5, which will avoid issues related to having infinite variances, for example, when the shape exceeds one half, but also issues relating to the fact that the lightlift is not regular. The fact that the likelihood is not regular when the shape is less than minus one half, one half end. And so the other reason why we chose that formula here is that, in fact, this transform parameter has the same interpretation as psi itself as a shape parameter when it is close to zero. So you can just look at phi, this transform parameter, and think about psi, which is quite interesting, I think. Now for the trend problem. Now, for the trend parameter here, we also used a particular formula here. And so this puts the parameter between the interval minus 0.008 to plus 0.008. And you might think, okay, this is a very strange way to restrict the parameter space. It looks like it's strange, but actually, in our point of view, it actually avoids estimating. Avoid estimating time trends that would not be reasonable. So, actually, what we do here is we did it in this way, and it allows actually to have, by putting a Gaussian prior on this, it allows to have 95% of prior chance to have a plus or minus 6% change per decade, which we think is quite a lot. Okay, anyway, so this is our link function. Now we have. Okay, anyway, so this is our link function. Now we have new parameters and we will assume that they are multivariate Gaussian. So we assume that they are jointly Gaussian. They are driven by some fixed covariate effects and also some structured spatial effects driven by an SPD and also unstructured spatial effects. So you can see here how these parameters are modeled. So we combine them across all locations into certain Across all locations into certain vectors, psi, tau, phi, and gamma. And here you can see that we actually model them using some fixed effects. So beta are the regression parameters. Covariates are included in these matrices, these design matrices here. Then we have some spatially structured effect here, u, psi, u tau, u phi, and u gamma that will be essentially assumed to come from a spatial Gaussian matter. Gaussian matter random field, which we approximate using the SPD approach by Finn Lingran and coordinators. A is a suitable projection matrix from the nodes or the knots of the triangulation onto the data location. And these epsilons here are unstructured IID spatial effects. All right, so this is the latent structure. And in terms of And in terms of coverage, we have 14 coverates which are related to floods. So, one of them is the catchment drainage area. Another example is the average annual rainfall summer. Another one is the flood attenuation by reservoirs and lakes index. Another one is the flood plane extent, and so on. So, we have 14 covers that we think might actually be related to the response variable. Okay, so the space. Okay, so the spatial runoff effects that I said were assumed to be Gaussian and driven by an SPD. So this leads to Gaussian Markov runoff fields that have sparse precision matrices and makes computations fast. So you can see here the triangulation that we used covering the UK, the UK, and then essentially the process, these processes you are actually fine at these triangulation nodes, but then you can project them back to the data location. You can project them back to the data location highlighted here in blue using this projection matrix A. In terms of priors, so we assigned Gaussian priors to all the regression coefficients beta. We used what is called as panelized complexity priors for the other hyperparameters, so the range and the standard deviations or the precision of the structured and unstructured quantum effects. Bundom effects, and if you want to know more, I can give you more details, but or you can check the paper. But essentially, we have a nice way to set priors, and then we make inference using an approximate Bayesian technique. So, here the problem is that we have quite a large data set. We have more than 500 data locations, so it's quite a lot. So, we have to deal with side dimensionality of the data dimension. Dimensionality of the data themselves, but also high-dimensionality of the latent parameter size. Okay, so we have these four different vectors here, okay, and all of them are of dimension the number, so wait, yes, so you have a vector each of each of which here is of dimension 554, right? So we have quite a lot of So, we have quite a lot of latent parameters that we need to infer. And so, we cannot just use MCMC techniques. This would just be too long and we would have to check convergence of these parameters. And this would be really a pain in the next. So, we considered using Inlab, but Inlab was not possible in this case because it does not allow you to have a multivariate link function, if I'm correct. We should have a channel link. We should have. We should have okay. Maybe okay, okay, good, good. I'm excited to hear that. So maybe it will be possible in the near future, but at least it's not possible. It was not possible when we wrote this paper. And so our solution here was to design a new technique, which is actually in some sense quite similar to INLA. And we called it Max and Smooth. It's a fancy name for something very simple. So what Something very simple. So, what we did was to essentially at every single site, we estimated the G V parameters, the four G V parameters, including the time trend, sideways, so separately at each site, okay, by maximum lightlink. But we also estimated the feature information, so we have a measure of the uncertainty of those parameters at that site, and then in the second step, we smoothed the. We smoothed these parameters by approximating the likelihood with the Gaussian likelihood with the correct curvature coming from the fissure information that was estimated before. And so by taking this approach of approximating the likelihood by a Gaussian likelihood, we can then actually smooth those parameters using a Gaussian-Gaussian hierarchical model, which is very easy and very fast to fit. To fit because we can take advantage of the conjugacy of the model. Okay, so max and smooth. And it's actually an approximate, fully Bayesian technique. And it's very fast and accurate. And it also benefits from having a lot of replicates. So one of the great things that I find about this approach is that, okay, the match step is very fast, right? You just fit, you just estimate the MLEs at each time separately. At each time separately, there's nothing very fancy, and you can do it very fast. But in the second step, you only deal with these parameters, so you don't have so they are only observed once, right? I mean, you don't have to deal with the number of time replicates anymore, right? So, I think in INLA, another issue is when you have high-dimensionality both in space and time, right? So, having high-dimensionality as a whole, right? Dimensionality as a whole, right? If I'm correct, maybe not now anymore, but yes. Well, at least here, you deal with the number of time replicates in the first step only. In the second step, you don't have any time replicates anymore. So it's essentially insensitive. The time that it takes to do this imprint is insensitive almost to the number of time replicates that you have, which is great. All right, so back to the application. So we have 14 coverates. We first did some kind of model selection, including them in the model in a forward manner. And you can see here for all these parameters, psi, tau, phi, and gamma, we looked at a measure of the relative test error by including those parameters one by one. And we saw, we did. And we did that using a spatial effect included or not. So, without it, this is this gray line here, with it, this is this black line. And the final model that we selected actually comprised these covered here in these different model parameters. You can see, in fact, that here the spatial component was in the end included only in psi and tau. We see a small gap here between the test error with and without the spatial effect. Test error with and without the spatial effect, but actually, if you look at the scale of the relative test error, this is very negligible. And so, that's the reason why we actually didn't include the spatial component in phi, which is the shape, the transformed shape parameter, and gamma, which is the transformed trend parameter. Yes, transform. Is this somehow related to like the purple base? Because somehow you're using data to construct AI on the image. Side field, and then you're smoothing the keys. I'm not sure I would see it as empirical based, but it's true that we have this step where we select coverates first. So, and actually, this selection was done for each parameter independently using INLA because we can model each parameter separately using INLA. But I'm not sure you would call it empirical base, but anyway, it's true that we use the data to actually select some appropriate coverage, but then we use all the data again to actually pick them all with those selected coverage. So anyway, so you can see some of the results here. So these are the estimated spatial effects for the transformed location and transformed scale parameter here. Nicely smooth. Nicely smooth. And here you can see a comparison between estimates you would get by using the MLE at each site and the model, the Bayesian model that we fitted. So here you can see the location, the scale, the shape, and the trend parameters. And at the top, here you can see the same thing for the transform parameters psi, tau, phi, and gamma. And the plots show you the comparison between the estimate of the Comparison between the estimate at each site, ML estimates, versus the model estimates. And you can see there is not so much change for the location parameters and the scale parameters, which we actually expected because we have actually quite a lot of information at each side to estimate those two parameters, location and scale. But there is quite a big change in the shape parameter. And this is nice because if you look at the uncertainty of these ML estimates, it's quite big. It varies from minus 0.5. Quite big. It varies from minus 0.3 to plus 0.3. And we don't believe this range is really reasonable. And so the model that we fitted was much more, was able to somehow pull information across sites to reduce that uncertainty. And we see a nice shrinkage here. So we see the range of the estimates only goes from minus 0.1 to plus 0.1 in the end. In the end. And this is the same story goes for the trend parameter. And so, here, this is the kind of results we would like to produce in the end. So, you can see here return level maps, return level plots for three different stations. And you can see the data in black, and you can see the model in solid black here with the uncertainties. And at the bottom here, these would be CDFs at those sites. So you can use the model to estimate these return-level curves and the CDS. And you can also, I haven't shown it here, but you can also use it to predict at unobserved locations. So just summarizing this part, so latent Gaussian models provide a very rich and flexible framework for modeling the marginal distribution of extremes. Here illustrated with maxima, but you can also use it with. With maxima, but you can also use it with peaks over thresholds over space and time and capturing the different kinds of trends that you see in the data. Out-of-sample predictions is easily accessible using a Bayesian approach through the posterior predictive distribution. And you can make fast inference in high dimensions using some approximate Bayes technique combined with the SPD approach for modeling the latent components in the model because they lead to sparse precision. Because they lead to sparse precision matrices. So here we use MaxN smooth for the approximate base technique, but you can potentially use something else. The main drawback of this modeling approach is that we assumed conditional independence at the data level. And this means that we cannot really properly capture the potentially strong and complex extremal dependence structure that actually governs the data, right? So if Right. So, if estimating this dependence is key or is important for the problem at hand, then you should actually be using these models with conditional independence at the data level. So, in summary, they are great for estimating marginal distributions, but they are not so great for estimating the spatial dependence. Yes, Leo? Okay, I'm just wondering, so you're computing your thoughts. The train gives you seed of big size. Is it because you have a phenolized complexity trier on it that takes it towards zero? You mean this plot here? Yeah. You have a phenolized complexity trial over the shape. So, here, if you remember the model for the shape parameter, this is this phi, right? This is the transform shape parameter. Actually, we don't have a spatial component. Actually, we don't have a spatial component and we have only one covered. So essentially, it's assumed, I mean, without it, this would just be a constant shape over space and time. So the only reason why the shape varies is through this covering. So there is a lot of pulling going on here. It doesn't look like it varies much from zero. So kind of other, and there's grassly different interpretations for a positive to a negative. So my bottom, when I just call it zero, So my problem, when I just call it zero, are you could, but it would, you could, but it would end up with a worse model in terms of performance. Like, I mean, as you can see here, for example, this is phi here. If you include one coverage, you drop, I mean, the test error drops from here to here. Okay, so there's you could argue that this is not a big drop because of the scale here. The scale here, yeah. So, potentially you could fix it to zero, but I just wonder if there's some physical intuition for positive and negative j parameters there on those straight lines. I don't know. I don't know if there is any physical meaning in this, in having a change from positive to negative in this particular case. Yeah, it's a good question. Yes, Francis, you know. Okay, for the GV distribution to be valid, for the GV distribution assumption to be valid? Yes. Well, at least, I mean, looking at those graphs, I mean, in black, you see the data in I mean, in this kind of piecewise constant line, this is the empirical celiac, and in solid line, this is the fitting model. It doesn't seem to contradict at least this GV assumption. But just because it fits for a given block doesn't mean that maximum distribution. On a small watershed, you might see a relatively weak temperature. Relatively weak temporal dependence, and therefore, a lot of equivalently independent observations over here. In a big watershed, especially if the stream gauge is downstream towards the mouth of the stream, you're integrating information over a very large area. And therefore, those stream flows from one day to the next are quite temporary. Right. So the effective block size is small. The effective block size would be quite small. Because right, um, yeah, I mean, you're right. I don't have a perfect answer to this. Uh, yes, you wanted to add something, yeah. So a detail first. So you have the Fisher information, so you actually have the covariance among the parameters. Did you use that or did you just use the diagonals? No, we used to. No, we used that information. Okay. Okay. So these are sort of multivariate normal fields that you're fitting. Right. Okay. Okay. So I've always wondered in a data set like this, these hydrographs are probably daily, right? So why not use a generalized Pareto model instead of GB? Yeah, that again, I think you could again, but you have to, then you would have to deal with temporal dependence and seasonality and things like that. So probably taking maxima was a kind of easy way to avoid these other issues. Okay, thanks for the questions. So let's move on. We're still in part. We're still in part 1.2. Okay. Okay, so I probably will have to skip some parts. So now I want to briefly mention Maxwell processes, which have been historically very important in spatial extreme value theory. So now, and this is really interesting when you actually care about the extremal dependence structure. So suppose that you now have a That you now have a random process that you want to model. And for the same reasons as I explained before, in fact, the max stability can be transferred to this spatial process. So in fact, it turns out that max stable processes are the only possible limits that can arise, non-degenerate limits that can arise for maxima of IID random fields. And they have the max stability property, which now reads in this way. Reads in this way. And it turns out that Maxable processes are compliant with universe-mary theory. So actually, the marginal distributions are GEV. And they can be represented in a nice way in this way. I mean, so this is the representation of Maxwell processes with unit fresh margins due to De Han in 1984. And essentially, you have two ingredients, Ri and Wi. So Ri are points of a Poisson process on the positive gray line with intensity. Positive reline with intensity r minus to the r. So this is heavy-tailed, and wi are IAD copies of a random process which is non-negative and have mean one. So you can think of wi' as capturing the profiles of storms or heat waves or whatever you're dealing with, and Ri as being the magnitudes of those events. So you multiply the profile with the magnitude, and then you take the point-wise supremum over an infinite An infinite copies of them. So, from that representation, you can actually do quite a lot of things, right? So, Ri are points that are given to you, but Wi are just random processes that have just a few assumptions, right? So, you can then start to model W and this will give you different maxable models. So, you can have here, this is a list of Maxable models that you can use: the Smith model, the Schader model, Brown resting. Model, the shadow model, brown restnick extremity, and so on. From that representation, you can also derive what are the joint distributions when you observe sorry when you observe the spatial process at D sites. Okay, so it turns out that it has this form exponential minus V, V is known as the exponent function, and it has this particular form dependent on this W process here. And so the game. And so, the game here is to try to find W processes that comply with the conditions I mentioned before, that will give you a flexible model, but at the same time, where you can actually compute this expectation in close form so that you can actually make inference. Okay, and these are examples of those models. You can also write down what the likelihood would look like. So, by computing the derivative of this function here with respect to z1, z2, to zd, it just turns out to be this ugly. It just turns out to be this ugly expression here. And you see, this sum here in the middle is a sum index over all possible partitions of the set one up to D. So this is the number of those partitions is equal to the Bell number, which is growing very fast to infinity as dimensionality grows. And so this is difficult to actually make likelihood inference for maximal processes for this reason. Okay, let me just illustrate here the construction of maximal processes based on the Processes based on the Smith model in R. So, this process here assumes that these W processes are Gaussian densities with a random center that comes from a Poisson process in R and with a fixed variance. And so the first of these storms might look like this, right? You have a first Gaussian density hit by R I D, the magnitude. And so this might be the first one. And then the second one might look like this, right? Like this, right, with maybe a smaller amplitude, and you keep simulating these kinds of storms. You can do that in decreasing order of the magnitude, and then at the end of the day, you take the pointwise supremum at each site, and you end up with this solid line here, which is a realization from the Smith-Maxwell process. So, this is just an example in R, but you can also do it. In you can also do it in higher dimensions. So, this is in 2D. You can see a realization from the Smith process, the Brown-Resnick process, and the extremal T process. And so, the Smith model is usually thought to be too smooth to be realistic in applications, but Brown-Resnick and extremal T processes are much more popular with that perspective. So, I mentioned the problem with full likelihood. You can try to derive the density in dimension 2, 3, 4, and so on, and very soon you will. And very soon you will be stuck, right? So, in dimension 10, even you have 100,000 terms in the likelihood. In dimension 100, so 100 would be like the number of sites that you observe your maximum process at. You would have about 10 to the power of 115 terms in the likelihood, so it's just impossible. So, instead, what has been the classical approach is to use composite likelihood and especially pairwise likelihood. And I don't think And I don't think I have a lot of time to talk about this, so let me skip that part. And I assume most of you actually know more or less what the composite likelihood is. But essentially, what it does is you, let's say, if you work with pairwise likelihood, you look at the likelihood contribution for every pair of sites, right? And then you just combine them together in, you multiply these likelihood contributions together. You multiply these likelihood contributions together, and so because this is easy to compute, then the composite likelihood is easy to compute. You can do that in dimension two, or three, or four, whatever your computational flexibilities allow. Okay, but you can also look at the vector approximation. This is nicely linked to what Doug talked about. So, this has been explored in the Gaussian case, but it has also been explored in the Maxible setting in this recent paper that. Recent paper that we have written here that is in revision now. And so let me also skip the details because it was already talked about before. What I think is really nice with the Vecchia approximation is that contrary to composite likelihoods, which are not valid likelihood in the sense that they are not likelihood functions of a valid process, in fact, the Decca approximation is truly a likelihood approximation. It is, it provides a likelihood. It is, it provides a likelihood function that is the likelihood of a valid process. And so, in that sense, it is more natural to use. And I think personally, that's one of the reasons why it has been found to work so well in practice. Is it max stable or is it just approximate? Max stable. Ah, you mean the approximation? Well, in the Gaussian case, the approximation is Gaussian. Approximation is Gaussian. In the maxable case, I don't think it is maxable. But anyway, here you can see the idea here. So you see the Veccha approximation when you use only two, well, actually one neighbor in the conditioning set. So this leads to likelihood contributions of dimension two at the maximum. And this is based on the coroninita wise ordering. And then here this is. And then here this is three, so two neighbors in the conditioning set: 5, 13, and 124, which is the total dimensionality of the grid here. And you can see that already from 5, the approximation is pretty good, right? I mean, if you compare this plot here to this plot here, right? So this is the actual correct true process. And the process you get with only five values of four in the conditioning set. So, four in the conditioning set provides you already with a very good approximation. Okay, so we applied that technique, so composite likelihoods and the Vecca approximation in an application to study Red Sea surface temperature maxima. So, let me skip again the details. Here, essentially, we have sea surface temperature measurements across the Red Sea at a very high spatial resolution: 0.05 degrees times 0.05. Degrees times 0.05 degrees, which leads to 16,000 crit cells. That's really a lot in terms of spatial dimensionality. So, what we do to somehow make our life somewhat easier is to just keep every fourth longitude and latitude points, which bring it down to a thousand, more than a thousand locations. It's still extremely big in the max table setting. This dimensionality here is really big. But with the Becky approximation, we can still make inference and get reasonably. Inference and get reasonably efficient estimates. So, here what we did was to fit the Brown Rustic process both in the isotropic case and anisotropic case. So the Brownerustic process is defined by taking these W processes in the construction to be log Gaussian processes, so exponential of a Gaussian process. And this Gaussian process is determined by a Verogram which has this particular form here. Particular form here. And so we have a range parameter, spatial range parameter phi, a smoothness parameter ν, and we have some anisotropy parameters. So the stretch and the rotation parameter embedded in this matrix omega here. Okay. So I don't want to talk about these numbers too much, but just to say that we actually used the Becca approximation and different types of composite likelihoods. Types of composite likelihoods in the anisotropic case and isotropic case. We try different orderings for the Becquia approximation. We try different numbers of variables in the conditioning set. So from so here, two, three, four, five corresponding to one, two, three, and four values in the conditioning set. And also composite likelihoods of order two and three with a cutoff distance that also varies according to this value of m. So essentially what we find. Value of m. So essentially, what we find is: so, here this is the results of a cross-validation study, and it shows you the log scores that come out of this cross-validation study. And you don't really make sense of these numbers, but just looking at the table here, you can see that the part that corresponds to Vecchia gives log scores that are always smaller than those that you obtain with the composite language. So, this means that the Vecchia approximation seems to be more efficient. Approximation seems to be more efficient, providing you more efficient results. Now, if you look at the computational time, so this is what you get. So, it can be quite intensive. These are numbers in seconds, right? But again, if you compare these values, for example, if you take the Vecchia with D equal three and compare that to composite with D equal three, so in principle, they should be more or less comparable because the likelihood terms involve. Because the likelihood terms involved would be of the same dimensionality, you can see that here the time that it takes is often much less than the time it takes for composite lightly. So essentially, the Vecchia approximation has these two benefits of providing you with usually quite efficient estimators and also having faster inference. Are the orderings just different draws or are they different schemes? Different schemes. Different schemes. Okay. Right. So P1 is coordinate-wise, then we have random, then we have middle-out, and then we have this maximum ordering defined in the paper by Kinas in 2008. All right, so some results. So these are actually the contours of the X-ray male coefficient, which is a measure of dependence estimated based on the best fitted Maxwell model with the Becquia approximation. Maximum model with the Becchia approximation here. You can see that it's indeed anisotropic and that the contours kind of align very nicely with the geometry of the red sea, which is nice. And here you don't see much, but essentially these are fits from the model, both for the best Vecchia method and the best composite method, so top and bottom, and in the anisotropic case, left and isotropic case, right. And you can Isotropic case, right? And you can not see probably that the Vecchia method provides you with a much better fit in this case. And this is the same thing with respect to different dimensions. So let me skip that for the sake of time. So essentially, classical extreme value theory relies on these Maxable processes, but they have a number of drawbacks. Some Maxable models actually lack flexibility for capturing long-range dependence. For capturing long-range dependence. That's one issue. So, dependence at long distances. All maxible models are max-stable, so they cannot capture weakening extremal dependence, as we will actually talk about later. Likelihood-based inference is very difficult. The Vecchia approximation helps to somehow make it easier, but it's still not easy. And also, maxable processes are defined for block maxima. Processes are defined for block maximas, which are usually not real observations. So they have been also criticized for this. And also, Yanis has written a nice paper with Kirsten that shows that actually conditional independence in realistic, I would say, maxable processes implies already full independence. And this makes me think that if it is impossible to construct non-trivial Non-trivial conditional independent maximum models, then I don't know if this class of models is really a class that can provide really real signals in practice, because conditional independencies are very natural in practical situations. So, overall, my perspective is that I think the extreme value statistics community tends now to move a bit away from maxable processes for the above reason. Maxable processes for the above reasons and others, and instead consider peaks of the threshold approaches which lead to more flexible and realistic models and also easier inference. So I don't know if this is shared by Jenny or Anthony or others, but at least this is my view on the field. So I also want to briefly mention these RPARITO processes, which are essentially equivalent of Maxable processes for threshold exceedances. There was a question at the back. I had a question about that in the last slide. Yes. In the climate change situation, we're now pretty used to using non-stationary covariance in blockbanks. How do we do this in a few century threshold? You can imagine that if you set your threshold of reduced percentile, all the temperatures that exceed that will be the latter part of the. The latter part of the record. So, how do you construct a non-stationary freshman? You're talking about non-stationary in time, right? Non-stationary in time. It's hotter now than it was in the back of the nation. And you're talking about non-stationarity in the marginal distributions or also in the dependent structure? So, okay, if your interest is to model non-stationarity in the marginal distributions, then you can actually use the techniques I presented before using these latent Gaussian models. So the data level would be perhaps the GPD distribution or point process likelihood, where you have marginal parameters in which you can incorporate the trend. And so the only difference with respect to what I presented would be. With respect to what I presented, would be that now the data level would be different. The data level would be driven by a GPD or point process rather than a GEV like it. So that would be the only change and you can use exactly the same techniques. Now, if your interest lies also in estimating the dependent structure, then I mean, there are ways to make the dependent structure of these paradigm processes that we will talk about non-stationary. I think probably. Non-stationary. I think probably Jonathan will talk about it later this week. So, how to build and fit parallel processes that are non-stationary in space or perhaps in time as well in your case, right? Yes, conditional covariance. Conditional on covariance. So it's actually conditional blocking patterns. That understands how blocking patterns will affect certain things. How covariant is clear? And I think I think this is a challenge. I mean, for sure, there are still challenges that we face, right, when you use peaks of a threshold data. It doesn't solve all the issues. But I feel like But I feel like it's a more natural framework because you exactly deal with the actually original events that took place. Block maxima, okay, it means something on the, let's say, station level, right? You know what it means for annual maxima at a particular site. But when you try to understand dependence, so like a point-wise supremum of different. Point-wise supremum of different runoff fields that may actually not correspond to real observation. You can try to model that using maxible processes, and actually, the dependence structure you will be estimating tells you still something about the tail behavior, but you fit a maximal process to somehow artificially created data, which makes it a bit less natural, in my point of view. Yes, sir. Yes, Sarah. Just a slightly different cake in the context of where max state processes are. In terms of, I suppose what I'm trying to say is this is very rough. The only people who understand the maximal processes are statisticians. And so trying to get it into another field is a major challenge. Also, a reason why it may well be a It will be a difficult part. Right. So maybe don't make the effort. Right. I mean, skip the maxible part and go straight to parato processing, all these kinds of models. Maybe Anthony had something to add as well. I mean, Micronesia operated processes are harder to understand. So it doesn't happen. But I wanted to come out on condition independence. Essentially, your new likelihood approximation, vecca approximation, is assuming conditional balance within the likelihood of approximation. Right, so essentially the vehicle is is a composite likelihood in itself. Well, yes, but but I mean you're somehow condition you're saying if I condition of these things, I don't have to worry about control. Right. So, how does that relate to the work that Yanis and Christine do? Because they're saying we assume conditions and applies. Right, so that's the thing. So with the Vecca approximation, you get a valid likelihood approximation. So the likelihood is an approximation of a valid process, but that process, again, comes back to the previous question. Back to the previous question, will not be actually max stable. Yeah, yeah, because it's a reason, actually. Yeah, yeah, and less to explain why you get a better fit. It's always been an issue with using was it like the extreme of the coefficients go up too slowly. And yours, if we go back to your previous slide, or the slide tour, this one. Yeah, yes. Yeah, yes, that's right. And the red lines in the lower lines are going down, going up too slowly. Right. Yeah. And once in the top, going back. Yes. Presumably because you're somehow imposing this independence at longer ranges. So, what happened here in this particular method? So, this is the best composite likelihood method in the Method in the isotropic case. And it turned out that for that particular case, the best composite likelihood turned out to be when we fixed the shape parameter to be in the in the Verogram to be 0.5 rather than, I mean, we had three values, 0.5, 1, and 1.5. And the best case here was 0.5. And so that's the reason why it gives this kind of strange shape. Strange shape. In the other cases, the best value was one and provides a much better fit. But even so, I suppose what I'm contrasting is that the two pictures are barred, the left. This one and this one. Yeah, the red line is clear. In both cases, the red lines are clearly higher in the top and just the red lines. And that presumably is related to back your approximation in the pens that you're building. Right, so So but we feed the same model, right? I mean the top one is not accidental. No, no, we still feed the anisotropic brown rising process. So the model we want to fit is the same. It's just the composite likelihood that so the likelihood contributions that we use in the composite likelihood that would be different in both cases. In both cases. For the Eausian case, the Vecchia approximation has the same parameters, has the same parameters as the other one. So you get a consistent estimate. I don't know if it works in the extremal case. So, sorry, your question is. I'm just saying if it carries over in the extremal case, then your estimates of the extremal coefficients based on the Becchia will not be biased compared to actually estimating the complete model. So, your question is whether the Vecchia estimator is consistent? Yes, I think it is. I think it is. Actually, the vehicular likelihood is a particular case of composite likelihood, so the same asymptotics for you can check the time. Okay, it's gonna be short. Okay, so our paradox processes, so what they are, they are essentially the equivalent of maximal processes, but based on threshold exceedances. But based on threshold exceedancies. Now, fortunately or unfortunately, there is no unique way to extend the conditioning event y greater than u to the case of a spatial process. And so initial work has focused on defining exceedancies in the sense that the supremum of the process over the spatial domain is large. Okay, but this is the contribution by Ferrera and Dehan in 2014, for example. 2014, for example. But that's not the only possibility to define a large quote-unquote process, right? So, more recent work has actually defined what they mean by a process to be large in terms of a risk or cost functional. So, the supremum could be one such functional, but you can also look at the But you can also look at the integral or the infimum of the process being large, or the maximum sum or minimum at a finite number of sites as well, or perhaps the process at a single site being large. And this provides you with a richer framework to condition on events of interest. And so, the only possible limits for threshold exceedances in this sense. In this sense, so given that your R-functional exceeds a large threshold on a standardized scale, then the renormalized process will actually converge to a limit process that will be called an R-Pareto process. So R-Pareto processes are the only possible non-degenerate limits for R threshold exceedances. They have a representation like this: R times tab. Like this, R times W, where R is a Pareto variable with unit parameter, and W is an independent non-negative process that satisfies R of W is equal to one, almost sure. These limits, R parallel processes are again threshold stable. So given that this R function of this R of X exceeds a certain Of x exceeds a certain level v, then x divided by v has the same distribution as x itself. Okay, so it's the same kind of notion that we saw before. And they have this asymptotic characterization that I mentioned. You can actually write down the likelihoods fairly or more easily compared to maximal processes. So the likelihood would actually look like this. Now, the other thing we have to deal with when we fit the parity. With when we fit Pareto processes sensoring. Because, for example, if your risk functional is the maximum, it means that the maximum is large. So at least one site will experience an extreme event. But maybe some other sites will still have small values, right? Okay, so you need to account for that when you make inference to make sure that the small values will not impact or influence the. The estimation of the extra dependence structure. So, what you need to do is to censor these small values so that they don't impact the fit. So, the censored likelihood looks like this. This v function is the same exponent function that I talked about before. And this means that we are taking actually partial derivatives of this function v. K here is a factor that depends on the risk function itself. On the risk function itself, so whether it's the maximum or the sum, and so on. So you can see it's simpler, although censoring still leads to fairly demanding inference because centering amounts to essentially integration. And so these models can be fitted in higher dimensions than maxible processes, but still it is still not very easy in very high dimensions. Okay, I will just skip this slide and move. This line and move to the second part. Okay, so now we have talked about these asymptotic models. So in the case of marginal distributions, GV, GPD, in case of spatial processes, max table processes, and parallel processes. Now, I would like to talk about more recent approaches that solve some of the issues that I mentioned about. That I mentioned about maximum processes and so on. But first, I need to talk about the asymptotic dependence classes. So, when we try to characterize the strength of external dependence in the process Y, a way to do that is through this measure chi, also called the tail correlation coefficient. So, what is it is it's simply a limiting conditional probability of this form. So, we look at the process at one side as one being large. One being large in the sense that it exceeds a high marginal threshold, given that it is also large similarly at another side. And then this U is a uniform quantile, and we just let it go to one. So this chi here is a measure of extremal dependence. Now, if chi is positive, we say that we have asymptotic dependence, and with ski, when chi is And when chi is equal to zero, we say that we have asymptotic independence. For these asymptotic Exxon value models that I talked about, essentially this chi is always positive. You can show that this chi actually takes this form here. And so as u go to one, essentially you have only positive values that can occur unless the process is exactly independent. And so this implies that these asymptotic extreme value That these asymptotic extreme volume processes cannot capture the situation where the dependence is weakening as the extremeness of the events is increasing. So asymptotic Ixumbal models are always asymptotically dependent unless they are exactly independent. And this is a major limitation in practice because empirical evidence shows that usually environmental data suggests that the dependent strength is weakening as the Weakening as the level of extremists increases more and more. If you want to characterize this strength of sub-asymptotic dependence, and if you want to build models that have more flexible forms of tail dependence, it is helpful to look at this assumption by Letford or this model by Letford and Tone in nineteen ninety six. 96. So essentially, what I said is to assume that this chi measure at level u can be expressed using this expansion here. This is a slowly varying function, so we can ignore it more or less. And here you can see there is a coefficient eta, which is called the coefficient of tail dependence, which actually tells you how fast this chi u go to zero as you go to one. So if the limit is zero, you have asymptotic independence, but there You have asymptotic independence, but there can still be pretty strong sub-asymptotic dependence at sub-asymptotic levels, at finite levels here, right? And this eta coefficient will tell you what this sub-asymptotic dependence is. You can see, for example, the example here. This is for a Gaussian distribution. So, chi of u always goes to zero, so a Gaussian process is asymptotically dependent, but the rate at which it goes to zero is a very easy, But the rate at which this goes to zero changes depending on the correlation, right? And so this will be captured by this coefficient eta here. Okay, so the first class of models that have been introduced that can actually capture asymptotic independence and various values of this coefficient of tail dependence. The first class is inverted maxible processes, and another one. And another one is Mach Nietzsche model. So, Jenny has done a lot of work on this. So, all your questions should be directed to Jenny, I guess. So, the idea of inverted Maxible processes is simply to swap the tails of a maximal process. So, the upper tail will become the lower tail and the lower tail will become the upper tail. And it turns out that the lower tail of a maxable process is always asymptotically independent. And so, in this way, it provides you with a nice way to construct new. Way to construct new models that have this asymptotic independence property. And so it enriches the class of asymptotic independent models. So how you do this? You just, okay, you take a maximal process Z on fresh margins, and then you look at the reciprocal, one over Z. This will swap the tails. So this resulting process here will be asymptotically independent. You can compute the eta coefficient that I mentioned. It's just one over the extremal coefficient. Over the extremal coefficient v11. Now, one issue is that it still has difficult inference because it involves maximal processes. So, we cannot avoid really composite likelihoods and things like that. Another way to build flexible models is to use these max miction models. And so, what you do here is you take two processes. One, Z1 is a MaxStable process on the unit pressure scale, and Z2 is an independent inverted Max Stable process. So, one is asymptotically dependent. Process: so one is asymptotically dependent, the other one is asymptotically independent on the same unique pressure scales, and then you mix them together in this way. So, you take the maximum point-wise, location-wise, between a coefficient A or weight, if you wish, A, multiplied with Z1 and 1 minus A multiplied with Z2. And A is a coefficient between 0 and 1. So, potentially these. Potentially, this model can capture both asymptotic dependence and independence, right? So, if A is equal to one, you end up with a Max Stable process. If A is equal to zero, you end up with an inverted Maxable process. So, it goes from asymptotic dependence to independence. And it can even actually capture a change of asymptotic dependence class as a function of distance, depending on the type of maximum processes that you specify here. Now, one issue is. Now, one issue is that it's somewhat highly parametrized because you now have parameters driving the MaxTable process, parameters driving the inverted Maxable process, and you have this extra parameter A, which might be difficult to infer. And so, this is one of the things that has been a bit criticized about this approach. And again, likelihood inference is also a bit difficult because it involves maximal processes as well. When you say that it's highly parametrized, are you also taking in? Are you also taking into account that you have to estimate the marginal distributions to transfer these crochet? Okay, this would be some additional parameters as well. I'm saying here highly, well, it's not really very highly parametrized, but it's just that you have parameters in Z1, parameters in Z2, and you have A. Yeah. Right, so it's you're not fitting a single model, but this model is composed of two sub-processes, if you wish. If you are okay, one scale mixtures. Okay, this is another approach. So if you remember the construction of MaxSable and Pareto processes, they were actually built from scale mixtures of this type, R times W. Right, in the case of Maxable process. In the case of Max Deblede processes, you have the supremum over Riwi. In the case of Pareto processes, in fact, it is a scale mission. And in both cases, R was a variable that had a heavy Pareto tail, Pareto-like tail. So in the case of Maxwell processes, this R was a point of a Poisson process with intensity R minus 2 dr, so it's like a Pareto. And in case, in the case And in the case of Pareto processes, R was a Pareto variable itself. So you can actually get more flexible families of extremal dependent structures by changing actually the distribution of R, N, or W. And this is the idea here. So, if you want to know everything about this type of constructions, you can actually check that paper by Sebastian, Jenny, and Thomas Obitz. They have done a lot of They have done a lot of work to actually study extremal dependencies of these types of models in a lot of different cases. So, there is one particular case that is quite interesting, in a special case, it's Gaussian scale mixtures, when W itself is kept as a Gaussian process. Because we like Gaussian processes, and as one mentioned before, there are a lot of correlation functions that can be used, and everything is simpler when you can have some Gaussian component in your model. Okay, so the question here. Okay, so the question is that if w is Gaussian, what would be the tail behavior of R times W depending on R itself? And you can actually show that if R is paratotailed, then you have asymptotic dependence. But if R is wibule-tailed, then you have asymptotic independence. And so if you seek a model, if you want to build a model that can link these two cases together, Cases together, then the question would be: how can you build a distribution that bridges pareto-tailed distributions and widow-tailed distributions? And so in that paper, what we did was to actually propose that model here. See, it has two parameters, beta and gamma. It's actually a wider-tailed model with index beta. So asymptotic independence will kick in. But as beta goes to zero, this term will converge to log. This term will converge to log r, and this whole thing will become the Pareto distribution. So we would get asymptotic dependence on the boundary of the parameter space. And so this provides a framework that can bridge asymptotic dependence in independence. Okay, so that's one first type of scale mixtures that could be quite interesting. Another type is a model that we proposed with Jenny in 2019, and it has this form. So R times And it has this form. So r times w again, but then r is defined differently. Now, r is a Pareto random variable, and w is a process displaying asymptotic independence, could be a Gaussian process, but transformed to have the Pareto scale. So W and R are on the same scale. And then we again mix them somehow together using this parameter delta between zero and one. So the idea here is that if delta is less than one half, then If delta is less than one half, then in fact this term here will dominate in the tail, we will get asymptotic independence. But if delta is bigger than one half, this term will dominate in the tail, and because r is spatially constant, this will induce asymptotic dependence. And so you can actually show it formally, and you can actually derive the coefficient of tail dependence in explicit form. So this is the expression here, and this is indeed true. So you have asymptotic dependence. True, so you have asymptotic dependence when R is bigger than one half and asymptotic independence otherwise. And this provides a nice model that makes a nice smooth transition between tail dependence classes. All right, let me skip this. So, and let me skip this application as well. I just wanted to illustrate these types of models by just a small application here to wind speed data in the network. Wind speed data in the Netherlands. So, we actually reanalyzed some data set that was analyzed already in some other papers, Thomas Opit and also us in another paper, where we actually showed evidence for asymptotic independence. And so here what we have is data at 30 stations from 2000 to 2008. We took the months from October to March to avoid the modeling of non-stationarity, seasonality and so on. Stationarity, seasonality, and so on. And we estimated the marginal distributions non-parametrically for simplicity. So we put everything on the same scale and then we fitted different dependence models. One thing that we had to deal with was anisotropy. We, again, saw evidence for anisotropy, so we dealt with that in a kind of two-step approach. We first fitted the model to estimate these anisotropy parameters. Parameters, so the stretch and the rotation parameters, and then we kind of transform the locations in a way that we could actually fit an isotropic model to the data. So that's what we did. And this is the table where we compare the different fitting models. So, what do we have here? We have a Gaussian process or Gaussian copula model where we fit the copular structure of a Gaussian process. We have this hot. We have this hot model, the random scale nature model that bridges asymptotic dependence and independence. In the case where beta is set to zero or when beta is free, is estimated. We have the model we proposed with Jenny, the HW process, and we have an R Pareto process where we set the risk functional to be the maximum at the different sites. So these are the estimated parameters with the standard errors here. Standard errors here. And I just want to draw your attention on the BIC quantity here. And so this points to the fact that for that particular data set, the model we proposed with Jenny, the Huser-Wats-Worth model, is actually the best, at least in terms of this criterion. And surprisingly, our Parato processes, the RPA2 process is actually the worst, even much worse than a Gaussian process. And you might want Process. And you might wonder why that the case? I mean, our parato-processes have this nice theoretical justification that they arise as, you know, the only possible limits for threshold exceedancies and so on. But this has to do, again, with the assumption or the property that they cannot capture weakening dependence as the events get more and more extreme. And I think it's well illustrated here on this graph. Here on this graph. So here you can see the chi measure, this measure of extremal dependence, chi for just a pair of side and plotted as a function of the threshold U on the uniform scale. In black here, you can see the estimate from the data, then the non-parametric estimate. And the different colored lines correspond to different models being fitted. So in this purple line here, In this purple line, here is the Huser WhatsApps model, the best fit, and provides a pretty good fit here, as you can see. But you can see actually that the data suggests that this chi measure actually decreases, right? While a Pareto process would just assume that it's constant above a certain level. So this green line is the fit from the Pareto process. So if you see a decrease. So, if you see a decreasing behavior here, probably a Pareto process would not be a good fit. And this is something that these scale mixtures and this, yeah, this other model, even the Gaussian process can actually capture. So, some comments here. So, typical environmental data exhibit weakening extremal dependence as even become more extreme. This is a key feature that classical asymptotic, so maximum and parado models. Asymptotic, so maxible and parallel models cannot capture. And random scale mixtures of the forms R times W for various choices of R and W can be especially interesting here to bridge these two scenarios of asymptotic dependence and independence. Now, there are some drawbacks as well of these approaches. So, likelihood inference remains relatively intensive if we have to censor these non-extreme observations. And also, because this random variable R. And also, because this random variable R is essentially spatially constant, this creates some background dependence at any distance. And so this means that the model cannot capture full independence as the distance between sites even increases to infinity. And yes, also, I mean, it can bridge asymptotic dependence and independence, but we will have a fixed asymptotic dependence class for every pair of sites. We cannot, these models cannot capture. These models cannot capture a change of asymptotic dependence class as a function of distance. One exemption that we saw is these max niche models. If you specify the maximum process appropriately, there are some more recent alternatives as well, but usually this is something difficult to model. So I will skip this and I will skip the also the conditional special extremes. Also, the conditional spatial extremist model, that's too bad because I wanted to give an introduction about this because there are some talks about this later this week. But I guess we'll hear a lot about this type of model as well. But maybe I can just give you a very brief, very brief introduction to it. Okay, so as I mentioned, these random scale constructions, R times W, are quite interesting because they can really. They can bridge asymptotic independence and dependence. But they have two key drawbacks for application to large spatial products. One is that sensor likelihood is still intensive. So it prevents applications, let's say, beyond 30 or maybe 50 sites. Second, you have this kind of background dependence at any site, right? And so the random variables do not become independent as the distance. Variables do not become independent as the distance between sites increases arbitrarily. And so the spatial conditional extremes model was actually built to address these two concerns. And so essentially, this is, okay, let me skip this. This line here shows you essentially this spatial conditional extremes model. So, what you do is to look at So, what you do is to look at one site being large. You pick a site of interest and you look at all the events that exceed a certain high threshold at that particular site. And then you try to characterize the rest of the process at the other side. And the model is that the rest of the process will be like a certain function a of the conditioning random variable x of at this naught plus maybe a scale. Plus, maybe a scale, so the different function b of that variable x of this naught, multiplied by z0, which is a residual spatial process. And so now the, and this is actually motivated by some concrete examples, right, looking at what happens under asymptotic dependence, independence, Gaussian process, and all types of processes. So now the question is: what kind of functions can you pick? Functions can you pick A and B and residual process that what can you pick in order to have like a flexible modeling framework that can cover a broad case, a broad range of scenarios? And so in the original paper by again, Jenny and Jon Tone, one of the models they proposed was to take A to be of this form. So a certain function of the distance between site S. Distance between site s and the conditioning site multiplied by x. And so this function is written like that. So it could be one up to some distance delta, which will give asymptotic dependence at short distances. And then it decays exponentially fast for distances beyond delta. And then B can be, for example, like this, or could be simply X to the parabeta. And then the goal. X to the power beta. And then the goal would then be to estimate beta and estimate these other parameters, so delta, kappa, lambda, and also the parameters that are embedded in this residual process Z0. There are again different ways to define Z0 as transformed Gaussian processes, but I don't have time to explain that now. Is S naught fixed? Like you have a problem and you just say, okay, S naught is this location. This location and end of story. Right, so okay, you have to pick a site and then you write down the likelihood for that particular conditioning site. Okay, and usually the likelihood is nice because if you make some Gaussian assumption on the residual process, then the likelihood would be essentially a Gaussian likelihood, which makes it fast. But then you're right. I mean, there is usually no very appealing way to pick a good conditioning site. So, what they have done in the origin paper is to In the original paper, is to write down the likelihood for like a selection of conditioning sites and then combine them together in a composite license. And so essentially, let me skip all the details here. So the spatial condition extremes model can actually capture complex forms of extremal dependence. Complex forms of extremal dependence, and we'll hear more about this during the week. Including this change from asymptotic dependence to asymptotic independence, which is nice as a function of distance, it can, in principle, be fitted to quite large dimensions because it avoids the need for sensoring, usually, unless maybe you're modeling rainfall data or something like that, and you have to deal with, let's say, point mice at zero, something like this. Something like this. And it is actually attracting quite a lot of interest currently in the extreme value analysis community. And so we'll have multiple talks about this during the week. Now, some of the drawbacks I want to mention as well is that there is no real unconditional representation of the process, which is somewhat inconvenient. And then, if you have a model for, let's say, a Let's say a conditioning spatial extremist model, assuming that particular site experiences a high value at this site here, and you have a different model, similar model there, with a different conditioning site, then reconciling these models may not actually be easy. And there's a lack of consistency between separate models based on different conditioning sites, which makes it a bit difficult. Another difficulty is that Another difficulty is that this model can sometimes also be quite heavily parametrized, right? Because we have all the parameters controlling the A function, all the parameters controlling the B function, all the parameters controlling the residual process. So you have these dependence parameters, maybe also marginal transformations to make this residual process non-Gaussian. So it can actually be quite heavily parametrized, let's say between, I don't know, eight to twenty parameters, maybe. To 20 parameters, maybe, so which can be quite difficult sometimes. All right, so I just want to end with this slide and then I start. So these are just some perspective for current and future research. So there is still the topic of trying to build and propose new models that are flexible for extremes, combining synthetic independence and dependence that are non-stationary. Stationary that are flexible for space and time. There is the topic of combining machine learning and extremes. Of course, this is the topic of this workshop. Here I mentioned amortized neural inference with intractable extreme value models. We'll have actually quite a few talks about this, and this is a new exciting way to make very fast inference for very complex models that have intractable likelihoods. So I'm quite excited about it. So, I'm quite excited about this new line of research, and we'll hear more about it during the week. The topic also of building sparse graphical or SPD models for extremes, we'll have a few talks about this also during the week. I also mentioned here causal inference for spatial extremes. I'm not sure we'll have a lot of talks about this, but this is also quite interesting, I think. Yeah, and that's it. So, thank you very much for your attention. I apologize for having to skip too many things, but I will. I apologize for having to skip too many things, but I will make the slides available to anyone anyway, so you can check in later. Thank you very much.