We've talked about CSP. I'll define all these terms, except SPPs, of course, which all of you know. And this is joint work with two amazing students. I had the pleasure of working with Josh Rattensiek and Saison. Okay, so what are CSPs? I think this audience doesn't need an introduction, but just to set some notation, we will talk about a predicate over some finite domain of some bounded RP. And really for this talk, I'll focus And really, for this talk, I will focus on b equals 0, 1, the Boolean domain. And an instance, you have a set of variables, and certain k-tuples of these variables are supposed to be constrained by b and your goal is to find an assignment which satisfies all the constraints or maybe as many constraints as possible in the optimization. So, as you vary p, you get different problems. And of course, this is a very well-studied and influential paradigm, you know, ranging from NP completeness to the PCP theorem to unique game. Completeness to the PCP theorem to unique games, and indeed spearheading a lot of the SDP-based approximation algorithm. And you know, prototypical problems, three sat, unique games, you name it, lots of problems can be captured. So, what are promise CSPs? This has been quite an active thing in the last several years, but not all of you might have seen this. So, what is P C S P is? It sort of originated in a work I had with, this is on some timing, in a paper I had with Kerr Austin and John Hollow. I had with Kar Austin and John Hostard about 10 years ago, and I think the framework was more crystallized when we worked with Josh later on. So, here instead of one predicate, you have a pair of predicates and they have the same additive, but one predicate is stronger than the other. So, P implies 3. So, just to give some examples, in the first two canonical examples, which are important in the theory, for example, P can be 1 and 3. You can have 3 variables, Boolean variables, you might say exactly 1. That's an NPR problem. Q can be 1 or 2 out of 2. Q can be 1 or 2 out of 3. You might allow 1 or 2 out of 3, which is just not all equal to this. Of course, P implies Q. Both P and Q, C S P P, C S P Q are both in PR. And another example is that a version of SAT. So Q is R, which means just the SAT. At least one of the literals is true. Here you should allow negations for it to be sensible. Of course, that implies. So, what is a P C S P which is parameterized by phi, Q? In a P C S P Q, in a PCSPT command q instance, you are given again n variables and constraints on some tuples. I promise you that there is a satisfying assignment which satisfies all the constraints when you put p there. But I only ask you to satisfy it according to q. So that's the problem. And a canonical example actually is like graph colouring. So p may be 3 colorability, q may be 6 colorability. So I can give you a 3 colorable graph and ask you can you 6 color. And ask you, can you 61? A problem whose complexity is still too long. But 1N3 sat and NAE3 Sat are both NP-hard, but surprisingly, the combination is in polynomial time. And if you are bored in this talk, I will come back to an algorithm for this later, but you can think about it. It is a nice exercise. So, if I tell you exactly 1 out of 3 are satisfiable, you can find an assignment which satisfies 1 or 2 negative points. You will avoid monochromatic. This can be viewed in terms of sort of hypergraph 12, you will avoid monochromatic. Avoid monochromatic hypothesis. And the same holds for the second problem as well. In fact, this problem was the basis of this ATH paper, the two precepts on SAP actually. And in general, you allow multiple pairs as well. So that's PCSP. So why PCSPs? Well, first of all, for this audience, it's a different model of approximation. Rather than approximating the number of constraints you satisfy, you approximate how you satisfy it. You satisfy it in a relaxed manner. So it's a different notion and graph colouring is a So, it is a different notion and graph coloring is a very well-known approximate graph coloring is a well-known problem. Can you ten colour the three colours? It captures those problems, it captures many other problems. And what is also exciting is that it gives you new insights into the theory of classical CSPs as well. Note that it generalizes classical CSPs. If you make P equals Q, you go back to CSP. And it's also, you know, fun and challenging. So, we regulate it. And this talk is about robust algorithms for PCBS. So, what are robust algorithms? And first, I'll say this. What are robust algorithms? And first, I will say this in the context of CSPs. So, this was introduced in a paper by Uri Zwick in 1998. It's a paper I really love, it's an amazing paper. So, what Uri said is that, okay, there are problems which are easy, like two sat and so on. If it is satisfiable, you can find a satisfying assignment. But what if it is almost satisfied? There is an assignment satisfying almost all the constraints. Then, can you find an assignment which also satisfies almost all the constraints? On the constant for an f of epsilon, which is you know also goes to 0 as epsilon goes to 0. And indeed, it turns out, and the motivation for this was like Hascard's hardness result, which showed that for linear equations, this is strongly not possible. But the Gomans-Williamson algorithm said for Max Cut, you could do this. That actually followed from Gomans-Williamson. But if you want to do it for 2 sat, it was more work. So, Zwick got cube root epsilon, which was later improved by Charikar Mataricha, Mataricha plus 5 root epsilon. For Hansat, also you can have such a robust algorithm. And this term robust comes because it means that you have a satisfied with the algorithm which is somewhat robust to notes. Even if there are few corrupted constraints, it can handle. For once at the guarantee is exponentially worse. So you can only do 1 over log 1 over epsilon. And also these bounds are known to be optimal under the QGC. In fact, showing that the max cut bound is optimal was in fact one of the original motivations of Subhash. Original motivations of Subhash in his original paper, it was one of the first applications of UGC to hardness. And it also in a later paper with Juan Zhao, we showed that onset also this behavior is there. So these things, this is the story. And again, there are also hardness. As I said, Hastar's results said that this is not the case for every problem which is easy. For linear equations, if there is a tiny bit of noise, you are in trouble. You cannot satisfy more. So, you can naturally ask for which easy. To ask for which easy CSPs can you have a robust algorithm? First of all, the CSP should be easy because if it's hard to tell if it is satisfiable, you can't have a robust algorithm. But if you can tell if it is satisfiable, when do you have a robust algorithm? And you know, and we have a beautiful answer in the case of CSPs and something called bounded with CSPs. I won't define that. But bounded with basically means that some sort of combinatorial local propagation, like for 2 sat or 1 sat can decide satisfiability correctly. Decides satisfiable T correctly. Whereas something like linear equations requires Gaussian elimination, which is a global algorithm, and those are not. In remaining cases, it's entity hard. So you probably guess where I'm going. So CSPs, robust algorithms completely understood. What's the story for these species? So that's the point. So I give you a Boolean and I will focus only on the Boolean case here. That's already hard enough. By the way, so this theorem here holds for all alphabets. That was in fact the point of our proposal. It was already known for the Boolean. Of our proposal. It was already known for the Boolean case, right? But here I focus on the Boolean case. So, what should I ask? I give you a 1 minus epsilon satisfied, for instance, of P. So, there is an assignment satisfying most of the constraints according to P. You find me an assignment satisfying most of the constraints according to Q. If you put epsilon as 0, you get back the old PCSP, but this is the robust version. So, now which Foolient PCSP is a robust art. Okay, and what's the tool for doing all this study? And what's the tool for doing all this study in CSPs, P C S P's? It's something called polymorphism. So, what is a polymorphism? It basically is a closure property of the solutions of the CSP. For example, if you take 2SAT, an interesting property is that if you take any instance of 2SAT and any three satisfying assignments to it, and if you apply component-wise majority for each variable, you output the majority of its values in the three assignments, you will get a new assignment which is also satisfied. Also satisfied. So that's what, and this is true for every triple of solutions to every satisfiable instance. So that's a closure problem, some sort of a discrete convexity. You can take three solutions, combine them in this non-trivial way by taking majority, you get a new solution. If you take 3 sat or other LPR problems, you will not have. And this is what we say, majority 3 is a polymorphism of 2 sat. Okay. So, okay, this timer is wrong. Okay, so now what about for PCSP? We are going to do the same. Now, what about for PCSP? We are going to do the same thing. If you plug in any solutions according to P, so if you satisfy according to P, you apply the polymorphism, you will get a new thing which satisfies it according to Q. So that's going to be the tool. Why do we study with this tool? And we say it's a polymorphism for PCSP. I'm not defining it super formally, but you get the picture. So, you can non-trivially combine solutions to produce new solutions. And in the case of PCSP, you produce a weaker new solution. You produce a weaker new solution which only satisfies it according to Q. And polymorphisms have some structure. In particular, there is something called minor closed, which means that if you take a bigger polymorphism, you can identify some variables that will also remain a polymorphism. That is easy to see. And therefore, these are called minions. So, that's one thing. And so, the set of polymorphisms have the structure. But if you take CSPs, they have further structure. You can take polymorphisms, and because the output also satisfies it according to the same predicate. Satisfies it according to the same predicate, you can keep applying it and you can get more and more polymorphs. For example, here if you have majority 5, you can go down to majority 3, but for CSPs, you can from majority 3 by composing majority recursively, you can actually produce all majorities. For PCSP, you can't do this. So, this is a fundamental difference making the theory much more difficult. In CSP, you have this rich algebraic structure, something called a clone. P C S P's you do not have that. The only thing structure you have that is the The only thing structure you have that is that it's a mini. Okay, so why am I going with polymorphisms? I should keep an eye on time. Polymorphisms precisely capture why they are the right tool to study this problem because they exactly capture the power of reductions and really if you have more polymorphisms you can reduce one problem to the other. So, a rule of thumb you want to have is that if you have more polymorphisms, you will have algorithms, if you do not have any polymorphisms, you will have hardware. And this is completely established for And this is completely established for CSPs. For a CSP, you have efficient satisfied with the algorithm if and only if you have a non-trivial polymer. Non-trivial, I won't define, but something very basic. That's completely established. Such a theory is far from clear for PCSPs, and that's what we have been trying to address. So, that's the picture you should have. So, what do we know for Woody and PCSPs is that for certain special cases, for example, if for symmetric cases, For example, if for symmetric cases, what do I mean by symmetric? Both P and Q are symmetric predicates. They don't depend on the number, they only depend on the number of models in series. And the examples I showed you were symmetric. In that case, we have a complete classification in Josh. There are certain cases when there are three kinds of polymorphisms which lead to algorithms. Parity, which is like linear equations, majority, which is like FUSAT, and there's something called alternating threshold, which I'll define later. Alternating threshold, which I will define later, but not too important, some sort of signed majority which also gives algorithms like in this problem I said, 1 in 3 versus not all in 3. All other cases are in p half. This is for exact desired unit. So, what we would like now like is to have some such picture for PCSPs and that is for robust PCSPs, which is what this work is. So, what are the results? Our results, first of all, algorithmically, unconditional result is that if we have a majority or Is that if you have a majority or alternating threshold polymorphism of every odd entity, then you automatically get 1. So, what is alternating threshold? It's basically just this. So, it's like majority except the odd even places are minus. The odd places are bigger than the even places. It's a non-trivial Boolean function. That's all I think about it. So, it and if that polymorphism exists, you get one. And what is nice about this is that this gives a conceptual explanation for Zwick's algorithms. Explanation for Zwick's algorithms. Zwick gave for 2 sat, he showed there was a robust algorithm and 2 sat has majority. So we explain why we get this in a more and of course it then covers more cases. And same thing for CONSAT also we cover, you know, we get an explanation. And what is our specific algorithm? So for majority which covers cases like 2SAT, max cut, but also some PCSPs, we get f of epsilon, which is polynomially smaller. For alternating threshold, which covers this 1 in 3. Threshold, which covers this 1 in 3 and MAE, we have this exponential loss as in the case of on sign. So, that is those are the algorithms. Again, I will try to give a little bit of glimpse of the algorithm later. And we also have hardness result, but this has some sort of asterisks. So, for Boolean, symmetric, folded PCSPs, we say that if you lack majority and alternating for some odd alpha, then you immediately become under the mean. immediately become under the unique inspiration. So the previous thing said that if you had majority or algebra, you either have majority of all odd articles or alternating threshold of all oddarity, you get algorithms. If you miss even one of them in both columns, you get algorithms. So it's actually a type. So the earlier theorem is not about symmetric, which applies to anything. Yes, so the earlier theorem is unconditional and applies to all things. In fact, it only cares about the the polymorphism. Polymorphisms. This one, as I said, has some caveats. Well, first caveat is, of course, the hardness is under the Games conjecture. Maybe we live with it these days. But it only again works for symmetric. We really for the hardness we sort of need the symmetric. And that was also the true of the picture I showed earlier for the exact satisfying. Algorithms are unconditional, the hardness was symmetric. So, all predicates are symmetric. Folded is a minor thing, but it is a bit annoying. We do need to allow negations of variables. This is some technicality. This is some technicality. So, we allow negations for. So, we have this tight picture. I think I have a diagram here. So, this puts both of these together, you get an efficient, robust algorithm for this class, the restricted class of symmetric folded PCSPs, if and only if you have majority for all order L or AT for all. And that is the picture. So, the three blocks split into two blocks. Why did one block disappear? Because parity is very noise sensed. So, these Gaussian elimination type algorithms are not noised. So, that can Type algorithms are not provided. So, that can just take it away that way. And there are some interesting cases here which are also have overstarred. So, now what we are saying is that for this p out of 2 p sat or 1 in 3 and not all equal 3, not only do you have decision algorithms, you also have robust algorithms. Okay, so that is the picture. Yeah, that is the results. So, hopefully, this, I know there's a lot of new information if you haven't seen these things. You haven't seen these things. Now I'll say something about the technical parts, but I'm happy to take questions here. So I'm still confused. So only 2SAT, there is a robust value. Yes. So, and in PCP, you are only expanding like Q is bigger than so that automatic, right? No, no, no, so but so for example 2 sat, so this is another problem. Suppose I give you 2 out of 4 sat. So P is 2 out of 4 and Q is 4 sat. So it was not clear how to extend which algorithm. I mean now of course we can and indeed it's pretty. Now, of course, we can, and indeed, it's pretty related to the CMM algorithm and so on. But it took some. But the nice thing is, now it's very general. It really explains why these algorithms exist. Even though their mechanics ultimately are not that different. They are SDP-based as a member. So, so far I have talked about robust status five P CSPs. SDPs haven't figured at all, and they come because these robust algorithms, even for CSP case, were SDP-based. CSP case but SDP case. So that is the thing. So, overview of techniques. So, again for algorithms, we write the basic SDP relaxation, nothing fancy, but then we have to be a little clever in the rounding with some variance of random hyperplane rounding. So, nothing really we did not know for a long time, we just have to apply it in some new ways. And for hardness, there is something also in some sense, it is what we knew. We know that if you get an integrality gap, you immediately get hardness, courtesy of. Get hardness courtesy Raghavendra, and that is what we do, but again, the mechanics of how we do it is a bit more nuanced. And our conjecture, if you want to, so what is our conjecture? Our conjecture is that in some sense, SDPs are it for robust algorithms. For any PCSP, we conjecture that if you want a polynomial time robust algorithm, the basic SDP must correctly design satisfied. So, this is our conjecture. We do not know how to prove it, but the conjecture. We don't know how to prove it, but the conjecture is true for CSP. So, you could say there is some basis for it. And of course, it is the case for all the cases we have classified so far. So, if you want to again take one conceptual message, you can say SDP-based satisfiability algorithms are somehow robust. If you do something like Gaussian elimination, the other class of algorithms, that's more robust. So, here is a robust algorithm for this. This is how much time do I have? 10 minutes, okay. So, let me tell you about the robust algorithm for one in three. You about the robust algorithm for 1 in 3, not all equal. So, let us start with the basic L P relaxation. Again, I am promising you it is satisfiable according to 1 in 3. So, what can I do? If I have a constraint on 3 variables z1, z2, z3, I can say that the LP values must be in the convex hull of these three satisfying assignments. So, I can write a linear constraint like this. So, put another way, I can say that these zi's I am going to use the same as the L P variables. I can say there is some distribution on these such that Zi is the marginal. These such that Zi is the margin. That's what an LP does. It is just a slightly different view than you might have seen, but this is really the standard SDP, LP relaxation. For each constraint, you say that the LP values must be marginals of a distribution supported entirely on the satisfied signals. And what does an SDP do? An SDP also captures pairwise marginals. So you add these second moment constraints. So you basically will say now, and you will also put a PSDN as constraint. So you will say that the first So, you will say that the first moment must be the inner product of Vi with V naught, where Vi is the vector for variable i, and the inner product Vi Vj should be the expectation of x. And x should be completely supported on this. I can write such an SDP. So, now how do I round it? Well, so in this case, we know because it's completely supported on this with probability 1, these two conditions are 2, and if we just do a little bit of Conditions are true, and if you just do a little bit of algebra, this basically means that the sum of the three vectors should be minus V naught. You can even write this constraint directly because you can really think of as, you know, V naught as 1 or minus 1. I can just get confused. So I get 1 minus 1, minus 1. So we take V naught as 1, one of the vectors should be V naught, the other true should be minus V naught. So you get this. So this condition you can write for every constraint. And now perhaps the rounding. constraint and now perhaps the rounding is clear right. So, you sum up these three vectors, you can project away the component according to V naught. So, you basically take Vi prime to be the component of V i perpendicular to V naught. Now, those sum up to 0. So, if you do a random hyperplane rounding with probability 1, you will cut. You won't, all three won't be on the same side. That means you get a satisfying assignment which is not all 0 or all 1. You have satisfied not only. You have satisfied not only. There are other algorithms for this thing too, which are not SDP-based, but SDP-based one is amenable to making robust. This is an algorithm to decide 1 in 3 versus NAEC. Now, how do you make it robust? It turns out to be a little bit tricky. At least it took us a little longer than it should have, I guess. So again, now the only change is that you will basically say that on average, most of the mass will be on these three satisfying sides. Of the mask will be on these three satisfiable signals because it is satisfiable, most of them are satisfiable. And if you do the arithmetic, that will effectively imply that the sum of the norms of the component perpendicular to V naught is not zero, but it's small. Okay, and that's some of the picture you should basically have. So, these three things are going to be this. Their sum is very close to zero. So, really, the algorithm is going to be to fix some threshold P and do random hyperplane rounding outside that wall, outside that red. That ball outside that red ball, there, and you will choose this threshold from a geometric size. So, that might not have been too clear. So, again, yeah, so if it falls here, you do random hyperplane rounding. If it is here, you do something else, you do some sort of threshold rounding. And the precise thing I have written down here. So, the rounding approach is that take a random n-dimensional Gaussian vector, and if the inner product with this Vi prime, the projected thing, is at least some threshold, which depends on Vi V0. PI V0, then in that case you go to 1, otherwise you go to minus 0. I do not expect you to follow this online, but just to show that this is the more or less the rounding. And the point is the way these things work, you can show that this is going to be in trouble only if this delta is comparable to the norm of this projection rate. In all other cases, one way or the other, it will satisfy the NaE3 constraint. That is just two cases you have to analyze based on whether this is large. Based on whether this is large or small. So, one thing is that if this is kind of large, then because this is just at you know minus delta at best, this is all this basically becomes like this sign rounding as previous time. On the other hand, if all of these are small for some other reason, it satisfies it. So, the only trouble is when delta is comparable to this, which could happen, but to avoid this, you pick delta from a geometric series. So, the chance that it is comparable to this, say, within some factor, constant factor, is just factor is just small and you get this log 1 over epsilon because you will do a geometric ratio of r and we may not use r some theta and there will be some 1 over theta you get this log 1 over epsilon by log log over epsilon ok so how do we do hardness for hardness we can follow this paradigm you know just giving an integrality gap so we are going to give however we are going to think about an infinite integrality gap so the variable set will be all So, the variable set will be all unit vectors in some n-dimensional thing. And what are the constraints you will put? You will basically put all constraints you can so that the SDP will be feasible. So, if the inner products of these vectors and also with V naught are consistent with marginals of a distribution supported entirely on P, you throw in that. So, basically, put the canonical things you can. So, the idea is that the SDP solution will be just these vectors itself. So, this will be satisfiable according to. This will be satisfiable according to SDP, and that's what we do. And then there is a few steps to go to hardness. So, this is our overall picture. So, we want to do this for any PCSP without these things. So, we do some case analysis to identify the easiest such PCSPs, which you know, and this is where we use symmetry in a heavy way, we do some case analysis. And then, for those cases, for all of them, we show it. Those cases, for all of them, we show integrality gaps for this sphere instance, and then by compactness argument, we can translate it to a finite integrality gap. And then you apply Rag-Wenzer as a black box, you have robust hardness. So, really, the core work is in these first two things. The compactness is also there. So, where in this sequence they lack or remotely? In the first step. Yeah. So, because of the first step, you can say that because they lack these things, these they must look like circumstructures. These things, they must look like superstructures. So, basically, you reduce to five cases and all those. So, for example, you don't have to consider a case one in three NAE3, which of course is easy. So, you reduce it to a few. How does it show that your SDPE solution is of a certain form? Like, how does it exploit it? So, I'll come to this in a second. So, the code work is this, and that's where the nice new thing is. I think the sphere-Ramsey arguments are nice. For example, just to step back, here is a nice fact, okay, beautiful theorem of Matusha controller. Beautiful theorem of Matushak and Rodel, which says the following. And this is related to the configuration. But this is a purely just combinatorial fact. If I take any two coloring of a sphere of large enough dimension, you can find k vectors which have the same color and have same inner product as long as this gamma is more than minus 1 over k minus 1. So what is significant about this? When gamma is minus 1 over k minus 1, it is the simplex. And of course, you can do the northern hemisphere, southern hemisphere coloring and Northern hemisphere, southern hemisphere coloring, and there won't be a monochromatic symphony. But the moment gamma is a little bit bigger, you must find monochromatic. It's a very nice theorem. And this even applied as a black box basically allows us to show that there is no integral solution for a wide range of them. Because and why is what is the connection here? This monochromatic would basically mean that you will violate this not all equal 30. So there will be no integral solution. An integral solution is a true coloring and we will say any true coloring. Coloring, and we will say any true colouring must have a monochromatic. So, it is not satisfiable as a not all equal. And not all equal is one of the key cases which comes. This doesn't cover all cases. So, in certain cases, we have to prove some new steer-Ramsey results, leaning heavily, of course, on Pragmatus Code. And I'll give you just one example to just sort of make it a little bit concrete. For example, if you take this case, which is one of the cases we have to consider, so p is 2 or 5 out of 5. So, I have 5 variables, either 2 or 5. So, I have 5 variables, either 2 or 5 must be satisfied. Q is NAE5. Not all should be 0 or all should be 0. And of course, it is a valid PCSP because P implies Q. And again, you are going to put constraints on, excuse me, it's time to think. You should put constraints on these tuples. You can put, we only argue about some subset of constraints, which is enough. The other constraints we don't use. So here is a distribution which it then turns out to work. You put 5/6 of the mass on couples which have of On tuples which have of weight 2 and 1/6 on the all ones. And note that it is entirely supported on 2 or 5 out of 5. So, whenever the inner products have such a structure, you put a double in this thing. And it turns out it works out to be exactly this condition. So, Vi is orthogonal to V naught and every pairwise inner product is gamma for some gamma more than minus 1 over, in this case, pi minus 1. So, k is pi. And now we know thanks to Mathusia Proto. We know thanks to Mathusha-Rodel that any two coloring, so any Boolean assignment to this infinite instance must have a monochromatic k-tuple matrix. So it's not satisfiable as an any. And the point is once you come, you know, use compactness and make a finite instance, you get a tiny gap, 1 versus some 1 minus some tiny bit, and that is enough, but that tiny bit is a fixed absolute constant, and that's enough by Raghavendra to give you 1 minus epsilon versus some constant bounded away from 1 hearts. Constant bounded away from one hardness. So, there is no robust. This was one of the easy cases where we could lean on Matushet Rotel as a black box. Some other cases we had to work a little bit to prove the necessary sphere Ramsey like and yeah, so that's basically all that I have. So, we have this nice picture, albeit for this case of Boolean symmetric folder PCSPs. And I think the most major restriction here, I would think, is symmetric. Restriction here, I would think, is symmetric and also Boolean. But I would be perfectly happy with the classification for Boolean, that already seems plenty difficult and deep enough. But it would be really nice to remove symmetry for the hardness. And this has been a problem in our this program we have been executing for from the beginning. Some of the non-symmetric cases we don't know how to handle. Actually, most of the natural cases are symmetric, but still, theory is incomplete. And the point is, the algorithms and SDPs are closely related, hardness are closely related. Our hardness are closely related to SDPs. If you have an algorithm, the SDP is doing it, and if the SDP has an integrality gap, which we have to show using some new methods like Spear Ramsey, then you have robustness. So SDP is holding it. So there are many challenges as you can imagine. So first of all, there are situations where there is no robust algorithm, the hardness cases. So it will be really nice to extend it to non-symmetric cases or beyond Boolean, both look difficult. Look difficult. One thing I swept under the rug, which is a bit annoying, is that we only said in general you allow multiple predicate pairs, but our hardness somehow only works in the case when you have a single predicate pair, P, Q. We cannot do P1 Q1 and P2Q. Sort of a technical shortcoming of our proof that will also be really good. Our algorithms do not care, you can have multiple pairs, but the hardness has that weak spot as well. And for the case when, and I am also interested. And for the case when, and I am also interested in some of these questions here, but the case when there is a robust algorithm, you can first of all ask if these guarantees we get, especially the exponential loss for this 1 in 3 NaE3, can you improve it? I don't know matching hardness. In fact, I realized that we do not even have weaker hardness results like we know, for example, for 2SAT or MaxCut for this. So that's also a nice question, and this is something we are thinking about right now. And again, just to contrast with the CSP. And again, just to contrast with the CSP case, for 2 sat and 1 sat, we have tight UG harms. So, may don't know. Okay, so I think I'll start. Questions? That's next. Don't forget to come back at 11 to move one more time. Don't forget to come back at 11 so we have one more left.