So it's going to be a quick announcement first. Yeah, so folks, two quick announcements. One, I hope you'll come to the theater tonight in this room at 7:30. I really hope the actors come. I also hope everybody else comes. And if you have family here or other people here who are with you, please bring them along too. It should be fun for everybody. Them along too. It should be fun for everybody. Second announcement is: anybody who's interested in signing up for one of the 10-minute talks for today, there's still slots available for that. Not for today. Not for today. Thursday. For Thursday. There's still slots available. But we won't have a session tomorrow night. So next session is Thursday of the sir. It's my pleasure to introduce Joseph Ma from CUNY Stat Island. Joseph, if I remember, Staten Island? Joseph, if I remember from many years ago, I had some of the first results I ever saw on random results with three manifolds: random Haguard splittings, random walks, three mapping class groups. So I'm excited. So today we get singularity measures, so can person maps. Yeah, so I'd like to thank the organizers for inviting me. And yeah, thank you for what you described. So yeah, I realize I'm not actually going to talk about anything we're going to do. Anything really to do with not. And so instead, I'm going to sort of take the liberty of saying some sort of very basic things to sort of motivate why we would think about this question. And so feel free to interrupt me with questions or like groans or boredom or whatever arises. So yeah, and so when I get on to the result, this is everything I say is going to be joint work with Vaikov Gadri, Thomas Etel, Catherine Fatt, and Charlotte Yeo. And so, yeah, so maybe I'll just sort of. I'll just sort of so the basic motivation is to ask sort of what does a generic object look like? Well what sort of properties does a generic object have? And so if you'll need And so, if you're me, the objects you're thinking about are usually some sort of like collection of geometric mathematical objects. And as soon as you have sort of infinitely many of these things, it's usually an infinite collection that doesn't come with some sort of canonical probability distribution. So, most infinite collections just sort of setback, and when you want to choose one at random, you end up having to make some sort of choice about how to choose one. And so, maybe instead of saying, And so, maybe instead of saying anything in general, I'll just start off with an example and show what happens with a specific example. So, in fact, let's think about a group. I'm just going to pick a totally specific group. Let's pick three group on two generators cross C. This group is so specific, I can actually write down for. Is so specific, I can actually write down a presentation for it. So, actually, let's say that the A's and the B's generate the free group, and the C generates the Z direction. And so, what I want is I want C to commute with the other two elements. So, this is some group, and so if you've thought about this group beforehand, you might have some opinion about what you think a typical element of this group is. Of this group is. And so I'm going to give you two different ways of trying to sort of choose an element at random. And the first way is going to be to look at the Kelly graph. So in order to define the Kelly graph, I need to pick a generating set. So this is going to be my generating set for the group. Let's call it S. And so the Let's call it S. And so the KD graph for a group, well, it's a graph, so the vertices are just group elements. And so what are the edges? So I'm going to connect a pair of group elements if they differ by multiplication by Multiplication by generator on the right. So the edges are going to correspond to ggs, where g is any element of the group, and s is in my general set. And maybe just to make my life easy, let's take this to be a symmetric deriving set. So for the purposes of building the K world, to make that Derring set, symmetric says throw in these verses as well. So given any finite generative group, this gives you some sort of relatively finite graph. Graph. And so, what does this graph look like? So, okay, this is a very simple group, so I can tell you exactly what the graph looks like. So, if you just look at F2, then the Cayley graph is just the infinite full value tree. So, I'm going to choose the convention that if you go to the right. You'll choose the convention that if you go to the right, that corresponds to doing A, and if you go up, that corresponds to B, and the reverse directions correspond to those inverses. So there's the identity. This is A, and that's probably AB up there. So what does the Kelly graph of F2 crossed at C look like then? Well, it just looks like an infinite stack of these connected by vertical lines corresponding to moving in the C direction. And so I'll just say that my artistic And so I'll just say that my artistic skills are totally inadequate to drawing a useful picture of this. So I'll draw something, but I'll just invite you to imagine that I drew something comprehensible. So you've got a stack of layers of these things here, and they're connected by these vertical lines corresponding to the C direction. And so because we have this sort of very explicit structure here, I can actually give you a very explicit description of the board of radius n. So let's write, say, B. Write, say, BN1, say this is the ball of radius n in the caddy graph center of 1. And so now what I can do is I can say, okay, I'm going to pick an n, and I'm going to, that gives me a ball in the category of this finitely many group ones that's in there, and I'm just going to put the uniform probable distribution on that. Put a uniform probability distribution on that finite set and pick one from there. And so that's sort of the typical element from the ball of radius n in the Kelly bar. And so, you know, we know various things. So if we look at this thing here, so if we look at the ball of radius n just in f2, then the size of that grows roughly like three, it was three to the n. So there's some constants I'm ignoring, but as you head out into Kelly Graph, As you head out of the Kennegraft, at every vertex, once you're out away from the origin, at every vertex, there's three new directions to go in. So the growth rate is basically three to the height. And so what does this ball look like? So sort of at height zero, so corresponding to the zero in the z over here, what you have, so if I think of my z direction coming this way, height zero, I exactly way at height zero, I exactly have the bored radius n in F2. So here I have the border of radius n in f2, which has roughly 3 to the k elements. So if I go up to height 1, I've now lost one unit of distance because I had to go up in the c direction. So I have a slightly smaller ball in the Kaylee graph up here. So that's the ball of radius, so when I say k there, I meant n, that's going to be the ball of radius n minus 1. n minus 1, and so I have roughly 3 to the n minus 1 elements, and so on. So that's exactly what the border radius n looks like in the Kellogg graph for this group. And so then we can ask, say, what's the probability, so if, so let's say this element I'm picking at random, let's call it Wn. What's the probability WN say is at height zero? 0. Which I just mean that the z coordinate in this product is actually 0. So, because of the way exponential growth works, a definite fraction of the elements are going to be at height 0. And so, when I thought about this late last night, I decided that fraction was 2 thirds. Of course, maybe that's wrong, it's some other number. It doesn't really matter. It's some positive number that's bigger than 0. So, you're getting a definite fraction of stuff at height 0. And so, you know, and so this may or may not agree with your original. Or may not agree with your original intuition about what typical elements look like in the Kelly graph, but actually, typical elements in the Kelly graph are actually pretty close to zero in terms of the z-hype in this specific model. And so that's a feature of the model, and you may or may not like that. Any questions about anything I said right there? Yeah, so, right, there's a negative one down here. Oh, so it's Joey. Yeah, okay, thank you. Yeah, okay, thank you. Okay, so so yeah. I d I I I yeah, I do not know exactly what number is, but it's some number very similar to this, possibly a different one. So yeah, so you're dead right, I forgot about the negative ones when I was drawing pictures yesterday. So yeah, yeah. So yeah, so you've got the downward direction as well. Yeah, so then it's probably one trigger. Okay, so what's another way of picking up? Way of picking elements. So another way is to say do random walks on the group. So if you pick a generating set for the group, you get a K graph. If you pick a probability distribution on the group, you can use that to generate a random walk on the group. So I'm just going to remind you a little bit about random walks. So yeah, so you pick a probability distribution. On G. And for the purposes of this talk, all probability distributions generating random walks would have finite support. So I'll probably forget to say that as we go on. But for the moment, let's just think about finite supported probability distributions. And I'll just say, you know, maybe geometric group theorists think of this as somehow maybe slightly more exotic than this, but of course. More exotic than this. But of course, you know, over here, the exact coding graph that you get depends on the choice of generating set. So if I picked a horrible generating set, I'd have actually got a graph with sort of pretty bad local structure. It would have been quasi-isometric to the original one, but probably not in any way that I'd be able to do anything useful. On the other hand, arguably it's just as natural to pick a probability distribution on your group, and in fact, these things are much more computationally tractable. Computation tractable. If you want me to actually do a random walk of length 100, I can write a very short computer program that just does that. It just picks group elements independently according to mu and multiplies them all together. But if you give me a complicated, even in this group here, if you give me a complicated generating set and then ask me to describe exactly the elements of length 100, that's a very painful sort of thing to have to do. So in some ways, this is actually, in many ways, this is actually more computationally practical than maybe the more common. Maybe the more commonly studied setup. So, and in fact, in this example here, I'm going to give you a very specific one to think about. So, I've got six generators. Pick each generator sort of independently with equal probability one over six. And if you do that, you exactly get the nearest neighbor random walking this graph. So, start at the origin at time zero, and then at each subsequent time, just go to one of the nearest neighbors in the graph. Time just go to one of the nearest neighbors in the graph, picking one of those edges with equal probability. So, this specific probability distribution u exactly gives you the nearest neighbor random walk on this graph. And so, and then you can ask this same question. I mean, you could ask, well, what do generic elements of the group look like according to random walks? And in fact, you can ask here, what's the probability that your random walk is back at height zero? And so maybe I'll just say, And so, maybe I'll just say a little bit about the underlying formalism of random walks. Because at some point, if I take finitely long random walks, somehow everyone's always very happy with that. But if I start to take infinitely long random walks, sometimes people can use funny looks. So, yeah, so just a little bit of formalism. So there's a step space. And so this is the collection of steps you take in the random wall. So what am I doing? I'm picking elements from G already from S for my generating segment. And I'm picking them independently with equal probability. So I can think of that sequence of steps as just my group with this probability distribution on it. I'm just taking a cantible product of. I'm just taking a countable product of those probability spaces. And so, a product of probability spaces is again a probability space. So, this is a perfectly nice probability space. And what does a typical element of that look like? It just looks like whatever sequence of group elements you get from this process. Pick elements of the generating set one after the other, pick them independently with that given probability distribution. But if I give you the steps that you're taking, you can reconstruct the path. You can reconstruct the path, the full walk that you're taking. About these steps, or where you get? These are the steps. They're not what you're actually. But if I tell you, so each of those G's is in S, yes. Just to confuse everyone with a different notation, yes. But of course, if I tell you the steps, then you can reconstruct the way it should be. So that's the part space. And somehow, sparkly lightly, the And somehow, sparkly knowingly, as a set, the path space is the same, it just has a different probability distribution on it, which I don't have a sensible name for, so I'm just going to call P for the moment. And so, what does that look like? That looks like where you end up doing the random walk. So, where are your time n? We're just at the product of the first n steps. And so then every time you take the next step, you just add on that by multiplying on the right by the next element. Right, right, the next element. And so, then, what is this measure on this space over here? It's just the push forward of the product coming down. So, that's the sort of underlying probabilistic machinery that makes this work. And so, then you can ask a question. So, let's ask the same question we just asked over there. So, what's the probability? Okay, so I made it mean. So, here I'm running WN. Now, this is the random warped length thing. Over there, it was the random element from the board of tip. Elegant from the board of the K graph of radius n, here it's actually the random walk. And so, what's that probability of this is at height zero? Well, actually, in some ways, random walks are much nicer than caddy graphs when it comes to, say, group homomorphisms. So, the nice thing about a random walk is that it pushes forward to another random walk under the group homorphism. So, if I look at this map from F2 cross C, there's just the projection onto. Cross Z, there's just the projection onto the Z factor, which is homomorphism. And so, if you're doing a random walk over here, you're also doing another random walk over here. Just look at the image of the generating set. Some generators go through identity, that's fine. So, we're not quite doing the nearest neighbor random walk on the standard Kenny graph here, but we are doing a symmetric random walk on Z. So, this random walk over here pushes forward to a symmetric random walk. So, we get a symmetric random walk. On Z. And so it's a well-known fact that for a symmetric random walk on Z, the probability that you're back at height 0 is going to, that you're back at the origin in Z tends to 0 light 1 over n. So this probability that you're back at height 0, okay, is like 1 over square root of n. And hopefully many people have seen some sort of result like that before. It's actually a totally explicit computation involving. Involving binary coefficients, I'm not even sure exactly who to attribute it to, but it definitely goes back to at least Pollya, who was one of the people who first started thinking about maybe random walk on Z, but I'll like on the higher dimensional analogs of Z. And so, and notice that it's just different here. Here we have positive probability, over there you have probability going to zero. So you're really sort of seeing sort of different behavior here depending on which model you pick. And I don't know, it's one better than the other. Well, they're just different, and they give you slightly different answers here. You slightly different answers here. And so that's just something to bear in mind when you're trying to choose things at random. You might think that, oh, well, all of these choices are going to be, you know, it's all going to be the same in the end, but actually, it might not be. That exact choice of model you pick might actually influence the sort of behavior of your generic elements. Any questions about anything I said right here? Do you give a moral reason for the difference? Beyond saying, oh, it's commutative, so things cancel. Is there like a deep reason? Is there like a deep reason? There's a deep reason. I'm not sure. Maybe not beyond products are special or something here. Yeah. Yeah, actually I don't know any sort of super deep reason why this happens. This just happens to be an example where it you get slightly different answers. Yeah. So I think the short answer is no. So, I think the short answer is no. Or rather, I should say, not that I know. Maybe that's the mega. Okay, so well, in both cases, what we actually get with this setup is we get a sequence of probability measures. So, in the first example, balls of radius n in the curly graph, as n gets bigger and bigger, we get a sequence of uniform distributions on those balls that are getting bigger and bigger. So, at time n, so So at time n, so actually the end, so at time n, where are you? The set of locations at time n is actually the n-fold convolution product of the meet. So there's a sequence of convolution measures that are telling you where the random walk is at time n. And so if you're mean, you can't help but ask your lack, so what happens when you take the limit of these probability measures? So you should get some sort of limiting measure on some hopefully useful object. So let's think about the limiting. So I didn't give names for any of these things. So there's Telegraph measure at time n, and there's random warp measure at time n. And so let's say they limit to something. Let's call that mu gamma. Let's call that mu. And so the thing is, these things aren't going to limit to anything useful on just the original kd graph. What you want to do is take some sort of compactification of the kd graph so that you have a sequence of probability spaces on compact space. And then you'll just be at a weak star limit for free. So what we'd actually like to do is look at these limits on this. Limits on this original space union some sort of boundary. So if your group was Gromov hypholic, the natural boundary you could pick would be the Gromov boundary. But actually this space is not actually Gromov hypholic, it's a product. And so maybe pick the visual boundary. So if you actually think about the KD complex, if you throw in squares for the relations, you actually get a genuine cat zero space, which is a ray of nice metric space. And you can look at GDs in that space. And so you could look at, say, You could look at, say, limiting measures, so look at them on, let's call it gamma union, gamma bar, where this is, say, the visual boundary. Any breath measuring would just mean the uniform measure of the ball of free East N. Yes. And so that gives you a sequence of measures as you change N, and so just take the limit as N has some value. Actually, offhand, does anyone see what happens when you do this? Okay, so if we think about the visual boundary, so what is the visual boundary going to look like? So let's take some path in F2 and then look at the sort of z factor above that. That's going to give you basically a copy of R2 path. Let's even call it Z2. So this is in the F2. Z2. So this is in the F direction and this is in the Z direction. So when you're looking at the visual boundary, you're heading off at some angle based on some baseball. So if you're heading off at a positive angle, you have to be growing linearly. So whatever angle you pick, as you head off in the F2 direction, you're actually going to be going, you're going to be growing linearly. But of course, the probability that you see anything at linear height tends to zero in both of these models. So in fact, the limiting measures here are both supported just on the limiting copy of F2 in the visual bandwidth. Limiting copy of F2 in the visual boundary. So the limiting measures are actually supported at a height zero. And in fact, in this case, yeah, they turn out to be exactly the same because of strange symmetries of F2. So in this sort of, so this is a totally non-typical case. It's a very simple case, but it's totally atypical for this reason. When you look at both random warp measure and Kdigraph measure on F2, they end up being the same thing. Here, actually, you don't ever see any information away from height 2, and so both of these limit to the same thing. So you have this odd situation where, when you look at the measures in the interior, you're seeing When you look at the measures in the interior, you're seeing some slight actual difference that you can name. But then, when you take the limit, both things end up being the same on the boundary. So, both of these things end up being the same, and they're supported at height zero. And so, maybe that wasn't what you were hoping to happen in this setup. So, let me give you a good example. Maybe some people will like more or less. Like more or less. So let's look at Space Group. Someone in the audience must like surface groups. So let's look at this setup here. So, in fact, here, let's look at the fundamental group of the closed surface. And in fact, I'm going to say closed hyperbolic surface. So, my favorite example is the genus II surface, but what I'm about to say works for any higher genus surface. And so, if you pick some If you pick some sort of hyperbolic structure on here, so then the universal cover is genuinely going to be a copy of, it's genuinely going to be isometric to hyperbotic space. So that looks like a disk, the hyperbotic plane, and so there's going to be some actual polygon I could draw if I was more artistically skilled that actually ties hyperbotic space. Hyperbolic space. And so then, this is an example of a Gromov hyperbolic group, and it has a Gromov boundary. So here, or in fact, which is here is the same as the visual boundary. So the boundary of this thing here is just a sphere of infinity. Sorry, the circle of infinity. So this is the boundary of H2, which is a topological S1. And so then you can run these two. Choices of probability measures that I described earlier, you can do Kevin graph measure and you can do random walk measure, and in both cases, you'll get limiting measures on the boundary. So everything I said earlier sort of goes through in this setting. You could run the same sort of things. You can look at the elements in the border of radius n in the Curley graph, take the weak limit of that as n tends to infinity, or you could do random walks. And so then both of those things will, in the limit, And so then both of those things will in the limit converge to some measure supported on the boundary of your group. And so here's a question. So let's give these things names. So in fact, Kaydigraph measure in this case here is basically Lebesgue measure. So if you do Keygraph measure here, you'll get Lebesgue measure out on S1. And so the question is, is Lebesgue measure equal to the measure arising from a finitely supported random warp on? The fundamental group of a compact surface. And in fact, this is a long-standing open question. So everyone expects the answer here to be no, but no one knows how to do it in general. So in general, the answer is no, we don't know. So in general, this is open. But some special cases are known. And so Kosenko and Teotihuacan. So uh Kosenko and Tiotso Kosenko and um sorry I have to just quickly look Carreso so um various partial results are known for example if you pick the surface corresponding to a regular n-got and you take basically the generated set corresponding Basically, the generating set corresponding to reflections in those sides or something. In some special cases, and there's some variations on that, you can show that these two measures are definitely what's called mutually singular. But in general, this is a long-standing open question. We expect them to be different, but there's no sort of general proof. And so maybe this is a good time to just say what do I mean by continuous or singular measures. So, in fact, let's just say So in fact let's just say uh the two measures new one and new two are mutually singular if there are sets A1 and A2, well, so that one set has full measure with respect to one measure and measure zero with respect to the other one. Measure zero with respect to the other one, or and vice versa. So such that mu1 of a1 is 1, mu 2 of a1 is 0, and the other way around. So mu1 of a2 is 0, mu2 of a2 number a2 is 1 is 1. So somehow each measure is sort of seen in a different collection of elements as generic. That's totally distinct. Generic. That's totally distinct from the other collection elements. So, yeah, and so in general, we expect these two things to be different. And I'll say, actually, I like this problem because this is one of the few cases where the compact case is actually harder than the co-compact case. So, if you pick a finite volume surface which has a cusp, somehow cusps have all sorts of special structure. And actually, I'm not going to say everyone who's proved results about that, but it's been known for some time that actually. That actually, in the cost case, the two measures are actually mutually singular. I guess the earliest case goes back to like Kinchin's work on continued fractions or looking at SLTC. And maybe I'll just say, so generically, actually, even if you just change the probability distribution mu generating the random wall, so for sort of, you know, just distinct measures, you expect the limiting measures on the boundary. The limiting measures on the brand reach also be reached as simple. So, even if you just change view to a different view, this should be the case as well. Okay, so um okay, so that was example two. You assume A1 and A2 to be oh yeah, yeah, that would help. If that would be, yes. I mean, it's not super important, but yeah, you might as well assume them to be destroyed. Yeah. Not destroying their intersection hesitations. Yes, in yeah, so without Yeah, so without loss of generality, you can throw away intersection. It's going to be measure zero. So yeah, so let's look at the third example. And so it turns out, yeah, this example is kind of hard. How you distinguish Lebesgue measure from random wolf measure on the On the surface of infinity to hyphotic space. So we started looking at, you know, what's a sort of next simpler example where the two things should really be different. And can we show that they're actually different in that case? And so the next example we looked at was this example of three manifolds of fibring over the circle, which is, we're going to call sort of the canon first an example. So this is example three. Joseph, before you go on, Joseph, before you go on to Can Thriston Maps, could you say a word just about the example of a three-manple, like a closed hyperbolic three-manple? And two measures? Oh, yeah, so right. So you can, okay, you can do this in any dimension, what I just said. So take a hyperbodic n-manifold and look at the hitting measure coming from random walks and try and compare that to a vague measure. And so for compared manifolds, that's open. As far as I know, in all other dimensions as well. Dimensions as well. However, again, if you have cusps, it does work out. So maybe I should say for if you had, yeah, if you have a hypotic n manifold, so n greater than or equal to 3, what's the correct non-uniform, let's write non-uniform mattress. So that basically means that it's a finite volume, but it definitely has at least one cost. Then they're distinct. So then the two measures are usually singular. The two measures are mutually singular. So let's say LeBay. Okay, so I'm going to use this fancy symbol perpendicular for mutually singular. Lebesgue and random walk measure are mutually singular, and that's due to Randecker and Teyotso. So they did the higher-dimensional case. If you have cusps, they basically did a much fancier version of the cusp case here and got it to work in all dimensions. But again, if it In all dimensions. But again, in the Co-Compact case where you actually have finite quantum and node tasks, then as far as I know, the problem is still open in all dimensions. So yeah, so let's think about Canon first and that. So okay, so I'm going to pick a hyperbotic three-manifold, and it's going to be a very special sort of one because it's going to depend on. Special sort of one because it's going to depend on a surface map. So, what I'm going to do is I'm going to take surface cross I. So, here's surface cross I, and what I'm going to do is I'm going to put an equivalent to ratio on this. I'm going to glue the bottom to the top by some sort of gluing that. So, here phi is a homomorphism from the surface to itself, and I'm going to identify x0 with phi of x1. X1. And so here is a surface, a state of orientation-preserving surface oriented. And in fact, if you only care about the resulting three-manifold, it's natural to think of these things as maps.isotopy. And then, of course, you're already just picking an element of the mapping cluster. So this is something in the mapping cluster. And it's a theorem of first end that says m phi is hyperbolic. So this resulting manifold that you get. So, this resulting manifold that you get is hyperbolic if and only if phi is what's called pseudonosom. So pseudonosum maps have lots of special properties, which I won't tell you about this minute. Maybe I'll say a bit more about them later on. But there's some concrete criteria. If you give me a reasonably generic helmet-class group, then the resulting free manifold we construct from here is actually. Here is actually a hyperbolic manifold. And so, by the way, when I say hyperbolic, I mean genuinely hyperbolic in this sense here. It's not just raw and hyperbolic, it's actually a quotient of hyperbolic space. So, the universal cover of my manifold is actually isometric to three-dimensional hyperbolic space. So, there's H3, and so the boundary of H3 is the sort of Is the sort of two-sphere at infinity. And again, there's some concrete fundamental plane you can pick, which is a finite-sided polyhedron which tiles all of H3. So that's sort of one description of the universal cover of my manifold. So here's another description. So I should be able to build the universal cover of this thing here by taking the universal cover of the surface across R. So if I unwrap, so here's another description of my universal cover. Of my universal cover. So it's going to be the universal cover of the surface. So what does that look like? So my surface, it's a Genus2 surface or higher, so that's a hyperbolic surface. So it's going to be a copy of H2. So this is going to look like H2 cross, what if I unwrap everything in the vertical direction, it's going to look like H2 cross R. So here's a copy of H2. And so now I've got this sort of this vertical product coming through here. And so of course, this And so, of course, this must be completely wrong because hyperbolic space is not a product. So, what's the resolution of this apparent paradox? And apparently, when Thurson was first thinking about this, when he first started to think about hyperbolic manifolds, he was like, well, I'll think about these ones here because clearly they can't be hyperbolic because of this picture here. So, the resolution, of course, is that this is a topological product. It's not a metric product. So, this is a topological product. So it's not a metric product. Because after all, I mean, hyperbolic space itself is just a board, it's a product, it's R3. It's just got a non-product metric on it. So in fact, it turns out this picture is very nice to draw, but it's fundamentally misleading. Because this copy of this surface here is embedded in here in some way, but it's not embedded in any. Way. But it's not embedded in any nice way. As you move out towards the boundary, it gets sort of more and more folded up until eventually the limiting, the image of this limiting curve here is actually a sphere-filling curve on ST. And so then that's the result of Canon-Furston, which is why I'm calling these Canon-Furston examples. So there's an inclusion map here from the universal cover of the surface into this space here, which is the same as that space over there. And so what Canon the first showed was that that inclusion. Caden the first showed was that that inclusion map extends continuously to the boundary. So let's write this down. And so it extends to a continuous map from the circle to the sphere, which is actually onto. So this is onto, it's a sphere floating curve. And so now this gives us actually four possible measures on S2. So, what are the four possible measures we could pick on S2? So, we could pick the vague measure on S2. That's a perfectly nice measure to pick on S2. What you could do is you could take a random walk on the three-manifold group. So, pick a probability distribution of finite. Take a probability distribution, a finitely supported probability distribution mu on this thing here, and just do the random walk, it will converge to the boundary and give you a hitting measure on the boundary. So that's another measure you can get over here. So those are two sorts of measures on S2, and I'm going to call these the sort of three-dimensional measures. They're coming from the three-dimensional geometry. But what I can also do is I can take the two-dimensional measures. So I have the vague measure on S1, I have Lebesgue measure on this circle right here, and I Right here, and I also have the hitting measure. I also have the hitting measure arising from a random walk on the surface group. So if you take a random walk on the surface group, it's going to converge to the boundary over here in this circle model. And in each case, you can push them across using the Canon-Thurston map to give you measures on S2. So these start off being measures on S1, but then you push them across. S1, but then you push them across, and they end up giving you measures on S2. And so the result of the various authors, it's alphabetical order here, is that the two-dimensional measures are mutually singular to the three-dimensional measures. But I'll just say what we can't do is we can't distinguish measures in Do is we can't distinguish measures in here because that would be great. We would love to have done that, but in fact, we didn't do that. So, um, so yeah. And so I'll just say, why did we look at this example in particular is because there's no particular obvious way of distinguishing these collections of measures by just looking at the support of the measures in the subgroup. They all sort of have full support agent. So if you pick any other surface subgroup of a treatment hole, all surface subgroups are either quasi-focusian or virtually fibrous. Quasi-foxian or virtually finite. So they're all going to fall into either this category here or they're quasi-foxian. So the Linux set is actually going to have strictly, it's going to be sort of a quasi-circle sitting inside S2. So it's easy to distinguish because it doesn't have full support in S2. So we can show these two things different and in they tell you about how we actually do that. Yeah, so I'll just say in just a couple of minutes, I'll just give a so what sort of tools do we use? So one piece of So it turns out to be convenient to look at random gdzigs. We've got these measures on the boundary. Just pick two points. If you pick two points according to your question. Just a quick question. So there's also the cusp case for bundles. Do you also have a result there? No, we didn't think about that because it basically follows from the other work on cast cases. So, in the cost case, you can actually distinguish both of these, and then if you actually look at the way in which they're distinguished, that's going to distinguish them all from each other. All four are. Yeah, so all four are going to be different. So, this actually follows from, essentially, if you look at what Randeker and Tiotze did in the three-dimensional case, that's going to distinguish that from the two-dimensional case. So, we haven't thought about it, but I claim it follows from the work of Brian Decker and Teot. So pick two points and then just connect them by a GDZIC. And so in fact, we're going to look at properties of these sort of random GDZICs and try and distinguish those. And so the basic intuition is that if you pick a GDZIC over here, then these two points here typically get mapped to different points in the Cameron-Fursal map. And so you can look at the corresponding three-dimensional. And so you could look at the corresponding three-dimensional GEsic determined by this thing here. And so, in fact, what should it look like in this picture here? Well, it should look roughly like something which projects down to that, just maybe sort of at different heights as you go along. And so there's various ways of making them precise. And so one tool we use is, well, actually, we don't use, we use an analog of a result of Mullen. So Kerr proves something like this using the Prove something like this using the actual metric coming from the flat metric. For technical reasons, we all use the GDC flow, so we need to get things work in the hyphawic metric. So, anyway, whatever, we do some analog of that. And then the other piece of heavy machinery we use is actually due to open pan, and they study the GDs flow with components. But basically, what we showed is that in the two-dimensional case, Showed is that in the two-dimensional case, your GDZIC has to spend a definite proportion of time close to this base fiber. And in the three-dimensional case, as you move along the random GDZIC, the probability that you're close to the base fiber actually tends to zero. So that's the geometric way of distinguishing these two measures, and that's some of the machinery that goes into showing that. Again, this is probably a good place to start. Questions? Yeah, questions? So just be greater than GMD6, so that means you're like, are taking points of the GMD6 with respect to one of your measures there? Yeah, so just make, just choose two points independently from the measure. With respect to, like, say, the level 8 measure, I'll choose two. That's a random GM6 there, and then we consider. Yeah. So you could pick a random GDSIC here and try and compare it, let's say, a random GDSIC from one of the. Compare it, let's say, a random GVs from one of these things here. And the aim is to show that they have sort of different geometric colours. What happens if we ask the same questions directly on the Kelly graphs for these groups? Right, so because we're in the co-compact case, the Kayleigh graph is actually quasi-isymmetric to H3. So that Cayley graph measure is going to give you the base measure that we get. So in this specific case, it's fairly straightforward. Actually, in the custom case, it's Actually, in the cusp case, it's not so straightforward. So, in other cases, it's harder. Right, so yeah, particularly once you leave the family of hypotic bridges, then it sort of becomes, starts become very difficult to actually understand the geometry of trade graphs. Unless there's some special property, like, you know, if they're cat zero or something, then they fall into some other class of machinery. But up to groups, just a sort of difficult answer. Yeah. But here you end up. But here you basically end up with a vague measure from the K graph things. But we haven't actually done the fine analysis for that. So we never actually looked at the exact border of radius n, but it's roughly the border of radius r for some r in terms of, if you think about these genesic flow results of counting battle points in hn or h2, you can use all of those. We haven't actually done that, but it all fits in with action. I have a very broad question, which is maybe That's a very broad question, which is maybe stupid. So, you're looking at these different measures of choices of things. Did you find a difference between what's useful for proving theorems and what's useful for actually sampling on a machine? So, this makes sense. Yeah, it does. And actually, so in fact, as far as I know, the only one that's really computationally efficient here is doing a random warp coming from a finite Random wall coming from a finite probability distribution. That's the only one. Because even, you know, even trying, I mean, there's this whole field of machinery, like snapping and stuff, actually trying to do computations in HN1, in H2 or H3. So even like, you know, Nathan was talking about problems of like actually trying to compute the derivative domain on the nose. So if you really want to do very accurate calculations here, and say the border of radius n in the K graph, it starts to become a bit awkward to work out exactly what's going on if you have to end up doing something. Exactly what's going on, if you have to end up doing something like looking at tilings and doing shape and X. So, as far as I know, the only one that's actually computationally super efficient is taking. If you want to take a random walk of length any of the given view, that's actually super quick. I think that's the only one super quick. Okay, there's no questions from the ether. In which case, um thank you again, Jason. Well, us too. 