Lucas Pio Vidian who will tell us about the role of stochasticity today. Well, thank you very much for the invitation. And yeah, I try to, I mean, some results, it's going to be very specific results, so it's not going to be a general talk about the role of noise in stochastic learning algorithm in general. But still, I try to convey that some. Still, I try to convey that some specific things happen in a very specific model that is non-convex. And I'm going to try to convince you that stochasticity has a real role in the selection of a precise minimizer of the empirical loss in learning. Okay, and once again, if I'm too fast, don't be scared to interrupt me. I mean, it's the more we have interaction, the better it is, I guess. I guess. Okay, so let's just introduce my setup. So I have a fixed number of observations. I'm not going to talk at all about generalization. So really n is fixed during all my talk. I have a prediction function parameterized by some parameter w. Okay, and I want to minimize the empirical squares. That is here. An example of parameters from. Um an example of parameters laminate, but I'm gonna be very fast on these are linear models, for example, neural network, and we will consider some intermediate uh easy networks uh that show already show some non-convex features of the learning dynamics. And an important thing is that we we are going to be in another parameterized setting. And what does it mean for me? It means that there exist many, many parameters that interpolate totally my data set. Okay, so for which we have the For which we have the global optimum that is zero of my empirical risk. Okay? So this set that is a manifold of big dimension, d minus n, d minus n approximately. So is a huge manifold and my aim is to understand to which minimizer my precise algorithm will converge. Okay, so I'm going to talk about the role of stochastic. Talk about the role of stochasticity. So, I'm going to try to emphasize the difference between GD and LGD. So, I just briefly recall what those are, but I guess this is okay for the audience. And an overall message for me is that in the context case, for example, for linear parameterizations, often the aim is to show that HGD is as good as GD can be in terms of convergence rates, etc. So, we can see HGD as a particular So we can see HGD as a perturbation of the LGD trajectory as a perturbation of the GD trajectory. Even though there are some differences in some cases, this is the overall picture in the linear and hence convex case. Whereas in the non-convex case, I try to convey the idea that it's super different. Because of non-convexity of the trajectory, then we have this picture in mind. We have this picture in mind. I mean, this line is a manifold of interpolators that are the manifold of zero losses parameters. And because of non-convexity, the trajectory of GD and HGD can be very different. And the aim will be to characterize which is the difference in some specific settings. Okay? And it is, I mean, it has been shown empirically that the noise as a That the noise has a real impact on generalization. Alright? So I'm not an expert in this empirical literature, but still you have many papers, even today, that try to show that stochasticity has a real role on generalization. And here are three old papers that try to convince us that really mini batches optimization is is important. Optimization is important. Okay, so I talk a lot about the selection of a global minimizer. So here is my slide. Is there a consensus on other people who I think? Yeah, yeah. This is why I I said that o I was not an expert. I will show that in my setting it's it's it's true. I mean there are some I mean there are different I prefer to say the the implicit biases are different. The the implicit biases are different than better than good or better. I I dislike these words uh because as a matter of fact, you select something and if you want a minimum L two implicit bias, then it's good for minimum L2 problem. And if you want a minimum L1, then it's better for this. So better is a bad word for implicit biases. I prefer a characterization. Okay? So I will show that the characterizations are different. And I don't want to debate whether it's good or bad. Or bias. Okay, so this is really my motivation. The training algorithm, GD and AGD, I will focus on both, has an implicit bias and stochastic plays a role in this selection. Okay, to be a little bit more precise, so I have the square loss once again. Let's say that my descent algorithm, for example, Gd or L G D converges. Okay, I will denote by W infinity Alex the limit. W infinity R is the limit of this of the trajectory. And when you look at the loss, I mean, this gives you an information. You have reached zero loss. But as I said, this is a super huge set. So you need a finer characterization to distinguish between GD and HGD, assuming that I have converged to a global optimum. And in the talk, I mean, there can be many characterizations, but in the talk, I will focus on On what is called a minimum norm solution. So let's say that R is a norm. It says that W infinity alg is among all the global minimizer minimizes intrinsically some norm R that we have to find depending on the algorithm. And it can be for example the L2 L1 norm and I'm basically going to talk about L2 and L1. So for S, R is going to be such a C. Such a C. Okay, and it's in contrast with explicit regularization. I will not I mean the the descent method will be on uh on really the empirical risk without any regularization. Okay, the first example because we we always begin with linear predictors and the convex world for linear predictors okay so it's w scalar x um you can write exactly the You can write exactly the iterates of Gd and LGD, and there is a nice geometric property that is super simple, but it's super well known since decades, is that the iterates of Gd and LGD, they belong to an affine space W0 plus the span of the data set. Okay, so they go straight in this space. Okay, this is our presentation of this. Okay, so the orthogonal of the spine. So, the alternative span is a huge face, but it never sees it. Okay, once you have initialized and it goes straight. And thanks to Pitagora theorem, it's super easy to show that, in fact, among all the minimizers that is this affine space, it chooses the projection of the initialization over this space. So, it's really what we call a minimum L2 norm, and it's super well known. Okay? And in this case, in this convex case, you see that there is no differences in the selection of the implicit bias by the, whether it is G D or A G D. And it's going to be the same for momentum gradient descents because of this geometric property. So if we want to distinguish between uh the implicit basis, if we want really to understand the role of stochasticity, then The role of stochasticity, then we may want to go like in the non-convex world. Okay, and this is what I'm going to present now. So, if you have questions so far, I mean, I'm sure that everybody knows it's a question. For the SGD, what does it mean for infinite n? Are you sampling with replacement? Yeah, I'm sampling with yeah, I fix n, I fix the number of input uh out input outputs, and I mean something with replacement. I have until I have until ten uh eleven eleven ten eleven. Or in another time. Yeah. Okay, cool. Thank you. Okay. So the setting I'm going to consider is really the first non-convex setting you can think of if you want to complexify a bit the dynamic. Okay, it's not a general two-layered network with general activation or coloured. It's really you rewrite. You rewrite your linear predictor as a product. Okay? So, in terms of predicting power, it's not better. The set of prediction functions is exactly the same, but you rewrite it as a dot product so that here the function in terms of this u and v is no longer convex. Okay? It has created some non-convexity. It has created like a. It has created a lot of symmetries. So the prediction parameter, the parameter is in 2D now and it's non-convex. And this setting has received a lot of attention, I guess. Almost all the people are in this space. So many people know what this setting is. For me, it's the first setting where we can prove things, characterize things properly, and still is non-convex, so non-trivial in terms of optimization. Okay, so as I said, the final model is the same, but the training has changed. And we're going to witness the key features of what is non-context optimization. And a striking example of this is the role that will play the initialization. Okay? And the stochastic will play a role, but I will first recall the known result for gradient descent. Because when I want to understand Because when I want to understand the rule of stochasticity, I first always need to understand the trajectory of the deterministic counterpart of HGT that is in HGTV. Okay? So, what I'm going to try to show is like first we this is really how the project starts with the plot. We know from previous results, I'm going to talk about this like in two minutes, that this rotor analyzation of the linear This rotor harametrization of the linear predictor will bias our trajectory towards sparse predictors. Okay, so this is why we reproduced our data with the sparse predictor. Okay, I guess it was like it has only four non-zero components. And then we ran for a certain initialization Gd and A G D. And in this plot, you see that Gd is That GD is pretty, I mean, it does a pretty fair job into recovering the sparse controls. Okay? So I'm here plotting the difference between the trajectory, the linear predictor corresponding to this multiplicative parameterization to the ground truth, the sparse one truth. So GD does a pretty good job already. But then if you compute the same thing for AGD, then it goes like two orders of magnitude. Then it goes like two of the magnitudes below. So there is a real effect. I mean, it's not a small factor to. Wouldn't the two lines sort of converge if you decrease the initialization? Yeah, so this really depends. This is a good question. This really depends on initialization. The more the initialization is big, the less the the gap will be big. I I'm gonna show this afterwards, but yeah, just to for this plot, for the motivating plot, We're seeing here the distance from the minimum L0 norm solution. Sorry? That's what we're seeing? Yeah, yeah, yeah, yeah, yeah. This is the minimum L0 solution. Is it like in the recoverable setting, like RF settings like this thing? Yes, it's the same as you really produce, I I don't know, uh hundred it was hundred uh It was 100 input-output samples with this sparse detector. And you want to. A Gaussian data? Yeah, a Gaussian data. The statistical setting is not really the one I'm interested in. I'm interested in optimization. But there is only one. There is only one such. Yes, there is only one such, yeah. And in fact, it will coincide with the minimum L1 norm so they share this. Yeah, yeah, and the minimum L1 norm in this case is exactly the minimum L0 norm because Exactly the minimum as you will know because of. Yeah, they take Gaussian, yeah. So, in particular, like someone pointed out, you can probably show that as you take the initialization to zero, the growth will coincide. I'm gonna recall it because it's important even to state the difference with this. We have to clarify it. I will recall it in one second. So, really, I want you to imagine this plot. I want you to imagine this plot. So GD goes towards something, the sparsest is here, LGD is closer, and really what we want to understand is: yes, really this gap. Okay? So, and I'm going to say exactly what Shri said, meaning, okay, let's consider the gradient descent. And I'm sorry for people that don't like gradient flows, but I'm only gonna consider no way. I'm I'm only con going to consider um continuous time uh um continuous time uh dynamics. So maybe it can be proven but I'm sure it's more difficult to see how the discretized dynamics uh follows this uh continuous dynamics. But even in the stochastic case, we'll see that uh I'm gonna cover only the continuous dynamics. But how do you specify gradient flow for stochastic Flow for stochastic. Yeah, yeah. I'm going to try to convince you that it can be good models. But you will not convert this SGG to Langevin. No, no, no. Okay. In the previous slide, were those plots of GD and SGG with a fixed step size? Fixed step size. Very, very small step size, if possible. It's a very, very small step size, but it's fixed. It's a constant step size. We'll see in two seconds that despite the stochasticity. Second, that despite stochastic updates, there is an intrinsic property of the stochastic updates of LGD that makes the noise vanish so that you will converge to a global autonomy. No, but it's important to understand that it's not a you don't sample a gift flow and it's not long at any of the okay, so I recall the result of this nice paper. I'm going to consider really the simple setting where it's not even multiplicative, it's only the square. Just to simplify notations, because we have a, I mean, the norms are going to be explicit, etc. Let's say that you initialize according to a sum scale alpha. Let's say it's going to play a super important role in the next results. We run gradient flow on this. Flow on this empirical risk. And then, what this article shows is that the implicit bias, of course, depends on the scale of the initialization. But to be more precise, if the initialization is infinite, then you're biased towards the minimum L2 norm. And if it goes to zero, towards a minimum L1 norm. And for the L2 norm result, I mean, it can be remembered. So L2 norm results can be reminiscent of what happens in the NTK regime, where really you have an intrinsic Canon method that is really the implicit bias of the Democrats. And surprisingly, yes, when the economic is very small, then it's going to be biased towards a minimum L1 norm. That is good. I mean, not good, but that is some strange choice at the end of the day, but let's recall. Let's recall that it is very bad for optimization time. Okay, because zero is a sudden point, so you escape very slowly from zero at this point. So, in terms of convergence, it's kind of a nice effect, but in terms of convergence rate, you pay the fact that you initialize it on a saddle. Can you just say again what's going on with the W squares? So, are you forcing all of your weights to be in the middle of the square? All of your weights to be able to do that. Yeah, to be positive in this case. The analysis is not for this model in this paper, but I prefer to stick with it because every expression is going to be simpler. Simplest. Simple. But again, I'm trying to understand, like, are you implicitly using a positive or explicitly using a positivity constraint then? Kind of, yeah, yeah, yeah. In this case, yes, but in this paper, it's not treated as a square. You have a really w1 squared minus w2 squared, so it captures both. Minus W squared, so it captures both. Oh, I see. Okay, so I think that's a more general square. But it doesn't change any insight. Kind of the same analysis. Yeah. This is just for the result of my slides that I prefer this model, but it can be, I mean, it doesn't change any of the insight. I mean, you reparameterize by U square, so it's the L1 norm that is. The difference between the sense was four or so. It's exactly zero, yes. So the thing is, this is a preference to Europe. Yeah, yeah, yeah, it's really uh yeah, it's not my result. Just yeah, to show the role of stochasticity, I need first to recall you what is uh what is the result for the good info. Sorry, we've got one more question. Is there anything special about the initialization point or will uniform on the sphere of the rhythm radius? I guess that there is no there is nothing special. Just the once again this potential is uh it's more complicated to write. In this case, Complicated to act. In this case, it's super simple, it's really the entropy. So between the stochasticity? So here comes, I'm sorry it was a bit long before stating the difference, but I needed to go through this. So I'm not going to mention the architecture. We are going to consider the stochastic value inflow. I'm going to talk about what is the stochastic value inflow in my case. Initialize at exactly the same point, have the same step sizes. Have the same step sizes when I compare empirically both trajectory. And I ask the question: okay, is the implicit regularization problem modified or is it the same as in the context case? Okay, and those are empirical results. So here is a training loss. And you see that, okay, when I plot LG D one, LG D two, it's because we have sampled uh different uh data points. different uh data points so that even if the law is the same uh okay i'm not talking about the law so so that the the trajectory is different because we have sampled different uh inputs okay and um here you see that the the training loss goes to zero and it's 10 to minus 15 and it's it's eventually going to zero here so despite the fact that it's stochastic it goes the global uh optimum Goes to the global optimum zero. It goes to the global optimum, sorry. And the stochasticity, you see that it saturates here and here. You see that it's always below the so the difference with the sparsest predator is always lower when there is stochasticity. And the global optimum retrieved by the stochastic redescent depends on Depends on the choice of the iterates you've chosen through the process. Are these different uh samples or different random selections? Same samples, but different choices. Different uniform change uh choice. Yeah, yeah. And it's like two where different random what if you run SGD more than two times, for example a thousand times, does it always have to be below GD? But what do you mean? So so here on it has converged. So I run uh as many iterators this. Yeah, but i if you sample the S G D path say a thousand times. So here in the plot you do it twice. Ah but you're gonna have uh a lot of uh does it always go below the G D? Yeah, yeah, it's always I'm gonna prove it theoretically and it's uh it's always below. Sorry, I briefly what is the difference between the yellow and blue line? It's the fact that you It's the fact that you've selected data points at each iteration that are different between blue and yellow. Oh, I suppose I just do blue instantiation. Yeah, for example, X1, X2, X3, and the other one is X3, X2, X1, 11, X3. Yeah, this is really why you see the variance between that. Okay. So uh um the the the first thing I need to do uh is to model with a continuous flow. To model with a continuous flow. So, this is really the LGD iterate writes like this with IT uniform over the data set. And the way I'm going to rewrite it is that I'm going to force a full batch gradient into this expression and I'm going to add and subtract it. Like this, I'm going to see the difference with the gradient descent. Okay, so you have this gradient, and then you have this term, work psi, I did. And then you have this term, work psi, I didn't write it because it's a pretty heavy expression, but it's really, I mean, the random factor coming from the stochasticity. All right? And this is how I will model my stochastic flow. So one thing that is important and I'll try to motivate is that this noise is not a pure Brownian motion. A lot of people go with a pure Brunian motion here. Go with the pure Boolean motion here. And this noise has a very specific geometry. And the first thing to observe is that this noise scales as the loss itself. Okay, so when you've reached the global optimum, zero loss, then your noise vanishes. So you need to have a term proportional to this. And this is really due to the fact that we are in an over-parameterized setting where there are global optimum. Zero loss. Yeah, this is sorry, this is a Brownian motion. Yeah. And how can you justify this model for the covariance of the noise? It's because really if you do if you discretize this SDE, then you will exactly match in terms of flow the trajectory of the SGD. Okay? It is because the noise covariance, so the covariance of this noise. Covariance, so the covariance of this noise exactly matches the covariance here. Okay? And the second property that is super important, and we've seen in the context case that the geometry of the iterates was important, it follows lines, is that this belongs to a very specific geometric set. And here it's really W times the span of the data set. You really see this here. I mean, it's a very specific direction. Very specific direction. It's a very specific sub-manifold that the noise is into. So we are in dimension D, and this manifold is of dimension N. So the noise has a very specific structure in LGD. No, you're good. Did you drive a general model for the SGD flow beyond this particular loss objective for over franchise? Yeah, I mean you can, but it's not yes, it will take more work to do. Yeah, it will and yeah, the response will be more heavy. But the fact is, I want to respect those two guidelines. The noise covariances have to match for this approximation to be coherent when the step size goes to zero. And I want to respect the geometry of the noise. This is important. I mean, if I ran uh other models, other SDE, then I would not have my result. So so of course the noise coherence match. So so of course the noise coherence match only for the next step. If you take multiple steps then the noise coherence starts to match. Yeah yeah yeah of course yeah it's a really uh it's a linear approximation at first order. It's an approximation at first order. Maybe a general question about using plastic gradient flow to SGD. So in the GD case, if the step size is small enough then you can bound their distance and you could actually do some stuff You could actually do some stuff with that and improve that they're not going to be too far away. So in the stochastic case, do we have anything like that? It kind of feels to me almost like you could have different models of stochastic case. You're totally right. And the only thing that really convinced me is this one, is the fact that when you plot the parameter in parameter space, you know you have all different S D's with different With different Bonian motions here, and you have HCD in yellow, and this seems to match. So I have my two guidelines, I plot, it's experimentally almost the same. I mean, personally, I'm convinced, but I don't have infinite time bounds. I have the same depth for LGT. I mean, it's usually it explodes as exponential in time. Even if I know that if you follow an almost convex trajectory, it's not going to. An almost convex trajectory, it's not going to be the worst case. Yeah, you don't need the worst case. But I think, yeah, I mean. But so if you were to run experimentally SDs with different covariances, maybe it would also go. No, I'm sorry, I don't have to experiment. I wanted to do this today. But yeah, you can see that if you forget the loss, it's going to be super bad because it's not going to converge. So that's important to use. If you forget the structure, W times X transpose. Times extra spose, this will uh totally be uh like uh out of control, like really. So, iterate analysis. Okay, it is not it is not at all the same thing. So, empirically, both of the aspects you are necessary. Yeah, I just have a comment about this question also, Rachel. So, I was using a sanity check and seeing what the covariance is patched in the sanity check I was using is that the gradient of the loss is X transpose X W minus Y. So, it's kind of, you know, So it's kind of linear in scale in xw minus y. It's linear in the norm of the residual. The square root of the loss is also just like the norm of the residual. So they have the same units. So I was using that to check the current dimension also that this is very finely tuned to the squared loss. And you can derive general models in terms of the gradient, of the model, etc. I mean of the formula. You have the scale that is the loss and the geometry that is w times x transpose and those two. U times extra square, and those two things are necessary for the results to hold. I have many questions. So, one is like, I'm still a little bit confused because, like, at least for square class, when you take step size to zero, like both the both the flows converge to gradient flows. Exactly. This is true. This is not gradient flow, so what is the gamma flow? There is gamma here. So, there is a step size that is. So there is a step size that is there. This approximation, i maybe this it will pit answer the the question of NADA, but this is a negative result. This approximation is not an order two for we know that variant um descent is a is an order one approximation of the flow. This uh does not yield an order two approximation of the stochastic inflow. So it's really uh the same order in terms of approximation at infinitesimal step size, but I believe this is. But I believe this is a good model at non-zero step size when you want to capture the stochasticity. Yeah, and one more question is like, okay, this is more like a comment. Like you plotted this, which is kind of very convincing. But can you also try to break this? Like when does it do you actually have a difference? Like what if you don't use like RIP matrices? What if I didn't use what? What if I didn't use what? Like the regression problem, the X's are not Gaussian. If you use something very different from Gaussian. Oh, it's the same. I mean, okay, here? There is no Gaussian hidden. No, no, no. Empirically when you plot it, that's a convincing plot to show that the SDD is doing SDD-like operations. But it's also interesting to empirically see when it will break it. In terms of the distribution of the X, the data? No, yeah, you can. That's one way to vary it. Yeah, but. Yeah, but I don't expect it to break because of this. Maybe, maybe I don't know. I've always performed with the Gaussian data, so you may be right. I don't know. No, I myself don't know, but I'm just trying to say that whenever we have an empirical observation, it's a good idea to try to understand it. Yes, to try to break it with. I try to break it with other models, but not with other distribution of other methods. Okay. Okay. So you may. So you may not be convinced, but I'm gonna. Oh, no, no, I think that is convincing. I was just saying that I did. Sorry for a trip. Is it theoretically, do you think it might be possible to have some kind of a global result and to even for convex objectives, tying SGD and SDE to have a bound between the SDE and SGD? Something like that. Better than GD, better than the gradient flow? To show that it matters just like we have for gradient flow, maybe something like that. In terms of step-size precision, no. My answer is no. It will be beneficial maybe for constants. I mean, the constant between the O of gamma will maybe there. If you understand correctly, we would know what is a better like a. We know what is a better like a someone order approximation of S G V which is given by some stochastic. It changes the drift term. Yeah, or let's say the three s three halves order approximation which does not need to change height or all that stuff. Yeah, so you know what it should be and here it gives a too complicated expression which you simplify some. So you know what all the differences. Yes, exactly. Yeah, we know how to go at the the other. One more question first. Alright, one more question. Something that just I realized during the question. So you have convinced me that the behavior of these things for convex models in general should not be so different, right? Like the basic suggestion. So if you run the same program for convex losses, do you get a stochastic flow where it's obvious that you are? Like this. If you model uh in the linear uh linearly parametraise model. Linear Lipper matrix model, you just forget this, and I have plotted my thesis because it was a work that I began two years ago that shows that the trajectory is almost the same. And it's a okay, so now I can I can state the result the result. So Result. So recall the gradient flow. The fact that we want to, I'm going to try to put emphasis on the regime where the initialization goes to zero and where the implicit bias is given by the L1 norm. So this is the theorem. So with high probability, the iterates convert toward the global optimum of the loss, even though the step-second gamma is constant in my model. And it satisfies. And it satisfies exactly the same implicit bias criterion, but with alpha infinity, that is an effective initialization, something we can call an effective initialization, because it is the initialization alpha times a negative exponential of a negative term. And this term is gamma, some diagonal effect of Dunterke-coherence matrix, and the integral of the loss. Integral of the loss. And the two things that I want to put emphasis on in this expression is that first, alpha integrity is always smaller than alpha, because this is an expansion of a negative term. The bigger the step size is, the bigger the effect is. The bigger the gamma, the bigger the effect is. And this is stochastic because the loss itself is stochastic. This is a stochastic. Is stochastic. This is a stochastic object. Okay? So this may be the reason why, if you believe in my model, that you observe, if you sample differently the input-output, you observe different implicit biases. And yeah, you can develop at first order in when alpha goes to zero to see that it's really a polynomial effect when alpha goes to zero. If we want a small L1 If we want a small L one penalization, then we would like uh this effect to be uh more sparse when alpha goes to zero. So it's the same R as in the gradient flow. The same R, but with the R was depending on the parameter, and the parameter is changed. And that R is the hyper entropy? Exactly. In in this uh in this um purely square uh term it's really the the entropy. The entropy. When is the model with u square minus v square? This is the tabolic entropy, but I wanted to simplify the result in my slide. What is zeta here? What is it? This is a sample. Just to show this polynomial. Alright. I have a unus three more slides so you can. Only three more slides, so you can. So maybe you can take the slides from the table. Okay. Okay. Then I ask myself a question: is that you have the integral of the loss, okay, as it as the loss converges to zero, fast enough the integral is finite. But what if I prevent the iterates uh the to converge to global optimum, so the loss will not converge to zero, this will be infinity, so that this will be exponential of minus infinity. This will be exponential of minus infinity, so that this would be zero, so that the implicit bias will exactly be the L1 norm in this limit. This is only an intuitive calculation, but still, it has some nice effect. I mean, you don't have to take alpha to zero because if you prevent the loss from converging while having this geometrical property, you'll have an effective L1 minimization effect. Okay? So imagine that we inject deliberately some label. Deliminately some label noise at each step. And we want to model this by usochetch bailing flow. This will prevent the loss to converge to zero. And if I believe that my result still holds, then it's going to be the case that it's going to be final. Exactly with this L1 norm. Without resorting to small initialization. That is bad for um for uh for uh speed of convergence. Okay, now Okay. The result assumes that the loss con oh no, the result states that the convergence is part of the result. Yeah, it's part of the reject noise such that the loss does not converge. Yeah, but just it was a I mean I do not just an illustration. Yeah, I do not tell you how I make this while preserving the geometry, but I uh I mean I will show you in the next slide that I will inject some noise. I I change the model just by injecting some noise deliberately in a good uh space. In a good space. This is not like. This kind of represents it because if you inject noise such a way that it doesn't converge, you can't converge to something which is where x beta equals y. Yeah, it's only a nucleative calculation. And we'll show the real theorem just after. Okay? For me, it was a nice effect because everybody says, okay, but you at the small initialization, your convergence time is super long. So you don't wan you want to to go out uh this uh this bad saddle point uh initialization. this bad saddle point uh association. Okay, so this is what I call the continuous time version of LG D plus label noise. At each iteration if you uh choose a sample and you add deliberately some noise to the label, then you can show, and similar with similar calculations and the continuous time model, that it's exactly the same model, but you've replaced the loss by the scale of the noise you've added. Okay? Effectively by the loss plus this this The loss plus this noise scale, but as this will be the dominant term. So if you buy my model, then I can derive exactly the same result. It was pretty surprising that it's not at all the same proof technique. But what we have proven, it's not an archive yet, but it's been accepted to cut, so I won't have any problems with this result. So um if you define the unique minimizer if you derive data such that there is a unique minimizer of the this um Lasso problem, there is a conic constraint because of positivity, but let's say this is a lasso problem. Okay, when delta is uh is big the the solution of this Lasso problem is uniformly zero, okay, and in this case we can show that all the Show that all the parameters collapse to zero, almost surely. And in the standard noise regime, if the injected noise is small enough, then you can show, we have shown that the iterates go to zero outside of the support of the minimizer of the LASO program and inside the support of beta star, the the global minimum of this LASO program. The global minimum of this LASO program, then the law of beta t behaves as a certain measure that concentrates near beta star at the scale square root of gamma times delta. So it's really an almost sure convergence outside of the support to a deterministic object, despite the fact that it's stochastic. But inside the support, you really have a gain distribution that concentrates as That concentrates at the exact scale that is here. Okay? So, and those are our plots of this setting. Like this is GD, LGD, and LGD plus level noise. And the most relevant plot is certainly this one. When you have GD, that does a pretty good job at recovering the sparsest interpolator. Then LGD goes a little bit better. This is the previous work. And if you add level noise, then it's really. Level noise, and it's really goes as far as the noise you've injected. And yeah, my take-home message will be: if you want to, I mean, considering appropriate stochastic grading flow can lead to interesting results if you work properly your noise covariance. And for specific toy problems, we can show that the noise can bias the dynamics towards very different uh minimizers than the deterministic periodic flow. Data ministry flow. And maybe the program is really a two-step approach. This is why we took this example because the grain flow was super well known, super well studied. There is a very geometrical insight in this trade-off flow that we leverage to understand the role of tokacity, but really we need to understand first the helium flows to characterize the role of stochasticity. Thank you. 