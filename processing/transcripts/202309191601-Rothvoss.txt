Thank you, Zach, and thanks a lot for the organizers inviting me. I hope this talk is going to qualify as an approximation to algorithms talk. There's a quantity that's going to be exproximated, and there's an inequality on slide 35. This is the bar that you have to cross. This is joint work with my very soon Victor IS who's talking IS. Alright, so this talk is about lattices. About the lattices. So, what's a lattice? You take a matrix with linearly independent columns and you look at all the integer combinations that you get out of the columns. So, this is a little lattice in two dimensions. And there's a quantity that's kind of well studied in lattices, and it's the covering radius. So, for the covering radius, you take a convex body k, and then the covering radius, it's the Then the covering rate is the smallest number r, so that if you scale the body by r and you place copies of r at every lattice point, then you're covering the whole r to the n. This is a sort of an equivalent way of phrasing this, is that you could say it's the minimum number r, so that if I place a copy somewhere in the r to the n and I scale it with r, then I'm intersecting the latest point. Alright. This is a quantity that it's kind of a hard quantity in the sense that there isn't a good certificate that the recovery gradients is at least something, that there isn't a good certificate that the recovery releases are close something. So let's talk a little bit about some kind of lower bounds on the covering ratings that kind of make it easier to have. Okay, so for that, we do the following week, we introduce the We do the following. We introduce the determinant of a lattice. The determinant is the volume of the parallel pipe that is spent by any basis. You can think of one divided by the determinant of a lattice as the density of the lattice. So the sparse of the lattice is the largest the determinant. All right. Okay, so here's a Okay, so here's a. Now let's throw in some cover x party k and let's think about what could be a lower bound on the covering radius. And here's a suggestion. If you take the nth root of the determinant of the lattice divided by the volume of k, that's a normal. So why? Because you need to scale k by at least that amount so that you cover the points in the R to the N on average once. But then cover But then covering points on average once isn't the same as covering points once. So if you use this as a scalar, then there may be points very close to the lattice that you cover very often, and then others even cover at all. So even in two dimensions, this lower bound can be arbitrarily bad. Okay, so what else could you do? So let's take a subspace W and let's look at the projection of the lattice into the subspace. Of the lattice into the subspace, so you get another lattice, the lower-dimensional lattice. And let's look at the projection of the convex body into the subspace W. So if you project, the covering radius can only go down, because if you have a covering, then the projection of a covering is also a covering. On the other hand, that volume-based argument that we just had, it actually can be a lot better for the projection than it is for the original thing. So here's a little bit of a fake picture. In two dimensions, you could imagine that. In two dimensions, you could imagine that if you would project of this subspace double, you would actually get a reasonably tight approximation to the covering ranges. All right, so this kind of low bound of the covering ranges, this isn't a new thing, this was already studied by Cannon and Lovas in the late 80s. So they considered a quantity like a proxy for the covering radius, and this is the best volume-based lower ball that you can get by producing. Base low ball that you can get by projecting into any subspace. It looks a little bit ugly, but it's sort of this is a kind of nicer to handle. In some sense, like if you actually have the subspace W, you can compute what's determinant, you can compute what's the bottom. So you have a new go out. And Canon and Lovas already proved that this proxy cannot be arbitrarily bad compared to the real coverage radius. So they proved that. Yes. So they prove that for any lattice and any convex body in n dimensions, this proxy is the most effective n away from the real covering lattice. Okay. So n isn't bad. And actually, there's a conjecture by Daniel Dadouch that the real gap between this proxy and the covering radius should only be logging. So where's the log n coming from? There's already a construction in the paper by kind of In the paper by Kenna and Robus, you just take a suitably scaled simple X, and there you can see that there can be a gap of log n between the covering radius and that function. And so Daniel conjectured this would be tied, and he was interested in this because he could prove that if this was true and you were able to find that subspace W reasonably efficiently, then you could speed up algorithms for solving engineer programs dramatically. This is sort of why we got interested. This is sort of why we got interested in this as well. All right. Any questions so far? Okay. So the main result of this talk is going to be that we kind of prove the conjecture, model the issue that we're not proving a log n, we're proving a poly log. But sort of the consequence that Daniel described in his thesis actually applies. In his thesis, actually applies, and we can solve integer programs in n variables in time log n to the n. The way this is phrased here is that you have a convex body, you can find a point in the convex body, intersecting the integers in time log n to the n. Okay, and that what was known before was basically n to the n. So, if the conjecture were true, then it was below n to the order. Okay, so there's an O of n, so the There's an O of n. So this is basically, this is this constant plus 1. I didn't want to, you know, update. So, okay, we broke this constant a few times, and I didn't want to update the slides every time. So, all right. Okay. Other course. All right. There's a few other things falling out of this. This proof. One is it's kind of geometric. Okay, let's if you if you have a convex body k, no one guarantees you that this thing is symmetric, so it may not be. And you may want to work with a symmetric body. So there's a standard symmetric body in convex geometry that usually you consider. This is sort of the ultra-symmetrizer. This is k minus k. It's the difference body. It's also a name that's marks. So this is symmetric, convex body that's bigger than k. Coin is body that's bigger than K. So potentially the covering radius could be dramatically less. And what's coming out of this is that actually it can't be less by more than a polylog factor. So whatever theorem you have that means that your body is symmetric, it kind of applies more or less also for the non-symmetric case. Oh, right. And okay. So for everyone who does sort of preparatory optimization, you may have heard what is the flatness concept. Heard what is the flatness constant. I don't really have a lot of time to define this, but we can improve the bound on the flatness constant in dimension n from basically n to the 4 thirds to n fully dot n. I want to spend a little bit of time on that kind of magical algorithm of Daniel Dadouche, and so you can see where things prove there. Proof there. Alright. So this is Daniel's algorithm basically from his thesis. Okay, so there's a convex body K. You want to find the lattice point. And okay, so first, it could in principle be that K is so giant that the covering radius is less than 1. Meaning that no matter where I put K, I always contain a lattice point. Then you can actually, you can shrink K a little bit until. A little bit until the covering radius hits one. And you're not actually losing that. Well, you will still contain a lattice point, so it's without lots of generality. So let's assume that the covering radius is at least one. Then you do the following. You apply the main theorem. You find that subspace W so that's in the proxy, the definition of the proxy, so that the proxy approximates the current gray as well. And now you look at the projection on You look at the projection on that subspace W. So there's a new lattice, the projected lattice, and there is a projection of K. And you compute all the projected lattice points that fall into the projection of K. And then you recurse on the fibers of K. So you can imagine that if W has dimension D, then you can, like for every lattice point here, down here, you're getting an N minus D dimensional sub-problem to solve. It's like an N minus D. sub-problem to solve. It's like an n-d-dimensional integer programming problem to solve. So this is it. So this is a recursive algorithm. Okay. So there's a few things. So obviously the number of lattice boring series should be sort of bounded. Otherwise you have like a giant branch of the factor. How much is it should you find a big polytime or you don't care like exponential time? Uh there isn't anything polynomial. There isn't anything polynomial here. So 2 to the n, 2 to the n is, you know, more than enough. Okay, yes. All right. Good. So, okay. Right. Okay. So I'm going to later show you some kind of a long constructive version about like, you know, where Dublin comes from. You can also make it constructive, you lose a log factor. I actually use another paper that Daniel also kind of wrote. Another paper that Daniel was so kind to write for us so that we could actually have this paper. Anyway, so then, okay, the number of projected, okay, the number of points and the projected latches that are in the projection of k, it's actually, it's small, it's bounded by a poly log n to the d, where d is the dimension of the subspace. So this limits the branching factor. So this is good. And this one is coming from the other thing. This notable form. This is basically whatever quantity we have from our theorem plus the log factor from this theorem is this, yes. All right. Sort of more crucially, that actually we can enumerate the points, these lattice points, we can find these lattice points very efficiently. So there is no kind of extra loss. More trivial. Montrivial because, in principle, it could be that W is everything. So, W could be the whole R to the N. And then I haven't really gained, I only have gained the information that there are not a lot of points that I have to find. But sort of, the projection actually makes this a very easy to solve integer program. And we're going to come to that in a second. All right, so you open the right chapter in Daniel's thesis, and you find this statement. So you have a lattice lambda. Now, this is in D dimensions because this is sort of listed in the projection. And you have a convex body P, projection of K. And then the following notes that actually you can upper bound the number of lattice points by, well, as long as you have a handle on the covering radius, it's upper bounded by the ratio of the volume divided by the determinant. Okay? Okay, so this is a very ugly bound if the covering radius is very small, because you can imagine that you can have like, you know, you can have a very dense lattice like this, and then you have P like this, and then depending on how you shift it, you may contain a gazillion lattice point or line. So then you get a bad bond. But if the covering rate is nice, let's say this is one, then actually this is proportional to the volume divided by. This is proportional to the volume divided by the determinant. And this is the quantity that we're controlling in the projection. Okay? Alright, so this is one thing. The other thing is that for some magical reason, we can actually also compute the points in the same time. Okay? So this, in some sense, is not a general integer program anymore. This is an easy integer program. So why is this the case? So you have a convex body, let's say in D dimensions. Let's say in D dimensions, you can find what's called an M ellipsoid. It's an ellipsoid that kind of approximates the body reasonably well, and with 2 to the D many copies translates, you can cover the body. Additionally, 2 to the D many copies of the body cover the ellipsoid. So you know that also the ellipsoid, none of the translates of the ellipsoids is going to contain too many points compared to this. And then you can just enumerate all the points in the ellipsoids. And then we know. And then we know this is a result by me can't even. So enumerating points in another code is an easier problem. The same time as one? As whatever the right-hand side is. Times 2 to the. Because nothing goes well. Okay. Alright. So this is the algorithm. Okay, good. Okay, good. So if there's no question, I will try to outline the ingredients of the mean proof. And 30 minutes isn't really a lot of time for that, so we'll see how far we get. But if you have any question, please let me know. Okay. All right. Okay. Yeah, I should say that there's a recursion that's coming up. You want to solve a problem in n dimensions. You spend 2 to the n, and then you have a And then you have a branching factor of polylog n to the d, and you recurse in d dimensions less. Okay, it's your homework's problems you give to your students that this really is good running time of polylog n to the n. Alright. Now let's sketch the main argument. Okay, so there's a few convex geometry ingredients, and I hope you're going to get too tired if I list all of them. Alright, so. Alright, so here's a quantity. It's slightly odd if you see this the first time. You take a convex body k, and so for the rest of the talk, let me just assume it's symmetric. There's some additional technicality if not. So anyway, so you have a symmetric convex body. There's a quantity that's called the L-value of the body. And it's the following. You take a random Gaussian and you check what's the average length of a Gaussian. Length of a Gaussian if you use the norm that's induced by k. So actually, if you imagine if you let k grow, then this norm gets smaller. So this is sort of inverse proportional, and I think of this L value as sort of the average thinness of k. All right. So now if you look at a symmetric convex body k and you look at the polar, like here's a little picture, here's k is a polar. A little picture, here's k is a polar of k, then it can actually happen that the l value of k and the l value of the polar, they are both very, very large. This picture, it could be that sort of k is thin in this direction, and for that reason, like the l value of k would be large. The polar is thin in this direction, and for that reason, the L value of the polar would be large. So, in some sense, the convex body k is well, it's not good condition, it's not well scaled. And there's And there's a very deep result in convex geometry that you can always well scale a convex body K, symmetric convex body K. And this is due to Figiel, Tom Turk, Yegammer, and Pizier. So for any symmetric convex body K, there's a linear map so that after you linearly transform K, the L value of K times the L value of polar is only L log N. It's only n log n. So it's only a logarithmic factor larger than it would be for the Euclidean bond. Okay? So the message is that. So the message is that you can kind of rescale a symmetry complex 10k so that in an average sense it looks like a ball up to a log factor. And the log factor is good. It's a lot better than what you would get from like John's theorem. Okay. Okay. Alright. But this is, it's really only true in an average sense. For example, if you take a projection of this thing on an n half-dimensional subspace, the volume behaves like the projection of the corresponding ball of the subspace. But if you take lower dimensional subspaces, this sort of starts to degrade. So for example, you have some short diagonals of length one in a cube, you have long diagonals. You have long diagons of running through there. And the cube is as nicely scaled as it gets. So there's a limit to. Alright. Okay. So we also need to talk about lattices that are nice. And a nice lattice is one that is stable. Okay, so what is a stable lattice? A stable lattice is a lattice that's normalized, so determinant is one. Determinant is one, and it doesn't have any sample lattice that's denser than the lattice itself. So all the sample lattice is lambda prime have a determinant that's at least one. Just an easy example, for example, the Z to the n is stable. But it's certainly not the only stable address. Stable addresses are no better. No, no, nice properties. Okay. All right, so there's a lemma that we can get out of. So, there's a lemma that we can get out of both of the concepts that we just introduced. So, we can get an upper bound on the covering radius for a lattice that happens to be stable. And we can get an upper bound in terms of the L value of K. Okay, and you remember that the larger K is, the smallest the L value, the smaller should be the covering radius. So it kind of seems to making sense. But there's only a log vector that we use. Alright, so for the most of it, unfortunately, you will have to believe this. I want to say what the ingredients are. So the proof isn't long, but there's two heavy hammers that go into it. There's the so-called transference theorem by Balaszchik. It basically tells you that, well, if you have a symmetric convex body and the L-value is less than a small constant, then Is less than a small constant, then for any lattice that you give me, if I delete the points in my body, sort of the Gaussian waves drops remadratically. It doesn't have to make sense. Heard of the terms. The other big thing is that there is the so-called reverse Minkowski theorem by Regev and Steven Stavitovitz. So the statement itself says that you take a stable lattice and it limits the And it limits the number of short lattice vectors. This is connected to actually the whole subspace lattice business in the sense that, well, it was known, there was an argument by Daniel DeBouche, that it was actually prove the subspace flatness conjecture for ellipsoids. And so naturally, kind of you want to prove the subspace flatness conjecture for general complex bodies. This is the paper you would start reading, and this is what we did. And so we use this as a black box, but also kind of our main proof follows a lot of the strategy. Their proofs are strategy. Alright. So anyway, in this short talk, I'm never really able to do justice to this papers. This is a very beautiful paper. Alright. Okay. Okay, more. Uh okay, more more context. Okay, so you have a lattice, lambda, and uh you take a soft lattice like this one and you you look at the span and you take the well you take the orthogonal complement of the span and you look at the projection of the lattice into that orthogonal complement. So you get another lattice, this red lattice here. This is called Lattice, this red lattice here, this is called the quotient lattice. And in many ways, taking the quotient lattice of lambda with respect to sub-lattice lambda prime, it kind of works like factoring lambda into lambda prime times the quotient lattice. So for example, the determinant of the lattice factors into the determinant of the sub-lattice times the determinant of the quotient lattice. And there's a long list of other properties that all like work for the coercion lattice. Okay, so we think of taking coercion lattice as sort of decomposing lambda into like several lattices that are all called long. All right. Okay. I think this is the last concept we need, but this is a very important one. Alright, so let's look at the letters lambda and And so for every sub lattice, lambda prime of lambda, we draw a two-dimensional point where the first coordinate is the dimension of the sub-lattice, the second coordinate is the log of the determinant of the sub-lattice. So you get infinitely many points. You take the convex Hall and you look at the lower end mall and you look at the extreme points of the lower end mall. At the extreme points of the low analog. So every of those extreme points belongs to a lattice. And you can think of this as this is the densest lattice with sub-lattice of lambda with this given dimension. Because it's the one that minimizes the log of the neutral. And actually, there's a long list again of properties that one can prove. So, first of all, That one can prove. So, first of all, for every of these extreme points, there's a unique lattice belonging to it. And these extreme points, they form a chain. Very nice, very nice property. All right, so if you look at the envelope, then just by convexity, the slope is going to be increasing. If you translate this back to the lattices, then it means that this quantity here is increasing. This is the Is increasing, this is called the relative determinant. So, this is kind of the determinant of the quotient lattice normalized by the dimension. Okay, but the final property is the most important one. If you look at the quantum lattices in this chain, they are stable. Alright. So, what this means is the following: you can take an arbitrary lettuce lambda and you can Lambda, and you can decompose this into quotient lattices. They are orthogonal, each of them is being stable after you skip it. Okay, so it does mean that we're kind of, we can find stable lattices and arbitrary lattices. Okay. And for stable lattices, we have an upper bound on the covering radius. That's kind of neat. All right? All right. Good. Okay, I know I don't have a lot of time. I don't have a lot of time, so I'll be quick in just sketching. Work! All right, so say, okay, we have our convex body K. For now, we assume it's symmetric. We put K into L position, meaning we apply this argument by ELFL. We rescale K so that the L value times the L value of corners divided by L. Okay. Now after this, now this also turns. Now after this, now this also transforms the lattice of course. After this, we look at the lattice, we look at this canonical filtration, and we look at all the closure lattices that we know are stable. And let's say the i-th quotient lattice has dimension di and ri is going to be the relative determinant. Alright. And okay, you can group the Okay, you can group the indices so that these numbers are kind of increasing geometrically right now. And okay, so then you start working with this. You just, okay, we know how to upper bound the covering radius of a stable lattice. There's some kind of triangle inequality happening, so you can get some handle on this. Dependent on the relative density of the most important last quotient lattice. That is, you can get a lower bound on the proxy by picking a subspace that's coming out of the condonic confiltration. So, you get something that also proportional to the relative determinant in terms of the very last quotial lattice. So, this all looks great. And then you put things together and then you realize it doesn't work because this will only work if the very last portion of the lattice is actually large. Large. But it may not be. So then you go back to the little cube that we have been drawing earlier, and then we realize that it really cannot work because, you know, what do I do if this very last Mojang lattice, the span is one-dimensional, it could be, and maybe k is a cube, so maybe it spans this, or maybe it's a diagonal. There's a root and factor between it. So there's no way on Earth how, just with these little parameters that I have. Just with these little parameters that I have, I can actually determine my quality precisely. Okay. Well, then you have two weeks of panic, and then at some point, you know, there's a small fix to it. Alright, so you prove something weaker. You just take your canonical filtration until you reach half the dimension, and the only upper bound, the covering radius that belongs to that half of the filtration. That half of the filtration. You upper bound this by log n log squared n times the proxy. But this is enough to get an induction, and there's another log factor that you lose by nuts. But that works automatically. All right, so this was very quick, and I'm sorry this didn't make too much sense, but I want to end with two open problems. I didn't want to squeeze into the approximation. Algorithms don't have problems, but all right, so. Alright, so my PhD, Professor Fritz, his favorite open problem is: is there a two-do-do the end-time algorithm for solving n-variable integer programs? So this is still open. And there's also a limit to this approach. You really can't, because of the simplex construction that I mentioned earlier, this really doesn't seem to be a way to go below the log n to the R. So this is open, and there's some kind of orthogonal approach is that people Approach is that people haven't spent too much thought on making algorithms work in polynomial space. So basically everything we know for lattices, all these two-to-the-n-time algorithms or not n-to-the-n time algorithms, they all use exponential space. So it would be wonderful if we could even solve the easiest problem, let's say shortest vector in the L2 norm, in single exponential time and polynomial space. And in fact, the best thing we can do in polynomial space, even for the simple problem. Can do in polynomial space, even for the simple problem, it's that n to the entire variable by carrying. So, anyway, I'm going to stop here and thank you for listening. Yeah, so this framework you described, how does it sort of simplify framework you describe how does it sort of simplify for the case in ellipsoid you don't need the induction because you will I mean you don't have the problem that the slide okay you have a ball and you you have a projection of a ball it's a ball and you have a projection of a qubit can be anything so that you it's it's sort of more of a one-shot argument It's sort of more of a one-shot argument. You still look at the canonical filtration, but you don't need deduction, and you just sum up your terms and call for that. And I guess the eight-side you can actually assume is a ball by just transforming into the latest. Yes, yes. When you say you apply induction, you mean like you recompute the L body and all of the log n times, log n times you apply that these. Log n times you apply that PCA theorem, and log n times you rescale, always for like low-dimensional sizes.