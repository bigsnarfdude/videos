So, welcome back everybody. I'm happy to introduce Joel Friedman. He's a professor at the University of British Columbia in the Department of Computer Science. Received his PhD from Berkeley and taught at Princeton until 1993, and then has taught at UBC since. Joel researches graph eigenvalues and expansions and their applications to a wide variety of fields, including mathematical physics, error correcting codes, and differential geometry. Thank you, Bill. That much I didn't know about my work, but. Yeah, I guess I have one article in the Journal of Differential Geometry. Anyway, I want to thank the organizers for inviting me to this really interesting conference where I'm finally learning all these terms that I've heard about machine learning in more detail. And I thought I'd talk about. I haven't really used Hodge theory except in like In like early 90s for a certain computation, but I thought I'd talk about something completely crazy and off the grid called sheaf theory. And most people, when I use sheaf theory techniques, they say, what's going on? What are you doing? Then I learned that actually there's going to be a talk on sheaf theory here. So I thought, no. Great. But. But so I thought I would give just a little bit of background. So for me, sheaf theory looks like the following. Sheaf theory, if you think about sheaf theory to compute the Betty numbers of topological space, there's 20,000 different kinds of homology and cohomology theories. Sheaf theory is not particularly Theories. Chief theory is not particularly distinguished there, but it's kind of, it works in settings where you have very few functions. Like in algebraic geometry, the typical functions you have is maybe a polynomial ring or some piece like that. Very few functions. So sheaf theory sort of works well with few functions. We have settings with few functions, I'd say. And particularly in algebraic geometry. And it sort of got a kind of a working over by Cotton Deek. Over by Kotten Diek and many of his colleagues who wanted to develop a much more general type of sheaf cohomology theory. So sort of vastly generalized by Martin Deek and the many people who worked with him and wrote up. Many people who worked with him and wrote up this material. And sometime around 1980, there were a few computer scientists, theoreticians, who suggested you might be able to solve P versus N P with a general enough kind of cohomology theory. So the works of Steele and Yao and also Ben R around Around 1980 suggested maybe so to solve heat versus energy or much easier questions maybe. Maybe there's some sort of cohomology of Boolean functions that you should try to develop. And with that, you could get a measure of how complex a Boolean function is. So I have always, you know, really had this dream of trying to doubt. This dream of trying to dabble in this. And so at some point I went back to the works of Lutten Diek and read them. And while he was discussing things in algebraic geometry, the models were so general that you could also do things in combinatorics. So when he says, you know, injective module, I think of a partially ordered set and something kind of lives in that direction. And you can do a lot of these things. Of these things just with discrete objects. And this was sort of my original goal. And, you know, it kind of still is every once in a while, but haven't really solved p% P or even the far easier problems. You know. You know, got hopefully a few decades left in me, so maybe. But at some point, somebody mentioned to me the Hanniban conjecture. His name was Lahom Bartoldi. But he stated to me: it's a conjecture. Stated to me, it's a conjecture in sort of commentatorial group theory, but he stated to me as just a graph theoretic question. And it turns out, when he stated the graph theoretic question, I very quickly realized that a lot of this mechanism and a lot of the tools that when he uses an algebraic geometry can completely reduce this to a question about sheaf theory. So without saying what this is, this can be reduced to a question. This can be reduced to a question in graph theory. It's about subgraphs of Cayley roughs. Rather than state this formally, I just prefer to give sort of a high-level description. So it gives you... It gives you. It gives you graphs. So you have sort of a big graph set in which everything happens. And you have subgraphs. Let's say y1, y2, up to y sub n. And it asks for an inequality between the Betty numbers. And I'm not going to tell you. And I'm not going to tell you which particular ones. This is a Cayley graph on T generators for a regular graph. This is subgraph in symmetries. But for the sake of giving concrete examples in sheaf theory, I'll just assume that somebody hands you a bunch of subgraphs of a graph. And they say they want you to prove the following. They want you to prove that for every other subgraph, x, you have a certain inequality. You want to prove the following. You want to take the first Betty numbers of these graphs, the reduced Betty numbers. It's just a slight modification. r equals 1 to m. You want to intersect these graphs with x. Take the sum of this reduced bedding number. And you want to show that it's less than or equal to the reduced buddy number of x times a constant, where this constant comes from. where alpha is given by sort of trivial situation where the x is actually all the set. So where you get, you know, you plug in to x and plug in the whole graph set, and you get sum of i equals 1 to n of the reduced by the number of i sub i. Of y sub i. Because if you put in the whole graph here, you get just y sub i back. Here you get the reduced random number z. This is how you define alpha. So you define alpha by what you get when you plug in z. This turns out to be entirely simple in all of the questions that it raises. And it says, can you, for the x equals z case, can you prove this? X equals Z case, can you prove this? And here's the high-level talk before I get into details. There's two what I like to call magic tricks. Or surprising things that come out of this. First is I'm going to solve this inequality for x equals z. Now, it's already solved for x equals z. So the first trig Well, let me back up. The first trick is I'm going to look at this as described in sheaf theory. So, go one step further. Let's say that alpha is an integer, and so on. So, you could think of this as really you're taking alpha copies of z and these graphs y. And these graphs y. So you're really sort of comparing y1 up to ym, really the disjoint union of these to alpha copies of settings. The first magic trick is there's a natural way to think of solving this, which would be to get a surjection from this set of graphs to these alpha copies. And I'll sort of explain why you want this. This This is this. No such surjection of graphs exist. But as sheaves, there are tons of surjections, graphs. But many of the sheaves that in a natural way are That in a natural way is associated to the graphs. And this is sort of the real thing I would stress for this kind of conference. So the talk you heard before was trying to use sheaves to model something. Put vector spaces on the edges and the vertices. This is This is different. This says if you have a graph, there's a sheaf associated to it as a subgraph of bigger graph. It's entirely trivial what this is. It's not, once you see it, you'll say, what's the big deal? This is, you know, as she's, this is sort of very kind of almost, you know, trivial as she's. But the point. But the point is, when you have a surjection, the thing you want to study is the kernel. And that produces something new, something that you may not have anticipated at all. So this is sort of the modeling thing. It's not that the sheaf itself is built to be interesting in some way, but you have some data, maybe a graph, some visual complex, whatever, and they're kind of Kind of simple, natural to define, but it's neither of these things that you're looking at. It's a way of surjecting and getting a new sheaf. So this, the real point is, you know, you get some sort of new sheaf whose meaning is strange, but it's sort of like the right thing to look at. That's the first surprise. And the second surprise, magic trick, is you use Chief's theory to solve the conjecture for the case x equals n. For x equals z, this inequality is trivial because it's an equality, because that's actually where alpha comes from. If you say, I'm solving this conjecture for x equals z, everyone says, get out of here. But this solution even Immediately implies a whole conjecture. The conjecture for all x. In other words, you build something that trivially gives you this bound for the trivial case, but somehow this thing that you're constructing has Thing that you're constructing has some kind of hidden information in what you're building. And the information makes it that if you're interested in certain kinds of facts, the trivial case implies the entire collection of cases. And uh I should be able to come to this. Come to this, and I'll probably just, you know, wave my hands at this part a little bit. So, the main idea is you have new sheaves that are suggested by certain sort of simple sheaves, and the things that come out of them have some kind of information, and it doesn't solve your problem, but sort of says these are the sheaves to look at. The new sheaves don't really. The new sheaves don't really solve the problem, but it sort of tells you where to look. This is the very high-level description of everything before I go into a bunch of low-level details. So you should any questions about this is sort of where we're heading. We're trying to compare subgraphs of a graph to a number of copies of the graph. You said that you're sort of using this language copy to parallel as an integer. Is that just sort of an analogy, or are you saying where? Is that just sort of an analogy, or is it the reason we're actually restricting alpha to integrate? Well, in the Honorabeum conjecture, the kind of problems you get, alpha will always be an integer. In fact, alpha will be the reduced Betty number of one of the y sub i's, and they'll all be isomorphic. So there's a lot of hidden structure I'm not going to get into, because that would probably take all the whole 50 minutes. Very briefly, maybe what the reduced heavy methods are? Sure. Sure. So probably the way to say it most briefly is that the reduced body number of a graph Number of a graph. So it's just the maximum of the first Betty number, actually, minus one max of zero. If G is connected. I guess what I would assume it would be without knowing the definition. Knowing the definition would be just the rank of the homology in degree one of the reduced homology group. Yeah, of the reduced homology group. So that's what it is. Yeah, yeah, oh yeah, yeah. This is the rank of the reduced homology group. Okay. Betting numbers can mean a lot of different things. Yeah. So, you know, if you think of a tree, it has zero has one connection. It has one connected component, and its first bedding number is zero because there's no cycles. And then if you think of a graph that has a cycle and maybe has some trees growing over it, it has one component and one cycle. Two cycles, it's connected and has two, and the reduced bedding number is just zero, zero, one, two. 0, 1, 2. It turns out the reduced Betty number, most people, you think of the first Betty number as the number of holes or cycles. I heard this term voids for the second Betty numbers. But you know, you could think of it as you just subtract one and max with zero. But there's a whole bunch of other ways of defining this. Finding this. And then if G is not connected, you just sum over the connective components. So you take all G parameter connective components, G, and you sum over this maximum of Zero first bed number. But it's the reduced homology. There is a very good reason why this invariant is somehow much better behaved in a certain sense, such as under covering maps and so on. But you could think of this to a first approximation as all of this stuff will really address any kind of invariant, like zero-athbetic number, first bet. Like zeroth betting number, first betting number, reduced first betting number, and so on. Chief the research setup for that. Sorry, one last clarification. The y sub i's, is that a fixed set of subgraphs ahead of time? So yeah, so in the Hannon Neumann conjecture, it'll give you a z, which is a certain Cayley graph, and then it gives you any subgraph y you take all its translates. So it gives you a whole class of problems. Yeah, I mean, the number of translations in the Calegraph will be the, you know, the order of the group. It turns out that this is the order of the group. And so this turns out to be just reduced by the number of one of these. So it's always an integer. There's a reason. They state it a little bit differently, but this is how you can think of it to a first approximation. Okay. Other questions before I head into the details? Okay, if not, let's go into the, you can just sort of forget all this and I'll define what is a sheaf. So as in the previous talk, As in the previous talk, you have a graph. This is a sheaf on a graph. So the graph has vertices and edges. Probably want to orient the edges of the graph. So let's say they're oriented. So, again, each edge, you have a tail of the edge and a head of the edge, and these are vertices. So, you can have multiple edges, you can have self-loops, and so on. Loops and so on. Sheaf will be just the opposite of what it was in the previous talk. But it's if you want to take dual sheaves, you can you can first model them. So sheaf uh sheaf f nice bodies. Which means you fix, let's say, the reels. It doesn't matter, you fix a field. And, you know, think of the reals of complex numbers, especially if you want to do Hodge theory on it for each. Um for each edge you have a vector space, and for each vertex you have a vector space. These are, say, F vector spaces. And then you have restriction maps. So for every edge, it has a head and the tail, the tail of the edge, and the head of the edge. And for each one of these, we have a map. You have a map. You have a map, let's say for each edge, you have a tail map that goes from the space on the edge to the space on the tail of the edge, and a head map that goes from the edge to the space on the head of the edge. And the spaces can be arbitrary. This can be arbitrary. So the body numbers we find as follows. We don't, we don't get the direct. It's like for convenience. And you don't even need the directed structure. If you actually look at what sort of Palton Diego's theory tells you, it'll explain that the choice of an orientation. That the choice of an orientation is the choice of coordinates of a certain kind. So you don't, it's a little bit of a lie, but it just makes it easier to have a direction. Why is this a skeet and not a curve skeep? Because you're thinking of the topological space as a graph, and all the open subsets are subgraphs. Okay, so that's the topology. It's the topology that changes. So if you put an Alexander topology on the On the post of the graph, then you would, this would be called the code sheet. Okay. Yeah, a lot of people don't like this convention, you know, but there's a reason, and it's tied to what the Holy Neumann conjecture makes it more convenient to express it. It doesn't really matter. You know, this could be a post set, this could be a category, some form. The Betty numbers, so it's you define a map. It's you define a map D from the sum of the edges to the sum of the vertices. And this is like the incidence matrix. This is sort of a generalization of the incidence matrix. And the way you define it is probably Is probably, you know, what you would expect. So, what is the edge space of the sheaf? It's just the sum of the individual edge spaces. And what's the vertex space of the sheaf? You sum of all vertices of the vertex space. So Simplest kind of sheaf. So simplest sheaf is just so all values are just this field itself, the one-dimensional vector space. And all restriction observated energy. So if I have a graph here, gee, you just put an F everywhere. Everywhere. It's called a constant shift. And we put a little bar to remind ourselves of the sheaf. This exact same notation was used in Arnie's talk previously. The simplest thing is you put a one-dimensional space on everything. This one-dimensional... One-dimensional, this is just a set of, you know, a copy of, say, the reals, if you think of the reals, for every edge. And this is a copy of the vertices. So in this special case, we should make sure that I'm really defining the incidence matrix. So when you have out of the edge space and the verfict space for MB the Caucasian sheaf. This is just, you know, whatever your field is, one for each edge and one for each vertex. And the incidence matrix is going to be defined as the usual incidence matrix. So the idea is, you know, you have The idea is, you know, you have your edge space, your vertex space. These are no longer a matrix, but it's kind of like a block matrix. Because it depends on what the dimensions of the spaces you attach to the vertices and to the edges. This might be one-dimensional, this might be three-dimensional, and so on. But the same idea for each edge, you're going to put a plus here, a minus here, a plus at the head, a minus at the tail. This is, you know. Yeah, yeah, my bad. Should be the other way. It should be vertices and edges. If you're operating on column vectors. So for each edge, So for each edge, you have, you know, its head is the head of the edge, and you have the tail of the edge. And then maybe the head of the edge is here, and the tail of the edge is here. And you're going to put a plus one and a minus one there. And plus one, minus one times what times the restriction maps. Restrictions. So, in other words, I define this map vertex space by taking, let's say, So, for each element, let's say, V, that's in the direct sum of the space of all the edges, I'll just define it on, let's say V is just lines of. F of E alone. So I'll just take a standard basis vector, except it's done with the whole space, and you map, V maps to. So you apply F of E times the head map of V, and you subtract F of E times the tail map. B times the tail of A. And in the case of the constant sheaf, this is just, you're putting this in the so this belongs in the belongs in the oh, these not a grade letter. Let's call it the letter. This this is lies in the in the head space and this lies in the tail space. So this is a generalization of the incidence matrix and the best. And the buddy numbers are just in the usual way. So once you have this incidence matrix from the edge space to the vertex space, you define the first bedding number of f to be the dimension of the kernel of f. More generally, you define the first homology group for graph. The first homology group of f to be the kernel of f. And the zeroth of any number is the dimension of kernel of D. And the dimension is the co-kernel of D. And more generally, the zero. And more generally, this homology group is coconut Okay. So the first example is if F is a constant sheet, then you just get D is the usual dissidence matrix. The second example is let's say you have T is a graph A B C and you take a subgraph A B this is a subgraph, but then there's a sheaf. But when there's a sheaf that nicely describes the structure of this subgraph as a sheaf, so we define the sheaf associated to the subgraph. As what? Everywhere that's not in the subgraph? Yeah, yeah, zero everywhere it's not in the subgraph. I mean, what else could you do? If you look at the whole setup here of the incidence matrix, what this gives you is essentially the incidence matrix of the subgraph. So if we look at a subgraph and the constant sheaf associated to that subgraph is just this zeros. What is D, it's, you know, it takes you. It's, you know, it takes you, the edge spaces just has the single one edge there. Really a map of F to two copies of F from these two vertices, the edge space of the sheaf to the vertex space. And it's just the incidence matrix of the subgraph. So beyond this D on this new sheet that we've created is just the incidence matrix on subgraph. So you can encode any subgraph of the graph as some kind of chief. So far, not much is happening. Right, because we would use a sheep operation probably coming from the inclusion, actually. Yeah, so that's the next thing. So now I said how many conjectured something about subgraphs of a graph. And that's where you get a map of sheets. So, you know, we remark that if you look at the sheath that is associated to the subgraph, there is a map to the sheath associated to the graph. What is a map of sheaves? Well, you know, we have these values. F, F, F, 0, 0, 0. 0, 0, 0 and F, F, F, F, F, F. A map of sheaves just means you map the values to the values. Let's just think of this as a map of sheaves. You map the values to the values. It means that you map this over here. You map this over here, you map this over here, you map this over here, this over here, this over here. You map the vertex spaces. Vertex spaces, edge spaces. And what you can see is this is a map of shapes. In other words, it respects the heads and the tails operators. The caution is that there's no map there's no map that goes the other way. This is where you have to get a little bit familiar. To get a little bit familiar with the way sheaves work in order to use that. But what you would want is that, you know, when you take the, say for each edge, if you take, let's say you're mapping in general, if you map F1 to F2, so you've got a map from the space of each E to the space. Of each E to the space of F2 at HE. And you have for each, you could take, say, the tail. That gives you a map from F1 of down to the tail of E, and a map the tail of E. Tail of E two possible maps. This is the sheaf map. And for sheaf maps, this has to be compatible. And the particular definition that a lot of people don't seem to like about going from the edge space to vertex space. Going from the edge space to vertex space, the merit that it has is that if you're a subgraph of a graph, you get a map this way. And you don't get a map the other way. And you kind of have to look at this and stare, you know, why not, you know, if you have a it's, you know, now. To map from the constant shape to somewhere on the sheaf, which has something like this in it, you have to take this to the, this has to be the zero map. There's no other map. But if this is the identity, and going here is supposed to And going here is supposed to be the same as going from here to here, which is also the zero map. And the only map you can really take is the zero map. So the only compatible map, I shouldn't say there's no, but there's nothing, there's no non-zero map. The only non-zero map. So is that the same as saying that the only map to go from ground to sub-ground is essentially a projection? Is this actually a projection? The only map, you know, the general philosophy is that if you have an open subset of a topological space, then there's a map from the restriction to the open subset extended by zero into the original thing. If you have a closed subset, it goes from the thing to the closed subset. And similarly for covering mouse, other kinds of things, but Kinds of things, but it's a basic mechanical feature about the way you set up your sheaf theory. You want subgraphs to map to graphs in this sort of sheaf theory. That's what you have. Okay. So I thought I would maybe end with at least the first magic trick. Magic trick number one. You have to believe me when the husband is. Believe me, when the, say, the Hannon-Neumann conjecture will produce the following question. You have a graph, which is a Cayley graph. And this is probably the simplest one to illustrate the ideas. It's a Cayley graph on the integers modulo 3. And it's got self-loops, but... And it's got self-loops, but that kind of makes your life easier. And what we will try to look at is two copies of Z. Well, I'm calling it G. Two copies of G. Z. Let me call it Zed, because that's what I called it in the introduction. Z. And then you'll have three copies of a group. And then you'll have three copies of a graph, which is a single graph and its translates. And the mechanics of exact sequences, which I certainly don't have time to discuss, but you should think of the usual way of computing the homology or cohomology of a union of two things from the individual cohomology groups in their. The individual cohomology groups in their intersection. For whatever reason, I will want a surjection from here to here. You can't do this as graphs, because if you map, you know, three graphs to two, one graph will be map to one graph, and it can't be a surjection. Just the combinatorics doesn't allow for it. Graphs on the left, they're the same graphs as the ones on the right. So everything is heaven over Z. This is your Y1, Y2, Y3. And here are two copies of Z. Are all the same graphs? Are there only two self-loops? Right, right. I mean, but yeah, I mean, this, no, this is A, B, C, and this is very specific. ABC, and this is very specifically A B C A B C A B These are all isomorphic graphs, but you specifically think of them as lying as sheaves in this sort of ground thing. But it turns out as sheaves there's plenty of. Sheaves, there's plenty of surjections. So the sheaves that you get associated to this, the disjoint, disjoint, two disjoint copies of this corresponds to the sheaf where you have two copies of F everywhere. everywhere. F, F squared, F squared. This is really F squared, I've said. So, I mean, of course, when I add the sheaves, I don't, I'm not separating the things. I'm adding them as sheaves. I get F squared at every edge and vertex. And if you sort of see the way that sheaves work, Work and have a map like this. So it turns out that if you have a connected piece of this sheaf, it has to go, the entire thing has to go into this as, especially if this is just a one-dimensional thing, you have to take, no, one times this to something in here. So the claim is. So the claim is, no, the map 1 goes to, say, lung 0 will be a valid map of shapes. Just as a matrix taking a one-dimensional thing to a two-dimensional thing. And there's a map where one maps to, say, 0, 1. And then on this third thing, there's a map that takes 1 to 1. It takes one, two, one, and one. What sheaves allow you to do is it allows you to map objects, but when you think of them as sheaves over the same space, you can get maps that if you have, for each thing here, you have at least two down here. At least two down here, then this is a surjection. And if you have, you know, at least two edges here into a two-dimensional space and you take just three independent vectors, it's a surjection. And then what comes out of this surjection is you look at a kernel, and this is sort of the magic thing that this theory produces. It takes a concrete problem about comparing things on two graphs, and it does some kind of strange kind of morphism. Now you can define this without sheaf theory, but somehow the point is that all the constructs of sheaf theory, exact sequences, everything like that, goes over in this setting. So this is sort of like some kind of a twisted. Some kind of a twisted map of graphs. But when you look at this kernel of this, it's sort of the right thing to look at. And that's probably as far as I will get. Okay, questions? Maybe we'll just say a few words by trick number two. Yeah, trick number two is if you, so this really, this comes up at the sort of, this is from the trivial case. And what you get in terms of sheaf theories, you get this sort of sum of Sum of the Y subi's that I'm just going to add into the sheaf associated to Z, but you have alpha copies of it. And you get this sort of kernel of this map. And what you wind up wanting to do is you know that the first reduced bedding. That the first reduced betting number of this plus the first betting number of this is an upper bound on the first betting number of this. By the way, that exact sequences work. This is true really for any homological invariant. It's not just the Betty numbers, but there are other So, the way to prove that this one is less than that is to show that you want to really show that whatever invariant you're looking at, you want to show that the reduced by any number of this zero takes a number of pages. It's fairly long proof. It's intuitively, you can give an intuition, but this is what you want to proof. Give an intuition, but this is what you want to prove. And then you come back to people in here, and you say, ah, I proved it in the case x equals z. But it turns out that if this is true, then it's also true that if you essentially restrict the kernel to x and extend by 0, it's automatically less than or equal to this. And if you define this reduced by the number in the right way, This reduced Betty number in the right way, it's absolutely automatic. So, what it means is this reduced Betty number is encoding things because if the reduced Betty number of something is zero and you restrict it, well, subgraph, it's also zero. I mean, that's maybe not really surprising if you think of If you think of the first Betty number, and if you take, here's a tree, and here's another tree, and here's yet another tree. And if you take this as a graph, this is zero. And if you take the first Betty number of any subgraph of this, you also get a You can also get a forest. You can also get first betty numbers here. And sort of an automatic property of first betty numbers. And the first betting number and the reduced betting number satisfy these properties. Good question. Thanks. Questions? Motivation for the Motivation for the conjecture? Yeah, the conjecture came out of, you know, there's all these people like Vihara and Hausen, Hannah, all these people, who didn't think they were really doing graph theory. But after a bunch of articles are written and people look at it a certain way, you can say they're essentially doing graph theory. So the Hammanian conjecture. Um says that if you have, say, two subgroups of a free group, and this is a finite rank, then if you take the rank of H intersection K, it's less than or equal to the rank It's less than or equal to the rank of H times the rank of K. It's not the rank, it's the reduced rank where you subtract one from the rank, assuming that it's a negative. That's the exact same subtracting one that comes up here. Hannah Neumann, following the work of Hausen a few years later, showed that it's valid if you have a two here. But in practice, every single example had a one here. The question is: you know, are you missing something conceptually from what you're doing? And so, you know, two here was done by Hannah Neumann following work with Hausen and a few others in the say mid-1950s. But later, But later, this was converted to Stinson just to a problem in graph theory. And once I became aware of this theorem in graph theory, my sort of interdisciplinary meeting where people are talking to each other in different fields, it became clear to me that that sheaf theory was a way you could formulate a stronger conjecture. You could formulate a stronger conjecture. And there's a number of similar conjectures, not when you have a free group, but when you have virtually free groups and things like this, where they don't have the constant. And where there is another sheaf theory question that's interesting. But there's almost no, very few tools that work to prove results like this. And, you know, my technique. My technique, as Warren Dix has pointed out to me, works by the skin of its teeth. In that, if you have a free group of order 2, that's sort of the thing I was working on. If you have a free group of order 3, this follows because a free group of order 3 is embedded in a free group of order 2. But my graph-theoretic sheaf techniques will not solve that, will not give you the conjecture in that case. So, this is like one of these skin-of-its. This is like one of these skin-of-its-teeth, you know, phenomenon. But the original motivation came from the theory of free groups. But, you know, what is this rank? You can sort of, calling the work of Stinson, you can express this just in terms of finite graphs. Other questions? People formulated the Hanlon conjecture with Labus. Conjecture with Labus theory instead of Grau theory? I don't know. There's a number of different formulations. One of which, through the Warren Dix combinatorial, I don't, I'm not the expert on that. But there are a few of them over the years. Thanks, Joel. I'm curious, if you proved P versus MP with a magic trick, will you reveal your trick or whatever? Oh, the trick that you're trying to use is pretty clear. So from a when you have a short exact sequence, And you think of, you think of, think of, you know, you think that the sheaf corresponding to the intersection of two subgraphs, which maps to the disjoint union, which maps to the union. So So, when you have the mayor be a torus sequence, you get from this kind of two things, consider them separately, consider their union, you get relations between them that are expressed in terms of exact sequences. Sheaf theory, in a sense, gives you a vast generalization. Although there's really only a few cases you typically use of this. But it gives you a more general situation, which says that for a sequence like this, For a sequence like this, you have an exact sequence of homology groups. Let's say F1 to F2 to H1F3 to H mod of F1 and so on. So the language of exact sequences tells you that, you know, if you Sequences tell you that, you know, if you want the dimension of this, you take this modulo of that and this modulo of that, and you get the dimension. But it also gives you an inequality. It says that if you have any sequence spaces, A goes to B, goes to C. Then you know that the dimension of B is also equal to the dimension of It doesn't equal to the dimension of A plus dimension of C. So one thing that people don't know, and sort of the next step to P versus N P, is formula size. What is the shortest formula to represent a certain Boolean function? What's the shortest circuit size for a Boolean function you could presumably solve P versus N P. Almost, but we're way beyond this. But how does formula size work? Well, formula size works like this. The formula size of, say, a Boolean function, you know, f and g is less than or equal to the formula size of f plus the formula size. Of f plus the formula size g. And the formula size of f or g also satisfies this inequality. So formula size, if you could find a way to have a cohomology theory that said that there's some representation of this Boolean function, this one, such that there am. Such that their AND finds its way into a short exact sequence, then you might be able to characterize formula size in some sort of homological way. And if you could do that, and if you could compute the homology groups of a lean function, you could give a lower bound for the formula size of a function on this basis. So, you know, this could think of both. You could think of both in the Hano-Dermin conjecture and Boolean complexity theory, you get inequalities about vector spaces that lie in exact sequences. And this is formula size, circuit size is probably very closely related to P versus N P. If you could show that an NP-complete problem like SAT or 3-Color had a large circuit size, larger than polynomial, you got equal to NP. And if you can show that it has a polynomial size, there's a good chance you can find an algorithm in polynomial time as well. So, but circuits, uh sorry, circuit size. But, you know, this is you know much easier. Formulas are just sort of Formulas are just sort of like computation trees, whereas circuits can be arbitrary graphs. So this is kind of the way I wouldn't, you know, Phil and Yao and Ben Noor, there was kind of this buzz around where I was saying, look at this as a way to solve P percent P. It was this real kind of buzz at the time. But for formula status, I think this is. But for formula size, I think this is a realistic expectation. And not expectation. A realistic approach. It's as good as any, because no approach really gets you anywhere close to it. You want super polynomial bounds for certain Boolean functions. And no one's close at this point. Oh, already. Oh, sorry. Sorry.