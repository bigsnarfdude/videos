My former PhD students, Rafael Coyot, Georginia Rashi and Damiano Rombardi. And okay, as the title says, it's about putting marginal moment constraints. So let's just a brief overview of the talk. I will start with the introduction of a problem and then I will present the results that we have on this type of problem or we can find some Or we can find some optimizer. Then I will discuss how the moment constraint optimal transport may approximate optimal transport problem. And in the last part, I will show numerical results that we have developed for multi-marginal symmetric optimal transport with many marginals. Okay, so just to set the notation. Uh, to set the notation, so what we are considering is quite very classical optimal transport problems. So, for example, the basic one is with two marginal laws, mu and mu. Okay, and what we want is to minimize the cost, C, which is assumed to be lower semi-continuous, among all coupling that have the right marginal law, so mu and u. Okay, the multi-marginal version, of course, if we will consider Of course, we will consider capital M marginal loads, and we will try to minimize again the cost among the coupling that have the right marginal loads. And I will also give some word on the martingale optimal transport problem when the mu is lower than mu in the convex order and you want to minimize the cost. The cost with the constraint that the coupling is a Martingale coupling. Okay, so what are moment-constraint optimal transport problems? In fact, we are considering exactly the same problem, but instead of assuming that we ask to fit exactly the marginal laws, what we ask is to only fit some moments. Okay, so moments have to be understood in a So, moments have to be understood in a white sense, so not necessarily powers of x, or it's described by functions. So, we will consider domains in Rd, okay, x and y. And we will consider the same number of moments for ease of the presentation, but we could so we will consider n moment functions for the low mu. Of course, Law mu. Of course, since we are considering moments, we assume that these functions are integrable with respect to mu. And the same, the psi, the function psi, are the moments that we are considering for new. Okay, and then the set of coupling that we are considering is this one. So what is this set? It's a set of all coupling such that the moment with respect to With respect to phi m is exactly the same as the one given by mu, and the moment with respect to sine of y is the same as the one given by mu. So basically, the set of all couplings that have the same moments as the probability law mu and u. Okay, the same moment for phi and psi. Okay, and then what we Okay, and then what we want is to consider the same optimal transport problem. So here I have written this for with two marginals. So we are considering to minimize the cost among all these couplings that share the same end moments with mu and new. Okay, clearly. You okay, clearly, if you fit exactly the law, you fit in particular the moments. So, this inclusion is trivial. And so, since this inclusion is trivial, you clearly have you do the infimum, the minimization over a larger set. And so, I n is smaller than the optimal transport problem, the classical optimal transport problem. So, natural questions are, do we have questions are do we have uh okay do we have a measure that attains the the infimum and under what uh assumption we may uh have if we are we may have a convergence of i n toward i when the number of moments goes to infinity and also we would like to know if we could get some convergence spin okay Okay. So, what are our motivations? So, one motivation was also that in some problem, in fact, optimal transport problem in some practical case are given with moment constraints. This is the case, for example, for Martingale optimal transport in finance, because what we are considering are the Considering are the robust price bonds given couplings that are martingale couplings, but for which we only know the value of Europe notions, which means that we know the moments with respect to these functions that are the call option payoff. Okay, so of course, yes. So, of course, yes, I've said the problem is also interesting per se. So, this can be interesting, for example, to look at optimal transport, knowing some covariance structure, or I don't know, many problems that could be of interest. And our second main motivation was that it was also a way to relax the classical optimal. To relax the classical optimal transport problem. And so we thought that it could be a way to get numerical methods to approximate optimal transport problems. Okay, so now I will present results on moment-constrained optimal transports. So we will use intensively the Chakalov's theorem, which basically Which basically is a consequence of Carate-Dorgi theorems on convex sets. So what we consider here is a measure which is not necessarily a probability measure. And we assume that it is concentrated on A. And we consider a function lambda that will describe all the moments, let's say. All the moments, let's say. So it is in dimension n zero, and we assume that it is integrable and that this quantity is finite. And then Chaklov's theorem says that it's possible to find a discrete measure that weights only k points with k lower than n zero, the number of here of more. Here of moments that we are considering, such that this discrete measure with direct mass at z1 has exactly the same integrals. So then, if you think of probability measure, of course, there is no reason that this discrete measure is still a probability measure. So, usually we add in the function. We add in the function lambda the constant so that the discrete probability measure of the aggregate is also a probability. So then we will need n0 plus one, let's say, points. Okay, and now we will use this and I need okay some what we will call What we will call admissibility condition. Basically, what we assume is that the optimization problem that we are considering is not void. So, we assume that we can find a coupling that matches the moments such that the cost is finite. And thanks to Chakalov theorem, if this is true, in fact, this is equivalent to find weights and points. weights and points such that which exactly the same set of coupling such that this is finite and therefore if see if the cost function is finite valued you admissibility always holds okay so this is uh a first theorem uh that describes uh okay that describes okay the the structure of some minimum so what we assume is that the function the moment functions are continuous okay and for uniform integral reasons we need to assume that all the moments functions are dominated by some function theta mu and theta mu function theta mu and theta nu to the at the power s with s small smaller than one and we will consider the same minimization problem but assuming in addition okay you see that the integral of theta mu plus theta nu is smaller than a okay this is a technical requirement that we that we had to To okay, to get tightness and to handle the problem. And if you consider this problem, then you can show that for A0 and for any A greater than A0, this problem is finite and admits a minimum. And besides, thanks to the Chekhov theorem, we can find a minimizer which is a discrete. Which is a discrete, uh, a discrete measure that weights at most two n plus two points. Okay, and if you know that the measure mu and u only weights some set sigma nu mu and sigma nu, then you can choose your points in this set. Okay, so this is important for new. For numerics, because if you look at this, then you have a candidate for the minimizer. And so you can try to solve this problem by minimizing with respect to pk, xk, and yk, this intimum, for example, using a gradient descent or any optimization procedure. And there is And there is this result, this theorem, this theorem, which is interesting because if we take, okay, now k greater than 2 times 2n plus 3, so it's a bit more than here. So if we accept to have more particles, then we can find between any two any two Any two discrete measures that fit these constraints, that satisfy these constraints, we can find a path, a continuous map between these two points such that the cost that we are minimizing along this path is monotone. So this is interesting because this says that there is no local strict minimum and so And so gradient type method may be interesting. And to prove this theorem, this is again based on Chakalov's theorem. And we basically construct by, okay, we first use Chakal theorem to transfer each initial. To transfer each initial point to a measure that weights only two n plus two n plus three sorry points and the same for this and then we just take essentially a convex combination between the two measures okay so just some So, some comments on the first theorem that I've mentioned. Okay, there are many extensions and variants that could be stated. In particular, the constraints, the blow-up constraints that we have put here, that one can be removed if you assume that the moment functions have compact support and that C goes to infinity at. goes to infinity at infinity okay we have of course a multi-marginal version of this uh of this um of this theorem so uh this is exactly the the same so now we assume that we we want to fit for each marginal uh capital L moments capital L moment functions and again Again, if A is large enough, there is an infimum, and we can find a minimizer with this time at most mn plus two point. Okay, so this is interesting, why? Because you see that the number of points that you have to consider grows linearly with respect to the number of marginal hem. And okay, since each point is parametrized. Since each point is parametrized by m coordinates, then the parametrization of your measure grows in m square, but there is no curse of dimensionality with respect to m okay and in the particular symmetric case, I will come back later on to this. It's even much better. Okay. Just a word on Martingale optimal transport. So this is a, I recall here the problem that we would like to consider now assuming only moments. Okay. Matthias Vegalboke and Marcels have proven a check-off theorem in this case, but unfortunately, this is expected. It is required much more points. required much more points. So instead of 2n plus 2, it is required 2n plus 2 to the square points. Okay. And therefore, it's not so probably not so convenient for numerics. And we have made some few experiments, but what seems to be more adapted is to replace the martingale constraints. Place the Martingale constraint by assuming some moments again. So we have this constraint with functions chi L. Okay, and like this, we can on this problem you we can show that there is a minimizer that has only two n plus n point plus two points. Okay, I will not take okay. I will not take okay. I will not talk much more on this. And now I will discuss our results on the coverage of the moment constraint optimal transport towards the optimal transport problem. Okay, what we need are assumptions on the growth on the moment functions. Moment functions and as okay, we need that this quantity is finite, okay. Where theta nu and theta nu are functions that are used to bound uniformly the moments. And what is expected, so what we need is that the moments function that we are considering character they characterize the functions mu and u. So these are these. And you. So we serve this condition. And under these conditions, okay, what we can show is that, okay, when n is fixed, there is a minimum. There is a minimizer. We take here A0, which that is given by the quantity here. And there is a minimizer. And as n goes to infinity, there is a convergence of this minimizer to the optimal transport, the classical optimal transport problem. And so we can show that from a minimized sequence, we can extract a sub-sequence that converges in distribution towards a minimizer of I. Okay, I will not. Okay, I will not detail the proof, but it's a very classical argument. And then we have obtained some, in some cases, convergence trace. So, this here I present the case where the domain are simply the compact set 0, 1, okay. And we use a piece. And we use a piecewise constant moment. So we consider the following set, T1N and Tmn. So these are basically the intervals of length 1 over n. And the moment functions that we consider are simply the indicator functions of these intervals. Then the moment constraint optimal problem is simply given by this. We are on a compact set, so there is no need to. Are on a compact set, so there is no need to use the blow, the blow condition, and the optimal transport problem is simply this one. And we have the following result. If C is a Libschitz, then the moment constraint optimal transport approximate the classical optimal transport problem with a speed of k. Of the speed of k over n okay, and we have also proven, but it's more technical, some one over n to the square convergence rate for piecewise affine test functions, but only in the cases of w1, so this cost, and w2. Okay, now I will present Now I will present in the last minutes our results for the approximation of the multi-marginal symmetric optimal transport problem. So what we consider is a cost that is symmetric. So for any permutation, we have this identity. And this is particularly considered in a density functional theory for quantum chemistry. They usually consider the Coulomb cost to model the, for example, the interaction between capital M electrons. So they all interact with some repulsive force. And the cost, the typical cost that we are considering is this one. Okay, we consider pi s, so this is the so this is the the set of coupling okay so we have capital m marginals and we assume that all marginals matches the the capital n first moment of a given of a given low mu okay mu n stands for the moment with respect to phi n of mu okay and we Okay, and we also introduce this pi, which is the set of coupling, such that we only assume that this integral, the mean, okay, is equal to the integral of the mean of the phi n is given by phi mu n. Clearly, the first one is included in the second one, because if you have this for any M, little M. M, little m, then taking the mean, you get immediately this, and then we can the problem that we are interested in is this one, the Roman constraint optimal problem is this one. But by symmetry, at the end, it is the same to consider this on the second set, because if I have a solution of this, then by symmetrizing it, I get back. It, I get back a solution for the first problem, and therefore, the minimizer of i can be written from my previous from the previous theorem that I've stated, it can be written as a combination of at most two n plus two points. Okay, and then if we symmetrize it, we get a solution for this problem. This problem. Okay, now I present shortly a constraint of dump trans dynamics. So overdump transform is a stochastic gradient when V is a potential function. When you want to do the same, but assume that you stay on some manifold described by a function gamma, this is called constraint. This is called constraint over Dam Plangevin. And Stamelle, Eli√®re and Loga have studied the same kind of problem, but instead of assuming that you want to minimize the potential function, you assume that you want to minimize the expectation of the potential function. So this is why we are considering here the mean of potential. And also, instead of having a constraint exactly. A constraint exactly. This is a constraint in expectation where gamma k you see here is the mean of the gamma. So this has been studied. And this looks like the problem that we want to consider. Okay, so I do not detail, but here we do not consider variable weights, but fixed weights. Okay, we've noticed that it's better numerically, even if we increase the Even if we increase the capital K, it's better to do this than trying to optimize the weight. And then the minimization problem that we have to minimize the M-COT problem typically falls in the framework of constraint of Adam Prangevin with a potential function which is the cost and with the constraints which is described by our Constraints, which is described by our moments functions phi. Okay. And this is interesting because the constraint of Adam Plange has been studied and it is known that as t goes to infinity, there is some usually some ergo invariant distribution. And okay, this is very formal because we are not able to check the assumptions, but To check the assumptions, but our constraint of Adam Plangement should converge to this minimization problem. So try to minimize this. This is the entropic regularization that is used also for the synchron algorithm, for example. And so we try to minimize the same, but with this moment constraints. Now, just for two, three minutes. Just for two or three minutes, I will present our numerical results on this. So, first, to check that the algorithm works quite well, we have checked on a case where the optimal solution is explicit. So, this is a result by Colombo de Pascale de Marino, that says that in dimension one, for this cost or also the Coulomb cost, we Cost or also the Coulomb cost without the regular Islation epsilon varies. The optimal transport is given by a kind of transport map. And we have implemented this on this regularized column cost with this marginal. So we have made much more numerics, but only present a few results. So this is with five. So, this is with five marginal laws. Okay, we have taken Legend polynomials for the moment function, and we have plotted the empirical measure that we have obtained. Okay, so what is drawn here? So, this is with 10,000 particles with this coefficient of regularization and with 20 moments. In red is plotted the In red is plotted the exact theoretical optimal transport, and in blue, these are the points that are weighted by our approximation. Okay, and you see that it concentrates quite well around the vertical. Okay, and you see here, this is plotted how the marginal laws are fitted. So, this is with 20 moments. With 20 moments, then the same with 40 moments, you see that the marginal lows are better fitted, and you still are close to the theoretical optimal transport. Then we have applied this on a more physical problem in dimension three. So with 100 marginal. So this is quite, as far as I've understood, I'm not a specialist. As far as I've understood, I'm not a specialist from quantum chemistry, but it was usually numerical methods allowed to go up to m to equal to 10. So this method was able to go further. So we've taken, on this example, I take mu, the uniform law in the unit ball. The moment functions are tensor product of orthogonal polynomial. Orthogonal polynomial with respect to mu. And we have made a choice called Ipervoli cross to select the moments. Okay. And what we obtain, so this is the convergence of the cost in function of the number of iterations of the gradient descent. And just okay, it's not so easy to draw a probability law on. Uh, probability low on 100 marginals. So, what we have drawn here is the dependence between the first coordinate and the second coordinate of the same particle, of the same electron. So, let's say the x and y axis of an electron. And you see that you are okay. The darker it is, the more is the higher is the density. So, you see that at least it is not uniform and you have some. Have some pattern of dependence. And here we have drawn the dependence between the radius of two different particles. So all they are correlated. You see that this region is more weighted. So this is typically the kind of things that you can calculate. So thank you for. So, thank you both for your attention. And here are the references of all right. Thanks a lot, Avalier, for your nice talk. We have maybe time for a very quick question. I have a very quick one, which is from normal recombination when you try to compute a When you try to compute actually the points for Chakolov's theorem, I had the impression that just doing gradient descent essentially is not competitive, that you know, it seldom works because you have to find the axis, the points, and the weights. And then people cook up some Frank Wolf gradient descent, but it's not clear that it's really the minimum. Whereas you mentioned, if I understand you correctly, that it actually works well for your finding the minimizers. So. So yes, because in fact, yes, the fact is that it reduces okay, in even multi-marginal case, clearly it reduces some of the dimension that you are considering. And yes, it works quite well. Okay, there are some issues. You may have some plateaus. You may have some plateaus. Okay, there are some issues that I've not detailed here, practical issues to make it work. Okay. But yes, here clearly it can lead to competitive algorithm. All right. Thanks. Can you have a quick question from here as well? Yeah. Go ahead. Just have you tried or thought at all about doing numerics through the jewelry? The doing the works through the dual problem, I mean, the sort of extra type of approach. So, I think, I mean, we've tried this on the MLT with the call options, and that worked quite well as well. No, yes, we have only focused on the primal in this work. But, of course, it can be interesting to look at the other one. All right, just one more question by Simone. Yes, thank you very much. So my question is twofold. So one is how do you choose the moments? And the other one is so it seems to me that in the multi-marginal problem you're Your formulation is in some sense quite similar to what Friseke did in a paper of, I think, last year or something like that. So if you can comment on that and on the choosing of the marginals, sorry, the moments. Okay, for the second one, I'm not sure to be able to answer. I don't know whether. I don't know whether well enough the work of Friseger. Okay, for the choice of moments, it's quite, of course, it's related to the mu and you that you on the marginal that you are considering. So basically, since we were considering marginal on compact set, we've taken, okay, in the one-dimensional case, we've taken a Legend polynomials. Legend polynomials. Okay, it seems that orthogonal polynomials could be a good way. We have also tried some stuff with kernels, Gaussian kernels to smooth the indicators of being in some cells. Okay, this can lead to some interesting results too, but yes, I would say that the answer is usually it's good to take moments that are related to your marginal loss. So I have not an absolute answer. Thank you, thank you. All right. I think now it's time to switch to the next talk. So thanks again, Orion, for your nice talk. For your next talk, could you stop sharing your screen?