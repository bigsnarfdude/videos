During my PhD in the lab of Sergeant Ostoich at the ENS in Paris, in collaboration with Murdad Jazayeri from MIT and Nicola Merik and Hansem Son, postdocs who were postdocs at Islam. So this work is about flexible timing. So and I think one way of introducing this type of task is to think of the audience of a concert. The audience in a Of a concert. The audience in a concert can very easily track the rhythm of songs by clapping, and they can adapt very well to different time scales, ranging from a few hundreds of milliseconds to a few seconds. This type of tasks are theoretically interesting because the task itself has dynamics. It's the time scale of the song they need to follow. And this time scale needs to interact somehow with the dynamics. Somehow, with the dynamics, can you hear echo? Sorry, I'm not getting any echo, but nope, okay. I don't hear any echo. Okay, so I'll continue. I think I was saying that this type of flexible timing tasks are interesting because it has the dynamics of the task itself that interacts with the dynamics of neurons in the brain. So, one very So, one very remarkable aspect of cognition, animal cognition in general, is the ability to generalize to novel stimuli. In the context of timing tasks, this corresponds to generalizing very easily to new time scales that have not been seen before. And several authors have pointed out in recent years that this generalizable behavior is very hard to achieve in artificial systems. So we wanted to study how generalizable behavior can arise. And we started with a hypothesis that's based on recent experimental findings. In particular, several experimental studies have found in the last 10 years that when the activity of a population of neurons is recording while solving a cognitive task and it's taking into account altogether, they found that the Together, they found that the activity lies in a low-dimensional subspace. So, we wanted to postulate the hypothesis that maybe it's this low-dimensional arrangement of neural activity that gives rise to generalizable behavior. So, what we did in this study is to study this hypothesis in the context of flexible timing tasks. Context of flexible timing tasks. So, let me explain what type of flexible timing tasks we studied. So, we studied a series of timing tasks, and also how we trained artificial neural networks to solve this task, because what we want to control here is the dimensionality of neural activity. Feel free to interrupt me at any time if there's something unclear. So, what's common to all the timing tasks? What's common to all the timing tasks we studied is the fact that at some random time point in the trial, there is a brief signal that we call set that tells the network to start producing a time interval. So if this was an animal, that would tell the animal to start producing, to start counting time to produce a time interval. What we want in the artificial network is to have a readout, the output, which is a linear readout of the Linear readout of the firing rate of all neurons in the network to go from some initial state to some final state once this set signal arises. So we want this signal to ramp up to the final state with a specific slope so that it reaches this final state at the moment we call go. And the time between go and set is what we call the produced time interval. So, this is a flexible test because, in different trials, the network must produce a different time interval, and that depends on previous intervals, on previous inputs. So, that really depends on the specific task. And we studied first two tasks. The first one, QSECGO, it's based on experimental studies that have already been published from Adutya Zayeris lab. And what we're assuming here is that there is a What we're assuming here is that there is a constant input during the whole trial that we call Q, and the amplitude of this input is what determines whether the produced interval should be short or long. So higher amplitudes would correspond to longer produced time intervals. The second task that we studied is a bit more complicated because now there's no constant tonic input in the background. Tonic input in the background telling the network how fast to produce an interval. Instead, the network receives two pulses, two transient inputs, and the network needs to measure the time elapsed between those input pulses. And then there is a random delay, so the network holds that in memory. And whenever the set comes, the network must reproduce this sampled time interval. Time interval. So, what we will do is to train recurrent neural networks on a series of produced time intervals, let's say on four different output intervals. And the way we're going to test generalization is by looking at new inputs that the network has never seen before and seeing how this network generalizes. So, we will say that a network generalizes when it's able. network generalizes when it's able to produce when this input-output transformation is a smooth function and longer intervals or longer inputs correspond to longer intervals and shorter or smaller inputs correspond to shorter intervals so this is how we're testing generalization and now we want to control for the dimensionality of neural activity Of neural activity. So, we're going to train recurrent neural networks, and we're going to train two different types of networks. One network is just a vanilla RNN, what I'm calling an unconstrained recurrent neural network, where we're training each synaptic weight using backpropagation through time. And what's novel in this study is the other types of networks we're training, which are constrained to be low rank. So, we're taking the connectivity metrics, we're fixing the rank of We're fixing the rank of that connectivity matrix, and we're training the singular vectors of the connectivity. Also using backpropagation through time, but we're not really training the synaptic weights directly, only the singular vectors, so that during training, the rank of the connectivity is kept constant. We're doing this because the rank of, if a network has low rank connectivity, it shows low dimensional activity. So that way we can really control for the dimensionality. What we'll do is to train for different ranks and find the minimal rank required for solving these two tasks. So this is the setup we followed. And if there are no questions, let me move to the results. So we train on two different tasks, the QSAGO and the measure we go. And we train these two different types of networks, constrained and unconstitutional. Different types of networks, constrained and unconstrained. So, for this first task, where there is a constant tonic input in the background, we found that for the low-ranked network with minimal rank, in this case, rank two, the network generalizes well. So it performs well on the four intervals we use for training. And then the network interpolates well and is able to smoothly extrapolate to high. Extrapolate to higher amplitudes and lower amplitudes than those seen during training. However, when we train in an unconstrained way, these networks, they also learn to solve the task, but then they don't generalize well at all. At some point, they even stop producing the RAMs we're asking them to produce. So, this goes in line with our hypothesis that this low-dimensional constraint on the activity helps. Constraint on the activity helps generalizing. So then we looked at the same results for the measure wago task, and we found instead that there was no clear difference between the low-rank network, in this case, rank three was the minimal rank, and an unconstrained recurrent neural network. We found that these networks, they are just not able to produce intervals that are longer than the longest interval used during training. Interval used during training, both for low rank and unconstrained recurrent neural network. So, Guillaume is asking: what was the training stopping criterion with this experiment? So, we actually trained for a very, very long time and see if it converged or not. So, sometimes it would just not converge at all, and for a long time, it just wouldn't reduce the loss. It's true that. The loss. It's true that it's possible that we're not training long enough or we're not initializing the networks properly. So, this minimal rank, for example, is something that we found using always the same protocol, but maybe. But yeah. Maybe if we train longer or if we initialize differently, we would find different solutions. Also, Sara is asking: what about testing for Asking, what about testing for interpolation instead of extrapolation? So, we also see interpolation here, right? So, interpolation are the black lines between the colored dots. Those would be interpolations. And we find that all networks, unconstrained and low rank, they're able to interpolate within the training distribution. So, it seems that the difference is more for the extrapolation. And so we found these results that are in agreement with our hypothesis that the low-dimensional constraint helps for generalization, but it wasn't the case for this other task. And the difference is the structure of the inputs. So in the first task, the input is tonic, while in this measure weight go task, the inputs, they're only brief pulses. So what we did was to update our hypothesis. What we did was to update our hypothesis and say that low-dimensional neural activity might allow for generalization in flexible timing tasks when there are tonic inputs that control the system. So there is another question from Alexandro Peyer. Is there a random matrix added to the low-rank connectivity? No, we're really training a pure low-rank network. So we're really fixing the rank and there's no random. Fixing the rank, and there's no random connectivity added on top of that because we really want to make sure that the activity is low-dimensional. Great. Okay, so now we wanted to test this new hypothesis that when you have a tonic input, then this low-dimensional constraint might help generalizing. And for that, we designed a novel task, which is measure wake-go. Which is measure wait go task, but now we're adding a contextual tonic input, so it's very similar to the measure wait go. There are two pulses that are given to the network. The network must measure the time between those two pulses. There's a delay, then the set comes, the network reproduces it. But we're adding a fourth input that we're calling context. And the amplitude of this input tells the network whether the sampled interval comes from a distribution of short intervals. From a distribution of short intervals or a distribution of long intervals. So, actually, this contextual input is not necessary for some tasks. A network that perfectly solves the task could just measure the interval, reproduce it, be perfect at it. But this input is giving additional information, saying, because this input is constant and tonic, therefore the whole trial duration just tells the network prepare yourself because the interval. Prepare yourself because the interval is probably going to be on the shorter side of the distribution or in the longer one. So we train both unconstrained and low rank. Again, the minimal rank for this type of task was three. And maybe let's start with the unconstrained network first. So we found that here now there are two curves for the input-output transformation. One corresponds to the One corresponds to the fast context and the other one to the slow context. We find that networks are able to solve the task for the eight trained intervals in both contexts very similarly, and they interpolate well between those classes. And now when we ask these networks to produce longer intervals, they just give a very strange response. And we actually found by training different networks that there's a lot of heterogeneity here. Of heterogeneity here, but in general, they don't extrapolate well. Then, when we looked at rank three networks, the low-rank constrained networks, we found that these input-output transformation, it always looks like a sigmodal function. But what the context is doing is parametrically changing the linear range of this input-output function so that it really matches. So that it really matches the intervals for that context. So it seems to be the case that now at least this contextual input is having an effect in a run-constrained network. And the question is now whether these networks, they're also able to generalize to context they haven't seen during training. We only used the context, but we can now test for new contexts. So we first tried for context. We first tried for context with amplitudes larger than those seen during training, and we can see very well how the output of the network produces is biased or the range changes towards longer output intervals. And the same thing happens now when we try novel context that interpolate between the two amplitudes. We see the same thing here. The same thing here. And even for negative contexts that haven't seen, haven't been, have never been seen during training, they also produce responses that are shorter. So it seems that just by imposing this low-rank constraint, the network is taking into account context and it's able to generalize to this input. I thought there was a question. Question. Okay, so this goes in line with our hypothesis that low-rank, this low-rank constraint helps generalizing whenever there's a tonic input controlling the system. And the question is then, what is the mechanism underlying this type of generalization? One advantageous thing of double-rank networks is that they're easy to reverse and Easy to reverse engineer. At least for rank two and rank three networks, we can visualize the dynamics that these networks generate. So we can look at the dynamics for the QSEGO task. This was a rank two network, so it's two-dimensional dynamics. You can think of this latent variables, kappa one and kappa two, as the principal components, for example, of the recurrent activity. Activity and for the mutual weight go task, we had a rank three network, so the recurrent activity will be three-dimensional. I'll start explaining the QSECGO task. We can visualize the flow field of the dynamics, and we found that there are two stable fixed points, two saddle points, and one unstable fixed point here at zero. But what's interesting is that no matter where you initialize your network, the activity will very quickly go towards the Will very quickly go towards this red line here that I'm calling a neural manifold. That's a non-linear neural manifold. And then the activity will evolve slowly along the neuromanifold until it reaches some stable fixed point. In the RAM3 case, it's harder to visualize the flow field, but we found a very similar thing. Instead of having a ring-like manifold, now in 3D, we found a sphere-like manifold. This sphere-like manifold, but the dynamics are the same. So, no matter where you initialize your network, what happens is that the activity goes very quickly towards the surface of this manifold and then evolves slowly along it. And this is interesting because when we plot the trajectories of the network when they're solving the task, we found that all the time they just evolve along this neural manifold. So, for example, for the This neural manifold. So, for example, for the QSEGO task, the activity is at the beginning at one of the stable fixed points. And then, when the set comes, well, this transient inputs will push the trajectories away from the manifold, but very quickly they will converge back to the manifold and then slowly evolve along it towards the other fixed point. And it's this part here that generates the ramping output for solving the task. And it's harder to see for the mission. And it's harder to see for the measure weight go task, but it's basically the same thing. The trajectories go away from the manifold whenever their input pulses, but then they very quickly converge back to the manifold and evolve slowly along its surface. So this is the first ingredient: the fact that the recurrent connectivity is generating these nonlinear manifolds for solving the task. The second question is: what happens now when we add? Is what happens now when we add this tonic input in the background? What happens to these manifolds? So, I was showing here the example for fixed Q amplitude, but now we can plot the manifolds for different Q amplitudes for the QSEGGO task. And for the measure-weiggo task, now I will look at the manifolds for different contexts. Before, we only had the fast context, and now we can add the new context. And what we found is that these manifolds, there are That these manifolds they're basically invariant or largely invariant to changes in the background amplitude in this tonic input. So here for the QSIGO task, we found that especially the part of the manifold that is explored by trajectories doesn't change when we change the background input. And the same thing for the measure-away-go task. So it seems that this mechanism is based on having. This mechanism is based on having a non-linear manifold that is whose geometry is invariant to changes in the background input. And there is still something that must change because of this background input because they're necessary for solving tasks and the task performance is different. And what is changing is the speed of the dynamics along this. Of the dynamics along this manifold. So, when we look at the average speed of the dynamics along the regions of the manifold that are explored by trajectories, we find that as the Q amplitude is increased, the speed decreases. So, basically, what these networks are doing, they're producing the same dynamics, but they will do so at a lower speed. And that's how they're able to generate intervals that are longer. Intervals that are longer or shorter. So that's the key mechanism for flexibility. Based on this, we then went to the data, to some experiments on very similar tasks in Murdaja Zairi's lab. And we wanted to check whether the geometrical arrangement of trajectories in our low-rank models and in the neural data are consistent or not. Data are consistent or not. I'll start with the low-ranked model. One of the first things we see just when we apply principal component analysis to trajectories is that this is the activity during the measurement epoch. So, right after the first pulse, when we tell the network to start measuring time. And what we see is that trajectories they're grouped by context. So that, well, since this is an obvious. Since this is an obvious thing in our model, because we're assuming that there is a constant input in the background that will have different amplitudes for different contexts, so we can see here that along what we call the contextual dimension, trajectories are grouped by context. And then if we project out this contextual dimension, what happens is that trajectories explore the same invariant nonlinear manifolds. Invariant nonlinear manifolds so that they largely overlap in neural state space. And what we can do to make a more quantitative analysis is to correlate the projection of neural activity at the beginning of measurement of this contextual dimension and see whether it's correlated or not with the speed at which trajectories evolve along this recurrent subspace. Recurrent subspace. So, this is indeed what we found. These are the two contexts here: the slow context and the fast context. And we see a strong correlation between the speed of trajectories during measurement and their projection onto the contextual dimension. And since this is a model, we can also try intermediate values for context and we can draw this relationship between the speed and the projection. Speed and the projection onto context. The question is whether we found the same thing or not in the data. And we found that the data shows very similar signatures. So if we just do PCA and we plot the three principal components of the data during measurement, we also found that trajectories are grouped by context. And when we project out the single dimension, we found that trajectories are different. Dimension, we found that trajectories largely overlap with each other in this two-dimensional space. And then we can look at how the speed is correlated with the projection on two contexts. And we also see this strong correlation between these two contexts. Now, in the experimental task, there were only two contexts. So we cannot map the full transformation between the projection and context and the speed. But we found very similar. But we found a very similar picture in the neural analysis and in the low-ranked network. So with this, I would like to finish. So to summarize, we found that low-ranked networks, when they're driven by tonic inputs, are able to extrapolate to novel stimuli in the context of flexible timing tasks. And the first thing we found when we reverse engineered these networks is that they generate. These networks is that they generate non-linear manifolds for solving the task. Hamtonic inputs keep this geometry of these neural manifolds invariant, but modulate the speed of the dynamics along them. Analysis of neural recordings in monkeys showed the very same signatures in the geometrical arrangement of trajectories that we postulate in our low-rank train network. Our low-rank train networks. We also did some theoretical work where we designed by hand low-rank networks that reproduce the same mechanism. So if you're interested, I'll be happy to talk more maybe during the questions, or you can also check the preprint. And I would also like to thank the collaborators in this work, Sergian Ostej, Murdad, and Nicola Mereg, and Hansem Son, who did the experiments. Who did the experiments and performed the analysis of the neural data? Thank you very much. Thanks. Thanks, Benuel. Okay, so I saw a question pop up from Sarah Soya. Do you have an intuition of why the required input is tonic? Yeah, that's a good question. That's a good question. So I think maybe the intuition is what happens when the input is not tonic, when you just have brief input pulses. And what happens in that case is that everything needs to be generated by the network. So what we're basically asking these networks to do is to generate a very slow signal. And when the input is atomic, they're learning to generate a slow signal. Let's say. Generate a slow signal, let's say of, I don't know, 10 times the membrane time constant, but they won't generate a signal that's 20 times the membrane time constant. In a way, they're not learning a generalizable behavior. They're really carving out the connectivity so that they can produce a very slow signal, but they won't produce an even slower signal. They don't really understand the mapping. They're just able to produce this slow signal and they're This low signal, and they're lazy, they will just learn what you tell them to learn, and they will stop there. I think that that's really the key. Okay, thanks. And then I think Guillaume had a question. You can just unmute if you want, Guillaume. Awesome. All right. Thank you. Amazing talk, Manuel. Thank you for sharing this. I have a question regarding the interaction between the dimensionality or complexity. The dimensionality or complexity of the task and the actual solutions that you found. So, as you're aware, there's like a great work that's been, you know, came out recently from the Sicilo clan showing universality of these solutions for these simplest, simple tasks, right? Where you find the same manifolds regardless of network architecture in some sense. And I wonder here if you think that the solutions that are found here are universal to the task. Found here are universal to the task itself. And if you've verified that the low-rank structure is, in a sense, a tool to help training go faster. You know, that's why I was asking earlier, there might be some, the same solutions found in higher dimensional or higher rank networks. And in a sense, I'm wondering if this is evidence that low-rank connectivity helps to learn these things faster, or if really there's low-rank connectivity. Or, if really there's low-rank connectivity in the brain, that's evidenced by the similarity between the solutions that you find and the neural data. So, have you thought about these questions a little bit? Yeah, that's a very good point. And I really think the way you think about it, so to me, this low-rank constraint is just a way of regularizing the solutions. And probably, if you train a high-rank network, Kavanila R. A high-ranked network, Kavanila RNN, where you change the regularization, or maybe you add more noise to the inputs or something, you get very similar solutions. What I can say in this case is that when we train high rank, we don't find the same type of solutions and they don't generalize. We also did, because we're training high rank, but that doesn't mean that the dimensionality is going to be high. And actually, we found that the dimensionality of this full rank networks, it's also pretty low. So, what I'm showing here. Low. So, what I'm showing here is the cumulative variance when we do principal component analysis of the low rank and the full rank. And they also look like low dimensional activity. It's not as low dimensional as with minimal rank. But we do find that these solutions are not the same because one generalizes and the other one doesn't. Yeah. But that's we do think that there are other ways of maybe imposing this generalizable behavior. That's just hard. Yeah. That's a hard contrapositive, right? Because you sh there might be tricks to train full-rank systems that lead you to these solutions, but it's kind of an infinite space of tricks that you need to be exhaustive on. So it's a difficult question to answer, but I see your point, right? Yeah, I agree. In a way, we found one trick, but I agree there might be other tricks. And I mean, the brain is probably closer to a full-ranked network when you start. To a full-ranked network when you start learning a task than a low-ranked network. So, yeah, I agree. Cool, thank you. Yeah, and then I see a question from Christina. Is the geometry of the solution in the unconstrained network qualitatively different? Yeah, so it is qualitatively different. Maybe not. Um, maybe not in this picture because this is something that you will always get whenever you have a constant tonic input in the background. But this picture is different. So we don't really find this invariant manifolds for the high-dimensional network. What we find is that for each output interval that this networks learn, there's a different trajectory. So that when you try something in between, they tend to be able to do that. Something in between, they tend to go to the trajectories that are learned. So in a way, we found that there's a different subspace for each trajectory that is learned for these high-dimensional networks. And that's part of the reason why they don't really extrapolate in the QSETGO task. So, yeah, I think the global answer is that there are some similarities when you train an unconstrained network, but there are also. Constrained networks, but there are also some qualitative differences between those two types of networks. Great. And then Pavel had a question, a couple questions. How do you actually train the network? Can you elaborate more on the process? So maybe just more about the training. These networks, they're pretty big. They have about a thousand. They have about a thousand, yeah, a thousand neurons. And we're just training using back propagation through time. So standard techniques with PyTorch, the key point. Sorry. Yeah. So the key point are the parameters that are trainable and those that are not trainable. So we're training the input patterns, the output patterns, and we're also training the recurrent connections. Training the recurrent connections. So in the out-constrained network, we're training each recurrent connection. So each connectivity weight between a pair of neurons. And so we run the full trial and then we apply gradient descent with the standard libraries. The difference is for the low-rank network. So for the low-rank network, we are initializing random low-rank connectivity metrics. Low rank connectivity metrics, and then we're training the left and right singular vectors. So we trained a rank one network, then we would be training two vectors for the recurrent connectivity, the left singular vector and the right singular vector of the connectivity matrix. So instead of training n squared parameters for the recurrent connectivity, we would be training 2n parameters. Great, yeah. Yeah, and if you know, folks ask questions and they want to ask follow-ups, feel free to just unmute. Any other questions outstanding for Manuel? I had one. I'll go ahead. So yeah, it's very cool talk and yeah, really nice mix of. And yeah, really nice mix of modeling and experiments. So I had a question about the dynamics, the low-dimensional dynamics. So you had this picture where you showed the face plane and it looks like you have these heteroclinic paths between these saddles, I guess, and stable nodes. So I had. So, I had sort of a couple sub-questions about that. So, one is: how fragile are these if you add noise or if you disturb the network, like if you just add white noise to the differential equations? And then the other question was, what does the next most stable dimension look like? So what's creating the slow passage? The slow passage? Is it that there's some other structure that's causing the flow along the heteroclinic path to be slow? And if I was to perturb the system, it would move a lot faster. How close do you have to be to that heteroclinic path for it to move as slow as it does along the path? Okay, so maybe for the first question, which is Okay, so maybe for the first question, which is about the noise, how robust these structures are. Yeah, they're very robust. So we're actually training with noise. That's something I should have said also in the previous question. We are adding some random noise during training to the units, some independent private noise to each unit. And we can also crank up the noise after training. And it barely interacts with this non-linear manifold because, well, this noise is high-dimensional. Noise is high-dimensional, and so the projection of this high-dimensional noise onto this low-dimensional structure would be pretty small, and then you have a little effect. So, yeah, for example, here we're starting at this initial stable fixed point, and there is noise in the system, but the trajectories are not moving away from this fixed point, so they're fluctuating around this value. There's also the question whether these structures are robust to noise and the connectivity. And that also seems to be the case. So, if you add noise that's completely uncorrelated with this low-rank structure to the connectivity, in general, it won't have a major effect on this neural manifold. And then, the other question, if I understood correctly, is about the dynamics along the heteroclinic. Dynamics along the heteroclinic orbit. So, how slow the dynamics are here. And yeah, can you phrase it again? What did you mean? So, yeah, you have this, you have slow flow if you're right along the red line, but you could imagine moving off that red line, both in Kappa 1 and Kappa 2 space, but also in other dimensions that are not pictured in the slice. In the slice. So, how, I don't know, how robust is the timing of that slow passage to moving off in the many different directions there are in the R. So, for other directions, that one's easy because perturbations along other dimensions, are not the Kappa 1 and the Kappa 2 dimension, will just decay back to this Kappa 1, Kappa 2 plane at the time scale of the membrane time constant. So, that's something that we can assume. That's something that we can assume that if the time scale of the membrane constant is not interacting with the time scale of the task, it doesn't matter. And then for how fast the dynamics change here, well, if you step away from the manifold, you will quickly converge towards the manifold. And that's something that is trained. So you can actually train networks to be more specialized, to have slower dynamics to this region, and to decay faster to this region. Faster to this region, so I think they just learn to be as slow as they need to be along this manifold, and as fast as they need to be, they move away from it. So, that's something we haven't tested, but probably if we train with stronger noise, we would see that the dynamics here are faster so that they can converge quickly and are more robust to small perturbations away from the manifold. Gotcha, yeah. Yeah, I almost wonder if you could do some. Yeah, I almost wonder if you could do something. I mean, it's not an oscillation, but something like a phase response analysis of this 2D system, perturbing off, you know, how much does that disrupt the timing of the flow? Yeah, so we have theoretical models that reproduce this structure, where we say what is the correlations required between singular vectors to produce this type of dynamics, and we also find the parameters. And we also find the parameters that set the speed along the manifold, not the speed outside the manifold. So that's something that you can control easily by changing the types of correlations in the connectivity. Great, thanks. Well, let's give Manuel another round of applause. Thank you. You can use the buttons in Zoom or feel free to unmute. Zoom or feel free to unmute. All right. So thank you so much, Manuel. And our next speaker is Nick Waters from MIT, who will actually give a