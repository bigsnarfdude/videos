Thanks very much, Alex. And thank you, you the organizers, Caroline and Mark and Hernando for the organization and inviting me here. And so my talk is about the joint modeling of multimodal imaging data. And in particular, here is a structure imaging data vector and the functional connector matrix. So since we have Since we have multimodal imaging data available in all the large consortium data, so it's convenient for us to investigate the association between different imaging modalities. This could be useful because it can tell us, for example, what structure imaging features can influence the functional patterns or functional connectome or function or functions of brain. Functions of brain. So, potentially, we can know if there is a lesion or damage in brain structures, what could be the potential impact on the brain functions. So, the joint modeling of the functional brain imaging data and the structural imaging data could be interesting and helpful. And however, this analysis of the multi-model imaging data is challenging because you have multi-variant imaging. Have multivariate imaging features at the outcome and multivariate imaging features at the predictors. So, which could be very challenging. So, here for the imaging data, we mainly consider the two patterns. And first is the vector pattern. So, for example, each subject, you can list each feature into a factor, all the features into a factor of the structure imaging data, or even some. Structure imaging data, or even some function imaging data, but it's in a vector format. And since many of WAPFAS are working on functional connectivity, for the connect home data, particularly in a region-level analysis, we can store the connectivity data in a covariance matrix. So each of the diagonal elements is a variable of interest. So, in this study, I just want to use this. I just want to use this functional connecting convariance matrix as outcome and these vectors as predictors. So this could be so in that we can just first have an impression of what's association, what's association like. And to get a general view of the association, A general view of the association, we can first just translate all the off-diagonal elements in the function connective matrix into a vector and then do a vector-to-vector association. Then we can easily get a rectangular matrix that with each element shows individual to individual association of functional imaging features, FC features to SI features, structural imaging features. I feature structural imaging feature. So here in this here in this heat map metrics, I would rather use my finger index. So each one shows what's the association level between the SIF variable and the FC variable. So it's just like okay. Okay, that's good. Thank you. Thank you. Okay, great. So it's a lighter means there is association and there is a blue means there's less association. So it could be challenging even look at the individual just pairwise comparison because it's easily at the middle. It's easily at the minimum there could be millions level because just think about the connectivity. If you focus on Atlas with 200 or 300 regions, there could be more than 30,000 edges variables. And the structural imaging variables could be hundreds levels. And you time these two together is easily at least millions. You can just, if you do a final resolution, it could easily go up to It could easily go up to billions level. And even you do a mass univariate method, and we always criticize that. It could lead to very large sample size to have multiple corrected inference to have some information left there. And furthermore, even you have a very large sample size and the interpretation could be very difficult because if you have SI and link to FC. SI and link to FC Edge, and then how to explain that? You don't have a systematic view of that. And so, some traditional techniques we can do is, for example, we can do some dimension reduction. For example, we can just first do dimension reduction to the connectivity matrix and then decompose that into a few components. And for each component, ideally orthogonal, and then we can do the And then we can do the regularize the regression or panelize the regression on each component. For that, we just could lose special specificity and sometimes could be difficult to interpret what the association really is. And another method, a set of methods, is a popular kinetical correlation because this is some technique we know to handle multivariate to multivariate association. But the kinetical correlation, if you do for the whole use of the traditional kinetical correlation analysis, you can lose the specificity because you have weights on each of the features. So you don't know where the signal comes from. And if you have the sparse version of CCA, you could just mitigate the association level because you do the sparsity, you have biased estimation of the association or correlation. Or correlation. So, the strategy we propose is generally two-step procedure. So, first is a pattern recognition procedure is just like we first get the pairwise comparison, pairwise association analysis as this matrix, and then to recognize whether there is some pattern from there. So, the idea we propose is a multi-layer network or some. Or some my student named that MOAT is a multi-network association method. So it's a kind of a pattern recognition method. Essentially, it's just like you can note these two metrics actually are just isomorphic. So it's just the difference is just you shuffle the order of the nodes on the SI and FC. So this is the FC, this is three, could be 30,000 or something, this could be the hundreds. Of something, this could be the hundreds, and then after you shuffle to certain levels, you notice there's a kind of a dense pattern there. And because this is just SI variables, this is FC variables, and then another layer variable, because you can see from that, because FC is coming from a network, connect to network. So another layer is to see whether this FC actually is coming from a dense network. So there are two levels of network. So that's why. Levels of network. So that's why we call the multi-layer network. First is to each FC edges association with the SF predictors. And then the second level is to see the selected edges whether they can become a network, organized network in the FC space. So first we come with some notation that so here we just use a So here we just use D as a number of subjects because we easily spend out all the alphabetic letters, Greek letters. And then we use Ij to indicate the kind of two nodes of functional connectivity, FCH. And then, for example, we have N nodes in the FC matrix. Then we have just n choose two edges. And then the number of outcome is just n choose. Tom is just and choose two. And then we indicate F the H by the Yij D. And then for each predictor as I measure, we just use that X K D. So then we for association between FCH and SI, we have the beta IJK. So it's just indicated the association between the FCH and Between the FCH and the structural information. And then to do the pairwise association, just the kind of screening purpose, you can just consider just a regular regression for that. And then our focus is just for the pattern recongrelation is to say, we want to see where is the beta ijk not equal to zero and whether there is a pattern of this could be. Whether there's a pattern of this could be existing in the overall association patterns. So the Martin Layer graph model is essentially can be considered at the first level as a bipartite graph model. And for a bipartite graph model, we need to define the two set of nodes. So for here, is the two sets of nodes is the first set is the SI predictors and Predictors and the F is the functional connectivity edges, and H is the kind of a connection between them. Specifically, we let the kind of beta ijk equal to zero is no connection. Beta Ijk equals not equal to zero means there's an association, there's a connection between them. And then each node of this bipart graph can translate to edge in a functional connective space. In a functional connective space. So there is a that's how we come up with the multi-layer graph. So this is the layer one, this is the second layer. So some basics of the bipartite graph is just think we have 5% of the edges are non-zero. Then for one node in the SI, we suppose to have the total number of Have the total number of nodes here. We have this one can be expected to connect it to 5% of that. And if you consider two nodes out there connected to the same set of the FC here, the chance could be simply, if this is a random graph, it's 5% times 5%. And then if you enlarge the number of SI share the same set of connections to here, the number quickly goes to zero with the number of Go to zero with the number of SI increase. So, and which makes the problem more interesting is the FC, if you constrain the FC can automatically become a network in the FC space. So the probability of these two events combined together is very rare. Just like you buy a lottery in the US and then come back lottery in Mexico, you win both of them. So it's like a very rare event. It's like a very rare event. So, in the general model, we just consider that that's something, the rare event is something we are looking after. And then for the pattern, we generally have a kind of multi-layers, general kind of graph model is a multi-layer model. It's just like we have a subgraph here. So each circle is a subgraph. So each circle is a subgraph, and all the I connected to the same set of FC, and FC amazingly just become a network in the FC space. So this is kind of a way to find formally defined that what a sub network would be. And so detect the subnetwork of the multilayer subnetwork is somehow similar to detect the cluster. Somehow similar to detect the cluster in the traditional fRMR data analysis. Just in the traditional fRMR activation analysis, the cluster is just floating there in the 3D space with the spatial adjacency. It's much easier to grab that and make some inference, just like June and many of our others yesterday's talk. But Armin's talk. But here is the subnetwork is like a cluster. Sub-network is like a cluster, but in a multi-layer graph space, and which pose some challenge on the extraction and the inference. So since we look at here, there is some community-like format, but it's not really a community. And all the byclassing community detection method may not applicable here because you can see a vast area of here is just black. And then if you use community detection, you may miss the patch. Community detection, you may miss the pattern. So, we are kind of focused on the so-called dense sub-graph extraction method, which is a pattern recognition method that is widely used in the computer science and machine learning community. And so the first, this is from Wikipedia. The basic definition of a density of subgraph is just like I say, all the red edges is a subgraph. The density of a traditional definition. Density of traditional definition of density of that is the number of edges in this binary graph divided by the number of nodes. And so how to seek the maximum subgraph with the maximum density is a kind of a popular area in the computer science. And so if we look at the density from a statistical point of view, I think you can consider the denominator actually is L0 norm. Actually, is L0 now because this is the number of potential edges you can number of nodes here. If nodes you take a square, it's essentially the LA0 shrinkage of beta IGK in that space. So it's linked with the kind of LA0 shrinkage of the object function. So our group did some work on that. And so we So in Wu's work with my former PhD student, we have a more adaptive format with a tuning parameter, just like we put the tuning parameter on the LL0 shrinkage. And then we provide the inference based on the dense graph discovery to show how to make inference of a detected subnetwork. That's by my paper. And then we have an extension to bipartite graph and in different scenarios. Different scenarios. So, this is actually some concept of the dense graph. So, we think there are some subgraph is very, very informative, but latent. And you use different strategy of a dense graph sub-extraction. You re-organize the order of the graph. You can see this is a dense graph without tuning parameter. This is graph with a tuning parameter. We want the tuning parameter because certainly there are some false positive information here. Positive information here. We want to get rid of the false positive noise and make the finding more reproducible. And this is our target, but we can get a similar estimation strategy by using by defining the kind of density. So you can see here is the tuning parameter, which is a kind of tuning parameters, which we can just rewrite this into the LR0. LR0 should I press okay fine? Yeah, I just so um so for the objective function here is just based on the inference levels of a beta ijk we get here to recong as the pattern. So we want the two layers of a dense stop graph with a tuning parameter. The tuning parameter is like a tuning parameter like at the L-zero shrinkage. Zero shrinkage is normal in front of that. And then we just implement that using some gridding algorithm that doing intuitively just extract the number, a set of number of subgraphs and make sure each subgraph is dense. And the complexity is a grading algorithm essentially is so we could. So we call it greedy because each time we just kick out one node, the node has the least association with the other set, SI or FC. So each iterative will kick one of them iteratively, and then at last we what left there is the dense graph of our target. So by using this graph, certainly it's very fast. So you can see this is the complexity is almost linear. And then even you have very large data. And then even you have a very large database, you can finish that in three minutes. And then, which could be good if you, because permutation test is popular in neuroimaging research, it's kind of can be compatible with the permutation test. And we prove something about the consistency of the estimate and show some of our approximation properties. So the consistency is pretty clear when the sample size goes to very large, then the This pattern will be very different from the other patterns. I think this is very unique. And so there are some few points I want to mention is it's kind of because we are detecting a pattern that by nature it can just surprise the false positive noise because it's SI, for example, SI feature can be false positively connected to FC edge. To FC edge, but it's very difficult to connect it to a set of FC edge. So the false positive just won't come very systematically. So by nature, it can just suppress that. And also it accounts for the dependence because just the correlated shows and columns, just the two SIs, they are very correlated with each other, then their association profile on the FC will be very similar. So they are more likely to become. So, they are more likely to be covered by one subgraph. So, this is implicitly accounted for the association. And also, sometimes they naturally also borrow power, just like if you have some false negative by one FC pair to twice I, but if it's a false negative, but the other neighbors will be positive. So, you again power that. So, next we'll talk about the next. So, next we'll talk about the statistical inference. Is just first, we want to test whether there's at least one just a sub-graph could exist. And on the null hypothesis is just a random graph. And so we derive a lemma on that is to show that the probability of a subgraph actually is related to the density of both in the first layer. Of both in the first layer and the second layer, and the sizes of first layer and second layer. So you can see it will probability of that existing exponentially just goes down with the density and the size. And so which we have that, just every time we select the maximum subgraph and plug into this formula, we can have an exact test. So we know exactly what's the probability of. We know exactly what the probability of a detected subgraph is. And then if this is less than the error level alpha, then we can reject that. And so sometimes just like that is difficult because we can have multiple subgroups. So for that, we have to follow just class-wise inference. We need to do permutation test. Essentially, we just, but in the permutation test, we can. In the permutation test, we can say that the test statistics is essentially borrowed from the probability of how rare will be the VC. So it will make more precise inference. And before that, after we extract the subgraph, we can perform some traditional multivariate data analysis, just like a different version of CCA. A different version of CCA or shrinkage regression to seek the joint association between the FC matrix and the SI vectors. So we will first talk about data and then we talk about the simulation. And then we actually process all the imaging data of the UK BioBank. And then we have just, but some data are missing the FA data, some data, most of Data, some data, most of a lot of them missing actually cortical thickness. And here we just use FA from DWI and the cortical thickness as the SI features. And then we just use FC outcomes from the brain netum atlas as commonly used with 246. And then that's our dimension of. That's our dimension of y and x. And then we just use the commonly used negative log pijk for the pairwise comparison, pairwise association analysis parameter. And then so that's the data look like. And then that's the input of the W matrix. And then that's kind of the FC cluster and the SC cluster we grab the subgraph. We grab the subgraph. And interestingly, so the numbers, so you can see this is proportional. So this is about 100, this is 30,000. So here we have 79 nodes. And over there, we have 23 SI features. The 23 SI features, most of them, 20 of them, actually. 20 of them actually are from FA. So it's from a white matter microstructure integrity. And then only three of them are just cortical thickness. And so this is just like the distribution of inside of outside distribution of a t-value. And there is a definitely a positive association. So this is, I think, is just the average association. Average association between them is surprisingly high. So within the subnetwork, we have a 0.0, 0.8 to 0.7 association level. And so we also perform within this, we perform kinetical correlation. And then you can see that the correlation from them are very high. So because we did the pattern recognition first, so we know there's an association between them. There's an association between them. And then, here is if we do the whole matrix, just like a whole SI and FC, we do the sparse kinetic correlation analysis, the correlation level is very low. And so here we just show the just the SI patterns with FC patterns. I think the FC patterns is more interesting. It covers all cedance network and temporal from Temporal frontal network, and what's the other one? It's a pareto-temporal network. So, basically, a lot of them are related to the cognition process. So, somehow we already know that some part of the FA is very associated with the cognition. And maybe in the future, we can do a mediation analysis to see whether the FA to the F C can lead to the cognition as a mediation. So, we also did the simulation analysis. So, in general, we just generate the two kind of the patterns like this. And then each of this is coming from the different burn connecting some network. And then we have input matrix. And then, based on this, we generate our X and Y and then compare our method with. Compare our method with different backlasting methods and sparse CCE method for the pattern recognition. And so here is our method to somehow to recover that. And this is just the three commonly used by classing method. And here is just two sparse CCCA method for the purpose of pattern recognition. Pattern recognition so the dense subgraph extraction seems to be kind of overperform the kind of byclassing method of community detection methods or the sparse CCA methods. And somehow, yeah, here we summarize the sensitivity specificity for the FC and SI and all the pairs. So, yeah, and also we just summarize the computation time. And most of the algorithm tends to be pretty fast. It's just two or one minute, just with some bypassing algorithm could be slower. So how many minutes do I have? I lost track of time. Yeah. Okay. Yeah, yeah, yeah. So it's just a discussion. I don't want to go back. So basically, we propose some strategy. Propose some strategy. So, just one strategy is there could be other more complex models to deal with that. It's just to handle the matrix outcome with vector predictors. And our strategy is to extract the pattern first, then we can examine the association. And so, the first two points. And then the algorithm we develop is based on the pattern recognition, then subgraph extraction. Sub-graph extraction, and then we add several tuning things, such like how to select the tuning parameter objectively and add the tuning parameter at all the LL0 norm, and then add another layer of the sub-graph destruction to recognize the pattern. And it turns out to be very accurate and computationally efficient. And then we can do inference of the pattern as well. So that's, yeah, that's also a summary. Has a summary, yeah. Okay, so I think in the interest of time, we probably can take one question, but if Jerryne can get set up in the meantime, that would be good as well. Sorry, I'm where there's technical. So, have we got any questions for Cher? Yeah, Mark.