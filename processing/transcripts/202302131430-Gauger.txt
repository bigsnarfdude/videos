Thanks to all the organisers for the very kind invitation. Real pleasure to be here in Utah Bans again, because I feel very lucky to come here for the second time. I was here in 2019, the meeting which Marco also co-organized, and that was on charge and energy transfer at the meeting. So I was sort of feeling nostalgic. This is sort of almost, this is a slide I dug up from three and a half years ago, and I think it's actually, it sort of does lead me into the topic well. Lead me into the topic well. So, what I'm showing you here is this is a single molecule, this is porphyrin, it's connected to two graphene leads, and then there's some messiness here that's the sort of chemical linkers which make it bond. And then, what you can do is you can apply a bias across these leads, and you can actually have a bottom gate that's not shown here, but you can queue the molecular orbiters and you can arrange the charge transport through this device. So, this is what I was talking about a few years ago, and I won't go through the details, but we do. And I won't go through the details, but we derived a Master equation and we call it a generalized quantum Master equation, it sort of gives you different limits. I don't really want to talk about the details other than here's what we did with it a little bit later. And this is collaboration with people in Delft. This is a different type of atom between two gold leads. It's suspended, so it's not a big thermal contact. You can kind of heat the two sides and you can look for the thermal current. And in order with your Current. And the model we derived gives you a pretty good agreement, I'd say, between a measured thermal current and a theoretically calculated one as a function of the gate voltage that just shifts your molecular orbitals around and the applied bias voltage. So why is it interesting? This is a really messy system. It's strongly coupled to more than one type of environment. It could be strongly coupled to the fermionic leads. It's got vibrations, it's got a molecular reorganization, and what we found is also got spin. It's also got spin. So there's at least three different types of environment. And that is, we kind of dealt with it in that master equation, but in that case, our model didn't work for all the molecules. It only worked for the ones that were relatively weakly coupled to the gold list. So now I'm talking about tools which may allow you to actually look at strongly coupled systems and look at systems which have more non-markothing dynamics. In this case, there's nothing really non-marking because all you're interested in. Really, not my coding because all you're interested in is a steady state curve. Okay, so this was just this motivation. Here's a little bit of an overview of different techniques for modeling open quantum systems dynamics. It's a selection of stuff I've worked on. It's certainly not non-exhaustive, right? And it doesn't include many good examples. But if you take Bora and Pitt would show it and follow the standard recipes, there's a few different things you can do. You can just write down. You can just write down a phenomenologically motivated Lymblad mass decreation. That kind of gives you non-unitary dynamics. I kind of call it maybe it's an atomic picture. It works in some pretty clean cases, but not in the kind of more realistic cases. You can go, use a microscopic interaction Hamiltonian, and you can derive a mass state version from that. Again, following the standard recipe, and then you get what I want to call microscopically informed kind of Informed kind of environmental influences, and with that, you can recover detailed balance, you kind of get your thermodynamics right. So, that's definitely an improvement. If you're more strongly coupled, you can go and do a Polaron transformation, you get a Polaron master equation, you can again derive it, take it to block red feed level, or you can secularize if you want. And that allows you to deal with strong coupling. It's actually on the previous slide, it's what we did for the vibrations, but not the That's what we did for the vibrations, but not the leads. So, but if you do that, then often other parts become perturbative. So, you're kind of dealing with one environment exactly, and in some sense, you can capture some non-Markovian features, even if the Maastricht equation is Markovian, because you've moved into a different frame, you capture something you might call non-Markovian, but you're still being perturbed. Another thing you could do is you could use a numerical technique, such as. You could use a numerical technique such as the quasi-adiabatic path integral. That gives you accurate properties, dynamics, valid normal regimes, full non-Markovian dynamics. So that's an improvement. Traditionally, that's been costly computationally, but that's, I guess that would have been the gold standard. I should say, of course, there are some other techniques, hierarchical equation of motion. There's KDOPA developed in Orm. It's done from Orm. There's many other things. Sorry, can you say what the difference is between? Say what the difference is between phenomenological and derived. Oh, right. This one, I would just say, okay, I've got dephasing going on, I stick in a sigma Z Limb blood operator. This one here, I've got a microscopic interaction, Hamiltonian, between system and environment, and I derive a master equation. It may end up having a sigma z like the inbutt operator, but then the rate is informed by the microscopic coupling operator. It kind of uses the spectrodensity, et cetera. I don't just plug things in. Just plug things in. I can derive them, making approximations, but it's all nothing, it's kind of just operationally or phenomenologically rotated. Okay, so what I want to talk about today is mainly this stuff here. That's a sort of new set of tools called the Process Tensor Matrix Product Operators. The two examples I will talk about are called ACE and Tempo, mostly ACE. And I think that is an improvement on the path integrals. They're sort of more versatile. Integrals, they're sort of more versatile, you can get many time correlation functions, and it can be a lot faster. So, I think that kind of pushes what you can do in numerically. So, implementation, this is easy, you just take your linpad dissipator, plug in your operators. Weak coupling master equations, straightforward computer can do that for you. If you kind of write, it can give you a block-red field time. So, if you plug in the interaction Hamiltonian, Holaron master equations, as far as I'm aware, you need to sort of derive yourself. You can do that on paper, not. Write it yourself, you can do that on paper, not terribly difficult, but it's a bit more involved. Path integral, hard to code up, but kind of easy to use if you have a software package. And the same is true for this class. If you kind of use a software package, it's not necessarily hard to deploy those. So these are more easily interpretable. These ones are more black box numerical. Not completely, because increasingly we're kind of understanding what these things mean better, but definitely in the mean better, but definitely in the black box direction. Okay, so I showed you the molecule. I'll switch to a different model system and I'll stick with that for almost the rest of the talk. This is something I've done, one of the things I've worked on for a long time, but also it's a nice clean model system. What you see here is a quantum dot, island of between 1,000 to 10,000 atoms of a semiconductor with a lower band gap and a higher band gap surrounding. And then it kind of looks like And then it kind of looks like this. You've got your particle in the box. This is the conduction band, the valence band. There's either nothing there, or you can have a bound electron hole there, that's an exciton, and it's a nice two-level system. You shine a laser on this thing in a rotating frame after rotating wave approximation. You kind of get a nice two-level system Hamiltonian, which you can use to drive rapid oscillations. But you've also got an interesting coupling to the lattice vibrations, the phonon environment. To the phonon environment. And in a sort of simplified form, you just say if you're in the excited state, you have an exciton there. The charge distribution is a little bit different, and that means all your oscillators, all your phonons, are slightly displaced. That's kind of the explanation of this. And these are all just homolog oscillators. So, is that Markovian? Is that non-Markovian? Kind of what you get from this? Well, depends. Depends what you do. You shine a laser, pulse on that, and the laser's fairly intense. And the laser is fairly intense. You convert your Rabbi oscillations and you expect sinusoidal kind of oscillations. But it's what you see is it's different temperatures here. They're sort of dammed. And they get more damped as the temperature goes up. So the temperature depends on the rate. And this is completely Markovian dynamics. In this paper from a long time ago, we derived a Markovian master equation based on the interaction Hamiltonian. On the interaction, Hamiltonian predicted rates between these sort of red dashed lines and then measured them pretty close. So you can kind of, Markovian dynamics absolutely fine. Same system. All we've done is turn this a different lab, different sample, et cetera, but the same idea. Use a very, very weak laser. And then instead of measuring the kind of population, in this case as a function of power, here we're just looking at the resonance fluorescent spectrum. We kind of look at what Spectrum. We kind of look at what's the energy of the photons which get scattered, or the resonance fluorescence we get from the sample. And this is at two different intensities. So this one's really kind of very small, weak laser power. And you kind of, you see the sort of broad sideband feature here. This is the photon sideband as it's known. You you get this one peak, that's your uh zero photon line. This is just a two-level scattering photons of your Rayleigh scattering. Your Raylay scattering. If you kind of zoom in there, you just see this kind of Lorentzian peak. That's this peak here. But if you've got a sufficiently good equipment, you see there's a lot more going on. Go to higher intensity, kind of looks the same if you zoom out. If you zoom in, you start seeing your model triplet here. So this situation, you cannot model with the same master equation we model that. You don't get that spectrum. The Makovian master equation doesn't give that to you. The theoretical model here used the Polaroid master equation. And that worked. Polar and master equation. And that worked in that case. And here's a situation where the polar and master equation also didn't work. And what's happening here is kind of the idea is we are applying two laser pulses with two different frequencies. Neither of them resonant with the transition of the two-level system. And the kind of really the idea sort of behind this is, could you excite this quantum dot without having any photons at the frequency at which it will emit? Because then it would be a nice single photon source. Be a nice thing for photon source, we don't have to worry about filtering out the excitation holds. So, if you do that, you get something pretty messy. This is a pulse area, you just ramp up or down your laser power. The pulse contrast is kind of more power in red versus blue and the other way around. At zero, they're equal. That's the way that that's what's shown here. And you kind of get, again, measuring populations or the experiment counts, this is the experiment, you get a pretty complicated kind of map here. Kind of map here, and neither the weak coupling nor the Polarmaster equation worked. So, that I guess not Mercury or not, but we needed the path integral technique to get something which looks like a reasonable agreement. So, this is the kind of system, right? That's the setup of the quantum dot. And I think I'm showing you three examples here where for the same system, same environment coupling, you see very different regimes and you need very different theoretical tools. Okay, so that was a lot. Okay, so that was a long motivation. Let's get sort of to the meat of what I want to talk about. Think about dynamics of open quantum systems. We've got some finite dimensional system of interest, that's the circle with lines here, and it might be coupled to maybe a kind of hybrid of different types of environment. There might be some mosotic modes, there might be fermionic modes, there could be spin modes. In principle, in the spirit of what Philip was telling us earlier, we should just maybe solve the Schrodinger equation neutrality for everything. Putting an equation monitoring for everything's fine and dandy. In practice, we can't because the Hilbert space is too large. So we can't possibly hope to solve that exactly. We need something that helps. So what we'd like to do is find some compressed representation of this whole messy environment, all the different bits of it, and say, actually, we can somehow compress that, we can represent it in computer memory, we might be able to propagate this sort of suitably compressed version. Suitably compressed version. What do we lose? We lose access to everything that's going on in the environment, but that we don't care about. We only care about the system, it's fine to sort of compress our way. So that's the mission, basically. That's what we want to achieve. Won't talk about this. This is sort of similar to what Mila showed us later. This is a comb or process tensor that shows the flow of information, what that would look like. I'll do that slightly more formally here. So, let me introduce. So let me introduce what we mean by a process tensor. So first step is we discretize the time evolution. We do a trotterization and generally speaking we have a system Hamiltonian, we have an environment Hamiltonian, the HE here includes the free environment Hamiltonian and the interaction bit and we can break that up and we sort of get for time steps delta t errors of order delta t squared. Errors of order delta t squared. Make the time step small enough, we can split this up into sort of two separate steps. And what that implies in Liouville space, we write our density matrix in Liouville space. We can kind of propagate our density matrix like that by time step delta t, and we make a small error, but we can always make that sufficiently small such that we don't care about it. If we do that repeatedly, and here's where I'm being slightly hand-waving, but if we do that repeatedly, Hound waving, but if we do that repeatedly, we can kind of represent it like this: we've got a starting density matrix for the system. We evolve it by a short time, just using a free time evolution, M, that's the system Hamiltonian, that generates that. And then we have an environmental influence, and we kind of just keep alternating. So free system evolution, environmental influence, etc. And that's how we propagate our system. And now this guy stylized. This guy stylized I here that contains all the information about the environment, but crucially it has to be an ordinary covina to remember kind of what happened, what the system might have done to the environment at an earlier time, and we'll come back and feed that back into the dynamics at a later time. So this is your, I call that an influence function. And the problem with that is, so up to the trautarization, this is sort of just still an exact mapping, but the problem is we still can't represent the I, we've not really solved the problem. We've not really solved the problem, we've sort of just reformulated. We still need to find a way of making this tractable. And the way you can do that is if you move to a tensor network representation, you make the product state, you make it the product operators, and do the compression. And that gets you from something that looks like this to something which has these cues here, and they're kind of connected, but we've sort of compressed a lot of the information away, and we get something which we can store in memory. So this is kind of what's led to the development. This is what led to the development of process tensor tempo, where the original tempo was based on floppy path integrals, and then with the Jürgenson and Pollock's work and work from Havan's group, that can be reformulated to more elegant and more efficient form. So that's the idea. Oh yes, I should say, also look forward to Greg's talk later this week. He'll be talking about similar concepts but in a with a kind of different applications in mind. Applications in mind. So, what's the new thing we did there? We said, actually, let's throw all the things overboard that people have done with the traditional path integrals. Let's just construct this process tensor directly. We're not using a Feynman-Werner path integral or anything like that. We just do it directly, microscopically, explicitly from the microscopic Hamiltonian. So, the ideas about total Hamiltonian looks like this: system Hamiltonian is some over-environmental. Hamiltonian, some over environmental modes, we can break down the environment into different bits. So we have a bath of phonons, we just treat each oscillator individually. We've got a quasi-continuum we need to discretize, and we maybe approximate it by being a thousand moles. We treat each harmonic oscillator individually. Because the harmonic oscillator, we also need to truncate its Hilbert space. But once we do that, it's easy to write down a Leo-Willian formula. It's easy to write down a Leobillian for it. And then we can kind of get this Q thingy here. I think that makes sense. So we can get a single mode process tensor explicitly from the interaction Hamiltonian. And the quality is not great here. I hope it works. So once we've done that for one mode, well, we go and do it for the second mode. So we've got our Q's, and the second mode is the P's, and we can kind of build that up. And the whole trick is then to find out how to. The whole trick is then to find out how can we combine those things and compress. And that is possible. I won't go into technical details, but you can merge the process tensors, you compress them by singular value decomposition sweeps, you have a threshold below which you throw away singular values, and then it's rinse and repeat. And this is kind of what's indicated here. So this is sort of our system evolution, and these are all the, these are the sort of, you know, together each of these crystals. Together, each of these Christmas trees is a compressed cue, but you can see it's sort of iteratively, it's got many different layers. The colour is not great, but we're sort of starting with the first environmental mode, and we kind of work our way down to Kmax or NE as it's up here. And we just keep doing that. We treat each mode of the environment one at a time, merge, compress, next one, etc. Turns out it's pretty efficient. It's not as bad as you might think, and it's pretty powerful. So, let me show you some examples of what we can do with it. So, here's what might seem relatively boring. We've got an environment of just two modes. This is known as the resonant level model. It's a model for charge transport. We've got a system, this kind of this one guy in the middle. We've got two filled sites. Each side we've got a coherent coupling. What do we expect that allowance to be? Expect the dynamics to be? Well, we can solve that as a closed system and we get sinusoidal oscillations. And you can write down, actually, you can solve it analytically, you can write down the formula. What does our algorithm do? We can express these two guys' environment, follow the recipe, that gives us exactly the same answer. So we've not gained anything, but arguably this is infinitely non-Markovian. Information is never lost. Never lost. If you make a petition, we think of that as a system. So it gets more interesting if we add more modes. You can go from two to four. Four is this purple dots here. That still has some sort of oscillating behavior, but it looks a bit modified. We can go to ten and we can do a perturbative expansion. We say it should be quadratic at short times. It sort of follows that. We can go to a hundred um environmental levels. Environmental levers. And that gives us something which looks pretty much Markovian. That sort of follows an exponential dynamics on this time scale. So with the same algorithm, the same numerical approach, we can span all the way from, well, we didn't do anything, we just ran the same code and changed the number of mode experiences. So with the same tool, we can span all the range from fully microbiome to fully multimicotian. I was talking about quantum. I was talking about quantum dots, though. So here's an example of a quantum dot again. We've got the ground state, the excited state. We have a time-dependent excitation pulse, which is non-resonant. And then we can sort of see what happens. This is really just benchmarking and seeing if it works. So you could decide to not worry about photons at all, and only treat the decay and photon decay as splint blood operators. And you kind of get this curve. You don't get a lot of excitation. And obviously, after the And obviously, after the excitation process is finished, it kind of waves back to the ground state. You can use ACE with just phonons. That line is kind of not very faint. But if you use A's with just phonons, no decay, you kind of get up here. You have some sort of phonon-assisted excitation. It gets much better. If you're tuned to higher energy, your laser photons, your driving delivers more energy than is required. Photons take the actual energy. Photons take up, take the excess and allow more excitation, and then you kind of stay there. You can throw in A's to photons and photons, and you've got this purple line. And that actually, in this case, agrees with treating phonons with croppy and the decay optically. Why is that? Because the optical decay is fully Microville in the sense. It's the phonons which are more Microvian. So we kind of expect this agreement. But the point is you can do it. With COPI it's not easy to do both the photons and the phonons fully The photons and the phonons fully numerically. You can add on a Limpad operator, but it's difficult to treat both environments without approximations. Where does it change? In this case, we're just starting in a fully excited state, and then we watch the decay. And if there's no phonons, actually, hang on. Oh, yes. So here these blue circles, there are phonons. The decay is Lindlat, so we get exponential decay. There's no driving anymore, nothing else. There's no driving anymore, nothing else going on. If there's no phonons and we use A's to model the photons, we're sitting on the same line. So the phonons didn't do anything if you just have a decaying two-level system here. This is for this case where the bandwidth of the optical environment is relatively broad. If you stick this thing in a cavity on some engineered electromagnetic environments where you have a smaller bandwidth for the electromagnetic environment, then you start to see changes. Then you start to see changes. In this case, whether or not photons are present, this is both A's, kind of makes a difference, and you have non-exponential decay. And I would say that's an example of non-additive dynamics. You've got an open system and two types of environment, and you can't really treat them separately. You can't just add on different lint plat dissipators or anything like that, or even treating one environment exactly but not the other, doesn't give you the right answer. The right answer. You need to have a tool which can deal with both sort of on the same footing. Could we have done it differently? This is a brief aside. I think I don't have too much time. So yes, you could. You could use a tempo approach. This is just a single two-level system with two bars, optical, an electromagnetic optical coupling and a phonon one. And in this case, sort of as a function of the optical temperature, you see in some levels you can get population. Levis, you can get population inverted. You get more population in the upper state than the lower. And again, that's something that's non-additive. You need a numerical tool which can do that. I should say if the coupling to the electromagnetic environment is very, very weak, you can actually find a polaron model which works reasonably well, but only in that limit, not general. I want to try and get to the end on time, more or less. Another example of what you can do: this is two quantum dots. You're looking Two quantum dots, we're looking at the second-order intensity correlation function, so we collect the photons, send them beam splitter as two detectors, and looking for coincidences. And if we had just, if we had two emitters and they were completely independent, we'd expect this sort of this dip in the G2 to the 1.5. And then we have two situations here, one that's kind of hard to see, but one where they should super-radiantly decay, and each emit has its electron environment, and the other where they shouldn't superradiently decay. And the other way they shouldn't separate into the decay because they're too far apart. But we still try and erase which path information with his lens. And each has its own environment. Pretty easy to use the ACE approach and kind of add a second environment and then get our two-time correlation functions by kind of just diagrammatically. We have our point A and B here, where we extract. If we do that, what do we find? What do we find? We find the exact phonon calculation. There's a sort of very fast decay on a picosecond time scale, and then nothing much kind of happens and we sort of recover. That's the blue line here. We have Markovian defasing. We kind of go down. We have a big anti-dip. We come back up and the green line is combined. Why am I showing you combined? Because here's some data from my colleague Brian Derrida's lab, where we kind of get something, we have two dots. Get something, if we have two dots, they're all resonant but not super radiantly emitting. You kind of see this sort of partial antidip. It's not a perfect map, but qualitatively, we can kind of recover that. Now we're talking two emitters, each in an individual environment and vibrational, and both kind of decaying collectively. Right, so we're going to wrap up almost. So, automated compression of environment, we've got numerically converted. We've got numerically converged dynamics. The convergence parameter is the time step, the compression threshold, the singular value decomposition threshold, xi here, and the number of environmental modes. The modes don't all have to be of the same type. It's very easy to use spins, fermions, bosons, anything you want. So, this molecular junction I showed you, you could do spin, the leads, the vibrations, all that stuff. The form of the interaction is not limited to anything. Of the interaction is not limited to anything specific, doesn't need to be random. You can use dispersive coupling, non-linear, James Cummings. Use any decomposed environment, including an environment which is dissipative itself, but all we need to be able is to write down a Liabilian for the small blocks of the environment that could have Markovian noise on it, that could be driven in a time-dependent way, or that's no problem at all to include. Our environment doesn't need to be Gaussian, and you can have any combination. And you can have any combination of non-additively acting additional environments. It's easy to use if you use the existing code, and we have open source code. What you put in, system Hamiltonian initial density matrix. You break down the environment into its different modes, give the initial environment density matrix, final type times that the compressions I showed. This is the input, you get the output. Relatively, couldn't be, couldn't, I don't think you can make it much easier. Think you can make it much easier. There's code here. For the tempo, there's also some Python code. This is C. There's some Python code. We're working on getting ACE into this Occupy framework now. Limitations? A couple of minutes, maybe? One or two. Performance. It gets slow for large systems. There's something you can do with time evolving block decimation, so you can you can use the techniques that exist. Techniques that exist. It gets slow for long times. Something we're working on when we're recognizing self-similarity and come up with a divide and conquer scheme to kind of find more efficient representations. The thing we don't know how to solve can't be with highly correlated environments. So the environment needs to be, you need to be able to break it up. That doesn't mean correlations arising through the mutual coupling of the system. It means sort of directly attracting environmental things. Okay, so this is. Okay, so this is, I think this really is my final slide. With this divide and conquer, this is looking at the quantum dot again, the mono-triplet, right? Strong driving of the quantum dot, getting the spectrum, but this time, what I showed you previously was a polaron mass equation. This is done with ACE, and we're using about ten million time steps, and it sort of takes about ten minutes. So kind of we're getting to the point where we can get long and infinite time dynamics. Dynamics. And I won't talk about this in the interest of time, but we can go up to moment five or six emitters and we can kind of further and further analysis as a super radiance example. Right, I should highlight Moritz. He really developed the method, he developed the code. He deserves all the credit for ACE. So this is we've also got mountains in Scotland, but it's usually cloudy and foggy. This is on top of Scheehaldian last May. Last May, and this is just an overview slide on the difference of stuff between operators. With that, I'd like to finish. Thanks for your attention, I'm very happy to take questions. Okay. Yeah. So, do you look at, or are you interested in looking at multi-time fire legends at all, or is it just mastering legends? Um we we have. Uh uh Uh here. I mean that's a so this is a this is a G2, right? And tau here is a time delay, so we're kind of looking at correlations as a function of time delay. So it's a two-time correlation function. Could you do more? Absolutely. There's no reason you can't plug more things in. It's kind of at some point gets numerically challenging, but that that's as far as we've got. But that's as far as we've got. My question is kind of obviously that process sensors are really good in the same equations. So if you're just looking at master equations, could you compress it further to only take the information down and say the average and state or? Possibly, although we don't really control the compression very well, we kind of just use our singular value decomposition suites and we have a threshold and we're working on sort of trying to understand what the network and bond dimension represents. And And one dimension represents, and maybe there's some stuff you can do, but I guess it's I'm not quite sure how we'd go about it. I think the one thing where we are getting more out is this divide and conquer scheme and there you can kind of learn to keep the memory forever, you can truncate, you exploit the fact that different blocks look the same, you're sort of sliding your memory window or your memory kernel along. So that would, I guess, maybe throw away. I guess maybe throw away that, or that's a way of systematically throwing away time correlations beyond a certain memory time. And that gives you obviously the pick of the focus boost to say, is there a, what is the effect of controlling the complexity of let's say I use my boss controls very fast for seems like a fact of it for my sorry. So the process tensor we construct is Process tensor we construct is completely independent of the system Hamiltonian. It doesn't matter what the system Hamiltonian is, what we do with it. However, if you want to resolve the dynamics, your time steps need to be small enough. So if you want to, if you're coming in with very fast unitary dynamics on your system, you want to resolve that and then your your delta t becomes very small in order to do that. And then that makes the process tensor, it has more legs and it sort of takes up more more memory. Sort of takes up more memory. So, this is kind of what we did with the monoclips that actually there's two time scales in there. You have the photon, the photon time scale, they differ by three orders of magnitude. And that's where the sort of self-similarity and that sort of stuff helps. But in principle, I'd say it's kind of agnostic to what you do to your system. You just start based on the microscopic example. It really is the kind of parameter, as I said, we've got a delta T, we've As I said, we've got a delta T, we've got the singular value decomposition threshold, and then it's how do you discretize your environment? How many modes do you include? Those are the sort of three things you control. You want to be able to resolve anything you're interested in, but beyond that, don't really automatic, that's in the title, it's an automatic compression of environment. We don't derive anything, we don't tailor it to any particular application. It's the same with tutor code. I think it's a question related with the two previous ones. Is this a way is there some way to know beforehand some particular environment fits with the simulation? For instance, you can compute, I don't know, perhaps in the Gaussian case, correlation functions or know by looking at some width of correlation function or something that active testimonies are going to be more efficient or less, or is you know is there some some Is there some some some I don't know some label that uh you can check or some check that you can do to to be sure that the simulation is going to be efficient. So efficiency. We don't have a performance guarantee or anything for it like that so we kind of we run it we know how big the process tensor is based on the input prompt of the truth but we don't know it in advance. Is it we've in the paper we do have the In the paper, we do have a list of examples, including dispersive couplings with kind of runtimes, and it gives you some, probably some sort of ballpark idea. But how efficient is it? I guess. So you can't go same idea with a process tense, but differently constructed. You can use tempo, that's the Jürgenson-Pollock approach. This one only works for Gaussian environments. And with Gaussian, if you've got a smooth Gaussian environment, that's faster. If you have a high That's faster. If you have a highly structured spectral density, ACE is faster. So it kind of, but we didn't, we sort of had an inkling that should be the case and we tested it, but I can't necessarily give you numbers in advance. So with some understanding of what you're expecting, you can certainly kind of know which tool to try, but I can't give a full guarantee at a one. You can, however, you can combine the Gaussian process tensors with the Ace process tensors. The ACE process tensors. That's all about, you can have many, the whole thing about ACE is merging process tensors. So if you've got a fast way of constructing a Gaussian one which is compact and works well, use that. If you've got something which you think is a non-Markovian spike in the spectrum density, use an Ace one from that, merge the two from that. And that's probably going to be better than trying to resolve a model structure in the Gaussian framework. And also you could do spins and things you can't do with the Gaussian framework. Okay. Are there any questions from the online audience maybe? And so, how about the fermions? Yep. Fermions? Fermions, yes. So I guess that's sort of the rest of the world. I guess the sort of the resonant level model gives you a flavor in, right? That's that's so yes, you can do that. It's not a problem. So it's in the if you download if you download the C code, you can set up a number of fermion modes and run your open systems problem. Have you guys tried doing some strongly correlated problems? So depends what you mean by strongly correlated. So we can't deal with strongly correlated environments. So in that case, I think they're that that In that case, I think that still needs maybe more development, yeah. So I think there are things we might be able to do in due course. Because the regs are the bond dimensions become high, or bond dimensions become high. And then it becomes a beast store in memory, talking about a terabyte process tensor. So so there's some work, uh there's a couple of papers that recently came out which we've had Columbia that seem very similar to TechPoint. Yeah, actually we noticed that. And actually we went with some of our twenty fifteen results on condo physics. And it seems it seems quite similar. Yeah, so we've kind of we've played with a condo effect and things like that. I think in principle it's doable, it's still challenging numerically. And we may need to look at the literature and see whether there's any tracks we can match up. So for somebody who doesn't really understand this very well, like me, there's some sort of Like me, there's some sort of miracle going on, it seems to me, because you expand in delta t, yes, and then you iterate, right? So, what's the basic mechanism why you don't accumulate errors? Well, we keep the delta t small. Oh, but in some of the slides, you said you can go to long time somehow. Yes. Yeah. We can go to long time, so There's a question whether you necessarily accumulate errors. So I think based on the Trotter decomposition, you might. I don't think you necessarily pick them up. It just happens that you don't pick them up. Do you know why, or is there a reason why that something is? I'm not sure. So we get what we expect. Of we get what we expect as far as we can benchmark, it converges. It's exact sometimes, it's actually better to operate with a larger delta t. If you make your time steps too small, that can cause problems. Why? I'm not entirely sure. I mean, you, you, you're... So I guess there's a trade-off. Often you find a trade-off between how big you want your delta t to be and what your singular values that show it. Because you're thinking for a fixed process tensor, might be Process tensor, you probably don't want it to be much larger than a few gigabytes to be useful and fast. And then you can, if you don't throw anything away, it becomes large. So it grows as a function of, as grows with delta t and grows with the, in a less predictable way, it grows with the singular value threshold. And so there's some trade off. We find actually it's pretty forgiving. Wh why exactly does it not lead to bigger accumulation of errors? I'm not sure. Does anyone else know? I'm not sure. Does anyone else know? Well, anything else? Well, then thanks very much, Eric. Thank you. So we will have a short break now and briefly implementing the score. 