Exchange of appropriation. Yeah, thank you, June, for the invitation. Thank you for all the organizers for organizing this interesting workshop. I'm a little bit of an outsider to several of the topics that were mentioned in the previous talk. I'm familiar with them, but I don't actively work on them. And you can think about this talk as being half of sharing something interesting we have been doing, but also there are some nice interest. Also, there are some nice, interesting connections to random graphs, which I'm not sure what the right direction is to explore next. So, maybe people here who are familiar with random graphs can, I was hoping for feedback or thoughts or ideas. So, the part that I'll present will be about a joint work with Tudor Manol, who's a PhD student at CMU. And there's like four parts to this, like in some sense, four terms that I will explain. There's sequential estimation of convex diversity. Of convex divergences. And the tools we'll be using are reverse sub-martingales and exchangeable filtration. So I'll kind of go through just one slide for each of those and then get to what I think could be interesting from a random graph point of view. Okay, so here's a summary of the talk. I'll come to it again at the end. This exchangeable filtration, some of you might have come across it already. It comes up in some parts of It comes up in some parts of probability and statistics. It's a very interesting object that I argue arises very naturally. Convex divergences also appear very naturally. Every F divergence, scale divergence, T V, integral property, metric, Wausha stein distance, they're all what I call convex divergences. I mean, entropy is a convex functional. And so people should have come across explicit instances of these, but you know, the general, I'll explain what a convex diversity is. General, I'll explain what a convex divergence is more generally. Now, one nice fact that turns out to be very useful that we find is that convex divergences are reverse submartingales with respect to the exchangeable filtration. So if you haven't seen reverse submartingales before, I will come to that. And it turns out that there's something called a wheels inequality, which is backwards in time. And that allows you to convert fixed time concepts. That allows you to convert fixed-time concentration inequalities to time-uniform concentration inequalities or bounds that are valid at stopping times. And so, that's the part that I will explain. That's work we have done. And the part that's a little bit more open-ended is the connection to random graphs. And one connection is that a sequence of random graphs, when you ignore the labeling or ordering in which the nodes are entering, that can play the role of an empirical distribution in this exchangeable filtration. Filtration. And one question that I have, which I'd love to hear about, is if any of these techniques are actually useful for the study of random graphs. Okay, so one slide each for the first four parts, and then we'll get to some random graph stuff. Okay, so what is a convex divergence? This, I think this concept was introduced by Rockefeller in the 1970s. So over here, let's script Px, denote a set of Borel probability measures over some. Set of Borel probability measures over some set script X will be in Rd for the moment for the first part of the talk. And a divergence is just it takes in two probability measures and it outputs a positive real number. Okay, so that's a divergence. And we will call a divergence a convex divergence if it satisfies this type of a Jensen style inequality in each of its arguments. So when I plug in lambda mu1 plus 1 minus lambda mu2 in the Lambda mu1 plus one minus lambda mu2 in the first argument, and lambda nu1 plus one minus lambda nu2 in the second argument, then that divergence is less than equal to lambda times the divergence between mu1 and nu1, plus one minus lambda times the divergence between mu2 and nu2. So if it satisfies this type of an inequality, it's called a convex divergence. And pretty much most divergences you can think of are convex. So like I don't expect you to actually read it, which is why I've put it in half translucent. Like if you've heard of Translucent, like if you have heard of integral probability metrics, they are convex. Example, like you know, these usually take they have a function class, and you take the soup of a function class of the integral difference between, you know, that when you take that function's expectation under p, you know, minus the function's expectation under q, that difference soup overall f, you know, that's an integral probability metric. And different function classes yield different, you know, divergences. You know, divergences or total variation distances gotten with one of them, Kolmogorov-Smirnov distance with another, the kernel maximum mean discrepancy with another, and so on. So, you know, the details don't matter, but you know, the integral, that's a very large class already. You know, all kinds of optimal transport costs. These are also convex divergences. And then if you've heard of phi divergences or F divergences, depending on the literature, these are more like ratio style divergences that look at things like likelihood ratio. That look at things like likelihood ratios. So, you know, chi-squared and KL and things of that kind, they're all F divergence or phi divergences. And so all of these are actually examples of convex divergence. So we're going to be interested in convex divergences and having techniques that kind of work for all convex divergences in one go. Okay, so the second piece is related to sequential estimation. And so there's this concept called a confidence. Estimation. And so there's this concept called a confidence sequence. It was introduced by Herbert Robbins in the 1960s and studied by Lai and Siegmund and Robbins in the 1960s and 70s and Darling. And so the concept is very simple. It's just, everyone knows what a confidence interval is. A confidence sequence is a sequence of confidence intervals that has a time-uniform coverage guarantee. Okay, so that's all it is. So let me go through that a little slowly. So suppose now we have, you know. Slowly, so suppose now we have you know uh two distributions p and q, like we saw this divergence on the previous, it's going to be two distributions. So we have xt, you know, a sequence of observations from p, and y s, a sequence of observations from q. And, you know, maybe like in this case, this we are interested in confidence sequences for the divergence. So using the data, we're going to try to capture D of PQ. Okay, so for some convex divergence P, we want to capture that and we're going to use Capture that. And we're going to use these sets CTS. So, CTS is using T samples from P and S samples from Q, I'll construct a random set CTS. And this random set, CTS, obviously what you'd like is the set is shrinking as P and S get larger and larger, this set shrinks in size. And typically, it'll be an interval. We already know DPQ is non-negative. So, CTS you can think of as just being a subset. Can think of as just being a subset of the positive reals. And what you'd like is that for all T and S simultaneously, as you collect more and more data, you just get more and more and more sure, but you have a simultaneous coverage guarantee for DPQ or every interval in this sequence covers with probability one minus delta. So it's called a confident sequence. And another equivalent definition of a confident sequence is in terms of stopping times. So another way of saying the same thing is that actually for, and you know, I'm Actually, for uh, and you know, I'm kind of pushing away the filtration for the moment, but basically, for all stopping times, as you're observing this process sequentially, you can just choose to stop at any point of time in a fully data-dependent fashion. Um, and you can stop the two sequences actually separately if you want. But for any, you know, you know, stopping times tau and sigma with respect to actually the canonical data filtration, you might like that the probability that that C tau sigma. That C tau sigma is basically a valid confidence interval. And so it turns out that these definitions two and three are equivalent. So it turns out that if equation three holds for all possible stopping times, potentially infinite, you know, no restriction on stopping times, that's equivalent to two holding, which is it holds uniformly over all time. Okay, so this is what a confidence sequence is, and it's super useful in sequential estimation, testing, multi-arm bandits. Testing multi-arm bandits, things of that kind. Okay, so like we've in past work, we have developed confidence sequences for lots of interesting objects like quantiles or means of random variables for matrices. So we have covariance matrices, things of that kind. So we have developed confidence sequences for very interesting objects. Here, we are trying to say, can we develop a confidence sequence for the, let's say, the K-divergence between P and Q? As I get. KL divergence between P and Q. As I get more and more samples, can I estimate this divergence? And if you did, you'd be able to do sequential testing. So this you can obviously like the null hypothesis for sequential two-sample testing, the null hypothesis is that P is equal to Q, which means basically that DPQ is zero. And the alternative is that P is not equal to Q, which means DPQ is greater than zero. So if you want to do non-parametric to sample testing and you want to do it, To sample testing, and you want to do it sequentially, one natural way to do it is to track a divergence. And as soon as the divergence, the confidence set does not contain zero anymore, you're confident that this is away from zero, then you can reject the null and you can say, I think P is different from Q. So you can use it for sequential testing, but there's actually several other uses of divergences in these types of sequential applications. I don't want to go too much into it, but hopefully you can imagine some interesting applications. Some interesting applications. Okay, so that's the second concept. The first concept, convex divergence. Second one, confidence sequence. So, third one is the exchangeable filtration. So, now I don't know how far back this goes. I mean, I know that, I mean, DeFinetti maybe implicitly used it and Hewitt Savage maybe explicitly used it, but it's a super interesting object. If you haven't seen it before, this is beautiful. Okay, so the setup is to describe the filtration, just think about one distribution for a moment and. For a moment. And let's say that you have samples x1, x2, dot, dot, dot, drawn from the distribution P. So the exchangeable filtration is a reverse filtration, which means it's a shrinking filtration. So usually we think about filtrations as increasing. This is a reverse filtration. It's shrinking. And essentially, it denotes the sigma algebra. ET denotes the sigma algebra generated by all real-valued Borel measurable functions of all of the Of all of the entire sequence, of the entire infinite sequence, which are permutation symmetric in their first T arguments only. And then they can depend however they want on the rest of the arguments. Okay, so that's the exchangeable filtration. But if that's confusing or maybe taking ahead to wrap around it, the way I think about it informally is that ET is the information known to an amnesic oracle. This is an oracle with amnesia. So they're an oracle, so they know. So they're an oracle, so they know the entire future, they know what's ahead of them, they know XT, XT plus one, XT, they know everything. And the amnesia part is they have forgotten the order of events in the past. They know things that have happened, but they don't remember which order the things happened. And so it's just some permutation of x1 to xt minus 1, and they're not sure what the right permutation is. So if you think about an amnesic oracle living their life at the very first instant, they know the entire future. Instant, they know the entire future, but as time passes, they actually know less and less and less because they start to forget the past as time progresses. They can still see the future, but they've forgotten the past, so they know less and less. So, if you can imagine an amesic oracle, that's what the exchangeable filtration is capturing. And kind of informally, you know, you can think about Et as the sigma field formed by the empirical distribution Pt and Xt plus. Pt and Xt plus one, sorry, that should be xt plus two, and so on. So the empirical distribution basically has forgotten the order of the first t but keeps what the observations actually were. So informally, you can think about it that way. So that's what the exchangeable filtration is. So very interesting object. So the last piece is what is a reverse submartingale. So a reverse submartingale is a submartingale when you flip time and you look at time backwards. You stand at infinity and you look towards the. You stand at infinity and you look towards zero rather than standing at zero and looking towards infinity. So we look backwards, it's a submartingale. And so we start off with a reverse filtration. We need to reverse time. So we start off with the reverse filtration. In this case, it's just simplest. Think about the exchangeable filtration itself. That's a reverse filtration. And an integrable process MT is a reverse submartingale with respect to ET if the expected value of MT given ET plus one. mt given e t plus one is greater than or equal to mt plus one so it's a sub martingale if i look backwards but if i look forwards it actually looks like the x it seems as if the expectation is actually um decreasing like mt plus one is like smaller than mt in in this sense um and so it looks like it's it's a decreasing process but if you look backwards it's a subbarting it um so it's like there's an analog of uh um okay so Okay, so an example of a reverse martingale is an average. Averages are reverse martingales. So, reverse martingale means there's an exact equality there. But maybe slightly more interestingly, the empirical distribution is a measure-valued reverse martingale. So now that can be, you know, what is a measure-valued martingale that can be formalized. But intuitively, you know, if I, the expected value, if you in quotes, the expected value is valuable. Value, if you in quotes, the expected value of the empirical distribution, if I give you e t plus one, is just pt plus one. And so that's it satisfies that kind of property. Now, it turns out that some people might be familiar with Doob's inequality forward super martingale, submartingale, things of that kind. There's actually one of Doob's famous inequalities stems back to Veil from 1939, Jean-Weal in 1933. From 1939, Jean Ville in 1939 wrote a PhD thesis that actually introduced for the first time martingale into probability theory. It was a concept in gambling and so on. It was actually introduced into probability theory by Jean-Wille, and he proved a martingale inequality for non-negative super martingales, forward non-negative super martingales. And there's an analog of this forward, non-negative supermartingales, the wheels inequality. There's an analog of that for backward. Analog of that for backward reverse submarting gals. Remember that reverse submarting gales behave like forward supermarting gales. And so there's this analog of this type of result. And so that's what I've mentioned over here. Now, many of you will, I think that the right way to view this is a time uniform version of Markov's inequality. So Markov's inequality tells you if you have a non-negative random variable, then the probability that it is larger than u is less than equal to its expected value divided by u. To its expected value divided by u. Okay, but now if you have not just a single non-negative random variable, but you have a non-negative reverse sub-martingale, then you get a time-uniform version of Markov's inequality. You say the probability that there ever exists a time at which it exceeds u is less than equal to its expected initial value divided by u. And so it's beautiful. And I've cited Lee 1990 for a version of this from a textbook because it's for the reverse version. But for the forward super martingales, this stems. For the forward supermarting gears, this stems back to wheel and it's a time-uniform Markov's inequality. It's beautiful, it holds for non-negative supermarting gears. That's extremely powerful and has tons of statistical and probabilistic applications. Okay, so that's nice. So we are going to now try to think about things in terms of for divergences. And so if you think about, you can think about the one sample divergence or the two sample divergence. So NT is like a one-sample divergence, like you can think of D of P. divergence like you can think of D of P T Q minus D of P Q and M T S is where both you know you have samples from both you know both q p and q and so you have d of p t q s which are empirical distributions minus you know d of p q and uh you can you can use uh you can basically conclude from uh you can infer that um this process uh phi of pt where phi is any convex functional Where phi is any convex functional, phi of pt is a reverse sub-martingale with respect to the exchangeable distribution. So, convex functionals are like entropy. So, entropy, for example, is a convex functional. Now, think about the maybe countable or discrete number of atoms first. If you have to talk about it in the continuous case, you have to do some smoothing. So, you have to do kernel smoothing, then you have a kernel-smoothed entropy. That'll still be a convex functional. But basically, if you take a convex functional, then a convex functional Then a convex functional of the empirical distribution is a reverse sub-martingale with respect to the exchangeable filtration. That's actually a very beautiful, non-trivial fact. And then you can take divergence, you take a convex divergence psi and then psi of PTQS. Now you have to get into partial ordering. I'm going to slip that under the rug for the moment, but you can prove that this is a partially ordered reverse sub-partingale with respect to the exchangeable filtration. Respect to the exchangeable filtration. Okay, and then from that, from any convex functional and any convex divergence, you can prove that this process NT is reverse submartingale and MTS is a reverse submartingale and so on. So these have to be integrable, but whenever they are integrable, they are reverse submartingales. So that's an interesting fact. Like we deal with a lot of these objects all the time. These are plug-in estimators of divergences. They appear everywhere in the literature. This seems to be a nice Seems to be a nice fact that they're reverse submartingales with respect to exchange rule filtration. Okay, so what can you do with it? How can you use this fact? So you can use this fact. I'll give you two examples of how you can use this fact. So like one is you can prove that plug-in estimators for any convex divergence, they're always upwardly biased at any stopping time. At any, I mean, you know, fixed times are special cases of stopping times. But like for even if But like for even if for any stopping time, the expected value of d of p tau q is larger than d of p q. So it's always up, there's always an upward bias. And the same with the doubly indexed one as well. If you plug in pt and qs and you stop it separately at tau and sigma or at t and s for any fixed time or any stopping time. So this is just true for you, you just get this fact for all divergences in one go. I think that's nice. In one go. I think that's nice. If you start to look at special cases now, you can use this to convert fixed time inequalities that you're familiar with to time uniform inequality. So I'll give you two examples which are interesting. So one is a DKW inequality. Hopefully everyone here is familiar with Massart's version of the DKW, the Dwaretzki key for Wolfer-Witt's inequality. It's an inequality on the difference between the empirical CDF and the true CDF. Empirical CDF and the true CDF in infinity norm. And it looks like a Hoofting style inequality. And now, what you're seeing over here in Corolli 13 is using our machinery, you can get a time-uniform version of it, which is very clean. So it says that the probability that there exists any T at which the difference of Ft, the empirical CDF, minus the true CDF in infinity norm, this difference ever exceeds some constant over square root t. This is a small. Some constant over square root t. This is a smaller order term. This is the more interesting term. This is like a log log t divided by t and a log one by delta and a small constant up front. The probability that it ever exceeds this is at most delta. And if you ask what does the original dkw inequality look like, basically it drops this leading order term and it drops this log log t. And so otherwise, you basically get the dkw inequality, but you get a time-uniform version of it. And this is very clean. Very clean, very tight constants as well. It actually works super nicely in practice. Here's a different one. Let's say that you have an alphabet of size k and you're looking at the total variation distance between the empirical distribution, you know, you're drawing from some distribution which is finitely supported on a finitely supported alphabet. And the empirical distribution P T minus P, you can ask its total variation distance. But again, the probability that at any point in time this ever exceeds time this ever exceeds some small this is a you know lower order term that depends on k and this term depends has a log log d dependence so you can view these as finite sample or time uniform versions of the law of the iterative logarithm okay so the law of the iterative logarithm you know is a very classical asymptotic statement these are time uniform versions of the lil that um you can use to actually you can use to derive things like the lil so here's an example things like the LIL. So here's an example of an LIL that you get. If D is a convex divergence such that d ptp is sigma square by t sub Gaussian, you know, then you know and the expected divergence is actually small, which both of those facts actually happen frequently. Then you get some kind of upper law of the law. Obviously, if you want a lower bound, you have to use different techniques, but you get an upper Lil, which I think is also. Which I think is also an interesting fact. So, anyway, so you can use these kinds of observations about convex divergences, reverse sub-martingas, and the exchangeable filtration to derive very interesting statements. So, if you look at the paper, there's plenty more. We have it for KL divergences when you have absolutely continuous measures or smooth Wascherstein distances or smooth entropies or maximum mean discrepancies. We have lots of these. mean discrepancies. We have lots of these types of results in the paper. And they actually capture both in the one sample and the two sample case, they actually capture the right behavior at the null and away from the null. So somehow the confidence sequences don't differentiate between are you at the null or are you away from the null? They just capture dpq, whether p is equal to q or not. And you can ask, do the rates at usually the rates at the null and rates away from the null are different, but the confidence sequences that we have naturally capture that. Is that we have naturally captured that they naturally degrade and they have the right rate at the null and right rate away from the null. So, so a very, I think, nice technique to get these kinds of time uniform statements for sequential problems. Okay, so that's what we've done. And let me spend the last few minutes telling you a little bit about what's a bit shaky. And here we'd love some feedback. We don't know where to go or if it's interesting and so on. Okay, so I think of graphs as being light. Graphs as being like empirical distributions, and that's what, like, if you show me a graph, I'm like, oh, that's like an empirical distribution. So, um, you know, one question I have is: given a sequence of random graphs, let's say these are node exchangeable, but we keep only the structure of the graph. We don't actually note the ordering in which the nodes were introduced. So then, does the graph itself behave like an empirical distribution in the sense that the sequence of random graphs forms some kind of a graph valued reverse martingale? A graph-valued reverse martingale. Is that something that's formalizable? Maybe. So, as I mentioned earlier, the empirical distribution is a measure-valued reverse martingale. So, is the sequence of graphs a graph-valued reverse martingale? Does that question even make sense? You know, for those who are unfamiliar, like some, you know, the basic definition of a node-exchangeable graph, I'm assuming most people have seen a long line of, and there's many people in the audience here, like Shubhu Bradasen and many others who have worked on this stuff. us and many others who have worked on this stuff with Jennifer Chase and so I'm sure lots of people have seen this but if you haven't um essentially node exchangeability means that the law of uh you know that sequence of graphs is invariant to relabeling of the nodes through permutation and so on so you can this is related to graphons if you've heard of graphons okay so um you know here's a simple fact that you can prove we this is not in any published paper this is just like some random notes that we have but if you have a function that is sub added But if you have a function that is subadditive, node subadditive, then node subadditive means that if I take f of g and a union of two node sets and the corresponding graph with a union of two node sets, then it's less than f of g u plus f of g b, which is the graph restricted to node set u and node set b. Then, you know, then you know, such a sub-additive node sub-additive node sub-additive node symmetric graph function is a reverse of Martingale with respect to. Reverse of Martingale with respect to the exchangeable distribution filtration. So, here we define the exchangeable filtration as the sigma algebra generated by node-symmetric functions of the random graphs. And so, that also yields an exchangeable filtration. And so, informally, anytime you don't care about which node is which, you're counting things. Like, let's say you're counting motifs, counting triangles, counting edges, things of that kind. Those are node symmetric in some sense. And those would be things. Sense and those would be things you can calculate under this kind of an exchangeable filtration. So, what this statement says: if you have a subadditive function, then it is a reverse submartible. You actually need a weaker condition. You need a condition which I call a leave one-out style condition that this condition arises actually very naturally in our previous work, and it helps us prove that there are many properties related to radar marker complexities, generalization error bounds for machine learning algorithms. We're able to show that. We're able to show that generalization error bounds for machine learning algorithms also hold at stopping times because things like Rademacher complexities, theory for fixed, all of the theory in the literature holds for fixed times, but it can be extended to holding for random times by observing that those functions satisfy this kind of a leave one out style property, which gives them a reverse sub-Martinger structure, which allows us to port fixed time results to time-uniform results. So this is kind of a weaker property that one needs. I don't know if anyone's seen this kind of a property. Needs. I don't know if anyone's seen this kind of a property in the graph literature. I'm not sure. But unfortunately, like natural graph functions, like counting motifs, are they're actually super additive, not sub-additive. So maybe one interesting direction is to think about the cut metric. Like I talked about convex divergences, but everything I was doing was real value things that made sense. But now we have graphs. So maybe we should think about the limiting graphs, which would be graphons, let's say dense graphs for the moment. Let's say dense graphs for the moment. And it's well known you can define a cut metric between these, between the graphs and the graphons. And so then is the cut metric between two graphons, is that a convex divergence in some sense? What is the object over which we're trying to make it be convex? And then hence, can it be sequentially tracked from two sequences of random graphs? So if I give you a sequence of random graphs of growing size, and you'll calculate the empirical cut metric between Empirical cut metric between those, you know, you know, the sequences of random graphs and things like that. Then, um, yeah, then is it I so I'm not sure, like, is it possible to sequentially track these things? Unclear. Okay, so I'm going to stop here, but question is, what is the right formalism for random graphs to talk about graphs being reverse, you know, martingales? Is any of this interesting or useful? Is any of this interesting or useful? I'm not sure. I should just acknowledge Aditya Gandharade, who's a PhD student at Boston University, with whom I've been talking about these kinds of things. But I'd love to hear from all of you who are experts on this topic of whether any of these would be interesting for graphs. Thank you. Thank you, Aritya. I guess there should be a lot of questions and comments. 