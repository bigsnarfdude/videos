Question. What I want to talk about today is a setting in which algorithmic fairness plays a dominant role in the inventory, a traditional inventory model, and to think about ways in which we can extend the kinds of algorithmic tools that we've used in settings which don't consider algorithmic fairness explicitly, or may even have to say. Pretty much shouldn't be there. I have to be less dyslexic than I might be. Just a little bit of a little bit of a little bit of a test. Cool. So, and in this work, which is joint with one of my Cornell PhD students who did this and is really the lead author on this work in a big way, but he did it as part of an internship that he had at MSO Language Board. So, we're going to talk about. So, we're going to talk about the joint replenishment problem with Indina and I towards fairness. So, indeed, and there have been a range of settings in which fairness conditions have been added to a layer of combinatorial optimization problems in any number of settings because algorithmic decision-making is driving these settings in a substantial way. And so let's just dive in. I mean, many. So let's just dive in. I mean, many of you may know what the joint replenishment problem is, but let me just start with it from scratch there. So imagine that I have n commodities that I want to purchase. And I have a finite time horizon, one up through capital T, and I have a set of demands for each commodity. So now what I'm doing in this representation is each shape corresponds to a different Shape corresponds to a different commodity type, and where you see the shape aligned with a given moment in time across this time axis, that corresponds to a demand for a commodity I at that time little T. Okay, so and what the joint replenishment problem is meant to highlight as an inventory model is the trade-off tension between fixed ordering costs and the cost of holding on inventory that have been that's ordered less frequently. That's ordered less frequently. So, the other key element that's going to be is a monotone holding cost. That if I consider ordering demand of type I at time t, and I place that order sometime earlier in the time horizon at time s, there's a holding cost that's there. And all I'll assume about those holding costs, though typically they're often modeled in a linear way over time, I'm just going to be assuming that they're modeled. Good. So there, and the way to think about this is that we're going to serve all of these demand points by a sequence of replenishment orders, or something called general orders. Or sometimes called general orders, in which in a given general order I can restock any subset of the different commodity types. And the way the cost structure is going to work is that we have separate bulk ordering costs for whether I'm placing any commodity whatsoever. And then those are general ordering costs. And then they're going to be specific replenishment ordering costs for each commodity type. These are going to be uncompatible. These are going to be uncapacitated models, and then once I've made a decision to go for a given commodity type at a given point in time, I can order as many of them as I want. So, for example, one set of decisions I might make is I might have indicated those particular start moments in time as being when I'm placing any orders whatsoever. So, just sort of, you know, as you think about it, that from this order, I'm going to. From this order, I'm going to have to service all of this. Now, as I move forward in a given period of time, I have to tell you, oops, I can't wait to tell you which item types I'm going to specify. So, in this case, I clearly had to order all of these. Here I'm ordering those. You know, I may have ordered these. Now, notice I have a triangle in my I have a triangle in between these two ordering points, where that just means I must have placed that order back there and so forth. Okay, so that's the decision-making setting for the joint replenishing problem in the traditional classical sense. Any questions? Really, just please interrupt if anything I say is. But the demands of deterministic, you know, I know everything. I'm dealing with an offline world, and you know, the simple offline version, yes, I know it's just, you know. Version. Yes, I know. It's just theory map. So here's a little bit of notation. I'm going to assume that they're capital N item types. The general ordering cost, the fixed order point for placing any orders whatsoever is going to be K naught. For each of the particular N commodities, there's going to be item specific ordering cost K sub i, and we'll let capital D denote the set of all demand parts. Demand parts. Now, the perspective and how I'm going to wrap in fairness into this is the point that, okay, in practice, not all demands are met. You know, Jake, you can set me up even better talking about stockouts. That indeed we're not going to be able to meet them. And so, as I think about what that means in a joint replenishment setting, I'm going to perhaps be making decisions in a coordinated way. Decisions in a coordinated way of which requests I'm going to reject or which items are going to be viewed as being stockouts, and that is going to then change what I'm going to do. Now, how might I model that? There are a variety of natural ways. There might be penalties associated with not servicing requests, or I might just put various constraints on exactly what limits. And it's indeed that latter approach that we're going to be taking. And one thing. And one thing is a recent paper of Swatikupta and Wong of sort of saying that there are fairness considerations in house stock bads get held. And if you just sort of take an intuitive view, the map on the left show various things about household income and socioeconomic group. And on the right, say, so stock out frequency. Now, there are all kinds of reasons that might be drawing it of what kinds of commodities are actually being stocked out. Are actually being stocked out. There's a lot that could get hidden under the hood that's there, but I'm just using this as an example to highlight that there are fairness considerations as you think about these coordinated decisions about which requests you're going to supply. So, I don't know, we're going to skip that slide. So now, one way that one can model it. That one can model it, very puzzled by the order of my slides, I must have screwed something up last night. That is that one way of thinking about it in a simple way, which has been used in a number of other fairness-augmented combinatorial optimization settings, is to imagine that I have associated with each demand point a color. So you can think about this as a demographic of who's making the request, and if I think about And if I think about a bounded number of requests that are going to be rejected, of stockouts that I'm going to permit, I may want to have a color-constrained restriction on how I'm going to limit to say that everybody has to share the pain in a balanced kind of way. It's not going to be done without being sensitive to that. Okay, so. Okay, so let's go back and sort of think about the sort of math programming tools. This is the end of the day. I'm just going to give you some of the highlights of some of the ideas that are there. You'll see me whiz over lots of your slides along the way. So if you go back to the simple vanilla, the first joint replenishment problem thing, one of the nice things that's there is that an integer programming formulation has a, the natural integer programming formulation has a very Programming formulation has a very strong linear programming relaxation. It can be used to design approximation algorithms in a very powerful way. Since I'm going to be layering things on top of it, I'm just going to start by reviewing what happens here. So you're going to have decision variables, we'll call them y sub s for a given order point s of whether or not I'm placing an order at that time s. I'm going to have a decision variable y sub s of i of whether that item was included in that order in time s. order in time s and I'm going to have service variables x of i t x of s of i t of thinking about is the demand point it serviced from the order point at s so of course then the objective function is I'm just going to add up all those things according to the things that I'm actually paying for that I need to be sure that for each demand point I'm actually serving it and then the usual kind of if-then kind of constraints Kind of if-then kind of constraints that if I'm serving demand point IT from an order point at IS, I better order commodity I at time S. And similarly, if I'm placed a commodity order of a commodity I at time S, I have to pay for a general order. So the classical standard integer programming formulation, as I said, there are all kinds of nice approximation items you can design straight off of that. Okay, let's just do things. Okay, let's just do things one add one step up. And imagine now that I'm just allowing the fact that I can reject some of the things. So I have this sort of notion of outliers. And again, this is an IP which has been used in the past. Now the natural thing is I'm just going to have a decision variable. Am I rejecting the orders for commodity I at time t or not? And as I said, I'm going to think about it in this bounded way. About it in this bounded way. I have a total R of how many rejections I'm allowed to do, and I still want to do it. So that's the non-fairness kind of thing, but a bounded rejection. But now I can just go up to the colorful version of this. And this is really just then I have a different allocation of how many rejections I can do. I can even put in a weight that corresponds to the characteristics of what it means with respect to. Characteristics of what it means with respect to a given color C, and I can, while I'm at it, throw in a penalty for rejections as well. So I can ramp up this integer program to do the full range of things overall. One thing which I'll use as an example when I go forward is that amongst the kinds of general holding costs, one kind of holding cost is very simple to sort of get some of the intuition of the algorithms that we're doing. Of the algorithms we're doing in place. And that's imagine that the only element of holding cost that I care about is perishability. Either I have room to do it, but at some point that commodity is going to spoil, and in fact, the holding cost becomes infinite. So I really end up with sort of a feasibility element to that. That simplifies the integer program that I gave by getting rid of the holding costs. There actually ends up being no distinction between the assignment variables and the Between the assignment variables and the order variables, and we end up with a cleaner integer program as well, assuming that my cleaner goes forward. So we end up with an integer program that looks like that in this infinite case. I've been talking all along, like this integer program is going to have an LP relaxation, which is good, but has an integer program that is horrible. And that even with one item type, that one can get an unbounded integrality gap. One could get an unbounded integrality gap for that intro program. So I've now set you up to believe that all life is good and we're going to go down lane, and it seems like I've taken a left turn, but in fact, we'll recover. Sorry, David, or maybe you're like, this is caused by the fairness thing or the perishability that makes sense. This is just perishability. So this is, I mean, this is down to a very simple model. I'm not talking about fairness. This is just once I allow. I allow for a bounded number of rejections, I've got problems. So it's perishability combined with rejections. Yes. But perishability is just one particular structured kind of holding cost. So, yeah. Okay. And now, approximation algorithms in this domain have been well studied. I won't go Well, studied. I won't go through all the various constant factors. What are sort of the bottom lines of what we've done? In this work, we're given the first constant approximation algorithms that handle these sort of, in some sense, either colored versions or actually you can think about it as bounded dimensional feature vectors of what you're trying to do. And if you sort of think about the contributions to that feature vector. The contributions to that feature vector of how rejections contribute, that's really the true setting in which we're doing, and we're able to build constant approximation algorithms. But along the way, we built enough machinery that even just the simple rejection versions, we're getting improved constants in terms of the approximation guarantees. And indeed, this builds on a range of literature that's talking both about sort of bounded outliers, about the sort of a few papers that talk about these color. A few papers that talk about these colorful versions of problems to capture notions of fairness. Yeah, I'll do something to give you some algorithmic idea, to give you some hint as to what's going. So again, going back to the very simplest version, again, in the perishability domain, why does the LP in that case do well? One of the reasons it does well is that if you think about the fact that you had these y variables which were indicating Y variables, which were indicating whether or not I was placing an order. In a fractional version, they're no longer 0, 1, that I have some fraction of an order being placed at a given moment in time, in this case, from times 1 through 10. And in some sense, the LP in a fractional way pays for fixed orders. And so as those fractions cumulatively get to the next integer, then you've cumulatively across the LP paid for one of those orders. The LP paid for one of those orders, and that can help you determine order points along the horizon. You can do it in a purely deterministic way. So, in this case, if we place it at every time we hit an integer boundary of that, we actually place an order. So we order at time one, at time four, at time five, and so forth. And then we do that. And this allows us to leverage this. And a standard other trick that gets used is. Gets used along the way is that, and this allows for various expectation calculations that allow better balancing for all kinds of things, is that, okay, you can do the same thing, but do a random shift. And all of a sudden, things that it's sort of the randomized rounding equivalent of how you actually introduce randomization into this context. So, these are all tools that are well known overall. One of the things that happens, of course, is that, and this is That happens, of course, is that, and this is to sort of impress upon you in doing it at this scale as to why this bounded constraint version is hard or has an element of hardness, is that you're only building in expectation satisfaction of the projections. And that's sort of a key element here. So, I want to give you a hint as to the overall structure. We are going to make use of the LP, and in some sense, we're making And in some sense, we're making use of the LP that the LP fails, and if I had gone through the bad instances a little bit more carefully, you would have seen, fails for very stupid reasons. It fails when you really don't place very many orders and when an overwhelming fraction of the order of the cost is absorbed by a very small number of fixed orders. Now, if that's true, first of all, in practice, you know what you're going to do. You're going to figure out the hypocritical. You're going to figure out the critical orders, and then you're going to use the LP technology to figure out everything else that isn't so obvious that you have to do. And we're going to do exactly the same thing in terms of the approximation algorithm. We're going to fix a big constant. I'm not going to care about how bad that big constant is. This is all about, you know, principle, what we could do in theory, not really what we would want to do in practice. And we're going to sort of guess the M most for some being constant M, the M. For some being constant M, the M most critical order points, and also the most expensive item type, signal item orders that we're going to place. And what we're going to do is say, okay, fix those variables and then use the LP to solve everything else. And then having done that, even then, what we're going to do is we're going to use the LP solution to split the set of demand points into two. It's just then we're going to use two different teams. Into two instances, and we're going to use two different techniques in order to take care of the demand points in those two links. So, here's a sort of a schemata of what I just said. We guess the most expensive pieces, we strengthen the LP relaxation, we solve it. We're going to use that LP relaxation in exactly that first schema that I use in terms of knowing some critical thing to compute some general orders that I'm going to want to do. Some of those orders. To do. Some of those orders are going to actually naturally be able to take into account some of the demand points. Others won't. And that's how we're going to do it. And then we're just going to merge things together. Maybe this is telling me I should be over. Okay, I don't know. It's just a black slide, Doctors. Oh, I see. I clicked the wrong button. Okay. Okay, so indeed, this is what I just said a moment ago: that the first instance are going to be: we're going to place these initial order points in a way exactly analogous to what I showed you in the very simple ganila version. And the demands that actually are feasible, remember, I'm in the perishability case, so I've got some general demand points as far as the holding cost is concerned. Is it Cost is concerned. Is it feasible to place those orders for those general orders? If so, place those orders. That's going to take care of some of the orders, but not all of them. And what we're going to do is use the LP to guide, we only had a total rejection cost overall for each color or whatever. And we're going to say that the LP then tells us which of those demands go for here, which of those demands go for there, and that allows us to partition. And that allows us to partition the rejection bound into two separate instances, and so we can satisfy the two pieces separately. I'm now going to whiz through slides that I'm not going to actually talk about because really I want to get us back on schedule for the panel. As I highlighted, there's sort of two underlining techniques that we use. Hype edge rounding is one thing. Use. Hype edge rounding is one thing which, as a general technique in the theory of approximation algorithms, is a bit underutilized. And I mean, in some sense, the high-level view of hypotranding is that I have some constraints that I know has to be satisfied with equality. And then I have some direction that I can move towards making something more integral because I want to get rid of fractional sources. Because I want to get rid of fractional solutions in my rounding. And I sort of could think about two antipodal directions. And one of the good things is that one of those two directions is going to be downhill. Now, typically, how pipett rounding gets used in literature is it's the tension of just two variables. And in some sense, one thing that's new in our work is we're defining sort of hyperplanes of equality and sort of having a hyperpipage rounding that just allows us something that's projected. That just allows us something that's projected into this hyperplane that says that, okay, this is what's going to remain constant, and that's going to allow us to do the inductions we want. I'm not going to go through all those examples. One thing, when we get to multiple colors, that's already set cover hard. So, if I allowed an unbounded number of colors, I can capture set cover. I'm never going to do anything better than log in. So, our results are for a constant. Our results are for a constant number of colors, and the running times are exponential in the number of colors. And that result says that we need that from a complexity point of view. The exponential for the guessing? Sorry? The exponential in the colors because of the guessing. You guess for each color at the end? Yeah. There we are. You're just good at just setting me up, right? So indeed. So, indeed, that's the guessing. Amongst the games that one can play, that I've got this balance between the first kind of things that are feasible for the original general orders and the residual, that in fact we can sort of mix and match a little bit about what happens in those two phases, and that helps us to achieve somewhat better constants. And so, in general, I want to sort of take a step back. Sort of take a step back. I mean, what I'm trying to view as sort of a bigger agenda is what kinds of algorithmic tools do we have in our toolkit that can be extended to capture what kinds of fairness instruments? And in some sense, the bottom line is for this setting, both from a theoretical point of view and from a practical point of view, we do have the tools. Now, yeah, I'm not talking about, you know, it's Yeah, I'm not talking about, you know, it's exactly this in terms of understanding that the LP rounding technology can play a role to capture this if we sort of combine it with some, in effect, some natural, greedy, you know, partial enumeration that gives us. But the analysis tells us what to be mindful for and how to design the heuristics for the real problem. And I'm not saying that we should be thinking about. That we should be thinking about reasonable constant algorithms that are exponential to the number of features that we're thinking about, and really quite badly exponential. But in some sense, the reason for thinking about these kinds of theoretical models is to highlight, if we're trying to capture what kinds of constraints in what we're trying to achieve, how we should think about designing heuristics so as to be able to extend our current tool set. So, in some sense, that's sort of the meta message. That's sort of the meta message. And now I'm going to switch for a pure advertisement. One thing that there will be a sort of, hopefully you won't be able to avoid it, a series of announcements is that a group of us have been organizing a summer school that will take place just the very end of May. The idea is to bridge between the CS community and the OR community, and in general, And in general, a broad number, because last time I said they said, well, what about the statistics community? And what about, you know, in general, to bridge across the multiple communities that are having, being impacted by and trying to have an impact in AI. And the structure will be a series of overview lectures helping to sort of make sure that the group of students has a sort of common language. Common language. And then there'll be two two-day modules, each on a given theme where there is sort of a footprint in multiple communities, one exactly in fairness and one in reinforcement learning. And the last time somebody put up a slide like this, someone reported back to me that Chomsky doesn't actually know she's doing this. So they just volunteered her without actually letting her know. So I'm a little hesitant putting up all the names, but I think everybody else knows that they're really doing this. Everybody else knows that they're really doing this. Chopsy knows by now. And the idea for these two-day modules is that they'll be really a collection of sort of brainstorming exercises and even some hands-on starting points of trying to get a research foothold with some data sets curated in advance overall. So it'll be a mixture. Overall, so it'll be a mixture in two days of some amount of tutorial lectures, and there. And the idea is to bring together something of the order of 15 to 20 PhD students sort of coming into their second years. They're getting from the OR community and 15 to 20 students from the CS community, and then really trying to sort of build research teams, small groups that go across, that sort of help connect overall across the two communities. So a bit of that. So, a bit of advertisement, but maybe actually a good setup for the kinds of issues that I know are on the questions for the panel that's coming up as well. And with that, assuming I can do that, thank you very much for your attention. For the first time. Just recorded, yes.