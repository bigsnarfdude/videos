I'm going to present a work with SHA, which we published in code this year, and it's on inherent limitations on characterizing dimensions for learnability of distribution classes. And it's a trend in machine learning where it's been always a central topic to search for dimension-based characterizations of learning problems. So the most popular one is the VC dimension for binary classes. The VC dimension for binary classification, then there's also the Little Stone dimension for binary online learning. Recently, there was this DS dimension for multi-class pipe learning, and there are many more. On the other side, there are many pipe learning problems which do not have such a scale and variant characterization. Most importantly for this talk, distribution learning, so path learning of distribution classes. Of distribution classes, but our result we also have results for classification learning with respect to a class of restricted marginals, then classification learning when the prior knowledge does not come in form of a hypothesis class but in the form of a generative model, and then also learning real-valued functions with real-valued losses. And in this work, we show that all of these tasks do not have. All of these tasks do not have a scale-invariant characterization. So, in order to make this claim, we of course need a formalization of what it means for there to exist a combinatorial dimension. So, we give two formalizations, one qualitative one and one quantitative one, and then we show that no such dimension exists for these learning tasks. And therefore, in particular for distribution learning, but also for a number of other learning classes. So, just as a reminder of what we could want from a characterization, here is the fundamental theorem of statistical learning for binary classification, where we say the VC dimension characterizes park learning qualitatively in the sense that a class is A class is parked learnable if and only if it has finite EC dimension. It also characterizes park learning qualitatively in the sense that it gives an upper bound and a lower bound on the sample complexity. So for both of these notions, we will give a formalization. I will start with the sample complexity one. And we only, so we saw that for the VC dimension and pipe learning of binary classification, Part learning of binary classification, we have an upper bound and a lower bound. But we are showing negative results, so we only focus on the upper bound for now. So we say: so, given a learning task and the set of all possible classes, we say a weak sample complexity dimension is a mapping that maps from the set of all possible classes to the natural numbers and infinity. To the natural numbers and infinity. And we also need a function f that then takes this natural number and parameters epsilon and delta, such that if we take the sample complexity for a particular class q, the sample complexity is upper bounded by f, the dimension of q, and then we take epsilon and delta into account. So the intuition here is that there is an upper bound. Here is that there is an upper bound that only depends on this dimension and doesn't need any other information of the class. And then in our work we show that basically the number, the family of potential sample complexity functions that we get for distribution learning is too rich to be captured by the natural numbers. Yeah, so we do this for distribution. Yeah, so we do this for distribution learning and for the other tasks I mentioned. And before I go into the details, I'll give the other formalization. So this notion of finitary characterization was a refinement from Charles' previous work. And here we kind of want to focus on the structural aspect of the VC dimension. So we say the VC dimension, so we say a finite recovery. So, we say a finitary characterization of a learning task is a countable set of formulas such that a class is learnable, is not learnable if and only if the class satisfies all the formulas in the set. So the second condition is that for every element of every formula in the class and any And any formula in the finite theory characterization and any hypothesis cluster that satisfies this formula, there is a finite evidence set such that every superset of that evidence set satisfies this formula. And so the intuition behind this is the formula A alpha D basically represents basically represents H shatters are set of size D. So for the VC dimension, we can think of these formulas as the VC dimension of H is greater or equal to D. So a class is not learnable if and only if all of these formulas are satisfied and for And for any of these formulas, we have a finite epidemic set. So we need only 2 to the D many functions to show that a set of size D is shattered. And this shows that the formula is satisfied. And then any superset that contains these 2 to the D functions satisfies this formula. So are there questions about this definition? Because it's a bit abstract. And you can also challenge it if you think it doesn't match your... It, if you think it doesn't match your intuition of what a dimension should be. Okay, so I guess there are no questions, so I move on. So this definition matches, kind of satisfies a lot of the, or satisfies by a lot of the notions of dimension that appear in the literature. In particular, the Ledoson dimension and the BS dimension satisfy those. And now our results show that there's no such dimension for distribution learning, and there's also no weak sample complexity dimension for this distribution learning. Now, our results are based on this general theorem. So we say that any task that satisfies the following two properties cannot be characterized by a finetary characterization. Characterization. First, this is a very weak requirement. Every finite unit of learnable hypothesis classes should be learnable. This is the case for, I think, most tasks that we can come up with. And the other one is that we need there to exist a class Q0, which is learnable. And then we need a sequence of not learnable index by k. For every k, q0 is an epsilon k approximation for k, and the epsilon k need to go to zero for growing k. And this approximation is measured like in is with respect to the task relevant metric. So for distributional learning this would be total variation distance. Is this for proper learning as well or? This for improper learning as well, or um so um this is so we only focus on improper learning um because um basically um for proper learning you get uh okay sorry um I guess it depends on your definition of so okay for realizable learning proper and improper learning are the same for distribution learning and for I can And for agnostic learning, it can make a difference. So I get into the definition of pack learning soon, and then we can talk about that, but good question. So I just showed this result and then I go into the details of the definition. And then what's the other theory? If I get to it? So far, you're just talking about any general learning problem. It has nothing to do with distributions. And you're just setting the state of what does it mean to have, what does it mean even to find dimensions? Yeah, so I mean that for any learning problem. Because I think these are different from algebraic dimensions. So if you have a topology, for instance, I don't know, right? For instance, I don't know, right? So they're not defined this way, right? Yeah, so we only show something for the definitions that I mentioned before of dimension. And of course, we can have a discussion about whether this captures your intuition of what a dimension for inequality should be. But does that answer your question? I'm just trying to understand, like, this tailored to us getting samples. Towards like getting samples and having a number, like that's not the element of something. Nothing to do with something yet. Nothing yet. Nothing yet. So basically I show, I first show that if those two properties are fulfilled, then a task cannot be characterized. And then I show that those two properties are actually fulfilled by distribution. And for that, I actually need to look at kind of statistical properties. Shall I move on? So, this is a pretty simple proof. So, by way of contradiction, assume that we have a finite degree characterization for such a random task, fulfilling the two conditions. Now, for every i and every k, the hypothesis class Hypothesis class, Qk satisfies the formula alpha i because qk was defined to be not learnable and any not learnable task satisfies all the formulas in alpha i. And then by the second property we also have these finite evidence sets. So we can look at these evidence sets qk alpha i and then we know for every superset of those. And then we know for every superset of those that alpha i is satisfied. And now we can look at the following hypothesis class. We now take the union of q0 and then we take the infinite union qi alpha i, where we go for over all i's. So basically for the ith hypothesis class, we we This class, we take the ith formula and look at the corresponding evidence. Now, by definition, every alpha i, there is a hypothesis class qi such that satisfies it. So, there's this evidence that Qi, alpha i, that is a subset of P, and since every super And since every superset of those evidence sets satisfies the formula alpha i, p satisfies every formula alpha i, therefore it satisfies the whole class. And thus it is a thus by according to our definition, this class should now not be learnable. And then we show that this class is in. And then we show that this class is, in fact, learnable. And this we can easily do by basically saying for every fixed epsilon, we can look at epsilon k. And now we know that this finite class, or that this union of finite classes is an epsilon k approximation. So if you want to learn until up to epsilon k over 2, it's sufficient to it's sufficient to learn this. So if you want to learn up to 2 epsilon k, it's sufficient to learn this. This we can do by the first property and then we have constructed a learnable class here that according to W should not That according to W should not be learnable. And that's how we get an impossibility result. Are there questions about this proof? Yeah. Not about the proof, but which of the conditions are not satisfied by a standard PAC learning? For standard PAC learning, we do have a dimension. Which of the conditions is not satisfied? So for park learning, we don't have this construction of not learnable classes that we can approximate by learnable. That we can approximate by a learner hook class. So, basically, the key thing is we will see the construction in a bit, is that basically for different epsilon, we get different learning behaviors. And for binary classification, we just can kind of characterize pipe learnability independent of epsilon. So, that's kind of the main property that this describes. Yeah, so now to show like this main theorem, we just need to show these two properties. The first property, I hope you believe, that every finite union of learnable classes is learnable. And for the second property, we look at the following construction. So for Q0, we just pick a hypothesis class that consists of one distribution, and this is the distribution that has And this is the distribution that has all its mass on the point zero. And then the QK oh, sorry. The QK looks like this. So we take a mixture of two distributions, one heavy part that is centered on the point zero, and one not heavy part that is set on some arbitrary. Set on some arbitrary uniform distribution over some finite subset of the natural numbers. So if you want to look at this class for some fixed k, we see that this class is very easily learnable if you just want to learn up to 1 to the k. learn up to 1 to the k because we can just learn by looking at this distribution that has all its mass on the point 0. We don't need to actually look at the data, it's sufficient to just output this deterministically. But then if you want to learn for a much smaller epsilon, we would actually need to approximate a uniform distribution over some other epsilon. That over some arbitrary finite set, and that's not doable because that's a not learnable problem. And or like if you put it in the terms of the previous theorem, we can approximate every class QK with Q0, but the class itself is not runnable. So, and that gives us the theorem. Are there other questions about this proof? Otherwise, I'll just go on to the weak dimension. So remember, the weak dimension was just a mapping to the natural numbers that gives us an upper bound. And in order to show that no such thing exists, we need to define a sequence of Of, we basically define the sequence of learning rate such that for an arbitrary class, we assume that there was such a dimension, then there would need to exist such a sequence of possible learning rates. And then given that f, we can construct a class for which the sample A class for which the sample complexity grows faster than that, grows faster than any element of that sequence of rates. And the notions that we rely on are like here is the notion of co-finality. So for two subsets of some ordered domain, A and B, we say A is A and B, we say A is cofinal in B. If for every element of small B and B, there exists some element small A and A, such that A dominates B. And we look at the notion of co-finality for the ordering of eventual dominance. So basically, we compare the two functions by which one grows faster in the limit. And then we can again very generally show that any task for which there exists a sequence of tasks which are a sequence of classes such that these sample complexity functions are co-final in the sequences over the natural numbers, then there's no The natural numbers, then there's no such characterizing dimension. And then we can show that this is the case for distribution learning. I just go over the construction. Very similar construction to D4, just that now we don't only index by k, which is the weighting parameter between the heavy part and the part with low. The part with low weight, but that has like that is hard to learn. But here now we limit where this second part can sit. So now we say we take a mixture where the part of the mixture that has weight 1 over k is a uniform distribution over some set A, which is a subset of the natural numbers until N. So those are like, so for the class Q K4, this would look like this. And then the number of possible, so for QK8, we basically have more flexibility. So we have now two parameters, one that controls at which level. At which level learning becomes hard, and then the second parameter n controls how hard learning becomes. So if you look at the sample complexity for some class QKN, if epsilon is greater than 1 over k, learning is easy, it can be done with constant time. But if we look at as much smaller epsilon, Much smaller epsilon, the sample complexity is linear in 1. And then we can just kind of combine different qk qn's in order to control the sample complexity. Yeah, I guess we don't have time to go into the details. Or does anyone have want to go over the details or has questions about this? So, basically, for any given sequence, we can now define a sequence of classes where the hardness of learning up to one over k depends on k. And then assuming that this g That this g would be the rate that is given from the weak sample complexity function. We can use this to construct a counterexample. And then we can do the same, basically, the same kind of construction for all these other tasks. So, for a classification learning, when the Classification learning when the class of marginals is restricted and for classification learning with the prior knowledge given as label conditional marginals and also for learning real-value functions. And the conclusion is that any of these characterizations would require scale sensitivity, would require scale sensitivity. I'm sorry I kind of put that I'm sorry I kind of put that, so the definition of learning that you asked before because I wanted to go back to that part. Just realized I didn't answer it. So this is the distribution learning task that we are focusing on. And the park learning definitions look like this. And basically, Basically, proper and improper learning are equivalent up to a factor of an addition of one when it comes to alpha. And all of our results we show for improper learning, but there's a very tight relation between the two. So that's because we can reflect the distribution or We can like predict the distribution or something like that. Proper and improper learning are improving and improper learning. Yeah, exactly. It's basically you find the best within the class and then or you find something that might not be in the class but is comparable to the class and then you can just kind of find the best within the class. And then there's like an additional approximation error that you need to add. That you have to add. So you said this was for us non-scale sensitive dimensions. How do you actually define being scale-sensitive? So basically this would be that like scale sensitive would so all we show here is basically that we get different behaviors for different epsilons, right? That's kind of what all of this relies on. And then you can have like a definition where you could basically have the sorry, have basically have this definition, but it would be you would get a definition like this that would for every epsilon you would have a formula doubly of set of formulas. What the other dimension? This one. So you do have you do allow dependence on epsilon in the sample complexity upper bound. So Complexity upper bound. So, what's missing? So, the dimension itself, so the mapping D does not depend on epsilon. And then if you, for example, if you look at shattering dimension, which is epsilon dependent, or like is scale sensitive, it would have this dependence inside the function. Yeah, exactly. And then the same thing would hold here. You would just, for every Here, you would just for every epsilon, you would have a different W, and that would just blow it up. So, basically, yeah, those are the results. A few kind of future directions would be kind of: can we do something for scale-sensitive learning? And then in this case, we would get like again a distinction between. Again, a distinction between two possible versions of this characterization: one being uniform, one being non-uniform, and the notion of uniformity basically, like in this original definition, we only say that the evidence sets need to be finite, but for dimensions like the BC dimension, it's uniformly bounded. But some new notions, for some new notions, this is. For some new notions, this is not uniformly bounded. So, basically, for distribution learning, the next question is whether there is a scale-sensitive dimension, and then whether it would be uniform or not uniform. Yeah, I think I'm over time. Thanks. So, you talked about distribution then with respect to pretty much Do you think it would pretty much apply to other oceanic questions? Like, you have to ask this to normally? I would have to think about that. I'm not sure. So, basically, if it satisfies the same kind of properties with the epsilon sensitivity that would be a case, maybe it's like a good exercise to be done. Oh, we don't have that. And are you willing to record on our break, right? Yeah, no break. 