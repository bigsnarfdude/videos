Joint work with several of my collaborators at the University of Toronto. So, as we heard over many talks in this workshop, that machine-type communications for IoT type LRTPs and for sensor networks is a crucial requirement for future 5G or 6G wireless communication systems. So, in 5G system, we have this requirement triangle. Enhanced mobile broadband is one axi. By broadband, is when Xi, massive machine-type communications another, and UILC auto-liable relativity communication another. In 6G, now a triangle becomes a hexagon, but massive communication remains a crucial requirement for cellular cables. So, we're talking about a huge number of devices who can potentially be active, but the traffic is sporadic, so that at any given time, only a few of them are there. At any given time, only a few of them are talking about it. So, this talk is about how to design a model system to support massive connectivity. So, the type of questions we're going to ask in this talk is, should we use contention-based strategies or scheduling? And we're going to look at the question of how do we detect sparse activities in the context of massive MIMO systems. And we're going to ask the question of what is minimum required, minimum feedback required to schedule users into To schedule users in an uncolliding fashion. And the main points of this talk is as follows. Luca claimed that to support massive connectivity, the use of non-arthogopilites is inherentable. And further, compressive sensing techniques are indispensable for these device activity detection problems. And MASMIMO can significantly enhance device activity detection. But in many cases, channel estimation is actually the main bottleneck. So we have to take that. Actually, the main bottleneck. So we have to take that into account. And we're going to make a point that scheduling is a viable alternative to contention for London access. And finally, we could characterize how much correlation, how much coordination bits are required to ensure the scheduling is done in a multiplied fashion. And we're sure that scheduling feedback required for completion of pieces. The scheduling requires is very low. The amount of number of bits required to schedule it's great. Parts here, to schedule it's a bit. Okay, so let me start with the story. So, we're looking at a system where you have large limited potential devices, but only a few of them are active. So, we talk about 10 to the 5 or 10 to 60 devices with radio traffic. So, the traditional way to provide random access is to use aloha or their variations, where the scheduling, the scheduled user, is uncoordinated. In a sense, the transmission of the users are The transmission of the user surveys are contention, then you have to do some content resolution strategy to make sure that there's no collision at the base station. As an alternative, we can look into coordinated random access. So this requires the base station to detect who are the active users and further to transmit a few bits to those active users to make sure that they're scheduled and uncolliding fashion. So, this talk is about the cost and the benefit of coordination. So, we're going to look into So, we're going to look into both how you detect the number of users, the number of these active users, how to design those farms to actively detection outcomes, as well as what's the minimum of bits required to ensure an alignment scale. So these are the two questions we can ask. So let me start with a classical slotted Aloha strategy. In classical Aloha, it's involved contention and it's uncoordinated and there's no communication between the base stations and the users. The base stations and high-end users. So, what we do is that there's a finite set of orthogonal high-defin sequences, and each user will pick one of them at random. And transmission is successful if no other user also picked the same sequence. And if there's collision, then users have to re-transmit. So, this is a classical slide of Aloha. So, the classical analysis of Aloha is as follows. If you let x to be a random variable denoting the number of users who are transmitting any particular slot, then because x is Start. And because x is the sum of independent Bloody trials, it follows a Poisson distribution. And if you optimize the parameter of the Poisson distribution and recognize that successive transmission happens if and only if that there's waves are active, then if you optimize the boundary of the Poisson distribution, you see that the optimal boundary is not equal to 1. And this means that you're successful with a quality for nobody. And in other words, 63% of sauce are wasted either because nobody's Slots are wasted either because nobody's transmitting, or if more than one user is transmitting, so they can't. They have to do solve calculations as well. So, this is the classical aloha. And you can do better if you do coded slot aloha. So, this is the work that I was mentioning yesterday in the talk. And what you could do is to use an error correcting code and let user transmit at multiple slots and to have coded transmission across multiple slots. Then, some of the slots will be done colliding. Then, based on these non-colliding, Will be non-colliding, then, based on these non-colliding slots, you can do error decoding to resolve the other slots and allow other users to be detected same time. So this is the idea of a coded slotted aloha. And if you use repetition code, this is called contention resolution diversity slotted aloha. And this idea is very similar to erasure decoding, decoding over by an erasure channel. We have these variable nodes and check nodes, and you work by these synchrotrons, and try to decode. At these synchrotrons and try to decode one at a time and do this unrepeating so that you can decode user sizes over multiple sites. Now, if you optimize the code, which means that you could use an optimized user-node degree distribution and use something like robust solvent distribution, then you can theoretically get all the wasted slot back. Okay, so if you optimize this degree distribution, then overall throughput can go as high as So, as high as one. And this is an asymptotic result as a number of users protein unit. So, this is the coded SMAT aloha analysis. Now, if you look at this code in Smart Aloha, there are disadvantages of SMART aloha. One is that multiple transmissions are needed for each user, which means that there's increased power consumption. And if you want to do collision resolution, it also increases delay. Also, increases delay. And finally, you have to use a practical coding strategy. So you would have less than optimal frequency. So this is the main problem with coded slotted adults. So in this talk, then we're going to look at scheduling as an alternative approach for contention. Now, scheduling, of course, is much easier to do because the user can be scheduled into an interactive slots. However, it requires the base agency to coordinate the scheduling, so it requires feedback kits by the basic agent users. Feedback gets fundamentated users. And the contention-based strategies are often justified based on the assumption that the cost of coordination is too great. And what we really want to characterize in this talk is what exactly is the cost of coordination, how many bits are needed to coordinate these users so that one know the line. Okay, so that's what we're doing. So, in terms of coordinated random scheduling, we're going to look at strategies, this is a strategy where we have multiple phases. In the first phase, Phases. In the first phase, there's activity detection. Out of total N users, K of them are active, and they will transmit pilots to identify themselves. Then the BaseStation will detect those K users. That's phase one, activity detection. Then there should download feedback. The BaseStation will send a common feedback message to all the active users so that they can be scheduled into different slots. So in phase one, we have to look at the question of how to perform activity detection. Then, once users Action. Then, once users are scheduled, then you do uplink payload transmission. So then, users are transmitting multiple slots, and these are scheduled transmission. But what's the real question is, what is the minimum amount of feedback needed to ensure collision 2? So that's the question we've been answering. All right, so let's first go to the user activity detection part of the strategy and to see what are the algorithms available to us to do user activity detection. So channel models as follows. So, channel model is as follows. Y is a received signal. Alpha is an indicator indicating whether a device is active or not active. And if it is active, it will transmit a signature sequence, Sn. Sn is a signature sequence. And this Sn goes undergoes the channel, so the Y is received signal, which the sum of all the signature sequences received from all these active users. And all the other users are non-active, so they're analyzing. So if you look at this problem, this is really a compressive sensing. Look at this problem. This is really a compressive sensing problem. Okay, so consider a case where the Bayesian has multiple antennas. So the received signal over L pilot, the pilot sequence of the length of L, the received signals can be thought of as a matrix of L times N matrix, where M is the number of antennas at the base station, and L is the length of the signature sequence. So you can write the overall channel model as follows. You can stack the signature sequence as column. As columns of this procedure sequence matrix, then if a user is active, that means its row is non-zero, where this row is the channel of the active users. So if you do this matrix multiplication, you see that this is how it comes out. Basically, the signatures multiplied by the channel, each user experience, each user's channel is one by n vector. And when they multiply together, they get this L by N integers. And so your job is to detect which user. And so, your job is to detect which users are active. Also, you don't know the channels, so you potentially need to understand, you need to estimate those channels as well. And this is why this is a compressive sensing problem, and specifically it's a multiple measurement compressive sensing problem, a multiple measurement vector compressive sensing problem. So, they can be solved using algorithms like a parsite message passive AMP. We're going to have a post-session AMP at that tomorrow. So, let me have a brief introduction of what AMP is like. Suppose we have a single antenna case, then you only receive Then you only receive an L pilots at a base station, a base station with a single antenna. You can detect which of the columns, which of the entries of this column vector is non-zero and which ones are zero. So you have these sparsity in the activities. So you can formulate a problem like a Lasseau problem. You try to find X that matches Y as much as possible while penalizing the sparsity on the sparsity of X. So what we're doing. So, one way to solve this is to use something like ANP. And central to the ANP is this soft thresholding function, which comes about in this form, so that if the received signal is close enough to zero, then it probably is zero because it's probably the noise. If it's further away from zero, then it's probably just the input. So, this is this soft structure wing function is central to the analysis of A and P and reflect this. Of ANT and look like this. Then you can derive the ANT equations via graphical models to think of it as a message passing algorithm based on the check nodes and variable nodes, where they pass messages against each other to each other. And if you do a mini-max type of analysis, then soft thresholding emerges as central components. And what's very nice about AMP is that there's a scale of evolution that describes the progress in the iteration. So you can analyze the progression. So, you can analyze the progression of the well. Now, this minimax thresholding, soft chesting emerges as a minimax solution, but if you actually know the channel statistics, you can actually design it by denoising. So, here's how the AMP work. AMP basically does three steps. It does correlation with the residual, then it does a denoising step, then the iterative step. So, if r is a residual, r is y minus s times x. x t is my current estimate of who is non-active, who is active. An estimate of who is non-active, who is active. If y minus s of x is residual, then you do a matched filtering of the residual and add it to x, and you do a demoiser. This eigenfunction needs a demoiser to get a new estimate of x. Now, this AMP has this ensounder term, the conflict of physics analysis, that improves the performance of this evolution analysis. So, this is the AMP, and as I mentioned, that if you can actually take advantage of the channel. Can actually take advantage of the channel statistics to design better denoiser. And the state evolution of the MP actually allows you to characterize, this is the evolution, basically characterize how much noise, how much effective noise is in the residual. And if you go through this analysis, and if you take advantage of the fact that you actually know the channel statistics, you can actually analytically write down the arrow of the overall detection. Of the overall detection, both in terms of the niche detection error as well as possible probabilities. So the optimum detection rule is based on this log likelihood ratio and based on this threshold. And by changing this threshold, then you can trade off these two types of errors, type 1 and type 2 variables. So here is a plot that shows that if you do this type of analysis, it actually does very well in terms of agreement with the experiments. So under different types of Under different types of thermal transmission power and internal parameters, you can actually understand this AMP analysis. Okay, so this is the single antenna case. If you go to multiple antennas, then you can use the M and D version that we do in the algorithm. And you can actually prove that in theory, as number of antennas basically go to infinity, the perfect detachment is actually possible. However, if you run this Run this multi-MNV version of ANP on these types of problems. In practice, multiple antennas present a many challenge. This is because convergence becomes slower. That's one reason that multi-antenna ANP does not work as well. And the second reason is that the channel estimation is poor. So QANP tries to estimate the channel as well as trying to figure out who are active or who are not active. And because every user uses a random signature sequence. Is a random signature sequence and those sequences are not authoral, then these non-arthaminal sequences introduce essentially product contamination, and that's a bottleneck when you use ANP type of algorithms to do user activity detection. And channel transformation is upper bounded by the channel transmission is poor due to the contamination across these paths. So, this is how MP algorithm runs. In the MIMO case, again, you have the computing residual, then you do match filtering, then you do denoising. So this is the steps involved. So as a quick recap, the AMP is a practical algorithm. It's a practical sparse user activity detection algorithm. It has a state evolution, which is good. And we have a denoiser that can be designed to match. Be used to design to match channel statistics and detection becomes accurate as a number method. The implication for network design is that non-orthographic pilots should be used for massive random access. And massive MIMO needs to be deployed for good user activity detection performance. And AMP is a good algorithm, but its performance is limited by the fact that the pilots are non-orthogonal and have. Fact that the titles are non-orthogonal, and that limits the performance of channel estimation. So that's the main drawback. So, can we do better than that? It turns out that you can do better than that if you forego the requirement to estimate the channel. So, instead of formulating the problem of estimating both the user activities and trying to estimate the channels, but instead of doing that, we reformulated the problem and only detect the user activities and not worry. Detect the user activities and not worry about channels. If we do that, then we can actually do better. So if you look at this input-output relationship between multiple users and the received signal, and you can rewrite this in the following form, where you extract the user activity and the channel separately. So you have a diagonal matrix indicating whether the user is active or not active. And these diagonal entries are zero. If they're not active, Diagonal entries are zero if they're not active, and the diagonal entries is the large-scale fading if they are active. Then you can extract the small-scale fading into a separate matrix. This is just H matrix. So this is the equivalent representation of the input-opper relationship as before. So what's interesting is that now if you look at this input-opper relationship, if you consider the case we have lots of antennas in the base station, then each antenna is the result of. Is the result of multiplying the signal sequence by this diagram sequence and by this one column of this channel ratio. So you can think of each one of these as one observation of these channel activities. Now you have many, many of these observations, and if let n go to infinity, you have lots of these n, if you've let number of antenna go to infinity, you have lots of these observations. Then this allows you to detect who are the active. Detect who are the active based on a maximum likelihood estimator. So, this is the idea of Alex Franger and his co-authors, and they formulated this maximum likelihood estimation for those user activities without having to estimate gamma. And the idea is the followers. If you write down the likelihood function of receive signal y condition on the activity, so this is gamma is activity, this is either zero or a large-scale fading. Then you can write this as a sum of these log of the likelihood functions across the antennas, because all the antennas are IIDs, IID across the antennas. And if you unpack this log-likelihood ratio, then you see that this likelihood function becomes log of sigma plus this trace of sigma inverse times this covariance term. This covariance term is the covariance of the z sigma. Sigma. Okay, so just remind yourself this sigma is the activity multiplied by the signal sequence plus the intrinsic noise. So this is what a sigma is. So what you're trying to do is to find the gamma, find the zeros or non-zero entries in the gamma, so that this sigma matches this received covariance matrix that you actually observe across the antennas. So this gives us a different formulation. So, this gives us a different formulation of trying to understand, try to detect the activities, in the sense that we no longer estimate the channel, but rather only take advantage of the fact that we know the statistics of the small scale fading of the channel. So we assume that the small scale fading is ID Gaussian. Okay, so this formulation allows us to write down the likelihood function in this form, where the sigma is s. where the sigma is s times gamma times s transpose plus sigma squared i, then you see that the optimal gamma is the actual activity factor along with the large scroll fading. And what we're trying to do essentially is to make this term the same as this imperfect observed covariance matrix. So this is the idea of Frankler and his co-authors. And his co-authors where they call this covariance method, because this is based on the covariance of the observed sequence. So, if you analyze this covariance method, you realize the following. So, before we have this activity detection, that's based on detecting not only the active measures who are active, but also their respective channels. So, we're trying to detect this matrix where which ones are non-zero, and if they're non-zero, what the channels are. And if they're non-zero, what the channels are. Because that's the original problem. But instead of looking at this problem, now we formulate this yy transpose. So we take y times this formation transpose. So then if you look at this matrix, this matrix becomes this times its own transpose. Then each row of this column, you have to multiply by each column up here. So in the middle matrix becomes a diagonal matrix, and if the number of antennas goes to infinity, then these will converge to the logic scale fading in the diagonals. Fading in the diagonals. So if they're not active, it's zero. If they are active, then you have this one row of channel times one column with its own channel. So when you multiply them together, you get a large scale of 80 of the genes. So now this problem becomes that of detecting which users are active and which users are non-active, where these active users have non-zero diagnotons, and these non-zero diagnotons are logical fittings. So we essentially estimate these logical fittings without having to estimate the scale. Without having to estimate the scale small space, the scale theta. So, this problem is a much easier problem than this problem because there are many fewer number of parameters to estimate. So, this change of perspective allows us to actually have a different phase transition between these two different problems. See, in this problem, what we're doing is we observe L times M when we're observations. This is the total number of observations we have. But how many parameters are we estimating? We're estimating M times K. We're estimating m times k. k is the number of active users. So each active user, we're estimating a vector of size m. So there are k times n parameters we need to estimate. And we have r times n number of observations. So it stands for a reason that the number of users that you can detect in this case is approximate order l. But if you look at this covariance method, the observation is y times y transpose. So the number of observations again is actually L squared. Observation again is actually L squared. So this is the number of observations. And the number of unknowns here is the number of large-scale fadings. So what you'd essentially be able to do is that you can detect as many of order L square of these active users. That's the key. Yeah. Just so I understand, that bottom equation, what's the thing on the left is observed, that L by L matrix, right? And then that matrix multiplying before and after the sparse one, that's unknown. One. That's unknown. Or this one. Yeah. This is a signage sequence, which is known. So that is known. Yeah. So everybody is assigned a situation sequence. You will transmit a situation sequence if they're active. I see. And then how big is the number, like, relatively, of the sparsity of that diagonal one compared to the size of that signature sequences one? So we're talking about, say, 10,000 users, n for 10,000. Say 1% of them were active. So this is a sparse activity detection, probably. So, this is a sparse activity detection. So, there's a sparsely here, and there's also a sparse E here. This is incomplete. Okay, so this covariance method allows us to detect essentially order of L squared over users. So, how do you do this in good practice? You can write down this likelihood function. And it turns out that you can do quiet descent on this. Each step on Coriant descent actually has an analybic solution. So, this is a fast algorithm that allows you to do quiet descent. To do coordinate descent, and that turned out to work very efficiently. So, here's a comparison of AMP versus covariance methods. So, both are active detection problems. One is trying to estimate a channel, and using AMP, you get a rough estimate of the instantaneous channel. But this channel estimate is not so good because the pilots are non-orthogonal. Everybody has its own random sequence as the pilots, so they're not perfectly orthogonal. On the other hand, if the No? On the other hand, if the covariance using the covariance methods, only the users' activities are needed, and about scale fading is being estimated, so we don't count scale scales, scale fading too much. And computationally, AMP is more efficient when k is less than L, and if N is small, if number of antennas of the base station is small, and the covariance method is more effective when the number of users, number of antennas is large at the base station because he can explore this. This H times H transpose. Okay, sorry, yeah. Sorry, in the previous slide, did you impose any sparsity in Galma when you're talking? Yeah, so users are sparse. So there are 10,000 users, 1% of them are active. So they're in currently... Oh, great. So yeah, it's interesting. No, we did not. But it turns out that you don't have to. Empirically, we saw that even if you impose a sparsity on here, it doesn't help much. It doesn't help much, which is interesting. It's non-negative, that's why you don't need to. Oh, it's that's right, that's right, that's right. Permatrix on the right-hand side, the noise. Oh, yeah, that's a noise. And the pattern is just the assumed ID calculus. N just ID custom? The matrix with n columns? That's oh yeah, it's a sequence we assume it's the ID custom. So so then this is a a linear combination we run all the matrix issues. Yeah, that's right, that's right. That's right. And left inside is a linear combination, sparse linear combination of rhyme. That's right. That's right. That's right. That's right. That's exactly the point. Which ones are known? Which ones are these linear constraints? Okay, so the fact that K is applied to order L squared is important. And in fact, you can observe this empirically. So here's kind of looking at how many. Looking at how many users can be detected versus the length of the sigure sequence. You see, at length of 30, you can already detect something like 400 users. So this order L squared is not interesting in theory, it's actually if you look at this empirical project. Now we need to understand why this L squared is true. And it turns out that you can understand this by looking at the line. The large likelihood estimate of this large-scale thing. So, the idea is the following: if you look at this large Maxwell likelihood estimator, you can use Fisher information to look at what is the regions that this Maxman likelihood estimator is expected to perform on. Now, if you analyze this algorithm under correct descent, this is hard. But you can, instead, we can analyze the true optimum of True optimum of this minimization problem, assuming that you get the maximum active solution. And we can do that by looking at the fissure information matrix and to see what we can say based on the form of this fishery information matrix. So here we assume that number of antenna go to infinity and just look at the isyentopic limit as number of antennas, case number of observations, lots of observations, and just see how many users we can content. So here's a program here. So here's a program here. So you look at the feature information matrix, which is expected value of the second derivative of log of the likelihood. And the intuition is that if this curvature is small, that means that there's lots of uncertainty in your estimator. So your estimation is not as good as if your curvature is large. So that's the idea of fission information matrix. And sufficient information matrix gives you a Kumar-Law lower bound, the lower bound on the expected mean square error. Expected mean square arrow of the gamma of the thing that you're trying to estimate is lower bounded by the inverse of coefficient coefficient. And the usual analysis of maximum electrical estimators says that under some regularity conditions, then you expect that the maximum acclaim estimator to converge to the true gamma. And also you expect that the arrow in the maximal accuracy estimator. The arrow in the maximum accurate estimator to converge to some Gaussian around the true around the true pair. So this is the, and the movers of the fission information matrix gives you the variance of this Gaussian, covariance of this Gaussian, Gaussian arrow, Gaussian arrow. Now, these type of consistency and asymptotic normality conditions are derived under the condition that the maximum accurate estimate. The maximum likelihood estimator should be consistent. And let me rephrase myself: the regulated condition for consistency and asymptote of normality of the maximum advocate depends on the fact that the true parameter is, one, identifiable, and two, that the true parameter should be in the interior of these region. The reason for that is that if The reason for that is that if gamma 0 is a true parameter, if there exists some other gamma prime for which the likelihood function is exactly the same, then clearly you cannot identify this gamma 0. So you require this type of the. And second, you require the true parameter to be the interior of the feasible region, because otherwise, the difference between the maximal estimator and the true true parameter cannot be Gaussian distributed because you're cut off at the tail. Cut off at the tail. So you need to be in the feasible region. Now, these conditions are usually pretty reasonable and mild, but for this user activity detection problem, it turns out that these conditions are not always satisfied. The reason is that if you have L squared number of observations, but the potential number of unknowns is much larger than L squared, then you could potentially have some other gamma prime, which has the same likelihood function, but it's different. Likelihood function, but it's different from gamma signal. Okay, so these conditions are not satisfied. And further, a lot of these true parameters in fact lie on the boundary of the feasible point. And this is because gamma 0, which is a sparse vector, has zero entries most of the time, so that the arrow cannot be Gaussian. So you have no boundary. So in terms of that, we can actually understand the phase transition of this maximum accurate estimator based on these regulatory conditions. These regulator conditions. So we can unpack these regular conditions to understand the phase registries. So mainly got as follows. It turns out that a necessary, sufficient condition for the consistency of the maximum accurate estimator, that is the estimate grows into the truth boundary as n goes to infinity, is the following, is if these two sets have the intersection, which is only a zero vector. So what are these two sets? The first set is n, which is basically no space of the fission information entries. Space of the fisherial information entries. And the other set, C, is a set of x's for which xi's are greater than 0 if i is in this index of i. What is this index i? This index i are the indices where the two parameter is 0. So outside of this i, x can be plus or negative, plus or minus, they're both fine. But if you're in this index i, then x i can only be positive. So the idea is that if the true parameter is 0, then If the true parameter is zero, then because you're estimating a positive value, so the arrow can only go one direction, it can only be positive. If your true parameter is a positive number, then arrow can be both positive and negative. So this c is a set of vectors where xi is positive. Okay, so this is c. And what's n? n is basically the nose space of a fission information matrix, which means that if you are going in a direction of this x, if you're going in a direction of x, which is in the nose space of the fission information matrix, All space of the efficient information matrix, then these x's are where you cannot identify the parameters. So that's why if this n and the c, if the intersection is a zero vector only, then this means that you can identify with the true parameter. Because if they're not in zero, if there's some other vector here in the intersection, then along those vectors, you're satisfying the fact that the parameters are always positive and the fishing information matrix actually is zeros. The fishing information gets actually zero, so that we can limit it by. Okay, so this is the condition that we derived. And this condition needs a numerical characterization of the phase transition of the covariance method. In other words, we can look at set of n, k, and l, outside of which the maximum-axiom estimator cannot approach the true value, even in the marginal n. Okay, so here's a plot of what's happening. A plot of what's happening. You see, here is an example where gamma 1 and gamma 2 can both can only be positive, but gamma 3 can be both positive and negative. Then if you look at this, this is the vector C, this is the space C. If you have a C like this, then N, this N space, can only be one dimensional. However, if you have a C which is for which gamma one, gamma two, gamma three are both positive, then you can have a two dimensional n. Then you can have a two-dimensional n. So n is the space with official information which is known. And this dimension of n is roughly n minus L squared, because you have L squared in balancations, you have n total number of parameters to estimate. So the dimension of no space is roughly n minus L squared. And dimension of i, the i where this non-zero is n minus k, because there are k active users, the rest of them are non-zero. The rest of them are non-zero. So, in order to have the condition that when intersect C is zero, this will give us that k has to be less than L squared. So, this is another way to understand why that you have displayed transition and when k is less than L squared. Okay? All right, so this brings us to the scaling law of the maximum magnet formulation. So, in traditional compressive sensing problems, you have this restricted isometry property. You see that number of oscillations needed. You see that number of oppositions needed is order roughly k times log of n over k. Now, if you have this type of capacitive sensing problem, where the covariance is s times the diagonal matrix times s, that s is a signature sequence, then you can unpack this in terms of control route product of these s matrices. And if they satisfy robust no space property, then this is the kind of spending that we're going to get. Spending dollars well squared is ordered k times mod squared. k times mod square of n over k. Now it turns out this condition n intersect c is zero. This is close related to the robust Mohr space property. You can show that these two things are related and this allows us to show that the consistency of the maximum accurate estimator has the same scaling law which is that rau square is equal to order k times log square of m. So this is what the scaling law of the maximum nature estimator is correct. Estimated is correct. So let me show you how numerically these works. This is the theoretical plot, basically numerical plot, and you see that they agree fairly well. And so this is the part of the talk that deals with maximum hydrogen SD. All right, so far we have looked at how to detect those active users, but that's only the first two phases of the Two phases of the random axis. In the final part of the talk, I'm going to look at the feedback for collision-free scheduling. That is, if you have already detected users, so we use, say, covariance method to detect the users, then the next stage is we need to have sent those command messages to these active users so that they'll be scheduled in the non-colliding slot. So, in the final part of the talk, I'm going to ask: what is the minimum amount of feedback needed to ensure collision fees that? Please get. So when you think about it, we have k active users, which need to be scheduled in those k slots. But the total number of users is large, there's n users in total. So if you use a naive strategy, what you need to do is the following. So if you schedule k out of n users, then you need to assign a unique index to each of these n users. Then the basic issue would detect the current k out of the users based on pilots. Then what they do is they can list those. Then what they do is they can list those k active users one at a time in the order in which they should transmit. Then each active user defines the index on the list, then wait its term for transmission. So this is the naive strategy. So if you look at this naive strategy, we require the total number of bits transmitted to the user so that it can be scheduled is k times log n. Because log n is the number of bits required to identify each user, the k of these users. You have to list them in the list. And if n is large, If n is large, for example, n is 10 to the 6, log n is about 20 bits. Now, if you look at these random access strategies where each user is transmitting, say, 50 or 100 bits, then 20 bits is significant over. In fact, you need to transmit k times 20 bits because you need to list all of these k users, and k times log n is actually significant. So, in the last part, I asked the question: can we do better than this? It turns out that. It turns out that you can do salading better for two reasons. One reason is there is flexibility in terms of the order in which the users are scheduled. Because I don't actually care how to schedule these users in any particular order. Among the k users, there are k factorial number of different orders. So any one-of-be-order would have been okay. I don't have to specify a particular order. I just have to specify one non-colliding order. Man colliding order. So you can remove this type of redundancy here, and that would already save you a little bit of bits. But it turns out if you do that, the total number of bits required to schedule them in an Kalangi slaughter still scales as order of log n. So it's hard to get rid of this order log n. But there's a second reason that this value strategy is not optimal. And that's the more important reason. And this more important reason is that each user only needs to know its own slot. Each user only needs to know its own slot. It does not need to know other users' slots. So, therefore, if you remove this extraneous information, then you can potentially do better. You don't have to tell every user how every other users are scheduled. You should have to tell each user how he himself should be scheduled. So, this can potentially do better. So, let me give you an example of why this can do better. So, suppose let's look at the case of K02. So, only two users who are active. Among, say, 10 million users. Are active among, say, 10 million users. Take two of them active, and what you want to do is to schedule them into two different slots. First user is first, second user is second. Who goes first, who goes second? And now your strategy is you have to take log n of indices, then times two. That's how many bits you needed. But actually, you can do much better. And the way to do much better is the following. You see, you can assign a different index to every user. And each user has a unique index, so they must differ in at least one bit. Being at least one bit. So then you just have to specify which bit they differ. And each one can just examine their own bit. If there's zero, they go to the first slot. If there's one, they go to the second slot. And that ensures that they're non-connected. But how many bits are needed to specify this? Log n is the size of the index. And the position of this bit is log and log n. So therefore, you can only require order log argument bits to make sure that they match locally. Alright, so you can do much, much better than log n. Log n is much, much better than log n. Now, it turns out that even log log n is not the best thing you can do. This is because you have a lot of freedom in terms of which index they differ. Chances are they're differing many, many different indexes. You can choose one arbitrarily. So you can choose the first index from which they differ. Now, the position of the first index in which these two indices differ is a geometric distribution. Is a geometric distribution. And if you work out the entropy of a geometric distribution, it's bounded by a constant, independent of n. So this means that the number of feedback is needed to specify who goes first and who goes second can be bounded by a constant independent of how large n is. n can be 10 billion or 100 billion or trillion, it doesn't matter. You can specify how many, you can make sure that the schedule is not alighting without having to without having to have a feedback bit that scales with any function of A. So it can be function, it can be a constant. So this is an example of k rotor 2. So it turns out that you can work out the general case of arbitrary k, arbitrary n. And we have the following theorem, that the minimum rate for a variable length, collision-free feedback code, it's variable length because you need to do entropy coding on the geometric distribution. Do entropy coding on the geometric distribution is upper diamond and north diamond centrally by k log e. So this means that the number of bits needed to ensure that the k users are scheduled in non-colliding slot is linear in k independent of n. Okay, so this is interesting insight. So how do we achieve that? So here's an example of how to do this. Suppose you have nine users here and three of them active and you want to schedule these three users into three To schedule these three users into three independent slots, three non-caligning slots, and how do you do this? So, the idea is to construct this random code. So, in each code, each code is a column. What you do is randomly putting one, two, or three into these positions. So, these are the codewords for each position. So, suppose that a particular set of users became active. So, here, users one, four, and seven became active. One, four, and seven became active. So you just look in one, four, and seven location whether there's a collision or not in the first cohort. If there's a collision, well, let's go look at the second coordinate and third cohort, fourth covert. So you just keep going like that. And look for the first cohort in which they are non-colliding. So one goes to three, four goes to one, and seven goes to two. So this is the first cohort for each non-colliding. And then you transmit the index of the cohort. Okay, so this is what you do. Now, what is the distribution index? Distribution index: the probability that each particular covert is non-colliding is k over k times k minus 1 over k times, etc. 2, 1 over k. Okay, so this is the probability that one particular covert happened to be non-colliding. And that's k factorial over k to the k, which is a very small number, as k is large. So the first index for which they're non-colliding is a geometric distribution whose parameter is this p. Parameter is this k vector over k. This k vector over k. And the entropy of that geometry distribution is essentially log of 1 over p. And that is log of k to the k over k vector, and that's approximate k1. So this is how, using variable net coding, you can schedule users into non-colliding slots in a way that's linear in k and independent. So this is how the coding strategy works. So the conclusion is that the cost of coordinating collision feescheduling can be Collision V scheduling can be as small as 1.44 paddles per user. And that's a main linear collision chip. All right, so let me show you some simulation results to see how do we compare uncoordinated random access with scheduled random access. So what we're going to do is we're going to look at a slotted slotted aloha, some variation of slot aloha. And we look at the slotted random axis where the basic issue has m and 10mm. Random access where the basic region has M antennas, K is the number of active users, M is the number of single antenna users, and the channels are ID. So let's just assume that's fast feeding, so each slot has a different channel. So we're going to compare something called coded pilot access. So what you do is each user, if you do uncoded random access, then each user will pick a pilot at random, then transmit those in those slots at randomly choosing, repeating their payloads. Repeating their pay-downs. So, collision happens if two users happen to transpick the same pilot. So, that's when collision happens. And if there's no collision, then the vision can perform channel estimation and data decoding for that user, then you do this CSS interference concentration. So, this is taken from this paper where they looked at coded pilot transmission. In contrast, we can do schedule random access. Schedule random access is law one. Everybody transmits a pilot so that those users can be identified. Those users can be identified, the item user can be identified. Then they can be scheduled. So, PO scheduling means to put them into a particular slot and to assign a signature sequence, assign the pilot sequence to them. So, you're actually scheduling not just the slots, but also the pilots. Then, because they're scheduled, then can be perfectly protected. Okay, so that's the schedule random access. So, here's a plot of the successful transmission per slot versus number of active users. Versus number of active users, where n is 10,000. We have 10,000 users, and a random number of users with 10 active. This blue curve is a schedule random, schedule of access. It behaves very well because as the number of users gets larger and larger, it's always going to schedule as many users as there are slots available. But if your number of active users is more than the slots available, then the performance will start to go down. So this is scheduled access. Scheduled axis, then if you look at code the pilot axis, its performance can track the scheduled random axis up to the point where you're at the point you're fully loaded. And if the number of users expect the number of users more than a number of slots, then the performance goes down drastically. Now, for those coded pilot access, there's an actual parameter that we have to retune, which is what is the number of slots that each user will transmit. Each user will transmit. If you transmit two slots, those three slots performance will be different. So, in order to track the scheduler user access, for this parameter setting, we find that each user needs to transmit three slots. Then you do successive interference cancellation to get the code. And if you do two slots, if you do beta equals two, transmit two slots, then there's just a mechanism performance cap between the two. Now, transmitting three slots, of course, requires. In three slots, of course, requires a lot more energy, so there's an energy process. So, in all cases, if you want to operate where you are fully loaded, in a fully loaded regime, then the schedule random access is much better than the uncoordinated random access. And this is all at a cost which is very small. If we we only need 1.44 bits per user, and here we have about nine hundred users, that's why it's about one point three kilobits. And this is And this is typically less than 1% of the typical payload. So, if you can afford this feedback bit, then the schedule random access is a valuable option. All right, so let me conclude this talk by saying that massive device activity, massive device connectivity is a key use case for future 5G or 6G research agenda. And the classical contention-based random access does not use utilized resources efficiently. Coding can. Efficiently. Coding can alleviate some of the difficulties, and we looked at theoretical analysis based on AMP and coherence method for detecting the users. And we looked at the feedback for vision-free scheduling and made the point that the amount of feedback needed to schedule these users can be very small. And we can have significant performance at a moderate feedback of 1.44 bits per user. And that's a small overhead. And the schedule analysis is naturally environmental. And the schedule random access is actually a viable option for massive random access. Great, thank you.