So what I'm planning to talk today is about some work that I've been doing trying to learn from pairwise preferences. A bit on background, so I guess a hypothesis that I've had for a while, maybe almost conviction at this point, is that a lot of what we want to do in building trustworthy AI systems depends quite strongly on being able to interact and learn from human feedback. And learn from human feedback are going to be able to go. And one of the bottlenecks in learning from human feedback is sort of how you ask people questions and get signals from them, which can be quite tricky in lots of settings. And generally, a generic hammer that people have found useful is to try to get signal from people by asking pairwise preferences as opposed to, say, asking for labels for examples or asking for some score for some. Some score for some. So, pair of preferences seem like a good hammer. I give you two examples or two signals of some kind. Tell me which one is more related to some outcome, or tell me which one you prefer, something like this. Seems a fairly low effort way to get signals from people. So, this is the general premise for some of what I'll talk about today. The work I'll talk about today is with some excellent students, Zach Stanford and Hansau, who was an undergrad when we did. And Hans Hau, who was an undergrad when we did some of this work, who we related to the first part of the talk. So it's a short talk. I decided not to do a ton of detail. I'm going to talk about two ideas that I hope are, I guess, if all works well, they're sufficiently intriguing that, you know, when I put the paper links at the end, you might take a look at the paper, look in more detail. But I hope to give enough sort of context to help understand. To help understand some of what the ideas are here. It's broadly related to the general theme of this whole workshop. Again, tied to thinking about human signals as part of the goal of learning from, sorry, for trustworthy learning, and specifically, I'll touch on robustness questions tied to learning from television. The first one will be trying to learn a decision model from A decision model from human preferences, tackling some issues and noise and uncertainty that might come up in the way, at least parts of how human biases might show up, or population biases might show up, human decision making. And then second, I'll pivot to digital auctions and the particular gap that's sort of not well handled by some of the existing tools for auctions, but tied to, again, trying to get preferences from here next point. Preferences. That's on human experts. So, the first one. So, the human decision, inverse decision theory is, I would say, reasonably popular, at least growing area of research, some within psychology, some within machine learning, statistics. But roughly, at least the psychological motivation is a model for how humans infer other people's preferences. Humans infer other people's preferences. So, the idea is that human interaction, so I'm talking to some other person, I build some model for what I think their preferences are, and I do this by sort of some signal that I get from the interaction process. Some of this has been distilled down to machine learning contexts in ways that are useful for lots of examples. And as a particular example, particular setting, a human machine. A human-machine interaction, where we have some model of the world. Here I'm specifying a predictive machine learning setting, or a predictive modeling setting. I'm trying to predict some label Y possibly. I know something about the distribution of Y given X. But depending on preferences, where I threshold the distribution to decide on what is considered a positive or non-positive label might change. Non-positive label might change. So there are many ways to look at this. One way is to think about this as maybe specific biases if you're in a cost-sensitive setting. So for instance, in medical decision making, there might be asymmetric costs for false, positive, and false negative errors. And so even though the sort of probability of the label given inputs, you know, you might have a model for, you may not want a threshold of 0.5, which is generic. At 0.5, which is generic Bayesian optimal threshold for accuracy. You might want to threshold somewhere else to capture this asymmetric cost for different kinds of errors that you can make. So, as a sort of way to motivate this kind of setting. So, to try to solve this problem, one of the ways that you can try to do this is, again, you have some model, p of y given x, you want to estimate the decision rule that takes in this model and Rule that takes in this model and interaction with humans to decide the mapping. And I'll argue in a little bit that, sort of, in general, the mapping is sort of well explained by some kind of thresholding. Also, to map this to some other lines of work that people may be familiar with. The idea here is quite close to what has become popular again in machine learning with the advent of generative models of sort of learning from human preferences. So, if you're familiar, for instance, with RLHF, For instance, with RLHF, there's a kind of paradigm there where there's a pre-training side where you first learn a model, tell you something about text distributions, and then you learn something about how humans like to interact with the model, with the sort of RLHF fine-tuning. It's a very similar paradigm, where here I learn a model of the world, so py, y, given x, and then I want to learn a decision function based on that model. So another connection when it connects to another point. Okay, so in conceptual form, this is often or one. This is often, or one of the ways that I think about this, is tied again to the framework in cooperative inverse decision theory. So, again, inverse decision theory is about trying to get signal about sort of decisions based on preference signals. In a cooperative sense, we're going to try to do this hopefully in a little bit more interactive way. And I think the key part of this will be be Of this will be being careful about what specific pairwise preferences we ask of the humans. That's actually the key sort of difference from some of the existing work in this literature. In the basic classification setting, a cartoon to sort of set up the problem. So I know something about the overall distribution. I would ask the machine would ask, again, a human expert. So if you like my. Again, a human expert, so if you like my sort of medical decision-making setting, you can again imagine a diagnosis decision-making engine, and you're trying to pick where the threshold should be for making some diagnosis decision for a particular patient given the input signal. So underneath, I have some model that hopefully gets me a reasonably calibrated distribution of the label P of Y given X. But again, I need to decide on a particular decision. So there's not enough distribution I need to decide on. So, there's not enough node distribution to decide on sort of how I'm going to actually act for particular samples. I mean, do we just take point? Is the setup clear? H decidable depending on py given that one-dimensional thing, or is a multi-dimensional thing? Yeah, so in general, for binary settings, it's just maybe one-dimensional thing, because it's sufficient to sort of fully characterize a problem. And more complicated, everything I'll talk about today, I think, will be binary. Everything I'll talk about today, I think it will be binary, but in say multi-class or other settings, it might be a more complicated object. But here it's just a one-dimensional thing. And an argument that I have not made all that well is that in many settings, you can show that thresholding functions are a good sort of decision-making outcome. Let me formalize this slightly more. So, if I convert this to a problem of sort of Bayes off-model decision-making, where Bayes optimal decision making, where I have the population distribution. I have some metric. So, you know, usually we optimize accuracy, but you can imagine optimizing F measure, you can imagine optimizing weighted accuracy, Jacard, whatever. There's a nice literature in thinking about what Bayes optimal decision rules are that shows that for almost every metric, the Bayes optimal decision rule, sorry, there exists a thresholding rule that's a Bayes optimal decision rule. So, in fact, you have to work quite hard to come up with. In fact, you have to work quite hard to come up with a classification, like a measure of performance where the optimal thing to do, sorry, where thresholding is not an optimal thing to do. It's a bit trying to make sure. So there are other potential final functions that may be good decision makers, but for almost every utility function, thresholding is sort of provably a good decision making. So optimizes some metric. Meaning you can't do better by doing some more complicated decision making role. Dealing some more complicated decision-making role, knowing the distribution. Does that make sense? Okay. Okay, so that's the setup. A few pictures. So like I said, there's a bit of work on this. There's work in particular on inverse decision theory. Some of the gap that we're trying to close with this setting is inverse decision theory is passive, so it just uses, it assumes that you sort of, I guess, ID, give human. Give human pairwise preferences and get signals from them. What we show is that being active, so picking which pairs to compare, allows you to do this quite a bit, to solve this problem quite a bit faster. So I can pick a good decision-making rule much faster than passive observation. So if you like, it's a kind of active learning comment. So maybe not surprising. Maybe the more interesting bit is that we generalize things a bit to handle various kinds of noise. So allow, for instance, So, allow, for instance, for noisy decision-making from the expert. So, they're sort of inconsistent in how they give signals about what the sort of right decision for a particular input is because they're uncertain maybe or just in a particular decision. It handles some versions of population, so multiple decision makers. You want to do aggregation over not a single decision maker, but I want everyone in this room to decide. Everyone in this room to decide what time we break for lunch. I guess too late now. Break for dinner. And so that leads to a kind of distribution, depending on how you characterize the population, over potential decision-making rules. So it changes the framing of the problem. I'll discuss that a little bit. And then in some settings, you can extend this to when you're missing context at various times. I find that the most interesting, so I'll talk about that a little bit. So, I'll talk about that a little bit. There's a particular version of this, which shows up in lots of human interaction settings, where one of the ways to say it, if you're familiar with some of these, particularly HCI kinds of work, is maybe a framing effect. But the way people tell you their preferences changes depending on exactly how you show them or ask them the question. One of the ways that this is sometimes described is known as a description versus exclusion. Described as known as a description versus experience gap. One example that I like, again, from both from medical decision-making setting, is that sometimes the answers you get about, say, clinical decisions from, say, a clinician or a doctor differs when you give them, say, a table of, I describe a patient via maybe a medical record. So I showed a medical record and asked them to tell me something about this patient's medical record versus the decisions they might make. Versus the decisions they might make if I actually have the patient in front of them and they're interacting with the patient and using that to get the signal. And this could be different even if the actual quantitative data doesn't differ in any sort of meaningful way. So all the signal might be the same, but something about the interaction mechanism sometimes changes how humans express their preferences. So we handle some, the extensions allow us to handle some versions of this. Versions of this. One way to sort of see this, one place where this ends up showing up, one way to characterize this is in this human assistance game, is you can think about this as humans behaving differently when they know that the robot is observing. And this ends up, again, leading to some difficulty in getting good preferences. Sometimes, and the setting that we can handle, you can think about this. Setting that we can handle, you can think about this as coming from non-realizability because the robot is only getting to pick one decision rule, but the human might have sort of a more complex, somewhat more complex decision rule. So one way to think about this, I guess, is perhaps their compounds that are not observed that end up making decision rules somewhat more complicated than what might show up purely in a tabular or purely descriptive setting. Okay, so in terms of how this actually maybe In terms of how this actually maybe works in a bit more detail, so what we show is that we have a couple of claims. One is more an assumption, but we show some examples where this works. We show that for many of the settings that I mentioned, of the noisy settings, so noisy settings where I have a single human with a sort of noisy decision-making role, I have multiple humans or multiple decision-makers that want to aggregate their. Decision makers that want to aggregate their decisions together, that many of these settings are characterized by a noisy decision rule. So, sort of, you can think about this as a family of decision rules where instead of the threshold being some point where I pick, say, 0.5, where I pick above 0.5, I have positive decisions, and below 0.5, I have negative decisions. Instead, we have a decision rule that's a distribution. So, on the right, what you should see. On the right, what you should see is, I guess, instead of having a single decision rule, I have a distribution over decision rules. I want to be able to capture the case where we have distributions of decision rules. What this ends up meaning for the decision-making problem and have a distribution over potential thresholds is that instead of having a single step function as the mapping from probability to decision, I now have a setting where the A setting where the decision rule is randomized, but as long as the expectation is monotonic for respect to thresholds, we can handle that reasonably well. So I said that value, but let me sort of restate that a little bit. So the specific claim made here is that as long as the expected value of the decision rule has this monotonic form, then the procedure I'm going to talk about in a minute allows us to handle decision-making settings that are noisy in this way. Making settings that are noisy in this way. And then an additional statement is that many interesting settings can be characterized as a noisy decision-making setting, as described here. Hopefully that's slightly clearer than the demo from a second ago. I'm happy to maybe answer questions offline. Okay, so I'm interested in going from probability models to decision-making rules. I've extended from the setting where decision-making rules are a sort of deterministic threshold function to cases where. Threshold function to cases where I have a stochastic decision-making rule, but it's still within a nice family. So it's a family where, again, its expectation has a nice monotonic behavior. How might I sort of figure out the good threshold in an active way? So it turns out that in the purely deterministic setting where the decision-making rule, so going from probability to outcome, is simple thresholding, that binary search is an optimal procedure. Is an optimal procedure. So I can, essentially, I can convert the problem of which example should I show people such that I can figure out the right threshold as a binary search problem. I sort of look at all the whole my set of examples. I map it to probability space. I binary search in probability space. Binary search here, I split into a half. I compare. Do I prefer decisions above or below a certain threshold? A certain threshold, whatever direction people pick tells me a signal of where their likely threshold actually is. I sort of refine this procedure in the standard way. So, familiar binary search, it's just standard binary search, but modify it a little bit to be able to get pairwise preferences. Binary search breaks when the threshold is not a single point, but now it's a distribution. So we have to modify the procedure. It turns out there's a simple modification called the probabilistic bisection algorithm that works quite well. It's a fairly simple idea. I start with a I start with a, it's sort of a pseudo-Bayesian kind of algorithm. I start with a distribution over where the possible thresholds are. If I have some prior, I can pick that prior. If not, I can start with a uniform distribution. I ask for pairwise preferences. So I ask for a signal of, you know, what I really want to know is: is the threshold above or below something? I convert this to examples and I ask, do you prefer this example to this other example? This other example. When I do this, I get some signal about which direction is more likely. I convert this direction to an update on my sort of distribution over possible thresholds. So very similar to a binary circuit now with the updates are the standard sort of Bayes type updates, given a signal. It turns out this you can show as optimal, even with noise. And what you end up at the end is a distribution over where possible thresholds are. For where possible thresholds are. And as I'll show in a little bit, you can expand this to use voting, for instance, to handle, instead of having a single decision maker, having the whole remote decision maker. So again, I was going to use this to decide something about our dinner time. Then the procedure would be in different rounds, I would ask everyone to vote about some preference before or after 6 p.m. I would I would sub-sample people and have votes, get the majority votes, use the majority votes to get me a signal about whether people want to go earlier or later. And I do this over procedures and can show that that approach will allow you to figure out the decision-making rule from the L1 distribution. Okay, so the paper has details on the analysis. The high-level is you can use sort of stopping time type analysis to Type analysis to figure out to show that this approach will work. So, roughly, you can convert it into taking an epsilon ball around the optimal and then figuring out the first time that the measure of that epsilon ball exceeds a half, which is enough to tell you that you've sort of figured out the optimal threshold. So, in the paper, we have details on the analysis for this procedure. Some examples. So, first, I'm going to First, I'm comparing this to the IDT case. So, if you remember, the proposal here is to do active sampling. So, we're asking, looking for where to ask people for payrollized preferences, as opposed to passive sampling, where you assume just random observations of parallelized samples. And what I'm showing here is, you know, I think published papers, so you have a good result. But active sampling is much, much faster at getting the right threshold. Faster at getting the right threshold compared to past observations. So you need many fewer interaction steps in order to pick up the right threshold. We also show that you can use this to explicitly, so I mentioned in the second setting where instead of a decision rule with a specific threshold, say I have, and I have a decision rule that's itself stochastic. That's itself stochastic. And in that case, what you want to learn is maybe the distribution of potential decision rules. And so, this is a setting where you can do this. The way that we do this is essentially we just modify the estimation algorithm to estimate a bunch of different quantiles of the distribution over decision rules. So, this allows us to essentially just map out the stochastic decision rule here. We apply this to a setting where we have this sort of missing information. We have this sort of missing information that characterizes this description experience gap. Show that indeed for some simulated settings, you end up with a decision rule that's stochastic in a particular way, and then we can recover this using the noisy estimation procedure. Okay, so that's done on the first one. I'm going to switch gears to a second idea, roughly related, again, where we want to learn from pairwise human preferences. So I'm interested now in. So, I'm interested now in digital good options. So, the motivation is, I guess, with the advent of lots of, say, generative models or other models that can create digital goods, there's a pricing problem. So, there's some difficulty in figuring out what's the right price for, say, a generative image from Delhi or GPT or something like this. Or GPT or something like this. So the current solution is: you know, there's a subscription model. OpenAI or Google charges $20 a month or something, and you just pay that amount, and that's sort of how things are set. There's a somewhat different approach used in, say, web advertising or other kinds of application settings, where people use auctions to try to get signal on what the optimal price should be for a particular item. Be important to go item. So, if you might be familiar, for instance, and one of the largest applications of this is in ads. So, you know, all the ads we get online. Every time you search something, there's a quick auction run to figure out what a bidder might pay for your impression, for being able to show your ad, their ad, on your search result. And the argument here is, and I'll show two sides of this, but the argument here is perhaps there. But the argument here is perhaps there is the opportunity to do this kind of adaptive pricing for digital goods. And perhaps there are some benefits that we can get from being able to do this. So that's the goal. I want to be able to price some digital good. And this is a challenge. So ideally, and the way that things work in, for instance, advertising settings, is that you have a market. So you have a bunch of people getting to show a particular result. To show a particular result, users all submit bids. And then, based on the bids everyone gives, you have some mechanisms for picking which bid you're going to select as the winner of that bid. If any of you know sort of a little bit of a mechanism design, you might know that variations, in the simplest setting, the optimal thing to do is a second price option. So you don't pick the bidder with the highest bid, you pick the bidder with the second highest bid. And you pick this because there's a sort of theory and practice. Because there's sort of theory and practice showing that this is incentive compatible, meaning there is no utility to the bidder to lie about their bid if you pick the second price. So the optimal bidding procedure for any bidder is to bid their true value for an ad, as opposed to picking, you know, trying to game the system in some way. So they can't game the system. This is a sort of informal, but as you might know, informal statement of a very precise result. Precise result that second price options, auctions are optimal for certain versions of digital ad auctions. Now, this doesn't break, sorry, this doesn't work, double negative there, but this doesn't work for digital goods because unlike a standard auction where, again, I have a single good, in this case, for better or worse, you're the good. Your search is the good, and then the bidders are the population. Good, and then the bidders are the population of people trying to sell you things. And the digital good auction, say, for DALI image, there's only sort of one person. So there's only one bidder. And so running a full auction doesn't actually work because there is no population of people to do the bid. And so the argument we'll make in a second is that learning approach might actually be helpful here instead of sort of relying on a standard mechanism. So, we want to try to set fair prices in a digital good option. One idea, maybe the naive thing, is have people just sort of say what they want to pay for a DALI response. You might imagine what goes wrong here. You're unlikely to state your actual value. You'd want to just pay minimal amount for. For an interaction. So, the gap, the issue that shows up here is that you don't get incentive compatibility. So, people will not tell you their true value for an item in this setting. So, formally, we're trying to come up with some mechanism where an agent has some utility that you don't know. You get to see reports of their utility in some way, and you want to find a good option that only uses the upside. That only uses the observations of which good was requested and the value that people stated for that good in order to figure out sort of a fair pricing. And so, what we show in the paper is that you can come up with a couple of different feedback procedures, some of them using the pairwise thing. So, the pairwise thing is, you know, fix a price. Would you prefer some option or another option for that price? Or maybe a more direct option. Here's a possibility. Direct option. Here's a possible generation. Here's the price for it. Thumbs up, thumbs down. And we show that there's a way to design an interaction process with this such that you can get much more truthful user signals about their value for a particular digital gift. So under the hood, we use the second price again as sort of standardly used within the settings. The main difference is that instead of using standard auction, Instead of using standard auction settings, we need to do some learning under the hood in order to figure out user valuations. And then, once we can learn user valuation for items, then we can use a second price on top of the learn model. And we can show that this ends up capturing many of the good properties of second price options, in particular incentive compatibility. So users do not do any better by lying about their value and they're incentivized to actually state their true value for their clients. One application for this that we found quite interesting is to go from Quite interesting is to go from pricing to actually costs. So, the cost version of this is that, as you might know, data is maybe the most important part of building many machine learning systems. And in particular, human data that comes from signals from human preferences that are often done, I guess, particularly in the news, as you might have seen. There's a bunch of news about some of this work being done in places where labor. In places where labor might be cheaper. But what is most, one of the things that is concerning about this is that often users are asked to label potentially output that might be psychologically damaging to them in various ways. And so there's been suggestions for trying to capture fair pay for red teaming other kinds of interactive processes with generative AI models. The argument I'm going to make here is that I don't have this, what we're Cares that I don't have this. What we're going to suggest is not necessarily the perfect solution for this problem, but we think it's a meaningful way to try to engage by thinking about sort of adaptive pricing tied to, if you like, pay, well, I want to be careful here, but want to add a tool to the arsenal that captures differential individual costs for labeling a particular image from, or labeling some signal from a generative model. Signal from a generative model. And so that's what you can convert this tool to use. So you can turn this into a reverse option. So you have this as sort of a way to label hazardous pay or differential pay for how potentially adversarial a particular signal is the human is labeling. And if you apply this, so we have some results in a paper where you can show that if you do this, you can again query. You can again query one, you need fewer queries. Two, we can show that regrets, or sorry, in this case, welfare, so how close people's utility or preferences are to sort of uniform over individuals behaves much better using this proposal seeker compared to doing something a bit more naive. It's a new space that they're not good baselines for this problem. Okay, so here, empirically. Okay, so here empirically, same results. We show that the sort of pairwise feedback mechanism has much better welfare properties than, say, either uniform allocation so everyone gets the same set of items, or doing some simpler sort of naive regression strategy, though it's somewhat worse than if you could figure out the optimal allocation of people's items, but much better than other approaches that approach it as well. Okay, so I've spent some time talking about a couple of settings where the key problem is. Where the key problem is to learn from human preferences. And the first one for trying to learn decision functions, and the second one, trying to price digital goods. And I'll point you to papers. If I did my job, which I may not have, you may want to read some of the papers. First one was AI stats last year. Second one is on archive right now. A little more detail. We are out. I think so. Does anyone have any questions? Can you say the math needs a new proposal with more details? I didn't fully catch it. So suppose a new good comes in and each piece will submit a B, right? So how do you do that? Yeah, so for each good, we learn a regression. So we use the pairwise premises to learn a regression of. Firms to learn a regression of their value for that good, and then we run second-price auction against a request sort of utility for the good. So you feed learning first to feed the learning? We can, I guess it's so it's not fully supervised in that you don't get a value number, but you get a pairwise preference signal at a price. So you get something, some signal about About whether or not their value is greater or less than a certain number for a particular grid. You can use that to solve a kind of, I guess, almost ordinal kind of regression problem, because you have this, again, signal about value greater or less. You can fit a function underneath this that says, you know, what is a prediction for value such that it set it follows the signal that you observe. Folllows the signal that you observe, this regressed value being greater or less than. So, did they accept or reject, for instance, a bit at a certain price? So you can't re uh apply the second price auction directly because you only get feedback on paris comparisons, you never get actual price auction. Pairwise comparisons, you never get actual collection. So you need some way to figure out pricing from the pairwise feedback.