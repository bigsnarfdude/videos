Below in France we have an empirical crown himself. I think the vice-director of this institute doesn't need any introduction, but anyway, so it's going to be a pleasure to listen to Jose, who's going to speak in stability and instability in the non-linear integrate and try another model. You have your action. Okay, so I'm going to speak about something we have done in collaboration with Marie Jeffrey, who is around here, and Alejandro Ramas, both at the university. So Alejandro is a student and he's currently looking for a postdoc. So if somebody is interested, he's also informed. And so it's about a model in neurodynamics, which is some kind of mean-to-fin model, a population model for the behavior. A population model for the behavior of a large group of neurons. So, if we have a population of neurons that we consider is uniform in some sense, so they are connected in some uniform way that we will explain later, then a model which was sometimes old for this population is this one. So, it's a model based on a stochastic behavior of the microscopic system. And this PDM presenting here is a mean field PE. Mean field PD. So let me explain I read the PDE and we mention how the microscoping policy is working in a few minutes. So this PDE contains a diffusion part, it contains a drift part, and then we have here the non-linearity, which is an interaction, and this represents the fibromyalgia neurons. So the unknown here. The unknown here is a key. It depends on time and it depends on V. V is the voltage of the neurons. So what's happening is that the neurons are at a certain voltage and then getting charged because the other neurons are firing. So in a natural way, they have some natural drift towards some given potential. So here the natural drift is towards zero, right? So we have made it uniform. So they are also moving randomly because of this diffusion part. Of this diffusion part, so we are moving via Brownian motion, and then the firing of the other neurons influences them also. So, it's a non-linear behavior. If you have a V here, so the coefficient B here, if it's positive, then this is an excitatory interaction. So, it means that if the neurons fire, then it's more probable that the other neurons will fire too. And if the B is negative here, it's the other way. So, if the neurons are... Way, so if the neurons are fighting, then they always are inhibited. So it's harder than what they will fight. So the only non-linearity in this model is in this red term, in this NP. And so this term here just represents that once the neurons, they reach this VF, which is this firing potential, they will come back and re-enter the system at VR. Okay, so this is the re-entry potential. And the reset potential. So, this is a basic model. Let me write also the model with delay. It makes sense that the interaction between the neurons is not immediate, so when they fire, the influence they have on the other neurons is a bit later. If you want to take into account this delay, one of the simplest ways to do it is to take into account the number of neurons that were firing, but these seconds ago. You could make this more complicated if you want to. more complicated if you want to if you want to maybe be closer to the modeling so you could take you could take into account the firing in a whole range of times but this is a nice nice model for what's happening so this model comes from a microscopic point of view so this this microscope these microscopic systems are groups of a lot of neurons which are doing this they are moving by some By some motion here, okay, and they have the potential is also evolving in time by using some drift. And here we have the interaction of the other neurons. So this n that we have to define here is the total time rate. So if you want to define microscope existence, there are several ways that you can do this. Some of the ways that you can do it, it's by It's by considering the firing in the previous segments. So if the neurons fire, then a little bit afterwards they are interacting with the other neurons. Okay, so here are some papers of people that worked on this. So we have a well-known paper by Brunel, which explains this kind of model. Then we have Have these people here, the Ladou English Reverend Tanté from 2015. Then we have papers by Chevalier, Cassius, Dennon Boulevard, and Chevalier, which were studying the model which is not structured by the potential, but is structured by the time elapsed since the last discharge. So there's a family of models who are structured by this kind of variables. They are slightly different, but Variables, they are slightly different, but the limits between this kind of stochastic systems and the mean 5 DB that we are considering is done in these papers. So, the most close to a rigorous limit that we can find is this paper here from 2015, for the current model that we're considering. So, let me explain a bit the main feature in this kind of model. So these are the very basic properties that it has. So, first, this model will preserve positivity. So, it's a physical model. You can understand this P as a probability distribution. It gives you the probability distribution that each neuron is at a given potential. So, if you have an initial condition which is non-airline, it will be non-nairy forever. Then it also preserves the mass here. So, the mission rate is preserves the mass is that we have, let me go back to the Have, let me go back to the equation. So, the boundary condition that we have here, that I didn't mention before, is that the boundary condition, the Pd, the density, is always zero at this potential Vf, which is what we would call it on the right-hand side of it. So the coefficient of this term here, this NP, is minus the derivative of the border. Minus the derivative at the border. So it's the flux. So what's happening is that all the flux of the neurons going out of this region of the constituent is coming back at a reset potential DR. So this delta represents that. So we're getting all of the neurons which are exiting this region and they're coming back at the potential DR. So if you formally just integrate this and calculate the 10 data value integral, you see that the You can see that the total mass is total. So, very good. We have this kind of mass conservation. And then, this jump we will see later in some simulations. So, since in the equation itself we have a delta function, which represents all the neurons which are exiting this domain and coming back, you have some kind of discontinuity there on the derivative. So, the solution is continuous, this is known, we will mention later, that the derivative of the That the degree of your solution has a discontinuity there. Okay? So, this discontinuity has to be exactly equal to this parameter. Okay, so here's some things which are known about this equation, and then I can comment a bit on what the difficulties are. So, first, this equation is interesting because of several things. One of them is it's a model which people in neuroscience are actually using. So, this is a simplification model. So this is a simplification of the model. The real model in your science, for example, has a V-dependent coefficient on the A, and even the A is a coefficient for the diffusion. So more realistic models have a non-linear coefficient for the diffusion. So these are some kind of simplification. And on the other hand, this model is a kind of a simple phonetic PD, but it has a non-linearity which is Linearity, which is one-dimensional. So it's a non-linearity only on one quantity. It doesn't depend on the whole function, it depends through only one parameter. So it's interesting because it's a model for other cases what this happens. So I'm thinking, for example, in some population-type systems, like a Becked-Dorian equation, okay? Or maybe these VGK models, where the non-reality is much more complicated, but actually depends only on some uh set number of parameters, like this. Set number of parameters, like the velocity, temperature. So it's this kind of nonlinearity. So, what can you say about this equation? The nonlinearity makes it very hard to find complete conditions under which you have global existence. So, it is known that in some cases you have existence of solutions always. So, if the interaction between the neurons is inhibitory, this is fine. The reason is that the Is fine. The reason is that the firing of the neurons is only helping actually for the neurons not to blah, blah, blah. But so let me see. So let me mention also this thing first. So as soon as you have B positive, there are initial conditions that will blow up. But if you have some kind of excitatory interaction, then for some initial conditions you have blow up. This is something that was proved in this. That was proved in this figure here. Then there's a condition, there's a test for blow-up. You blow up if and only if the time rate goes to infinity. So it's a non-linearity, so it's very natural that the only way to blow up is for the time rate to go to infinity. Then if you have delay, then the assistant is blowable. So the equation with delay, you can actually make it a linear equation on a sequence of time intervals. Of time intervals. So if you solve the equation in the initial time interval, then you keep going for the next time intervals, and this gives you global existence at least, even if possibly you may have divergence of some quantities for infinite time. But for every finite time, the solution exists. Then what about the stability and instability of equilibrium, which is the one which I'm most concerned today? In general, for the linear equation, In general, for the linear equation, you can study the stability. So, this was done also in this paper in 2011 by José and Bertán. And the difficulty is when it's strongly non-linear. So if the B is far away from zeros, then it's not easy to show whether the solution is stable or unstable. So this is what we want to discuss a bit today. What do you see numerically? So, what do you see numerically? I will mention in a minute. Then, another issue which is quite interesting is to see whether you have periodic solutions. So, periodic solutions, periodic solutions are interesting from the point of view of the behavioral neurons, because the people of neuroscience, they are looking for this kind of solutions. It's a common behavior that you see some kind of periodic trial of a group of neurons. So, it's interesting to find solutions which do it. Which leads. Okay, so these are recent work where they are found by Sam. So evidence is given that you may be able to find some theory solutions by comparing heuristically to a simplified model. So I want to comment on this also. Let me show you some simulations. Do you have an idea of how it's working? First thing I want to show you? You mean the video? It's possible. Okay. It's okay. No, it's okay. So let me repeat this. Okay, so this is an example in which the system is easy. In which the system is easily going to equilibrium. So here, so this maybe the scale is not very good, but if you can see there's a kind of kink here. So the solution has a little discontinuity in the derivative. Here, the firing potential is two, and the reset potential is one. So the solutions are going from two to one. They're jumping there. So this is one. This is one case I wanted to show you. It's a case in which you go to equilibrium here, B is equal to 0.5. So the firing rates, this is a typical plot of the firing rate. Okay, let me see if I can make this bigger. Where it's somewhere and then very fast it goes to the equilibrium value. So this one, it's an example of a blow-up solution. Of a blow-up solution, so it spends a long time in some kind of transient state, okay? And then at some point, it will blow up by making this lobe really large. So, for blow-up, this has to be always going to infinity. So, it takes some time. Yeah, they will do this. It's already getting getting steeper. And that's this kind of a very fast blow-up. So I'm going to show you. I mean, rate it goes to infinity. I mean, rate it goes to infinity like this, okay? And so, this kind of behavior, it's hard to find conditions which tell apart the two kinds of behavior. So, given an initial condition, it's hard to tell which one is going to happen. So, another case I would like to show you, so these are simulations for the stochastic system, okay, with many neurons. And the yellow one. The yellow one, the last yellow one that maybe you can see here, which is a bit spiky, okay? That's a simulation of the system. And this here are this pseudo-equilibria, which you will get for the linear system when you fix a given firing rate. We will mention this later. So this approximate quite well the behavior of the system, and this is the behavior, this is the expected behavior of a system with B large and with delay. And with delay, okay, so for infinite time, this is what you expect the system to do. So it's somehow it's blowing up in infinite time. Okay, so the slope here is going to increase on the right-hand side. Okay, so that's it. I would like to go back to this. Then this one is an example where you can see the appearance of periodic solutions. So, periodic solutions Should only exist, so this is not proved as far as I know, but should only exist when you have a positive delay. So, what you need for regular solutions to exist is that you need a positive delay and you need a beat quite negative. So, we found out that this is what people in the literature of the data equation call delay negative feedback. What's happening essentially is that the neurons are firing a lot. So then they get very inhibited because... They get very inhibited because the system is very inhibitory, and then it takes some time for this effect to be felt. So, by the time this is felt, the neurons are very inhibited and they concentrate in very low potentials and the firing rate is very low. Then, since the firing rate is very low, the inhibition this this inhibition is no longer there. So, after some time, they forget that there was a inhibition and they go back to the usual state. Inhibition and they go back to the usual state. And then again, this happens when this inhibition due to the final rate gets kept again. So what you see here is the behavior of delays. And for a small delay, you see that the spanning rate still goes to some kind of equilibrium. But for a large delay, you see periodic solutions of different periods. Of different periods. So, if you take the longer delay, of course, the period of these period solutions should be longer. So, one open problem actually is to show that these period solutions actually exist for this reception. So, this is an intuition that we used some time ago, which is this. I will remark on this later. Later. So there you go. So, for all the results that we use, we actually happy with this existence theory by these people here, which we have a couple in the room. And we try to do estimates, but always using solutions which are known to be already regular. So, let's say I will give. I will give you some details of what one can do for different cases. So, the simplest case is the linear case, but we will discuss this. Then, I want to discuss a bit how to pass from linear to non-linear. And then we talk a bit about linearized case. So, this is a linear version of the previous equation. The linear version is just mixing this firing rate to some background rate. So, you can imagine this as following one neuron. Following one neuron in a background of neurons which are firing at a fixed rate end. So you get an additional either push or pull towards this fighting potential, depending on whether B is excitatory or inhibitory. And it's a simple equation because, as you see, it's a linear equation, it preserves mass and positivity, so it's a Markov process. And the variables we're using are the same as before. The only change is that now this N is a fixed background fire break. So, what can you say about this? This is from this paper in 2011, essentially, that you can find the equilibrium for this equation almost explicitly. The only part where it's not completely explicit is that I have to get this this coefficient here. This coefficient here. Okay, so this A infinity, I have to choose it so that I get mass one because I'm looking for probability equilibrium. So this is a very similar calculation as what you would do for the Frank-Planck equation. You write it as a divergence form equation, you write it as a drift or something, and then the equilibria are given when the drift is equal to zero. You can solve this almost explicitly. You can solve this almost explicitly, okay? And you get this kind of uh this kind of uh problem. So this thing here, this quantity here, we call it I of n, and this is the one you need to normalize. Okay, so I save this later for the nonlinear case because it would be interesting. Okay, so the equilibrium is almost explicit except for a constant. Then it's a Markov process, so it always has a family of entropy functions. We can find a lot of The functionals. We can find a lot of Dyapunov functionals for this equation. For each convex function phi, we get an associated Dyapunov functional. This is the kind of entropy that we get associated to that. And here, you can make more or less long calculation to obtain the dissipation, but this is well known for all these familiar models. So, this, for example, is a particular case of Particular case of calculations done in papers by Michel Michelin-Pertin, where there's a general family of models, positivity-based models in biology, where you can find this kind of entropy. So this is not a new result, was not really a new result then, but it's sometimes tricky to find out. So it helps to write it in this way, which will remind you maybe of the way that you do for the Of the way that you do for the Fokker-Planck equation, if you want to make an easy calculation for the entropy, you do it for this term, and for this term, you can do something which is not too far. So you find out this. What do you get? You get the usual Fischer information from here, and then you get another term, which has to do with the fact that neurons are jumping from this fine potential to the reset potential. So, whenever you have a Markov process and you do this kind of entropy, what you Do this kind of entropy, what you find on the right-hand side is an average of all the possible jumps of the system. So, this takes care of the infinitesimal jumps due to the Fokker-Planck term, and this one takes care of the size one or size F minus VR jumps from the fine potential to the reset potential. So, both are good terms because both are negative, so both are helping. But actually, if you want to prove conversations with. If you want to prove convergence to equilibrium, you can forget about the second one. To prove convergence to equilibrium, you can find a bunch of inequality with the weight P infinity to show that you go exponential to equilibrium. So this is quite standard and I think a lot of people here are very familiar with this. So once you have this kind of inequality, it's actually a kind of entropy, entropy production inequality, and you get this kind of behavior. And you get this kind of behavior. So the relative entropy between your probability distribution P and the equilibrium probability distribution will go to zero exponentially fast. And this lambda, you can get some kind of constructive estimate of this lambda. So for the linear equation, it's reasonably okay. So what about well-posedness? So the strategy I'm going to explain of going from the linear program. Of going from a linear problem to a non-linear problem, it's a little inherited from some ideas in kinetics here. So the first thing we will do is to try to get good estimates on a useful Banach space for this linear equation. So why do I do this? You will see later that in order to go from an understanding of the linear equation to the non-linear equation, I have to see what happens to the remainder. Happens to the remaining terms. And the remaining terms are related to this firing rate. And the firing rate is the derivative of P at a certain point. If I want to estimate this properly, I need a good estimate on the n-infinity norm of the derivative of P. So if I just have a control of the entropy, I don't have a good control on this kind of filing very n. So I need to do something about that. Okay, so the existence theory that we use is, as I mentioned, The existence theory that we use is, as I mentioned, the one from this paper. And when I say well posness, I mean uniform estimates for these linear equations. I want to know that if I start with a finite norm in this space or in another space, then at time t, I still have a finite norm in the same space. This is the kind of a period estimate that I want to prove. So I'm going to choose this one, this norm. I'm going to choose this one, this norm for the rest of the talk. So this norm contains the first part is the entropy norm. It's an actual norm where you get the k of the entropy. So if you look at the entropy, that's the weight that you get. Then we need for sure some kind of control on the infinity norm of the derivative. This is the reason why we're adding this. And then for technical reasons, it's easier to add the infinity norm. add the infinity norm of p and the l to norm of the derivatives. So we will consider this norm for the evolution of this problem. This is the main aim of considering this norm. The reason is that now I can do this kind of bound. If I ever find in these perturbative terms anywhere, I find defined rates, I can bound it by the norm that I'm using. Okay? So So, what can you do for the linear problem? The first thing is, since you have this Poincaré inequality and this entropy structure, it's clear that you have a spectral gap in the natural space associated with the edge-to-entropy. So, this spectral gap was already known in previous papers, and I would like to change it to a different phase. So, there's a technique that has been developed in the case of the In magnetic theory, for example, it's useful for the Boltzmann equation. For the Boltzmann equation, you have a good theory for the linear or linearized Boltzmann equation, but if you want to actually use it to prove something on the non-linear Boltzmann equation, you need to have a linear theory in better spaces. So these techniques in this paper by Walden Michigan and Wong, they were developed exactly for this. So if you have some linear operator and you know that it has some spectral that negative spectrum. And you know that it has some spectral data negative space, maybe you can make this space larger by splitting the operator in some suitable way. So, a related idea, which is, I mean, it comes from the same place, but it's not the same, is the one that I mentioned here. And we used it in a paper a long time ago with Emo and Stena Mishta, which is based on the regularization of the equation. So, if you can take advantage that the equation regularizes your Regularizes your dynamics a little bit. So, if you start in a space, maybe you end up at time t in a more regular space, then you can take advantage of this to find a spectral gap in a smaller space. So this is the result that we can prove. Finally, for the linear operator, we still have spectral gap in this very strong space. It's not only converging in this natural entropy space, but it's also converging in this stronger space. So it's stronger because it contains a norm. So, it contains the norm for the entropy. So, in order to do this, you need to show some regularization estimate. So, from this equation, this is fine, you can get some regularization of this norm if you start in the space where you have a finite entropy. So, if you start in this large space with a finite entropy, at a certain time, you are Certain time you are regular enough to be in this space, and this coefficient here, this three-fourths, is the same that you will get for the heat equation. So, this delta function is a bit in the way of proving things, but only as far as you get to a space where this equation doesn't lie anymore. So this equation has bounded infinity norm and the derivative is bounded. Derivative is bounded. So it's reasonable that you can prove this kind of inequality. Of course, you cannot go further. For example, if you want to derive this, it's not true because the solutions clearly have this kind of discontinuity in the derivative. So these are kind of a regularization estimate. And if you use this, you can use this trick. This is a trick to change your linear space where you have your spectral gap. So you start, you want to estimate the norm in this structure. Estimate the norm in this strong space, and you say, Okay, if I wait some time, let's say I wait for time one. So, this is what I'm doing. I'm waiting for time one, and then I'm doing the rest of the evolution, okay? So, after time one, I know that the dynamics is regularized, so it's enough to know that I have finite norm in the larger space. So, this time where to norm is finite, it's enough. Then I know that this operator. I know that this operator has a spectral gap in this larger norm. So I know there's some decay. So here, strictly, you would get e to the minus lambda t minus one, right? But this is just a constant. Okay, so I represent this constant by this lesson button. So you get this, okay? And now this space is larger than what you have. So it's smaller than the norm in your initial space. So you can do this from time if you wait for a time one, let's say. Time one, let's say, and if you if you want to do it for time between zero and one, you have to prove this kind of well-positance estimate for this phase. Okay, so this works for this for this linear equation. And now I want to show you why is this useful. Okay, let's do in the nonlinear case the local stability of equilibrium. So we can look either at a weakly nonlinear case, which is this mode. Case which is B small, or we can look at a case where we are close to some equilibrium, and then this is we have to look at the linearized equation. I want to discuss both of these things. So, what about the nonlinear case? In the nonlinear case, the equilibrium looks the same as before, right? Now, the difference is that this n is not infinity. It depends on n infinity. So, this h is no longer given, it's something that. Longer given, it's something that has to be self-consistent. So I still have to normalize this, but this normalization constant n infinity is there and it's inside the exponent. So it's a quite a linear problem to solve this, but you can do this, okay? And if you do this, you will find, so let me do a picture here, okay? So Okay, so what I need to do to find equilibrium is to find places where this I of n, I remind you that is, I call I of n everything which is multiplying the n infinity up there. Okay? So what I want is I of n infinity to be equal to n infinity. 1 over n infinity. So what I need to do is to do this kind of graph. I write here i1 and this And this is going to be n. And when these are equal, then I find an equilibrium. Okay? And it's a one-dimensional equation because in a linear idea, it's one-dimensional. So what's a kind of graph that we get here? So we get something like this, okay? So it starts somewhere here, and you get things like this, okay? So we'll show you what it's saying in a minute. Okay, so I will say what it's saying in a minute. So if P is negative, you will always get only one cut. So the 1 over I looks like this. If B starts increasing and it's positive, you still get one cut, okay? Something like this. But at some point, this will be enough to do this. So you will get two cuts. So, you will get two cuts. When you get two cuts, you have two equilibria. And the numerics and all the conjectures seem to show that the equilibrium associated to this cut is going to be stable, and the equilibrium associated to this cut is going to be unstable. Okay? So, it has all these kind of interesting phenomena, where in some cases you have only one equilibrium, in some cases, you have two. And so, how would you go about proving stability of the app? Stability of the app. I have this picture. But this picture, I mean, I wanted to add this because it only contains a part some graph for the negative B. So if you increase B and go above zero, you will see this kind of graph. So this is the continuation of this diversion. So let me look at the entropy in the nonlinear case. So in the fully nonlinear case, we In the fully non-linear case, we don't have any general Diablo function. There's no general quantity which is clearly decreasing for the system. So you can do the calculation and you get this. You get the same as you had before. This is the same as for the linear entropy. And you have this second part, which is hard to control. And it makes things hard because it contains these derivatives of p. Derivatives of p. Okay, so it's a it's a bit um uh hard to control it, but uh it can be done, okay. So it is something that can be done in the if you work hard on this, on estimating this with the standard method, you can maybe you can do it. So, in the case of delay, it's the same, but you get something that has the Is the same, but you get something that has the that gets delayed. So, what can we do for small B? So, this was already mentioned, already done. So, this is a result which was known in this, maybe by Gabrillio, Saloa, and Smetz. And how do we do it here? So I want to take advantage of this new space that I'm considering. Of this new space that I'm considering. So the main advantage of this space is that this term that I see. Yes, do I want to do this? Exactly. So this term that we take as a perturbation, because we're considering small B, it contains this N. The difficulty is that it's hard to control this N. But now, in the normal way, this N is already. This n is already bounded by the norm. So you can do this kind of a bit of an ideal calculation, but here for this dv, I have to use a regularization that we mentioned before. If I do this, so this is a bit of a sketch of the proof, but you will get this kind of inequality here. Okay? So this inequality is not a simple Roma inequality because it contains this T minus F. Okay, it's a kind of a Volterra inequality. Kind of about Volterra inequality. And this can be solved. I mean, you can give conditions for the asymptotic behavior of this. And if the B is small, then this shows that the U has to go to zero exponentially fast. So it simplifies the argument in the sense that really the new term that we have, it's a perturbation. Let's say it's a non-singular perturbation of your equation in the norm that you're considering. The norm that you're considering, because you have a stronger norm. So, this is a way to reprove these results for the equation. And what about the linearized case? So, the nice thing is that for the linearized case, the same is true. And other techniques are harder to use for this kind of linearized equation. But it can be done in this case. So, this is new in our results that if I consider this. If I consider this Pd and I consider a given equilibrium, then I consider I can look at a perturbation around this equilibrium. And the approximation of the equation to first order, which I get, is this one here. So this is the usual linear equation, which you get from this term here, from one of the terms that you get money raised in this one, and from this term here. You get that one. And this, it's the one, the perturbation you get. The one the perturbation you get from the only non-linear term that I have, which is this one here. So you have that the linearized equation, it's the same as the linear equation, plus a perturbation that has to do with the, so it's small, the B is small, and it has to do with the degree of this equilibrium. Again, it's a, somehow it's a 1D perturbation because it depends only on one parameter. So this equation I need to look at. So, this equation I need to look at if I want to look at linearized stability. If I want to know whether the equation is stable close to the equilibrium, I should look at this linear PD here and say whether it has a spectral gap, what the eigenvalues are and everything. So, now the nice thing is that if you want to show that this linearized equation gives you non-linear stability, you can do the same type of equation. You can do the same calculation as before, and if you don't have this kind of norm, it's very hard. So, now this calculation is almost the same as before. The only change is that here you will get u squared, while before I had one linear term. So now, where's the smallness coming from here? The smallness is not in the b, the b is general, but you need two conditions. First, you need to know that this operator here has a spectral gap. Okay, this is fine. Has a spectral gap. Okay, this is fine. Then you need to be close to equilibrium so that you can say that u squared is much smaller than u. Okay, so this u is a difference to equilibrium. And if you are very close to equilibrium, then this is much smaller. If you stay very close, for example, you could estimate, so here you have u times u, you could estimate one of them by something very small. And then you keep going with the same linear inequality as before. Okay? So one nice thing is that once you have this very The one nice thing is that once you have this very strong norm, it becomes easier to bridge the gap between linear and non-linear. And the last thing I want to mention is this. I have about five minutes, maybe? Okay. So, okay. Something interesting about this linearized problem is that if you assume that you know the solution of the linear problem, then essentially you can solve it. Then essentially, you can solve it. Okay, this is something which was a bit surprising for us. So did I write this? Okay, it's fine. So I used the double-met formula in this previous equation. So I get the solution of the linear problem plus this integral term that we get from this variation of constant formula. Now. For you. Now, this only depends on this one-dimensional parameter, which is n. So, what I do is I take, let's say, I take n of everything, I take the derivative of vf, and this is now a closed equation that depends on this. Okay, it depends on h, which is a time-dependent function that I obtained from my knowledge of the linear problem, and this one, which you get it similarly also from the initial. Also, from the initial condition. So this can be solved independently. They are not happening to the equation. So if I'm able to solve this, then it's clear that you can do the asymptotic behavior of this. It's also not too hard to show that if you know that the firing rate goes to equilibrium, then the whole solution could go to equilibrium. Okay, so I just need to study this equation. So this is something we found out that That is, I guess, relatively well known in the literature on D-Day equations, but it's a bit hidden. And it's this. So this equation here, it can be understood. And if you do Laplace transform of this equation and you study this theorem that gives you the asymptotic behavior of H, sorry, of N in terms of the asymptotic behavior of H. of the asymptotic behavior of its Laplace transform, of the poles of its Laplace transform, then you get to this. So the behavior of this equation has to do with what H is doing. This is fine because it's always functionally converging to zero. So the behavior, the main important part of the behavior is due to H. So this is something you can test, for example, numerically. Example numerically. So these are some things that we still need to do properly. But I want to highlight that if you wanted to do some numerical study of whether the equation with delay is stable or not, this is quite hard. Because the equation with delay, it's an infinite-dimensional system. It it doesn't it depends on the whole solution in a whole delay time interval. Delay time interval. So it's hard, for example, to linearize or discretize and to find the eigenvalues numerically. So this is easier and you can see the action of the delay here. So if the delay is zero, then this is not here. And if the delay is large on the complex lines, this becomes turning very fast. So this is a consequence that we can. So, this is a consequence that we can give. This is another thing which is quite surprising, and is this, that, okay, I need to find out about the series of this function. And it seems very abstract, but it has a link actually with the slope with which these lines cut here. Lines cut here. It's a bit surprising. So I cannot calculate completely this Laplace transform of h, but at least I can calculate it at zero. What is it at zero? At zero, it happens to be this. Okay? It happens to be the slope at which this cuts here. Which, I mean, this is something which may be more general than we know. We only know this for this equation, but I would like to hear exactly also something similar. Of something similar in a kind of a similar equation. So I need to know about this. And I know something about the behavior at zero. So this is a consequence, right? Let's say that you find an equilibrium where at zero this is already larger than one. So at some point, when psi goes to infinity, it has to go to. When psi goes to infinity, it has to go to zero. But at some point, it needs to cut one. Okay? So you get this condition that for the equilibria, which are here, you see numerically, I mean also theoretically, you can see that since once it cuts, it has to cut back. The derivative here has to be larger than one. So if this derivative is larger than one, this equilibrium must be stable, must be unstable. Must be stable, must be unstable. For the other equilibrium, it's a bit harder, but you can say this. You can say that once you cross here some part where the derivative is smaller than minus one, then on the, let me go back to this. So if this is here, smaller than minus one, so it should be fine because this is a large. But then this, it's making it turn, okay? So if you take a large delay, If you take a large delay, then this will have to cut at some point. It will have to wrap around the number one. So in that case, you get this, that if this happens, then for small delay, you will not have any instability, so the decision should be stable, but for less delay, they should be unstable. should be unstable. And this can be, at the very least, you can see the whole picture vertically. And it's something that a question that we haven't solved completely is how far you can go in the rigorous proof that this holds in, or let's say, this dichotomy holds between small delay and large delay. Okay? So essentially this is what I wanted to This is what I wanted to explain, and thanks a lot for listening. Okay, questions? Very nice, sorry. Now, you are using a fixed delay, right? Yes. Pixie layer, how time? How about introducing like a memory different detail? Different ways with time. This is done in some models, and there are models which where the structuring variable p is not the voltage but the time elapsed since the last discharge. And yeah, so Gertam and Nicolas Torbes, which are around this and that will work all of this machinery will work. So the problem is this is very tight. This is very tied to at least the last part, okay? That you have a one-dimensional non-linearity. So, meaning, but here the detail it would depend on the whole time range, right? So, it changes. But the previous part is fine. So, this idea from kinetic theory that if you want to make a perturbation of the equation, then it's good to have a good banana space. This is very general, I think. Okay. Other questions? You said that it crossed one time up to a certain point and then it crossed twice. Can it cross more? Or you know that not for this model, but Handler, my husband, Nicolas, they're working on modification of this model that can have three caps. Is it right? No? If you include additional effects, like a refratory video, we can have them back. Like a refractory video, it can happen. But in this model, no. In this model, it's either one equilibrium or two equilibrium. Or none, maybe none. So at some point, so I should have continued the drawing. At some point, this is happening, right? So the general idea is that corrosion in the right direction is stable, in the other direction, is unstable. That's it, that's true, without delay. With delay, if you cross two streets, If you cross two sticks, then you may have instability due to periodic solution. All right, Joseph? The right question now is: what about period solutions? Yeah, yeah, this is the right question. So one initial idea we have for this, which I didn't explain very completely, is that if you have a very long delay, If you have a very long delay, it seems that the solution we do is I actually have a video that Alejandro prepared. Maybe I can show you this. So this is for B equals to minus 12. So what's happening here? This is what I mentioned at the beginning. The neurons staying at a given value of the N. Value of the n. And then they realized that there's some inhibition going on, so they go back. You still see, and now they come back. This is the kind of periodic solution that you can see if the delay is long. So what's one way to look at them? So you could say, you could try to think that if initially the day is very long, then for a very long time, Then, for a very long time, it's a linear behavior, right? So, you study the linear equation in this interval. Then, after this delay ends, you look at the condition in the previous interval, and now you look at the new linear evolution. And if you do this numerically, you get a close match to what the solutions are actually doing. So, we try to do this. We try to say, can you prove the existence of periodic solutions or behavior? Or behavior by following this kind of shifted linear evolution. So this is fine again for small b. For large b, it's hard to do it. So proving the periodic solutions, we don't have a good way of showing it at the moment. No. So you can prove instability, but you don't know due to what, right? I don't know. All right, so since we have the answer to the right question. Have the answer to the right question. I suppose we thank the speaker again.