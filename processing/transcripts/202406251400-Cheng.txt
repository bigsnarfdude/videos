Thank you very much. Thank you for having me. Thank you for extending the invitation to me. He's my pleasure to be here. So, as you can see, today's talk is in a dispute time setting. I apologize for that. I hope in the end you can still find something relevant. So, this is based on John Module Stephen, Microsoft Advisor. So, the goal of this project is to enhance our understanding on how the view game approximates Ampire games. Proximates empire games. So, this is my first project in MiField games. So, some of you may find the idea a little bit lean toward elementary size. So, please bear with me. So, usually, this problem can be tackled from two directions. One is to construct anti-equilibrium from metal equilibrium. The other one is to capture anti-equilibrium with mirror equilibrium. And if I'm not mistaken, the first direction is much better understood when a developed curve, but understanding for the second direction is still. Understanding for the second direction is still developing, especially in the case where we do not make any assumption on the uniqueness of the equilibrium. So here, let me also mention one of the major assumptions that we use in our project is this symmetric continuity, which means in the MPL game, when making the decision, the player only cares about her own state and the empirical state distribution. And this empirical state. And this empirical state distribution affects her decision continuously. We'll give more details on that. So there are plenty, I mean a massive amount of works in Minfield Game, but here, let me focus on those that try to advance the capturing capability without uniqueness assumption. So the first line of work is in a continuous time, second line is in both descriptions. Continuous time, second line you see both discrete and continuous time. We draw a lot of inspiration from them. On the brand skin of things, when compared to the first lines of work, we try to establish a stronger result with stronger assumption. When compared to second lines of work, we're trying to tell a story from different perspectives, although we are more or less work under arguably a similar set of assumptions. And there are also many technical nuances I try not to dive into. This I try not to dive into in this talk. So, we will first set up an MPS scenario and then we will construct the corresponding mainfield scenario and then we'll compare the difference between the regrets from both scenarios. And this regret is, in fact, also used or known as exploitability in other literature. And very recently, I know that this regret has a very restricted use in an algorithm setting. So now I regret using the. Setting so now I regret using the word regret in my title. Anyway, um, so here the setup will fix the numbers of players for the rest of talk for simplicity. I'll assume finite state space and actual space, but the ideas can be easily extended to more general spaces. And then we also use a finite horizon setting. So the transition kernels of a player is a mesh of value function that depends on the precise position. Depends on the precise positions of all players and the action of the player herself. So, usually, this kind of setting is too general for the meter approximation to work. So, later on, we need to introduce more assumptions. But for now, we will be sticking with this slight, this more general setting just to illustrate some ideas. Then we use a cost function and then a randomizing action kernel, which means the player got to flip a cone and make some decision based on the Based on the observation of the full population. And so a player's policy is collection, time index vectors of action kernel, and then a scenario is the collections of all policy from all players. And we use a very standard dynamics. We assume everyone starts in the same position, and then they will, even the realizations of the current state, they will draw the action randomly. And then after that, they will be transit to the next state in an independent way. Transit to the next state in an independent way, of course, conditions on the realization of the current states and actions. So, everyone uses the same objective: the expected total cost. So, now let me introduce a theory of decision process. So, this is for the notions of regret. So, here, from the player S perspective, if we assume player M may revise a policy while others maintain their original policies. Maintain their original policies and also allowing the player n to observe the current states of all players precisely. Then, this kind of optimization problem can be solved by establishing dynamic principle in terms of Brahman equation here, very standard stuff. And we use some dummy random variable to represent the generations of action and the transition to the next state. Okay, so now we can talk about the regret. It's also very straightforward, it's just a Very straightforward, it's just a difference between the outcome of player n under the original policy against the best possible outcome by assuming all others maintain their original policies. And then we have an equivalent definition, which is a total stepwise regret. And to see why this is equivalent, we just remove this conditional expectation using terminal property, switch the orders of the finite sum and expectation. And then this restarts, which has a lot of shader, except for V1. Except for V1 and V capital T plus one. So V capital T plus one is set to V zero by definition. And well, what we have done so far is just to give a name to this episode in the epstron ampere equilibrium. And obviously, if we have a situation or scenario where ops has no regret, then it is not equilibrium. Okay, so now. Okay, so now here is the assumption that facilitated the mid-field approximation. We have this notation here for the empirical state distribution. So the first two set of consumption basically guarantee or ensure that the MPI game has weak interaction. And then in the condition three, we assume that when making a decision, the player only cares about her own state and the imperfect state distribution. And then in the fourth one, we have some continuity. In particular, this condition three. In particular, this condition three and this part of four constitute the symmetric continuity we mentioned in the beginnings of the talk. And now I need to mention a discrepancy. So under this kind of assumption, it is more natural to think of a regret that also satisfied some symmetric continuity constraint. But the regret we talked in the previous slides allows poor observation. So there is this frequency in terms of our modeling. But in the end, this kind of difference can Again, this kind of difference can be neglected because, under this kind of assumption, it can be shown that they're kind of the same thing when n is large enough and also without suitable enough modulus continuity. We will revisit this issue in a few more slides. Here, just let me mention that heuristically, this assumption guarantees that the impacts of individuals is small, and this will be eventually. individual is small and this will be eventually responsible for the concentrations of the empirical measure on the on some deterministic measure. And then I need to stress that this kind of approximation really relies on the modulus of continuity mentioned here. So later on I will go through an example that provide some quantitative understanding. Okay, so now we have a set of MPA scenario. We want to construct a corresponding medial scenario. Corresponding mean field scenario. So the construction, let's first introduce some oscillatory terms. Let's call this one the mean field measure. So the idea is to say that embryo game is independent under the mean field measure. And meanwhile, we want to define vectors of state distribution. So definition is recursively is done recursively. So at timing one is simply. At time angle one is simply the direct measure concentrated at an initial position, and this allows us to generate a gain up to time two. And then, for time two, we simply take the state distribution, the average of the state distribution. This we define like the psi two, and then we can move on forward iteratively. And then now we are in a good position to talk about an important technical results, expected the propagations of chaos, which basically says that the empirical measure. That empirical measure will concentrate at some deterministic stuff, which is specified above. And for our purpose, we also need to consider this slightly perturbed version, so propagations of chaos. So the perturbation is that we allow one player to revise a policy to whatever she wants. It doesn't have to be continuous at all, just whatever as long as it's measurable and well defined. But meanwhile, we still define this society. Still define this society using the original scenario. So, somehow you can still show that we have some approximation. So, the proof is relatively straightforward. For t equal to one, it's obvious. So, for our larger t, at least for t equal to two, we are dealing with independent sampling. So, Hoffman inequality, we do the jobs. And then for larger t, we need to resolve some induction. So, we first assume that the previous step has. Assume that the previous step has the approximation, then we can do a replacement here to cashi t. And then the symmetric continuity will say, okay, this will not induce too much error. And then again, we're just dealing with independent assembly. And then we can use something inequality again. And there is another result will be used, which says that the state marginal under the mean field measure is approximately the same as the state marginal under the original measure. Under the original measure. Okay, so now we can define the mean view measure. Sorry, the mean field scenario. So here, by mean view scenario, I specifically means a vector of state action distribution. This turns out to be a convenient object to work with. So the definition is also straightforward. So for time t, the state action distribution is simply defined as the average of the state action distributions of all pair under the mean field measure. So this is for under. The Minfield measure. So, this is all under the MiField measure. This definition immediately tells us that the state marginal of the joint distribution at time t is exactly the sign even half. And then we can also tell that these vectors of state action distribution corresponds to a scenario correspond to a scenario in the mid-view game. So, which means if I draw the state action pair from the joint distribution and then transit them to the next output. And then transit them to the next approach using the transition kernel. We will recover exactly the state marginal at the next approach. And then we also look at the conditional distribution of action given state for the representative payers policy. Of course, this policy, I mean, download, it is expected to be some sort of a weighted average of all MPS action. And also this weight depends on X. And also, this weight depends on x, which is current state. And it's a weighted average in the numerator. We have the probability of payer n visiting state x. And then the denominator, we just do the sum across all pairs. Okay, so then we can introduce an auxiliary MDP from the perspective of the represented pair. So now, because this cassiet is deterministic, so Society is deterministic, so we can sort of omit that from the situation. And then, so the representative player only cares about her own state when making a decision. And because of the discussion we have in the previous slides, it's natural that the representative player use this induced policy from the joint distribution. But if we consider the hypothetical case where all players maintain their original policy and the represented player is policy and the representative player is allowed to change her policy. So in this hypothetical case, this natural measure or sorry, this natural policy may not be optimal. So this leads to the following optimization problem as usual. It can be solved by establishing a diaphragmic problem principle with some dummy random variable that represents the generations of random state and transition to the next state. That's right. The generations of random action. Okay, so we'll show one last. Okay, so we'll show one last technical result before we move on to the comparing of the regrets. So, this technical result is about that the optimal value function in the MP game from player M's perspective can be approximated by the mean field stuff I just defined. So, to show this kind of result, basically, we need to analyze some alternative scenario in the MPR game setting. So, this alternative says that This alternative says that player may revise the policy, but she will use an original one after time t and then change to the optimal one after time t. So that's why we have this kind of perturbed publications of chaos will become very useful improving this kind of result. But just to give another perspective, these populations of chaos approximately make the empire game independent. Approximately make the MPR game independence, and then thanks to the symmetric continuity, if we replace these things to the psi t, then from player m's perspective, the game barely changed. So the corresponding value function can be approximated accordingly. Okay, so now we can compare the regrets. So let's start from the MP regret. We write down the first slide using the alternative definition of a total stepwise regret. Those stepwise regret. Then, thanks to this approximation result, we can do some simple replacement. This will induce a little bit of error. So, in the next slides, I will write this conditional expectation into the integral form. And then, by observing the highlighted part here, we can use property A again to do some replacement. So, after this, observe that inside this expected. Observe that inside this expectation, the only randomness is this cosine, sorry, this xnp. So then we can just write everything in terms of integral. And now integrating with respect to the state marginal of player n. And then we use property b to do another replacement. So that is good. What we have done so far is that the individual regret in the empire game. Regret in the MPA game can be approximated by this calculation that involves only turns defined in the meanwhile zero. And then I would like to detour a little bit to revisit the discrepancy I mentioned earlier. So under our assumption, it's more natural to think of regret under the symmetric continuity. But basically with similar idea, we can show that this kind of regret is not much difference from a regret that allows for That allows for observations. Okay, let's do the one last thing. That is to look at the average required in the MP gain. So thanks to this approximation, we can write down the first line. And then I will group the higher return into one single integration. It turns out the integrating measure is this joint distribution. And after this, I will split this integration. After this, I will split this integration into two parts. It's like first integrating with respect to the action kernel, software represented player. After that, integrating with respect to the state marginal. So inside here, we are basically computing some sort of sub-optimality. And that becomes more obvious if I write everything into the expectation form. So this is what I would define as the mean fields. Mean fields total stepwise regret. How will we do time? Oh, can be. So yeah, we are first computing the sub optimality at H time T and then aggregate. And then with a similar reasoning as before, we can show that this is the same as the mean field n regret, which compares the aggregated outcome directly. So to put everything So, to put everything into one theorem, so it says that if the MP has a MPI environment, has a weak interaction, and then the scenario satisfy that all players use symmetric continuous policy. And then if we construct the mean field scenario accordingly, the resulting regret will be approximately the same as the MP regret. So, this in turn tells us that Me view game will not miss well-behaved equilibrium in the approximate way. And then we can also enhance the story a little bit by showing that the state action distribution will concentrate at this joint distribution, which tells us that we can look at MIV game to extract some high-level statistical information on what's going on in the MPA game. And all these results. MP and all these results do not require ample polymer. So, this could provide some additional utility. And I should all stress again that all this approximation really relies on the modulus of continuity. So, the last part of the talk, I would like to go through a very simple example that says that if I have a very bad modulus of continuity, all the approximation will become Or the approximation will become vacuous. In particular, I will miss equilibrium for memory game if I log into the mean field game. So here we consider the general environment. So at each time, the player got to choose whether they want to go to level one or stay at level zero for the next approach. And level one is an absorption state. And then level one at t equal to two has a reward of one or cost of negative one. And then in the end of the Of negative one, and then in the end of the game, everyone will be penalized by 10 times the portions of player that share the same position as her. So, in the end player scenario, let's consider the following. We assume nobody will go to level one at t equal to two. And then at t equal to two, the player will look at whether there's anybody in level one. If there's nobody there, they just randomly scatter evenly across the two levels at the end of the game. Level in the end of the game. But if there is at least one person in level one, then they will all go to level one. So this is a situation like where everybody reaches an agreement to not get a foot. And if one person gets a foot, then the other will engage in punishment regardless of cost. So we can show that this is amber equilibrium. But then this policy is technically. This policy is technically symmetrically continuous, but it has a very rough modulus of continuity, which are of a scales of 10. So if we leave everything to the mean field setting, we will get a regret of 1, which says that the mean field approximation is missing equilibrium. And then in the literature, there are similar examples. So our example shared a similar feature that we have a disturbing environment and a certain German environment and of certain state. So, the natural conjecture that comes out of this kind of example is that if we do not have this kind of setting, mean view approximation may still work. And in our paper, we also try to do some technical efforts, try to integrate into our analysis some general state space and visual version. But anyway, I'll conclude my talk here. Thank you very much.