Thank you, everyone. Thanks to the organizers for putting on such a great workshop. Yeah, it's my pleasure to be here. Everything I'm talking about today, I want to just say, is joint work with Mikhail Farab and George Willis, both at the University of Newcastle. And everything I'll be talking about is some ongoing work. So much of this is still in progress and builds off some previous work of George's. So, yeah, since I. So, yeah, since I think the talks have all been at different times of day, and I know not everyone has been at all the talks, I'm going to start by defining Narraton's group. Probably most people here know what Narraton's group is, but let me just review a little bit and we can talk about some notation that we'll be using throughout. So, Narraton's group is usually defined via almost automorphisms of trees. So, what we'll start off with is: we'll say a tree T. A tree T is almost regular if all but finitely many vertices have the same balance. And let's say valence E, and I'll just declare that Valencia. And I'll just declare that valence E to be d plus one. So all but finitely many of my vertices will have valence E D plus one. And then we'll say an almost automorphism then an almost automorphism of T is given by a triple A B C A B C where A and B are both finite subtrees of T. And phi is an isomorphism of the complements. So phi mapping from T not including A to T not including B is an isomorphism. Is an isomorphism of these infinite four. So when we remove some finite subtree, what we're left with is a collection of trees. And so then we're looking at isomorphisms among the collection of trees that form the complements of A and B here. So, but what we need to do here when we're talking about these almost automorphisms is we really need to. Those ohmos automorphisms is we really need to talk about equivalence classes of ohmos automorphisms in order to make the composition operation well defined here. So we'll say we say A1B1 V1 is equivalent to A2B2B2. If we can essentially expand A1 and A2 into some larger tree. two into some larger tree such that the maps the phi maps agree and more precisely if there exists a such that a1 and a2 are both contained in a and such that phi1 restricted to t takeaway a agrees with phi2 restricted to t takeaway a so in other words we can always extend So, in other words, we can always expand our trees that include, you know, the piece that we're removing, we can expand to make them larger. And if we can expand both pairs in such a way such that the maps agree, then we're going to declare these two almost automorphisms to be equivalent to each other. And so then we can say Narrowden's group is the group of equivalence classes of ohmos automorphisms that we have. And this was defined by Meriden in the early 90s. Okay, so it's sort of common to go back and forth here, I think, between rooted versus unrooted trees. So I just want to talk about why I'll probably be drawing my trees as rooted trees for this picture. For this picture. So let me grab a picture here from down below. Okay, so here's an unrooted tree versus a rooted tree. So if we work with unrooted, for instance, ternary trees, what we can always do is Ternary trees, what we can always do is we can declare one vertex to just be the root, and then we can just pull it. We pull that up, and suddenly we have this tree here where my root has three descendants, but all my other vertices have two descendants. And this idea of only allowing finitely many vertices to have different valency allows us to kind of force the bad vertices to be at the top of this tree if we want. So, with this picture in mind, I'm going to be drawing my. mind I'm going to be drawing my my almost autumn work of them by looking at rooted trees and so in this case what we can usually what we can do so let me grab a pair of trees here and then the standard way then we'll And then the standard way then we'll sort of represent these ohmos automorphisms. So remember, ohmos automorphisms, as we said, they're defined by having two trees as well as an isomorphism on their boundary. So to represent these, we can take, say, this tree here to be A, this tree here to be B, and then all what we need, the property A and B, what they need to have is they need to have the same number of leaves. Need to have the same number of leaves. So, here both trees have seven leaves. And then, what you can do is you can just number the leaves one to seven on my domain tree. So, one, two, three, four, five, six, seven. And then I can define how these are going to line up, how the complements are going to line up with each other by numbering the leaves in B. So, say one, two, three, four, five, six, seven. But then we're allowed to do a little bit more than just shuffling the complement trees. Shuffling the complement trees. What we're also allowed to do is we're allowed to perform automorphisms on each of these complement trees. So then we can add some labels here: say alpha one, alpha two, alpha three, alpha four, et cetera, where each one of these alpha i's is some automorphism of a rooted tree. And so this is one way that we can think about these elements in Narratin's group is using these sorts of diagrams. And this will be relevant to us as we go through. And this will be relevant to us as we go through and try and understand how to do various calculations in Naradin's group. So, Naradins group has been studied in the world of TDLC stuff for quite a while. It's one of probably the more studied examples of a TDLC group. So, yeah, so Naradins group is a TDLC group, of course. Although its topology is not the one I think that most. So, its topology is not the one I think that most people think of right away. Usually, your first default assumption is to try and put the compact open topology on Narradins group, but indeed, we have to work with a slightly different topology to turn it into the TDLC topology. But some nice properties about the Naradins group that make it worthy of study is one, it has a dense copy. So, yeah, if you know anything about Higman-Thompson groups, which is the perspective that I come from when I'm thinking about these things, so the Hignan-Thompson. So the Hignon-Thompson groups, the Hignan-Thompson group is dense in Neridan's group. If you haven't seen the Higman-Thompson group, it's really defined very much the same way as using these tree pairs like what we have here, with the exception of you're not allowed to do any extra automorphisms on these complement trees. These complement trees. So, all you're able to do is you're able to permute the vertices around and you do this sort of tree pair representation. All right. So, another reason to be interested in Naradan's group is that Capujian showed that it is simple. So, Capujian proved it's a simple TDLC group. TDLC group. And even more than that, Vader, Capras, Galander, and Moses showed that it has no, it has no lattices, making it the first of its kind. All right. Kind. All right. And so, yeah, so it having this dense copy of the Hidden and Thompson groups and having the simple structure and having their lattices, it makes one, you know, one maybe interested in trying to perform various types of calculations in the Naradens group. In general, for any TDLC group and any automorphism of a TDLC group, one of the things that one might want to study is this object called the scale. So now that we know the group, let me define for you what I mean by the scale. You, what I mean by the scale. So the scale, let's say. All right, so for any TDLC group and any endomorphism, say alpha from G to G The scale of alpha is the following. So S of alpha is defined to equal the minimum index of, so alpha U intersect alpha U, where U is a compact open subgroup. All right, so for a given, yeah, for a given alpha, this number will be some finite number. And it gives you some sort of measure that helps you to try and understand what this endomorphism is doing. It gives you sort of a measure of compressing and expanding properties of the endomorphism. So we'll say a subgroup U is minimizing. Is minimizing for alpha if it actually achieves this minimum. In other words, alpha or u is some subgroup where s of alpha is actually equal to this index here. Okay, so there's been a lot of work over the years for trying to understand the structure of minimizing subgroups and computing scale and different groups. And so the goal here is to compute this specifically in the case of Naradin's group and even more specifically when the automorphism is coming from conjugation. So in other words, let X be some equivalence class A, B, C. There's some element in narratives group. So considered as an automorphism of N of Meriden's group via conjugation. And then the goal then is to use the tree pair. Is to use the tree pair to understand the scale. In other words, can we start out with this representative of ABC and try and use it to try and actually come up with some algorithm that lets us compute the scale when we're thinking about this conjugation? The starting point for doing this sort of thing is to first understand the dynamics. Of thing is to first understand the dynamics of such a tree pair. So, step one is to understand the dynamics. Of such a pair. So, A, E, B. So, yeah, there's been a lot of people have studied this sort of thing. A lot of people have studied this sort of thing, looking at the tree pairs, trying to understand the dynamics on the boundary of the tree. And so, yeah, and so what we're going to talk about here is something that was first developed by Matt Bren that has, you know, various simplifications have been made that allow one to understand sort of what's going on just by starting with some kind of tree pair representative. So more specifically, let me define a couple of things that we'll need here before I can really make this precise. Before I can really make this precise. So, first, so if we let X be this pair, A, B, C, or triple, I guess I should say, and Neridan's group, then a chain is a sequence x zero, x one, through x n. One through Xn of leaves of A union B such that the starting vertex in the sequence is an A. And when I apply phi to the ith term in this sequence, I get the ith plus one sequence. So let me show you a picture of such where we can. Picture of such where we can sort of identify some change. And the picture I'm using here comes from a recent paper by Goffer and Lidderrel. So I'm just sort of borrowing one of the examples from their paper here. Okay, so here what I have is I have two trees here that have collected. Trees here that have collectively 13 leaves each on them. So, just some explanation of what's going on in this picture here. So, first of all, this element that we're drawing is actually an element in the Higman-Thompson group thought of as a subgroup of Meridins. So, there's nothing extra going on other than commuting the complement trees. And what I have here is, so this tree here is my A tree, this here is my B tree, and I've used gray to show the overlap. To show the overlap. So the light gray portion here is the portion of the tree A that shows up also in B. And so then you can see that this portion here is in A but not in B. This portion here is in B, but not in A. So we can start identifying some chains by looking at this picture and just tracing out where various vertices get mapped to. So the first type of chain that I'm going to be interested in here is something called a wandering chain. Chain. It will say a wandering chain. This is one that begins in A, ends in B at a vertex not related to the starting vertex. Starting or just as an example here, what we can see is we can take, say, this vertex number three here. So the vertex three, it goes to this vertex here in my B tree. And now it can't, I can't keep applying phi to it because. Can't keep applying phi to it because now you know now it's this vertex that's in B, but that doesn't exist on, it's not one of the leaves of A any longer. But what we have here is that this, the first vertex, these two vertices, they're neither parents or children of each other, right? They're not related at all in terms of, yeah, so neither one is a descendant or an ascendant of the other. So this gives me an example of something of what we're An example of something of what we're calling a wandering chain. Another example would be a chain where we start out here at this vertex, it's labeled 12. So 12 ends up here, but 12 on my B tree, so this is also a leaf of A. And so it's a leaf of A, and it is, I believe, this one right here. So now we see where 9 goes under the automorphism B. So phi sends 9 to 9. Again, 9 here is another leaf of A. It's right here. It's right here. And we keep going. 10 goes to 10, which is here. And so, what we see now: this is the end of my chain. My chain has now reached some vertex that's in B that is not a leaf of A. And it's, you know, we started at 12. We ended at 10. And these two vertices are either ascendants or descendants of each other. So this is a second wandering chain that exists in this pair of trees. A second type of chain that we're going to be interested in is something called periodic chains. So in this case, a periodic chain is a chain that could, so it's a chain roughly, let's say, that could continue forever. So, what I mean is it's going to essentially cycle for us, forever cycling. Cycling. So, an example of this would be starting at the vertex labeled six in my picture. So, the vertex labeled six goes here, but the six is again one of my vertices of A, one of my leaves of A. So then it gets mapped here to 13 and 13. So, this is my 13 vertex in A. This is a little bit confusing, probably, to follow as we're going here, but hopefully. As we're going here, but hopefully you can sort of see how this is going. Uh, okay, so it goes to this, um, then it gets mapped here, and we just keep tracing this out. And let's see at the next step. So this, and the next step, it's back here to the starting vertex six. So we followed this sequence of vertices by applying this automorphism phi. And what we're seeing is the vertex six, it goes through a sequence of steps. It goes through a sequence of steps, and as we iterate, this it comes back and ends up back at the vertex 6 after a series of iterations. A third type of chain that's going to be useful for us is what we call attractor chains. So this is when Xn is a descendant of X0. So when after following the sequence of leaves here, what we end This sequence of leaves here, what we end up with is a vertex that's a proper descendant of the vertex we started with. So, in my picture here, such a thing would be this vertex four. This would be the start of my chain. So, if I apply my phi to it, what I see is four gets mapped here. And if you look closely, you'll see that this vertex here is a parent of this vertex right here. And so, what we see here is this is giving us an example of something that. us an example of something that is in a tractor gene. The idea here that you want to think about is that points on the boundary of my tree are getting pulled down. So they're getting, we're sort of pulling things down towards this point on the boundary. And finally, the third type of the fourth type of chain we're interested in here are repeller chains. So repeller chains are the opposite of attractor chains. So this is when after a series of iterations, And then after a series of iterations, X0 is, oh, I think here in this example, I should have said ascendant. Let's see. And yes, a repeller chain is where X in, let's see if I say this correctly, X zero. No, apologies. Okay, sorry. A repeller chain is when X0 is a descendant. Of Xn. All right, and so in this case, an example would be if we start at the vertex labeled number 11, it's first going to get mapped here. But this vertex in B is again a leaf of A. And so it is this leaf right here. And then we see that five puts us here. Here. And so, what we see is we started at this vertex, and after a series of iterations, ended at this vertex, which is a proper ascendant. So, yeah, so these repeller chains are doing the opposite of these attracting chains. So, repelling chains are things that are pushing you away from the boundary of the tree. And then we say that a pair, so well, a pair of trees or maybe. Or maybe I should instead of referring to pairs, let me say a triple is a revealing, so a triple say ABC is a revealing triple. If one, every component of a takeaway B contains Contains a unique propeller. And when I refer to a repeller here in a repelling chain, I'm referring to the starting vertex. So it has the starting vertex of some repelling chain. And similarly, we want every component of the takeaway A to have. To have contains a unique attractor. So in other words, yeah, here this is going to refer to the end of my chain here. So this example here that I have, so we didn't trace fully everything out, but this example by Gopher and Ludale and their papers. By Gopher and Loudale, and their paper is an example of a revealing pair. You can check carefully here by going through each one of these definitions and see that we basically have three components. So we have this component of A that's not in B, we have a second component of A that's not in B, and we have a component of B that's not in A. And you can check carefully through these definitions that each one of these components is either going to contain the start of a repelling sequence or the end of an attracting sequence. So, Matt Bren proved that every element of the Highman-Thompson group actually has a representative given by a revealing pair. So, Brin, every element of the Higman-Thompson group has a representative given by a revealing. Revealing. Yeah, so I'm using the word triple because there's really three pieces that define these three things. But I keep accidentally saying revealing care because I think in the literature it's usually called revealing care, referring to the pair of trees. But I will say, so revealing care and revealing tribor, this meaning in this terminology here, whether or not you refer to, yeah, the thing describing. Yeah, the thing describing as a triple or a pair. All right, but anyway, the point is, Matt Brand showed that every element of the Tigman-Thompson group has a unique representative or has a representative given by a revealing pair, but he really didn't give an algorithm for producing it. And so some algorithms have shown up in the literature. So I will reference again this Goffer-Lodauer paper for an algorithm. So Goffer and Lodau. I'll give an algorithm for obtaining it. Their method of obtaining it involves a technique called rolling, where basically what you do is you take one of these components here and you add it in some extra locations. And at each step of this procedure, you basically reduce the number of components that fail to have an attractive. Components that fail to have an attracting or repelling vertex. And so, yeah, there's an additional property we will sometimes want on our tree pair representatives. So, sometimes we'll also want something called pre-tidy form. So, additionally, we will sometimes need A, B. A B C to be in quote pre-tidy form. Let me again just give kind of an intuitive meaning for what we mean by pre-tidy form. So we'll say, let's say if you have some vertex V. Which is a descendant of a repelling vertex. As we iterate this mat phi, as we've seen sort of in these examples, as we iterate this mat phi, this element v is going to get this leaf, this vertex v, it's going to get moved along as you apply repeated iterations of phi. Apply repeated iterations of C. And so what's going to happen is, you know, this vertex V, this descendant of this repeller vertex, it's going to get dragged along until eventually it becomes a descendant. A descendant of an attracting piece. And so, what we're going to be interested in is the situation where maybe some, so as this vertex V gets dragged along, all of its descendants are also getting dragged along. And if all of its descendants, I'm just going to say. If all of its descendants, I'm just going to say in words here rather than write out this formal definition. If what happens is if there's some level where all the descendants of B, after it's being drag along here, if they all line back up with each other on the other side, then what we want is we want this vertex B to have never become undefined. So what I mean by this is when we talk about these almost automorphisms, we call them almost automorphisms because they each Them almost automorphisms because at each step you're defining something that's an automorphism of the tree up to some finite number of vertices or some finite number of vertices where the automorphism where things are being undefined here. And so what we want for something to be in pre-tidy form is we want that if after some iteration, it looks like this vertex and its descendants have all stayed together. We want them really to have actually all stayed together. We don't want them at... All stayed together. We don't want them at any point, we don't want any point on this vertex v to be undefined. So, this is a very hand-wavy definition I'm giving here rather than getting really deep into the weeds here, but hopefully this gives some sense to you as to what we mean here by this pre-tidy form. So elements are not in general already in pre-tidy form, but as it turns out, every element of Naradin's group. Has a conjugate and pre-tidy form. And George has this method for computing this conjugation. All right, so now once we have this automorphism, and yeah, so once we have, okay, so once we have. We have X and pre-tidy revealing form. Step two is to build something called the transit graph. Okay, so the transit graph, what it is. Okay, so the transit graph, what it is, it's going to come from these pair of trees that we have, and it's going to be a graph that's going to let us try and understand these, again, it's going to give us a simplified way of trying to understand some of what's happening for this particular element. So again, there's a lot of definitions here. And so rather than give formal definitions for each one of these things, I'm going to go through this meter alcohol for example and show you an example of a transit. An example of a transit graph. But let me start by saying the transit graph is made up of four separate pieces, and these four separate pieces are going to correspond to these periodic chains, these attracting chains, these repelling chains, and the wandering chains. So we'll call the transit graph. I'll call it script G sub X A, and it's going to be made up, yeah, of these four. And it's going to be made up, yeah, of these four pieces. So G sub X A E union G sub X A plus union G sub X A minus and then union some bits that come from the wandering. All right, so this first All right, so this first portion, this E, so this is going to be, this is going to be a graph, a finite graph, okay with vertices coming from the periodic chains. The G plus. The G plus part is going to be a portion of the graph that we build up by considering the attracting chains. G minus here then is going to come from the repelling pieces. And yeah, I've already said that the fourth piece is going to correspond to the wandering pieces. So let me bring up an example here. So, what I have here in this picture is I again have this. This picture is: I again have this lead-alfrocker example. But what I've done here is I've combined the portions of, I've sort of combined things down into a single picture. So more precisely, what I have is I have some, so this red portion here, let me highlight it here. So this is, remember, this is the component that's in A, or yeah, this is the component that is in A, not in B. B and similarly over here. And then I also have this component that's in B but that's not in A. Maybe I'll highlight this here. So this is the component that's not that's in B, not in A from their example. And so what I've done is I've just drawn the tree here that I get by taking A union B. So this is A union B. Union B. So this is A union B in a single picture here with colors to sort of indicate which portions are coming from A and which portions are coming from B. And then what you can do is you can trace out in this A union B, you can trace out all of your chains and see what's happening at each one. So for instance here, this green chain here we see is one of my repelling pieces. So this was these arrows correspond to what's happening under the various iterations of phi. So yeah, you may. Of phi. So, yeah, you may or may not remember from much earlier that, yeah, that what we have here is this vertex here that's being sent to a vertex two levels above it. So, in my picture here, what we have here, so I told you there should be a portion that corresponds to just the periodic chains. The periodic chains here are drawn in teal. So, if you look here and you trace out what happens, I get this cycle in teal. And so, this cycle here is called. And so this cycle here is corresponding to this portion of my transit graph. And I have another vertex that's also part of a periodic chain, but all that's happening is it just keeps getting sent back to itself here. And so this corresponds to the second portion and teal. So this is what we are calling G sub X A E is this portion of the graph. And yeah, so what I'm eating telling you is. And yeah, so what it's relay telling you is how much we have of this periodic portion. And then similarly, now what I have is I have a portion for my repellers. So here are my two repelling pieces. So what's happening here, this one here is coming from this pair of vertices. So I have these two vertices that are indicating that this one's jumping up to levels in my tree. In my tree. But as it moves, it's going to drag along these two vertices alongside it. So that's why we have this portion here. This indicates this remaining portion that was in A, but not in B, that's going to get pulled along as we do this repelling action. And notice that what happens, though, is as these pieces get pulled along, eventually we can see that these are also the start of wandering chains. So what I have here in order. Chains. So, what I have here in orange is my wandering chains. You see that they're all starting on a red portion and ending in a portion that's in B but not in A. And that's going to correspond to these paths here in my transit graph that start in my A part and in on my B part. And then finally, the fourth piece of the graph is going to correspond to these parts that are in B that are not in A. Parts that are in B that are not in A, and so this is what's happening here. And so, what you can see is I have these sort of so this is my G minus, this is my G minus, this is my G plus, here is my wandering pieces, and here I have my periodic pieces. And all four of these pieces together form this single graph that we're calling the transit graph. All right, so from this, then what we need to do is we're going to take this transit graph. And if we assume that our starting element that we're working with is in pre-tidy form, then we can use this transit graph to build the next thing, which we call the admissibility scheme. So all of these things that I, yeah, all of these graphs were first or were defined by George. So step two is to use To use the pre-tidy transit graph to build admissibility schemes okay, so I see that. Okay, so I see that I only have a few minutes left here of the talk. So let me just kind of say in words again, rather than writing out the formal definition of this admissibility scheme, what the admissibility schemes are going to be looking at is they're going to be a way of trying to understand automorphisms of this transit graph. So what we're looking for is we're looking for automorphisms of the plus and the minus part that essentially agree with each other. So in other words, I want to be able to do some automorphisms here, such Automorphisms here, such that, and automorphisms on this side, so that when I pass along these transit chains, the automorphisms agree with each other along them. And this is going to allow you to build a whole sequence of graphs. And again, yeah, I apologize that I'm, you know, only have a few minutes left and won't be able to really formally define this. But then what I want to say here is proposition. Willis is that let X equal A E C be an almost automorphism of T in pre-tidy form. Then the scale of X The scale of x is equal to d factorial to the k divided by m plus, where d here is the degree of the tree that we're working with. K is something called the weight of the pair, which again can just be read by looking at the tree, the pairs of treaties, and where M plus. and where M plus is a particular is the size of a particular set of admissibility schemes. Okay, yeah, so I only have maybe two minutes left, which is not enough time really for me to define anything further. So I think I will just end here. Thank you. And here. Thank you.