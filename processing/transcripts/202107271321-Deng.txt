So, for the afternoon session, our first speaker is Yu Deng from the University of Southern California. And so, at a very young age, you already made several very important contributions in water waves, stability of 2D Euler equation, and random dispersive equations. So, today he will give us a talk on his recent exciting work on full derivation of the wave kinetic equation. question thanks for the invitation so it's a pleasure to to speak at this very nice workshop with a lot of different people from different backgrounds so okay so i will start um so today i'll be talking about the derivation of the weight equation so basically it's um the central topic in the theory of weight turbulence which i'll describe in the talk so we start with So we start with the parents that will first start with the background, we'll describe the physics and mathematics of the theory of wave turbulence. And then we'll describe, we'll discuss some of the recent previous results and also introduce our main results. And after that, if time permits, we'll maybe discuss a little bit the ideas of the okay. So we start with the background. So basically, what we're talking about the theory of wave turbulence, so from the general point of view, it belongs to the so-called problems, the turbulent six problem, which is stated as the manual treatment of axioms of physics. So specifically, we are interested in the picture of statistical physics here, where the main idea is that we start with the system of With a system with a large degree of freedom, like the microscopic system of the particles or waves, as we see, and then we'll take a limit where the degree of freedom goes to frequency, and then we'll deduce, and then we'll, and I forgot to say here that we're taking some form of statistical averages here before we pass the limit and then we deduce for these statistics. Reduce for these statistical findings, we use the equation that governs the macroscopic behavior, or in some cases, the mesoscopic behavior of physical moments. So, a classic example here is the derivation of Boltzmann's equation, which I'll be in the next slide. It was introduced in the 19th century, in the 70s, and it was rederived from the system of Derived to derive from the system of particles under known elastic conditions. So, in the so we'll be looking at this Bolton's equation in the so-called Polesman drive stain in it, which we explained in detail later on. And this was done by Fion of Landfloy in 1975, and there were also subsequent works elaborating and keeping this result, and this is especially in the work of Gannon. especially in the work of Ganner Synvron and Paxia, where they derived this, where they was able to make this fully reverse. Okay, sorry, is there a problem with high sound? Yes, there seems to be some your voice seems to be a little muffled. I don't you're using the microphone from the laptop or from the iPad. I but okay, so maybe is it good now? It's still a little muffled. Okay, I'll try to connect the sound with the with iPad there. Okay, is it better now? Actually, much better, yes. Okay, okay, yeah, I don't know why, but. Okay, okay, yeah, I don't know why, but somehow it's like this. Okay, yeah, so this is the this is the the the Boltzmann equation, and in the next slide I'm going to discuss the details of this of this problem. And since the theory of wave turbulence can be seen as an analog of this, so we'll first describe the Boltzmann equation. So basically, we consider the we're having like n particles in Rd, and each we're viewing this, each particle as We're viewing this each particle as a ball of radius r, where we'll let n goes to infinity and r goes to zero. And then we assume that these particles undergo elastic collisions, and each particle has a state, which means a position and a velocity. And initially, we assume the states of these particles are random and independent. And then, of course, they are going through collisions, which will change the velocities. And so we are going to have this evolution. So, we are going to have this evolution of these states. And then we're having this n-body goes to let n goes infinity, the degree of freedom goes to infinity. And then we're considering the evolutions at later time. And then we're considering the limit as n goes to infinity. And we're considering the density function, which means the portion of particles at a particular position and a particular velocity. And then we're looking at the, which is a statistical quantity, and looking at the limits, evolution of this statistical. The limits evolution of this statistical quantity. Okay, and the theorem of Lanford states basically that if you assume that these limits are taken in such a way that this quantity alpha, which is n times R D minus one, which is comparable to one, then in the limit, the density function is satisfies the average equation, which is exactly given by the Boltzmann's equation, which is a transport equation. Equation, which is a transport equation with the right-hand side given by a collision kernel. Okay, so this is basically the idea. We have a large number of particles, and then we look at the statistical quantities describing the statistical average behavior of these particles, and then we take infinite degree of freedom limits, and then this will, then we can derive the Boltzmann equation from this limit. And the important feature of this is that we start with time reversal dynamics, and we by looking Dynamics, and we by looking at the statistical quantities and taking a limit, we get the Bolsonaro equation, which is time irreversible. Okay, so this is the case of the system of particles. And of course, from the physical point of view, it's natural to consider the same derivation for system of waves. And here we're considering the system of the large seasonal wave with a large number of wave modes, and these wave modes. Or wave modes, and these wave modes are modeled by the Fourier modes in a large box of size L. Well, we take L goes infinity. So, basically, this L or more precisely L to D plays a role of the number of particles N in the Boltzmann switch. So, essentially, here we're having this large box of size L, and then this is like the X space. And in the fourth space, in the K space, we're having a mesh of Having a mesh of mesh size one over L, which goes in the number of moles, of course, goes infinity when L goes infinity. Okay, and the more when we're considering these wave modes, these wave modes also go undergo collisions, and here the collision just means non-linear interaction of waves. An important example here is the non-linear Schoren's equation, which if you look at it, you can say call it a four-wave interaction. They call it a four-wave interaction or cubic equation. Of course, you can also consider three-wave interactions with a quadratic equation, but here it's kind of natural also to consider the cubing non-inshoeing. Okay, so this is a cubing non-inshoe equation on the size L periodic box as we describe in this picture. And then here, instead of size R of the particles, here we have a parameter lambda, which corresponds to the amplitude of the waves, or you can, since the lambda is can be. Since lambda is can be also put here, you can also view it as a strength of the nonlinearity. And here we normalize it to define this alpha to be lambda squared times L minus D, which represents the strength of the wave interactions. And with film dimension is at least two. Okay, so this is the setup of the equation. And as in the case of particles, in the case of particles, we're assuming. In the case of particles, we're assuming the initial states of the particles are independent and random. And here we're also assuming the initial states of the wave modes, more precisely, which is the Fourier coefficients of the initial data of the Schr√∂dinger equation. We're assuming these four modes are also independent and random. And more precisely, we'll consider so-called well-prepared or random phase amplitude initial data, where we have this each work of this LOD is just like a normalization constant, and then we have this. Constant. And then we have this Fourier coefficient, which are independent. And we have this square root of an initial k. Initial is kind of a Schroder's function, which decides a profile of the data. And then this either k is independent, like random variables. It could be Gaussian, it could be non-Gaussian. That doesn't make a big difference. Okay, so this is like the random, so the random phase MP. So, the random phase amplitude means that basically the phases are random, the amplitude is random, and they are all independent. Okay, so the modes k for modes k ranges in a lattice of match one over L. This initial is a short function, which is supposed to be initial data of the wavelength equation, what we're going to derive later. And this e the KID random variables. Okay, so again, as in the case of particles, well, whilst having this very nice. was having this very nice independent you know initial distribution of the of the modes and then we are going um you know evolving according to the non-neutral equation and the question is what does the uh what does the the evolution at positive time what does it what's its what's it behavior uh in the limits where l goes infinity and we'll say al go to zero okay so basically we're looking at the first of all of course the looking at the variance or the you know the the square moments The square moments of these quantities. And then, after that, what we'll be discussing the general evolution of densities of these works. And that is the statistical quantity we're looking at and where our aim is to describe the evolution of these quantities in the net. Okay, and all these considerations leads to the theory of wave turbulence. And as we just said, the question is the behavior of these square. Is the behavior of these square moments and more generally, the behavior of these densities? Okay, so this question was first answered by physicists and when they predicted that these square moments in a limit are going to be governed by an evolution equation, which is called the wave-Kinetic equation. And moreover, they identified a particular time scale, which they call the kinetic time scale. They call the kinetic time scale, which is here is exactly given by alpha minus two, where alpha is a relative strength of nonlinearity. And they predict that this square moment is exactly approximated by the solution to the wave kinetic equation, rescale to this kinetic time, plus a limit, plus an error term, which vanishes in the limit when alpha, or L goes to infinity and alpha goes to zero. Okay, so the n is the solution to wave equation, which we introduce in the next. Solution to wave equation, which we introduce in next slides, and with initial data given by initial data here. Of course, if t is zero, then you just get initial data here in this term, and then you just get the initial data which matches it, and this term is zero. Okay, so this is the description, the prediction of physics. And the Waivin equation, which was first derived by physics, looks like the following: so it's like a dt of n. So it's like a dt of n with initial data given by this one, and this dt of n equals a cubic nonlinearity, or cubic nonlinearity is given by this integral over this submanifold of this k1 minus k2 plus k3 equals k. And then we have k1, you know, more k1 square minus more k2 square plus more k3 square equals mod k square, and times this non-linearity, which is cubic nonlinearity, are given in this form. Notice again that this awakening equation. Again, that this wave equation is not time irreversible. So, again, we have derived a time irreversible equation by taking the considering the statistical quantities from the time irreversible systems. Okay, so this is the basic physical picture of this wave turbulence theory. And this theory was first introduced roughly the 1920s. The 1920s, first in the work of Peirce in 1929, the paper discussed the weight-known equation associated with some discrete model. And since then, there has been lots of works, especially in the 1950s to 60s by the school of Zakharov, where they were considering the derivation of weighting equation for different, I mean, formal derivation for different. I mean, formal derivation for different models, including, for example, the water wave metal work of Hasselmann. And moreover, they considered also the special solutions, waving equations, which is known as the Komograph-Sykrov spectra. And moreover, they also considered the application to various subjects of physics. Okay, so there's a very nice book of Nazarenko that summarizes the previous results in the physical. Results in the physical aspect. Okay, so this is the physical aspect. Now, from the mathematical aspect, of course, the main thing is to establish a rigorous mathematical theory of wave turbulence, where the central question is a full rigorous derivation of the waving equation. And this has been open for a long time and has been open until very recent. Okay, so now the above is basically the physical. The physical aspect of the problem. And if you look at this problem from a mathematical point of view, there is also some very interesting and important questions that one can ask. So we start by looking at the non-linear Schrodinger's equation on the unit torus T to D. Of course, whether we consider the Schrodinger equation on the unit torus or torus or size L, which is differs by rescaling. So the question is, we know that for pseudo-A. question is we know that for pseudo-HS data we have global opposing this like for solutions for equations you know on on these torus but in general we don't know what is the long-time behavior of solutions like especially whether we have um you know energy energy cross k from low low frequency to high frequencies or from high frequency to low frequencies what is the what is the behavior of the hs norms and these are completely unknown so there is a famous conjecture bogan which which he asked whether there is a solution whose hs norm for s bigger than one becomes unbounded and this is completely uh completely open so important result in this in this direction of course there are many other results but this was the first one this was the result of colliander q stephanani takoka and tau where they they construct an example that show that this hsome actually can start from being start from being arbitrarily small and become arbitrarily large at finite time however one one However, one issue about their proof is that the construction, the data they construct is very specific, like it's a lacunary kind of form and it's very specific and it does not really represent the so-called most data. It's just a very specific choice of data. So it still remains a question whether or not what should we expect in terms of long-time behavior for generic initial data for non-initial Initial data for non-Nish-Schr√∂ning equation on the torus. Okay, so how is this question linked to the question of wave turbulence? And this is simple. So if you look at the wave turbulence picture, our wave turbulence picture is set up on a large box of size L, but by rescaling, you can rescale it to a unique unit torus of size one. And then after rescaling, we are reducing to this particular form of initial data. Again, they have this either. Again, they have this either k being random, independent random variables. And this Fourier coefficient here, this is a profile. So you can think this initial as a cutoff function is just restricting the Fourier modes to be in the box of size L. And then inside the box of size L, the Fur modes are kind of basically unit size of size one. And then so we can think that this particular data is like adapted to this particular Adapted to this particular profile, which is given by the square root of the n initial, is basically generic random data. This has independent firm modes, and each firm mode is independently given by independent, let's say, Gaussian. Okay, so this is this can be viewed as a kind of random generic data, which has unit size in HS1, where S is gamma over 2 minus 1, by this by this calculation. But any case, it's random generic data adapted with. Random generic data adapted with this particular profile. Okay, so now our question is: what is the generic behavior of the solution with this random data? And we have seen that this random data is nothing but the risking of the solution to the Schony equation we consider in our wave turbulence picture. And so, once we know the generic behavior of the solution to the Schony equation, which is given by the wave-PN equation, then we would also know. Equation, then we would also know the generic behavior of the solution of this strong equation of unitaurus with this particular random initial data. Okay, in particular, basically, if we know that if we have some kind of shifting energy for the solution to the wave-known equation, either we have a shift to the high frequencies or we have a shift to low frequencies, we could immediately deduce that there is a similar shift, either high to low cascade or low to high cascade for generic. For generic data, generic solutions to the non-Nishron equation of unit times. And actually, such thing is actually known that there are known blow-up solutions to the Wave PN equation, where you infinite time the solution forms a delta singularity at zero, which corresponds to some kind of condensates. And then this means that if we look at the generic data associated with the specific profile, which corresponds to Specific profile, which corresponds to that particular solution to the wave-pin equation, then we can construct the low, sorry, the high to low cascades of energy for large plasmo initial data. Of course, provided that we can justify the wave-ten equation approximation. And this is unknown. But so you see, this is naturally links the long-time behavior of non-interstruction equations to the picture of wave turbulence that we're discussing here. In the same way, we can also use this picture to describe. also use this picture to describe the behavior intermediate time behavior solution to the Schrodinger equation on RD, which we'll not talk about. Okay, so this is one side of the mathematical picture of this wave turbulence theory. So we'll be talking about the rigorous mathematical results soon, but before that, we discuss the possible scaling loss between L and alpha. Between L and alpha. So we recall that L is the size of a torus, which represents the degree of freedom of a number of wave modes. And alpha is the rescale, basically, normalized strengths of nonlinearity. So these two parameters are just like the n and r in the derivation of Boltzmann equation. And then we're considering this, you know, this scaling law when we're taking L goes to infinity and R will go to zero. Infinity and alpha goes to zero, we're considering the particular scaling laws, particular ways where we take the limit, where we assume alpha equals L to minus gamma, where gamma is something between zero and infinity. Just like in the Boltzmann equation, we have this particular Boltzmann-Grasking law, which where n equals r to the, you know, minus d plus minus d plus one. And here we consider alpha equals l to gamma, where gamma is between zero and infinity. When gamma equals zero, we mean we first take L goes infinity and fix alpha. L goes infinity and fix alpha, and then we take alpha goes zero, and gamma goes infinity is the other way. Okay, so in terms of physics, there does not seem to be a scaling law that is physically preferred, but there are natural restrictions for gamma for this scaling law, gamma, for the wave kinetic approximation to be valid. And basically, this will be a kind of technical, so I'll not explain it further, but basically, there is a restriction where gamma has to be between zero and one in this in the case of a in this in the case of any like arbitrary torus in in you know including the square torus and if you if you're willing to assume the torus has a generic you know generic like um a generic shape then you can uh improve this extend this region to gamma between zero and dimension over two okay and finally there is some there is also some particular scaling or gamma which um which has a mathematical importance which we'll be talking about later if we have time Talking about later if we have time. Okay, so this is the picture of this discussion of scaling loss. And now we start describing some of the previous works and which you before introducing our main results. Okay, so there has not been too many rigorous results, mathematically rigorous results on the wave turbulence, especially on the derivational waking equation. So I'll just list some of the some recent ones. So there is a work of Lukarina and Schwarz. Is a work of Luke Rina and Schwong in 2011 where they consider this creation equation and they consider the case at the equilibrium. Equilibrium means, for example, you have a Gibbs measure. And at this equilibrium, they are considering this evolution. Of course, here you don't have waving equation, which is kind of trivial, but they are considering these time correlations at the time zero and long time where they deduce some form of a mixing of this expectation. Okay, and then there is one. Okay, and then there is a work of Fowl where he looked at again around the weighting, the equation around the equilibrium states, which is stationary states, where he had perturbation, like small perturbations of this equilibrium state, where the perturbation vanishes in a limit, and he was able to derive the linearized waving equation, which is natural. You use linearized waving equation around the stationary solution. Around the stationary solution. Okay, so now regarding the full off-equilibrium problem, the first result was done by the work of McMaster, Jermaine, Hani, and Chattan, where they were looking at the full wavelength equation. They were able to justify the full wavelength equation, except that the time they get was very, very small compared to the Kendi time scale predicted by physicists. Predicted by physicists. Okay, and in two subsequent works, the work of myself with Hanny and at the same time work of Carlo and Germain, these works were able to derive the full wavelength equation again for the full off-equilibrium case up to the KNT time scale with the arbitrary power law. So T Knott to one minus epsilon, and for any positive epsilon, for the particular scaling. positive epsilon for the particular scaling law alpha equals L to minus one. So we'll come back to this scaling law later, but this is the one treated in these works. Okay, so now this is like well in these words we're almost going to the KNT time zero except for we're still losing a small power epsilon. Okay so we can visualize these results in this following log-log diagram. So the x-axis is The x-axis is a log L of alpha minus one, which determines the scaling law, alpha equals some power of L. So, this is determining the scaling law, and y-axis represents the time where you have up to what time you have the validity of the wave piano approximation. And so, this result under the green line is a result of in Bachmaster, Jermaine Shani, Hani, and Shatat, and the result. And the results under the blue line is the result of myself with Honey. And the results under the yellow line is a result of Colo and Jermaine, where they only consider the case of a rational torus. So this part is kind of a generic irrational torus. So I don't have this part. And finally, the region under the red line is what is conjectured by physics. Essentially, this red line here. Is essentially this red line here. So, from this red line, here is the prediction of the wavecan scale conjecture by physicists. Sorry. Okay, so let me make some remark on some of the previous, these previous results. So, first of all, in the work of myself with Hanny, we also included some partial results or skin loss that are different from the skin loss alpha equals L minus one, especially we included also some skin loss R equals L. Included also some skin loss Rv equals L minus zero. And in the much more recent work in 2020, the work of Kolo Germain, they also able to prove the existence of solutions up to this time, which is almost matched the kinetic time scale, but for more general scaling laws. However, they did not include the proof of the wave kinetic approximation. They just proved the existence, armature existence of solutions. And finally, we remarked that all these previous works. Remark that all these previous works all fail to reach exactly the KNT times here, which means that we shouldn't be getting the time t knt to the one minus epsilon. We should be getting like delta times the kinetic times t. And that would give full rigorous derivation of the wavelength equation, where delta should be independent, should be kept constant in the limit, independent of all the variables we consider here. Okay, and that is exactly the thing that we are doing in our Exactly, the thing that we are doing in our main result. And so here's our main result. We're able to justify the waving equation all the way up to including the KND time scale, and thus we're solving the full problem conjectured by phases. And we are considering in this particular scaling law, alpha equals L to minus 1. Okay, so this is the theorem. So assume the dimension bigger than 3, and assume we have some generic conditions. And this is a technical thing. And this can, this is a technical thing and can be removed provided we relax the things things a little bit. And then alpha equals L minus one. So at any time scale is alpha minus two, which is also equal to L to minus two. And then with delta being a fixed small constant for some absolute constant nu, we have this, you know, this L2 moment is exactly given by the rescale solution to the weight pin equation plus the remainder, which is uniformly goes to zero as L. Uniformly goes to zero as L goes to infinity, and of course, R goes to zero in this particular okay. Well, n is a solution wave equation. So, this is our main result. Let us make a few remarks about this result before proceeding. So, first of all, this result corresponds to this point, this green point. So, let me draw it here. This corresponds to this green point in this picture. Note that this green. picture note that this green point lies exactly on this um this segment which corresponds to the uh conjectural my physics the kinetic uh the kinetic time t kinetic equals alpha minus two okay and moreover we'll see later we can show actually that this result extends to nearby scaling laws where we have the original scaling law is alpha equals L minus one but then we can also do alpha equals L Do alpha equals L minus theta where theta theta between like the one minus one or 100 to one, okay, which corresponds to a little bit of segments, you know, in this in this red segment, which is drawn here as green segment, you know, around this point one. Okay, not yet on the boundary of conjectured admissible region, which is uh which is bounded by this uh red, uh, these red circles. Okay, so in this work, we have focused on this particular scaling law, which is physically relevant and also mathematically important, as we'll see below. But the same argument also extends to the scaling laws for gamma, gamma less than one, but close one. And more importantly, actually, if we consider this particular scaling of L minus gamma, then we don't need any generic assumption on total. So the generic assumption was kind of only a technical thing and it's thing and it's uh it's uh it can be uh removed as long as we relax a little bit not to consider gamma not equal to one but gamma less than one little bit okay and we'll be plan we're planning to uh in the in the future work to to consider the full case where gamma is between zero and one which will correspond to this whole uh segment between this point and this point which will be the full range in the case of arbitrary tokens okay uh finally we also mention recent work of stefneni and triumph Recent work called Stefani and Truan, which was posted kind of shortly after the completion of our work, where they were considering a high-dimensional discrete KDV equation. And more importantly, they are considering this KDV equation with the multiplicative noise. And under this assumption, they were also able to reach the kinetic time scale and derive the wavelength equation at this kinetic time scale. But they are also doing for a different scaling law, which is alpha equals L minus zero, where they first. Because L9 is zero. Well, they first let L goes to infinity and then let alpha go to zero. Okay. Okay. So this is the description of the results. So let us let me discuss a little bit like why do we care about this particular scaling law alpha equals L to minus one. So there is a particular mathematical point, mathematical reason for it. So especially in dimension d equals three in three dimensions. So if we consider three dimensions and recall that Dimensions and recall that before we have been doing a rescaling from the size L torus to unit size torus. And here, if we do the same rescaling, we start with the wave, the setup for the wavelength equation where we assume alpha equals L minus one. We start with this data on the size L torus, and then we try to rescale it to the size one torus. Okay, then what we get, we get this data, which is you know L to the minus one times this this random. This random again random for coefficients, what this particular thing here despires, like again a Schroder's function. Okay, so the point of this data is that it's like support in the frequency of less than size L, and then the four coefficients has size L to minus one. Think of this phi as a short function, or it says has unit size. Okay, so then basically, if you think phi is a cutoff function, let's say, you know, support it. function, let's say, you know, supported in the annulus of radius one, half, and one, then this is basically the, you know, the little opaque projection of this particular Gibbs measure initial data. So little opaque projection is clear because it's a frequency size L, let's say between L and 2L. And this Gibbs measure initial data is exactly this one. Okay, so it's the Fourier coefficient being k to the minus one and times independent, let's say, Gaussian. times independent, let's say Gaussian, Gaussian Fourier mode. So Fourier modes independent and with size k to the minus one. Oh, sorry. K to the minus one. Okay, because this corresponds to L to minus one here. Okay, and this exactly corresponds to the, I mean, exactly corresponds to the Gibbs measure, you know, the distribution of the Gibbs measure, you know, for the 3D cubic non-inch equation. And this naturally links our problem to the Gibbs measure invariance problem for the three-dimensional cubic. Problem for the three-dimensional cubic non-inshoeing equation. And in fact, we believe our results will be a, this result will be a crucial step in the, you know, if we were to prove, to attempt to prove the invariance of this Gibbs measure for this 3D qubit shoting, which will be the only remaining unsolved Gibbs measure problem, provided that the Gibbs measure invariance problem was solved in 1D by Baugain in 1994 and in 2D cubic by Baugain. And in 2D cubic by Bogey in 1996, and in 2D arbitrary higher order by myself, Andrea Namo, and Haitian Yue in 2019, which was part of the topic of Andrea's talk yesterday. Okay, so then after this, the 3D qubit is kind of the only remaining problem that is unknown, where the Gibbs measure is constructed and Gibbs measure is unknown to be invalid. Okay, so this is the Okay, so this is kind of a part of the mathematical reason why we care about this critical scaling law. So I'm very satisfied that this scaling law can actually be solved and the full mathematically rigorous picture can be established. Okay, so if you recall that in the beginning of the talk, I said that the things we're caring about is the are we're caring about is the is the distribution the density function of these of these fluoromodes at a little time and of course the first thing we're interested in is the is the you know the variances the l2 l2 moments but once we know the behavior of l2 moments we know they are evolving under the wave equation exactly and now the next thing is what what exactly can we know about distribution of these prototies and that's fortunately we can also answer so the idea is that not not only So the idea is that not only we can consider the asymptotics of these L2 moments, we can also consider L4 moments or higher moments or arbitrary mixed moments of arbitrary polynomials. And it turns out that these asymptotics can be calculated in exactly the same way as the square moment. And moreover, these moments, they contain all the information about the joint distribution of these works. And because of this, we can get a full description. get a full description of the of the asymptotic evolution of all of the of the distribution of all these frequencies and that is the following so recorded in the well-prepared initial data we have this four coefficients being the some some profile times the independence of random variables in particular we have independent fluore modes here so regarding the joint distribution you know at later time here i mean at later time it's natural It's natural to ask, of course, you know, first of all, if you look at the four and modes at linear time, like, are they also independent? They will not be independent, but are they also independent in the asymptotic limit where L goes to infinity and alpha goes to zero? And second, if we know they're independent, then what is the limit density of these individual frame modes? Like, you know, each one, what is the density? So, here, if you assume, you know, either k has a particular distribution, then what is, how does this particular distribution, how does it evolve? Distribution, how does it evolve? Okay, so the first question here: whether this independence is preserved in the limit, this usually refers to the question of propagation of chaos. And that's a word that occurs very often in physics literature. Well, physicists, they just assume that is propagation of chaos. So then, if you start independent, then at later time, you kind of remain independent, at least asymptotic in the limits. Okay, which asks whether the Okay, which asks whether the independent is uh independent modes are still independent in the evolution, and that is also answered by our results. So, this is a result with joined with, sorry, I forgot to say that all the results mentioned here are joined with Zarah Hanney from University of Michigan. So, this result that is in progress where we plan to probably be putting it in one or two months. And as a byproduct, we prove the following result which justifies the propagation of chaos, which means. Justifies the propagation of chaos, which means that, again, assume same scaling law. If this either k are Gaussian, then for any different, you know, finally different modes, k1 and kr, this joint distribution of these foreign modes approaches in law as L goes to infinity, approaches a distribution of r independent Gaussian variables, assuming the initial data is Gaussian, with variances exactly given by the evolution. given by the evolution of waving equation. That means not only the single, you know, the single of u hat pk square goes to n of the single variance of a single mode goes to the solution wave equation. The joint distribution of all any finitely many different modes also converges to the joint distribution of the Gaussian independent Gauss. Distribution of the Gaussian, of independent Gaussians. And moreover, so actually, here I didn't write the full generality. Actually, this result can be extended even if you allow R to be going to infinity, going to infinity even like a power of log L. I mean, it cannot go too fast, but R doesn't need to be fixed. R even can go to infinity like a power of sorry, power of log L. Okay. And the last, sorry, so I'll skip this slide. I'll skip this slide. So, last piece here is the case of if you assume the initial data is not Gaussian, in the previous theorem, we assume initial distribution is Gaussian. If it's not Gaussian, but if we still assume that rotation is symmetric, then we can deduce a very nice equation, evolution equation for this density, which actually turns out again to be agreeing with the prediction by physics, as in the book of Nazarenko. Okay, so these are. Okay, so these are the basic description of the results. And so, how much time do I have? You have about 13, 14 minutes. Okay, that's good. That's very good. So, now in the remaining part of lecture, I'll be briefly describing the main difficulties in the proof and the main idea and methods of proof. Okay, so the idea. Okay, so the idea, the kind of the bottom-level idea is actually very, very simple. So we just, the idea is that we're going to describe the solution because we are looking at this expectation of u hat t k square. Then, of course, the first thing is we want to describe the behavior u hat. And the way to describe it is to expand it as a finite, you know, formal Taylor series. So U0 is a linear. Series so u0 is a linear solution, u1 is um the cubic um uh like evolution, and the u n is um the n-thode evolution. And we choose the n suitably large, and we have a have a final error term that suppose we go to zero in the liquid. Okay, so in fact, um, there is an interpretation of the wave kinetic time scale predicted by a physicist that this time scale is exactly, um, is exactly the Is exactly the longest time, like on which you can have this valid expansion that converges at least form. So, for example, the TKID is the longest time scale. Well, on this time scale, you have the, you know, the higher order terms in expansion is not worse than low order terms. And there's an argument of scaling that can deduce this fact. So, this is also natural. deduce this fact so this is also natural from the from mathematical point of view that this so basically if you are kind of within within this time then this solution can be treated kind of in the so-called perturbative way okay so in particular if you assume the time you know t is less than l the minus epsilon times the kinetic time time scale which is the the assumption made in all the previous works you know which which i'll refer to as the sub-critical results then one can Subcritical results, then one can expand to a finite order n. What is n depending on epsilon? If you think of it like a n depending on epsilon, it's roughly like one over epsilon. And we can say the rest of the terms are errors because you have for each time, kind of you are gaining L to minus epsilon, for n times you are gaining, you know, l to minus epsilon n. If n is very large, then this is a very large negative power, and you can view the remaining part as an error. Okay, so then we need to estimate. Okay, so then we need to estimate this variety many expansion terms, u0, u1, un, and we need to get a sharp bound for this for this term. This is very non-trivial, but this can be done as in the work of myself with Honey and also in the work of Colonel and Germain. Both these works feature this expansion, this finite expansion, and the estimate of each of these individual terms. Okay, of course, this is the subcritical picture. Now we are looking at the critical picture. Now, t is less than delta times the Kennedy time scale, but delta is a constant. And this situation will be completely different. The most important difference here is that each term, this uk in the expansion, is better than the previous term, uk minus one, but not only by not by a power of l, but only by a you know a factor of constant delta, which means that if you want. Which means that if you want the very high order term to be error terms, then you want this u n, which of size del to the n, you want this to be very small. If you want this to be very small, then n has to be very large compared to, compared, you know, has to go to infinity with L because remember, delta is a constant independent of L. And you would like, for example, m bigger than this, log of L over log of one over delta, which goes infinity with L goes infinity. With algorithm infinity. So you need to perform like an expansion with almost infinite many terms. And why is this problematic? Basically, suppose you expand to order n and can say this term u n. Notice that each un is, you know, if you look at expansion, u0 is a linear Gaussian, u1 is a cubic Gaussian, and u n is like a Gaussian of order 2n plus 1. And if you look at the Gaussian of homogeneity 2n plus 1, and if you 2n plus 1. And if you calculate the square moment of u n, then it has a correlation between multimedia constants. And then if you look at it, you have, let me say this here. So, sorry. So, so for example, if you have expectation of GK1, GK2 bar, GKM, GKM bar, so the expectation of discussion. So, this is a sum of pi. sum of pi uh you know the characteristic function g sorry kj so the k2 prime kj equals k pi j prime um okay so just basically this what the the pi is the permutation of one two to n now here let's say m here m is very large so then the number of such pi is Large, so then the number of such pi's is m factorial, and so you see a factorial divergence here. And this factorial divergence, if you if you look at the you know the individual term, they only grow each term, you each time you only grow by factor of, you know, grow by a factor of delta, and then you just get delta to the n. However, you have a you have n factorial, so if n gets very large, this thing is not going to be convergence. Okay, and this actually, this factorial divergence actually was the main. actually the was was the main observation as the main difficulty in many of the previous mathematical and physical literatures that's uh that's how to deal with this this factorial divergence okay so how are we dealing with factorial divergence the idea is the following so note that each u n is a polynomial gaussian which contain consists of monomials like here this is a monomial okay and each two monomial has a correlation which has different terms corresponding to Which has different terms corresponding to different pairings. And again, noted there is n factorial pairings. The key observation here to avoid this factorial divergence is that only a tiny portion of these are actually large. Only a tiny portion of these terms are actually large. That means you have a sum, the worst terms, which are very large, which are outside delta to the n. Okay. And if all the n factorial terms are outside theta to the n, then you won't be able to come, this won't be able to confirm. Be able to come this won't be able to converge, however, here the point is that only the tiny portion of terms actually have this delta n size, and most of the terms actually have sizes much less than delta. Okay, so let's say the suppose the largest one of these correlations has size x. It turns out that number of correlation terms that actually has size x is MOC to the n instead of instead of n factorial. Factorial okay, and more generally, one can define index r such that for any term of index r, this the size of this term is l minus constant r times the largest possibility. And moreover, the number of terms with index r is almost c to the n times c out to a factorial. So, if you think r is zero, then number of such terms is at most c to the n, and you are good because you don't have n factorial. Because you don't have n factorial. Now, if r equals n, then you do have n factorial here. However, this is also good because you are gaining a lot of powers of L from these negative powers. And because of this, you can actually play with all these classification of terms and you say that the worst terms, they are very bad, but they are very small number of them. The better terms, there are many of them. Terms, there are many of them, but most of them are very good. So, in the end, there are still some of them. Okay, and this is the way, which is the main idea of this proof, how we get rid of this, how we get rid of this factorial divergence. Okay, so I think maybe I have five more minutes. So, I'll just describe a little bit the picture here. So, how does one design such an art? The idea is. Such an R, the idea is that we'll be looking at the structure of such monomials and such correlation. So basically, the monomials, if you look at it, it's a cubic equation. So if you look at it, it's just like dt minus i Laplace of U equals U square U. And so, okay, so U equals linear solution, okay, plus cubic. And cubic here can be represented by one ternary tree of this form. Okay, so you have this output, you have three inputs here, and let's say you have also a quintic, which means, for example, you have this ternary tree, you have three inputs being linear, linear, and this one is a cubic input. So this corresponds to this tree. So basically, you can order, you know, arrange all the terms on the monomials in terms of ternary trees. And moreover, if you look at the correlations of monomials, You look at the correlations of monomials, then you have the pairings between leaf of the trees because the leaf of trees corresponds to initial data, which is Gaussian. And then, if you take the correlation of Gaussian, you get these pairings. And then we are getting, by looking at correlations, we are getting these pairings of ternary trees. I'm drawing an example here. Here we have a pair between red and red, green and green, blue and blue, and so on. And here, these such structures, we are calling them couples. And then, so if you look at the And then, so if you look at the correlation, then the correlation is the sum over all the couples, and now classification of monomials, classification of terms are exactly corresponding to classification of couples. Okay. And it turns out that we can define the so-called special class of couples, which are so-called regular couples. I'm not writing down here, but we're calling irregular couples. Which are the couples where the corresponding term has a worst behavior, has the largest size. And then we can show that the number of regular couples is very small. It's only exponentially growing, not factorially growing. Okay? And finally, we can basically find our R, which is kind of distance of a given particle to the set of, you know, regular, sorry, is a distance of a given. So, is a distance of a given couple to the set of regular couple. So, basically, if you deviate, if you are having exactly regular couple, then you have the worst case scenario, you have the largest size of term, but there are only very few of them. If you deviate a little bit from the regular couples, then you are having a little bit better gain in terms of powers, but then you have a little bit more, you know, number of a little bit larger number of such couples. If you deviate more, you keep going on, you gain more and you have a larger number. More and have a larger number, and then so on. Okay, essentially, I'm not talking about the details, and I don't have much time, but the idea is that we are going to, you know, these saturated or regular couples can be constructed by specific operations, and we define the inverse of these operations, and we keep applying the inverse of these operations until we find something which cannot be. cannot be further reduced and we call this the size remaining couple R and this R is basically something that would work. So this is the last two slides. So it turns out that we can show the number of couples with index R is the most C R. So the factorial time is C to N. And then we need finally we need to show that for these couples of index. Of index R, we indeed can gain a power of L the minus C R. Okay, and this, how to do this idea is that in order to calculate this, calculate the size of the whole expression, we can actually reduce it to a so-called counting problem, which is associated with this ternary tree and couple structure. Okay, so I'll not explain what is the counting problem, but it's just, you know, if you have It's just, you know, if you have for each node, you have a vector here. For paired nodes, you have the same vector. And then for each node, K, L, E, N. So K is a node and L, E, M are its three children. Then we will have a system of linear equations, K equals L minus E plus M, and have a linear quadratic equation, you know, mod K square equals mod L square minus mod E square plus mod M square, which comes from the resonance structure of the Schaune equation. Okay. Okay, and for example, the counting problem associated with this tuple will be this, okay, which is a contains a bunch of linear equations and a bunch of quadratic equations. And the last piece of proof is that we're going to find a way to estimate the number of solutions to this particular system of particular. Of particular diaphragm system. And it turns out that we need to introduce a new structure, like derived from the couples. We need to introduce a new structure, which we call molecules, which is kind of a more flexible structure than couples. And we need to derive from the couple, derive the molecules from couples, and then we do a counting problem associated to molecules. And with these molecules, we're going to design a specific algorithm to deal with the counting problem so that we can To deal with the counting problem so that we can get the optimal upper bound for the number of solutions to the counting problem, which would allow us to estimate the size of this term associated with this particular cobble. And it turns out if this cobble is far away from the regular cobble, then we can gain a lot of power of L, which allows us to cover this logarithm, so this factorial divergence. Okay, and so this example. Okay, and so this example of a molecule which corresponds to the which is really just a directly graph which corresponds to the couple we are we're having in this in this previous slide. Okay, and this turns out to this molecule and for this molecule we can apply a carefully designed algorithm to reduce this molecule to reduce this molecule and to solve the counting, solve the upper bound of the counting problem in the optimal way. Okay, so then basically this is the main idea. Basically, this is the main ideas of the proof. So, okay, so I think I'll stop here. Thank you.