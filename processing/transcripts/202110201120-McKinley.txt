Can't go on that hike this afternoon as much as I'd like to. So, yeah, so I'm going to take a more methodological point of view on what Christine was talking about. And actually, this is also going to address some collaboration I have with Will Hancock, who is out there in the audience. And, you know, sort of my gravitation as an applied mathematician towards statistics and Bayesian statistics in particular. In particular, and most importantly, about how to use these Bayesian methods to really put error bars or express uncertainty in model predictions. So I have this slide at the beginning of all my talks. What I like to do is meet people who have cool tracking of different systems. Often they're under two different treatments. So, you know, this is these are pegylated beads, 500 nanometers in human mucus, one healthy, one unhealthy, and the upper. Unhealthy. And the upper right, these are herpes virus virions moving one in the presence of antibodies in human mucus, and then one not in the presence of antibodies. And it shows a function that antibodies have, which seems to be more relevant now that we find out that COVID, that the vaccine, the Pfizer vaccine expresses antibodies in the mitosal layer in the nose, which is interesting. I'm going to talk a little bit about molecular motors, which are down here on the right. Molecular motors, which are down here on the right, and then down here in the lower left. Sometimes I do ecology as well. This is a jackal on two different days. One day, uh, is just I guess a regular day, he wanders to a watering hole. The other one, a zebra carcass died, a zebra died nearby, and you went to go visit it, and that affected it. So, I'm always interested in when you see differences in these stochastic processes, you know, can you find underlying mechanisms? And it's always with an eye towards the following issue, which is the sort of The following issue, which is the sort of mathematical modeling manifesto. We have access to videos that track individuals on the order of seconds or minutes, except for the ecology, which is longer. But the biological processes of interest take place on the scale of hours or day. Often multiple stochastic models can emulate a given behavior, but they might make qualitatively different predictions, and that's the key. So, you know, you need subtle statistical traces that differentiate between different models. Then you want. Between different models, then you want to project to these longer time scales. And what I've been thinking the most about in the last several years is: okay, I'm projecting, but then I also want to express my uncertainty in that projection. So the one system, and this preceded my collaboration with Christine was with Will and Pete Kramer and John Fricks. And this was all about seeking codependence among. Seeking codependence among antagonistic motors, Will had noticed this pattern that, in general, if you look at cells and there's kinesins want to go one way, dyneines want to take things the other way, you would think that if you knock out the dyneines, then everything would go in the kinesin direction, but it doesn't. And conversely, if you knock out the kinesins, not everything goes in the dynein direction. And so, you know, our grant that we got funded was sort of around. That we got funded was sort of around trying to find this phenomenon in vitro through his experimental setups to sort of uncover the mechanism, potential mechanisms for codependence that they need each other even though they go in opposite directions. And for this particular talk, the experiment of interest is in terms of he can describe it better than I can, you know, using a DNA scaffold. You know, using a DNA scaffold, he can control directly whether there are one or two motors attached to a given cargo. So you can have, we have a data set that just has one kinesin attached to the cargo. We have a data set where there's one DDB, that's dynein dinactin big D2. Turns out dynein by itself is kind of, we were, you know, you know, capping it at the knees. It wasn't fully effective, but if you do. It wasn't fully effective, but if you do, if you have dynen, dinactin, and big D2 together, they're much more effective in transport. So, kinesin, you know, we have a kinesin data set, a DDB data set, and then a data set that has one kinesin and one DDB. Look at them. So, this particular track here is a kinesin starts at the green, goes to the red, projects very well onto a microtubule. This is the progression with time. And kinesin, it's very stereotype-steady walking behavior. Steady walking behavior. When you have Kines and DDB, you expect to see these tug-of-war phenomena. And sure enough, here it starts in the upper left, it moves along. There's a little bit of a pause at a certain point, and then it moves again, and then there's a pause at the end. When you project onto the length of the microtubule, you can see this progression. And I should say that all of this analysis was in the dissertation work of Melanie Jensen, who is a PhD student, graduated. Jensen, who is a PhD student, graduated in 2019, and she works for Slumberger now. And then this is what Christine was just describing. And I think it was Bill who had his video of things flying around inside of his cell. I said, this is catnip for a mathematical model. And so I have to have that slide too. You know, when I see something like this, I'm just like, wow, I have to understand everything that's going on there. And as Christine mentioned, so the nucleus is here. So the nucleus is here, and there is perhaps one type of movement. And then on the periphery, there's perhaps another type of movement. And this collaboration had the individuals that Christine described before. So Keisha is the lead analyst on this particular project. And this is the picture that Christine showed y'all, where again, we have these processes. So green is the start, red is the end. Red is the end, and we want to decompose them by some kind of change point algorithm to study them. And I want to emphasize at the very beginning that, in contrast to like hidden Markov models, where we might define a certain number of states, you know, this is the active fast state, this is the medium state, this is the slow state, and then conduct inference on the number of states. We wanted to take an approach that was sort of agnostic as to whether there were any states at all and just understand things from a And just understand things from a distributional point of view. And so the change point algorithms that you see do inference without any reference to underlying states. Anytime I talk about states, stationary and motile, and stationary does not mean the mathematical sense of stationary at all. It just means not moving much. That is something that is descriptive that we just put on after the fact. Okay, so we're not doing inference looking for those states. That's just purely obscriptive. That's just purely obscriptive, helps us understand the data. Okay. So I have to give credit to what was an inspirational talk that I saw. So it was we, Will and Pete and John and I, we used to have these May motor meetings up at Penn State. Veronica, I think, went, this is how I met Melissa Rolls and Veronica. We got together there. And there was a talk that was given by Sandra and Colada, which I thought was awesome on this. Which I thought was awesome on this stuff called she made a chemo analyzer. And the idea was that if you have a chymograph, or I guess it's a chymo, I don't know if, I never know if it's chemo or chymo, but anyway, what you can do is you can trace out a path, and then the user just clicks in certain places and says, here are my segments. And then the chemo analyzer takes all this and then moves it into, you know, and does it. And then moves it into, you know, and does an automated analysis that creates a distribution of segmental velocities. And I thought that was very cool. And then also run lengths and anything else that you typically measure in bi-directional transport. And what I love about this is that the automation, first of all, allows for comparison of large populations. And I also love that it addresses bi-directional transport, but the path properties are not described in terms of mean squared displacement. Describe in terms of mean squared displacement. And, you know, for those who've known me for a long time, I've spent most of my career more or less fighting against the use of mean square displacement as anything other, I mean, I think it's a useful tool to like say, okay, what's going on here? You know, is there a difference? It's a great summary statistic, but it's extremely noisy, extremely sensitive to all kinds of things that are hard to control. And also, multiple stochastic models can produce the same MSD. Models can produce the same MSD shape. So, I live most of my mathematical life down here dealing with sub diffusion and the different models that can produce subdiffusion and trying to understand which one of these may account for what you're seeing in a particular data set. Obviously, Brownian motion creates linear MSD, but has been mentioned several times in this conference. If you switch between active and anchored diffusion and you have enough paths and by the ergodic theorem, that will end up being a linear image. End up being a linear MSD as well. And as is relevant for us, if you have this switching between active and anchored diffusion, there are a lot of parameters going on. And if you see a drop in MSD, there could be lots of reasons why. And so, you know, one of the things that I like about this idea to not, you know, use the MSD to get a feel for where we are, but then do your diagnosis with a finer tip tool. That's where we want to make our contribution. Where we want to make our contribution. And so, given Sandra's talk, you know, and thinking about what she had done, I saw that there were several opportunities to make methodological contributions. So one, okay, that automation is great for large populations, but that still required a human hand to go click the path. And so we wanted to introduce a method that would not require the human hand halfway through. We wanted to automate the segmentation and then also. And then, also, when you have these comparisons, say, of speed distributions, well, if you see two speed distributions and they look different, what is a rigorous way to sort of put a number or even a p-value on, yes, these are different or no, these, you know, these are, you know, these could come from, you know, we don't have enough data here to say that they're from different underlying distributions. And then finally, the last opportunity is once we produce this way. Once we produce this whatever automated method, and we want to put it into our models, because I am ultimately an applied mathematician, how does the errors and the uncertainty and the inference get propagated to uncertainty and predictions that you make? So when we look at these two groups, if I want to contrast what we did, there were two, there were some significant differences. So in the work with Will that was spearheaded by Melanie. Work with Will that was spearheaded by Melanie. We developed an in-house change point algorithm. She did a full Bayesian development of this. We designed it, the prior distributions and everything specifically for motor transport data. The paths need to be projected onto a 1D line, which is a problem. We didn't realize when we were doing Will's data, which did project very well onto 1D. We didn't realize that we made assumptions that wouldn't translate to 2D very well. Translate to 2D very well. And the other problem is that it requires high-performance computing because it's so computationally intensive, you just really can't run it on a laptop. The work associated with that is in press right now, mathematical sciences, biosciences engineering. By contrast, what we did, oh, okay, and then by contrast, the work that is spearheaded by Keisha, we relied on a change point algorithm that is available in an R package. Melanie. Package. Melanie and her work had done a survey of multiple different types, and BCP is the one that won, finished second place to us in terms of handling motor stuff. And, you know, I mean, we only won by like 5%. So we're not breaking the world or anything like that with our Changepoint stuff. But anyway, so it's a Bayesian, but the other nice feature is it is Bayesian. The problem is that BCD is not designed with this application in mind, and the structure of the likelihood function. Of the likelihood function actually discourages finding certain very common types of motor paths. It does do genuine 2D inference and it can run on a laptop. So that's the sort of difference. So Melanie's stuff was in-house change point and fully Bayesian and Keisha's is something you can run on your laptop. All right, and to give you an idea about what these likelihoods or what you're dealing with when you're Likelihoods, or what you're dealing with when you think about this. Imagine a path that looks like this. So, this is a simulated path. Here's time and position. It moves, it slows down, and it moves again. And suppose that you assume that there's only one change point, okay? Then when you run your statistics and you say, okay, where is this change point likely to be? Well, you want your posterior distribution, your possibilities for where it to be. Well, it could be here or it could be here. Neither one of them are going to do a great job. Neither one of them are going to do a great job, but they're going to be, let's say, equally bad locations. And so we have this violin plot that shows the posterior distribution for this path. Pretty much there's a 50-50 chance that it's right here at the first logical place or right here at the second logical place. So this is the kind of thing that you see when you have an underspecified number of change points, you get a bimodal posterior distribution. It could be here, it could be here. They explain it equally. Be here, they explain it equally poorly. When you allow that same trajectory to have two change points, then and I apologize, I don't know what this other dashed line is here, but this is a heat map of the posterior distribution. And what happens is that instead of being bimodal, you have this one high peak. So you see this red region here. What this is saying is that, well, it looks like the first one's around time four, the second's around time eight, which is correct. Around time eight, which is correct. Note though, that if you put the time of the second change, yeah, this should say time of second change point, sorry. If you fix the time of the second change point to be this spot four, notice that you have this sort of horizontal thing going on with this little trough. What that's saying is that if I force the second one to be here, then any spot I choose in here. Any spot I choose in here for the first change point is equally bad at picking the data. And so the likelihood landscape is very bumpy, is what I want to say. And so it's not easy to explore. And there's a lot of thought that you have to put into how to explore this, how to jump up, you know, as Daniel Coons was saying the other day, you know, yesterday, you know, from Steve Frisset's work, you know, like you go up and try it out at a certain number of training, you know. Number of change points, you get something, you compare to how it works less, and everything like that. So, I don't want to go into all the details, but I just want to show that this is kind of what you're up against when you build something like this. Okay, so in the end, there are some commonalities and some differences in what we did. So, one thing in both cases is that we introduced, we did it, we broke the analysis in half. And in the first step, what we did was, because it was no matter what, there were some computational. There were some computational challenges that we wanted to stay within certain bounds on. So, in both cases, what we did is in the first step, we took sort of some approximation to the marginal posterior distribution of the number of changes. So, you know, you take whatever our algorithm is, it spits out some distribution. I have this probability of two change points, this probability of three change points, this probability of four. We say, okay, two had the highest probability. So we're going to. Highest probability. So we're going to take that as the truth. So in the first step, we pick for each path the assigned number of switches. And then the second step is conditioned on that, that assigned number of switches, we extract some path properties. So in Melanie's case, in the Kin1 DDB analysis, the output was for every path, there is an associated posterior distribution. Is an associated posterior distribution for the locations of the changes and the velocities. But in Keisha's stuff, the stuff with Christine, for each path, we said there's one representation. And so to stress the difference, you know, this is what it looks like. So the output in this other case, this fully Bayesian thing says that, though, there are probably change points in these locations. These are the associated velocities. But notice there's more uncertainty associated with those velocities. Uncertainty associated with those velocities given the little changes here. And there's a high, there's a joints, you know, distribution. It's very complex here. In the lysosomal transport analysis, you know, the laptop version, each path gets one representative piecewise linear. So this is time versus x-axis, time versus y-axis. And then in the end, it really just fits out this table. And then that's it for that path. Okay. And it's interesting to think about in these two different cases, and I guess where. In these two different cases, and I guess where I'm stuck right now as a methodologist is thinking about when I want to feed these through a mathematical model, propagate uncertainty, where do I have gains and losses associated with taking these two points of view? Let me show you just some overall results that we got, you know, the main, you know, the headlines, I guess, from the experiments. So for the Kin1 DDB, Kin1 DDB stuff, the main thing to notice is that the number of change points for Kinesin is smaller per path. There's typically no changes at all. The speeds are very stereotyped. So there's this, this is a kernel density estimation that comes from her analysis. These little dots at the bottom, I'll only explain if I have to later. And then, but for Dr. But for dyne, which is not as stereotyped in its behavior, you can see first of all that there are more changes per path. And then also the speed distribution is more spread out. And you see bumps in these kernels. And one important thing to note is that there were not enough paths, and we didn't have enough strength in our statistical algorithm to really make a strong claim that there was bimodality. But when you see KIN1 DDB together, When you see KIN1 DDB together, you see this trimodal thing, which again, we can't validate it and say that it really is. But there are definitely more changes. And we see these differences in the speed distribution, and we see possibility for multimodality. So there are good reasons why I don't do a direct comparison between these speed distributions, which will become apparent in just a little bit. Become apparent in just a little bit. But from a biology point of view, we got kind of the result that we expected. Kinesin, steady and taking its regular steps and doing its job. DDB, a little bit more varied in its behavior. And then Kin1 DDB, lots of pauses. Actually, it stays on the microtubules for way longer. There were lots of long, slow segments and things like all kinds of different little behaviors showed up with little moments of faster transport. Transport. Now, taking this idea of speed distribution and moving it over to the lysosome transport, here we actually, for reasons I'll get into later, we actually can do a comparison among different groups. And so on the left, you see a comparison of, so this is the speed distribution while motile for lysosomes in the periphery, which is blue, or the perinuclear region, which is black, and small versus large. And small versus large. And just looking at these kernel density estimations, you see that these look kind of the same and these look a little bit different, but there's nothing, you know, it's hard to say whether they are genuinely different or not. In order to actually compare distributions, what we did is we took a bootstrapped approach. So the idea was, okay, so number one, that we had way more observations in the periphery than we did in the perinuclear region. So there are two sources of potential noise. are two sources of potential noise between the two these are empirical cumulative distribution function curves so we're going from zero up to one every value is the probability that you know you're less than that this is a much more stable object than the densities and so when you see a difference it might be because of genuine difference or it might be because of sampling differences and so what you see in this blue cloud here is you take the the the Is you take the set that has more points in it, you sub-sample it. So there were 1,200 segments that were in the blue group. You sub-sample 209 of them, and for that sub-sample, create an ECDF. You do that a thousand times, and it shows you all the possible other things that could have come up just by sub-sampling, but just by difference in size. And then when you see that the black curve falls inside of. That the black curve falls inside of that, you say, Oh, well, the difference I see between black and blue could be just purely because of the sample size difference. By contrast, it doesn't look it, but there is a statistically significant difference between small and large, which is what we expect. A little, you know, there's that minor change between the two. For those who want to know the technical details, we use the Komogorov spear-off distance between distributions. And use that for our bridge drive. Okay. So that told us that size matters. You know, we've said many times a little bit for speed, but not a lot. And then there's no regional difference in speed. But here's our friend mean squared displacement. If you look at the mean squared displacement for, and this is noisy for reasons I won't get into, but if you look at the mean squared displacement in the periphery, it's moving much faster. It's moving much faster than the mean squared displacement in the perinuclear region. And the fact that, and then this is throughout the cell, the fact that the red looks a lot more like the green is an artifact that we had way more, we had more samples from the periphery than we did from the perinuclear region. So if they're not moving faster or slower in one region or another, then there must be some other property of the switching that is accounting for this loss of overall transport. For this loss of overall transport. And so, to get at that, this is where the descriptive model comes in. So, what we did was we, you know, we bend. So, all the inference is already done with all those segments. We took all those segment speeds and times, and we labeled them as stationary or motile, and then we put a little Markov chain model on top of it. And, you know, so I'm emphasizing this as post hoc. So, for example, in this, in this In this path on the right, there's a stationary segment. Notice that stationary still has some movement. And then three active ones. Sorry, it doesn't match our little cartoon over here. But I want to emphasize that usually when you have a continuous time Markov chain model, you can't go from a state to itself. When you do this kind of binning, you have to allow that though, because what can happen is that you can go from Go from in one direction and then move in a different direction, have consecutive active going in two different directions, your speed can even be the same, but then have two different active periods. And so the model has to account for that somehow. And so the kinds of parameters that we want to infer are, you know, the switch rates while we're in these states, the probabilities of transitioning from one to the other, these kinds of things. The challenge, notice that in every path, just intrinsically, if you walk in and it's Intrinsically, if you walk in and it's stationary, you don't know how long it's been stationary. If you leave and it's stationary, you don't know how long it's going to continue to do that. So you have a partially observed window of the states themselves. They're truncated. Also, but the good news is that for this particular thing, the likelihood is explicit and the Bayesian inference is straightforward. So this is nowhere near the level of difficulty model that Ruth Baker was talking about a couple of days ago. And so through a logic that I'll And so, through a logic that I'll show in just a minute, you can look at things like the proportion of time spent active, which is the specific example that I'll lay out. And then we can look at, for example, the average stationary period. You know, like a period is like if I have consecutive of these, I call this one motile period. And what you should note is that most of our paths are 30 seconds long. Are 30 seconds long at the longest, but the average stationary period we infer to be twice or three times that long sometimes. So, in some sense, we're just really not seeing all the inactivity. And note that we did, just as a sanity check, we had a way of doing this with just a direct estimate using traditional techniques. You get a plus or minus there. And then we put our Bayesian method up there to put a credible region. know to put a credible region which i'll describe in a moment so i just heard a beep am i where am i in time because i don't know when i started actually you have 13 minutes so if you want to leave 10 for questions you have a few minutes left or you can go a little over i'll take i'll take five more i'll compromise um okay so uh so anyway so this is so i'm going to talk about those credible regions in the last five minutes so just to kind of you know consolidate where we're at you know using Where we're at, you know, using these methods, you know, for the KIN1 DDB, you know, we saw it gave rise to qualitatively different speed distributions. When it comes to the lysosomal transport, there were different regional differences because they cause more the nucleus, which we talked about. And, you know, to some degree, you know, some of our goals have been met because we're reporting predictions with uncertainty. That's the credible regions that I'll talk about in a moment. And the speed distributions are reported with confidence bands. Okay. Reported with confidence bands. Okay, so everything sounds great. So now I have to like enter the confessional to like talk about my sins in terms of propagating my uncertainty. Okay, so if I'm looking at these, you know, this credible region, this number, you know, 0.059, you know, and it could be on either side, you know, our prediction. So the way that a credible region like that is generated is that you have to first create a model for the data. Model for the data, which depends on some parameters. You have to articulate a likelihood function for the experimental observation. So, use the model once to say how likely is it that I would see this data given this model. And then I have whatever my prediction is. And so I need to analyze the model to see what the prediction is as a function of the parameters. If these are my parameters, then this is my model prediction. And usually these are deterministic models often, or we're taking averages, which Or we're taking averages, which then become deterministic. And so it is a map directly from the parameter space into some other, you know, some output. We generate parameter samples from their posterior distribution, that's the uncertainty and the inference, and then we pump those through the model to get the posterior for our uncertainty and the prediction. So, you know, usually when people think about, you know, or the way that I've sort of, I think about Bayesian, I think about Bayesian inference now is not necessarily in terms of formulas, but in terms of sampling. Prior to knowing anything, if I have, say, a stationary to motile rate and a motile to stationary rate, let's just say, I don't know anything. So if I were to sample from I don't know anything, it would be maybe uniform over this region. Okay. And then that's not technically true what I said, but let's just go with it. So, you know, originally my uniform lack of understanding. My uniform lack of understanding is this sort of cloud of dots. Then, after I take the data, put it through my likelihood, it bunches together in some region, and my posterior distribution is like a little group. They're sort of grouped together, and they may have interesting shapes and all kinds of stuff that we saw. Now, when I want to think about like how do I pump this through a model, so you know, you have like a, you know, I have two parameters and you know, I don't, I'm a little short on. And I'm a little short on time, but so I'll have some state process. I can define all this in terms of continuous time Markov chains. I have a process that's either zero or one. My proportion of time spin active is the average of J over a long period of time. I have a nice, pretty formula. And then I have, you know, this nice heat map that shows, you know, if this is my stationary to motile rate, I spend, you know, in contrast to my motile to stationary rate, you know, I'll have a certain proportion of time I hear a lot. Proportion of time. Here, a lot active, here, not so much active. And so, how do I take this thing and then pump it through the other? Well, I can overlay that prediction. And my uniform misunderstanding previously led to if you take all these points, you take the value of your function and average them, maybe it's 0.5 before, and then I get values that range all the way from 0.1 to 0.9. When they concentrate, now my estimate is like 0.1. Now, my estimate is like 0.1, and then my credible region is 95%. Where do I go on either the middle, 95% of that distribution? So that's really a nice, like to me, that's a beautiful, you know, when I learn this, you know, as an applied mathematician, this appeals to me quite a bit. But it raises the question of what do we mean by data? Okay, whenever we're saying we go from Bayes to this other one, what's the data? other one what's the data in the laptop protocol the thing that you feed into your model is one piecewise linear approximation per path okay when you have that high performance computing protocol and every path has its own posterior distributions that set of posterior distributions is the data that i would feed into my model and if we want to think literally about this example that i've given about the posterior distributions for the parameters About the posterior distributions for the parameters, the main thing that I want you to take away is: you know, okay, posteriors are these famous, you know, the gamma distribution, the beta distribution. The things that they depend on, one of them, sigma and tau, is the total time spent stationary across by entire population of paths. Well, that makes a lot of sense if I have one representation per path. But if I have many, many, many representations per path, how do I try? How do I translate that into a quantity like total time spent stationary across all paths? Do I average them? Do I average before or after this step or the other step? There are actually a lot of different choices that you could make and the implications of each of those choices aren't clear. So the laptop method allows for very direct translation of uncertainty to the predictions, but the more fully realized Bayesian method introduces a lot of ambiguities. Introduces a lot of ambiguities. And then, as far as this comes to bear on these velocities, these speed distributions as well, again, when I'm doing the laptop, one representation per path, the sample size is appropriate. These comparisons are highly, the number of width of this blue thing here is highly dependent on the sample size. And so, for these things, the sample size appears to be huge because I have thousands of different possible velocities. Thousands of different possible velocities for each of the paths. But the point is, I'm actually resampling the same path over and over again. So, if I were to try to compare these using this method, they would artificially exclude each other because it sort of overrepresents the data. So, you know, that's my issue right now. So, what we want to do is sort of upgrade our in-house algorithm to 2D because that's, you know, we want to sort of have some. That's, you know, we want to, you know, sort of have something built for motors for this stuff. But the tension remains between a fully Bayesian approach and the computational burden associated with it. And then, you know, for all this work that we want to do to have a more robust approach, the benefit, by the way, of having these posterior distributions is, you know, you could say in the laptop version, well, what if you happen to pick a bad match? You know, like, what if you just disagree with the algorithm? In the fully Bayesian approach, you could say, well, that's just one of all of these. And you can kind of All of these, and you can kind of see which things are more robust and which ones aren't, but we don't know how to propagate stuff forward. So that's sort of where we are with the inference stuff. In the big picture, Keisha's working at a project where we want to build more authentic paths to understand this frame rate issue. Everything you saw here was at 20 hertz, and we want to use models to interpolate between different frame rates. And then, you know, the last bit, I'm over, but the last bit of just shameless advertising has to do with. Advertising is has to do with Riley Juneman's undergraduate work. So she won, not best undergraduate ever, but best undergraduate last year for a research project where what she did, she's a math and CS major, and we built what we called a dashboard of first path statistics for path categorization. The idea is to take a census across the cell. You feed in a path. She trained, you know, through machine learning. We had six statistics that we put as part of the dashboard. As part of the dashboard, you reduce every path to like a six-point, a six-dimensional point in space. She used a support vector machine to learn on that space and to categorize them. And the paper is not published. We're still working on that, but the app is up, although it's due for an upgrade in about a month. And anyway, it's a fun thing to play with if you have a little bit of time this afternoon. So I'll stop there. Thanks, everybody. And I guess, I don't know, are there any questions? I don't know. Are there any questions? First of all, thanks so much, Scott. Any questions from Zoom participants? If not, we can move to in person. I think Jay has a hand. Yeah. Hi, Scott. Hi, Scott. Hi, Jay. I knew you'd have a question for me. Back around in the middle of your presentation, you were talking about how you were sort of embedding a continuous time markup chain. But yeah, you said you had to include self-transitions, and I didn't get that. Something to do with What something to do with the change point formalism you're using that you can actually see there's what's distinct about the transition between mobile to mobile because it doesn't these two the right and left don't don't correspond right oh yeah the right and left don't correspond this is a uh and i apologize i didn't it's one of those things you make a talk and you're like oh wait i don't have a path i'm glad you said something A path. I'm glad you said something because otherwise I would have been like, wait, please don't. Yeah, I should put a little asterisk. So, I mean, look at so this is a stationary, motile, motile, motile path. So the stationary phase is in the beginning here, okay? And then it takes off in this direction to the upper right. I hope you can see my cursor. Yeah, yeah. And then it pivots and it goes right. Presumably, maybe it went to a different microtubule. Presumably, who knows? Different microtubule, presumably who knows what? Like, we don't have that kind of information. And then it turns again and comes back here. So it almost looks like a reversal, but it's, you know, it's shifted. I mean, maybe the microtubule got bumped. I don't know. It's because you wanted to be agnostic about the velocity that's chosen in the motile state. So you have different velocities, but you're not assuming anything about how those are chosen. Correct. Okay. Correct, okay, yeah, and and that's why I want this thing to be robust with respect to different thresholds. Like, if it really, if the if the result depended on 180 meters per second, then that's that's not a good, that's just an artifact of the way that we split it. Um, the fact that it survived so strongly with other choices of the threshold, and I didn't show it, but these, this, this, um, the speed distribution is and the way that relates to duration of the segments is extremely smooth. I mean, it does. Is extremely smooth. I mean, it does not look, it is not multimodal. It's the main thing, that's one thing that really jumps out about the live cells. So it's a strange thing to do, but we also really wanted a way to say, hey, sometimes it's moving a lot, sometimes it's not. And we wanted to formalize that. Yeah. Hi, Scott. It's Bill Holmes. I got a quick question. So I was wondering, so two things. I was wondering if you could expand a little bit more. I was wondering if you could expand a little bit more on something I didn't quite understand. To what extent, the first question is: to what extent is the estimation you're doing single path dependent? That is one path, one set of parameters, another path, another set of parameters, versus sharing information. And the second question, which is kind of a tag along with that, is: is there, would a hierarchical approach where you, well, kind of get a bit of the best. Well, kind of get a bit of the best of both worlds, single-path information with a little bit of shared information. Is there any value in that? Yes. So, to answer your first question, so when you say shared information, so what we do is it's easiest to actually, you can see it in this picture right here. So, we're not necessarily, so for the change point algorithm, we're not necessarily inferring parameters. Parameters, so much as what we're saying is that this thing spent this amount of time going on average this fast. And so if there are, say, heterogeneous, well, I mean, it's actually the result that we have in some sense. If there are heterogeneities across different regions, there's no crosstalk among those inferences. You can think of this as like, so there are these non-parametric techniques. Non-parametric techniques. And if I wanted to do this officially for this velocity distribution, and you know, and we'll, yeah. So I'm not, I'm learning about the non-parametric techniques, but you could imagine that you're feeding these, you know, into some huge, you know, velocity distribution, and you were trying to infer the shape of that. But the key is, and this is the thing that I want to stress, is that most methods that we have encountered. That we have encountered, that we try, usually impose some kind of structure on the velocity distribution ahead of the game. And what I want to stress in what we observed about these distributions is that different motor types will have different shape of speed distributions. And we want something that's flexible to all of that. So that's why we take path by path, just looking at this path, what is the apparent velocity? Looking at this path, what is the apparent velocity? And then I can have my uncertainty about that path. That's what's with Melanie's. And then we pull them over together and analyze them later. Did that address your question, the first one? Yeah. Okay. And then what was this? So what was the second one again? Well, I mean, so there are still parameters floating around in here. For example, speeds, number of, number of switches, and I'm wondering if there would be value. I'm wondering if there would be value in doing the estimation from a hierarchical perspective. So, my response to that is probably. I need to get smarter about it. So, I've tried several times to develop these fully hierarchical models. And what I run up against is: number one, they're computationally expensive. They're computationally expensive, but they also have a tendency to not explore the parameter space as much as they need to. So, even using something like Stan and Hamiltonian, Monte Carlo, which I've tried to start building that into what we do, I keep running into this problem that when I build these sort of super umbrella models, somehow the exploration of the posterior space is just impoverished compared to this, which kind of delivers. This, which kind of delivers, you know, something that is more representative of what's in the data. So I think there's a better way to do it, and I would like to do that, but I have not been successful in that. We should probably move on to the lightning talks, I think. Yeah, thank you, Scott, again. And thank you, both Christine and Scott, for the nice session. All right, so we're ready to move to the All right, so we're ready to move to the lightning talks, which will be at most two, at most three minutes for each person. And I don't know if you all decided how the in-person, we have two in-person people that are starting. Have we decided how we're dealing with that? I think the easiest thing to do at this point is just to do it the way we've been doing it. Yeah. I'm not trying to just do R2. Cool. All right. So in that case, Right. So, in that case, so hopefully, people can stick around to learn a little bit about these participants' research. And first up, we have Yonatan Ashanafi. Whenever you're ready. We do have a problem that we don't actually have an HDMI because the HDMI that would have been going to the computers is now going to this. So if we take it out to plug them in, we're going to lose the screen. Why don't we move the in-person people to the end and have them go via screen from the lobby or something? Yeah, we'll do the two in-person at the very end. At the end? Okay, sounds good. In that case, Arthur Molinas, are you on Zoom? Yes, I'm here. Should I? Yeah, go ahead. Okay. You can see that PowerPoint. Yes. Okay. So it's not full screen, but up to you. That's very true. Excellent. Go ahead. Hi, guys. I'm happy to be here and to tell you a little bit about me. I had no idea what the stuff. Just trying to myself and why I'm here and what I care about. So, I'm a cell biologist and mostly an experimentalist, and I care about the cytoskeleton, its organization, its dynamics, and its role in cell mechanics, but also about cytoplasm biophysics. And here on the left of the slide is just a representation of what I think about the cytoplasm might look like. It's a complex and dynamic environment, and it contains most of the metabolic processes. And so, the real question that I And so, the real question that I'm trying to go after is: are those properties of the cytoplasm, how crowded, how viscous, how dense it is, affecting biological processes? And can we understand how and why? And so the first project that we underwent to get to those questions was looking at the microtubule dynamics, because it's a process that is well known and well characterized, and trying to see if we could have dynamics by when we cytoplasm concentration. Cytoplasm concentration, and in yeast, we did that using hyperosmotic shocks, introducing cytoplasm concentration, and we saw that it slows down microtubnamics. And then we went on to show that the same behavior could be observed in mammalian cells and plants, and we could also reconstruct it in vitro by playing with the viscosity of the environment, showing that the viscosity of the cytoplasm can set up microtubodynamics for both polymerization. Dynamics for both polymerization and the polymerization rate. And then that project made me think about the cytoplasm and getting a better understanding of its biophysical properties. And for that, we are using the GEMS nanoparticles, which are biological probes that we can express in yeast and tracking them. And we are interested in the question of is the cytoplasm in a more homogeneous environment or heterogeneous environment, and can we say anything about it? And can we say if we can affect how Can we say if we can affect how heterogeneous or homogeneous it is? And basically, what we are seeing is that we have a lot of heterogeneity when we track the particles, multiple particles within one cell, but also between cells. And so we think it might be telling us something about how heterogeneous the cytoplasm is in these cells. And so that's it for my two minutes. Thank you very much. Pretty much two minutes on the dot. Thank you. All right. Next up, we have a Niranjan, Niranjan Sarbancala. If they're here. Yes. Yes, I'm here. Awesome. Go ahead. Thank you. Thank you for the organizers for this opportunity to present in such a nice conference. So in Cell, it is well known that multiple molecular motors work. That multiple molecular motors work together to carry a single cargo. But some in vitro experiments that looked at multi-motor transport concluded that when motors are rigidly coupled, they don't work together well. They interfere with each other's functioning and a decreased processivity of almost 50% is seen when they are functioning in a group of five. That leads us to a question as to how motors work together. Question as to how motors work together well in cell. We were curious to know if the fluidity of the cargo surface reduces this interference between the motors because of the freedom for motors to slide on the cargo surface. We developed a Brownian dynamic simulation with explicit implementation of motor diffusion on the cargo surface. And we see that in In rigid cargo, motors experience higher forces compared to the motors in lipid cargo. This decreased forces indicate the decreased interference in lipid cargo and that leads to decreased off rate of individual motors and hence increased processivity of individual bound motors. We also tested whether fluidity of the cargo surface increased. Of the cargo surface increases the availability of motors for branding. And that has been discussed in earlier works as well, including some researchers in this conference. And we also seen that in our cargo geometry, when certain conditions are satisfied, lipid cargoes accumulate higher number of pound motors than rigid cargo. We also see another subtle effect. See another subtle effect that, as the number of bone motors increases, cargoes come closer and closer to the microtubule, and that increases the effective on rate.