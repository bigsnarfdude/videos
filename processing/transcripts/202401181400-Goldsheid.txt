Ready to start? So our first afternoon talk is by Leah Boltscheid. Tell us about the products of Schrodinger Matrices. Thank you very much for inviting me to this conference. As many small small in the sense of the set of the number of participants. This one was unexpectedly it is always like that. You don't expect much but you get much more than you expected. That's what happened this time at least. So thankful indeed. Very useful. Right. I wanted to talk about products of later certainty. Talk about products or latest depending on the parameter. And I will explain first of all the motivation for that. I'll start with the yn plus one minus e to the power j plus q n by n Why then? Well, I'll write it immediately like that Qn minus P 1 Y 10 minus Y n minus 1 is equal to 0. I've seen this equation many times today. What is what here? Y Y n belongs to in general, I will have to consider Cn. It's a complex vector Cn, it's a C of M-dimensional vector and M here. M here and there are periodic boundary conditions. Y0 is ym and y n capital y1 is equal to yn plus 1. So we have these two boundary conditions. Here n is between 1 One and n right so if you try to solve this equation or before you solve it you get rid of this parameter g by writing and well we have seen all of that but I think I should I write I introduce Zn which is equal to Yn times e to the power ng then it is easy to see that what will happen is that that n then you will have this this equation will turn into minus the 10 plus 1 plus q n the 10 minus the 10 minus 1 q n minus e. I forgot e, which is an important thing. This is the parameter I want. Qn minus E as a t is equal to 0. And of course, as usual, we introduce the matrices for them. So E to the minus G is here. So E to the minus G fronted by N minus 1. E to the power minus N G. On the first level. Minus energy. On the first level. On the first time. It was a power minus G. Of course. Thank you. Thank you very much. Um now we introduce matrices Qn minus P. q n minus e n minus 1 0 and I of course didn't say but I said that y n belongs to Cm. So Qn is a symmetric matrix N by N. N by N symmetric N times N. E times E. And when I write E, I mean E times identity matrix. E is the same as the spectral parameter pi times. So it's a slight summabuse of notation. I hope it's clear. Well, it's not very often that. Well, it's not very often that I do it, so I will still put and there will be no abusable notation. Now, okay, of course you solve this equation by writing that yn plus one yn sorry, z n plus one, z n plus one z n Is equal to the product A n a n minus one z one z one that z okay thanks a lot Now Now, what is it? These are the periodic boundary conditions. We use the periodic boundary conditions. And then we immediately see that this is simply e to the power ng if no, sorry. Here everything is fine, but when I have I have z n plus 1. I see that z n plus 1 z n will be e to the power of ten j that one that zero. That's one. Okay, so we have I'll call this Sn, the product of these matrices, and you have the following relation that that that S M applied to Applied to that one that zero, Sm capital is simply e to the power jn that one. Now, why do we do that? Why do we want to make it? This is because there was a There was a problem also in 1996, 1996, Hatana and Nelson from Harvard. At that time I understand Katana was probably opposed, but not even true. They in Harvard and they published a And they published a paper where these were simply numbers. So they considered the one-dimensional Schrodinger operator non-self. Because of these guys, it's a non-self joint operator. And they discovered a remarkable picture, which I already discussed in the Robloom session, but not everybody saw it. Not everybody saw it. And for certain values of G they saw eigenvalues on this line, and it was absolutely perfectly. And there were also eigenvalues outside this interval. Ah, I forgot to say that. I forgot to say that in this case Qn was simply a random variable uniformly distributed on minus one one. So the spectrum, they simply computed on a comp, put it on a computer, computed the eigenvalues. And capital was 2,000. So quite a number. And these curves on which the These dots. You couldn't, of course, see them as dots because there were about a thousand of them. They formed a perfect line. And this was a puzzle. They didn't know why this happens. So this was a puzzle, in a sense. If you do this simple do this simple transform transformation you can see that what is the meaning of that of this relation if you substitute it here you will see that this number is simply the eigenvalue of the product of these matrices so you have the eigenvalue and And the following lemma We have in fact proved a very simple lemma. E is an eigenvalue of let's call this operator H, the corresponding operator, right? The corresponding operator, right? This one given by this expression. If and only if e to the power mj is the eigenvalue is an eigen eigenvalue well is Eigenvalue of S n to study these eigenvalues, you simply have to know the eigenvalues of the product of matrices. Now, of course, the remarkable fact about that was that this picture is a tense. Picture, as it turns out, obviously persists when n is in capital is 2000. You already see that this picture persists even though the spectrum of the limiting operator, if you forget about n capital, and you take n capital equal to infinity, then the boundary conditions disappear. There is no periodic boundary conditions. Boundary conditions and the spectrum of the operator H, which is simply an infinite matrix Qn I I and so Q1 Q2 and so on this matrix infinite No, this is e to the power nj. This is correct. Sn depends on e, right? That's that's the dependence. Yes. But they are, you simply have to find all these as, yes, of course, ascending. Yes, of course, the Sn depends on E. That's exactly the point. You have a product of matrices depending on a parameter, and you have to find now all the values of the parameter for which you know that they're very good question, a very good remark. I'm too much used to that, so I should have said it. You have to find all the eigenvalues, you have to find all the All the eigenvalues, you have to find all these values of the parameter. So, this is, it is now clear why you may have you may be interested in the properties of the products of matrices depending on a parameter. Yes. For H, you want E for G. Sorry? When you write H as a matrix, so do you need E to the power G? Yes, yes, yes. Yes, yes, yes. Yes. E to the power, whatever, g, no minus g here, e to the power minus g on the other way around. Yes. One of these. Yes, I sorry. No, but when you make the gauge transform, this t transform, you get rid of this parameter. Parameter. But yes, originally, this is the matrix. HG. Now, we heard a talk about this problem just recently, so I will not tell you too much, except this thing, except that, you see, in the limit, the whole area becomes spectral. Spectrum because it is, you can easily check it using the while principle and looking. Yes, if Qn are independent, identically distributed random variables, you can do that. And you can check easily that the whole area will be the spectrum of this non-self-adjoint operator. But before with the limit, you don't see any of the With the limit. You don't see any of these eigenvalues. There are no eigenvalues. The eigenvalues are only here. Okay. So I simply said it to explain the motivation behind one of the motivations. In fact, when I started to look at products of I started to look at products of Z with a matrix with a 2 by 2 matrices. This is 2m by 2m. This is 2m by 2m matrix. But when m is 1, this is a standard one-dimensional Schrodinger operator. And for me, I actually started to look at the matrices depending on a parameter in 1972 quite a time ago. quite a time ago and because I was interested in the localization problem already then and the first thing you you do you you look at the this is simply the solution of the eigenvalue equation and you want to know how does it depend on a parameter and the remarkable And the remarkable fact is that if you take the limit one line log of the norm of Sn and this is of course the Ryapunov exponent, the gamma and I will call it gamma V And I'm now talking about the one-dimensional case. And what happens is the following: that on the spectrum, and of course it is a random, there is a random, there is a randomness here. Random, in principle, with probability one, the right-hand side is a constant, it doesn't depend on the random. Doesn't depend on the random thing. But if I ask the question what happens when I change the parameter, then you fix omega s n of omega. Let me write it like that, sn of omega e and then the dependence of the parameter, then omega becomes essential. What happens, in fact, is the following. I will tell you simply the result: is that I'll put omega here too for almost every omega, this function. And the limit may exist, but let me take the upper limit, which of course concides where the limit is. Course coincides where the limit is, it coincides with the usual Lebanon Witz one. But in this case, the upper limit exists always, so I don't have to care about that. But on the spectrum, on the Schr√∂dinger operator, and you can easily compute, you have seen it is an interval or a union of intervals, a closed set. Set on the spectrum. This guy behaves very wildly. Wildly. It is everywhere. So, first of all, if I write simply the continuous function, some gamma v, if I forget about that, if I take the expectation, I call gamma t is x. Is expectation of gamma E. It's not quite the way you write it, but you see what I mean. And it is just some smooth, discontinuous curve, definitely continuous curve, whatever the distribution is there. And And but if you look at this function, so it coincides with this thing almost everywhere, it is positive, it coincides with it, but also it is everywhere discontinuous. It is zero at a dense set of points. Yes, you yeah. Okay. No, no, you're right. It's not the upper the upper limit. It is the upper okay, no, no, no. The other way around. The other way around. The lower limit, I'm sorry. Anyway, no, both exist. That's all I knew. Thank you. So the lower limit So the lower limit is everywhere discontinuous as a function of V on the spectrum on the Z plane and the jumps are very serious. It jumps to zero and moreover And moreover, actually, there are points where the sub-sequence of that grows slower than n to the power 1 over 2 roughly. I am not s subsequence. So, a random sub-sequence. I don't want to go into the into the details right now. I may The details right now. I mean, if I have the time, I can return to that. What happens outside of the spectrum? Outside of the spectrum, there is a uniform, any compact set. K. Outside of the spectrum on the complex plane, there is a uniform in E. Now Now convergence to this thing with probability law. So this is the result. Now how you achieve something like that? Yeah, why do I want to know it and what does it have to do with this problem? And this problem, since it was many years later, I already knew when I saw the problem. I already knew when I saw the problem, I already knew how to approach it. So I was in this sense. Now, I have to say that at the end, we proved localization in 1977 with Malcolmov and Pastor. This was the first proof. But this did not play much role. This was my thesis, and I thesis and I but it turned out to be very useful when I saw the Hatan-Nelson problem because I knew the following thing that outside the spectrum because there is one more thing you have you have the Lipunovitz point right and what is the sign of that of Of E. It is a product of two of three matrices U D1 of V D one just B D to the power minus 1 of V 0 0 to the diagonal matrix in two-dimensional case and then V these are two automatic. These are two orthogonal matrices, and this is a singular value. And this norm is this singular value, of course. So what does it have to do with the eigenvalues of the product SM? Now the lemma which is important here is that the eigenvalues of such a product under certain conditions when they are independent, oh, just shredding of matrices with independent, say independent shredding of matrices. The eigenvalues behave exactly like these things. This is not a priority, it is not clear why. This will be this. Why this will be the case, but with probability one, it is the case. And so we have S n of Y zero, Y one, Y one, Y zero equal to equal to what is the letter I haven't yet used I didn't use lambda right lambda of P times y1 y0 and this turns out to be an analytic function lambda n of course if I have n Of course, if I have n here, lambda n. But the remarkable fact is that this is an analytic function outside of the spectrum, on the spectrum. Of course, this is simply not true. It's everywhere, again, everywhere discontinuous function. But in principle, the absolute value of the The absolute value of the limit of the log log of the absolute value of number n behaves in the same way as the log of D of M. Right in the N S D N So it was already then when I saw the picture this picture it was clear to me that first of all what is the equation for this curve? You have the equation you have a formula for the Lyapunov exponent and so lambda of E Oh, gamma V, I said. Gamma V is equal to G. This is the equation for this curve. This is where the whole thing starts. And right, and in fact, you have to study. In fact, you have to study this equation: the limit of the eigenvalue lambda of V. The absolute value of V is G. Now this thing in fact is an analytic function of E. This is another function of function. This is another fact. So it's analytic outside of the spectrum. And because of that, you can prove the following fact. That not only are these eigenvalues nicely placed on this curve, but also the distances between them are very Are very regular. There is an analytic function f of z such that there exists f of z such that if I number them like E one here. E1 here and then E2 on the curve along the curve, then En plus 1 minus E1 E En will be I can write it like this, as was z. I can write it like this, f of z. No, actually, what we wrote, there was a reason to write it pi f of z n plus all small o n of 1 over n. This is in our paper with Boris Faruzhenko from 2003, which means the following: that once you know one of them and the One of them and the function, you actually know all of them. It's a very rigid thing. You simply can compute, and at the end, the mistake will be 1 over square root of n. So it's a really very nice, rigid thing, and this is what is sometimes later was called in a different context as clockwise behavior. But we knew about this behavior very long time ago. Oh sorry. Uh I think they they are they should be N. F Z and then they should be F of Z. F E N. F yeah. The denominator? F E N. No. I think they Z Oh yes, yes, yes. Of course. Niche Z. There are many of them otherwise. Of PM, yes. This is a very meaningless formula that I wrote, but I hope it is clear what I mean. Thank you, indeed. Great. Now, what happens when you have matrices? Soon after that, seven years after Hatano and Nelson published the paper, Hatano published another paper where he again computed on his own. Already he was at that time already in Japan as far as I know, and where he considered the case when Qn is simply our Our beloved qn1, qn2, and minus 11. So it's a strip of size 2. It corresponds to matrices which arise when you consider a strip of size 2 and He found out the following thing that there are two curves. Again, there is some spectrum which is real, and there are two curves: one, the other one for certain values of G. Values of G. I cannot explain all of them because there are critical values of G. We have all of that in our pattern. But I will not be talking about this, otherwise it will be not to talk about products or matrices. And he saw something like that. Here, the picture was not that. Clean because the other one was absolutely clean. It was something when I saw the other one, I couldn't believe first because I knew that the spectrum will be the whole thing, and how can it happen? And then I understood why. But here the calculations are much more complicated. So you in fact may see some eigenvalue here, otherwise. Here isolated and which is question why they are there, but it is the theory says that it is clear why they are there because the computer is not always perfect. In fact, again, you have the same story. This lemma applies in the same way, but now the problem is you have to control not just one, the first Napoleon span. You have two of them in this case. You have two of them in this case, and one of these, this one, is gamma one of V is a constant. This is the level line for gamma one, the larger one, five minutes. Yeah, thank you. And this is the large the other one for gamma two. Now, I told you that uh I told you that in my abstract, I told you that I will say at least something about the open problems, and I will now say something about open problems. It is not simple to control this thing on the well, on the first of all, on the spectrum. On the spectrum, if we forget about complex rings, if we consider the standard pseudo-emerger on a strip, on the real line, what happens is the following, that the smaller eponymous exponent you have, I'll draw it for the 4 by 4 matrix. You have gamma 1, only Gamma 1 of V which is greater than gamma 2 of V and then here we have gamma 2 to the power minus 1 of V and gamma 1 to the power minus 1. Do you mean negative? Negative? Negative. Negative. Negative. Minus one. No. I mean, it's leaped off exponents, then it's a negative. Yes, but here I. Yes. No, I okay. E to the I'm sorry. E to the power minus I consider the exponent of the power. The exponent of this matrix, e to the power in the limit, gamma 1 exp of this, exp exponent of that, exponent of this gamma one. On the spectrum, what we do know, and this is in our recent paper with Sacha Sweden, is that this thing behaves in a wild way. The small Varypunov exponent on the spectrum behaves in a way which is similar to what I described for the previous case. And this is, of course, for my This is, of course, for matrices, for matrices of arbitrary size 2n by 1. Now, we do not know what is happening with gamma 1. So, we have the following picture. If this one is if this is the fa this is the spectrum, right. Well, this is a spectrum, roughly, right? You can always compute the spectrum for these matrices. The lower Lapunov exponent, I will take one moment, just behaves in this horrible way for fixed dominant. You don't know that about the upper hypoene problem. An open question. An opening question. On the complex plane, this is a work in progress, and I am sure that both of them are analytic functions of that. But again, it's a non-trivial thing. At least I don't know a trivial proof. In the previous case, I could extract it from Wayne's theorem about the existence of L2 solutions. I could extract I could extract the fact that the Lyapunov exponent is nice analyzed function. But here you don't know anything like that. It doesn't work the same way here. So there are many open problems in our paper. So I promised, I thought I will say much more about open problems, but at least I'm listening. Problems, but at least I always let go. Okay, thank you very much. Questions? Yeah, this is in the 1D case, this is a direct consequence of uniform hypercity. The, you know, lambda n and dn, they behave singularly, and lambda n analytic function. That is because uniform hypericity. So in the regime of uniform hyperlicity, Lafayette exponents is probably harmonic. Exponents is probably harmonic. So there's no question. So now go back to the high-dimensional case. I don't know if there's a corresponding theorem, like outside of the spectrum, do you have, for the 4x4 cycle, do you still have some kind of uniform hyperlicity? Okay. I had a very, I know about uniform hyperbolicity and I know that there is this approach. Now these are independent matrices that you multiply. I extracted it in a very different way. Very different way back then, more than 50 years ago from, as I said, from the fact that there is a wild solution, wild solution on the plane. I didn't have the chance to explain how one in general has to control the product of matrices depending on a parameter. There is a theorem which says that there is a set, well, there is an There is a set, well, there is an invariant process when you consider a transformation of a unit vector, y, depending on a parameter, and it is not essential whether it is a spectral parameter or something else, into Ay divided by the normal Ay. The normal way, this is a unit vector. And if, and when you remember that this is, there is a parameter there, you have a random process. And if the trajectories of this were there to be uniform convergence, you need the trajectories of this process to be continuous. It's necessary and sufficient condition. And you can extract it, as I said. And you can extract it, as I said, from in the two-dimensional case, in the two by two matrices, from the Wild theory. But in general, it's much more complicated. And it's another lecture to discuss it. I thought that I will explain more, but I unfortunately didn't have. But I unfortunately didn't have time to prepare slides. That would be possible if I would be as advanced as these young people here and who prepare these slides in no time. But I usually take too much time to do them. You should do what Lama is. You should do what Lama is. And what if somebody else is trying to do that? No, this is a good idea. No, this is a good idea. I know. This approach, I know. I know. I have some people who help me, let's say. Thank you. I have a question about the Myarov exponent. Is every Myarov exponent a harmonic a sub harmonic function? Yes, yes. But I I think But uh I I think uh no no if I don't I I know that the sum of them of them is not the whole the sum of the first K-Laguna exponents, not all of them, but K, is a subharmonic function. Our paper is with such sodium is there and that you explain this thing. That you explain this in there. It is. And there is a sort of generalization of this formula. Phallus. The thalus formula? The thallus. No, no, no, no. This there is a The formula which tells you that the Lapunov exponent is the logarithmic potential of is a yes you you have to integrate logov at minus c of it's a famous thing. A special story about doubles. That's another anonymous guy. Among physicists, he name a similar questions. Thank you. So let's take a 10-minute break.