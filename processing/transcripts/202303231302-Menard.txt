Yep, thank you. So this um So this work came across as from an application, very simple application we started like 20 years ago to do maps, surface maps of air pollution. And then from this, I developed some mathematics and that's what I will be presenting. So, a long time ago, we were asked to develop kind of a Of an operational real-time surface analyses of air quality using the surface automation network. So it started in 2002 and then it gained popularity over time, especially for people who want to do health impact studies. They like to have those long data sets. It's not an assimilation. It's a very simple thing. You're just doing an analysis using observations with a model. Using observations with a model and try to make the best of it. And to try to make the best of it comes with estimating the error statistics. So that's what I will be showing. So the first part of the talk is some thoughts I have about error statistics and ways to look at this, how to compare with the Delhi method, for example. And then because And then because air quality forecasts are very expensive, at least in our shop, and we tried to minimize the number of integration of air quality forecast and we came up with something very simple that does an analysis using kind of an offline end sample that captures a lot of the features that we want without doing a rerun, no-end sample of rerunning the model, the air quality model. Rerunning the model, the air quality model. So, the first thing is that in any analyses, you need to have observation error covariances, background error covariances. How do you know those are good? How close are you? So, some time ago, I developed some basis, some theory about it, and that's what I will explain and illustrate. So, some people have seen that. So, some people have seen that, but I tried to recapture this in a different, little, slightly different framework. So, in order to know if your error covariances are the true error covariances, you need observations. Cannot do that without observations. And then, what I propose is that we will find the background error covariance, but only in the observation space, not the flu space. Space, not the flu space. So it turns out that if R tilde is the prescribed error covariance, the same for B, so if the prescribed error covariance is the same as the true R, in the same way for the background error covariance in observation space, this is equivalent to having those two criteria. The first criteria, which is B, is the innovation covariance consistency. Innovation covariance consistency. Does the sum of those two represent the innovation covariance? This is only based on observation and model. It doesn't have any assumptions about the covariance model that you're using. That's A, B, sorry. And then A is that in order to have those two conditions above, that your prescribed error covariances match the true ones, you need also another condition. You need also another condition, which is that the gain in observation space is actually the optimal gain. And the optimal gain is the one who minimizes the analysis error. So for the innovation Kubernetes consistency, we've been using that sometimes. It's like the chi-square thing or the J-min, which is not so nice in a way if you only want to have the variance. So you can take a look at just variance matching to see if you have the right variance. To see if you have the right variance. Because chi-square and J-min, they have a correlation length scale in there, correlation model, so it depends on more than just variances. Okay. Now the common gain condition is, that's a tricky one. How do you know that you have the optimal analysis or the optimal common gain? Well, it turns out that we can develop some diagnostics, a bit like Diagnostics, a bit like in the innovation covariance consistency, to know if you have an optimal common gain in observation space. And that's what I will be talking about. So we did that using cross-validation. So let me explain to you a little bit what it is here in our example, our application. So this is the surface observation of observation of ozone observation in real time each hour. In real time, each hour over North America, and that dates at least five years. So you have about 1,300 observations. We divide that in three data sets of observations. Two data sets is used to produce an analysis. Another data set is used to verify that analysis. And we do all the permutations. It's out that if you collect the statistics and here, artificially, I change the way. Here, artificially, I change the weight of the observation. So, when this number goes to close to zero, I put all the observations artificially made to be prescribed observation error variance goes to zero. So you had a lot of weight on the observation. And here, almost no weight. So, if you are to compare your analyses with the same observation that are used. The same observation that are used to produce those analyses, this is the curve you have here. Okay, so as you put more and more trust that you think on the observation, you just believe your analysis and then it goes to zero, basically. So it's not a good measure. However, if you use a different set of observations to evaluate your analysis, you find that you, for a certain ratio, for a certain observation weight, you find that this is there is. Way, you find that this is there's a minimum. And this, mind you, this is a log log-log plot. Okay, so it's a little curve, but it's important. So, in order to derive true error statistics to know, you need to have independent observations or observations that you withdrew from your analyses to verify how close you are. So, from this, I'll just talk a little bit about. I'll just talk a little bit about geometric use of analyses, Hilbert space random variables. So, this was introduced by Nero Z. And then, well, it's actually a classical results of mathematics. So, if you represent random variables in terms of Hilbert space or vectors, you would say that the background error is uncorrelated with the observation error, O T. Error, O T, then those two arrows are perpendicular, and then the analysis would lie between the observation and the background, a point in between somewhere. And this would be the analysis. When the analysis is optimal, it means that the distance between the analysis and the truth that you don't know is minimal. So that's what you see here. And when this is minimal, then you have this condition that we know about optimal analyses. So, with the ROC method, what do you have? Well, you assume that the analysis is optimal or that you know the true background error covariance. And then, you assume also that you have innovation covariance consistency. From there, you can derive what is the observation error covariance. Cross-validation approach is slightly different, a little bit more general, I would say. It still assumes the innovation covariance consistent. Still assumes the innovation covariance consistency, it's very important, but then it estimates the parameters of the covariance model that minimizes the analysis error variance. And I'll show how this is done. And then at the end, you have a new estimate of the observation error variance, covariance, background, and also analysis. So, now I've introduced a third dimension. Now, I've introduced a third dimension. So, this is the analysis plane. So, this is the observation you will use for the Ducky analysis. You have the background, the truth that you don't know. So, you use another set of observations, OC, that is not used to produce the analysis. You still continue to assume that the observations are spatially uncorrelated with the background, and so geometrically, you have a point that is perpendicular to the analysis plane. What does it happen? Plane. What doesn't happen then? Yeah, so the observation, the passive observation, is orthogonal to the observation that is used for the analysis because you assume that they're not spatially correlated. The same, you assume that the observation error is not correlated with the background error. So basically, you end up somewhere perpendicular to the plane. And say, okay, I want to know. And say, okay, I want to know where the analysis lies. Okay, it will lie somewhere in that line. So you can actually use the distance between the analysis and your passive observation. Try to find the minimum, as we saw before, to find what is the optimal analysis. And why is that? Because when the distance between OC and A is minimum, is actually when Minimum is actually when A is closer to the truth. So the minimum is actually a physical quantity that corresponds to parameters that gets you the minimum analysis. The analysis, as I say, is produced by, you can evaluate that because you have a new set of observations, a set of observations you haven't used. So this is a trial. Use. So, this is a triangle. This was discussed in a paper by Marseille. At the same time, I published this, but, anyways, things are like that. So, this is the plane, you see. So, if you see things in 3D, you have this right triangle, actually. And you apply the same Pitegoras theorem for the right triangle. So the distance, OCA, is actually the analysis error, AT. The analysis error plus the observation error. Okay, so this is written here. That's very nice. And so you change the parameters of your covariances and you find a point that minimizes the distance. And this is exactly what we found experimentally, right? So as we change the observation weight, we find that the That the passive observation minus the analysis variance, the variance of that reaches a minimum. And that means that for those parameter values, you have an optimal, well, close to optimal analysis. It turns out that some of those triangles also mean something else. For example, the Hollingsworth-Longberg the Rose is actually this triangle. This triangle, okay, from true analysis to observation, but again, it assumes that the analysis is optimal. Another one that we derive actually, okay? You can have ominous A, no, this is De Rosey, yeah, sorry. So ominous A times A minus B is the analysis error according to De Rose. So that corresponds to some geometry related to that triangle. So we devise a new one. Let's play. So this is A minus B, A minus B. Then you have, you would have the, but you need to have the optimal analysis to do that. Then you have another set of triangles for which you can develop this. You can also develop diagnostics not in the analysis plane, but using those cross-validation observations to develop. Validation observation to develop new diagnostics. So, this is one that I just said, but I'm not saying, but there's more than that. So, you can have you can use that plane, which actually, if this is a again, this assumed that the analysis would be optimal, and then you have this uh relationship. It's kind of difficult to cumbersome to derive, but that's what it happens. That's what it happens. Okay. So with this, we were able to develop and see what are the our problem was very simple. We just have scaling factor for the observation error and the background error. We just don't know what is the scaling factor we should use. So just by changing the scalar factor, okay, between observation error variance and background error, Between observation error variance and background error variance, we find the minimum. And then we have a better estimate of what should be a better observation error, a more truthful observation error variance, and a more truthful background error variance. Now, we did that and conduct an analysis. And that's the second part of the presentation. I go fast, but that's okay. You have time for questions. Questions. So, in the first part, like 20 years ago, we did those analyses with isotropic correlation models. And now we thought that, okay, well, we can do better than that just by taking just a very simple-minded climatological and sanitable over a period of two months for the same day, time of the day, and collect the Of the day and collect the correlation you would get, and apply the localization as we all do for ensemble and get the correlation in space. This is a, so I call that the kind of an offline analysis, offline ensemble correlation, right? And you see it does, surprisingly, it does quite good actually to capture the local topography near being near. Near being nearby land masses, water masses, and so on. And this is it shows up very nicely. So around Toronto, for example, there's no topography, there's nothing really that stands out, and the correlation pattern is pretty much circular. But in the West Coast, Pacific, California, you have theory is a theory. Have very esoteric correlation models, and that's really good because when you assimilate, you have all these observations, your neighbor observation here and there, and they produce increments, and you try to manage those increments by collecting all this information constructively. But if your correlation model is completely off, you don't construct constructively. Okay? So by So, by having those isotropic correlation models, you do see a big result. And that's what the weather forecaster told us is that your analysis is nice, but it's no boon along the coastal areas. So, with this, it's much, much better. And this is an example. Just by changing, we change nothing here. This is the verification of the independent observation. Of the independent observation with the analysis. And so we find a minimum for a certain observation weight. And if we use homogeneous isotropic correlation models, and we don't change any variance, we find a minimum somewhere when the weight between the observation and the background is about the same. But if we use those spatially Spatially intricate patterns of correlations, then we get, we can give a lot more weight to the observations. And the overall analysis error has decreased from something like 65 to 50. So, overall, you just by only changing the error correlation. Now, to implement Now to implement, oh yeah, and also what is nice is that those this metric of observation, independent observation minus analysis can also help you to determine other parameters. For example, you need to know what is the correlation length of your aesthetic combo. How do you get this? Well, you get it really directly from these observation minus analyses. And this is what you would do. Okay, and this is what you would get here. So, you get the information about the error variance, but also about the correlation length that you should use that would actually optimize the analysis error after using a tapering of Gastonic Poem for your ensemble. The crux of this is that you need the error variance, and that's the tricky part, and that's where machine learning could be very useful. Useful. Just by inspection, we found that the variance of the innovation is quite nicely correlated with the concentration themselves. That's for NO2. A PM2.5, it's a little bit more difficult, but there's something. Ozone, no correlation at all. And then nighttime. During daytime, you have a bit of a significant Significant, you need to construct a model of your air variance, okay? And so, if anything that would improve it, it would be to have a machine learning scheme, develop these relationships. I'm coming to a conclusion. So, the cross-validation is really nice because you can get an idea of the analysis error or the true analysis error. Or the true analysis error. And I think that's very important in general for chemical data simulation. Forecast error is one thing, but if you have a highly dissipative system for which you forget about the initial condition, the forecast doesn't give you much information about if you have a true analysis or not. So better take a look at what is the analysis error and use some methods to do that. And cross-validation is one. And cross-validation is one which minimizes. Well, if you have a lot of observations, you can do whatever you want. But here, you don't have so many observations, so you do cross-validation. There's a geometrical interpretation that lies all these diagnostics together, the De Rosie, Hollingsworthland, Berkeley, and so on. So it's a nice theoretical framework. And what we're looking for now is how to estimate online the To estimate online those parameters so we can run and do like a 20-year reanalysis that we're planning to do now. And also, Sina will present, but we found out that the same property actually holds for satellite observation. They don't need to be having spatially uncorrelated error for this to work. Why? Work. Why? Big mystery. But I'm starting to look into this. I mean, this is kind of a generalization of what the analysis should be. Thank you, Richard. That's a very interesting presentation. My question is. Presentation. My question is: when you experiment, how do you generate those cross-validation observations and guarantee they are perpendicular to the two observation and states? They are. The last part, so the first part is that how do you generate those set of observations? Yeah. Okay, well, this is, there's techniques, I guess, to do that. We did very simple. We just selected one station after three ID number. Station after three ID numbers, okay, and we look at this and then okay, geographically they look like random. Okay, and if you have random data sets of the same size, then you can say, okay, the statistics that you have over the whole domain represent a global measure. So that's one way. There's more sophisticated ways, I think, that Uber metrics separate, make sure that you don't have You don't have an observation that is too close, but belongs to you, the other day as I said. So there are many ways to do that. So that's one part of your question. Your second part is... Oh, yeah, that's all. That's all? Okay. But yeah. Overall, I think that the way to measure that we're getting closer to the reality is that we see that the fifth of the observation minus. The observation, minus analysis, residual, is overall smaller. And if you take a look at individual stations, before with the isotropic correlation models, you see all these big blobs. And if you reduce artificially the observation error to be very small, these blobs still persist. Whereas if you use this ensemble approach, you it's kind of kind of screwed. Everybody really agrees with each other. It's kinda nice. So I have two questions. Sure. The thing that keeps me up at night is for constituents, especially carbon, our R and R B are quite correlated, actually. With tropomy, for instance, there's a strong albedo kind of bias. So let me say our airs are correlated, not necessarily the and that albedo. And that albedo is going to correspond to where we have trees, and then turning on our model layers. So, you know, how do we deal with the fact that these, in fact, will have strong correlations? Well, you see, I construct other triangles. In a case where you have covariances between observation errors and background errors. And the De Rossi method doesn't work in that. But some of the backgrounds are. This, some of the diagnostic we use still works, okay? Because, well, we have to go through the derivation here, but for example, this the Hollingsworth-Lumber method still works in that case. What we've developed still works with the so I cannot go into the detail, but there are certain diagnostics that are less stringent on the assumptions you're making. Yeah, I but I mean, it's Yeah, I. But, but, I mean, it's not the end of the story, it's just the beginning of the story, right? It keeps me up at night because I'm not even convinced that the way that when we write out that cost function that everyone likes to write it, true, that has already assumed R and gain you can write the full common gain. And whoops, I can't. I'm actually going to see the equations of the main. Yeah, here. So you can write the full common gain, and then The full common gain, and then you see here the observation error is correlated with the background error because this is not a right triangle anymore. Yet, you can still find the valuable analysis in that case. So, there is some ways to relax those assumptions. And I think, well, I haven't put it there because it's not really, it's just, I think that the way we write the analysis error is a little bit long, actually. Long actually. It's more like the covariance between the innovation and the forecast error divided by the inverse of the innovation covariance. And that, if you take that, you get the generalized form here, okay, for the covariance. And it's much more directly related to the aerostatistics that you use. I think there's a link between that. The second part, just quickly. Just quickly. I missed sort of how you're not cycling, right? No, here I'm not cycling. Okay, so when in a cycled system, the bead is going to be much different than on a non-cycled system. Absolutely true. So the thing is that if we can do this online, I think we can. Yeah, but you mentioned about like a d a different logo time. And so you only show the uh observations uh error. Observations error and their structure should depend on the local time because of all the chemistry. Yeah, there's a bunch of assumptions. So I'm just curious about how they vary from early ammonium to light. Oh, we have all those diagonal cycles. We have the diagonal cycles of the of the compact support correlation length that you need to have and it all make quite physical sense in a way, okay? Physical sense in a way. Like PM2.5 doesn't vary much over time, over the time of the day, but ozone varies a lot, N2O. So it's kind of indicating that we are on the right track for that. But even the innovation covariance consistency is a very important condition to map, and there's a lot of operational system that they don't have it. So maybe we should hyperten. Just following up. So we do lots of analysis on, you know, when we do the operation system, I work with people. We do look at the O minus B or R minus A statistic in comparison to the R that we used in the operation system. So and then we see lots of inconsistencies, but we are doing our best. But these things, as Richard said, you have lots of statistics. So this is where machine learning can help us. We can learn those time dependence and those things. Yeah, yeah, yeah, because this is. And we'll see. Yeah, yeah, yeah, because this is artificially modeled. You know, those error covariance. You know, I can guess the best I can from physical insight, but it would be much better if machine learning would guide me towards a modeling of the error covariances. And then I can estimate the parameters based on that. Yeah. No bias, yes. But you can do any bias correction to with. If you go, let's say, to slide nineteen, how do you actually parameterize this convariance, the speed just vary the actual correlation parameter? Do you also change the smoothness or how do we define this? For the correlation of the background, the model error correlation? So if you parameterize a spatial covariance, you usually have three parameters, right? Usually there are three parameters, right? One describing the variance, one describing the smoothness of the function, and one describing the length scale. Well, the length scale, the length scale, and smoothness is given by using an ensemble of forecast models. So they're very different? You fix these in space? No, I mean the ensemble tells me what I should be using. It's those things. And because of noise, I need to use it. Then, because of noise, I need to tamper those correlation functions. So, this is what I get between Vancouver and the rest of the domain, and so on. So, this is clearly a snapshot estimate you get for an instance, but we actually parameterize it and then develop a non-stationary. It changes over time, each two months, changes over time of the day, each hour of the day, and so we collect all those statistics for different species and different types. For different species and different times and that's what we use as input for the simple analysis. Because it is really a sample estimate from data that you write, but nothing that is actually parameterized and evolves and screen, sort of. There are some, well for the correlation we use a model output only. What is parameterized is what do we need to use for error variances. We saw empirically that there are some dependencies, probably because it's positive quantities and so. It's positive quantities and so on, that there are some relationships between the sum of observation and error background with the concentration, which is very noticeable, for example, for MO2. SO2 is, wow, nice curve, all the points fit the line. So why is that? How could we improve that? That's a good question. Overnight time, well, ozone is very, I don't know what it is, you know, just a cloud of points. Don't know what it is, you know, just a cloud of points, so I just put a constant. But if we would know how to specify that with other chemical variables, for example, then that would be very helpful. What is the good thing about this? That runs fast, really fast. Because you have all the error statistics, you don't need to rerun your model, you can do 20 years, but you need to have the error statistics. So far, we did it offline, but I think there's some hope. But I think there's some hope to do it online, so it will run even it will be even faster to do. It's very nice while we're thinking how to identify this system. About your point about cross-validation, what if you're interested in a parameter, so not something that you don't observe? Can you still use? Sorry, can you still use this method? I don't think so. I think you need to have like a kind of lack of covariances, you know? You need to have time. If you don't observe the variable, I think time is of the essence. It's like you don't have a lot of information about if you have the right or wrong. Emissions. But if you let the emissions work their way over time, and then you take up the innovation, and so there are some, but we haven't done in-breed yet, but I think time is of the essence. Your diagnostic should involve time if you want to be able to capture the aerostatistics of unobserved headrooms. That's my feeling about this. 