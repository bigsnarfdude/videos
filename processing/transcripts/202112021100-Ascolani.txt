So, thank you very much for your really nice introduction and good morning to everyone over there in Mexico. Today I'm going to talk about a joint work with Antonio Leoi and Igor Trunste, again from Bocconi University. So I will talk about tweaks of random probability measure, especially in Birge-Nomparametrics, and also more in general about the And also, more in general about the concept of borrowing of information. So, first of all, I will try to convince you that the setting of the tree dependence is actually relevant in the Bayesian parameters of literature and that therefore we may need an appropriate generative model. Then I will explore its main prior and posterior properties. And finally, I will discuss a little bit more the general concept of borrowing of information and why and when it. And why and when it may be beneficial. So, first of all, when and how twists appear in the Bayesian parametric literature? Well, a simple example is given by hierarchical processes that are very popular to induce dependence between groups in partial exchangeable models. In this simple picture, we have three groups that are associated to three random probability measures, P1, P2, and P3, that are connected through a common latent beta. Through a common latent baseline distribution P0 that is itself random. So, this can be seen as a very simple example of a tree where all the observations are collected at the leaves. Moreover, there is a latent set that in this case is given by the single node P0, and we will see that the changing the latent set of nodes will cause great clustering, great change in the clustering. There is a great change in the clustering structure of the tree, and it may be more or less adequate to the specific phenomena of interest. Sometimes, instead, the shape of the tree will be dictated by the application itself. For instance, this is an example of a sequential decision process describing the contraceptive behavior in India. And again, this is an example of a tree where all the observations are collected at the leaves. Are collected that the leaves that describe the women choices. However, notice that the leaves may be at different levels, and this means that we need a more flexible specification than before. Finally, we could use a tree in order to describe the refinement of our knowledge. And then in this setting, we would associate each node to a subset of the observation. And as you observe more data, we consider more nodes in the tree. There are more nodes in the tree. And we will see that this very peculiar specification will lead to very peculiar clustering structures that may be very useful in some settings. So, in general, we will work with trees of this form, where each node corresponds to a random probability measure. The tree will be divided in different levels, where level 0 always corresponds to the root p 0. Moreover, we will call by xij. Moreover, we will call by xij the JT observation at node i, and we use the intuitive definition of most recent common ancestor of nodes i and j that simply gives us the closest node that is connected both to i and j. So notice that in order to give a generative model for a tree, we need to specify two things. First of all, we need to specify how we will sample the node and how the link will induce dependence. The link will induce dependence between the nodes. So, first of all, we will assume that each random probability measure is discrete, so that it admits this representation as a countable sum of random weight, Wk, associated to random atoms, Zk that are sample IID from some distribution Q that we will call baseline distribution. Then, you are able to give our generative model that is given in two steps. First of all, we two steps. First of all, we sample the root p0 from a discrete random probability measure with some baseline distribution p0. Moreover, and we did the second step, if we condition on the parent, then all the children are independently generated again by a discrete random probability measure whose baseline distribution is given by the father itself. So the idea is that the parent tells us which atoms are available. Which atoms are available for all the children. Moreover, we will consider a specific construction of discrete random probability measure that comes from the notion of completely random measure that I will here briefly recall. In particular, we say that a random object may be a completely random measure if its application to disjoint subsets are mutually independent. And the reason why we like this definition is that it leads to very Definition is that it leads to very tractable objects. In particular, just to give you some intuition, the key property is the so-called Levitt-Tinching representation. And it tells us that the moment genetic function of mu A can be written in this simple way, where theta is a positive constant, P0 is the baseline distribution of the atom, so it specifies the law of the atoms, and phi lambda is a quantity that depends on rho, that is a measure on R plus. That is a measure on our plus that essentially specifies the law of the weight. So we have P0 that specifies the law of the atoms, and rho that specifies the law of the weight. Moreover, under some suitable technical condition, you can normalize a completely random measure that is not a probability measure, and thus you obtain P that is called a normalized random measure with independent increments. And those will be our building blocks for our tree. Notice that. Notice that this class is fairly large. Indeed, it contains very well-known examples like the Dirichlet process and the normalized table process. So let me notice that in order to specify a normalized random measure, we need to specify the law of the weights through rho and the law of the atoms through p0 plus plus this positive constant theta. Now, then you are able to give our final definition of a tree that Final definition of a tree that is given it before, where each node corresponds to a normalized random measure with independent segments. And the reason why we consider this specific definition is that it leads to great tractability. And in particular, we will see that we will be able to derive many prior and posterior quantities. In particular, a crucial quantity will be the so-called gamma, that is essentially the probability of a tie induced by probability of a tie induced by the specific selective normalized random measure. And the usefulness of this quantity is immediately clear if you look at the prior correlation structure. Indeed, if you consider two nodes i and j, whose most recent common ancestor is at level k, then you are able to compute explicitly both the correlation between pi and pj evaluated on the same boreal set and also the correlation between observation coming from Between observation coming from Pi and Pj. So, correlation between observations that belong to different nodes. And notice that especially the latter is very simple. And if k goes to infinity, so if we go down and down along the tree, then this correlation will go to one. And this essentially means that if we move along the tree, then the observations and the random measure will become more and more correlated. And it is like in two. correlated and this is like intuitive with our this is like coherent with our intuitive definition of a tree moreover since we are dealing with discrete priors it makes sense to study the number of distinct clusters out of an observation that here we call by kn here for simplicity we will focus on the declair process and the normalized table process and we will consider two different scenarios so first of all we assume that all the observations have to be sampled The observations have to be sampled at some fixed level k. And then, in this k, the number of clusters will diverge almost surely with rates that become smaller as we have come down along the tree. So there is essentially the same behavior of the classical model, like the Dirichlet process, but with the only difference that the rate will become smaller and smaller, and it is coherent with what we saw about the correlation. We saw about the correlation. Instead, more interestingly, if you consider the different case in which you assume to collect a fixed number of observation n for each level, and then you make the number of levels go to infinity, then in this case, the number of clusters will be bounded almost surely. And indeed, it's quite peculiar, since most of the Bayesian non-parametric pier induce an infinite number of plastic. An infinite number of clusters a priority. So, this essentially means that if you change the structure of the tree, you are affecting very much the clustering structure of the observation. And it can be exploited, for instance, if you want to estimate the total number of tweets in a population, or if you want to estimate the total number of components in a mutual model. You can try and use the most tweetable shape of the tree. Most suitable shape of the tree. So, just to give you an intuitive idea, in this graph, we have shown the average proportion of distinct values in groups of 20 observations for three different models. The black line corresponds to the classical normalized table process, while the red line corresponds to the hierarchical normalized table in which all the observations are sampled at level one. And you see that the two curves have essentially the same behavior, but Essentially, the same behavior, but with different convergence waves. Instead, the blue line corresponds to the non-exchangeable model, where each node is related to 100 observations, and it is apparent that this curve goes to zero much faster. And this is connected to the fact that this model leads to a finite number of clusters a priori. So it is clear that the average proportion of new value must go to zero as n goes to infinity. must go to zero as n goes to infinity and it must go to zero very fast. So here essentially we have seen that we can obtain very diverse behavior for the number of cluctes and it also means that we can obtain very diverse clactan structures of the tree. In particular, just to give some intuition, assume that you want to sample n observations at level k. So it means that we need to sample n values from pk. However, pk is discrete. However, PK is discrete, so actually we will have LK cluster that we will need to sample. Therefore, we go to the previous level and you will have to sample LK observation from PK minus one. However, PK minus one is itself discrete. So again, we will have Lk minus one values to sample. So if you continue this recursion, then you obtain that you need to sample L0 values from the original. Sample L0 values from the original baseline distribution P node. So essentially, this means that there is a hidden clustering structure between observations at different levels, and moreover, that the root plays the role of selecting the unique values that you can find in all the nodes. If you think about it, it can be well interpreted in the metaphor of the Chinese restaurant metaphor. Estrument metaphor. Indeed, we will have that at each level the observations are divided in different tables, but the digit will come from the previous level. So this means that different tables in different levels may share the same digit. Moreover, if you consider this metaphor, then LK will just become the number of tables at each level, and moreover, P0 will place the role of the common menu from which all The common menu from which all the dishes can be sampled. So we have this nice, we have the night metaphor that tells us how the observations are divided and connected between different levels and therefore between different nodes. And we can exploit this night clustering structure in order to derive a posterior characterization. Indeed, we will condition on a sample X that will be connected to an arbitrary To an arbitrary subset of the tree, but we will also condition on T, that is a set of latent labels that describe this clustering structure. Indeed, if we condition also on T, then we will be able to obtain also the number of tables at each node and the number of customers at each node that are eating a specific dish. And thanks to this information, we can actually obtain a nice behavioral characterization that is given in two steps. That is given in two steps. Indeed, first of all, we have that the posterior distribution of mid zero, where mid zero is the completely random measure associated to the root, then this posterior distribution, given the sample, given the set of latent labels, and given a suitable latent variable u0, it is given by a sum of independent components. In particular, mid0 het is a completely random measure with modified intensity. measure with modified intensity, plus some plus plus plus some independent jumps associated to the unique values in the sample. So in a way this update is very intuitive since essentially we are reinforcing the baseline distribution exactly where we have observed the data. Moreover, and this is the second step, if you condition also also on the parent P star, then then then then the children will be independent and their posterior distribution will be given again by some of independent terms. In particular, we will have mi-het that again will be a completely random measure with modifying intensity plus some random jumps associated to the unique value. Moreover, notice that the distribution of this jump will depend also on the number of customers that are eating at that specific node at that specific table. node at a specific table. So here you see that the dependence also on the hidden cluster structure is crucial. Notice that the beautiful that this term is beautiful since even if you do not observe data for each node or you have missing data, you still can compute the posterior distribution and it can be useful to actually extract information also in case of missing data or partial information. Moreover, Moreover, this theorem is not useful and interesting per se only, but also because it can be used to divide marginal and conditional algorithms to sample a posterior. So essentially, you have seen that we need a good amount of work in order to accommodate for this dependence and this borrowing between the nodes. And clearly, if you compare it with the simple case in which all the groups are supposed to be independent, so there are no links between the nodes, then this Link between the nodes, then this model will lead to probably a more complex algorithm to implement and also a more complex model to interpret the cortex. So now the natural question becomes how can you be sure that this borrowing will be useful? But even more in general, how can you be sure that the borrowing of information works? Well, first of all, we will First of all, what we mean by borrowing of information is the class of procedures that try to combine data coming from different sources. And the classical example is one of the hospitals where we assume to observe patients coming from different centers, and we want to incorporate in the model both the homogeneity within each group and also the heterogeneity given by the possibly different treatment conditions. So, as you know, there have been So, as you know, there has been a lot of work in the literature in trying to accommodate type dependence in order to improve the estimate. And it led to hierarchical structure, nested structure, and so on. Moreover, overdote models also led to weigh both theoretical and computational challenges in order to be able to sample a posteriori, in order to be sure how the Order to be sure how the procedure looks like, and so on. And now the question becomes: does all this work in the end worth the effort? Or as in this comic, you have that you need to make much more work than what you actually gain in the end. So we will try to answer it with very difficult question, considering a simple transparent testing in which you are able to do all the calculation. This is still a preliminary work, so all Preliminary work, so all kinds of feedback will be greatly appreciated. So, first of all, we consider probably the simplest setting, the simplest data generating mechanism. We assume that we have only two groups, X and Y, and one is the multiplicity of the first sample, and then two the multiplicity of the second sample. And we assume that those two groups are independent. Moreover, within each group, we will have that the observation. Group, we will have that the observation are iid from some suitable distribution p and q. We will not assume anything on p and q apart from the fact that they have finite means mu1 and mu2 and finite variance, see variance, sigma1 and sigma2. Moreover, we will also assume to be interested only in the first view. So for instance, we want to estimate the unknown millimear one, or we want to estimate the density of the first. Estimate the density of the fluid. So, since this is a case in which the groups are independent by a function, it would be natural to think that in this case, borrowing should not be very beneficial. And we will see whether this is true or not. Moreover, in order to study this data, we will consider a very simple specification where the data conditional random parameters nu1 and nu2 are normally distributed with fixed variances. With fixed variances tai 1 square and tai 2 square. Moreover, we will place a plyo on any one and e2 that is given by a bividap normal distribution with correlation row. And this parameter, the correlation will be the one that encapsulates the borrowing. Indeed, if this correlation is equal to zero, this means that the posterior of the first group will not depend on the data of the second group. And so essentially we have no borrowing, we have no communication. We have no bordering, we have no communication between the two groups. Moreover, we will also assume that all the other parameters, and especially the common prior mean mid-zero, are fixed somehow. And we will see how they will affect the borrowing and how they will affect the correct level of the correlation. And after this, that we will try to understand also what this rule represents and actually who. Actually, what did that measure in the end? So, just to give you, just to make you understand why we consider exactly this specification, well, the point is that this model is conjugate, so it is very easy to make computation. Indeed, we obtain that the posterior distribution of nu is again a bivariate normal distribution, and in particular, the posterior mean for the And in particular, the posterior mean for the first group, as I recall it, is the one we are actually interested about, is given by a linear combination of the average of the first group, the average of the second group, and the prior mean mid zero. So this is very similar to the classical update of the normal-normal model, with the addition of the mean of the second group that explains how the borrowing of information is introduced in this very simple setting. In this very simple setting again, notice that if the correlation is equal to zero, then it means that there is no dependent between the two groups. And so in this case, B will be equal to zero. Moreover, notice that B and C need not to be positive. And this means that the sign of the correlation will have an impact on this estimator. In particular, if the correlation is positive, If the correlation is positive, so that we have prior positive correlation between the two means, then the posterior mean will be closer to the mean of the second group to the average of the second group relative to the prior. So in a way, we are getting closer to the second group. Instead, if this correlation is negative, then the posterior mean will be farther from the mean of the second group relative to the prior. So this clearly says that second... clearly said that setting the sign of the correlation actually makes explicit which kind of borrow you will have. This case corresponds to reinforcement, this case corresponds to repulsion. So the sign of the correlation will play a crucial role in the problem. So just to make you understand a bit of what we will study later, assume for a moment that we place a prior on rough of Prior on raw, for instance, a uniform distribution, and moreover, we sample some data from the well-specified case. So, we consider independent group with Gaussian curve. Well, the question is, how will the data choose the correlation? It may seem natural to think that this correlation would be essentially close to zero since the two groups are independent by assumption. However, it is interesting to notice that this is not the case. Indeed, in this course. Indeed, in this graph, you can find both the posterior mean and the posterior maximum of the correlation when the mean of the first group is equal to 10, the prior mean is equal to 0, and the mean of the second group goes between minus 15 and 15. And notice that correlation not only is almost never close to zero, but rather it follows very closely the value of michu. In particular, if michu is equal to 10, so it is. If mi chu is equal to 10, so it is very close to the true value of the search group, then the correlation will be very close to one. While if mi chu is close to minus 10, then the correlation will be close to minus 1. So this intuitively tells us that this correlation, this borrowing, does not measure a true dependence between the groups, but rather it measures a kind of functional dependence between the observation and a functional dependence between the distribution that are associated to the observation. Distribution that are associated to the observation. So now we would like to study this phenomenon from a mathematical perspective, and we would also try to understand why the data choose exactly this configuration. In order to do so, we consider a risk-based approach. So if you consider new one to be a generic estimator for the mean of the first group, then we will use the classical mean per error since, and this is not work since we are dealing with allocation parameter. And notice that here. Parameter. And notice that here we are taking the expected value with respect to the data of the first and the second. Instead, if Q is a generic estimator for the density of the first group, we will consider the expected Kullbach-Leibler divergence computed with respect to the true distribution of the first group that we have denoted by B. So, this is essentially the way in which I have decided to rank the estimator. And now our simple setting with our simple... Our simple setting with our simple parallel specification, essentially the estimator for the mean will be the posterior mean, nu one star, while the estimator for the density would be the predictive distribution. And in this case, everything could be computed explicitly, and the predictive distribution would be a Gaussian distribution itself. So, now, if we had to give an informal description of our main result, it would be the following. If you consider Follow. If you consider any choice of the sample size of the two groups, then the case without borrowing, so meaning zero correlation a priori, then this case is optimal in terms of the church that I have shown before, only if either the prior mean mid-zero is equal to the mean of the first group, either the prior mean mid-zero is equal to mitu that is the mean of the second group. We will try to understand why this. We will try to understand why this happens, but for the moment, I just would like to make you understand why those two cases may be peculiar. Well, if mi0 is equal to mi1, essentially means that the prior is already purposely specified since the prior mean is equal to the true value. Then it is quite clear that it is difficult to improve this estimate. If instead mi zero is equal to mi true, then it means that the second group contains essentially the same Contained essentially the same information of the prior, so it seems difficult that it should help our estimate. At least it seemed to me that we would simply add some variance. So now, how to make this theorem formal? Well, the idea is that we are on a simple first thing where everything can be computed analytically. And in particular, we can compute the risk and we actually can compute the derivative of the risk with respect to the correlation. And it is clear that if And it is clear that if the derivative when a rho is equal to zero, so when there is no borrowing, if the derivative is different from zero, it means that just by adding a little bit of the correlation, positive or negative, it depends on the k, then we could have a smaller risk. So the only cases in which the number of in k can be optimal are the ones in which this derivative is equal to zero. The derivative is equal to zero. And actually, we proved that this happens if and only if, again, mid zero is equal to mu1 or mid zero is equal to mi two. So essentially, we are saying that if you want the number of in k to be optimal, then at least you need to have that the pressure is perfectly specified for at least one of the two groups. And this is not even a guarantee that the number of ink will be optimal. Will be optimal. Moreover, we have also proved, which is the direction of the correlation that may lead to the great benefit. And the thing is that this derivative in zero is smaller if negative, and this means that we can reduce the risk by increasing the correlation if and only if this inequality holds. And this inequality is actually connected to a very simple geometrical interpretation that says that if that tells that if that if the true means me one and me two are on the same side relative to the prior so both on the right or both on the left then the suggested correlation is positive instead if the true means are on opposite side relative to the prior mean then then you should set the correlation to be negative so once again it seems that this correlation does not describe the actual dependence between The actual dependence between the data, but rather it helps you improving the estimate, connecting the similarities between the two distributions in a way. So notice that this theorem is not telling us that we have an estimator that dominates the number of in k since we would need to specify how to choose the correlation. We will talk about it later. But still, it tells us that essentially in any case, That essentially, in any case, apart from two very, very, very specific cases, then adding some degree of correlation that will not depend on the data, but only on the two values that are, of course, unknown, then this tiny, at least also with a tiny amount of borrowing, you will make a better work than the non-borrowing case. And this could seem striking since we have independent groups. Since we have independent groups, but again, we are saying that the correlation measures the functional dependence between the groups. Moreover, notice that this does not depend, that this does not depend on the data generating distribution. So you can have two normal distributions, or you can have two very different distributions, provided that the variance is finite. Then still you will have that some degree of correlation and of borrowing is beneficial. So, but then how? So, but then how and why this happens? So, I'm curious to understand what it is that the borrowing does that makes the estimate better in a way in terms of both mid-square error and pullback liability. Well, we can interpret it in two ways. If you think about it with a Bayesian perspective, then you can see it's borrowing at a kind of prior correction. Indeed, if you consider the marginal model for the first group, so So, before observing any data, then you get a simple normal normal model with some prior, with the normal prior with mean mid zero and variance of zero squared. Now, now assume you only observe data coming from the second group, but still you can compute the posterior of the first group conditional data of the other group. In this case, you still have a normal-normal model with updated mean and value. In particular, the mean will be a linear combination of the average of the second group and the prior minimum zero, while the variance will be always smaller than the original one. And this explains why if rho is correctly specified, and we'll talk about it later, then why this should be beneficial. Indeed, if rho is correctly specified, then this posterior mean will be closer to mu1, so to the true value, and will be To the true value, and we will be more informative since we will have less variance of the prior. Moreover, this also clarifies why we have supercooler cases. Indeed, if me zero is equal to mi1, this is the only case where we are actually introducing some bias since we are moving from the true value. Moreover, if mi zero is equal to mi true, it means that you are roughly at the same distance as before, but with less balance. Than said before, but with less variance, and it may not be very good, especially if mu2 is very far from me one. So, so just to give a little intuition, this is the pictorial representation of this effect. The blue line corresponds to the prior distribution that has mean medium. Instead, the red line corresponds to the update to the data of the second group, so the data coming from the other group. And notice that the posterior mean will be close. That the posterior mean will be closer to the true value one, that is what we wanted, and the variance will be smaller. So, we are more concentrated on a value that is closer to the truth in a way that the risk is always smaller if rho is correctly specified. However, notice that if rho is not correctly specified, and especially the sign of the correlation is not correct, then it means that the new update, the shear represented by the green one, may be much. By the green one, may be much much worse than the original prior. So, this means that the borrowing may be very beneficial, but it can also be very dangerous as well. Instead, if you consider a more classical and frequency perspective, then you can think about this borrowing as a new kind of penalization. Indeed, I recall that if you think about the classical normal-normal model, which mean zero in the prior, then the posterior maximum actually posterior maximum actually corresponds to the ridge estimator that minimizes this sum. And notice that the prior acts by adding essentially some penalization. So if you consider our case, then conditional on the data, also the posterior mean will minimize some formula. It may seem rather complicated, but if you assume m2 going to infinity, just to have some intuition so that we have infinite information on the second group. infinite information on the second group, then this is the formula that it will minimize, that is quite simple. And now that also here, rho essentially has a two-fold effect. Indeed, if rho is different from zero, then it means that here we will have a greater penalization that will lead to less variants of the estimator. And if rho is correctly specified, then this squirrel term should Then this squared term should reduce the bias. So essentially, the correlation tries to work both with the variance and with the bias of the estimator. So now there is a big question mark in this result that is how to shoot the correlation because I have always assumed that you are able to select a suitable value for it. However, this in general are very difficult, and I have shown to you that the correct value of rho will also depend on the value of the prior minimum zero. Prior mean with zero, so this also becomes even higher. So, especially from a bridge and perspective, it makes perfect sense to just put a prior on the correlation, for instance, our scale beta distribution. Then, if you consider the settings, it's quite simple to divide a gibbed sampler for sampling a posteriori, but also we could simply compute the maximum posteriori by simply minimizing this formula, that is the sum of square minus. That is the sum of square minus the likelihood, minus the local likelihood, and we can minimize it with respect to me one, me two, and rho, and then keeping me one at the point estimate for the mean of the square group. And if you do that, and for instance, you start by simulating the data from normal distribution, so this is the well-specified case, then you get interesting results. So here again, we are assuming that mu1 is equal to 10, that the prior mean is equal to 0, and that mu2 goes. And that Mitsu goes between minus 15 and 15. In this case, the left-hand side just depicts the correlation that I have shown to you before, while the right-hand side depicts the performances of both the rich estimator in red and of the borrowing estimator, so with a prior on the correlation in blue. And notice that this graph depicts the gain in mean square error relative to the sample mean that. Error relative to the sample mean that is the maximum likelihood estimator. So, in this picture, the higher the better, since we will have a lower. And it is clear that the blue line is essentially always higher than the red line, meaning that the borrowing cave where the borrowing is now chosen by the data essentially always outperforms the cave without borrowing. And notice that the benefit is the highest when me two is close to 10 or minus 10, that is close to me one is equal to one and minus one where the optimal values for the correlation are effectively one and minus one moreover you may think that this happens because because we have shown we consider normal data that it is actually the same that is actually the same likelihood that we have shown them for the model instead if you consider distribution with much heavier tails like shift disturbancy distribution with two degrees of freedom then again you essentially have the same draft Again, you essentially have the same drafts, and it seems that the borrowing case outperforms the other one. And moreover, here we have considered a smaller sample size for a technical group. Moreover, just to end up this part of the presentation, as for now we have looked at continuous data. But in general, you may be interested in looking at binary data and counting data. And so the question is. And counting data, and so the question is: How can we actually generalize the thrifty simulation for this non-continuous case? Well, the simplest thing to do is just to consider nu1 and nu2 to be parameters of a generic likelihood F, and then you can transform nu1 as desired. And this way, through a logic transformation, we can have binary data, or the exponential transformation, we can consider counting data, and so on. In this case, still the target distribution is. Still, the target distribution is given by bivirect normal, so essentially we have the same interpretation. In this case, we do not have a conditionally conjugate model anymore, but still we can compute the posterior maximum similarly as before. And essentially, we reach the same conclusion. This is the case of binary data where we have assumed a true probability that is equal to 0.7, and we let p2 go between 0 and 1. Here, here, instead of the Here here instead of the mean square error we consider the pullback-Liber divergence since we are dealing no more with location parameters. Notice that again the correlation will go between minus one and one and that the blue line is always higher than the red line. So this means that also if you consider binary data for independent groups, again I want to recall that here you're considering two streams of zero, one that are independent one from another. That are independent one from another, if you consider the borrowing, then it is essentially always better, at least according to the simulation. Moreover, if you consider counting data, then still you have the same interpretation, you have that the correlation goes between minus one and one, and that the blue line is always higher than the right one. And notice once again that this benefit is highest when the true value of the rate of the second group is close to the true rate of the second group. Is close to the true rate of the first group, that in its case is equal to 10. So, once again, it seems that adding some sort of borrowing is essentially always beneficial. However, I must see that we have not been able to the moment to derive the same theoretical properties for those two models. Since we do not have the connugging, it's much harder to compute the risk and actually to compute the derivative as well. So, just to wrap up, we have shown Just to wrap up, we have shown that many Bayesian models of interest and many Bayesian problems of interest can be described using trees. And therefore, we have shown a specific generative model for trees of random probability measure. And we have seen that this construction actually leads to great analytical trustability, leading to many prior and posterior properties available thanks to the great trustability of completely random measures. Completely random measure. Moreover, in the last part of the talk, we have shown that the borrowing information, at least in a very simple scenario, seems always to be beneficial. And this is something that was very interesting to us. But still, there are many things we would like to do. We would like to define your tree going beyond normalized measures, for instance, considering polio trees, or moreover, constructing a tree based on treatable covariance. Finally, we would like to extend. Finally, we would like to extend our result for the borrowing of information for more general models, so no more only conjugate model, no more only parametric model, no more only issue groups, and so on. So, this is some bibliography, and thank you very much for your attention. Thank you very much, Filippo, for your talk. We have time for a couple of questions. A very nice talk. So in your first, very nice talks, in your first talk about the tree-aligned random probability measures. So in principle, I guess you could even carry out inference on choosing the structure of the tree, right? Because you can carry out, like everything you told us about. Like everything you told us about was assuming you know the structure, right? Like for a given hierarchy. Yes. But you could consider two alternative hierarchies and compare them, right? So that you can. Yes, it is something that we could do and actually have thought about it. I mean, for the moment, we have considered the simple case where we assume that the hierarchy is fixed at the beginning, but still we're... Fixed at the beginning. But still, we would like to consider cases in which you can actually generate the structure of the tree itself and not only the case in which you have a fixed tree that you may have in some applications. So absolutely, yeah. Hi, Filippo. This is Luis. Regarding your first talk in the construction of the trees, do you have any results such that the final branches, Branches. So they remain having the same or known normalized random measure because everything is conditionally hierarchical. But the induced marginal distribution for the ending branches, ending dots, you might not know the distribution, right? Yes, I mean, this is something that happens in general if you consider a hierarchical structure. I mean, there is a specific case, if you consider the Indeed, if you consider the normalized table process, then if you consider it at a building block and you contact a hierarchy of normalized table processes, then in general you will have again a normalized table processes with updated parameters. But in general, you cannot derive explicitly the marginal distribution of the nodes, and this is the reason why you need to consider this conditional structure. I completely agree with you. I completely agree with you. Hi, I have a minor question about a potential connection with Redford-Neal's directly trees, which I saw about 20 years ago, where, well, the tree was built on exploring the data rather than fixed from the start. So do you think there is a possible connection or? A possible connection or um can you just repeat the structure of this model that I have not seen? I would say uh well not not readily no it's uh it's an evaluation of uh it's a branching process that that goes over zero and and defines a cluster on the go and then given data you you can find the posterior distribution of that directory okay um I mean did it did I mean, there is clearly some connection that you would like to exploit, and you have also thought about how to apply this in cases where you normally apply branching processes. But it is something that we have not really explored up to now. But I completely agree that it's a structure that seems to be quite similar. And it would be nice to see whether there is some connection that is, I mean, more precise than the intuitive one that I'm telling you. Thanks. M I see that there is a question from Andrei. Yes. Very short question. It was an interesting presentation, but this structure on borrowing that was also in previous presentations reminded me on the work of Geneva Allen and some others on beta integration. They used mixed-chain graphical models. Could this be related? Models. Could this be related in any case or help in any case? Possibly. I don't know. I'm not sure, but just asking. Thanks. Okay. Thank you very much. I'm not completely familiar with that work, but surely I will look at it. And clearly, this is a kind of structure that seems natural, since, in a way, this appears any time you have some phenomenon when, in a way, you need to make choices. And so, for instance, if you could. Choices, and so, for instance, if you consider if you consider sequential retrieval processes. So, so, so, so, thank you very much for the reference. I will surely next. Thanks, I think we do the very same. Thanks. Thank you. Okay, thank you very much, Filippo. And it's time to move on to the next talk. Thank you very much. Thank you. Can we thank the speaker again? 