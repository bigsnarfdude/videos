I'll be like a feature of course. Yeah. Sure. No problem. As you wish. Ah, here it is. We'll continue in the morning session with Jessica Min and she will talk about homogenization of the invariant measure for non-biological addictive weight. Thank you so much, Elena, for the introduction. It's really so great to participate in this celebration of TEMO. Of Timo. Timo was an extremely influential person in my career. When I was a postdoc at Wisconsin, him and the other probabilists at Wisconsin, they really were amazing role models. And they taught me so much and they offered me guidance every step along the way. So thank you, Timo, for setting the best possible example and for welcoming me into this wonderful community. So today I'll talk about homogenization, the invariant measure for non-divergence form elliptic equations. Non-divergence form elliptic equations. So, this is joint work with my collaborators Scott Armstrong at NYU Courant and Ben Fairman, who is currently at University of Oxford, soon to be LSU. Okay, so it's great that I'm coming after Attila because he did a lot of the background setup for me. So, in Attila's talk, we had the addition of an extra Hamiltonian. In this talk, we're not going to have a Hamiltonian. Okay, so we consider trace. See? Oh, Frayner, I think you were, you know, a master at. Oh, here we go. Here we go. We have minus trace of A of X D to U and the parabolic counterpart. So I add a UT term for this guy. Okay, so this A is going to be the symmetric matrix value function. And I'm going to think of this as an element of a set of admissible coefficient fields. Okay, so what I mean by that is I define omega to be the set of coefficients A. Okay, so these are matrix valued. These are matrix-valued. Okay, where I'm going to ask here, I have a uniform ellipticity assumption. So I'm going to say that the A's are sandwiched between little lambda times the identity and big lambda times the identity. And I also put a regularity assumption on this. So I just say that they're holder continuous. Okay, so I'm going to say, I allow you, every realization is some choice of a coefficient that does this. Okay. I'm pressing down. I'm really pressing down. That's also not moving. That's not signing it. Signing it. Oh, I don't miss. Did you pay for this? We made a sense of click instead of it. Now it's pretty cute. Let's see now. Okay. Perfect. Okay, so we're going to make this random by putting an appropriate probability measure. Probably an appropriate probability measure P on this set. Okay, so if I want to throw a probability measure on this guy, I need to find a sigma algebra, right? So for every Borel set U contains inside of Rd, I'll think of the sigma algebra generated by random variables defined on that domain. Okay, so f of u corresponds to this particular sigma algebra. And so I'll just define sort of the overall sigma algebra that I'll work with as f of R D, okay, and then equipment algebra. And then, equipped with the sigma algebra, I'll now assign a probability measure p to this space. Okay, and okay, I'm gonna need to discuss this measurable translation operator later on in the talk. So we have tau sub y. Okay, so I think of this as a way of translating in the probability space. Okay, and I just do that by a canonical push forward. Okay, so tau y A, right? So A is some realization of my environment, and so tau y A evaluated at X is just given by this transition. Okay, so Okay, so the assumptions on the environment are going to be one that P has Z D stationary statistics. Okay, so we saw this in the last talk. So that's just to say that I should see somehow the equal probability of seeing events, which are, you know, up to integer translations. And then the other assumption I'm going to put, because I'm interested in these quantitative problems, is that P has a finite range of inference. Okay, so what I mean by that is if you give me two borel subsets of R D that are separated by distance. Rd that are separated by distance, let's say, at least one, then the corresponding sigma algebras have to be independent of one another. So in Attila's talk, he assumed that we just had stationarity and arrogantity, which is some of the minimal mixing assumption that you need in order to guarantee that you expect some type of averaging. Okay, but here he assumed something more precise, right? So in a discrete environment, these two would just be like the assumptions of IID. Okay, so we've seen now that what's homogenization? So we've seen now that what's homogenization of the PDE? It's about studying rescaled solutions of PDEs. So now I imagine that I have an A, an omega, so a realization of this guy, and for every epsilon bigger than zero, I now study this associated problem. So here I pose it as a Dirichlet problem. Okay, so I think I'm solving the PDE on the interior subject to some boundary condition. Right, so you notice here that before, if A had somehow a range of dependence one, when I A range of dependence one when I perform this rescaling, A now has a range of dependence of size to epsilon. Okay, so in this type of environment, the homogenization statement, which we've seen already, is that there exists a deterministic matrix A bar, okay, such that for almost every realization, A, we expect that the U epsilons converge to U uniformly in this domain capital U, and U now solves, as Attila said, a homogeneous deterministic PDE. Deterministic PDE, which should somehow represent the average behavior of the system. Okay, so this is a type of punched result because I'm trying to show that for almost every realization of the environments, I still witness the same effective behavior in the limit. Okay, so you know, here I describe the problem as being a rescaling, but of course I could also say, well, maybe I should just make my heterogeneities at unit scale and instead consider problems on very large domains. It's completely equivalent. Equivalent. And the main challenges are: well, how do I identify the A bar? Okay, so the A bar plays the role of the H bar that we saw in the last talk. And how do I prove that this convergence of U epsilon, prove this convergence of U epsilon to U? Okay, and as we saw in the last talk, what's sort of the right PDE approach to this problem? If you've ever seen a talkative homologization, maybe you've heard of these correctors. Okay, so the typical onsatz is to say, well, So the typical ensance is to say, well, I expect that u epsilon should be approximately like u plus some correction term. Okay, so we call phi in this case the second order corrector because I have a second order equation. And okay, if I were to formally follow through with this enzatz in the way that Attila did, then I see that, well, if I just like stick it in, then I expect that once I plug it into my equation for this guy, I need some equation to be satisfied. We might call that the average. To be satisfied, we might call that the corrector equation. And we need that this epsilon squared phi somehow goes to zero almost sure. Okay, so this kind of gives us a hint of saying like, well, if I have to build the a bar, then maybe to build the a bar, I can, you know, try and understand solutions of this corrector type thing and get the right things such that I see this value. And indeed, that is really the typical approach that we've seen in these equations is that, well, maybe I can't. That, well, maybe I can't build the corrector, but I build some type of approximation. Okay, so I can study an appropriate approximate corrector, which will allow me to identify both the a bar and prove the convergence. So I could do both in one full sweep. Okay, so the construction a bar in these techniques, in fact, is often variational, which means that I don't have an explicit representation formula a priori. Again, that's to say that that was sort of the history of this problem coming from the PDE approach, because in fact, The PDE approach because, in fact, it is a problem that a study for fully nonlinear equations for which we had to use some variational methods to analyze that. Okay, so just some references on this general qualitative homogenization. I refer to the work of Kozlov and the famous book of Zhikov Kozlov and Olanik in the linear setting, as well as for the fully non-linear problems, the work of Caparelli, Sega Babies, and Huang. Okay, but this talk is really about quantitative homogenization. Is really about quantitative homogenization. So, given that we're interested in p-almost sure convergence of u epsilon to u, we now ask, well, for what functions f and g do I see a rate and measure, right? So, and of course there's some interplay between these two functions, right? So I'll maybe call this guy here the size of the fluctuations, okay, as I think about like, oh, what's the typical size of a fluctuation when I look at these graphs? And then I ask with what probability do I see a fluctuation of that size, okay? Okay, so just a long list of references in the linear setting that goes back to the original work of Yurinsky for fully non-linear equations that was studied by Caprilli and Suyonides in this elliptic setting. I also mentioned a work of Armstrong and myself for linear equations which relied on different techniques than what I'll talk about today. Those used some concentration inequalities. And maybe the starting point that I'll say of this project was a work of Armstrong. Of this project was a work of Armstrong Smart for applied for the fully nonlinear case, also for the linear case. But before this work that I'll speak about today, some of the best known result for under these hypotheses was, well, there exists some alpha in 0, 1, such that the fluctuations are going to be algebraic. But then here I have a very good bound on this stochastic integral group here. So with extremely Okay, so with extremely high probability, I see that these guys are actually with an epsilon to the alpha button. Okay, and so I wrote something like this here: e to the minus epsilon to the minus d minus. Okay, so what I mean by that is that, well, for every p between 0 and d, I have a bound like this. And again, I emphasize this is for like a Dirichlet problem. Okay, so I'm going to solve the problem on the interior domain subject to some bounding condition. Okay, but. Okay, but in fact, the first stochastic monetization result doesn't come from PDEs, it really comes from the work of Papua Nikola Lamberto. Okay, so here we'll consider xt, a stochastic process evolving according to the SDE. So dxt is equal to sigma of xt dwt, where wt is a Brownian function. Okay, so the sigma is going to be an R D into R D function, which is given, which is holder continuous, and we're going to Or continuous, and we're going to assume that we have this uniform non-degeneracy condition. Okay, so now I imagine that this sigma is somehow my realization of the environment, and the hypotheses that I put on it are in terms of some regularity, like what I had before, and a condition that looks, you know, mysteriously similar to what I had when I defined the space capital omega. So, indeed, if I just define, oh, oh, okay, okay. Command Lika indeed if I just define A to be one half sigma sigma transpose, then the infinitesimal generated of this process is exactly given by this map phi maps the trace of A d2 phi. Okay? And this is my non-divergence norm elliptic equation. Is my non-divergence norm elliptic operator that I was studying before? And I'll now denote PA to be the probability measure that's associated to realization of this guy. Okay, so now I have randomness of, you know, that comes from the SDE, and I also have randomness which comes from the environment. So I have two levels of randomness now in such a model. Okay, and okay, maybe, you know, it wasn't called stochastic homogenization, but the desirable statement. But the desirable statement was what's known as a quenched invariance principle. Okay, so the goal is to say, well, for almost every choice of coefficients A, okay, so here I'm indexing A by sigma, all right, so for almost every realization in the environment, the statement is that the rescale process, okay, given by this guy here, should converge in law under this PA measure as epsilon tends to zero to a Brownian motion, which now is covariance described in terms of this A bar. In terms of this A bar. Okay, so somehow the way in which Papaniquo invariant stated this result was for these quenched invariance principles. A bar would be... A bar is exactly the same A bar that I talked about in the homogenization setting. That's not explicit. So I'll get now to the argument of Pop Nickel and Verno, where they give a semi-explicit representation of that. Yes. Is sigma a random? Yes. Is sigma random? Yes, sigma is random here. So it's the exact same formulation of the omega that I had before under some stationarity, you know, arroganticity in the case of Papua Nicola and Veradon, but for what I'll assume today, I actually find it range. Okay? Okay, so you know, parallel to the corrector approach that I mentioned for homogenization of the associated PDEs, the idea that PDEs, the idea of the probability approach is really to say, well, we just need this ergonic theorem of the coefficients. Okay, so the formulation is in terms of what's known as the environment viewed from the point of view of the particle. So what that means is that instead of studying the path on physical space as diffusions in Rv, the approach is to say, well, we should actually lift up into the probability space using this tab. Okay, so I'm going to say now I think of a diffusion occurring. Now, I think of a diffusion occurring in omega. And what's nice is that A of X is going to be a Markov process under this PA measure. And so what that means in particular is if I look at the characteristic function that's associated to this Markov process, okay, so I can argue that this has to be a martingale. Okay, and once this is a martingale, I actually get that this quantity here is always the same as. Quantity here is always the same as what you get when you evaluate it at time zero, and that's just going to be one. Okay, so I have this property that for every t, I know that this quantity here is equal to one. And so the observation of Papanikolo and Varadon, sort of parallel to the corrector equation that you can do everything a willful swoop, was that, well, all you need to do is you need to identify an A bar such that for almost every choice of A, okay, and PA almost surely, so let's say. And pa over surely, so let's say p cross pa over Shirley, you need to see that this guy here, the limit as t tends to infinity, converges to some new number, you know, a bar ij, let's say. Okay? And if you could do that, you could replace this entire thing here by a bar ij, move things around, and you see what? You see that the characteristic function of this rescale process here converges exactly to this characteristic function here corresponding to the normal. Okay? The normal equation. Okay? So parallel to the corrector equation, sort of the heart of the argument here is to really be able to prove this ergodic theorem on the corrections. Okay? So if I want to establish an ergodic theorem, well, I need to find an ergodic invariant measure mu, let's say. So what that means is I hope that I can find some candidate mu such that I see that this average here converges to exactly expectation. Exactly, expectation with respect to this invariant measure. And this invariant measure mu, right? So, this guy should somehow live on omega, and I expect that it should be mutually absolutely continuous with respect to my probability measure script dp. Okay, so when I think of it like that, in particular, we seek some function n. Again, this is going to be like a random variable, and I think of this as being the density with respect. This is being the density with respect to dp. Okay, so, and in that case, we now have a nice explicit representation formula for the a bar in terms of this invariant measure. So, in this particular case, we can see that the a bar should be nothing more than a integrated against this invariant measure. So just some references on that from the probability perspective. I mentioned that it's the original work of Papanigo and Veradone, which was for diffusion processes, but this problem has also had a lot of developments over the last few years. Had a lot of developments over the last few years in the discrete setting. It goes under the name there of a balanced random walk in random environments. Okay, so I referenced the work of Greg Lawler for the qualitative argument of that. And for quantitative results of that direction, again for the discrete setting, I mentioned the work of Guo, Peterson, and Tran, and also some recent results of Guo and Tran. Okay, so now motivation for what I'm talking about today. For what I'm talking about today, which is somehow unifying the PDE and probability perspectives. So, the typical approaches using PDEs do not identify a bar using this invariant measure. It's by this variational approach. And well, since this model is non-reversible, the invariant measure does not have an explicit representation formula. Okay, so even to the work of Pop Minik and Lamberadon, the construction of the invariant measure is done by some approximation procedure. So, if you really want to compute Okay, so if you really want to compute things, how can we go about doing that? And what is that invariant measure N in this PDE setting? So it somehow appears in the work of Papua Nickel and Veradone, and we didn't understand it very well, so coming from the perspective of PDEs. And just more generally, how can we improve our understanding of these two methods, maybe individually and globally, to promote more collaborative approaches on this topic and to better understand this procedure? To do this procedure. Okay, so this is all motivation for what I'll talk about today. Okay, so Markov diffusion processes are completely characterized by their transition probabilities. So in the language of PDEs, that means that I need to study this fundamental solution, or another word for the fundamental solution is what we call the parabolic meaning. Okay, so for each realization A in omega, we consider, well, I studied. We consider, well, I study this backward Kolbogorov equation where I insert the realization in my environment here, A. And so I start from an initial condition, which is this 0. So I'll call P T X Y. So P is really indexed by a choice of A here. And similarly, well, you know, if I'm in a regime where I have homogenization, I can say that, well, I know an A bar exists. We know various ways we can compute it. So I can define P. So I can define p-bar to be the parabolic Green's function associated to the A-bar guy. And notice here, right, A-bar is just constant in this case. Okay, so here I had a variable coefficient. I've suppressed it for the purposes of notation. But A-bar here is constant, so this guy down here is really just the heat equation. Okay, so I know exactly what the P-bar is. I know it's a Gaussian. Okay, so the thing that we were interested in. That we were interested in is there's been a lot of study in quantitative homogenization about what properties of the limiting equation do you get to maintain in the context of homogenization. So the question we were interested in is, well, we know that the homogenized constant coefficient equation preserves mass. So we know that the Gaussian Os integrates to 1. But what about the fundamental solution corresponding to sort of every realization A? Okay, so what happens to the mass? Okay, so what happens to the mass in that process? So, a priori, when you have a variable coefficient, it's not true that you preserve mass. But if I know I'm in a homogenization regime, maybe I can hope that I can lift this property somehow. Okay, so do we witness effective mass conservation at large scales? Okay, so now I'll choose the first main result. Okay, so here we first stated it in terms of solutions of a Cauchy problem. So we're going to Of a Cauchy problem. So we're going to say, let V solve, right? So here I just have the solution of, you know, that Cauchy problem for this realization A. And let's say I start from a deterministic initial condition, but I tell you that that initial condition is tented from above by a Gaussian. Okay? So I'm like, you start tented above by some Gaussian which lives on length scale r. Okay? Now the statement is that, well, there exists some exponent gamma in 0, 1. Gamma in 0, 1, okay? And let's say a random variable, script Ey. Okay, now script EY is going to have these bounded stretched exponential moments, okay, for all p strictly less than d. So this is just to say we have really good bounds on what is the size of the script dy. And now I tell you that, well, if you're R, you know, if you're living on a length scale bigger than script EY, okay, so this is a quenched estimate, okay, so for every environment, I know exactly what script EY is. I know exactly what script Ey is, and I live sort of on a length scale larger than that. And I tell you that t is bigger than r squared. Well, the point is that we have a random constant. Okay, so we call that c of v naught, such that your solution has to look like c of v naught times a Gaussian. Okay, and with what? With the rate of convergence. Okay, so I'm saying that point-wise, you see that the graph of V of Tx is very close to a constant. Very close to a constant times p bar. Okay, so I have this Gaussian here, I scale it by this c of v naught random variable, and I say that the profile of this guy is very close to a constant times the Gaussian with a super good rate of convergence. Okay, so I can capture sort of exactly how big the size of this is because you see here I've got a Gaussian, but t is bigger than r squared. Okay, so I have a very small error in that size. And what is this c of v naught? It turns out that this. Of V naught, it turns out that the C of V naught is nothing more than the limiting mass in some sense of your solution. Okay, so part of the proof is to show that this limit exists. Okay, so this is a V of T x looks very close to some constant multiple times p bar, okay, and this guy here is random. Okay, so this is a quench statement. And you might ask me, I just repeated the estimate up here, you might ask me, so how do you relate C of V? might ask me, so how do you relate C of V naught to V naught? Okay, so it turns out that if your initial condition has just a little bit of regularity, so as long as it's holder continuous, this big ball, then you can actually estimate how different this is from the initial mass. Okay, so here it's like, well, you know, this is, let's just say, all small, okay, and you basically have an algebraic error there. Okay, so this is kind of reminiscent of the classical. This is kind of reminiscent of the classical fact that, like, under certain hypotheses, if you study solutions of the heat equation, they always, as time goes to infinity, they always asymptotically converge to the parabolic green function weighted by the initial mass. Okay, so in general, when you solve these Cauchy problems, you sort of always see this fundamental solution appearing in the long time limit. It's just that here, I'm not re-weighing by the initial mass, I'm re-weighing by something approximately the initial mass. And I just want to emphasize that these results, they really become deterministic for r bigger than or equal to the script dy, and we do have very good bounds on that. So everything here is punched. Okay, so that's like a very like a PDE sounding result. And now let me see if I can describe that in terms of the parabolic green function. Okay, so I go back to this random variable scripty y. Okay, scripty y has super good. Okay, the script dy has super good, you know, bounds as to its size. And now I say that, well, for any point little y in the unit q, I know that there's a positive random constant little m of y, such that if I go t long enough, okay, so I just have to let t flow long enough, then this, you know, parabolic green function of the random guy is very close to this random constant times p bar. Again, with a rate that's With a rate that's Gaussian and even better. So, this we can think of as some type of local limit theorem. Because I could have divided both sides of the equation here by m of y, and I would see that p over m of y gets me something close to p bar. But again, I emphasize that we have a rate of convergence for these problems. And okay, I stated that for y in the unit q, but really I can do that for any y in our unit q. And this is a type of homogenization result that says: well, if you look at this parabolic read function, this fundamental solution, as t tends to infinity, it doesn't converge to p bar, which is what you might hope and expect. You're like, okay, somehow like the random guy should just converge to the corresponding deterministic homogenized counterparts. Here, you need this correction, and this correction exactly takes into account this mass preservation issue. Okay, so you're starting from. Okay, so you're starting from something that doesn't preserve mass, and you're homogenized to something that preserves mass. And what's the M of Y? Okay, so I called it M of Y for suggestive reasons. It turns out that the M of Y is exactly a Z D stationary invariant measure. So this guy subject to some integrability condition. Okay, so what do I mean by an invariant measure? Well, you know. You know, from the perspective of PDEs, I'm going to say that the invariant measure M is going to be a solution of the adjoint equation of the non-divergence form guy. Okay, so this is going to be doubly divergence form. Okay, so you have two derivatives on this guy. Yes? So originally A is invariant under B shape. Yes. With A or D. Yes. Is there an intuition of volume or anything? Is there an intuition why you were giving something which is C D, but? Oh no, I'm sorry. My assumptions actually were only that you were Z D stationary then. Yeah. Yeah. Thank you for the question. So uh there are a lot of results in homogenization where people have assumed R D stationarity, but for these elliptic problems we can do everything just assuming V D stationarity. But if instead you assume R D stationarity? Then you could lift that to a statement of R d stationarity. Great. Thank you. Great, thank you. Okay, so how do you interpret that? You interpret that in some type of weak sense, okay? But the point is that this m of y, okay, so again, that lifts for me, this is like a quenched construction, okay? So for every environment, I build this m of y in some way. And the point is that if I lift that into the probability space using my tau operator, it's the exact same unique invariant measure that was constructed in Papua New York and Veradon. Okay, but now I can construct it so if Okay, but now I can construct it sort of using this idea of flowing for the PDE and doing it in a quenched fashion. Okay, so let me just say a few final remarks here, quantifying weak convergence. Okay, so now that we have the M, that's great. We built it up using PDE methods, but what can we say about it? So it turns out, again, going back to my random variable scripty y, which has really good moment. Really good moment balance here. For r bigger than y, it turns out that if I look at this m and I average it on a really big box, okay, so this here corresponds to the average integral on a box with side lengths r, this guy is very close to 1 with the rate of convergence. And on top of that here, well, again, since I'm doing everything quenched, in fact, I can say maybe I want to study large-scale averages of m against a. Okay, so if I look at these. Okay, so if I look at these large-scale averages of M against A, I get that this guy is very close to A bar. So we know from Papanique Law and Veridan that this guy should be essentially listed up into the probability space A integrated against M. Okay, but we do that now on the physical space and we do it in this punched way and we have a rate of coverts for that. Okay, so how do I interpret that? Well, the first result says something about, you know, what's one? One is just the One is just the invariant measure which corresponds to the A-bar guy. So, this first result here is like a weak convergence result that talks about homogenization of invariant measures. And then the second statement, again, now finally gives me a way of how do you compute the coefficients in this non-divergence form homogenization. Okay, so again, we can compute it so long as you know how to compute the fundamental solution. Okay, so it's like you take the fundamental solution, you integrate it up, you flow as t tends to. Grade up as you flow as t tends to infinity, you should eventually converge to this m guy. And now that you have this m guy, we have rates of convergence as to how you build up a bar. Okay, so I just want to say a quick remark about, you know, why we care so much about this scripty why guy and why we write our estimates in this way. So the prior estimates also demonstrate that all of our results exhibit optimal stochastic instability. So, you know, I have some statement about when R gets sufficiently big, you're When R gets sufficiently big, you're in business, so you can use Chibishev to turn that around to a rating measure. Okay, so what that means is I have some bound like this, let's say. Okay, so why do I care so much about that? So if I were to consider a random checkerboard, let's say, right, so I have a checkerboard, white squares correspond to when A is the identity, black squares correspond to when A is twice the identity, then the homogenized coefficient A bar is going to live strictly between these two dichotomies. These two dichotomies. But if I think about, well, what's the probability of deviating from a bar? Okay, it's somewhere between one and two. Well, if I think of, you know, what's the probability of deviating of size, let's say, at least a half, it can be no smaller than the probability of seeing all white squares or all black squares. And what does that guy scale like? Well, on a box of size r, okay, I have r to the d of these squares. So I see that, well, the probability of... So I see that, well, the probability of that is like one half r to the d, which gets me into scotch here. Okay, so we're writing our results in this way because we're trying to see exactly here on the right-hand side, can we maintain the speed to the minus argument? Okay, so I'll just finish on two really quick consequences. Okay, so one of them is heat kernel estimates. So as I mentioned, it's not true that when you have these variable coefficients that you look like a Gaussian. That you look like a Gaussian, but our result tells you that you definitely at least are sandwiched between two copies of the Gaussian related by this emophlogen. And then my final thing I'll end on is just to say, you know, now how do we go back to the sort of probabilistic perspective of this? Is that, well, all this stuff that we're doing, in fact, implies a rate of convergence on the ergodic theory for the environmental process, right? So I was like, the heart of the issue is we need to. I was like, the heart of the issue is we need to prove this ergodic theorem on the coefficients. So, what we're able to do is we're able to basically lift all of our estimates to some rate of convergence that you get for this ergodic theorem here. Okay? Okay. So, this is just to say that we proved the first quantity homogenization result for the parabolic green function using PDE methods. From there, we construct a unique robotic invariant measure from Papuan Equilibrium-Baradone and Quench for Ashen. Baradon and quenched fashion. We obtained several consequences, including heat kernel bounds and quenched quantitative periodicity. And what's sort of the further question is, well, you know, we have now this sort of optimal stochastic integrability of the right-hand side, but now we want to get optimal fluctuations. So all this is moving towards building a theory of universality for such options. Okay, so thank you very much. Any questions for Jessica? May I ask? You started with a Didicher problem. Yeah, right. But you told us in RD. Yeah. Any anything about Diriche? Yeah, so you can always look that Dir Schlei problem to a Cauchy Dier Schlei problem. So we needed to do that actually in our To do that actually in our paper, so we have started from this Dirichlet problem result and then we lifted it to like a Cauchy Dirichlet involving time dependence. And then from there, we use that plus this like capping by a Gaussian to be able to upgrade to solving like the full Cauchy problem. So yeah, that was a step in our funding. Yeah. So I guess like you wanted to say that you were defining like this A unified solution X T. The solution XT, and you mentioned that this limit here is actually continuous with dispute A. So, is there like a reason why? Right, so yeah, right, like you just expect that, like, you won't see, because you're measuring the environments using this script DP guy, right, like you don't expect to also be able to see this invariant measure unless you're, you know, on sort of sets that matter for script DP. sets that matter for script dp. I don't know if that's a satisfactory response. Everything is sort of done up to the choice of script DP. And so you expect that this invariant measure sort of lives on the same support as the script DP measure lives on. Yeah. Do you expect sisters to hold that you replace the ordinary Laplacian by some LP Laplacian? Um yeah, um so an L plaution may be like uh like or a P oplaution maybe not so much just because in the case of Plaution like I we lose a lot of the regularity results that we need in order to make these things work but at the same time what I do think is true going back now to Attila's talk is that I think that some of these ideas in terms of studying this invariant measure in this way In terms of studying this invariant measure in this way, there might be ways to sort of lift that to adding some drift or adding some lower order terms. You use the regularity of the A to prove this optimal estimate, but then your counterexample doesn't have the regularity, right? The not the counterexample, the test case will be checkerboard. The test case will be checkerboard. Yeah, yeah, yeah, yeah. Is it possible that it's not actually out, or is it? So I expect that, like, right, so you're right for my write-and-check report. I really need to sort of smooth out the boundaries there, but I guess I think that we, so there are some of our estimates that where we really use the modules of continuity for the A, but there are many of our estimates where we don't use actually any of the regular. Where we don't use actually any of the regularities functions on A. So I'm still sort of leaning towards the idea that I do think that this E to the minus this bounded up through P stretched exponential moment is the optimal thing for that. Let's say Jessica. Let's resume in two minutes to give people away.