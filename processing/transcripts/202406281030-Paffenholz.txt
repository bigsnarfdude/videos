Last one, it was like the highest weight factor in the conference. And I will please have Andres going to talk about polydv.org. Thanks. Okay, thank you. Thank you for having me here. Yeah, it's the last talk. It will be very gentle. I just want to more or less advertise a database for general geometric data where you don't need very specific types of queries, so where you don't need to make too many. Need to make too many connections between different artwork. You just can store some data you've computed. You want to preserve it for time, you want to make it successful to others. So, uploads the database for dispute geometric objects. This is not really a strict thing, but it came from Polymake. It now also might do OSCAR things. So, this is the type of data that we're aiming at: everything that can be handled by Polymake with OSCAR. By quantum mechanical BOSCAR. Four singular, so it's not fixed to a specific type of software, but the idea for the structure of the data may just come from these types of software systems. So it should provide a unified access. So you have a couple of collections like the Koitskaka database is in there, or smooth polytopes, or some tropical things are in there, and you have a unique interface that you can use. Interface that you can use for all of the data. So you don't have to look for a different data device, learn a new way to access it, learn new commands. You can just use the one or the set of commands you know in your software. Currently works from Polymake, it works from Roscar. In some sense it also works from Plain Python or Sage and from Julia. I'll come to this to the difference. I'll come to this to the different answer. Okay, it aims a little bit at two things. First of all, it should be easily accessible. So it should be online and you can just connect to the database, do your query, you don't have to ask anyone, you don't have to have an account or something, you can just use the data. On the other hand, it should also be an opportunity to store your data for a longer time. So if you're a PhD student and you don't know where you're going, you don't And you don't know where you're going, you don't want to put it up on the website of your university because you may leave, and then your website is down, you have to care about this. This happens quite a lot. This was also the initial starting point that we had data that was kind of vanishing, and we just barely got hold of it before the person who computed this left to industry. So, the idea is there is one place where you can put it, and we kind of care that at least it's available. Care that at least it's available and it's backuped, and there is a single address you can store this to. And it should, in this case, of course, come with full information on all the authors and references you need. So who computed this, what is the paper associated to this, where's the theory that belongs to the data, things like that. That should all be stored alongside the data. And then you have a stable reference. The data, and then you have a stable reference that you can maybe also put into your paper. I got the data from there, and it's computed by those people. And if you want to access it, use the link, and that should stay stable. Okay, and it should also make it very simple then just to play around. So, as I said, it's more exploring and then slight experimentation. If you want to do serious things, you probably need something else because then you need specialized database requests. Database requests. Okay, it started 2013 already, so it's by now ten years old, more than ten years old. It has a web front end. Maybe I'll just click on this one, once, and it opens. So this is the web entry. If you just want to click around, this is the way to go. You can also see what is in there, like the Here, like the one I often click on, is the Koitsaskake list of four-dimensional reflexive polytopes. Here we have the information about where it comes from. And if you don't want to do any sort of there's a search form and you can get some results, you can also download them. So, okay. Yeah, exactly. So, um, did I say anything? Yeah, I was thinking, yeah. Um, the idea is also it should not be tied to a specific software, so we are aiming for format of the data that is general enough so that you can read it from any kind of software. The only thing that varies is the amount of information you get with it. If you get a list of vertices, just List of vertices just as a list of as a matrix of integers, you may need more information about how to interpret. Is this rational? Is this integer, whatever? But at least you have the list and you can maybe look up the types. In other software, so it allows to specify all the types, but then you need information about the software that reads it. The software that reads it needs to read information that is stored in the database. This is the That is stored in the database. This is the right way to save. So I'll show this to you. Okay, exactly. So maybe a couple of words to coordinate different from other projects. Like there is the LMFDB. I think this is in a way it's a larger project because you have more data. But it's also what this does not want to be, it's a very specialized database. Database. You have a high amount of data with many links in between the things. And you have specialized your queries, your tables to this specific type of data. And this is not the case here. Just all plain documents that you can search for. And the format of the document is in no sense specified, except for some formal semantics. Okay, this is, and there are more databases of this kind that specialize on one type of data, one type of mathematical information. One type of mathematical information, collect this, and then provide a very specialized entry to this. It's closer to Marty, which we discussed yesterday evening, in the sense that we aim to have, where did I state this, stable references into data. So it should be a way to preserve the data for at least some time, a longer time than usual. Some time, a longer time than usually your web page on your department exists. And MARDI has kind of the same aim. It should provide easy access to existing data and provide long-term storage with references into the data. So that Marty has a similar aim. It started much later, so we were first, and then Marty came up with the same idea about two years ago, three years ago, maybe. Two years ago, three years ago, maybe. We'll see how this works out in combination. So, yeah. So, simple storage, we do not do any computations. That's also different maybe from other databases where you can submit data and they fill in the gaps and then provide additional information to make it uniform over the database. That's not the idea here. The authors provide the data in a reasonable format. Reasonable format, we would just store it for them with some restrictions, of course. It should be usable by other search. The information should be the same in all documents, so all you have content, you should have the same information for all of them, things like that. But we won't start computations in general. So it's easily accessible. What is also important, that is also part of MARTI, it should, in principle, allow the exact. Should, in principle, allow the exact reconstruction of the data. So we would not just store a matrix of integers, there must be information accompanying it that tells me exactly what type of matrix this is. Like this should be a matrix of integers. Or if you have a polynomial, this is a polynomial in one variable with coefficients in a specific ring. So that you can, from the data, get the exact math back the exact mathematical top. This is also often. Time. This is also often not the case if you have collections somewhere on web pages from people. You just put it up in your own format, and if you're lucky, you find it in the paper. If not, then you have to guess, maybe even whether it's the rows or the columns that are important in that matrix. This should not be the case. So we want to have, and I'll show you how we try to solve this in a moment, should be unique, reconstructive. And always, there's no reference. And always, there's no refereeing involved, so we don't judge on the data. I wouldn't judge on the data too much, at least that we enter, but it should come with some publication that justifies that the data is correct. So it's more like there is a paper, and if you have data, you can give it to us. It's not a contribution in itself. I don't want to check data. Okay, maybe a little bit about the background. We chose MongoDB, what LMFDB apparently left. MongoDB is different from Postgre or MySQL or MariaDB is a database that stores documents. So we just sample, if you think of polytopes, each document is a single polytope, and we store a list of these polytopes with them. Store a list of these polytopes with their properties. So it's not table-based that you say I have a table of vertices, I have a table of properties like they're normal or they're smooth or whatever. It's document-based. JSON documents? So the queries are then just text search basically in the document or something? We we just expose the original MongoDB query language. We we have some convenience functions, but in We have some convenient functions, but in the end, the query languages, the MongoDB language. Whatever that can express, you can do. It's not just text range. Yeah, yeah, of course, yeah, yeah. They're structured because it's JSON. Oh, it's just a job. Yeah, it's just a JSON, yeah. And you could search in the tree, of course, yeah, yeah. It's a JSON document. You have a structured search in that JSON document. You can specify specific fields. So there is the error polynomial, and this is maybe stored in. And this is maybe stored as a list of coefficients, and you can ask for the highest coefficient and say which documents in that database have in that collection, have at the highest coefficient this and that. But we don't write our own query language. So we have a couple of convenience functions because MongoDB language is not easy, but in the end, we just pass through what you type in or whatever is possible there. So on your DV web page, so when I query and click on something, do I just get this matrix that I cannot see pictures? I mean, how does it work? You get the yeah, sorry, if you click on one of the blue links. Or I mean, I it's a possibility. Okay, um maybe we should change this. Maybe we should change this. This just downloads the JSON. Okay, and is there a picture to see from the facets or a generation of pictures? Mostly this, okay, this is already four-dimensional, so what would you expect? That's your duty. You can download it and use whatever picture generation you want to have. But you can download this from here. You can also get it maybe as a. So there's a, maybe as a part before, there's a REST API. You can get the full JSON just over the web. I don't know, this is probably a different polytope, but this is the way the JSON is stored in there. And there's a unique address that is also hopefully not changing. So one could also use this as a reference if you have a specific document in there. This should be a stable reference to it. Reference late. Okay, but I'll come to this again. Okay, yeah, we expose the MongoDB language with some convenience functions in various packages. So once you know it's Mongo, you can just go ahead and just scrap everything we did because if you know how to program Mongo, you can do it directly. So whatever Mongo does, so there's a PHP pod, Python driver, CE, C Julia. Driver, CE, C Julia, you can just use this. You don't need to use what we provide. You need the access data, but that's public because that's on the GitHub. You can just read this out, or I can tell you. So there's a public account on this, and of course, for writing data, you need different accounts in the database. You can also have private accounts if you want to prepare your data, that's also fine, no problem. So far, we did this for the people that submitted, but it would work. So, the license is open source, so hopefully, it will stick. But even then, it's JSON documents, so we can just download the JSON documents, upload something else, it should be JSON documents. So, we're not restricted by MongoDB in the preservation of the data, essentially. Okay. Okay. Yeah, how can you get there? We have seen the web front end. That's maybe just for playing around a little bit. If you don't know, if this might be useful for you. The next level is you can, there's a plain REST API that I showed already because it was the open tab. There's an address that explains how you make up the calls. The calls to the how you make up the web address. If you want to have information, that's the standard way to do that REST APIs introduction. Okay, and one outcome of this might be this, right? It's the REST in the current, so it's REST current. We're asking for a single document by its ID, so the endpoint is ID. So, the endpoint is ID. It's in the polytopes, combinatorial, 0-1 polytope collection. So, it's a 0-1 polytope. It's combinatorial information that was done by Eichholzer a couple of years ago, 20 years ago, maybe by now. And it has a unique ID and that we can query, and then we use number 3. But you can also do file queries on this. And then you can just download, you have the JSON, but no interpretation here. Now you need to know what these numbers mean. What these numbers mean, or these strings even mean in this case, but there's nothing here. But you can also query the information over the REST API, so it's called a function. Okay, it started out with PolyMake, and I'll do my demo also in Polymake. The first one, at least, in Polymake, that was the initial application. We had the smooth reflex polytopes, they started to disappearing, so we tried to preserve them and then we. Preserve them, and then we came up with that the right idea would be a database, and then got going. That's also the most extended interface, the most convenient one for searching, aggregating, passing on MongoDB commands. Searching is some convenience functions. You can access all the documentation so you know where the data comes from, what you should cite if you use it, things like that. What is the relevant paper to read if you don't understand the data? All this comes from the data. Understand the data, all this comes from the database. And Polymet is very not the only one that has convenience functions for adding data, but this is my restriction. Okay, OSCAR by Polymake has also access to it, so it uses the Polymake access because there's Polymake JL and the one who's written it is gone. Anthony, who is part or did wrote part of this. They wrote part of this. So now, from Oscar, you can also access PolyDB with almost the same convenience functions by this path over PolyMake JL that is included on our screen. Okay, and those two also know how to interpret the data. They can read the additional information stored in a database that explains what data you actually have, what mathematical types is. I mean, within OSCAR, we already have. Within OSCA, we already have access to this graded ring database? No, the graded ring database is a different one. So, you mean that one of our CASPASIC? Yeah, so you have access to those because they're also in PolyDB. Yeah, you can access them via Oscar. All those. The four-dimensional quadska classification is in there all four million, four hundred million. 400 million of them. They're readily available. I guess they will appear later also as an example. Exactly. Yeah, so they know how to read the type information in those two systems. So you get a real mathematical object as the result of a query. There is also something for Python or Sage, which is almost the same. Is almost the same, right? And for Julia, independently of Oscar, I've just written some convenience functions so that you can have to remember the address and the file queries are put in the right form so that it works. But then you only get back the JSON. There's no interpretation. You need to read the definitions yourself and then make up something. Of course, you can come, for Sage, I can come up with some convenience functions. Can come up with some convenience functions that can read the type and then convert it to a sage type, but that is not a full conversion. You don't get back to the full option. Okay, and otherwise, of course, anything that has an Omongb driver can go there. You can also do this locally if you don't want to use our server. It's pretty easy to set up the same structure locally in your MongoDB instance. So we can also decentralize this, and you can always have a private instance. Can always have a private instance. If you just want to use the database and use our convenience functions, but don't want to make it public, just set it up on the computer and you have it. Okay. And again, queries are just playing MongoDB. So do you mean the data you should have you can get a data dump from the entire database or not? That's not automatic, but if you want to have one, I could do this, of course. Yes. And then you could just load it into yours. Yeah, yeah. But It into use. Yeah, yeah. But there's no public function that does it currently. If there is a need for it, we can do it. So it's development is on a request basis. If someone needs something, I'll think about how we can do this. If you're running it locally, is there a public-facing logo server that you can connect to? Is that what you're saying on this bottom? So the local and decentralized. If I wanted to run a copy of the web server locally, is that what you're saying that you can run it? Is that what you're saying? That you can set up it's com completely separate. And there's no, you so one could do this, but so if if there's a need, one could think about this. But currently, you could just set up a Mongo and then dump the data in. And so first initialize with the right structure, dump the data in, and then it's completely separate. So if there's a need for something else, no problem. So it's a I mean that sounds like it's gonna be a scalability problem once the database takes off, right? Yeah. Once the database takes off, right? Yeah, yeah. Currently, we don't have problems with the amount of data we think about is if it's actually growing to a size we can't handle anymore. So currently, there's still space. So currently, this is around two or three at most terabytes of data that we have to store, including backups, probably, even so this is not a problem. If this grows and it becomes a problem, I have to rethink. Because the problem I have to rethink. There should be a solution in that case. But maybe I wait until it actually need arises. Okay, more questions? Okay, just very quickly, this is what actually is in there. Currently, the one that always popped up here is this one, the reflex polytopes from Koitzo-Skarke. In my example, I'll, in my examples, use maybe there are one polytope classification and there's more. Some tropical stuff, this is pretty underdeveloped, but polymay can handle these things. But not many people are working on this currently, so not much is coming in. Yeah, we're mostly polytop people, so maybe this is the reason for this launch. Okay, some recent papers that use it, so we've Some recent papers that used it, so it's actually also used for research. And I will not come up with my own examples in the demos. I'll use something from this paper and from this, and I don't know where there's an empty item here that can go. I'll use something from this paper to demonstrate. So, and there are more below that, but I guess five years in the past. Okay. Okay. So here is how the data looks inside. So data item, this is not enough to reconstruct the data, but we store data approximately in this form. So everything must have an ID and it must come with information about the actual format of the data and we do this with namespaces and currently the only one used is Polymake, but that can be replaced by anything else. That can be replaced by anything else. So that tells you, so reading the namespace tag tells you about which software format can read this data. And also other software that can read Polymake data could check, oh, it's Polymake data, so I can read it. I need that particular deserializer and can read it in. And it comes with, okay, it comes with information where you find it. It comes with a version that is also important. That is also important because the format, storage format may change. If the software evolves, maybe we have new ideas, maybe some parts that we store, like the ER polynomial in this form, maybe we want to store a true polynomial instead. Then it's important that you have a version on your data so that the newer versions know in the old versions Earth polynomials can be in this form, and I have to rewrite this to read this actually. Rewrite this to read this actually because otherwise at some point your data becomes just unreadable because the storage format is not more additions. Okay, and then we have the data in a hopefully pretty searchable form, which for MongoDB mostly means you should almost be on the top level because otherwise queries very, very wrong. So that's why we also have to get rid of type information here, we have to pull it up to Here we have to pull it up to the uppermost level to make it searchable. You have a perfect correspondence between the format data and the version of Polymake. Yeah. It's the format. Yeah, yeah. Exactly. So this version of Polymake now knows how to read this, and all future versions, at least in Polymake, I think it's the same for Oscar, all future versions of Polymake recognize this if they read data and know how to get from 4.3.2 up to the current 4.3. Up to the current 4.9. All transformations that are necessary are stored, come with the software, and they are successfully applied. This is important because otherwise, this pretty much quickly becomes unreadable. These storage formats evolve. This is the need of new information. Okay, it's JSON, tree-like. There's one additional document we see in the next slide that then specifies mathematical type. Mathematical type. Here it's up to a certain software system that then you need to consult to really understand. If you don't understand directly from the string, what the type is, you need to consult the software, what it actually means. Okay, yeah. Creation wire namespaces, like here, that should name some software that can deserialize. Deserialize the information, or it tells other softwares if they have a deserializer for that type of data, which one they should use. The type field there, the format of that, is that conditional on what the system is? Yeah, that is already conditional on the polymer thing. So the ID, so the ID is just a name that should be unique to make it, if you want to make a reference to it. The namespace is. Reference to it, the namespace is kind of the first thing you should read. Everything else may depend on what you read, what you find there. You can also, so it doesn't currently work, but in general, you could also mix, there could be several namespaces here, and maybe then we will enhance this here so that entries here know to which software package or which data format they belong, and then maybe if they're singular and polymake, there's a part of data that can be replicated by singular. That can repeat by singular, part of the data can repeat by polymag, and if you have a system that can do both, it can read both. Can you just repeat a nsh entry then and then put the rest of the logo? No, this is an array, so you can just so and then you could just add singular with singular information so this is already present, yeah. The idea is there, but if it actually happens, we have to figure out probably how we separate. So the idea is there, but nobody ever tried. But the type is on the same level as the namespace, right? Oh, yeah. So that was what I was asking. If you can specify another namespace entry, where would you attempt? Yeah, yeah, yeah. Okay. First bug. If that should be able to mix, then maybe we would have to find a different way for that. Sure. So far it didn't occur, but I'll fix the format if it's necessary. Okay. Yeah. So, we want to have glossless retrieval. We want to store the information here. So, we have a separate document, as I said. You'll see this later. We discussed also this. We need to handle changes in the format. So, we need some kind of versioning on everything, on the database, on the data. It needs to be versioned so that we can go back and check what is the right way to do it at that time. That time. Yeah. If we're reading data, there's a real difference if you actually have a deserializer for that type of data or you just get to JSON. That makes a severe difference because if you don't and you just make up your object again, then maybe there's some initial step that takes time, checking if it's consistent, things like that. You bypass if you have a full DHC realizer for that time. Of the data. And it should be somehow normalized to data. We don't want to see sparse matrices or something like this here because that makes it unsearchable. If part of this is sparse and the rest is dense, then it's not searchable. Okay, so here's the idea. Here's the document again. Here's the idea of something to valid. Fixing the format here, you have, so it's just a JSON shima that tells you which properties. It tells you which properties are in there, it tells you to type, but it's always object. It tells you which properties, which information is actually contained in the document in the required section, that must be there. So if you do a search over the database, you can be sure if you search for a certain type of information, it's there for every document. So you know that if you don't find it, if you don't find an object with that property, it's not. Find an object with that property, it's not there if they're also required. Okay, and you need to define so maybe properties. This is from the polymate format then which one, oh, this is just the ID in namespace. Maybe this is not so interesting, how they are structured. For the properties here, then we specify a type. Like for being reflexive, there's a definition that should. There's a definition that should be a bool, and you find this in the definition section: how a bool should look like. This is the required part, and here are the definitions. I hope the wool one is among them. No, it's not. The int one is here, so if you find something of type common int, it should be type integer, and at that point, you may have to look into the software package and load it, what integer precisely means. Precisely means. Can you just go back to the required means? Yeah. Uh required, so you count funny talks in the age representation? It's just per collection. That is just per collection. Don't fix the format over all collections. You come with a set of objects you want to store. Then we can discuss which information should be there for this thing, and then we make up. This thing, and then we make up others required. Many things don't make sense. For combinatorial things, error polynomials, or for non-elattice things, don't make any sense. But here, we want to make sure every, that is probably from some lattice collection. We want to make sure every entry has that information. You want to search for some coefficient. You shouldn't miss one because it's not. You shouldn't miss one because it's not computed. Okay, exactly. So, maybe this is it. Theory took longer than I expected. Questions before I maybe just switch to a demo? This is free. If I want to put some stuff there, I'm not going to need to pay. No, no, no, no, no. Who is paying? Is pay. Currently, this is just hosted by the Berlin Polymer team, and as long as the storage is not exceeded or doesn't grow too far, I guess it's just paid by the amount of storage is just available. So they buy a new hard drive if necessary. If it grows substantially, then it may get. Essentially, then it may get different, and there must be some funding. How many gigabytes is this? It's below one or two terabytes, so it's not so much. We can check in the database. So I think a single collection, the largest one is around 300 to 400 gigabytes. And many are much smaller, but there are twenty, twenty-five in there. So that's the amount of data. It's continued existence is Continued existence is predicated on it not being too successful? Currently, yes. Yeah, but I guess if it gets successful, there will be someone who's willing to buy a hard drive. I hope. Someone will get grants. Yeah. Hard drive is not expensive. I mean, with this setup, I guess perhaps you can. So, you know, how So, you know, how how much does it cost, okay, at the moment? Roughly, I don't know. I think the biggest costs for OEDB at the moment are just the salaries of the people. There's a system administrator in Berlin that, in his time, in his pay time, cares for backups. This paytime cares for backups and for software updates and for renewal of hardware if necessary. I don't think it takes much time, maybe he spends an hour or two per month on this at most. It's my time for programming part of this, but the running cost otherwise is low. But that depends really on the amount of data that is collected. That's predicated on a university being willing to sort of host it for these little costs like having an internet connection and domain network. So somehow my next question indeed is connected to this because it is on university computers and some people in the university administration or IT department. I mean, I don't know how you get some. Offer because many of us have this centralization thing happening. How happy are they that anybody in the world can send a query to your computer and get an answer? I mean, it's more decentralized than Germany, so this is hosted by the Maths Department at TU Berlin. It's under full control of, and the system administrator is also employed by the Maths Department. So in that sense, it's under full control. Since it's under full control, it's there on server. It's in the university server room, of course, because you need cooling and this that you kind of want to build up yourself. But all hardware, all administration is done by them. I could also pull this to Damstad. I have full control of the servers there. So as long as we do have servers, that will exist. If that actually changes to we only get remote. We only get remote computing, some kind of hosting instead of housing, then it might get more difficult. I don't know. I can't guarantee what happens in our kids. When I hosted a database at my institution, it was done in the department, so I didn't have to go through central people. But it was extremely important that we had a separate machine that was not connecting anything else, that was a database server. But I'm almost But I'm s almost certain that if I asked to do this now, the bozos at the central administration willn't understand and it's a security problem. Yeah, that's what I'm saying. You'd need to assess the whatever. Yeah, yeah, yeah, it wouldn't fly. That's a problem with public databases. So nowadays, even if you told them I'm doing just this machine, but the fact that it is connected to the rest of the university network. I could imagine them. But they do not you do. If not the least. Yeah, if not the least because of just general lack of full competency or that the people in charge talk to lawyers when they talk to computer scientists. Or they simply don't know. Well, no, no, it's a lot of people. I mean, when I deal with people higher up in the university, the problem is the people who make decisions don't have a lot of base knowledge. Of base knowledge of what we're working on, but they do know how to talk to lawyers who know even less. I'm supposed to check, figure out how to make sure I'm not kidnapped from a market in a tropical resort town every time I travel abroad. They want me to take this training every year. It's like it's like a drinking game. But this is the sort of stupidity. But this is the sort the sort of stupidity act of goal. Anyways, you know, once they asked me, there was a time where I laughed out quite part of discussion. And they asked me to check me to certify that the institution I was listening was not. Is not the government less than fair or bad institutions. And then I discovered that the total number of Russian individuals that were problematic to the government was three. Those three that had been caught on camera or in Nobby Talk, they're all the place to kill one. No, I got that story. Yeah, so don't. I heard that story. Yeah, so those three, they were the only ones that were on the list. Yeah, okay, so anyway, it's okay for now. It's okay for now. Yeah, and I'm pretty sure in the German system it will persist for a while. We resist centralization pretty much and pretty successful so far. Unless there is. So far. Unless there is some huge security incident that everything changes. If you get hacked like University of Giesen, then maybe that might change. Especially if they get hacked through your database server. Yeah, if it would be our server. But I'm pretty sure we're on the safer side, at least. Okay. Maybe just a quick, so it's in Polymake. The first thing you need. Polymake. The first thing you need is you need a connection to the database. This is just by one command that opens a channel to that MongoDB database. You can get some information. I first strictly list to make it not too long here. So this is in the polytops combinatorial section. You can get a list of what is in there. If you want to do this programmatically. Yeah, you can just get a list of all collections in there. If you don't want to look at the website or if you want to program it for yourself, you have to json there. And you get information. This is on one collection done by Monica Blanco and Papua Santos on lattice polytopes in dimension 3 of width at least 2, and they enumerated all of them. It's finite. And I enumerated all of them. It's a finite number with a fixed number of lattice points. Width 2, because width 1 there is an infinite number. That doesn't work. But starting from with 2, it's a finite number. They computed this. I've put this in the database. I mean the information there. It's strictly in the interior. No, sorry, it's a very interesting thing. No, it's a full number of lattice points, 15 lattice points somewhere. So number of. So number of product has at least yeah it has at least uh four four it doesn't need to have an interior point whose widths two width two it has a little more yes yeah so so so width at least two points at least two the width width two is passive widths at least two If you have the narrow ones, there is an infinite number, you need to have one direction spread out. Otherwise, it's all finite. One can generate the other ones, that would be a separate project. If one wants to do this, it would be also nice. But we have the finite list here, and you get the full information. So, those are the papers you should maybe read. And this is the original source of the data. The original source of the data. I reprogrammed this to make it faster. Does it say how many of those things you have? We can check, yeah. I guess this author counts, but not this one. Yeah. Here. I just wrote it here because the query on this network took too long for it to talk. It takes five seconds or so. I didn't want to wait. In one away. That's the number of polytopes. 70 million three-dimensional polytopes of width, at least two, with at most 15 lattice points out there. Do you have any note for how this kind of database is useful? So what kind of query a third party has made to such a database? We'll see an example. Yeah, I have an example from this. I think in this presentation it's from This I think in this presentation is from the paper of. Oh, I always get the wrong raw. So yeah, it starts with four vertices up to 14 vertices. So the 14 one has, obviously, there are ones with interior of that point on the boundary because it's up to 15. Okay, we can just start it to get one of the polytopes. Get one of the polytopes with the find one, an empty query. We'll get just one, so this is probably just a simplex. I prepared some statistics that runs for a short moment so that we can see how this distributes. Number of vertices versus number of lattice points. And that will appear in a second, I hope. I guess somehow it's. I guess somehow the network is slow here, unfortunately. It takes longer than when I prepared this. So what if it takes too long for the... I mean, do you have a time cutoff? The MongoDB comes with its own cutoff that is hard to overcome. This is something like half an hour. So, oh, yeah. But I need to. Oh yeah. Ah, but I need to make this a little bit smaller so that it fits near this. Now it's a table. Let's expand on this one. Ah, it has. Yeah. Okay, you see here a number of vertices versus number of lattice points and then just a count. Just a fun query on this. Maybe for time reasons I Maybe for time reasons I skip. So you can verify from the schema that your data adheres to the format. If you don't believe this, you can just validate that the data is correct. Maybe we skip this and go to the unimodular polytopes. That's from the paper of Benjamin Null, pretty recent, May this year, unimodular polytopes and the new Halotide bound and touch of the unimodular matrices. I don't want to talk too much about what this is. Talk too much about what this is. There's one small computation he did with this. He looked at lattice polytopes that are unimodular, and that means that every full-dimensional sub-simplex, so every subset of the vertices that forms four-dimensional simplex, should be a unimodular simplex. So if we pull it to zero, it should generate the letters. That is his definition. There are some equivalent definitions, just if you want to know. All letters are polytoptini modula, all regular. Polytops unimodular, all regular lattice triangulations, or all lattice triangulations should be unimodular, it's the same thing. And it's not so hard to see, they are all compressed, so they have facet widths one, all of them. If they are unimodular, they necessarily have facet width one. So every facet has, and if you go one step in the lattice in, then you reach the final step. You're already on the other side of the polytrop. Okay. Okay, that information is stored in the collection. So we can get the 0, 1 polytopes, they must all be 0, 1. Compressed polytopes must be 0, 1, up to lattice equivalence. So we have them up to lattice isomorphism to 0, 1 polytopes. And so you can now just go there and check this. So I define some variables. So now I program some polymake to compute this. So this function just queries the database, it finds them in a specific dimension and all compressed ones, and then it runs through them. So I do it in naive way, I just go over all subsets of D plus one vertices, take out the matrix of those, check whether what they generate is a unimodular simplex, and if it is, I add it to my list. I add it to my list. A very basic, simple approach to compute this. Certainly, ways to make this more intelligent, but it suffices for this. That should not take too long. So it runs the query on the database. Takes a while. Yeah, now it's done. And I can just output the list. And I've also The list, and that was Benjamin experimented with. So he found in dimension two, here are two. That's even all of them that there are. Then in dimension three, we have four unimodulated polytopes. In dimension four, we have 13. In dimension five, we have 38. And if you know that there are 10 million of those polytopes, then you see not many of them are actually unimodular. It's pretty restrictive. And what he was interested in, now here we are. interested in. Now here we have the, so I just took out the names, the identifiers of those. I can now also run a query that takes each of those names and prints me, collects the maximum of the number of vertices they can have. That was the Nixis. So we have our unimodal polytops. I want to know how many vertices they have. I can run this query and I see the maximum in dimension two is the The maximum in dimension 2 is 4 vertices, 3, 6, 4, 10 vertices, 5, 12 vertices. And that I got out of the database. Now, and what he proved, so this is a theorem of Benjamin Nell, not of me. He proved the numeral polytopes, the number of vertices is 10 in dimension 4, and it's d plus 2 squared over 4, round it down otherwise, and you can check that this is actually true. So, and I guess the way it rounds True. So, and I guess the way he came up with this, he looked at the database, computed this, and then made his conjecture and proved it. Like, this was the way he got it there. Okay. Clear so far how this could work. So, maybe so I have more in Polymake, but maybe we can should quickly switch to Oscar and do the same thing again. So, with a different example, maybe, but So with a different example, maybe, but so in Oscar, okay, we need to load Oscar. That is the longest part here. And it's on my laptop. Sorry, that is a slow one. So, but I maybe can say a few words: everything that you can do with PolyDB and Oscar comes via PolyMakeJL. That is a new error. Does it affect me? Yeah, it seems like. And then it doesn't have network. Let's see if it loads the data or it doesn't. Actually, an error that I it's an error I don't have to care for. Okay, because it has loaded. Okay, because it has loaded. Well, let's see if it gets this one. Yes, it does. Okay, it's an error that I don't have to care about. So, or care about later. Okay, so again, it comes by a polymit. And that, so first of all, everything has to be in the right namespace. So, you need to prefix everything with making it into polymate polydb. And the second thing, and I'll The second thing, and I'll show you a bit later: you get polymag objects and you don't get OSCAR objects. There is a transparent conversion, but you have to do it yourself. In there, we'll see this. Okay. So, I've got the database. Yeah, maybe some information again. You can list. I didn't show this in Polymade. You can also here just list what information you can get, what is in that specific collection, what fields and What fields and then play around with this? So, here, which one do I use? So, if you want to do something with it, you have to tighten it properly before you need the connection and you need the. But if you want to do something with it, what you need is the name of the collection, that gives me the handle into the collection, this thing, and then you're good to go. Then you're good to go, right here. I connect to the collection. Now I have a handle into that Quatsaskarka database. This is the Quatsaskarka database. I can write my query. This is the convenient form of writing a query. I can check how many of those polytopes are actually in that Pythagoreskar list. That should be a 38. That was already written there, exactly. I can also find all of them and run over them and do something. And run over them and do something like asking for whether they are smooth or not, so whether they correspond to smooth varieties or not. And you see some of them do, some of them don't. What is missing currently, we can just pick up one. I have to make a query to get a cursor back. So this always gives a cursor back that with it in its in each iteration of that loop. Of that loop fetches one more document. If I want to have just one currently in Oscar, I have to do a query that just returns a cursor that gives back just one object, but I have to pick it out from the iterator, but then I can work with this. Okay. Yeah, there it is. So just one object back. And it's in the polymic format. You see this also by. Format, you see this also by this string information, and it's polymer vector of integer. Okay, very quickly, polyhedron and OSCA are a bit more convenient. So you can write something like cube 3 gives you a polyhedron that is a cube in dimensional 3, and then you can have the vertices as a nice list as an OSCA type. But there sits a polymake polytope below it that you can access by. It that you can access by.pm polytope. So, but that's transparent. All computations are done on the polymerc side. This is just a wrapper that makes it nice. But you can just use this in the old form by accessing the members of that. If you want to have a polymate polytope, you have to define it this way. Polymate polytope brings it into polymate, into the platform polytope, and then define the user polymate. And then you find the user in the polymer form, and you can do almost the same things with this. Yeah. And the point is, if you do queries here, what you do get back are polymake polytopes, not Oscar polytopes. You have to do the conversion yourself by, for instance, plugging them into into polyhedron, then you get non-Oscar polyhedron. That's something we have to think about. That's something you have to think about because only here all the OSCA methods from other applications are available. So if you now want to, okay, lattice points would work everywhere. But if you now, where am I? If you now want to define maybe the toric variety that corresponds to the normal fan of that polytope, you need OSCAM acids. So you need an OSCAR polyhedron. You have to convert first. I've done that here, then I comply all. That here, then I comply all OSCA functions to it, that all works, and then I can also go back, take the polydrophan of that torrent variety, that should be should give me back the original one, and I can use, for instance, an Oscar function to ask its both. Okay. Maybe get over this one because of time. There's one example I can maybe start. It's from another paper. It's by Das Fika, Tommy Hoffman, and Michael Jose. Tommy Hoffman and Michael Joswik two years ago on computing nano groups of L polynomials in Oscar. Is that a thing? It's more a demonstration paper than a real mathematical paper. So it shows some features of Oscar. Yeah, yeah. Well, our problem is very interesting objects, but I didn't think about their calendar. Oh, this connection is interesting. This connection is interesting. Okay, there's interesting. Yeah, this is in. But in the paper, they don't say too much about that. They just compute this. But this is interesting. So we still don't know error polynomials and these GALO groups do present some structure that is maybe worth studying. So I'll show you in a moment. It's all not done by me. So just to make this clear, I tried to understand what this point is. Paper. I tried to understand what the spoiler paper did really. Yeah, okay. It's probably more a software demonstration. You can use Polymake and real Oscar stuff and PolyDB to do something. And so that's the reason why I chose it here, because it combines these things nicely. And it's maybe not so easy to come up with simple examples. So, yeah, AR polynomials, I guess everyone knows. I hope. So it counts the number of lattice points. Oops. Oops. Yeah, so I have just for the cube, for the normal of the cube. And if you evaluate this, you see this number of lattice points in minus 1, 1 cube, dilates of the minus 1, 1 cube. We can now check in the database of smooth reflexive polytops. That's what they did. They computed the error polynomials, the Galloa group, and computed the order of the Galois group. And computed the order of the Gallio group. That is the aim of what they did. Okay, we can do this, unfortunately, only for the three-dimensional ones here, and I've copied the results down because it takes for the five-dimensional ones, it takes six-dimensional ones, it takes a couple of minutes, so I don't want to wait for that. So now they're there. Now it goes on. And now we see a set of the orders. The orders, that's the full computation once this is done. So, this is the longest part. Computing the Valova groups of those takes significant time, at least on my scope computer. I prepared it on a faster one, so I didn't realize until I tried it here. I will finish it at some point. Okay, while this tries, yeah, now it's gone. Okay, here this is a boring one. It's just order 2, and all 18 that they are in dimension 3 have that order. Let's just do not the computation and take the pre-computed results in dimension 3. You get this. This is the order. This is the number of polytopes. Dimension 5. And maybe the most interesting one is. Most interesting one is dimension six. That's also the one they discuss in the paper, is just one of order six. And this is maybe something one may want to look at. This has also different properties that are interesting. So we can single this out. It's this one. I get this. Yeah, so again, same problem. Oh, we don't have a file. We don't have a find one method, so I need to take an iterator. We can compute the error polynomial and a Galois group. That takes a moment longer than I thought. Become a six. Now that is the error polynomial over six. There it is. Maybe don't compute the vertices, the F vector. This might be interesting. This is also one of these examples, one of the few examples we know where the complex roots of the error polynomial do not have real part minus one half. So it coincides, this example also appears on the list of only four up to dimension six that do not have real part minus one half. Have real part minus one half. So maybe this is interesting. I yeah, I didn't work on this, so I won't judge, but I think there is a reason maybe to look at this. Okay, and maybe at this point I should stop. We should probably take our last time to thank the people who brought us here.