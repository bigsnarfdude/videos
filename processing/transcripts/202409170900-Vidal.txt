It's going to be much more about concepts and how to do things. But I hope that the part of the title that has orthogonal matching pursuit would give you a hint that hopefully there is a future path for herbotic analysis and the sparse representation in particular to have an impact on modern explainability methods for AI, which is what this talk is going to be about. Is what this talk is going to be about. I think explainability is becoming increasingly important because AI is working incredibly well in a rich variety of applications, but it's not just applications that we can play around at home, but applications that have the potential to impact people, such as healthcare, the justice system, and so on and so forth. And so, as deep learning begins to be deployed in these applications, the question of explainability and interpretability becomes critically important. If you have an AI self-driving car and it has an accident, who is going to pay for the insurance? You might want to understand why you didn't stop, why didn't you turn right, and so on and so forth. So, this explainability at the level that can be understood by the end user, by a human, is becoming pretty. This is another example. Coming pretty. This is another example of medical image diagnosis where these tasks are currently done by trained radiologists that are trained to look at images. And they would inform diagnosis, in this case, Alzheimer's disease, not only based on looking at the image, but trying to have some description of the image content that is related to the prediction of the disease. But the reality is that all deep learning technology today is really based on. Today, it's really based on making some prediction with a black box, and we don't really understand why it worked when it worked and why it failed when it failed. So, traditionally, the thinking has been that interpretability is sort of a constraint. If we are trying to maximize prediction performance, subject to no constraint at all on what the representation is going to be, that ought to be better than putting the constraint that it needs to be based on features that are in. To be based on features that are interpreted. And if you look at the history of AI in 70 years, the earlier AI techniques were all based on highly interpretable classification rules, like is the temperature of your sun above 100, so take it to the hospital. But today we are in the other end where we have highly accurate machines for prediction, but they are not interpretable at all. And so ideally, we would like to be in the top right where we have highly accurate. Right, where we have highly accurate prediction techniques that are at the same time highly interpretable, and bridging this gap is one fundamental question that is today. But because we care a lot more about accuracy, the way in which explainability has been done thus far is not by changing the way we do business, but rather taking state-of-the-art methods and trying to figure out what is under the hood. And by and large, there are many techniques that are based on are many techniques that are based on what is known as feature attribution, which means that you look, for example, at an image classifier and you want to understand what regions in the image were most relevant for that prediction. And what you do is that you produce these so-called heat maps, where red means highly important and blue means low importance. And then AI ops and experts now begin arguing as to why the model picked up on those features. And as you can see, And as you can see, it's often the case that these features may be meaningless, like having regions outside of the brain being useful for making the prediction. And so there is a huge debate right now that on the one hand, this is very good because I keep the accuracy of the model, but there is just a large number of papers advocating that this is by and large nonsense. In fact, there are many people that are very active in. People that are very active in former Twitter about the fact that we should really worry every time some AI technology is deployed in hospital because these techniques just hallucinate or produce regions that really have no correlation. I mean, they might be correlated, but they have no cause effect with what you're really trying to. So here's the philosophy. Here's the philosophy that I'd like to advocate. We'd like to advocate that in order to make predictions that are both, we need to make predictions that are not only accurate, but also you need to provide an explanation. Ideally, that explanation might be interpretable to the user. And you also want that explanation to be faithful to the model. What I mean by that is that it really needs to be the reason why the model predicted that, because I can come up with this explanation, but that's not really. I can come up with this explanation, but that's not really what the model actually did. So, the question of faithful to the model as well as interpretable to the user are the two basic. So, how do you do that? Or what are sort of ingredients that are needed in order to define mathematicians, starting points to have a definition? I think the biggest problem we have is that we don't know exactly what explainability means. We don't have a formal definition to work with. And so, this is kind of just stating. So, this is kind of the stating kind of what are the ingredients needed to even come up with a definition. So, if explainability is going to be for the end user of the AI system, it is important that it is made in terms of words, patterns, or symbols that are really interpretable to that user. You cannot explain to a random person in the street, you know, with a set of equations, why the model predicted what it predicted, or even talk about linear classifiers and. Even talk about linear classifiers and weights like that. It really needs to be for the end user. And that's where I said that the exponentials are actually not, they are dependent on the domain. The words and the vocabulary that you use will depend on the task, but they also depend on the user. Let me give you one concrete example. Suppose you're a clinician, you collected data about a patient and you made a diagnosis. Suppose you want to explain that diagnosis to another clinician. You will use certain words, certain You will use certain words, certain vocabulary. You know the other person is an expert. But if you want to explain the same diagnosis based on the same data for the same patient to the patient, more likely than not, you're going to use a different explanation. So this idea that explanations need to be adapted to the user, I think it's also important to have. Without making any formal definitions yet, I'd like to postulate that one way of doing so is via what we call Is via what we call query. So queries can be user-defined functions of the data that are interpretable to the user. Those queries might be, what is the color of the shirt? Or did you stop? What was the speed at which you were going? So imagine just a very large battery of all possible questions in human language that you may make. And they are functions of the data because, given, say, the trajectories of the car. Trajectories of the car, and given the question, you might be able to answer. And the idea then is to make explanations via a composition of these questions and analysis. So this has been tried, and one of the first attempts on doing so is the so-called concept bottleneck model. The basic idea of concept bottleneck models is that you begin Concept bottleneck models is that you begin with an input. In this case, the data is an image. You are going to have some set of concepts that is predefined and that is very specific for that task. Think, for example, an expert, a human expert on classifying birds tells you, or classifying animals tells you these are all the characteristics that are important to classify this set of animals. So that's the cap, horns, fur, wings, or legs, whatever. And suppose that you had data. Suppose that you had data, image pairs, and annotations for all of this very large set of talks. And suppose you could train, and this is probably likely, we believe that it's relatively doable with a lot of data today, concept predict. So now what you do, therefore, is you map the input image to this set of concepts. And then the idea of using a linear classifier on the concept to make the prediction, which in this case is the class. Which, in this case, is the class has been postponed. And then, what is the explanation? In this case, according to this paper, the explanation is based on the weight. If you have a linear classifier at the end, then high weights means that that feature, which is interpretable, it was whether it had foreigns or not, that that feature was highly important for that prediction. Yes, I'm going to define them mathematically in a moment, but the definition is going to be fairly simple: it's mapping from the input data to a semantic space that is, in this case, pretty fine. So to some extent, the skills are exactly what I mean? So depends on computer vision at what stage. Computer vision today is not like that. Computer vision today is the input is an image. I pass it through a network and predict the class. Computer vision, maybe in the 70s and 80s, was all about About objects that are composed of parts, and I can reason about the relationships, and that's what composed of the class. But there were no techniques to reliably extract those parts, not to learn how they are composed in order to form of them. And they were by and large limited to a handful of objects. Now the goal is to do this for at large. And without, I mean, whether you need to learn all of those relationships or not is part of the question. Part of the question. So, I am going to argue that this concept bottleneck approach has some good ingredients, like this set of questions and concepts. But there are a couple of things that I personally don't like. The first thing I don't like is the question of expressivity, like why do you need to use a linear classifier to go from concept to two? That's limiting. And the main reason why you make that limitation. Reason why you make that limitation is because you say, Hey, linear classifiers are interpretable. I like to argue that's another line. People always say that the linear classifiers are interpretable. I am going to argue that that's not necessarily the case because you cannot go to your grandmother and say the reason why you were denied the credit is because the weight of the classifier was 0.75. And that was the highest. But so. But so these weights need not be necessarily interpretable. But the other thing that I think is critical is that a linear classifier based on the weights, say I choose the top five concepts. Well, that's an explanation for the class because the classifier is going to be the same no matter what the input data is. Those weights are the same. And so you don't have explanations that are adapted to this particular input when you put a linear classifier. But then I need to know, okay, so all of those variants have been explored. So is explainability the value of the weight? Is explainability the value of the weight times the feature? Times the feature in all of the combinations. But I don't believe that those provide. It's very easy to construct examples where those things actually pay. But I hope that what I'm going to tell you convinces you that there are better alternatives. Because there is variability. Because there is variability within a class, and so some attributes might be present in some objects, not distributions are not unimodal, or because maybe some attribute was not visible in that event. And so I have no way to predict it. And so something else was a reason why for that image that was there. Well, at this level, what I'm simply saying is because explainability is for the user, I'm going to ask the user a priority, give me the set of all questions you might want to ask about, that if I answer them, you're going to be satisfied. And so I'm assuming, therefore, that the query set is sort of complete and sufficient. I'll define what that means in a moment. I'll define what that means in a moment, more precisely. But I'm assuming that the query set is very large, but naturally that might not be the case. And so the set of questions might be insufficient. And in fact, if you apply this scientific discovery, you're more likely than not trying to discover what the question should be. And so that's the limitation. Okay, so what I am going to be presenting next is what we've been doing over the last several years, which Several years, which is to propose a framework that is information theoretic. And that adds two ingredients to the concept bottlenecks. One is a theoretical framework that I'll explain in a moment. But the other one is that we don't want to predict all possible millions of concepts for every input image. What we want is to select a very small subset because multiple reasons, right? One of them might be Occam's raising. We want short explanations. Answer is we want short explanations, simpler explanations are better, and so the question of selecting what questions to ask is going to be one. The second aspect is that in order to be faithful, which was one of the other requirements, we are going to make predictions not based on the image. So you get the image, but you never get to see the image. The classifier will never see it. The classifier gets to see the image only through a small sequence of questions and answers. Small sequence of questions and answers. And the prediction of the class is just based on this small sequence. And this small sequence of seven in this case is a small subset from potentially. I mean, for this data set, there were 300, but seven words of. And therefore, the idea is simply that I need to have a mechanism to select one question at a time, answer them. And I need to have a classifier that takes as an input a variable sequence of questions and answers and makes predictions based on variable length. Based on variable length input, as opposed to classical classifiers that would just take the whole image. Yes? The framework can accommodate both, but for the purpose of this talk, I'm going to assume that they are binary, which makes many of the implementation details simpler. Is this like a decision or you go ask other questions? The mathematical definition is based on deciding, having an encoder that is so smart that can figure out what questions to ask all the time. The practical implementation is based on a decision tree. But it's not a decision, but the learning of the decision tree and the way in which the decision tree is defined is fundamentally different. Decision tree is defined is fundamentally different because it's based on a generic model as opposed to say part. I hope so, and that will become part of the definition in a moment. Okay, so before getting into the math, some of the basic things that we have Things that we have a couple of papers on how we did this, so I have one slide for this. So, very little details, but happy to answer them. First part is, well, where do I get the queries? So, option one, and this has been done in many papers, and we did that first, we asked the domain experts, or what are the questions that you care about. The second possibility is that you've got many data sets that can annotate it with attributes that can be used. So, the case of birds is one. That can be used. So, the case of birds is one such case. But if you really want to scale this up, you can now use ChatGPT to generate a very large query step. And so, different people have used different prompts. This is exactly what we did, for example, for the case of birds. So we take every task and we ask ChatGPT, I have this task, I need to do this specification. This is the list of 200 bird classes. Give me important visual attributes for that. Important visual attributes for that class. And so from now on, I've got a very large list of queries that were produced by GB. Second part, how do I answer the queries? If in classical explainability where what you want is extract a feature, the answer in the query is just, you know, is the color of pixel two, three. But if the questions are really semantic, you need to answer. You need to answer. So, option number one would be: you interact with a human and you put a human in the loop, and they're answering the question. Option number two is you have these pairs of data that are annotated, and I can train these classifiers. So this is what the concept bottleneck paper did. I had a data set, and I can train a deep network to make, to predict all at once, all at once. In the modern era, you can begin to see. Modern era, you can begin to say, hey, but there are all of these vision language models that you can begin using. And I can input a query and input an image, and they may be able to produce an answer. We try them and they fail. Clip is the nicest because it would be computationally so easy to simply say input image embedding, text question, embedding, dot product. If the dot product is high, the answer is. If the dot product is high, the answer is yes. If the dot product is low, the answer is no. That would have been beautiful. But if you look at the distribution of clip answers, the average is 0.5 and kind of a Gaussian standard. So they're very uninformative. And you lose all interpretability because if you tell the user, hey, the answer to your question is 0.7. You could use more sophisticated models, but because this is just one step on a very Just one step on a very complex system to do explainability, LLAMA and bleep are way too slow to be able. So, what we ended up doing, and this was established a month ago, a couple of months ago in ICLR, was simply use large vision and language models to generate several labels to then train a very domain-specific and specialized concept question-answering machine. Question answering which where the input is an image, the other input is the query, and then you have yes or no. To your point, yes, you produce the probability that it's true or not. Okay, so I'm not going to give the details. There is a lot more to this, but the main point is that for the purpose of this talk, assume that the questions are given and assume that I have a deep network that can take images and questions and answer. Yes or no? So, what I'm going to focus today is on the information theoretic part, which connects more to sparsity, which is how do we select what questions to ask and in what order. And so we've done three papers on this. The very first one was done using a generative model and to implement the information to be ready framework by the book. The second one was way more efficient. Well, with the first one, More efficient. Well, with the first one, we could only do MNIST. With the second one, it's way more efficient. And we prove that it's actually equivalent to the theoretically optimal framework, even though it's been trained with deep networks. And it's a variational formulation and it works very well, but you need data for training. And the third one that appears in New York last year is to realize that the The information pursuit framework, which is what is behind reference one and two, is equivalent to OMP under a certain case. And that's the main point that I want to talk about today. And so you can use now super efficient techniques from sparsity to select what questions to ask. But you're going to be curious, all right, if this is going to be OMP, what's the dictionary? What's going to be the data? And how are you going to get OMP? How are you going to get OMP extended and generalized to actually work with semantic questions and being able to produce explanations? So that's going to be the agenda. So I'll begin by telling you what the information pursuit method is about. And there'll be some theory about performance bounds. And then I'll move into how it can be implemented using OMP and how OMP is integrated with large vision and language models for that purpose. Models for that purpose, and then I'll have a glimpse of two applications, but we have done many more at this point. Okay, so let's begin now with a little bit of basic, very basic math. So, x is going to be the input, think of an image, y is going to be the target variable, think of the label for the image. And here is the more formal definition of a query. It's a collection of functions that map the input to a set of answers. To a set of answers. And we are going to assume, for the sake of simplicity, that it's discrete and think just of a battery of whatever number you want of human-like questions in human language that are binary. So that's what the query said. But at this level, there are therefore features extracted from the data. We would like to figure out how to learn and train what we. How to learn and train what we call a query. What is a query? A query is what's going to take a history of questions and answers that I've asked thus far, and it's going to select the next question to be asked. But more generally, you could think of the query as simply producing, I mean, compositions of this leads to a sort of meta query, if you wish, that answers all the questions. And to connect this with And to connect this with information theory, I'm going to need this notion of a code. And the code is super simple, is the sequence of questions and answers. This generalizes classical codes in information theory, which is just a sequence of zeros and ones that correspond to, right? And the query or how do I get the code in a sense, if you wish, is going to be a generalization of Half-Clan coding, which is one algorithm to go from data to a sequence of 0, 0, 1. One. Okay. So, with that, this is the formal definition of how we do explainability in this framework. Super simple to at least write down. So, we want a query that has two problems. One of them is minimality, and that is that it should produce the smallest possible number of questions. And the other one is efficiency. And I think finally, this relates to your question, right? We would like. We would like the total, which is going to be a small subset of the total, to be a sufficient statistic for prediction. Another way of writing this down is to say, at the end of the day, I'm interested in a prediction task. So I want your y given x, and I want your y given x to be the same as your y given the code. And so that's the notion that the code needs to be a sufficient. And so that sufficient equation is the one that tells me you can throw a very x. You can throw away X, and you can just replace it by the sequence of questions and answers, and then predict Y. Basically, this is also what, from an explainability perspective, guarantees the faithfulness to the model, because the model is just making the prediction based on the code. And I hope you immediately begin the code is going to be the explanation, because the code is the sequence of questions and answers. And so you make, therefore, the prediction you make is a function of the explanation and is faithful by construction and by design. This is here is where the linear classifier question comes. It's like, I'm trying to predict your y given x. At this level, I'm allowing you to use any classifier you want to implement this. It does not need to be linear. And the explanation is really comes into the code. And it's really because it's acting on the code and because you have a query or pi that selects what questions to act and therefore produces the explanation by a construction. Produces the explanation by a construction. And so, this is what we call explainability by design because you make a prediction and you produce the explanation as part. And so now x is a random variable, q is the query set, pi is the strategy that I use to select the question. And so the bars here are the length of the code. And the expectation is the expected code length. So what we want to do is to select a strategy that minimizes the expected code length. That minimizes the expected code line. All right, so in terms of writing it, hopefully it's clean and easy. And hopefully you're already beginning to draw connections with sparse representation here, right? The minimality is like the number of questions is like the L0 norm. And hopefully there is some sort of way to say that this is, you know, your dictionary times the sparse code is equal to what you're saying. The sparse code is equal to what you observe. But how do we get there? That's going to be my question for you: is like, can we really take all what we know about sparse representation theory and now do it for probability distributions with notions of independence, maybe being substitutes for incoherence and whatnot? That's sort of a question for the future. Yes. Sounds like any questions for both screen. Yes. In two or three slides, I'm going to have some theorems about performance bound, both with noiseless answers and with noisy answers. Yes. Yeah. I'm going to leave that as future work, but in principle, this is not restricting that because what is the generative model that relate data questions answered and talked about? Answered and target value. That could be a model based on the causality. And so my sense is that causality models aren't operating at the level of large language model kind of scalability. So for the moment, much of the implementation is going to be without causality. But I do envision 10 years down the road when causality is really broadly applicable to large data sets, that that's the right way to go because you would not only Way to go because we would not only be providing explanations, but there is a mechanistic reason as to why that was the case. So, excellent question, but I don't think we're there yet to be able to do it. All right. So, this is the framework. The challenge is that this is easy to write, but good luck with finding the optimal query that solves this optimization. So, what we do, and now we begin to connect to And now we begin to connect to decision trees, as we were asking earlier, is a greedy procedure. Again, greedy is good type of idea where we play 20 questions. And so what is 20 questions? The classical example that you can play with your kids. Actually, maybe I play with my not my two-year-old, but my 10-year-old. Guess a number between 1 and 10. Guess a number between one and 10 and asked me a question. And he said, Is it less than five? And then I asked him, Why did you ask me? Is it less than five? Well, he didn't quite know, but somewhat in his own words, he said, because it splits the space half and half. And so for those, so there is a way to selecting an optimal question. And the key idea of the algorithm is to ideally reduce the number of hypotheses to half and half. Hypothesis to have now. So, in a classification task, that's what you want. And the way it does so is you select a query, Q, such that the answer, Q of X, is most informative for Y, which is the target argument. You can use many criteria, but this is classically implemented with mutual information as the criteria. I want to emphasize that that first question does not depend on the input image at all. Like, you've not seen anything. Like you've not seen anything, right? You select x is a random variable, y is a random variable. So it really depends on the distribution. And of course, the distribution is related to the task. So the fact that you say, is it less than five, is because you know it's a classification task. You don't even know the number to produce that first. And so once you have an answer, you can immediately try to make a prediction condition on the answer that you have, if you have the problem. Have if you have the problem, right? And therefore, so now you ask the next question, and the criterion is the same as the query whose answer is most informative for why, but conditioned on the fact that I have an answer for the first question that I asked. And now the prediction is information, but conditioned on the answer. And so on and so forth. And eventually you stop. When do you stop? You stop when all remaining questions. All remaining questions are uninformable, meaning the mutual information is zero. Of course, in practice, you're going to implement this with, say, this is less than epsilon, as opposed to zero, because you have no way to know if all of the remaining answer. And so all I want to say here is that this is effectively a decision tree, but I am not training at every step. At every step, whether I go left and right. The criteria is based on a Denerton model that I've learned before. And as a consequence, unlike decision trees that are based in, say, ARC, you very easily with classical decision trees run out of training data because every time I ask a question, only half of the data set passes that question. And so I have less and less training data. So you can never train an infinite depth tree, for instance. While here, because Tree for it. While here, because you're supposed to have the generative mode, you can just go. So, in that way, it's very different. It's kind of what the decision tree is built on the fly. Okay, so great. So, let's implement this then. So, but before I do so, a little bit about the theory and is this greedy good? In sparsity, we know greedy is good and we have theorems. So, what are the theorems? So, if you bother. So if Q bar is the optimum, right, this is the optimal number of questions that are needed to identify what, how good is this greedy algorithm relative to the optimal optimization problem that I had mentioned at the beginning? So if you allow me to ask any possible question, and in this case, I'm cheating a little bit. In fact, this is an open question. These are binary functions of y. The framework we want is really for functions of x. Functions of x. Well, this is classical information theory, right? If you can ask any binary question about the variable, then the entropy is the upper bound on how many questions you can ask. For the guess, the number between 0 and 10 and uniform distribution, that's why it's logarithmic on the cardinality of the set, the number of questions. That's because entropy is the logarithm of that. Okay, so this is well known. So this is channel from the 50s. So, this channel from the 50s. What about okay? What I want to say though is that maybe I don't have it on the slide, but there are many theorems saying that this 3D strategy is really bad. It can fail. So, we wanted to understand why in practice it seems to work for many of the vision problems we have. And we came up with this definition of a query set that is delta away from 0.5. What does that mean? I'm going to be writing. I'm going to be running this procedure of asking a question and asking an answer. What is the good property that I want? I want that good property of splitting half and half, right? So, for that, there always needs to be a question that I'm very uncertain about, that is nearly 50. Okay, so the ideal definition would be: you need a query set such that there always is a question that is near 50. We have a relaxed version by delta away from that 50-50. Away from that 50-50, and that and that's exactly written here. There always is a question such that the probability that the answer is yes, given any history of questions that I've asked thus far, is 0.5 plus minus del. In this particular case, the theory is developed for questions about y. And so that's the limitation that I mentioned earlier. Extending these two questions about X is the next. What is the name? And so that's why Q here is a set of binary functions of Y that satisfies. Okay, so now you're going to tell me, okay, delta is zero, then it's probably very hard to satisfy. I'm going to show you in a moment that for many of the data sets that we have used, this delta is quite large. It can be even at 0.2, 0.3, 0.4. And the And the bound on the number of questions you need to ask is the entropy divided by little h of 0.5 plus delta, which is the binary entropy factor. So this number, even for delta being 0.2 or 0.3, is kind of a factor of two. So the main point is that for most data sets that you try in practice that are computer vision and language that we've tried, this greedy strategy is nearly optimal. Is nearly optimal, like at most two, in terms of the number of questions I need. Yeah, I won't ask for that exactly that last because there's a different type of way that people yes. They say the doubt will work well if your query set is good. But there's also a different set of questions you could ask about. Like for any query set, how many extra queries would I need? Any accurate storage that I need from this 3D algorithm versus any part of the problem. And one way of arguing about this is the module expectations. There's a difficult theorem to say, like, I mean, you will need is at most one factor times what. Like, one minus one of the eighth decades. But that's more or less the flavor. We are saying here that the ultimate thing to do would be the entropy of y. thing to do would be the entropy of y and that the factor is one over that quantity and that quantity depends on delta maybe i'm just going to jump ahead this is that that quantity and so if delta is 0.2 then you're just a factor of 1.2 if delta is 0.4 you're at a factor of two okay and so again you don't need your questions Again, you don't need your questions to be 50-50, but even if they are 0.9 to 0.1, which is a broad range, then you double by two the number of questions that you need to ask. And last but not least, to the earlier question about noisy answers, we also said, hey, what if with probability alpha, you flip the answer from yes to no? And so you lie every now and then with some probability. What is the With some probability, what is the cost that you pay? And so, in the denominator, you subtract h of alpha, which is that probability, and therefore, this i mean it's very intuitive at the end of the day. If they lie to you, you need more questions to more reliably predict why. Okay, I think I've already said this, so I'll move on. Okay, so that was just the framework, what we can do, why greedy is good, what are the Why greedy good, what are the performance-bound open problem is doing this with the actual questions of next, but so far we have informed. So, now let me tell you how we go from there to implementations and to get OMP information. So, what do I need to implement this? This looks nice, but okay, I need to compute mutual information, both unconditional and conditional. I need to maximize the mutual. Maximize the mutual information. But more importantly, to do this, I really need, first of all, a generative model. I really need this generative model for data, questions, answers, all of that. Thankfully, in the last 10 years, generative AI has progressed a lot. And so now we can take data sets that have these pairs of questions for the input and the label that is predicted. And the label that is predicted, and train a generative model. So, think about the bird classification. You give me a data set that all it has is the attributes for the birds and the bird class, and you train a generative model for that. Think of medical diagnosis. You give me all of the attributes in a radiological image or the radiological report, all of the answers to medical questions from the report, and why is the disease that is being detected. So, now you're training a deep generative model, and once you Generate model, and once you have P, then you can compute mutual information from it. Okay, that's probably you know that that's actually very. We've done it. You can do sampling, you can do MCMC, you can do, but because mutual information is an expectation, so you just need samples to you. So long story short, this is actually doable, and with the advent of deep genetic models, now it can be implemented. Now it can be implemented, but it is incredibly costly because to learn a deep generative model, now you probably need a lot of data. But most importantly, at every running of the algorithm, both during training and as well at inference time, you need to do a lot of your fanciest sampling techniques for computing mutual information in very high dimensional spaces because images or the number of questions could be very large, what have you. Moreover, you need to recondition the model at every iteration, and you don't know what the history is going to be. And you don't know what the history is going to be. So, worst case, you will need to learn a different generative model for every history. We figure out ways to avoid that for some applications, but it's not always. So, this was great, implementable, our first PAMI paper, but we could only do MNICE Digital classification with that and with probably a thousand times lower than having a deep network duplicate. So, correct, but not computationally. So, the second version of this, I see a lot of. The second version of this ICLR 2023 was to realize that mutual information is a means to an end. What the algorithm needs is what is the most informative query. I just need to know the query, but I don't need to know whether the mutual information is 0.3567 or whatever. I just need to give me the one with the largest. So the alternative is to say, hey, query, can I train a deep network? Train a deep network that you give me a sequence of questions and answers thus far and tells me what is the next question to ask. Once I have it, I add it to the queue and the query is going to give me the next question and so on and so forth. But in addition, from the history, I train a classifier that is predicting the class. So in other words, it's like training a query, it's like training a network to select questions that are going to be good for classification later on. And okay, there is a bunch of equations here, but the long Of equations here, but the long story short is that there is a relatively standard training objective based on expected tail divergence to jointly train this query and this classifier on the data. And so I no longer need to train the generative model. Instead, I train this directly. And the beauty is that there is a theorem in the paper that says that the most informative query given the history is the same as what the optimal querier. What the optimal query from this objective would do. Obviously, and the assumption that you implement the query with an arbitrary network of arbitrary capacity is like the results for GANs that you get the optimal decision if you optimize over the over p, over the probabilities. So if you optimize over the query, then you're going to get the right. But still, you need a lot of data and a lot of training. So what I want to Our data and all our training. So, what I want to talk about today is how do we do this with OMP? So, I guess in this community, I shouldn't tell you what OMP is about. So I'll just be very brief about that. But the gist of it is that you want to find a vector beta that has the smallest number of non-zero entries that is also linear. So, you're giving a dictionary, you're given it. And the OMP algorithm begins with beta being zero. Beta being zero, and so the support is also empty. And then you select the atom, which is in this case a column of D, that has the highest correlation with the residual. So the very first atom is the atom that has the highest. And then you add that, and then you repeat, and you stop when the reconstruction error is below. I'm sure you all know. Okay, so what is the difference? Okay, so what is the difference? I already made the analogy. I think there should be a connection between minimizing the L0 norm and minimizing the number of questions, except that this is probabilistic, is the expected number of questions. And there should be some relationship between reconstruct X from a dictionary D as predict Y from the code forever. Algorithmically, Technically, the information pursuit algorithm takes a history, the queries select the next question, and the classifier predicts why based on the history. In OMP, you begin with a empty support. Here, we begin with an empty set of questions and answers. We have an algorithm for selecting the next atom, and then we use that for reconstruction. So, hopefully, you may appreciate. So, hopefully, you appreciate that, in fact, these two things look alike. Selecting the next question based on maximizing mutual information versus selecting the next atom based on maximum correlation. The fact that you condition on previous questions and answers and the fact that you compute a residual based on the previous estimate of the paper. So, I told my students: okay, this has to be true. You just, you know, just OMP has to be the other. OMP has to be the algorithm to select the quest. The challenge is that information pursuit is based on a generative model. And so, what is the generative? What is the dictionary? All of that needed to be redone. And so, X equals to D times beta is not a model. You could say, oh, let's add noise. All right. The second potential connection was that OMP maximizes this dot product with the residual. So immediately it occurred to me that the right way to occurred to me that the right way to go is to define the questions about it as the dot product between a dictionary atom and a receipt. The index difference is the target variable OMP was designed for reconstruction, not classification. So that's another gap to be worked on. It turns out that my students tried and tried and tried, they can never get things to work with this query set, like defining a proper probabilistic model that when you maximize mutual information, you get exactly dot product with residual. Exactly, dot product with residuals to for the two algorithms to point, yes. You may have questions like your older one. Man, it's possible. That's not easy for people to do. So, like set of terrorists could have the same patches as a different model, and you know, they. And you know they when you ask like a folder you when you add the folder you because it's a really good thing makes them look like you're talking about so there are there are multiple there are multiple parts to it uh this we have not fully done we're working on it but with modern large language models you can map questions to talk Can map questions to tokens that are vectorial representations. So, the question of if two questions are semantically equivalent, can they be mapped to the same vector, is one active area of research on understanding the latent space of transformers to achieve that. We're not there yet, but let's suppose that that was global. There's the next part, which is if I had a redundant dictionary with repeated questions or very similar questions. Or very similar questions. Information pursuit is supposed to select the most informative one conditioned on the answers that spark. So that idea of selecting questions that are near 50-50 will fail. I mean, once I select, the idea is that information pursuit is more flexible in that regard, because it will select one of them. Once selected, the other ones will not, very like and not be selected again. So it avoids the redundancy. So all of these questions are, well, how do we do the mapping? All of these questions are: well, how do we do the mapping and how do we design dictionaries that satisfy that? Yes, in that sense, most to the other. And I'm going to say we are using some mapping that is not necessarily done proper. And that's part of the research agenda, like many of many things. Okay, so what did we do? And the answer is very weird. I have no explanation for what it is, but magically this works. But magically, this works, even though it's very uninterpretable. Let Z be, and it's actually super simple in addition. So let Z be a standard Gaussian. Let me define the target variable to be the dot product of the input x with that random Gaussian. And let me define the query to be the dot product of a dictionary atom with that random Gaussian. So now you can say, let me impute the mutual information between. Compute the mutual information between the query and y, or what, which is written here, because everything is Gauss and non-linear, you can actually compute that analytic. So that's the mutual information. And now you can say select D that maximizes the mutual information. And voila, it's the same thing as maximizing the total. So for this statistical model, selecting the most informative query. Selecting the most informative query is the same as the first step of OMP, which selects the atom with the highest orthography. I don't have the time, but in the paper, you will see the full set of iterations computing the mutual. I mean, it's really a homework problem, if you wish. This is not very deep. But what is if you compute mutual information condition on the residuals and you do the proper thing at the Residuals and you do the proper thing. At the end of the day, the selection of the K plus one atom is going to be the one that maximizes the dot product with the residual, but there is an extra step, which is some sort of normalization. So therefore, it's not that IP is the same as OMP, but it's almost the same up to a normalization factor. The normalization doesn't depend on little The normalization doesn't depend on little B, but the calculation, yeah, the normalization does depend on. All right, so how do we implement them? What we do, what is the dictionary, what is the image, and whatnot. We assume that we have an image embedding and a text embedding. And a text embedding, where the embedded image is a sparse combination of embedded concept. For this first implementation, we use CLIP. What is the right implementation moving forward? I'm not saying that CLIP is the solution, but I think the conversation earlier suggests that learning these embeddings should be part of the problem as well. But for now, all we do is clip embedding of the image is a sparse linear combination. Is a sparse linear combination of flip embeddings of the concept. You're going to get a sparse vector beta. And OMP, for some of them, the coefficients are going to be positive, for some of them are going to be negative. We interpret that as the answer is yes, or the answer is no. And the ones that are not selected are not used for the prediction. So, in a sense, beta is interpreted as the answers in the basic framework. And so, this is the entire pipeline. Entire pipeline. Image, you encode it with flip, text concept, you encode them with flip. You run this OMP algorithm to select the betas. And once we have it, we are training a linear classifier on the betas. We've also integrated this with the variational framework. So this is, this is maybe an original. Okay, so these are some results. This is an input image. These are the set of concepts that we got, as I said, from G. That we got, as I said, from GPT. These are what happens after nine iterations of running OMP. This is whether these are the sparse codes. The green means that it was positive, red means that it was negative. And these are the coefficients of the linear classifier that, as you can see, they're generally aligned with the sparse. Yes. Say it again. Ah. Well, okay. My answer to your question should have been no. Your question was, the algorithm predicted that these ones are the most relevant for predicting target. No. I don't know the class. Okay, number one. Number one, the number of classes in ImageNet is a thousand. And again, we need to change our philosophy as to how we think about it. What is the most important is a question that will split the thousand classes half and half. So the question might have nothing to do with tiger. And so sometimes not showing the image is better because we humans immediately try to think that the right questions are really questions about that tiger. But that's not true because that's the main point. That's not true because that's the main point. We don't know the class. We have not seen the image. And so we're just trying to have questions that are relevant for a thousand classes and doing those that in the faster way will allow me to determine that this they're ordered in the order of the iterations of OMP. I don't think I can answer them with an explanation other than that's because the dot product of the clip embedding of the question and the image was AIS. Okay, this is maybe a slightly more interpretable running of the algorithm. What we are plotting here is not only the set of questions and answers, but also for every row, we have the probability for the class. For the class. For image net, there are a thousand, so I'm only plotting like 10 here. Blue means that the probability is very low, and red means the probability is high. So at the very beginning, the model has no clue. It's uniforms, has no clue what it is. As iterations proceed, eventually, after about eight questions, you see that there are two competing hypotheses, like a bison versus a zebra. And then to determine, eventually, to Becca's point, eventually. Eventually, to Becca's point, eventually the model, once the distribution is sufficiently thick and there are only a few competing hypotheses, then the questions are about those competing hypotheses. But I can only analyze that at posterior. And so here, for instance, you can see whether it has short front legs was one of the questions. And after that, it peaked into biceps as being the right distribution. And so with 12 questions, it got the flat. This is another. This is another example on medical diagnosis. Here, what the query sets are symptoms. So, imagine you go to the doctor's office, they ask you what symptoms you have, yes or no, and then they try to predict what you have. So, again, the plotting is sort of the same. At the very beginning, there were two main competing hypotheses, whether you had seasonal allergies or laryngitis. And it was only towards the end with the questions of dizziness, inrush, and itchiness of the eyes. Brush and itchiness of the eyes. Itchiness of the eyes in particular was the one that broke the tie. Anyway. Okay, we've done text classification. We have a large project on explainable radiology right now where we're doing this for x-rays coupled with clinical reports. So, this is just, I think, the beginning. If you like numbers, accepted of all of the data sets we have done and comparisons with others. What is the comparison? What is the comparison? X-axis is how many questions you've asked. Y-axis is the accuracy of classification. Just to pick up on one plot, green, which is sort of in the middle, is what if you just select the questions at random. You don't do OMP, you don't do anything fancy. Just pick a random question from the list. And it's not that bad. People sometimes ask me, well, this looks like reinforcement learning. I could have the queryer look at the history of questions. Querier, look at the history of questions as far, selecting what question to ask is like taking an action. Well, that's the red curve. So, training reinforcement learning is an art, and it just doesn't work. So, the blue curve at the very top is, in this case, the information pursuit method. And this is replicated across many. These are two image data sets. This is the medical diagnosis data set, showing that therefore. Data set showing that therefore the algorithm produces the highest accuracy with the smallest number of okay. So, to summarize, I hope that I convince you that explainability is really important. I hope that I convince you that the way it's been done thus far, it's not very useful. It's more of a distraction than reality. I don't want to claim that this is the right way to define it, but I hope you appreciate that the idea of doing it by design and producing an explanation. By design and producing an explanation by construction is important. I didn't have the numbers, but maybe you can see it from here. The accuracy without explainability, we are always within one, two, three, four, five percent. So we're really closing the gap, meaning that you can do explainability without sacrificing accuracy too much. And I hope you appreciated that it could be connected to some of the techniques of this community. We are working on the question of learning perception. Are working on the question of learning query sets, ensuring that they're sufficient, ensuring that questions are sufficiently dissimilar. We are working on extending this beyond classification. We're coupling this with diffusion models to generate images with explanations to begin with as to what the content is. But I want to maybe ask a bunch of questions to you moving forward if you can help me. Like, can we make this more principle? Like, it's sufficiently principled from the information. Sufficiently principled from the information theoretic aspect, but the algorithmic aspect and couple with sparsity, we need new notions of incoherence to distinguish between concepts. If we're going to build dictionaries based on words, we need to have notions of measuring if they mean the same thing and incoherence this up to that. If I would like uniqueness of explanation, can that be connected to uniqueness of the support? Uniqueness of the support. How do we do that? How do we have the guarantees? The question of extending sparse coding to really operate on semantic dictionaries and jointly learning dictionaries and embeddings of complex data so that in the embedded space we can use techniques from sparsity. I think it is emerging. And the extensions of that with token representation based on LM in large language models, I think to me, is a very good thing. Thank you very much. And I went extra, so I apologize. What you're describing here from a statistic point of view is thought of as a hypothesis statistic. Of us are quite as successful. Yeah. The use of mutual information explicitly has both embeddings treated on the same footing, the mutual information embedded. Why is that? So typically, what testing would not I don't know that this is okay. Okay, I think you're right that there is potentially a way of doing this using a hypothesis test. Like the query is a hypothesis, and I try to. But the way in which we have done it is not fully a hypothesis test yet, because all we do is to select the most informative and get an answer for it. Somehow, the uncertainty about the answer of that question is implicitly captured in the fact that you have a generative model and that is used as. Generative model, and that is used as part of the computational mutual information for the selection of the next one. But we don't have a hypothesis test, really, that we reject one or the other as part of. I've not thought about how one would go about doing it, but I think it's a very interesting question.