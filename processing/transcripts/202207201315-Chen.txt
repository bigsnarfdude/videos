And also I will be a master's student with Dr. Warren, Dr. Warren here in this September. And today I would like to talk about how to address the centralized mass gradient formula when we have a misaligned sample point. And this is based on direct work with Dr. Warren here. First, I would like to give a quick background review of some important definitions. First, one is the simplest gradient. Let's consider a Let's consider a function f that is from Rn to R and a set Y that is in the form of some point of interest Y naught and some direction vector Ci. And in this talk, we assume that this set Y is twice for linear interpolation. And the simplest gradient of F over Y is just the gradient of the linear interpolation model of F over Y. And there is an equivalent definition for the syntax gradient. Equivalent definition for the simplex gradient that is in the form of some L matrix that just consists of all the direction vectors and the vector delta that is only related to the function values of f over the set y. Another important definition is the center semester gradient. From now on, we denote the previous set y by y plus. So its reflection set through the point of interest y now is just Of interest minus is just this side that contains all the negative direction vectors. And it's easy to see that y minus is also a 4 stat. So we can calculate the simplest gradient of f over y minus. And the central simplest gradient of f over the union of y plus and y minus is just the average of the simplest gradient of f over y plus and y minus. And similarly, we have an equivalent diminution for the standard mass gradient that is in the form of some L matrix and vector deltas with respect to y plus and y minus. So we can see that in order to calculate the center semass gradient, we need almost two times the function values as the simplest gradient. So, but the advantage of the center semass gradient is that if we denote the approximation. is that if we denote the approximate diameter of y plus y delta, then the approximation accuracy of the simplex gradient is all the one with respect to delta, and the approximation accuracy of the center semester gradient is other two with respect to delta. So although it takes more function values to compute, the center semester gradient is more accurate than the simplest gradient in the sense of accuracy order. And also And also from the definition, we can see that in order to calculate the center smart gradient, we need a point of interest Y0, a sample site Y plus that is poised for linear innovation and an exact refraction side of Y plus, which here is the side Y minus. So our main question in this talk is that what if the reflection set of Y plus is not exact? And we suppose the reflection set of We suppose the refraction site of white class is misaligned, and we denote this misaligned refraction site by white hot, which could possibly contain some point near the exact refraction site of y minus. So recall the definition for the center stimulus gradient. An obvious new approximate gradient would just to be simply replace this y minus with y tau and elements with l tel and get this second formula on the slide. Is second from the on the slide. However, we know that approximation accuracy of the simplex gradient is other one, and approximation accuracy of the center simulates gradient is order two. So a natural question is that what about our new approximate gradient? Can it still heighten accuracy of order two? And of course, it's not because otherwise there's no point of me standing here giving this talk. In fact, we can just look at a very simple In fact, we can just look at a very simple example in one-dimensional space. We let the function f to be x squared and we let y naught to be the original point zero. And we like the sample side y plus to be zero and delta. So its refraction side, its exact refraction side is zero and minus delta. And we like the misaligned refraction side to be zero and minus nine minus zero point nine dollar. Minus 0.9 dollar, which is pretty close to the exact reflection set. And then we just calculate these L matrices and magnetic deltas and find that in this example, the approximation error of our new approximate gradient is the 0.1 delta, which is only order one with reside delta. So we can see that even in one-dimensional space and even when the misaligned reflectance is pretty close to the Pretty close to the exact reflection set, we still can only get an accuracy of other one with reflected data. So, our question is: Can we do better than just other one? And that is the reason why we want to make adaptations to the center smash gradient formula. Our idea is that we want to take the structure of y child relative to y minus into consideration. For example, this For example, this is an example in the two-dimensional space. This upper black triangle is the sample side Y. So its exact reflective side is this dashed gray triangle with these two light blue points. And the missile light reflection side is just this lower black triangle. So we can see that the relation between each power point in y tau and y minus is first. y tau and y minus is first a stretching in the radial direction. For example, if we take these two points as an example, then the length between this vector and this vector is different. So there's a stretching in the radial direction. And also there's a rotation with a rotation angle, which here is a theta one. And also similarly for the theta two. And to define these two relations mathematically, we have the structure parameter Ki is given by the ratio between Ki is given by the ratio between the misaligned direction vector di tilde and the exact direction vector di. And the rotation angles is just the angle between these two vectors. And here we line this angle to from zero to pi without loss of general reality. And now we can give the definition for our W Sunday semester screen, which is the first formula on the slide. Actually, we can see that. Actually, we can see that our adaptation is just to add a matrix D in the formula, which is the diagonal matrix consists of the square of all the stretching parameters. And we note that if we let all the rotation angles to be zero and stretching parameters to be one, which means the misline reflection side will become the exact reflection side. And in this case, our adult-center semester gradient will become exactly the same. Mass gradient will become exactly the same as the center's mass gradient. So, in this sense, our adoptive center's mass gradient can be viewed as a general generalization of the center's mass gradient. Next, we analyze the R bound for our W center's mass gradient. This is R bound we find. We like f to be a C2 plus function, both centered at Y naught with radius delta bar, which means the I Which means the f is the C2 function and is a second-order derivative with a Lipschitz on this ball. And we also suppose both approximate diameter of y plus and y taught are less than or equal to dollar bar. And here is the error bomb we find, which looks a bit complicated, but in general, we can just build it as some ugly constant times maximum of zeta i times delta plus another ugly constant. Another ugly constant times delta square. So, which means we can look at this simplified version. We can find that the accuracy order of our W center semester scradium is of order big theta times delta plus delta square, where the big theta is the maximum of all the rotation angles. And the proof for this theorem is a bit long, so we divide that into three parts. So we divide that into three parts. But the idea is quite similar. In the first part, we suppose all the rotation angles are zero, which means there's only stretching occurs between each pair of point in y tau and y minus. And we consider the Taylor expansion of f over the sample point in y plus and y tau and guide the equation one and two. And we can see that if we multiply the first equation by ki square and minus the equation by Ki square and minus the second equation. Then we can cancel the second order term on the right-hand side and get equation three. And next, we can just use up similar techniques as the error analysis of the simplex gradient, which can be found in every classic DFO books, and obtain our error bound for this part, which is some constant times delta square. So we can see that if there's only stretching occurs between There's only stretching occurs between each parallel point in y tilde and y minus, then our new adopted center semester gradient still has an accuracy of other two with respect to delta, which is quite great. And in the second part, we suppose all the stratum parameters are one, which means there's only rotation occurs between each pair of point in y tilde and y minus. And similarly, we consider the Tyler expansion. The Taylor expansion. This matrix, A to the I in equation 5 is just the rotation matrix that rotates di to di taught. So this term will just become the di tau. And we can see that in this part, we cannot simply cancel this second order term, but we can still try to use the same technique as the previous part and get equation six. And got equary six, where there's still a second order term on the right-hand side. But the good news is that if we can find an upper bound for this second order term, then we can still use the same techniques we use in the simplest gradient analysis and get our error bound for this part. So the only question remaining in this part is how do we find an upper bound for this second order term? Our idea is that we separate the major. We separate the matrix in the middle into two specials, metric matrices, S1 and S2. I didn't show the expression for S1 and S2 on the slide because that is a bit complicated and it's not a focus for this talk. I just want to say that after separation, we can find an upper bound for this part and this part separately, which we know that an upper bound is just the maximum of the absolute value of all. The absolute value of all the Eigen values times the length of di squared. And next, we just calculate the eigenvalues of S1 and S2 and get our error bound for this second order term. And also, that is where the maximum of z i in the coefficient comes from. And then we just use some similar technique as the synthesized gradient analysis and get our error bound for this part. Get our error bound for this part. And finally, we just combine these two parts and get our final error bound for the general case. We also did some numerical experiment to explore the properties of our adopted center's mass gradient. Well, firstly, since the reason why we want to make this adaptation is that we think the direct generalization, which is this formula, is obtained by simply replaced by I simply replace y minus with y taught and l minus with l taught in the center semester gradient formula. It's not that accurate. And of course, we want to say that after adaptation, our adaptive center seamless gradient formula is more accurate than the direct generalization. And fortunately, this is true. Theoretical proof for this is included in the proof of our airbound. We also use MATLAB to create some test examples. We create a function. The first one is some summation of sine and cosine function. And second one is some exponential function. And in both tasks, we let 1 to be the first coordinate vector E1. And here's what we find. In the first experiment, we let all the rotation angles be zero and selection parameters to be 0.75. Parameters be 0.75. In both pictures, the x-axis is the delta and y-axis is the approximation error. The blue line is the direct generalization formula, and the orange line is the adaptive center's mass gradient formula. So we can see that obviously our adaptive center's mass gradient is more accurate than the direct generalization. And also, we know that because of Also, we know that because of the machine precision, then in practice, we cannot use, we cannot let this delta to be too small because if delta is too small, then the machine error will take over and cause an increase in the approximation error. That is also what we see from these curves. So we also used our error bound to approximate this minimum or optimal data we could use in practice. In practice, and we mark them by the orange vertical line on the side. So we can see that that is pretty close to what we see from the experiment. And in the second experiment, we change all the stretching parameters we want. And we see that both formula gives exactly the same result, which is not surprising because we have showed that both formula will become identical to the center response gradient in this case. Response gradient in this case. In the third experiment, we change all the stratum parameters to be 1.25 and we got a similar result as the first one. We also did some other numerical experiments, but for simplicity, I just list what we did and our result here. In the first set of experiment, we repeated the repeated the previous experiment. Repeated the previous experiment by fixing all the rotation angles to be 0.1 instead of 0. And our result has the similar pattern as when all the rotation angles are 0. And in the second set of experiment, we want to explore the approximation error relative to the rotation angle Î¸. So we fix all the structuring parameters to be around one and consider theta. And consider zeta from 1 to 10 to the power of minus 10. And we see that as all the rotation angle goes to zero, then the approximation error of our formula goes from order one to order two with respect to delta. This satisfies our theory. Recall that our error bound shows that our formula has an accuracy of other big theta times delta plus delta square. Plus delta square. So as all the setup goes to zero, then obviously the big set of will go through zero. So the error will go from other one to other two. In the third set of experiment, we want to explore the approximation error relative to the structure parameters. So we fix all the rotation angles to be zero and consider structuring parameters from one to ten to the power of 128. Which is crazily big, just out of fun. And also, we see that as all the survival parameters are one, then our formula performs exactly the same as the center semester gradient as we expected. And also, but as the fraction parameter goes to infinity, then our formula performs more and more similar to the simplest gradient. Similar to the simplest gradient. This is also not hard to prove from the definition. We can find that the limitation of our formula when k goes to infinity is just the simplest gradient of f over y plus. So in summary, we generalize the center seamless gradient formula and develop an adaptive center seamless gradient formula. We showed that our formula has an IP. that our formula has an accuracy of other big zeta times delta plus theta square where the big zeta is the maximum of all the rotation angles and we show that when the sample side y is not is not perfectly symmetric then our formula outperforms the center's mass gradient and when y is perfectly symmetric then these two formula performs exactly identically and as usage of our formula is that in Of our formula is that in practice, we could just like the missile line reflection side to contain some sample point we have already evaluated in the previous iterations. So in this sense, our W standard seamless gradient can be used to reduce function costs in DIFO algorithms. So the question is that in each iteration, how do we find the pair of sample set Y plus and Y tilde to make the over? And white head to make the overall approximation as accurate as possible. So, our first next step is to develop algorithms to find the best pair of them efficiently. And also, our work only focused on the determinant case, which means we require the L matrix to be invertible. So, our second next step is to explore the properties of our Davis under semester gradient in the underdetermined and overdetermined cases. Underdetermined and overdetermined cases. So that's all about my talk for today. This paper is still under construction. So, but if anyone is interested in taking a brief look at our pre-trend, then please feel free to email us. Thanks for listening.