We'll be talking about string diagrams for AD. So take it away. Hi, everyone. First of all, thank you very much for having me here. And I should start the talk with a disclaimer and also a warning. So this talk is part of ongoing work that we're currently doing. Me, Dan Gieker, my supervisor, Juan. My supervisor, Huawei, and his collaborators, David Sprunger and Fabio Sanassi. So some details are not yet entirely understood, and especially the notation is very rough around the edges. We still haven't, despite many arguments, we still haven't settled on good notation for all of the diagrams that we want to write. So apologies in advance for any unclear points, which I'm sure there will be many. So first, I should start. I would like to start by talking a bit about automatic differentiation, because in my experience, lots of people in the differential community, especially people who are semantically inclined, tend to be Clients tend to be vaguely aware of automatic differentiation, but they're not familiar with the nitty-gritty of it. At its core, as it's usually implemented, as it's usually used, automatic differentiation is a source-to-source transformation technique that takes a program that computes some sort of mathematical function. Of mathematical function and returns a modified program that computes the same function together with its gradients in a single pass. And a sort of a basic automatic differentiation framework might do a transformation like the one that you see in the screen. So it would take So it would take some. This is very upsetting that I can't spotlight anything without scrolling through the slides. So it takes some basic program, and in this case, I deliberately chose not to have any fancy features. It's just straight line, static single assignment code. This program comes. This program computes a fairly simple polynomial function. And a standard reverse mode AD framework might send this program into this program. And as you can see, this transformed program, which is usually called an adjoint, it has two parts. It has a forward pass that does exactly the same thing as the Exactly the same thing as the original function. And then it has a reverse pass, which, depending on your choice of framework and depending on your choice of interface, can be done immediately or it can be delayed and put into a closure and returned as a function. In this case, we're returning it as a function for reasons that will become apparent later. So there's this forward path that mimics the original function. That mimics the original function, and then this does this reverse pass which takes an argument which is usually called the sensitivity and it propagates this sensitivity back through our original sequence of assignments. And in doing so, it's essentially computing the gradient of the original function. Now, the resource reverse mode AD is important and Is important, and I really can't underline enough just how important automatic differentiation is to modern-day machine learning. And the reason it's so important is because there are only really three ways to go about gradient ascent. You can calculate the derivatives of your neural network by hand, which you're not going to do. You can use numerical differentiation, which Use numerical differentiation, which, especially in the case of deep learning, has a tendency to very, very quickly accumulate unreasonable amounts of numerical error, even for relatively shallow networks. Or you can use automatic differentiation. And so in an ideal world, one might be able to write a program like this, where you write your model as a standard Python. As a standard Python or whatever it is that you fancy, a standard function without having to use any sort of domain-specific language, without having to use any sort of restricted subset of the language. And then you simply ask your reverse mode AD framework to give you the gradient of that. And that program, even in fairly simple cases, as it's illustrated in this example, which is as simple as you can make. Example, which is as simple as you can make it, but you're still using quite a lot of modern advanced features that we take for granted. Um, like this mutable arrays and closures and all sorts of stuff. So the lesson to take home is that AD isn't just a funky theoretical concept, it's an Theoretical concept, it's an extremely valuable tool in real-world programs. And being able to put more language features through AD leads directly to massive usability improvements to machine learning frameworks. So, with that in mind, So, with that in mind, I want to focus on one particular reverse mode AD algorithm. And this is definitely not the fanciest and it's definitely not the newest. I think in recent, especially in the last couple of years, there have been some incredibly impressive AD algorithms published. Published, some of them by people who are attending to this talk right now. This is not one of them. It has a number of very significant limitations, and yet it's still powerful enough for real-world use. Indeed, I should say this algorithm and the particular instantiation of it that I will discuss today, they are currently in use. They are currently in use by some of the machine learning groups in Huawei. So it's certainly not a Toy algorithm. And it has a very interesting peculiarity, which is that it is known to be unsound. In fact, it's very obviously unsound because it starts from the very strange assumption that the tangent That the tangent type of a function type should be the product of the tangent types of every variable that is captured by that function. And if you know your differential geometry, you immediately realize that this makes no sense at all. So it may seem very strange that we're trying to prove soundness for an A-B algorithm that is known to be. For an AD algorithm that is known to be unsound. On the other hand, that is precisely why we're trying to do it. It's an AD algorithm that is known to be unsound in general. So we wanted to know exactly when it is sound and how sound it is. So what soundness sound with respect to what? I won't describe Perlmutter and Siskin's original. Perlmutter and Siskin's original formulation of the algorithm, though you can see here an extremely small fragment of it, because I'm going to discuss our own formalization of it, which if you're not familiar with Permut or Siskin's original formulation, you may have to take my word for it that it's the same. Word for it that it's the same algorithm, at least conceptually. But even if you're not familiar with the original algorithm, this talk will be entirely self-contained. So you can think of this as a brand new AD algorithm and still follow through. So as I said, we decided to frame this original algorithm, which was original. Which was originally given as a bunch of term rewrite rules. Instead of framing it through term rewriting, we decided to frame it through graph rewriting. And this is because internally we have a compiler that is based on graph rewriting. So that naturally fits very well with this approach. And the specific And the specific formalism that we use to describe this algorithm is that of higher-order string diagrams. And so I'm not going to give all of the details, although feel free to stop me and ask any questions if you see anything that you don't understand. But it should be fairly obvious what our diagrams represent. We use black dots to refer to well, both to contraction and weakening. We use these horizontal lines to tuple and detouple things together. And this is perhaps the more interesting feature. I said we do high-order string diagrams and so. Diagrams. And so our diagrams are truly higher order. So a node can be some primitive operation or it can be a big bubble containing another graph. And you should interpret those as just closures. So they can capture some wires that are coming from the environment. Are coming from the environment and they are subject to the obvious rules that you would expect to see. So you can push other graphs in and out of the bubble and you can apply them, which we denote with this upsettingly similar to logical conjunction symbol. So this doesn't mean logical conjunction. So, this doesn't mean logical conjunction, this just means function application. And of course, applying a graph that has been put into a bubble just means that you pop the bubble and you connect the wires in the obvious way. I think this is possibly the most or the only confusing section on the talk on account that the syntax is maybe not the best. The syntax is maybe not the best. So, if anyone has any questions, now is probably a good time for them. And if not, I'll just continue. Okay, fantastic. So, now with this language on hand, I'm just going to give a graphical, purely graphical description of Permutara Ciskins. Perlmutter and Ciskins reverse AD algorithm together with what I hope will be relatively intuitive explanations of why it does incorrect or deliberately incorrect things for closures. So we denote the top level AD transform by this blue box. You should read this blue box is a This blue box is a function mapping graphs to graphs. It's a meta-level operation. It's not part of the language of string diagrams. It denotes that a string diagram is being rewritten into something else. And so this is the top-level transformation. If I go back to my original Python example, this blue. Example, this blue box, what it does intuitively is it turns this original body of this function into the body of the corresponding adjoint function. And so these red and green boxes correspond to the forward pass here in red and the reverse pass here in green. That is Green that is stored into a bubble, which is to say a closure. So, if you think of what this does, the red part computes the original value of the function, and it produces a bunch of intermediate results that are captured by the reverse pass. And the result of the entire computation is a pair consisting of the value of the original function plus these. Plus, the backpropagator for it has been frozen as a thug. Now, we have a bunch of rewrite rules for both the red and the green bubble. So, for every element in our graph language, we have both a red and a green rewrite rule. And I think they're all fairly obvious. For the most part, They're all fairly obvious. For the most part, what the red bubbles do is they leave everything as is. They essentially replicate their contents, modulo some differences that we'll see later. But as you can see, all of these transformations are defined by induction on the size of the graph. So each of these cases take composite. A take a composite graph, transform one of the components, in this case, we're transforming one instance of weakening, and return recursively apply themselves to the rest of the graph. The rules for the green part are a lot more interesting because they're the ones that actually compute the gradient. And if you're familiar with reverse differential categories, you'll see things. See things that are very reminiscent, very reminiscent of the axioms of a reverse differential category. And of course, that should make sense because, after all, we are computing gradients. So, for example, the reverse pass rule for contraction, what it tells us is this graph with This graph with two outputs coming from the same variable that have been contracted, once we try to backpropagate through that, we will have two sensitivities. We will have two sensitivities corresponding to each of the contracted edges. And so, what we do with those two sensitivities is we just add them together. And we will later see that there's an Later, see that there's an analog of this rule among the axioms of reverse derivative categories. And so, similarly, if the original graph discards a variable, the rev transformation has the same effects, which is to say, just discards the same variable or the same computation. And the transformation for the reverse pass just Basically, inserts a zero for the component that was discarded. So, if something's not used, if a value is never used, then the derivative with respect to that value of the result of your function is, of course, zero. This is also relatively straightforward. So, constants just command. Constants just come out as is, and they have no bearing on the derivative of the graph. So they're completely eliminated from the they completely disappeared from the reverse pass. This shouldn't be just zero. This is for any constant. And at the end, we have a rule for when your graph has been entirely reduced. Has been entirely reduced to just a bunch of wires. The computation ends and you just output essentially the identity function as represented as a bunch of wires. Perhaps the more interesting rule is the rule for primitive operators, which does most of the heavy lifting. So we assume that. Um, so we assume that our langu or our language is endowed with a set of um primitive operators that comprise you know the usual suspects, addition, multiplication, division, what have you, sine, cosine. You can have any sort of any sort of first order smooth function. You can put it in there, and the reverse mode ID. The reverse mode AD rules to handle those operators are remarkably very, very simple. And this is the first time that the forward pass does something even remotely non-trivial. So what the forward pass does when it encounters an operator is it, of course, it computes and returns the value, the result of the operation, but it also copies the inputs that were passed to the operator. That were passed to the operator, and these inputs will be captured and passed to the reverse pass. So, if we go back a bit, if we go back to the original rule, we can see that the forward pass not only outputs the result of computing the original function, not only Function that not only outputs the result of evaluating the original graph, but it also outputs a number of intermediate values that are then captured by the reverse pass. If you're familiar with the technical nitty-gritty of implementing reverse mode AD, this list of captured intermediate variables will be familiar to you as the so-called Vengert list or Vengert tape. Or Venger tape of a computation. So, what happens to those input values to the operation that are then captured by the reverse pass? Well, the reverse rule for the operator basically replaces the original operator nodes by the back propagator graph for that operator. That's the main. Operator. That's the nomenclature that we use for it. It's just a gradient. So, what happens is these intermediate values that are stored in the Vengiert tape, if you're familiar with AD or captured by the back propagator, those intermediate values are passed to the back propagator of the operation. And the back propagator of the operation. And the back propagator of the operation is simply something, simply a graph that computes its gradient. And you can see the back propagators of some common operations are here below. You can, of course, give obvious back propagators for division and trigonometric functions and any sort of constant that you might be interested in. But for example, the back propagator for The back propagator for addition throws away the intermediate, the values of the original operands and just takes the sensitivity that is fed to it and just duplicates it on both outputs. But the back propagator for the product, for example, takes the sensitivity that is passed to it and multiplies it. passed to it and multiplies it by each of the temporary value each of them each of the intermediate values that were passed to the original operation and returns those products in in both outputs so you you also have the the back propagators for for toppling and detoupling which are exactly what you would expect Are exactly what you would expect them to be. In general, the back propagator for any linear operation is just going to discard its inputs and do something relatively trivial with the sensitivity that you're given to it. Now, here are the interesting rules, and I think this is the This is the only really non-trivial part of Mouter-Ranciskin's algorithm. What happens when you encounter a Lambda abstraction or a function application in the code? Well, so what we do is when you see a Lambda abstraction, what you do is on the primal side, on the forward pass, you create a Lambda abstraction. A Lambda abstraction that contains the entire the result of applying the entire top-level AD algorithm to the body of the function. So if you remember what the blue box, what the blue process does, if you remember my more or less intuitive explanation of it, you'll remember that this lambda, this lambda This lambda now computes not only the value of the original lambda, but it will also return, it will return a pair containing the original value of the graph f together with a function that will compute the gradient of that graph. And not only that, this lambda is copied. This lambda is copied. One instance of it is passed to the rest of the forward pass, and the other is captured as are intermediate values of operators, for example. The other one is passed to the back propagator. It's captured by the back propagator. On the other hand, what the reverse modes rule for the lambda does. rule for the the lambda does is it expects that the sensitivity for this lambda which is to say the tangent space that it expects to see the tangent space that it expects to find for this lambda is a tuple which is detupled at the input containing one sensitivity for each of the variables that have been captured Variables that have been captured by this lambda. So this rule essentially behaves as if a tangent value for this lambda expression was a tuple containing one tangent for each of the environment variables that are captured in that lambda. And I keep repeating this, but And I keep repeating this, but that this is really the trickiest part of this algorithm. And the part that we know is mathematically incorrect and seems to make no sense at all. So that's what we do for Lambda abstractions. What do we do for applications? Well, when we encounter an application on the primal graph, Application on the primal graph, we know that whatever lambda we get passed as an argument to that application will have been transformed into a lambda containing this adjoint, this enriched adjoint of the original graph. So we know that this lambda is now going to return not only the primal value that we The primal value that we care about, but also back propagator. So, what we do when we apply this lambda in the forward pass is we apply this lambda, which now returns the tuple. We detuple its outputs, and we keep the left side of the tuple as the value of the application in the forward pass. And the right side of the tuple is captured by the reverse. By the reverse pass. And here I also want to point something out and ask a question of the audience, which is the biggest problem that we have with this notation is we struggle to find a way to make it clear graphically that some of these values exist to be later captured by the back propagator. We don't have a super good way. Have a super good way to represent that graphically. So, if you have any ideas, let me know at the end of the talk because to me it's obvious because I've worked with this algorithm for a very long time, but it's very, very confusing to the reader. So, as I said, the forward pass, when it encounters an application, it applies the now-transformed function, obtaining a pair composed of the primal value and the back propagator. Propagator keeps the primal value for the rest of the forward pass, whereas the back propagator is captured by the reverse pass. And if you look at the rule for the reverse pass, what it does is it expects that there's a back propagator coming, that it expects that it expects that a back propagator will be captured, and it applies that back propagator to whatever sensitivity is coming. Sensitivity is coming in to the input of that function. So, essentially, what this achieves is to compute the gradient of the original function. We're computing the gradient of the original function, and the result is detoupled. The result is a tuple. The result of this, if I go back a few slides, the result of this. The result of this is a tuple that contains first the sensitivity of the function with respect to its input, and in second argument, a tuple containing the sensitivity of the graph with respect of all of the variables that it catches. And so, what we do is we dedouble that, we pass the sensitivity that corresponds to the That corresponds to the original input, to the wire that gave us the input to the function. And we pass the wire corresponding to the sensitivity with respect to the captured inputs to the wire where we got the lambda from. And if you remember the reverse pass rule for the lambda, the reverse pass rule for the lambda expects precisely that. The reverse pass rule for the lambda, as I said, acts as if the time... I said, as if the tangent space for lambda was the tangent of all its captured variables. And so this sort of closes the loop, and it means that both abstraction and application are wrong in that they pretend that the tangent space of a function is yada yada, but they're wrong in the same way, and so they cancel each other out. And we'll see later that what this implies. Later, that what this implies is as long as all of your higher-order functions only appear in the body of the graph and not in the edges, this means that this algorithm is correct as long as higher order functions only appear temporarily in the middle of your program. So, here's a worked example. I won't go super into detail. Go super into detail with this, but this is so this function is x squared plus x, except it's written in a slightly funky way. So instead of writing it in the obvious way, we take the x, we copy it, we pass one of the instances to the squaring function, and we pass the other one to an addition, we add to the resultant squaring. And if you apply the rule set. And if you apply the rules above, you see that you get this. You could, of course, continue expanding this blue box and obtain the gradient of the squaring function here. I didn't do it because it really does start taking a lot of space and it's not very enlightening. But what this does is it takes the x that you give it, it applies it to the squaring function, and the result of this application is, as I said, a tuple, the left-hand side. Said a tuple. The left-hand side of the tuple is the result of x squared, which is passed to the addition, and the addition also gets the value of the original function. On the other hand, the back propagator for this, which computes the gradient of this squaring function, is captured by the back propagator of the entire graph. And so, what this does is it takes this sensitivity input, it passes the sensitivity. It passes the sensitivity for the entire graph to the back propagator of the squaring. It takes this result and it adds it together with the sensitivity of the second side of the addition. If you take the rules that I've shown above and you go step by step, you can get. And you go step by step, you can get to the same result. But I wouldn't recommend it because it's going to take like 20 or so steps. We made the horrible mistake of trying to do this in a whiteboard and we run out of whiteboard four times. So after describing the algorithm, there are some technical properties that I want to go over, but I won't go into them in detail mostly. Go into them in detail mostly because they're trivial to prove. The first one is confluence. If you here, for example, you have an example of a graph which you could start reducing by looking at the plus first and then at the times, or you could look at the times first and then at the plus. And it doesn't really make any difference because whichever you pick, the other one is still there. So you can just. So, you can just apply whichever rule you didn't apply in the previous step. And this is true of every single rule that I've shown. Whenever you can apply more than one rule, applying either rule doesn't preclude you from then immediately applying the one that you didn't apply. And so this graph transformation is not only confluent, it is slightly stronger than that. It has the diamond property. So that's quite nice. So that's quite nice. We found that quite reassuring because we were slightly concerned that our implementation did things or did these transformations in more or less arbitrary orders. We were a bit concerned because we suspected that something might depend on which order we happen to apply these rules. Another thing that holds is a sort of an equivalent of the A sort of an equivalent of the chain rule where the red box composes in a straightforward, in an almost factorial way, and the green box composes in a contravariant way, which is what you'd expect to see. And this corresponds to, this actually follows very, very closely to how the chain rule works in reverse derivative categories, which I'll see later if we have time, but I'm starting to think we may not. If we have time, but I'm starting to think we may not have time. And finally, the most important property, and this brings me back to the comment that I made, we know that the rule for lambda is wrong, and we know that the rule for application is wrong, but they're wrong in such a way that they cancel each other out exactly. And so this is reflected in what I call the beta compatibility theorem, which tells you that if you're You that if you're transforming a graph that has a lambda abstraction that is immediately applied, you could have you can first transform the graph or you can do the beta reduction and transform the resulting graphs and either way is completely equivalent. And so this is, I mean, this the proof of this is. This is you'd imagine it's involved, but it's in fact relatively straightforward. I won't go over every step individually, but basically we just apply the rule for application. And here we see that the primal part of the application, again, has two components, the second of which is the back propagator that gets captured by the reverse pass. Then we apply the lambda rule. Then we apply the Lambda rule. The Lambda rule, as you see in the forward pass, turns the original Lambda into its adjoints. And in the reverse pass, as I said, it expects that this wire will contain a couple with all of the tangents of the original graph with respect to all of its captured variables. So it just detouples them. And here we can start beta reducing this graph. This graph, and you get the obvious thing, and now you can beta reduce this resulting graph. And here you need to expand this. So, if you expand this blue bubble, you get the toppling of the forward pass and the back propagator that comes from the reverse pass. So, we do one step of the blue rewriting, and we get this. Blue rewriting, and we get this. And now we d double. And now we can do, we can beta reduce the back propagator of the original function. And now all that's left is a bunch of shuffling of toppling and detopling. And what you get here is here you apply this forward and backward chain rule that we mentioned before, and what you get is exactly what you intended to get. Is exactly what you intended to get. I don't know why there's not another slide with the final result, but this is fairly obvious that the result is exactly this graph that you see in the right. Now, here comes the part which we haven't entirely finished yet, which is exactly what kind of soundness we intend to prove and how we intend to prove it. How we intend to prove it. The idea is to prove soundness with respect to reverse derivative categories with the caveat that reverse derivative categories only work on first order types. To the best of my knowledge, there's no such thing as a reverse differential lambda category. But then this algorithm itself is only correct. Correct for first-order types. So, what we expect to obtain is that if your graph F only has first-order inputs and outputs, which is to say, if you're never differentiating with respect to a variable of higher-order type, what you get from this AD transform is equivalent to what you would get from applying From applying a graph rewriting version of the reverse differential axioms. And so this graph rewrite version of the reverse differential axioms, I don't have much time to go into this because reverse derivative categories are a topic unto itself. But the grade interpreter from reverse derivative categories will represent with this purple box. We represent with this purple box and it obeys some very basic rules. So these are just string diagram versions of the reverse derivative axioms written in a slightly nicer form for our formalism. So you've got the gradient of addition is just copying, the gradient of constant is just nothing. Nothing, the gradients of functions, a pair of arrows, and projection. This is a generalized projection rule. And finally, the reverse chain rule, which looks more or less exactly what the reverse, exactly like what the reverse chain rule looks like for Graph 80. Now, the way we're currently going about proving this soundness result is we're using the beta compatibility theorem to say, well, if your graph only has first-order inputs and the first order output, and all of the primitive function symbols that you have have first-order types, then Then, whatever higher-order terms you have in your graph can all be done away with through beta reducing your graph enough times. So, essentially, any high-order terms in your graph will disappear after enough beta reduction. Or, in more operational terms, if you do enough partial. If you do enough partial evaluation at compile time, there are no higher-order terms in your program. And because of the beta compatibility theorem, it doesn't matter whether we do those beta reductions before or after. So we can say that the AD on our original graph is going to be equivalent to AD on a fully beta-reduced graph where there are no longer any higher order terms present. Any higher order terms present. And for graphs with no higher order terms, it's in fact very, very easy to prove, although I won't do it here, that every purple rule is equivalent to a bunch of red and green rules. And so proving soundness for the first order part of our AD transformation is in fact almost trivial. almost trivial the key the key insight is that you can eliminate all of the you can eliminate all of the higher order parts without compromising soundness because as I said whether you do beta reduction before or after AD doesn't matter at all so this is all this is all I wanted to say I'm just going to I'm just going to try and make it even clearer what point I'm trying to make. Ultimately, the main sort of claim that we're making is not a claim about this one AD algorithm. It's rather a claim that stream diagrams make it extremely easy to both express and reason about reverse mode AD algorithms, which Reverse mode AD algorithms, which are notoriously hard to develop, they're hard to describe, and they're hard to implement and they're hard to reason about. On a personal note, I've implemented reverse mode AD algorithms in actual code and the graph-based implementation is the only one that I actually feel confident is not. Confidence is not only theoretically sound, but that the implementation actually really does do what the specification says. This is part of an upcoming paper which will do the soundness proof in gruesome detail. And it also contains a formalization of exactly what these high-order string diagrams mean. These higher-order string diagrams mean, and what rewriting rules for higher-order string diagrams actually are. Although that's not my contribution to the paper, so if you have any questions about that, I'm probably not the right person to ask. So thank you all for your time. And I think if there are any questions, now's the time. All right, let's first unmute and thank Mari. First, unmute and thank Mario. All right, we have time for questions. Jonathan? Oh, yeah, sure. I'll throw out just maybe more of a comment than a question. There are models of reverse. Models of reverse differential lambda categories. One of them being the co-classy category of RHEL, and the other one can be gotten from a category described by Selinger and Valeron for a semantics for quantum computation with classical control or something like that. That's fantastic. Are those what's the what's the tangent space of What's the tangent space of a function type in those categories? Right. Well, the cotangent space, rather. I actually don't know what the cotangent space looks like in itself. I'm going to guess it's probably something like a pullback of probably like A cross B or something. Can you, is this published? Is there any Published. Is there any place I can read about this? I've been very, very interested. I don't know. I don't know if I think in the Cartesian reverse differential categories, the category RHEL was mentioned. And it is, it's Coklisi category, or the Co-Klisse category of RHEL with the multi-set co-monad was mentioned. And that's definitely Cartesian closed. And for the set. And for the Sellinger and Valeron one, there's a paper by, I think, Rick, Bloot, and Keith and Rory, where they show that a certain kind of co-monad always has a differential modality on it. And then you can show that the example given by Selinger and Valeron is one such kind of category that fits into their framework. So you can, I haven't worked out like. So, you can, I haven't worked out, like, I don't know if anyone's worked out exactly what the deriving transformation, as it's called, looks like there, but you definitely could just kind of engineer it from that from that paper by Rick and Keith and Rory. I'd be very interested in reading more about that. Whatever comes out of that will be very disappointing for me because, as I said, whatever models there exist. Models that exist of Cartesian closed reverse derivative categories, this algorithm is most certainly not sound with respect to them. And this much we know. But it would be interesting to see exactly how unsound it is. Robin? Okay, thank you. Yeah, you you showed all these rewrite rules. All these rewrite rules, um, and maybe I missed this, but did you say that they were confident or not? Yes, yes, they are, they are. Um, I mean, I mean, this is this here I only have one case, but essentially the argument is always the same, which is if you go to these rules, they all take a node in the fringe of A node in the fringe of the graph and do something to it. So, if at any point you can apply two of them, if at any point you could decide between which of them to apply, applying one of them is never going to preclude you from applying the other one because they apply to this joint part of the fringe. Notably, the chain rule is not one of these. Not one of these rewrite rules. So we, if we, for example, if we had the chain rule as a rewrite rule, we would have some non-determinism to choose how to partition your graph composition. But we don't do that. We always... You could think of all of these rules as being the chain rule plus some other axiom. Some other axiom applies to one of the, I'm not sure what word to use, outermost perhaps, terms. Does that make sense? Yeah, so you're saying it's an orthogonal system. Yes, that's exactly what I'm saying. Okay. Thank you. Any other questions? Uh any other questions? Um I don't know if you saw in the chat, Mario, sort of a missed question, but Bruno was asking if you wondered why it's called an adjoint and whether this has anything to do with adjunctions in category theory. But um, no, it has to do with matrix adjoints, yeah, um, it has to do so so the It has to do, so what you're computing in reverse mode AD is the matrix adjoint of the Jacobian matrix multiplied by the transpose of whatever vector you give it. So it's adjoint in the matrix sense, not really in any sort of interesting categorical sense, although I'm sure. Sense, although I'm sure I instantly regret saying this because I feel like there may very well be somebody in the audience that can make the precise point that actually linear adjoints are categorical adjoints, but it's not an adjoint in an interesting sense. I think you're right. Um, all right, let's uh thank Mario once again. And we've got about five or five-ish minutes until the MyTax talk. So as Christine said, those of you that aren't