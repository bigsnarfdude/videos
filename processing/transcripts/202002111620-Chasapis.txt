To introduce Let's remind you, because many of you might know it, of general problems, let's say on complex bodies. Now everything that I'm going to mention is part of a paper with Nikos Karmogenis. Okay. I don't have any big results present or technical proofs. Just a few comments, a few remarks. So, K generally will be a convex body in RN and there will also be a small K. A small k which uh is a positive integer between smaller than n. Okay, there's a conflict here between this small k and this character k, but it will be clear from the context. And uh the object of study is uh this thing. You take the topographic projections of uh k onto the subspace f, uh this raised to the minus n, and uh Minus n, and uh you integrate this quantity over the n-dimensional uh subspaces. So integration here uh from some moment on I will just denote it by this. But uh now let's be typical and uh you have a usual uh uh probability measure on the gas magnet. So you raise this uh minus one over k m. minus one over km and uh you could consider this uh quantity but uh and work only with uh bodies of volume one or else you can normalize by the volume of k and call this thing uh phi of the body. Okay, phi k of the body. And this uh has a name. This has a name uh and it's the case uh normalized if you want uh a fine graph in the curve of k okay now of course this thing reminds you of the usual pronuncia. Uh these quantities were uh These quantities were introduced by Dean Nico originally under a slightly different normalization. And the term I find here is due to the fact that these things are invalid. Things are invariant under a faint transformations. So under volume affine mass. Okay, this is a thing proved by Greenberg. Now there's a number of uh open problems regarding uh risk quantities, uh mainly posed by Ludwig. Uh let me just uh state one of uh risk conceptions I'd like so it's the following that uh for any convex body k and uh for all these uh dimensions k Okay? Um this quantity uh is uh minimized when the body is uh the ball. Okay, so this is the uh Euclidean ball. And uh okay, this conjecture has the sense that uh you know it is known, it's a known thing in the extremal cases. So for example, you know it when k is equal You know it when k is equal to 1, because this coefficient of the Sadaver inequality. And also in the case that this is n minus 1, this also has a name, it's what you call the petty projection. When okay, so the color the question is what happens uh in between. And uh going on to something uh less ambitious maybe uh the second problem was something that as far as I know was uh formulated by uh two Greek people. Okay, so this is a Greek problem. Uh this is uh Nikos Davis and uh Grigoris Powell who studied uh Study the, let's say, an asymptotic version of this thing. Which is the following. Can you find some absolute constants such that these quantities always lie always are, let's say, a vector of squared of n over k directly. And again, this has some sense because you can see that, for example, if k is equal to one, then again you have it. Okay. This is the reverse Santa Lohan equality itself put k. Okay, this one. In the other extreme, again it is known to hold. This follows from a reverse PET projection inequality. This was inequality proved by Zhang. But uh this is not uh This is not this thing is not known to call the case for any arbitrary dimension. Okay, let me state what is generally known about this thing. So, in the very same work of philosophers, they prove, I think, the only general known, let's say, the upper bounds. Let's say the upper bounds. Um so for every k well up to a constant and we'll use this notation always okay to denote the qualities that hold up to absolute uh constants. Um this is smaller than This is smaller than whatever is better from the these two. So the one bound uh is a bit uh farther than uh what you would like. So it has this term, the three over two uh times the root of uh logarithm of what segment. So this is uh it looks bad, but uh on the other hand this But on the other hand, it's good because if k is proportional to n everything disappears and you have an absolute concept that you have the correct ingredient. And on the other hand, you also have the k squared of n over k, what you would like, times an extra log n. Exactly the problem is here, this thing, okay, what you can do with this. But before going further, I would also like to mention the lower bound. This is something proved by Grigoro-Holbes and Peter T. Obarov. So you know that the lower bound holds for every convex body and for every you have a lot of the One of the order, let's say, of square of another. Okay. And the purpose of the data is to give some indications to what one might hope for, let's say. So, the plan is to tell you about some positive things for random sets and to tell you some And uh to tell you some to ask you some questions, maybe uh that have to do with a partitional case that also boils down to a specific polytope, not the random one, uh the cross polytope. Uh but let's see so first of all uh what can you say about uh random points? Okay. Okay, here what we are going to do is assume first of all that K is in the isotropic position. Okay, we have seen the definition before. This means that K is a center slot expanded. Its volume is equal to one and all the several moments here. Are the same independent of theta. And let capital N be a number that lies in this regime. So bigger than the square of the dimension, smaller than is the square root of square root of E and uh denote by KEN let's say the convex real the absolute convex like of uh capital n points where you can choose these ones to be the Independent and identically distributed according to consider the two cases, okay? So according either to the uniform distribution of the body. Uniform distribution of the body. Okay, let's denote this by lambda k now. So this is just the measure that gives to any set its volume, let's say, any subset of K. You always normalized by the volume of K, but now you have that K isotropic. Or the second uh measure would be uh what you call the cone measure. What you call the colon measure on the boundary of K. So this would be the measure that gives to A the volume of maybe what we would denote 0.1 times A. Whereas well then Well then in any of these two cases uh one can show that uh for any dimension for any dimension of the subspace uh a fine permanent integral could satisfy uh the boundaries you like uh with uh probability so a relevant probability to the right. Sorry, really not drawing to the side. Okay, now the proofs uh let's talk a bit about uh the first things okay. For the The first thing, okay, for the uniform measurement. The proof relies on a very simple idea and some very deep facts. So the deep facts are that you know many things are already known about the geometry of El Sado Puerto in this case. And uh use what's known on the asymptotic shape, let's say of k in the in this case. Okay, these are results. I think I should mention the two works of uh Daphnisian opposition celebrities. And to statement down there, this is Kn or this is Kn, of course, yes. Okay, the statement is about the point of this case. Okay, uh so So if uh the number of points is uh in this regime, then you know uh you have information on the volume of uh the voltage, okay, and uh also more things. So first of all, the volume of uh this thing is uh up to a constant bigger than the order of growth is uh bigger than the order of growth is uh a logarithm of uh capital N over the dimension over N times the stroke constant of uh the body. Okay, this uh curves with high probability. I do not care that much about exact uh uh estimates of the probability. Uh and the second thing is uh an upper Another thing is an upper bound that you have for the mean width of the convex plane. So this would be up to a constant smaller than the log maximum of the number of points times again the isotropic constant. Okay. And uh on some sky origin. And uh quite easy enough to give you what you want. Uh give you what you want. Uh first of all, if you have this uh bound, the first thing that you do is uh that you can uh you can bound the averages of the projections, which you give. So this average, let's say. This is a decreasing function of k. Into k equals 1, this is the equal to the mean divided of k. And we get here, omega k denotes the volume of the Euclidean ball, of k in the dimension k. So this is the Alexandrov inequality. And then using this bound Well, then you can use Markov. So the volume, the measure, let's say, of the set of subspaces for Set of subspaces for which you have an upper bound on the projection, the volume of the projection. There will be, well, if I put two times C here, this C is the this constant that is hidden here, see, times the volume of the ball times this. Well, this uh would be bigger than uh one minus two to the minus k. Okay, so this is Marcus Negro. And uh then we are done because uh now if uh you know that Km satisfies both A and B Then, recall that this thing, when this was the volume of k the minus one over m. So you have an upper bound for this. Uh this now is the denominator. And uh on the other hand let's write it. You have uh the integral of the projections. But now you can restrict yourself and uh change this to integrating uh minus of Think uh minus on uh the set A, let's say that this uh this project. Okay, because you have a minus sign here, so you go uh something big here. But then you can use this property that here the projections are uh bounded and uh here we have that. Well using this uh bound it would be like the measure of uh A times uh whatever is in here this would be to K. Kn and then all this to the minus one or Kn. Okay, if I'm writing everything correctly now, to the minus. So in the end, what you would yet want would be a constant, let's write this like now, like this. The isotropic constants cancel out each other, and you would have this. You would have this then over d h times this square root of n over the square root of n, or over the square root of k, which happens, which is here. Okay? And then the last step, this thing is smaller. It's smaller than an absolute constant, if you take tend to be bigger than squared. So I think this is the only step where you need to restrict yourself to something bigger than the square of the dimension. Now, all this could be stated. I think that everything all this thing is correct, even if you use instead of the uniform measure on an isotropic body, any isotropic local cave measurements. Because these things are Uh because these things uh are also known in this case. But uh uh this case is different. I mean uh you wouldn't get the results for the con measure uh uh immediately by this because uh this is not uh a low concave measure. Okay, so one has to look what happened, what happens in this situation. Well, I'm not going to say much. I'm not going to say much, but for this for the gold measure, you have to prove that analog, let's say, of this result. And we can do this without many details. You can give lower. You can give a lower bound of the same order with the same probability using what some people call a coupling argument. So if you have a random vector distributed according to lambda k, then with probability one, let's say the vector that you take if you normalize x, call it this y. Call it this y. This is well defined because with probability one this is not zero, okay? Is on the less on the boundary and is distributed according to has the same distribution as random vector according to the controlability measure. Plus the convex half of uh let's say if you take n Let's say if you take n independent copies of y, is something bigger than the corresponding random point of that is defined by random by n independent copies of x. So this gives straight, this shows you that a random port loop in this case is something bigger than what you get from the What you get from the uniform measure, and so the same governor bound holds. And now for B, okay, I will not do the proof. Essentially, you have to do whatever has to be done in the case of the uniform measure. So it boils down to comparing, let's say, the mean width of the politop to the mean widths of to the mixed widths of the tubules and joint bodies. Of the tube and joint batteries for this value of this order. There are some things that work exactly the same way. There are some gaps that you have to fill, but it's more or less a matter of computation. So we'll give this. And now how much time do we have left? Okay. Now something else that I want to say that might be more interesting is what happens in the case of a cosmology. Okay, so generally if general conditional, let's say, body. And also, first of all, this case boils down to the case of the cross-boilt. This is an immediate using factor, let's say, caused by Kovan Nazarov. So this is after an absolute constant any stroke can possibly. Any isotropic and conditional parties containing uh isotropic ematal, let's say, cross-point to. Um now let me consider uh something more general than this. You can uh define uh for a general exponent uh the corresponding uh alvertis. So you can see that due to this inclusion you have that for every exponent this quantity is something smaller than the okay up to a constant again this constant what you have for the first point The plus polygon. Okay, this n would be the due to the volume of the plus polytrop. And then everything boils down to giving to examining the case of the ball of L1, let's say. And now the approach that one can use is the one that was used here, alright. One that was used here, more or less, this kind of idea. So if you can come with a measure of the subset of subspaces for which the projections are well bounded, and when I say well bounded, now we need something of this sort, actually the spirit of the K times N. k times n. Well, if this is uh bigger than, say, some constant theta, okay to the k for some theta to a smaller zero and one. Then uh using this reasoning exactly, you can see that uh what we get for this quantity will be something smaller than what we want times this delta t minus one over p. So the point is to achieve such an inequality for a good constant here, okay. In particular In particular if uh we could have let's say delta equals to the e to the minus n then you would have what you want this thing is this thing for p plus minus n okay so this would be smaller than Smaller than, I don't know, V times. Okay. But all my attempts up to now were not successful. You always have something goes here. I don't know how to do it. Okay. Maybe I could mention a couple of these attempts, but now that I resolve, so I don't think you're going to hear about this. That's all. That's okay. What about this couple? About this? Couple. Couple? Yes. You're not talking about couple at the end, but you said you have to. You said maybe I mentioned a couple, but I don't hear. You want to hear about this? Briefly. Briefly. Okay, two things. The first one I can have something only for with a restriction on the k and it's Uh and it's not of uh the connector that I mean. The the upper bound here would be something worse. It would have also a term like this over K L M S and uh but the probability is bigger than what you would like it to be. You don't care for that big probability. You don't care for that bit, probably. You only be this would give you that uh this thing is well bounded. This is the ball uh essentially for all p, okay, for all uh p, except of s something very small. The other thing, if you try to stick to the correct uh order of growth here. The correct uh order of growth here. Uh you may have that for any t the volume of uh the projections are at the constant square root of k n. Uh but you don't have good enough probability. Probability will be something like a constant another constant uh over n to Over n to the k over times n minus k or to something like this. Well, this would give you that this thing can be bounded by what you want, or if it is something bigger than what you wish to mute. Something like this. So, you need this with something better. This is better. Let's send the speaker down. 