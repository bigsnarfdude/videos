Speaker for the morning, Peter Fritz, who's going to tell us about Bradian rough paths in irregular geometry. Just take it away, Peter. Hi, everyone. Thanks for having me. And sorry, I can't be there in person. I'll do my best online. All right. This is joint work with two fine people, Sebastian Andres, who moved to Germany, moved back to Germany recently to Braunspeck, and my former PhD student student. My former PhD student, student Yijiang Guan, who is now in Cambridge. All right, so it's about Brownian rough paths in irregular geometries. And fear not, I will introduce most things that I'm talking about. And in the end, somehow that last that go into some details about our project here, about our program. So, first, there's a lot of missing place where we just prepare all the ingredients. So, but let me start with some. But let me start with a picture which I stole from YouTube. And this was a student of Nathaniel Persticki, who produced a lovely motion picture on this particle dashing around in this wild surface. And that's a picture of what's called Liouville-Browner motion. Of course, the real Liou-Will-Brown motion lives in a surface that is somehow zero and affinity all the time. So this is a Affinity all the time, so this is a modified version of it, but anyway, it looks great. You should check it out. Um, this was introduced some 10 years ago, this object world motion in two basically independent papers by Nathaniel Perzdicki and Garbon and co-workers at the time. And basically, as I said, it's a diffusion, it's a brown motion on a surface, and the surface happens to be So, you can use R2 as a global chart if you want. So, it's a graph. So, and you know, what can we say about it? And this surface is random, and there's very precise motivation why you want to look at it. I will tell a little bit more about it. The question for us is not so much to give a talk about Liabil Prime Motion here. I mean, in part, I will, but the question I want to raise is: does this process, which is Process which is clearly important. Does it admit a reasonable rough pass lift? So, in my view of the world, it's an important property for a stochastic process if we can understand it as a rough pass or not. There are many reasons, but again, this is not a course on rough paths. So, what is the background? Well, somehow in the spirit of this meeting, stochastic differential geometry. How do we construct Brownian motion on some Brown and motion on some, let's say, general Riemannian manifold. There are several approaches. The poor man's answer is you write down the Laplace-Beltrami operator in local coordinates, and then, you know, provided everything is nice enough, you proceed with classical stochastic calculus, ETO, SDEs, etc. The better, the much better answer, of course, is that you lift the whole thing to the autonomal frame bundle. This is the famous construction. Autonomal frame bundle. This is the famous construction by Elise Elworthy Maliava. And up there, this takes the generator takes some square form. So you can solve it globally as a Stratonovich STE and then project it down. Very elegant, very sophisticated. There's a third perspective, which is the Dirty Form perspective. And so it's the observation that the Laplace-Pertrami operator actually takes a natural form of gradient composed with divergence, where this is the. Gradient couples with divergence, where this is the Riemannian gradient relative to the metric G here, and the divergence is relative to the Riemannian volume measure. And from that perspective, Brown motion is a process, or the technical term is a Hunt process in Dirty-form theory, associated to this Dirkley form, which is just what you get by basically, you know, it's a quadratic form here. You have the generator here, then you take the inner product with the volume form here. With the volume form here, is the F here. You do a little bit of integration by parts, and everything is somehow set up in a nice way that you have here exactly the gradient square relative, of course, to the metric that you have. And here's the volume form. All right, this construction, so any of these three constructions, apply in particular to this Pranum motion that we have seen on the previous slide where you move around on this nice surface or actually not so nice surface. Actually, not so nice surface. So, if it's a nice surface, then of course this works. If it's not so nice, then you need some new ideas, and that's where we're heading. The second ingredient here is we want to understand things as rough paths. So, what is a rough path? Following Terry Lyons and many others, that's first and foremost, that's a path. So, y is a path that takes values in D dimensions. Values in d dimensions, and let's think it has a sample path regularity or regularity like a brown motion. And then you try to attach the second-order object here. So then there's an actual question here. How do you define this? And if it's a brown motion, then you can use etocalculus. And if it's not, then maybe it's less clear. If y is very smooth, then of course this is canonically defined. And this you can push this up to in the young regime. In the young regime, when the p-variation exponent p is less than 2, then this works canonically. Otherwise, you should think of this somehow as given from somewhere else. So it's an axiomatic attempt to postulate the second order iterated integrals. So if y is smooth, you could take this as a definition. But if y is not smooth, it's rather the other way around. You give somehow axiomatically this second order information here. And then when All information here, and then whenever you need iterate integrals, somehow you use this a priori information. This is the way to think of it. So, in any case, if you want to write down the right axioms here, which we are not doing here, let me just mention there's a mixture of algebraic and analytic axioms. And algebraic ones are rather simple. Namely, you try to figure out what happens when you patch together these second-order increments. And because of this re-centering here, they're not edited. Centering here, they're not additive. So, what you have is something non-linear, but it's not complicated, and that gives a so-called chain relation. That's very easy to write down. It's not so interesting. It's important, though. The other thing is unique regularity. I already said if y is somehow in that p-variation regime with p less than 2, then this is well defined in the so-called Young theory. If P is greater or greater, equal, or greater than 2. Or greater than equal or greater than two, then this does not work. And so then you have to postulate additional regularity for this one. So this is, let's say, p variation, and then this you would have a scaled version, p half variations for these second order increments. And this works in the regime where p is between two and three, which is for diffusions, of course, that on Martingale's right regime. Now, I said already, in general, in this regime here, the right-hand side is not well defined. Is not well defined, so we postulate it. There's some abstract nonsense theorem that says when you, you know, given some path, paration path Y, you can always find some in general, non-unique second-order information to make this pair rough bars. So let's make sure you're not talking about that. On the other hand, this abstract nonsense construction is not useful because, or usually not useful, because when you have it in Because when you have some application, Y always has some stochastic or often has some stochastic structure, and you want that live to respect this one way or another. And so, if you do some abstract nonsense, then this doesn't. So, same in Martingale, of course, then this is done by stochastic integration. We talked about this already. And let me also mention this Wong Sakai approach. So, if you don't, if you're a physicist or like-minded, then you can always try to take your path or your project. To take your path or your process, you modify it, now everything is smooth. Then these iterated integrals are well defined, and then you try to remove epsilon and hope for the best, and you know, it may converge. So, fingers, let's spell this right, fingers crossed. This works, this construction works, of course, for Simon Martingills. I mean, there's a question about the uniform bounds and this p-variation, perhaps, but otherwise, this is the Monk-Sakai theorem that you know in action. There are classes of Markov processes. Are classes of Markov processes, Gaussian processes, which are outside the semi-martial world, where this still works, and this is more or less, I think, nicely written up in my book with Victor from 2010. So this is the second ingredient. And on we go. Let me have some geometric view on rough pass. What I said here, basically this general relation, this would work either way. You know, if you think of Ito Strotonovich, there's clearly a difference between Ito. There's clearly a difference between eto and stratonovich, but for this generalization, it wouldn't matter. This just captures somehow the additivity here of the uncentered increments. And when you put this recentering, then something comes out that would be the same for Ito-Stotanovich. If you want a second-order calculus, or somehow a first-order calculus like Stratonovich, then you need to make an additional assumption. And the first thing, let me remind you how in stochastic calculus we define. In stochastic calculus, we define the bracket process. So, if S is a D-dimensional steam martingale, so this is vector-valued, and this is also vector-valued. And let me remove this here, the typo. So, this, you know, this product here is matrix-valued. And this mat bold S here is somehow, you know, in the same spirit here. These are these iterative integrals, and then you can transpose it. And this is just, if you spell it out in coordinates, this is just. If you spell it out in coordinates, this is just a definition of the covariation brackets of Si Sj. And so in this tense notation, this becomes what I wrote. And you can mimic this here in this rough pass context and therefore define a bracket of this rough pass in this way here. And if you don't like the bracket because you want first-order calculus, then you just say, you know, rough pass is geometric, means like Stratonovich, when this bracket disappears. This bracket disappears. And in that case, this is the symmetric part that you somehow can recover from the product here, then only the anti-symmetric part matters. And this reduction here has been very popular. So instead of looking at y and iterate integrals, you may as well look at y and the anti-symmetric part. It has this pretty interpretation as an area. You can draw pictures where you put some area here, but let's not do this. And this pair here again lives now in. Lives now in, you know, this is anti-symmetric, so the space of anti-symmetric two tensors is SOD. Here, we live in this direct sum here, and that's as a linear space, it's clear, but it also has a structure as first as a Lie algebra. I don't fix the spelling here, Lie algebra. And as always in this needlebullen settings, you can turn it into a Lie group by just putting the Baker-Campbell cluster formula on the same space. So this one becomes also a Lie group here by putting in. League group here by putting in the truncated Baker-Kappelhausdefundel. Okay, so there's a group. Where's the one element? Well, if you take the post-zero, then this is the one element. And this is really the most basic, or perhaps the most, a very basic example of a Carnot group. If d equals two, then we're looking at a three-dimensional Heisenberg group. And let me also point out that because of a group structure, if you, if on this space here, if you take the core. If on this space here, if you take the coordinates directions at the origin, so I think of them as vectors, then you can just move them around and you get left invariant vector fields on the full space, the so-called horizontal vector fields that we call UI. So for this community, I think this will be nothing shocking here. Now, let's move to Brown in motion and how we turn it into a rough. And how we turn it into a rough path and later to Markov processes. So, if you have d-dimensional Brown motion, you know, and we want it to be geometric, then we can achieve this by using the Stratonovich integration here. This is just what I described abstractly. This is carried out in the Brownian setting here. Now, this anti-symmetric part here, it's called the Levy area. Okay. And this pair here, Brownian motion plus Levy area, which is one, one with the Area, which is one, one with the geometric brown rough mass. This is, I mean, most people just call it brown and rough mass, but it's also somehow. Um, so let's look at the structure. Um, it has why this group structure here. Well, this group structure is precisely what makes this process to have independent stationary increments. So, we're really looking at this here is really a special case of a Lie group-valued Leby process. No chumps. Levy process no chumps here, of course. It's also Markov, and the generator is pretty. So it's like the Laplacian, but with the partial i's replaced by ui's. So it takes Hermander some square form. You just have to use the horizontal vector fields. Okay. And it's symmetric Markov process. And what is the Dirty form? Well, here it is. Okay. So if you drop the hip here. Drop the hip here, which is for hypoellic. If you remove this, then everything you're looking at is somehow classical for Brown motion. But if you work with this somehow sub-Laplacian here, and then you do an integration by parts, then you see what you have here is the so-called sub-gradient, which is just basically the UIs acting on F lined up and then taking the inner product here with itself. H is the H is the HAR measure, and conveniently, the HAR measure here, title here, the HAR measure here is conveniently the Lebesgue measure on the Lie algebra. So we realize this group here on the Lie algebra. And if you just put Lebesgue measure on there, this turns out to be the Haar measure. So it's very concrete. And so this H is Haar measure, and this is, of course, not the H is Ha measure, and this is, of course, not the same H as this one because this is the domain of the steady form. So the classical in the classical Brown setting domain would be H1. It's here somehow H1 hypo elliptic, which I hope makes sense. Now, there's a general principle that you can, I mean, what you see here, it's everything looks like the Browning case. You just have to replace the partial i's by the u i's. And this is not a coincidence, you can make this somehow. You can make this somehow understand this from a much more general Markovian perspective. And here's some reference, but we don't need it. So let me move on. There's a bit more background. So I apologize to all the stochastic analysis teachers here in the audience, because this would be an example in your stochastic analysis class. If you have a Markov process or Markov diffusion with generator L, and then you're interested in In understanding the process where the generator L somehow is multiplied by sigma square, and sigma square, I mean, you know, L acts on functions. In the end, you get something depending on some space variable. So this here in general should depend on the space variable. Okay. So as a first guess, you can think sigma constant. And then if you look at the, for instance, at the backward equation or so, you see that this is just a time change. This is just a time change. And so this picture persists. And so, more generally, what you can do is for a given spatial function sigma here, you set up this random time here. So time change. Then you can do a little calculus here. And so, for instance, you can derive this quantity here that is instructive. And so if you change your B here, B can be any Markov process with diffusion with L here. So, diffusion with L here, and you look at this B composed with the inverse of this clock F here, so that's still a time change. Then, this is Markov with generator sigma square L. Okay, so it's an easy exercise. You use it, you prove it usually, just writing down a multiple problem, and then it's quick. Now, the reason I insist on this construction here is that this thing here is important, and it's an example of an abstract concept that maybe you do not. Abstract concept that maybe you do not teach in your first stochastic analysis class. It's a positive continuous additive function. In short, P C A F of the coordinate process B here, which I mean, you do this on the passports. Now, this is clearly takes values in. If you look at it, you know, values in C affinity, it's adapted. Okay, well, B is adapted. You'll only look up to tau here, tau here, it's adapted. It's also non-decreasing, continuous. Decreasing continuous, okay. I mean, clearly, under these assumptions here, that's all true. And also, if you look at this expression here and you do it from A to T, and then you compare it with A S, where you also shift this guy here, then this just becomes additive, and this gives you A T plus S. So you need the shift operator here on the canonical coordinate space. Bar space. And these are the axioms of a PCAF. Two examples. Well, this one. Two examples. Well, this one was the motivating example. Another one, just to make sure we're not talking about some abstract nonsense here, the local time is another important example of PCAFs. Next one, symmetric Markov processes and Diracle forms. And now, so I have to stress you with a bit abstract setting here. So the state space for us, it will always be this group G here, which is the local. G here, which is locally compact, it's a perfectly nice space, but so you can do this abstract and just look at some locally compact Polish space. You have some reference measure. The later fastest would be the Ha measure, but it can also be other stuff that is given to you, a rather measure on E. And then here is the dirty form, an abstract directive form with its domain, and there's some extension of the domain that I don't want to get into. Want to get into. And associated to the setup here, you have an symmetric Markov process, symmetric relative to this radar measure here that is built into your, I mean, it's part of the setup. Examples, brown motion, we have seen this. Another example is this one is which actually, in case James Norris is still in the audience, spoke earlier this week, he gave me this paper by Strook many, many times. Gave me this paper by Struk many, many years ago. I'm still grateful to him. This is a very nice example of a symmetric or gives rise to a symmetric Markov process that, in general, if this is somehow only uniform elliptic measurable, is not same in Martingale. So, formally, you do integration by parts, and then for the daily form, everything is very nice. Whereas, if you think of this as a generator, I mean, this is a big headache here. What's the domain of this generator? If this is, you know, you can't even differentiate this. So, very bad as a generator, very nice as a daily form. Is a generator very nice as a daily form. Brown and rough paths, well, we have seen this example already, and they are from a geometric perspective, there are special cases of Brown motions on Cano groups. So, in a sense, classical objects. And what you need to know for these PCFs is that they give rise to measures and there's a correspondence. It's called the review or review measure. And here's the definition here: so, A is your So A is your PCF, B is the process, coordinate process here. You look at this expectation of this integral here. You take one over t t to zero. This turns out defines measure. It's a review measure. It has a property, it charges no M polar sets in case you forgot what an M polar set is. Basically, these are sets that your process can't see. That your process can't see. Conversely, given a Borel measure that has this property that it charges no polar sets, there exists a unique PCAF so that you go back in the story. So it's really a correspondence. And you can make this very explicit. If you go back to this running example we had, remember this one, this D, where is it? Here it is. Here. If I take one derivative, then I have Df equals, you know, one over sigma squared D. You know, one over sigma square d d tau or something, and so this exactly situation we have here. The a here would be that the one over sigma square. And in that case, this is somewhat a nice situation when you really have, when this is differentiable, in general, it may not be differential, but it still continues increasing like a counter step. But if it's C1, then it's nice. And this review measure, you can basically write it down via its rather than. Via its radonic density to the reference version M here. So here it is. But in general, in its general setup, this is still true, but you cannot write this in this form because mu A in general is not absolutely continuous to M. So this just cannot work. Next one, time change for symmetric Markov processes. If you are again in the setting of this symmetric process, in the still Symmetric process in the stately form setting. So let's say this again, this Hunt process it induces a semi-group on L2. So symmetric here means you just, you know, if you take the inner product here in this L2 space, you can flip them around. And let's go for a, let's look at the PCF again with a review measure. And then you can define somehow, you know, F is the essentially the domain of the derivative form that you start with. And you can somehow tweak it. Can somehow tweak it and look at this distorted domain F mu. And you have to be a little bit careful. L2 consists of equivalent classes, and you have to make sure what exactly do I mean here. And so it turns out this is well defined exactly because mu does not charge polar sets. So there's a bit of fine analysis potential theory involved. And if you also assume that A has full support, so you go strictly to plus affinity just for simplicity, then you can definitely. Just for simplicity, then you can define very nicely this time change here using this PCF. A is the PCF, and that turns out to be again a symmetric process now with the measure mu A. Okay. And the Dirichlet form is maybe surprisingly, at least when it is the first time, it's identical. So how can it be? You change time and it's identical. The change here is all. The change here is all in the domain here. And let me at least convince you of this identity here. So, if you are in the smooth setting here and you just do what you do, I mean, the dirty form is the generator acting on F, and then you take the inner product relative to the M measure. So up to a sign here, this is the Derley form. And then you play around with these A's here, because remember that this Revuse measure here in this nice setting is AD. Here in this nice setting, it's ADM. So let's put an A here, then you divide by A. But this was exactly, you know, remember our discussion about the time change, how it looks at the generator. So the generator changes, and this is how it changes from here to here. But here, we also change the measure, and that somehow compensates, and so you end up with the same thing. That's again, probably an exercise in every class on daily forms. I need more background. Gaussian fields. Gaussian fields. So there are many ways to realize them, and I will be very quick here. White noise on RD. Then I don't want to go in complications due to RD. So often people do this on domains and there's something called a massive field. So I'll put all this under the carpet. So quick and dirty, we hit this white noise here with the regularization that you get by looking at. That you get by looking at negative powers of the Laplacian, the positive Laplacian. And well, if you do some quick power counting, then you see this regularizes and this is how much it regularizes. And of course, white noise is irregular depending on how, in which dimension you are. This regularity is basically minus d half minus. This regularizes by s. And so you can give this one a name. Let's call it h, the Hearst parameter. Okay. And then people. And then people define fractional Gaussian fields, Gaussian free fields. So let's be so I mean, this is a definition, right? In this generality here, you call this H here. We just define a fractional Gaussian field. The Gaussian free field, that would be s equals one. Okay, so s equals one. And well, you just plug it in here. And if you're in one dimension, sure enough, you basically, this means you integrate once and you go from white noise to brown emotion. So this is a d equals one. Nice to Browning motion. So this is d equals one. That's, of course, much more interesting d equals two. And here's a picture. Let me also point out covariance kernel just direct from the definition. Easy. The interesting thing in dimension d equals two is that for s0, you don't do anything here. You just see the white noise. It's super, super bad. It's c minus one minus. But if you regalize a little bit here, you just fall about sure to be a function. And so this is the free field in dimension two. It's somehow, you know, the H is. Somehow, you know, the h is zero, but it's zero minus, and so this turns out to be logocorrelated. If you go higher, people do this, it's interesting, then you know, it gets nicer and nicer. And all these pictures are stolen from a nice archive survey by Lutia and co-workers. Now, log correlated. So, again, you have a chance to see log correlated, and that happens actually when h is zero. And then you have this g, which I defined here somewhere. G, which I defined here, somewhere just g here as the covariance kernel that is then of the form, you know, log plus x minus y. And so, in all these situations here, you have a when you really do it on the whole of the space, you have clean, clean formulas and clean scaling. So, that the so I want to distinguish maybe between this GFF here and what people call the log-correlated field, the log-correlated Gaussian field here. Correlated Gaussian field here. And so, in dimension two, we have seen that the Gaussian free field is local correlated. And if you go higher, you can do that too. But then you have to work with the fractional version here. So you can't do just here s equals one. And you get perfectly interesting things. There's a reason why I want to sell you on this idea of log-correlated fields also existing in higher dimensions than two. So this actually has some applications. So, this actually has some applications, but don't ask me exactly what. This one here is you regularize with basically the pi Laplacian, what I inverse thereof, and that also gives you a local field. Here's a picture. I think for the sake of time, I will not go into that. And again, if you work in a whole space, you should, you know, you may want to put in some mess to be on the safe side. Otherwise, you have to work with homogeneous. You have to work with homogeneous structures. So, anyway, this is something else. Covaris kernel. We sometimes need these specle kernels, Bessel potential operators, but I also want to skip that. The important thing in this construction here is the Gaussian multiplicative chaos. And that works in a great generality and basically requires you to have a log-correlated Gaussian field. And so, if you've seen a few examples, and So, you've seen a few examples, and really you're not restricted to two dimensions. And the aim here is to construct a random measure in the following way here. So you start with sigma, could be Lebesgue measure as a start. H is your Gaussian field. Think of something log-correlated. So point evolution doesn't really make sense because, you know, log doesn't really make sense. But formally, or you know, when you modify, you can. Or when you mollify, you can write it down, and then you try to compensate here. So, this is really the big exponential, so that some randomization has been put in. And so, formally, that's the definition, okay? But it's really formal, so this is still unclear. And this was looked at a long time ago by Kahan, and he had a very general setting actually here with a metric on a metric space. So, I'll come back to that, and he also is. That and he also assumed some special kernel structure, which is not ideal in all situations. And then he can use mathematical arguments that tell him that when you do this here, when you approximate here, think of Cahun-Leuf-type approximations, then the whole thing converges using article arguments. There's a twist here that needs to be an twist here that needs to be an interplay between this reference measure here again could be lebesque measure and this parameter gamma that's the reason people make this explicit so basically what you need is that gamma somehow plays together with sorry this gamma here plays together with the with the sigma in the following form so you assume that that sigma here has some integrability in the following form so this here is the metric So, this is the metric. This is the metric. And then, okay, there's a definition here. You say sigma is in a certain class if this distance here, and you make it singular next to the at the diagonal with some minus alpha here. If this is integrable here in the double space, then it's some class here. And basically, you need this, if this is this class R alpha, and then your gamma needs to be less than square root of 2 alpha. Square root of two alpha, and this is the somehow the sharp condition where it works. People have moved on to discuss what happens when you have equality, you can still do some, you know, something, but then it's over. And the GMC is in this nice sub-critical situation. It's just defined as the limit that you construct here. Now, this has been revisited by a number of people that I mentioned in the beginning, and in particular, Nathaniel somehow pioneered the construction that That does smart averages and that has somehow that uses a bit more structure because here this was super general, here in RD, which or you know, on a domain in RB. But this is much more robust and has other advantages. So we have been guided by this one. And now finally, what is legal parallel motion? I have to look at my time. How much time am I given? It's 33. 45. 45 45 45 yeah okay that that should that should do but i have to speed up a little bit so level brand motion what is it it's essentially brownian motion where you distort you go from the euclidean brown motion and now you just distort the metric here but in the following way that you basically multiply it by this random quantity and since point evaluation doesn't really Um, point evaluation doesn't really make sense. This realmization formula is a minus affinity here. So, this is random, and you know, it's basically between zero and infinity all the time, so it's super spiky. On the other hand, it's only somehow a random pre-factor. So, so many things, angles are preserved. So, this is somehow compatible with a conformal invariance that is key in the theory, although this is not key to my talk today. So, if you So if you go with that formally for a moment, this gives you this Cahan method or this Gaussian GMC, Gaussian multiplicative chaos. That gives you a way to construct the remaining volume here, and that has a name that's the Leeville measure. Okay, so that's progress for gamma. We are now in two dimensions. So this construction here, if you're in R2 here, then it's In R2 here, then it turns out this and here is the Lebesgue measure, then alpha is one. So this works exactly when gamma is strictly less than two. And for the sake of, I mean, as a geometer, you know, you want more than just the volume form. I would like to understand the Riemannian distance functions. I would like to understand the geodesics and a lot of stuff. So this was for a long time not clear, and it's also not important for us here. But as a cultural remark, this was recently solved by. Recently solved by a number of people, I think Jason Miller and co-workers. Now, as a fact, here, this measure that you construct here is not absolutely continuous with respect to Lebesgue measure on R2. So nothing here, what I'm saying is new. This is these effects about this level, about this level measure. But it does not charge polar sets, which is something you have to at some point check, and hence it uses a PC. And hence it uses a PCF, right? And so now, quite naturally, you take your Brown motion, you do the time change with this naturally associated PCF, and you get a new process, and that's it. That's called level Brown motion. And that's a very canonical structure. And, you know, if you think of, I mean, here, for instance, we've mentioned the Riemannian distance. If you are familiar with Baghdadan Schotten. with Varadan short-time asymptotics, you know, it's very natural to look at the process because you may learn something from the process about the distance. So there's much motivation to look at this process here. It is a symmetric Markov process. Now you can use some general T-form setup. So you are in a framework of abstract TikTok processes. They're not super abstract or concrete enough, but you're certainly in the framework of Tiergley theory. Certainly in the framework of Digley theory, and this here is now a symmetric Markov process, but relative to the GMC, the M gamma here. And moreover, you know that the Digli forms algebraically they're the same, but the domain changes, and this is how they change. Okay, so this is, I think I should write it m gamma for all right. Now what about What about what? How can we connect these things and put them together? First question: Level Brown motion, now that we discussed what it is, does it lift to a random P rough pass? And the answer, okay. So there's a, you know, H here is this Gaussian field. So you sample from what you would like is that for almost every realization of your field, and then almost surely, you know, you get sample paths that are. Sample paths that are run on PRAF paths. Here, the answer: I will be quick: is yes, thanks to this abstract nonsense result by Larns Wigdua, that some lived always exists. And here is just a remark that Brown motion has finite p-variation p greater than 2, of course. And the time change doesn't do this away. I mean, time changes are very compatible with p-variation, not Halter, but P-variation. That's why this is the right language. The right language, and so yes, we can doesn't tell you so much. Let's ask a better question here: is there a natural choice for this guy here? Not just abstract nonsense. And so, well, if there is, maybe we should give it a name and then call the whole thing the level Brown Roughbas. And so I argue there is. Brown in motion is a martingale. Hence, time changes thereof are at least local martingales. And so, you know. And so, you know, this is not the way people think about Liebel Brown motion. They want to understand the Mervian structure. But as a matter of fact, it is a local martingale. Doesn't mean it's trivial. It just says it's a local martingale. And now you can fire up very general results that we developed over the last X years for martingales and semi-martingales. So you have, for instance, a very general Wong-Sakai result that would tell you that this works and it works in P variation or P-half variation. Variation or P half variation in this graded sense for the second level. So these are, you know, technical details on the rough master side. And so it works. Now, two remarks. The first one is both questions here are natural and as post non-trivial questions, and we could quickly answer them because the rough bath theory just has been developed to a stage where we have the tools to say, okay, now there's an Now, there's an open problem. I don't think anyone has looked very hard, but what we have done here implicitly, right? What is Wangzakai? B'swise linear, these are the geodesics in the Euclidean space. If you look at the Liouville quantum gravity, so the random geometry here, they're also a geodesics. They're quite new, right? So, because I mean, even the Right? So, because I mean, even the metric is quite new. So, understanding these geodesics is also new. And here's a picture if you want to see them. So, it seems very likely that when you next one, can we understand the structure of this essentially canonical lift as a Markov process? And the answer is yes, we can, because at this level, you know, here was the area and You know, here was the area, and we can also understand. So, X here was the level Brown motion, so that was a time change. And essentially, areas given by stochastic integration, and now stochastic integration and time changes, they work together, they're compatible. So, you can quickly see that your level Browner RoughPass is actually the time change of the classical Brown and RoughPass. So, in particular, you can play all these games here, look at the PCAF, etc. At the PCAF, etc. So, on technical level, this is a PCEF, the original one, this guy here, is a PCEFF here. That's a PCEF for B, not for the Brown and Rough path, but the definition is set up. It's immediately also one for the bigger object. And so this Levo-Brown-Rough path then is mu F symmetric. That's the reverse measure. And so here's the form. Not surprisingly, the Dirichlet forms are still the same. Dirichly forms are still the same. So, meaning this was the Diracly form for the Brownian rough pass, in case you remember, and this is the same for the Leewill-Brownian rough pass. What changes are the domains again? And so now the domains we are on G and G, this one is the H1 in the hyperliptic sense, and here. So, the two more twists. So, all this, you know, these different approaches, one to kind of this time change approach, they both agree and work well now. Works, but now it would be very natural. I mean, at this moment, we have somehow taken an existing object to the level paramotion, then somehow topped it up with something that we put on top. Can't we do it somehow together from the start? And so from this perspective, it would be quite natural to work with a log-correlated field where the log correlation happens directly on the whole group and not just on the underlying R2 or Rd. R2 or Rd. So, in other words, can we have a local correlation relative to the natural metric on this space here? And the natural metric would be the Cano-Kartidori one, which, you know, in case you haven't seen it, it's the natural left invariant metric on these Cano groups. Okay. The answer is yes, we can. At the same time, I don't want to hardwire here the metric structure of the group. The metric structure of the group because I want a more flexible setting. I want to recover, for instance, the previous construction as a special case, and that means I must have a flexibility to work with distances that are not really metrics here. For instance, I may compare two elements in here by just looking at the Euclidean difference here, because that will bring me back to the previous discussion here. So we need to allow for degenerate metrics to have a unified tree. Generate metrics to have a unified treatment. So, all this can be done, but the price is we have to revisit this whole GMC construction. Because, remember, even Kahan started with a metric setup, and we have here concrete examples where this is not the case. And in order to follow this program and see that everything can be done, at many places in the classical papers, in the subjects, people have used heavily that they have this explicit Gaussian structure. This explicit Gaussian structure of the underlying process, which is Brown motion, and that one we don't have any explicit forms there because we are now looking at somehow Brown motion in Cano groups. So all the explicit arguments are basically gone and we have to find substitutes which are more abstract, which requires to use a fortunately well-developed theory of essential potential theory and Kanho groups. So, the details of this are, I mean, there are many technical things, but I didn't find it useful to single out one. Let me just finish with one outlook. In the Leoville literature, most interest is ultimately about the conformal invariance. So, what happens here? There's a reason people liked D equals two so well. So, it's true that if you go beyond D equals two and you insist. Beyond equals two, and you insist to have some conformal invariance, that's the Leewell theorem that tells you this is a very rigid structure. Okay, that's a fact of life. And you have such Liouwell theorems also for the Heisenberg group. I do not know for general Carnot groups. If anyone does know, I would be very interested. The one reference I have is a 2007 paper by Thomas Prunson, I believe, analysts of mathematics with co-workers. And in the same paper, they also have. Same paper, they also have a notion of conformally invariant panets-type operators. If you don't know what this is, then that's okay. This is a class of operators. It's like think of LaParse-Peltrami, but you put in lower order perturbations to achieve a certain conformal invariance. And this was popularized in the library context in a recent paper by Teo Strom and co-workers. So, they looked at even So they looked at even-dimensional Riemannian manifolds. And in a sense, you know, we want to see how this looks like in a sub-Riemannian situation. And of course, the Cano groups are the model case for that. Okay, that's it. So thank you very much. Any questions for Peter? How about on Zoom? Anybody on Zoom have a question? Jonathan? No. I mean, I have a lot. Yeah, no, I have to absorb more, Peter. Sorry. Okay, great. Let's thank Peter again for lovely talk. So we'll reconvene at 10:30.