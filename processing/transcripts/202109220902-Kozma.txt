I assume you can all see my slides, and I have to admit that while preparing them, I realized that, or maybe after preparing them, maybe it's worse, I realized that I really don't know how well are people familiar with the background. So please stop me if I'm going either too slow or too fast because I'm really a little bit in the dark here. Okay, so let's let's but let's see what But let's see what we are talking about. And I think I will start with the notion of quasi-isometries. And this is actually what I was referring to when I said that I'm not sure how familiar people are with that. So let's start with the definition. So what is the quasi-isometry? Quasi isometry is the correct notion of isometry when you're talking about large-scale behavior. But let's start with the definition. So it's, I'm just, the setup is just two. I'm just the setup is just two metric spaces, and we are saying that the map between them is quasi-zonal if it satisfies the following properties. First of all, it preserves distances up to multiple active and most importantly, an additive constant, by which I mean the following, that the distance between any two points is, okay, bounded between this function of the distance, with special emphasis on the minus constant here, which allows. Constant here, which allows negative values on the left-hand side. And it should be onto, up to an additive constant, by which I mean the following, that you can read the definition yourself. So this is the definition of quasi-exonic, and let's discuss a few properties of that. Okay, it's an equivalence relation. Okay, it's not formulated. It's not formulated in a symmetric way, but it's an equivalent solution. If you have a quasi, if phi from x to y is a quasi-isometer, then you can have, then there is a quasi-isometry also in the opposite direction with the okay, there's no second composition because it's not onto, but with some equivalent notion of the composition being identity. And here's another manifestation of the importance of the negative constant. Of the importance of the negative constant, any bounded space is quasi-isometric to a point. Just the map that maps everything to one point is a quasi-isometric because this negative constant allows all distances in the image to be zero. It's not a problem. Well, they write it in a funny way. Okay, whatever. Okay, so quasi symmetry only measures large-scale behavior, and this is really useful because we are interested in large-scale behavior when we talk about large graphs, we talk about random work or infinite graphs. We are interested in the behavior at large time, at large distances. So that's why I'm saying that this is the notion of isometry, which is most natural in probabilistic settings. Okay, here are two. Okay, here are two remarks which are maybe less interesting for us today, that say that basically a continuous space can be quasi-isometric to a discrete space. On the other hand, you can't have anything you like, right? For example, quasi-isomative preserves, and this is very easy to say, the volume group, which is defined here. I'm not going to even speak it out loud. I feel I maybe have done this part a bit too slow. Maybe I've done this part a bit too slow. And in particular, for example, the various Z D's for different D's are not quite asymmetric. So it's not like you can do whatever you like. Okay, questions on this side? I think lots of people here knew that, you know, and they were playing with the phone while... Okay, I should instead of. Okay, here's something more probabilistic for you guys. Suppose you have two infinite Suppose you have two infinite graphs with bounded degrees, and I always, when I talk about graphs, I always mean that they are metric spaces with the graph distance. So the distance between points and the length of the short path. Then if they are quasi, then quasi-isometry preserves recurrence of the random walk. Okay, if one is recurrent and it's quasi-isometric to the other, then the other is recurrent, and the same for transients. So let's skip. So let's sketch the proof very, very briefly. Now, remember the Nash-Williams criterion. It says that a graph is transient if and only if the resistance to infinity is finite. The resistance can be written as the maximum over all functions which are zero at infinity. Note this condition. This just means that the limit at infinity is zero. This is that the limit at infinity is zero that and have arbitrarily small energy. So you can have functions with arbitrarily small energy and you just push these functions through the quasi-isometer. Of course, the quasi-do doesn't have to be onto. So you define on and also doesn't have to be one-to-one because of this negative constant. So but you But you choose if it's not one-to-one, you choose arbitrarily, and for points which are not onto, you just approximate by the nearest point, and that's enough, and that preserves the energy up to a constant. And that's the whole proof. So, I'm sure if it was too short for some people, but I'm sure for some people they knew it from elementary school. So, I'm making here some kind of forced Fourth average of the audience. Okay, questions about these slides? Okay, so let's do a bit of literature survey. So, what other properties? Now there are going to be no proofs because this is already these are already. These are already, many of these are already quite difficult to result. So, the following properties are preserved. So, a polynomial heat kernel upper bound. So, if, so what do I mean here? If a certain graph satisfies such an inequality, and you have a second graph which is quadrilized, then the second graph would satisfy a similar inequality with the same d but not necessarily the same context. But not necessarily the same constant. The same holds if you add the exponential tail, so that requires a little bit more on the first graph, and you get a little bit more on the second graph. The same holds for lower bounds. The Hanak inequality, if you don't know what the Hanak inquiry is, don't worry about that. It won't reappear, so you are not. So, you are not missing much, but let me still say that this particular result, because this was an important open problem for a long, long time. And this was really a significant breakthrough by Barlow and his co-author here, Mugukam. So, it wouldn't be reasonable to skip that, even if I don't plan to use the Hanaki. Even if I don't plan to use the Hanmachin equality at all. However, don't assume that that means that everything is preserved. And one counterexample which was found by co-author for this research, Jonathan, is that upper bounds which are not of polynomial bounds. Remember, this result was for upper bounds of polynomial form. These might not be preserved, and so that you don't think that it's clear. And so that you don't think that it's trivial when I'm saying preserve, for example, suppose I had an upper bound, an exponential upper bound. So what I would assume that a quasi-isometry would preserve would be an exponential, but not with the same constant, including above. So it's not like I'm playing with here that the fact that there is also a constant above and a constant below. No. Also, a constant above and a constant below. No, it's really, you get one graph which has one type of growth for the heat kernel up and down, and another graph which has an asymptotically distinct growth. And let me, let me, this example will be related to stuff that we do together. So I want to stress that this is that it's a real example and not a plane of definitions. Definitions. Okay, questions about this slide? I don't think there's anything else. No, okay. Questions about this slide? Okay. Okay, so let's move to harmonic functions, which also play here, we play a little role, not a very important role. So this is the definition of harmonic function of graph on graphs. Again, I'm sure. Graphs. Again, I'm sure many people here, most people here have seen that. And the graph is called, we find it because it's called Liouville if all bounded harmonic functions are constant. Okay, and this is exactly like the Liouville property for in classical complex analysis that says that all analytics functions on the two dimensions are if they're bounded up. If the abounded are constant, and here are a few examples. Z D was done already in the 60s using, I mean, abilianess. It's a very, very nice, very, very cute result. Nilipotent groups. By the way, this result is not. Okay, let me not go there. All nilipotent groups. I'm giving here to To references, not because there was anything wrong with Morgulis's proof, it's a completely full proof of this result. So, if all important groups are UV, but the proof is algebraic. And what Hibish and Salvkos did in 1993 was to give a completely analytic proof, which doesn't use any of the structure of milpotent groups, only the fact that they go polynomially, which I think was an important step forward. Think is was an important step forward, which is why I'm stressing this second proof of the same result. Any recurrent graphs, but of course, okay, maybe let's only look at the last example. If you take two copies of Z3 and connect them by a single edge, so you have two copies of Z3, which I'm drawing like cubes, and connect them by a single edge. And connect them by a single edge, then this is not a Luvian graph. And the function that demonstrates that is the following: the function of f is the probability that the random walk starting from x is eventually in one of the sides. Just give them their names arbitrarily and say that function, it's obviously bounded, it's a probability. It's easy to see that it's harmonic. Any function defined in this way is harmonic and it's Defined this way is harmonic, and it's not difficult to see using the fact that Z3 is transient, which is necessary because any recurrent graph is Nouville. So you must have both sides transient. It's easy to see that it's non-degenerate, that it's not a constant function. Questions about this slide? I guess it is a bit. It is relatively well known. This stuff, um, okay, but the Louvre property is not invariant to quasi-exomatons. This is a famous result of stereons. And let me sketch a proof by Italian Yamini of the same factor. It's basically the proof of lions. It uses basically the same idea, but this is significantly simplified. simplified the so it will fit in this box. Okay, so let's see what is how the proof does the proof go. Okay, take a binary tool and label each edge arbitrarily left or right. Okay, now if you take a random walker on this graph, then after one step it does have more, it will have, you look at the place where it is, you look at the path from where it is to the The path from where it is to the root, and you count how many lefts and how many right turns are on this path, and this they will be more or less equally equally half half. Okay, we need two graphs. So let's call the previous graph, which you call before T, let's call it T1. And the second graph will be T2, which will be a graph where it Will be a graph where every left term has two edges instead of one edge. Or if you don't like multi-graphs, then you can replace every left, every right term with a path of length two, and then the tree would look like this. Okay, so that's essentially equivalent. I'm not saying it's not completely, but for this discussion, it's the same. So either you duplicate edges or you add. Duplicate edges, or you add the middle points as you prefer. It doesn't matter. That would be the graph T2. And of course, this time the traveler will have more right edges than left ones. Oh, I said oh it opposite in the previous picture, but you don't worry about that. You can calculate the proportion exactly. Nobody cares about that. This means that there is a set of vertices, basically those which have left. Those which have left turns and right turns in a proportion strictly between the two values half and that value, which is hit infinitely often in T1, but only finitely many times in T2. Now, what do you do? You take your tree. You have here this mystery set, which in one graph is hit infinitely many. Infinitely many times, and the other graph is hit only finitely many times. You take a copy of Z4, you will have to imagine the fourth dimension. I'm only drawing three. You take a copy of a line inside and you connect them in an arbitrary fashion, like that. Okay? So that's the graph. That's the graph. That's the two graphs, which are quite automatic are just this graph, which is the This graph with this picture repeated twice. With the only difference is that on the tree, we are doing this analysis that I explained before of multiplying some of the edges or adding vectors. Okay. Okay, so these are the two graphs, okay, G1 and G2. In G1, the worker will enter Z4 eventually and will never exit, right? Because it hits our mystery set A, infinite. Set A infinitely many times, every time it hits it, it has a probability to go into Z4 and every turn. So it will do that. The probability one. And from this, it's not difficult to show that the graph is good. In G2, the probability to be swallowed by Z4 is a non-constant bounded harmonic function, exactly like we saw in the previous slide. And that's the whole point. Okay, so again, you have a tree, a perturbed tree. A perturbatory and a copy of Z4, which is an impact. That's the whole point. Questions? I guess everything is clear. It's very, very nice. Should I check the chat? Nothing in the chat. Yes. Jenks, question? No, it was not a question, but no one was saying anything. So I just was saying it's a very nice proof. Okay. Okay. I'll let. Well, it's been a while since I've seen Terry, but I'll let it tide. Okay, and my take-home message is the perturbed reconstruction is the basis of all known examples of instability. And this will reappear in a very surprising way in the second part of the talk. And this brings us to the second part of the talk, which is about. Okay, second part of the talk, which is about okay, or maybe that or time, okay, which is mixing times. Now, again, this I'm sure even more people here know, so I'm going to be quick about it. Well, I certainly have time, but okay, but let's still see the definition of the mixing time. So, what is it? Now we are in, I have to say, we are in finite graphs, okay? Everything I said before was for infinite. Before was for infinite graphs, but now I'm moving to finite graphs. Okay, so do this little switch for a few minutes, or maybe for until the rest of the talk, actually. Now we are moving to finite graphs. This means that everything will have to be, it will be as usual when you move to finite graphs. Everything will have to be a little bit more technical, but asymptotic, almost true instead of almost true, and so on. But okay, these are all technicalities. So, but okay, these are all technicalities that everybody here has in plenty of time. So, I'm sure that you will match. Okay, but what is the mixing time? There are three possible interpretations of this notion, which I will recall for you now. The most natural one is a total variation in mixing time. Let me remind it. You look at the distribution of the random walk at time t, and x is a starting point. So, p t x y is. And X is a starting point. So P T X Y is the probability to be at the point Y when you start, you run the walk from X. And you want to compare it to the stationary distribution, to pi Y. So you take the difference, you sum over all Y. It's why it's called the total variation mixing time, because you take the total variation of the total variation difference between these two measures, P T and Pi. You want to know. You want to know when that is small. It starts at two, and you want to know when it goes down below quarter. Quarter is an arbitrary constant field. You take the first time that that happens, and you take the worst mixing, and you take the worst starting time, starting point. And that's the definition of the mixing time. So I'm sure everyone has seen that. And the constant quarter is arbitrary. Let me not go there. That's well defined. Go there, that's a well-defined, well-known theorem. The L-infinity mixing time is very similar, except the requirement is much stricter. Instead of requiring that the total variation is small, I'm requiring that it's small everywhere. Oops, touchdown. And the LEGT mixed memory. The LEGD mixing time is always larger. And the final notation is the relaxation time, which I will denote by TRL, which is the inverse of the spectral gap of the finite graph. And you always have these inequalities. The relaxation time is the smallest, then the L1 mixing time, then the L infinity mixing time. And the ratio between the last and the first is no more than log. And other than that, beyond these restrictions, everything. Beyond these restrictions, everything is possible. There can be, you know, these three quantities can be related, can be anywhere on this log scale that remains true. Okay. And my question is, are these mixing times invariant to quasi-isometry? Since that's, as I said, the natural notion for a geometry that we are looking for, then that would be a given. Then that would be a given all the work that we've seen on properties of random or infinite graphs, that would be a pretty natural question. And it's been discussed. As you will see, it's not essentially this is a resolved problem. Okay, so this is just a reminder. Okay, this is just what do I mean by quasi-isometry. And I mean here, of course, I'm sure everyone guessed, but I mean that the concept. Guess, but I meant that the constant, the quasi-isometric constant, the constant that appears in definition of quasi-isometry should not depend on the size of the graph. Okay, let me not spend time on that. I'm sure everyone could have written this definition themselves. So, is it true? It's true for the relaxation time, which I remind you is the inverse of the spectral gap. And this is a well-known fact. It follows easily from the minimax, the principle. minimax the principle the minimax description of the minimal id well it's a minimum description i don't know why i wrote minimax it's not a fixed economy it's not for the l1 this is was first proved by jian ding and yuvalters in 2013 and the ratio can be as large as log if you remember we saw that anyway because t-rel is invariant Because T rel is invariant, that means the two graphs, which are quasi-isometric, cannot have the mixing time deferred by model log because the mixing time differs from the quasi-invariantly isometric property of the relaxation time by log. So, of course, they cannot differ by more than log. And in fact, they can differ as much as log. And this was proved by Yonata. Was proved by Yonatan and Hermon and Yubal Perez a few years after that. It's not true for L infinity mixing time, also. This was actually originally my question that I asked in 2010. And the ratio can be as large as log log n. Notice the difference here. This does not follow from what we saw in the previous page. And this is sharp. This can be more than long. This is again because there is. This is again because there is a second or at least two other quasi-isometrically invariant quantities which approximate tau infinity up to log log. One is the log so polar mixing time and the other is the spectral profile mixing time. Let me not go into the details of that. Okay, so sorry, before I start, questions about this slide? That's the last slide. Questions about this slide? That's the last slide before we go to groups. Okay. How much time do I have? Well, everyone knows what the mixing time. I spend even much, too much time. Okay. Okay. And the last topic, the last word in the slide that I haven't gone through yet, the last word in the title, sorry, of the talk, which I haven't gone through yet, is Kaleograph. Is a Cayley graph. So, what is a Cayley graph? You start with a finitely generated group. Again, most people don't know that. You have a set of generators and you create a graph whose vertices are the elements of the group and whose edges are simply given by G times S, G given from the group, S given by the set of generators. And let me not give any, maybe there are some examples in, but okay. Maybe there are some examples in, but okay. And you can change the generators. If you change the generators, it's easy to see, you get a different graph, but the two different graphs are quasi-isometric. So if you prove for a property of a graph or random walk on the graph, that it's quasi-isometric and invariant, you get automatically that it's a group property, which is an interesting fact. You know, things which are a property of the group and not of the Cayley graph. Of the group and not of the Cayley graph are interesting for group theorists. They allow to differentiate groups. So, this leads us to a question in general, for random work properties, maybe there are some random walk properties which are not quasi-isometric invariant for general graphs, but are quasi-isometric invariants for Kaylee graphs. That's quite a possibility because that's a much more restricted family. Family. Okay, so maybe there's something that can be done. In fact, some things like that are known. For example, here is an example of exactly this behavior. An upper part of either is quite important. Even when the decay is not polynomial, if you remember, when we discussed general graphs, I said polynomial decay is quasi-isomatical invariant. Other types of decay are not necessarily quasi-isometrical invariant. But for Kelly graphs, But for Cayley graphs, that's not true. For Cayley graphs, any function that you have, if one graph has a heat kernel bound of a certain type, then a quasi-isometric Kelegraph, if both of them are Kelegraph and are quasi-isometric, it will have the same kind of number. Okay, so that's one very interesting. One very impressive, in my opinion, example of where things are different when you move to Kurds. For the Louville property, if this is a famous open problem, one of the most famous problems in geometric group theory is the Liouville property. If you have two quasi-isometric telegraphs and one of them satisfies the Liouville property, does it automatically mean that the other one satisfies the Louvre property? Means that the other one satisfies the Liouville property. The counterexample of Lion Seniamine that we saw is most certainly not a Lyouville graph, and it's not clear how to make it, or how to fault it, or how to do something that looks like a Cayley graph and will have similar behavior. So, this is the famous open problem. Thound the counter example. You can rest for a few years. What about mixing times? What about mixing times? Okay, so let me remind you again the definition just quickly: L1 mixing time, L infinity mixing time, okay? And the L infinity mixing time is in fact invariant to quasi-isometry between two Kayleigh graphs. Okay, again, I remind you, this was not true for general graphs, but it is true for Kayleigh graphs. I'm making too much of a fuss here, actually. This and these are two manifestations of the same behavior, and the proofs are actually very, very simple. The proofs are actually very, very similar. So, don't take me too strongly, but so it's not like it's not really two independent examples, they are really similar. But still, it's interesting, I think. We don't know, exactly like we didn't know for polynomial, we don't know if you have one graph which is a Calendar, the other graph is not a Calendra, and the quasi-isometric whether the L infinity is preserved. The L infinities preserved between them. That's an L infinity meeting time. That's again an interesting question. And I can finally state our result. And our result is as follows. The L1 mixing time is not invariant to quasi-isometry between predators. Questions about the statement of the results? Questions about the statement of the results? Yes, Omer? Yes, so in general, you said that there could be a discrepancy of log n, and can it be as large as that? We don't know that, and I will get to that in the next slide. I will get to that in the next slide, so I will not stay behind on that. So, do they have like. So, do I have like something like 10 minutes now? Is that correct? Yes, yes, that's right. Okay, so I can tell you a little, little bit about the proof, not the full, not all the details, of course. So, but okay, but let's start with a few remarks. And the first, okay, I will get to Omer's question in a second, but let me first of all tell you what is the nature of the example. So, the groups are the symmetric groups, the symmetric group, the group of polar permutations. The group of permutations and the generators of transpositions. So, permutations that transpose to elements are giant, keep all the rest pits. In particular, that means that our graphs do not have bounded degree, which is a peak. Okay? We have a sequence of graphs. The graphs are quasi-zometric, of sequence of couples of graphs, the graphs are quasi-isometric, the quasi-zometric. Quasi-isometric. The quasi-isometric constant does not depend on k, but the degree does. And this is unfortunate, but we spent some significant effort to try to make a bounded degree example and didn't succeed. And I think that's very interesting. Now, to go to Omos' question, in our example, the ratio is log, log, log of the size of the graph. Of the size of the graph. Okay, n here is the size of the graph, which is basically k factorial, right? Because to correctly compare with the previous result, we need to talk about the size of the graph and not about k. So it's log log k, if you like. Again, we don't have an upper bound that's better than log n, so there is a, in some sense, a huge gap. In some sense, a huge gap between our upper and lower bounds. The upper bound is logged. We don't know how to improve that. Is that correct? I think we don't know how to improve that. And no, at least I didn't have any idea in these last 10 seconds. And the lower bound is log, log, log. So again, there is plenty of room here for improvement, I think, in this result. In this result. Okay. And I'm slightly. Okay, maybe I will spend some more time. Okay. Now, let's remember what is what there are lots of people here who worked on that, and I'm not the only one. So, what does it mean to have a Cayley graph of SK with the generators being transpositions? It means you have some graph. It means you have some graph, any graph that you like, and you perform the interchange process on it. You put marbles, which are all different, each one is unique, on the vertices of the graph, and then exchange them using a process every time you choose a nitrogen exchange. That's what it means to do a to have a Kelly graph of the symmetric group with respect to transformation. Group with respect to transposition. So there is an underlying graph which is very natural and very important in this when you study Cayley graphs of SK generated by transpositions. Okay, so there is an underground graph and the definition is just that you put an edge and I inject for any set of generators. So I can tell you everything. You will know exactly what is the example. Once I gave you The example once I gave you, I give you, I tell you, what is the gar? So, what is the gar? It's a perturbed tree. So, my take-home message for you today is all examples of instability are generated by Lyons-Benjamini perturbed examples. You just have to find out how to make it into a group. Yes, I guess I'm five minutes before time, but I think there's no point in welving into technical details of exactly how we do the how we modify the tree. So let me let me start. Thank you. Questions? From the room or from Zoom, I have a question. Yes, so just to make sure, because we need two graphs. So when you say the perturbed tree, we take the tree and then we perturb it and we get two graphs. Yes, basically, yes, basically, that's what you do. You have a you have a tree. It's more complicated than in the BMI Lions case, but yeah, basically, you have a. But yeah, basically, you have a tree, you have some graph which serves as a spool, which like the Z4, right? We had in the example, we had a graph in Z4. Instead of Z4, you put a copy of some kind of spool where the worker will go when you don't want it anymore. And you connect, you have like a special set inside, which is the Set inside, which is a little bit more complicated, but basically the same: counting how many left turns, how many right turns are, which you connect to the spool. That's G1. And G2 is generated by adding. There is no sense of doubling. Well, okay, you can double edges also if you like. But how we did it was by adding vertices in the middle. Bertie says in the middle. Daniel, if you are talking, I can't hear you. You are muted. Okay, Gil, does that answer your question? Yes, absolutely. Gotta? Yes. Did you try? So you take your finite graphs that describe the transpositions, right? Yes. Did you try to make it banded degree by parallelizing the Degree by parallelizing the yes, I tried, but it ruins everything. Yeah, that was the first thing I tried. But once you try that, the mixing time changes dramatically and just comes from a different source. Because it shortens the times significantly. Here, in this example, what determines the mixing time is single particles behavior. But once you do that, because the mixing time Once you do that, because the mixing times are cut in both graphs by a factor of n, and what happens is that the mixing time becomes a function not of single particle, but of multi-particle interactions, and the whole behavior changes. So it cannot be done naively. Might be, I'm still thinking that maybe it could be done somehow, but not just by parallelizing this example. At least I didn't succeed. All right, thanks. So, one question. So one question. So is it enough for the two graphs to be quasi-isometric to get also that the Cayley graphs associated with them are quasi? Yes. In this case, yes, I think. Yes, yes, that's it. Yes, that's okay. That's not how we prove it. That's not how we prove it, so it's very close, but I think we didn't quite prove it as you stated that any two quasi-isometric. We proved it by hands. I would now I don't remember if we did it by hands because. If we did it by hands, because we had an example where quasi-isometry is not does not of the graphs does not does not give quasi-isometry of the Kelly graph. So we just didn't have a proof and didn't want to bother with it. I'm tempted to all the second. So it might be true that. So I'm sorry, I forgot. So I don't know. So at this point, I have to not. Is G1 quasi-isometric to G2? Does that imply that the Cayley graph of Sn with respect to G1 is isometric to the K, is quasi-isometric to the Caligraph Sn with G2. We don't prove it as is in the paper. Maybe it's not difficult and we were just lazy, but and maybe there was some reason to do it this way. I don't remember. Do it this way. I don't remember. But right now, I don't know what's the answer. I wouldn't put it as an open question, but you know, just something that we didn't think about to see it. Are there properties which are invariant under quasi-isometry when you restrict to K graphs, but not for vertex transitive graphs? Yeah, I would certainly have put some if I knew. Put some if I knew. But of course, it might be true for Liouville. Of course, we wanted to give a counterexample for the Liouville. That was our overarching goal in this project. We got stuck because we couldn't, for various reasons, this doesn't seem to read there. And I think most group theorists actually think that it is invariant. That would be quite interesting. I don't know. Interesting. I don't know. Also, I think maybe also, I don't know about no, okay, skip costat. Okay, I don't know. There are certainly connections between random walks and geometry that are only true for calligraphs. That was basically what I was trying to say. So, for example, To say so, so for example, non-amenability implies the existence of bounded implies non-luvial. That's true only for Kelly graphs and not for general graphs. But other examples of quasi-isometh imbiance, I don't know. So, Gotti, along the same lines as Luca's question, are there properties which are preserved when you Which are preserved when you change the generators of a Kayleigh graph, but not when you go to a Kayligraph for a different group, but still have quasi-isometry. Well, no, not that I know. No, I don't know any example of that. That, like, certainly not any probabilistic example. No, no, yeah, I would be surprised actually because. be surprised actually because changing generators is a pretty evil pretty evil operation so graph graph wise and so so but you know that there is this famous example which was quite difficult to show of a transitive graph which is not quasi-isometric to any K-Ling. Isometric to any Calendroph. So, yeah, that was open for a long time until it was finally proved by who proved that. Gibby, do you remember? It was our roommate in the IHP. What was his name? Whatever. Anyway. What? Thomas. His official rights according to God. Oh my goodness. Okay, I'll remember that and I'll tell you. But Alex Eskin, yes, yes, yes. So he proved if two quarters, which I certainly will not remember, that so that there exists also it will it will not as Will you put that as as a corollary of a grand classification of the representations of various solvable groups? So it's like a big thing, like it was a collary of a big theory. So it's not something easy. That there exists a G which is transitive, but which is not quasi isometric to any case. Isometric to any Kelly graph. I don't know how if that says something about Peter's question, but at least it seems in the same vein. It certainly is interesting. Yeah. Thank you. Are there any other questions? Yes, maybe I have a naive question. It's Charles. What you mentioned two results in opposite direction for mixing time and L1. Have you thought about LP mixing times like L2? I'm sorry, I didn't hear anything. Could you speak up? The question was whether you considered. was whether you considered LP mixing time whether I considered what sorry L P mixing times or L2 mixing times they are all equivalent L2 and L infinity are equivalent and so all they're all LTs so so I don't know what is there to if there is some interesting thing to do there. That's a theorem, you know, it's not a reality, but it's a theory. For any people than one that all people need, you know, up to consent to that. I see. Okay, so no more questions. Let us all thank Gaddy again for the nice talk. And we have And we have a 30-minute break. We continue at the quarter past with Peter Wigler's talk. And we have coffee outside if you are here. 