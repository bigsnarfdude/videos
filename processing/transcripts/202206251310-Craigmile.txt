All right. Yeah, so I thought I would put my background just to be up Tunnel Mountain, which is just behind Burr's. So at least to give us the view that I would imagine is there today, but they said it was colder. So I was hunting through trying to decide which of my favorite pictures were suitable. The point is, there's too many, right? It's such a beautiful place to visit. Okay, so I'd like to talk a bit about spectral analysis using. Bit about spectral analysis using multi-taper wavelet. Sorry, well, it will include wavelet in a minute, but Whittle methods, and we're going to consider a lasso penalty. And this is joint work with Xu Han Tang and Yung Fang Shu, who's either at Ohio State or just finished up. Xu Han Tang was our PhD student. So we've already seen quite a few examples about thinking about looking at frequency domain characteristics of a time series, and we've already And we've already seen thinking about peer distees. There was discussion about how we can estimate coherent structure. And it's also been used, I would say, to great ideas to really think about, you know, what happens, you know, influence is what we just saw in the previous talk, but also pre-processing of times. So this really does. I'm going to hit a few things that we've already heard today, but I'm Heard today, but I mean, a lot of what we've seen today so far is looking at non-parametric estimation, which is giving a trade-off between bias and variance. Okay, but if you look at very often at these non-parametric estimators, even multi-taper spectro estimates, okay, they still look quite noisy when you're trying to get a stable spectral density function estimate. Okay, as we heard from David this morning, using a parametric approach normally is very limiting. Which normally is very limiting, just for the simple reason that we're introducing a model misspecification there, okay, in terms of not knowing actually what the true model is or the true class of models that we're considering. And that can compromise estimation. So what I'd like to actually talk about today is something in between, which is what we would call a semi-parametric model, where we're going to think about trying to model this log spectral density function in terms of a truncated basis expansion. Terms of a truncated basis expansion, but we're going to allow the number of basis functions to increase with sample size. Okay, and so what this allows us to do is to try and get this idea of getting at a stable SDF estimate without hopefully trying to improve the, say, the root mean squared error of estimation. So, what's the problem here? Well, clearly, if you're going to allow the number of basis functions to increase with sample, Of basis functions to increase with sample size, okay, then you're going to have to enforce sparsity, you're not going to allow every basis function in, okay. And so what we want to do is we want to select the basis functions we want, but there's also a lot of work that people have done to think about what could be computationally efficient ways to do that. Okay, as we've also heard today, you know, in terms of just using the basic multi-taper spectral estimate, okay, computation. Estimate, okay. Computationally, that's pretty convenient, right? Once you are able to calculate your tapers, okay, then things tend to go very simply, right? You know, it's easy to do a fast-figure transform, for example. And so clearly, you know, doing a semi-parametric method, we're not going to be able to do something, you know, quite as fast as that, okay, because we're going to base this off a multi-taker estimate. But we hope that once we've got We've got to then thinking about this: that computational efficiency is just as important as enforcing sparsity. Okay, so there's a pretty rich literature in terms of people looking at enforcing sparsity for estimating log spectral density functions or spectral density functions. There's a series of papers that look at it for wavelets of thresholding. So the basic idea is to take your log spectral density function, rewrite that in terms of a wavelet decomposition. Rewrite that in terms of a wavelet decomposition, okay, and then shrink the coefficients, okay, suitably so that we actually get a sharper signal. What's really nice about that approach is it's an order n computation, which is nice if we have, you know, for a time series of length n. In terms of these methods, there are also people enforcing it through different penalties as well. Okay, so for example, we could use an L2 penalty. So, for example, we could use an L2 penalty that basically looks at how wiggly the signal is and tries to enforce some penalty to make it not too wiggly in some sense. And there's a pretty old literature of people using penalized least squares methods. And then more recently, but still not that old, still not that new, 94, a penalized Whittle method. And at that point, if you think about it, taking an L2 pen. It and taking an L2 penalty is going to shrink coefficients, but it's not going to select coefficients, so it's not going to enforce any sort of sparsity. Okay, and so certainly the more recent methods that have adapted these older methods to multivariate time series, for example, and multivariate spectral density, matrix estimation, have had to spend a lot of time thinking about how to do model selection, often with cross-validation. Cross-validation, okay. And once you start to get into the cross-validation question, okay, then the computation really becomes quite unwieldy. And so that's something that we want to try and dodge in terms of thinking about this. So what's our approach? We're going to think of looking at a quasi-likelihood approach. So you could think of that as an approximate likelihood. For estimating spectral density functions, we're going to use what's called the Whittle likelihood, but now adapted for multi-taper spectral estimates. And at the same Spectro estimates and at the same point we want to enforce sparsity. I probably don't need to say to anybody in this workshop why multi-taper spectral estimates are cool, fat, hip, and trendy, right? Yes, everyone's happy for that point. But I mean, we know that there's a rich history of talking about the variance bias trade-off and really thinking about the components of variant that we account for by looking at these different components. The real reason that we also look at the little likelihood. That we also look at the little likelihood, this quasi-likelihood method, is that it's going to improve estimation over standard least squares. But of course, the trick here is to make sure that you can do this in a general enough way so that the computation isn't too hard. Okay, so let's just take a univariate stationary time series. Let's just say without loss of generality, we take delta, the sampling integral, to be one. Okay, there's the covariance function, and then we can take it as its Fourier transform. And then we can take it as its Fourier transform pairs to get the spectral density function. You can see here, I'm not going to spend too much time going over that. So I'm sure we're fairly familiar with that idea. But of course, the essential idea is that it's equivalent to consider either the audit covariance function or to consider estimation of the spectral density function as long as it exists. Going back to Wald in 2000, he Going back to Wald in 2000, he showed that there was a large series of estimators, including WOSA and a lot of the lag window estimators that can be rewritten in terms of a multi-taper spectral estimate. And so that's a pretty good place to start from. So let's just take the standard form of it, not the eigenvalue weighted one, but we'll just take the straight averaged multi-paper spectral estimate. So what we're going to do is we're basically going to using our Going to using our k-orthonormal data tapers. We'll taper the time series using each of those tapers, take Fourier transforms modulus squared, that'll give the k tapered spectral estimate, and then we'll just average them to get the multi-taper. Okay, so certainly something that we're all comfortable with. I can see people nodding. That's a good start. Okay, and then our question now is to consider this set of basis functions, right? And so we're going to introduce this basis function. Introduce this basis function expansion. Okay, the beta you can think of as just being the coefficients for the basis expansion. And as I said, we want to allow the basis functions p to increase with the sample size. Okay, when Shuhang started to look at this, we looked at probably far too many basis functions in terms of thinking about this. Okay, everything from polynomial, Fourier B-splines, wavelet bases, and some mixed ones as well. But certainly. Well. But certainly, in terms of what we saw, it tended to be that wavelet estimators actually tended to do pretty well in terms of the simulations that we looked at. And so I'm just going to restrict to that. But it's important to note that the method that we propose here works actually for all of these methods. Okay. And the computation also works, which is a nice appeal. Okay, so what is the multi-taper whittle estimator? It's really quite simple. All we're going to do is we're Quite simple. All we're going to do is we're just going to take the multi-taper spectral estimates, evaluate it that the non-zero non-Nyquist. So non-zero, remember, we were assuming a sampling interval of one, so just the frequency wouldn't be a half for non-Nyquists. I'll just call that NZNN. And we'll look at the Fourier frequencies there. Okay. And then all we're going to do is we're going to write down this estimating this quasi-likeloid for basic. For based on the multi-taper estimates. Okay, so if you look at this, this has a history going all the way back to Whittle, you know, back in the 1950s, where this would just be a piradogram here. Okay, and so all we're doing is replacing it by the multi-taper estimate. Okay, if you go back to Walden's 2000 paper, okay, and under a number of conditions, okay, where you use either the sign or DPSS tapers or selecting. DPSS tapers or sliping tapers as we were talking about earlier today. Then, marginally, for each frequency, as long as it's fixed, and there are other ways to not make it fixed, but let's just consider it as fixed frequency. Then we know that the multi-taper spectral estimates converge to basically a scaled chi-squared, okay, on 2k, where k is the number of tapers that we considered, okay, in terms of then, okay. terms of then okay so how does that help us to think about what this is actually what this is a quasi-likelihood function for well there's one more thing that we need to think about before we do that so first we can see that it's basically looking at a scaled chi-squared distribution which you can actually identify this over this would be a gamma distribution um but also we know that multi-taper estimates are correlated over frequency Estimates are correlated over frequencies, okay, in practice. Okay, and people have spent quite a bit of time trying to think about how to make an approximation there. I'll put down the expression here that says, you know, if we have a slowly varying spectrum, okay, and we're in the neighborhood of the frequency, then, you know, we can identify, you know, a nice, pretty accurate, in fact, expression for the covariance between two multi-taper spectral estimates. Spectral estimates, or estimators, I should say. But even with that, what can we say about this expression here? Well, if you actually look at it, this basically just corresponds to the sum of gamma likelihoods. Okay, it's a sum of gamma likelihoods. The gamma likelihoods, well, if you look at it, there is no correlation assumed in this expression here. You're actually assuming that each of them are uncorrelated. That each of them are uncorrelated with each other, okay, in terms of that spectro estimates over the different frequencies Fj. Okay, so more formally, okay, what we're actually looking at here is that if you write down an MT Whittle and multi-taper Whittle likelihood, it will correspond to a gamma quasi-likelihood. Okay, it assumes that asymptotic distribution at the non-zero, non-myquisfer frequencies, and it assumes independence between the fur frequencies, okay. For frequency, okay. Now, why does that matter? Well, what it matters is in terms of thinking about the theory, okay. In well, somebody with over the right was talking about medical statistics, right? In biostatistics, people are very used to thinking about looking at quasi-lactodes and what their asymptotic distributions are. They're less familiar with it in the case where we penalize, and that's what we're going to talk about here. So, how are we going to So, how are we going to estimate our betas? How are we going to look at this sparse representation? Well, we've got our basis all collected up into a matrix. We want to estimate the betas and we're going to penalize. We want to penalize for sparseness. Okay, and so what we're going to do there is we're going to juice what is called a lasseau-type penalty. Okay, this little lasseau penalty, and this basically says that any of those That any of those coefficients that are very small, we're actually going to make exactly zero. Okay, and the amount of control that we have on this penalty comes from this lambda parameter. Okay, so as we change the lambda, that basically changes how much penalization we have. Okay, and in addition to being able to estimate beta, as long as we could solve this optimization, okay, we're able to set some of them to zero. Okay, and the power of doing that is that if we have a sparse That is that if we have a sparse representation, okay, then hopefully we can recover that by using this sort of optimization. Okay, now, now, what's the real problem here? The problem here is the computation, okay? How do we actually get this computation to work? Okay, well, Yung Shang Shu, my collaborator, How State, is very comfortable in actually doing these optimizations. Okay, and it goes back to a lot of work that Boyd has done into re- Done into rephrasing these problems into a primal dual and introducing enough variables that you can actually turn it into. I wouldn't say a simple optimization, none of this is simple, but at least it's straightforward as in I can write it down in one page, okay, to solve it. Okay, and the other part about it is that it's very computationally efficient. Okay, so you can see what's happened is that this is the optimization I need to solve. Okay, and what I've done is Okay, and what I've done is I've added some extra conditions, okay, some extra variables. Okay, and this is the optimization we want to solve. And if we solve this one, this will be equivalent to solving this one. Okay, Boyd shows you how you can actually add to this, and we'll get to that in a second. Okay, so I need to be able to solve this problem. What is a way to do it? Well, if you look in the literature, especially in the wave literature, when people were considering. Of the literature when people were considering, for example, likelihoods, non-Gaussian likelihoods, and penalizing, they used an imperial point algorithm. And this was very computationally intensive. Okay, so what we're going to look at instead is called an ADMM alternating direction method of multipliers algorithm. And what's really nice about this is that the periteration cost is less than using interior point algorithm. Okay, and you can actually use distributed periteration. And you can actually use distributed parallel computing as well. And it's pretty efficient in terms of what goes on. So, for example, to get to an epsilon optimal solution, i.e., I'm within, you know, in terms of my norm, I'm looking at, you know, within epsilon of being for the phi beta function. Certainly, if I use an orthogonal basis function such as wavelengths, it's almost of order m. Almost of order m, proportional to how close I want the solution. And remember, m is the number of frequencies that we're evaluating our multi-taper estimate as. And then it's more expensive if I use a general basis. But it's more expensive, but not prohibitively so, depending on which bases you're using. So, certainly, if you have compact bases, for example, then you still get some benefit here. Fit here. So, as I said, it's not simple to derive the algorithm, but as you can see, the steps are fairly self-contained, right? So, as I said, the method is to start with these parameters that I'm trying to update for, but also to solve the dual problem at the same time. And what you can realize is that it's basically one, two, three, four, five sub steps that you're calculating each time. Okay, one is basically a way. Okay, one is basically a weight of least squares problem. There is one minimization, which is very fast and efficient to calculate because it's a one-dimensional optimization. Okay, there's a soft thresholding step and then basically two update steps. Okay, so if you know anything about Bayesian statistics, it almost has a kind of Bayesian flavor of doing a Markov chain Monte Carlo, right? But as I said, this is a very efficient algorithm to actually solving these sorts of problems. These sorts of problems. Okay, the other problem that I identified is that, in terms of what people do typically to come up with how they are going to pick the penalty factor, they use cross-validation. And what we found in our simulations as we started this research, or certainly Shuhan started his PhD, is that the cross-validation didn't do very well, number one. Um, and number two, it was very computationally intensive as well, okay. And so that really pushed us to start to look back at the literature and think about if there were other methods out there that actually performed better and were almost, at least for one of these methods, data independent. Okay, so if you can get something that's data independent, then at that point you're you're going to, and you can do better, then you're computationally you're going to do much better. So the first one is to use a scale calibration. The first one is to use a scale-calibrated universal threshold. This comes from the wavelet literature, but what we're doing here is using the quasi-likelihood theory for our Whittle estimator, okay, which gives us basically the variance estimate that we need, or the variance value, which basically follows from how many tapers we have to pick lambda. Remember, this is the penalization factor. You can also do a penalized You can also do a penalized likelihood as well. Okay. And you might be used to some of these methods like AIC ICAIKI's information criterion or the Bayesian information criterion. These are all forms of these sorts of information criterion. But for these sorts of problems, there's literature, Fang and Tang is the most famous, where they consider what penalty you should use as the number predictors increases. The number of predictors increases exponentially with the sample size, and what they show is actually a very different penalty than what you're probably used to. Okay, so a log log m, that's your sample size, and log p is the number of parameters. So this requires a one-dimensional optimization. This one requires no optimization. Okay, let's look at the time for a minute. We did work on some theory. For this type of audience, I won't go through it too much. I won't go through it too much. So, the basic idea is that what we're thinking about here is that we're assuming two things. The first one is that we have a sparse representation for the log spectral density function. The second one says that we have a compatibility condition. I'm not a super expert on compatibility conditions, I will be honest. That's my collaborator. But the essential idea is that it allows you to move from, in the traditional sense, least squares to L1 penalties. L1 penalties by considering ways to move from one to the other. For our problem, that's not true anymore because we need to be able to move from the Whittle estimator to the L1 penalties. Okay, and so these two conditions have to be true. The first condition is really just about the fact that we're trying to basically estimate something that has a sparse representation. And so that's certainly something that you would imagine to be true, right, in any given situation. To be true, right, in any given situation, the compatibility condition has to be checked for, or in some sense, has to be checked for each series. Although, for what we've seen, for what we've been looking at, we haven't seen any cases where that was not true. What's interesting about the theory here, I'm not going to go through it in the painful detail, is that we can basically bound the error that we're making either The error that we're making, either in terms of the betas for a given penalty or in terms of the log spectral density functions. And what's interesting about this result is it's actually, it looks like a random event, but or a random result, but it's actually, this has to do with what you actually observe for your given series at the time. So it's actually a deterministic bound on what happens. And in terms of that, that's not something that's that unusual in terms. Something that's that unusual in terms of what people are looking at less so. We have thought about how to try and make this result look more palatable. And really what that says is if we think of the asymptotic distribution of our multi-taper estimate, it basically leads to us conjecturing the following rate of convergence. So, what I want to show here is that there are two things that we need to think about. One is that it depends on To think about. One is that it depends on how sparse a representation you have, okay, to how well you're actually going to do in terms of your estimation. But also, it's going to depend on the length of the series. And there's a log m rate, okay, that actually goes slower. The important one is actually this m to the minus one half one. So the point is, is when things are sparse, s naught is zero. Okay. To zero, okay, and what we get is something very close to a parametric, right? Okay, but if you notice here, the point is that we're not doing something parametric, though, right? So if you remember David's talk this morning, right, David was saying, you know, and the parametric, okay, that's what's going to lead you to have the trouble, right, in terms of doing estimation. Okay, so if you want to get something smoother, okay, then penalizing is certainly something that you can do. Certainly, something that you can do. And if there's a sparse signal, okay, then you're going to get substantial improvement in estimation efficiency. And that's really the kind of take-home message from us. Okay, we did look at a number of different simulations. These are completely stolen. These three example situations come from a wavelet thresholding simulation result. We were actually asked how it would perform. Actually, we asked how it would perform in a Gaussian case as well. So, we actually changed the innovations to be Gaussian as well. So, if you look here, there's an AR2, they are for these are pretty famous examples of ones that have significant leakage, okay, because of their high dynamic range. This MA 15001 just has this large peak, which is very hard to capture, and we actually do a pretty Capture, and we actually do a pretty good job in capturing it. And then, of course, the non-Gaussian spectrum will be the same as the Gaussian one. And what I wanted to show you really to get a feeling for what we're thinking about here is the red is the true spectral density function. The blue is what we're estimating. And the gray is a raw multi-taper estimate, okay, with 10 tapers. And so that's really what I'm trying to show you here: is that we're getting, you know, there are. Is that we're getting, there is still some noise there and some uncertainty, okay, but it's a lot smoother than what you would get coming out of the multi-taper method. Okay, so in terms of trying to identify those spectrum. What we found as we go through these different methods is that if you don't penalize, then you do really badly in terms of integrated group mean squared error. And if you use cross-validation, you still do pretty badly. You do a little better. You do a little better, okay? But the cross-validation is so painful and so inaccurate in terms of this that you don't really do any gain whatsoever. The real gain comes into using the lessaux methods. Okay, we did consider least squares versus a Whittle method, and in each case, except for the MA 15000 case, all of these situations we actually do better by using the multi-taper-Whittle method. Than multi-taper-whittle method overtaking least squares. So there is an advantage to doing least squares. It's a little harder to say which penalty you should use, although you could certainly argue that the universal threshold is very easy to calculate. And maybe that's the one to go with. Okay, in each of these cases, the uncertainty of these estimators, we have a thousand similar realizations in each simulation. In each simulation, the confidence width is smaller than the symbol height. Okay, so you can see that there is significant differences. Okay, so we looked at EEG signals. Okay, so I'm not going to dare mispronounce this, so I'll just call it EEG if you permit me. So they're used to monitor brain activity and, of course, diagnose disease such as epileptic seizures. Such as epileptic seizures. So here's an example taken from the Mayo Clinic's website. Typically, what happens here is there are lots of locations on the brain, and then you're typically looking at how these signals crop, well, either first of all, correlate and cross-correlate. I'm just going to show you two marginal spectral estimations for the moment. So we're going to look at two channels of EEG, the left and right front cortex. The left and right front cortex of one male rat. Okay, and the idea of using a rat is that it's functionally relevant to human epilepsy. Okay, the data was presented back in this paper here, and we downloaded it from this website, although the website disappeared, and I have no idea how to find it again. So other than I have a copy of it. We're looking at a thousand voltages in microvolts with a sampling rate of 200 hertz. Rate of 200 hertz. Okay, these are the two left and right channels, and then here are estimates. And all I'm going to show you here are the two versions of it using the different choice of coming up with lambda, that penalization. Okay, either the universal or the generalized information criterion. And you'll notice for the left channel that it tends to agree quite well. There is a little uncertainty in terms of using the threshold for identifying the right channel. For identifying the right channel, if you notice, the left channel has a much stronger, tighter signal, but both are showing broadband spectral peaks. Okay, in terms of the right channels, it's a little grayer there. Okay, and at the moment, we're currently thinking about methods of uncertainty quantification for this. Okay, so the point here is that using our L1 penalized multi-taper estimate performs better or as good as. Estimate performs better or as good as previous methods. What's nice about this method is it applies to a large class of basis functions and their mixtures. Okay, so you can certainly use dictionaries here without any problem, although there is some computational hit for doing that. Cross-validation was a crummy estimator as compared to for selecting lambda, okay, compared to using a universal threshold. Okay, and as I said already, computation. And as I said already, computationally, there was a nice data invariance with using a red universal thresholding. And the algorithm is very fast in terms of using this. I will apologize at one point we haven't still not made our code available online, and that is something that we have to do. I know it's just about time to finish. There are plenty of ways that you can extend this too. You know, one that we're currently, I won't go through all of these. We're currently, I won't go through all of these. There's lots of things here. I mean, there's kind of three things I wanted to talk about just because I think they're so interesting relative to what we're talking about in all the other sessions, right? One, we've already had some questions, right? Francois asked quite a few about this, about uncertainty quantification. And certainly, you know, there is a big question about how you do uncertainty quantification. And as you get into multi-taper across, you know, coherence phase, you know, multivariate. Phase, you know, multivariate quantities, that becomes a much harder question, right? Especially if you're trying to smooth and try to come up with estimates that way. We spent a bit of time starting to think about time series prediction using estimated spectral density functions, right? People do this a lot, but you have to, or people have done this, but there's, you have to be very careful about it in terms of thinking about sampling rates. And then in terms of trying to get this In terms of trying to get this to do in the multivariate setting, at the moment, there is no general purpose theory out there in the statistical literature for Lasso estimation in these sorts of problems. So that's already a problem. There's a bit of a disagreement in how people use approaches, everything from Cholesky to non-Choleski. But this really kind of links to points that have already been made as well in this workshop. In the depending on ways that you think Depending on ways that you think about estimating things, it introduces either biases or variance or uncertainty quantification issues with trying to identify what happens in a spectral density matrix and its components. What we've noticed so far is that depending on the approaches that you use, you can get very different answers to A, the estimates, B, the biases, and C, uncertainty quantification. Okay, there's no magic. Okay, there's no magic answer to how to set out the multiplayer problem that gives you the same answer each time, and that's something very interesting. As I said, that's very early findings so far. We're still working pretty heavily on it. So, thanks a lot, folks.