Okay, good afternoon, everyone. Today, I'm going to present some work on bounds for MMPP-type phase type distributions. So, I will first introduce pH distribution, then I will define this MMPPPH distribution and then present the main results. Results. So, first, what is a pH distribution? Well, a pH distribution, or we call it a phase-type distribution, is the distribution of the absorption time of an absorption state in a finite state continuous time Markov chain. Well, I focus on continuous time case in this talk. It can be discrete time Markov chain. For instance, For instance, we have a Markov chian with five states and the fifth state is absorption state. And states one to four, they are transit states. So you start from a state there, you travel between those, jump between those states. Then at a certain time, you enter the absorption state, then you stay there forever. So the time until the absorption Until the absorption is a random variable, and we call this a phase-type random variable. And its distribution is called a phase-type distribution. In a more formal way, we use this YT for the underlying Markov chain with m plus one states, and this is the infinitesimal generator of that underlying Markov chain. Well, within this infinitesimal generator, So, within this infinitesimal generator, T is capital T is a sub-generator, and E is the column vector with all elements B1, and state M plus 1 is the absorption state. That is why this last row here, it's all zero, because this state is an absorption state. There's no transition in or out. So that's why we have all zeros there. We have all zeros there. And this is a quick example. You see, this capital T is given this way, where for this case, m equals 3. Now then you can see the rows, the row sum here, they are all zero because those are the rows in that infinitesimal generator. The row sums should be zero. Okay, now we're going Okay, now we are ready to define our phase type random variable, which is defined as the absorption time of the state m plus 1. Well, if the initial state of the underlying Markov chain is given, here we assume the initial state follow this distribution. Then the distribution of The distribution of X can be, or is well defined. In fact, by routine calculations, we can find this closed form expressions for the distribution function of X and the density function. Well, when you look at the two expressions for the distribution function and the density function, you find you find they are completely determined by this vector alpha and this sub-generator capital G. So we use this pair alpha t to represent the pH distribution and well we will call this pair alpha t as the pH representation of the pH random variable x. Random variable x. And here is a quick example. So t is the one I presented on the last slide, and alpha is a stochastic vector. So this alpha t that together determine a phase type random variable whose distribution function and density function are given in this way. Well, for phase-type distribution, some of the basic quantities of the basic quantities can be calculated from alpha and t very easily. Like this is the mean and the variance, the second moment, the variance and squared coefficient of variation. Well, we all know variance is a quantity people use to measure the variability of a random variable. Of a random variable. SCV, the squared coefficient of variation, is also a quantity people use to measure the variability for a random variable. SCV is especially useful when you want to compare the variability of two random variables who are at very different scale. Like the first one has a mean of 10,000. Has a mean of 10,000, and the second one has a mean of 10, then usually the variance is not a good measure to a quantity to compare the variability of the two variables. But ACV, because after this rescanning, this becomes unitless. So it's a good quantity for us to compare the variability of very different random. Different random variables. Okay, for phase-type distribution, it was introduced in 1975 by Marcel Newts. And since then, there's plenty of study on phase-type distribution and also lots of applications. For some results related to my current research, this is the first one and this is the second one. They're all about the They're all about the squared coefficient of x and a lower bound, which depends only on the order of the pH representation, nothing to do with those parameters within alpha and within the matrix capital T. And now the second one here is for the bounded phase type distribution. And again, we found Distribution and again we found this lower bound in terms of the order of the pH representation for SCV. There's of course many other things, but those two are closely related to the current research. Well in the current research we found a bound for S C V and which is actually independent of all the parameters. Of all the parameters, but it's only for a special class of phase-type distributions. We call them MMPP-type phase-type distributions. So, to present my results to you, I need to first explain what do we mean by MNPP type of phase type distributions. And then we need to explain why they are interesting and what are the issues of interest. are the issues of interest. So first, why we call them MMPP type? Well, so that leads to the definition of MMPP phase type distributions. Well, so first let me define the MMPP type of phase type random variables. We start with finite state Finite state continuous time mark of chan with infinitesimal generator q and q is irreducible and of the order m. Well, since this continuous time mark of chan is irreducible and with a finite number of states, so the stationary distribution or limiting probabilities exist for this continuous mark of here. And we denote Continuous mark of here, and we denote it as pi. Now, with pi and q plus one more diagonal matrix D, then we can define a phase type of random variable with this pH representation for which alpha equals pi, which is the stationary distribution of q, and t is q. And t is q minus d because the q is an infinitesimal generator and d is a diagonal matrix with non-negative diagonal elements. So this is a sub-generator. Therefore, this pair pi q minus d define a pH random variable and I denote that random variable as x. Well, similarly, Well, similarly, by using pi d and q minus d, I define another random variable I denote it as xd. And we call pi q minus d and pi dq minus d as the m p type pH distributions. It's a special class of pH distributions. So that's the definition. So that's the definition. Well, then the question is: why we call them MMPP type? MMPP is for Markov modulated Poisson process. So next, maybe explain to you why we call them MMPP type. And from there, you will find out why we are interested in those two random variables. So, MMPP, to define MMPPP, To define MMPP, again I start with the underlying mark of Chien, a continuous time mark of Chen with a finite number of states and also with infinitesimal generator Q. Now then we impose a Poisson process with arrival rate di in state i. When the underlying mark of Chen is in mark of chen is instead i then we impose such a poisson process so in this way we have defined a counting process and this counting process is called a markov modulated poisson process in the literature well a matrix representation for it is q mass d and d and the average arrival rate is lambda we denote it as lambda which is pi d e. As lambda, which is pi d e. Well, another way for you to look at this MMPP is to start with a Poisson process. Suppose you have a Poisson process with arrival rate number, well, the arrival rate depends on the state of underlying Markov Chen. So you have a Poisson process, but the arrival rate is changing when the state of the underlying Markov Chain is. State of the underlying mark of change is changing. So, if you look at this accounting process from this perspective, then you understand why this is called a Markov modulated Poisson process, because the arrival rate of your Poisson process is modulated by that Poisson process, that Markov chain. Well, what's the connection between this MMPP and the two? MMPP and the two variables we defined earlier. We defined X and Q, sorry, X and X D. Well, it turns out X is the time stationary inter-arrival time in this MMPP. And Xd is the event stationary inter-event time of this MMPP. Well, graphically, it looks like this. looks like this. This is your MMPP. Those are the events, or we can call it arrives. Suppose this MMPP is already in steady state, then at any arbitrary time t, this state is distributed according to pi. And then the time from this arbitrary time until the next arrived Next arrival or next event has this pH distribution x. Now, then at this point, you have an arrival. This is an arbitrary arrival event. Then right after this event, this state of the underlying Markov chain is distributed according to pi D. Then the time between these two consecutive events. Events is actually Xd. So that explains the role of X and X D in the MMPP. And that's why we are interested in those two random variables. So next, what is the specific issue of interest? Well, for that, I go to the main results. I go to the main results, and the main issue is SCV, the squared coefficient of variation of Xd is greater than or equal to 1. That's the issue of interest. Why this is interesting? Well, earlier we talked about if S C V for a random variable is a quantity to measure the variability of that random variable. Well, for M M P. Well, for MMPP, Xd is the inter-event time. So this SCV can also be used to measure the variability of the MMPP. That's why it's interesting. Why this greater than or equal to one is interesting? Well, for engineers or other practitioners, for them to model input process. them to model input process they need to they need to choose uh what kind of accounting sorry what kind of counting process they should use um if they want to use mmpp then they need to know the original process or the in the real input process the squared coefficient of variation of the inter-event time should be greater than or equal to one otherwise they shouldn't use Otherwise, they shouldn't use MMPP to model the input process. That's why this is interesting. Okay, now it's the issue of interest then. How do we prove it? For years, I think engineers already know SCV XD is greater than or equal to one. But I don't think there's a proof there. And the main contribution of this research is to give a proof. First, by routine calculations, this inequality is equivalent to this one here, which is related to the mean of the random variable x we defined earlier. Now then, this inequality is equivalent to this one. This one. And if you move this lambda to the other side, it is. Lambda to the other side, it is to prove this inequality, which is very simple. People with knowledge about linear algebra, they should be able to understand this problem. Now, how do we prove this one? Well, our proof depends on two lemmas. The first lemma is this matrix is non-invertible if and only if this only if this equals this, which is in our in the inequality we want to prove, the equality holds. If the equality holds, then this matrix is non-invertible, which means this is singular. And the second name is this matrix is non-invertible if and only if d equals number times i, where i is the identity matrix. So that means this matrix. This matrix is non-invertible if and only if all the elements on the diagonal of D must be equal to each other. Okay, well, if you combine these two nemers, then this equality holds if and only if d equals lambda i. And now we can draw a conclusion from this. That is, this one is equal to one. equals is equal to one if d is lambda i otherwise if d is not lambda i then this is greater than one it's greater than one well i think um we know if it's not one it is possible it is sorry greater than greater one than one okay it is greater than one or it could be less than one right but here i claim uh you know the I claim this has to be greater than one for the second case. This will become clear when I present some additional results we obtained. So that's the main results. And when we look back, then we realized in this proof, there are two key points. The first one is this observation. Observation. Like this matrix is somehow associated with the inequality we want to prove. That observation is a key step in the proof. The second one is the proof of NEMA2. The proof of NEMA 1 is straightforward. It's simple, but the proof of NEMA 2 is a little bit more complicated. Well, once we Well, once you have lemma one, lemma two, then as you can see, we can draw conclusions very easily. Except here, I still have to justify: if it's not one, it has to be greater than one. Well, we got some additional results in the approval. To present the additional results, I'm going to define this xx. Well, xx is actually very similar to our x. similar to Rx. And actually Rx is X1. When Q is an infinitesimal generator, X times Q is also an infinitesimal generator if X is positive. And even further, pi is still the stationary distribution of X cube for any positive X. So therefore, similar to So therefore, similar to the definition of x, we can define this phase type distribution with pH representation pi x times q minus d. So now we have all those random variable x. And the first additional result is we found, or we proved the upper bound for this, for the mean of xx. For the mean of xx. Of course, earlier we have shown this lower bound. Now, here we proved this upper bound. Well, we also proved two limits for the mean of xx. When x goes to 0, it converges to this upper bound, and the upper bound is greater than or equal to 1 over lambda, the lower bound. And when x goes to infinity, it converges to the It converges to the lower bound. Well, if you look at the first limit, now you know the mean of all of them have to be greater than the lower bound. If one of them is greater than lower bound. Because it's either below or above. But when x goes to zero, we know the mean is greater than one over. is greater than one over lambda. Then therefore for all x greater than zero, the mean has to be above one over lambda. Now remember the equal occurs only if d is lambda i. If d is not lambda i then you can you either always above one over lambda or always below. Or always below. And here, this shows it has to be above. Therefore, we have proved our main results. Now, in addition to those things, if Q has a special structure, like Q is time inversible, time reversible, then we have additional results. This function, as a function of x, is actually decreasing. And also we decreasing and also we proved the squared coefficient for xx is actually greater than or equal to one okay earlier we proved this for xd and it turns out it is also true for the random variable x so what is this time reversibility now i'm talking about a time reversibility of the process it's not a q Process. It's not Q. But if the process is time reversible, then Q has to satisfy this condition here. Well, it turns out some of the popular cases are a special case of time reversibility. Like if m equals 2, then always Q is time reversible. Of course, Q is irreducible, then it's time reversible. And if Q has this birth and this structure, and also This structure, and also if Q is symmetric. So, for this special case, we don't need lemma one and lemma two. We can prove all the results from this result here, and the proof is much simpler because of this spatial structure. So, those are the main results we obtained. And now, let me summarize my research. My research or summarize my presentation. We looked at two random variables. We call them MMPP type PH random variables. And we proved for X, for X, we found the lower bound and the upper bound for its mean. That's for X. And then for X D, we found We found that this SCV of Xd is always greater than or equal to 1. So, those are the main results we obtained. There are a number of interesting questions we are working on currently. I call them conjectures. First one is the SCV of X is greater than or equal to 1. We prove this one partially, but not for the general case. Not for the general case. And the second one is this function, we have shown under that condition, special condition, this is decreasing. Actually, we want to prove this is convex and decreasing. And we want to prove this function, this mean as a function of the diagonal elements of matrix D is actually convex. is actually convex. And we want to prove, this is a very strong and general result. We want to prove x is stochastically larger than xd. Now, all those conjectures, we have not found counterexample numerically. So all our numerical examples confirm they are true, but we are not yet be able to Yet be able to prove them yet, but we're trying. Now, references. Well, I talked about those existing results from those two papers. Our problem is from this paper by Azem and Yoni. And we proved one of their open problems. Their paper is about MMPP. MMPP. And then we looked at the those two phase type distributions related to MPP. We proved those results. I think that is my presentation. Thank you very much. Any question? Any question? Thank you. Any question?