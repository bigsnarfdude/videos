You can hear us. Yes. Do you see my slides? Yeah, we can see the slides, like not in person. Okay. Well, thank you for having me. Sorry that I couldn't make it in person. So here it's really, I tried to reframe our PCS framework. Many of you might have heard it in the past few years from the uncertainty point of view. And so a lot of my grouped work is from biomedics. My grouped work is from biomedicine. And we recognize that data science is not just one step, it's a life cycle. And we very much want to embed ourselves in the context. And there are many, many human judgment calls in this process. So uncertainty quantification is central for building trust in biomedicine and data science and AI in general. So currently, we only consider assume there is a good generative model, stochastic. You model, stochastic, and then we look at sampled summary variability. But if you look at the data science lifecycle, there's many, many other processes that drive uncertainty. So I want you to take home with that we are ready to tackle data cleaning uncertainty and also model choice uncertainty. There are many others which we'll still consider holistically. So this is just useful. I did a data science project in my applied statistics class with all the helps listed there. Class with all the helps listed there, and you can see that the different groups of students clean the data differently, and each of them even had a medical doctor to work with. So, there's some domain knowledge. And then in terms of the variability uncertainty from different versions of the data is about the same as bootstrapping the same clean data. So, this is a significant amount of uncertainty that we're not currently taking into account formally. And then, there are two recent papers really bring the attention from domain. Really bring the attention from domain scientists, social scientists showing that you have 73 teams of people who work on the same data, they didn't worry about different data cleaning. And just from the model choices, the algorithm choices, they get results about important immigration and hypothesis from the very positive and very negative. So that's why there's hidden universe uncertainty. Similarly, there's a paper from 174 teams of people working on two People working on two ecological data sets similar results. So the uncertainty is huge and qualitatively different. So to gain really trust and also maximize the data science and artificial intelligence promise, we need to really insist on more model checking that we used to do more. Now data is big. We usually jump into inference without enough model checking. Also to make sure the uncertainties And also, to make sure the uncertainties from human judgment cause are properly taken into account. We need a quality control own work. So, PCS is a framework that has been proposed since 2020, and I've started working on this toward this framework for 10 years. And we have a name, thanks to Tian Jung from Columbia University, called Revertical Data Science. It's really try to take the next step, follow up Leo Bryman's two cultures paper now over 20 years ago, contrasting machine learning. Ago, contrasting machine learning from statistics. So, we want one culture. We want to marry the machine learning and statistics culture, take something predictably as a minimum check for model or reality check, and take computability to heart and expand hugely stability from sample to sample variability and to many, many other perturbations, especially from human judgment costs. In one slide, PCS is really a single. Slide PCS is really a synthesis, it's a unification, it's a streamlining and expansion of the best practices, both from machine learning institutions. It's really try to build a platform for further development. It's not set, the philosophies are set, the concepts are set, but it's evolving. So in a nutshell, you really consider reasonable human judgment costs in the whole process. Right now, we formally consider data cleaning and algorithm choices. And even though Algorithm choices and even though data visualization you choose different colors, you have different conclusions, and then we have different pipelines because we have different steps. And then we use prediction at the minimum check to make sure we want a good pipeline. And among the pipeline, then we seek stability to aggregate a different result that pass the prediction pipeline check. And C is implicit. We have another paper on data-inspired simulation, try to set some guidelines for data science simulation. Some guidelines for data science simulations. So it's really about shaking your process as like a hardware device, assess the stability, instability with multiple metrics, and then the four ones that pass this shaking through prediction checks, then you aggregate. And context is very important. So I started pursuing stability about 10 years ago, and that was in the backdrop of reproducibility. So this is really for Reproducibility. So, this is really for scientific reproductivity, not like computing reproducibility. You need computing reproducibility, but we want a higher level. And stability is also prerequisite for explaining both AI and also machine learning. So, people will say that you do all these perturbations. We have to insist on reasonably because otherwise you never finish and you don't want to unreasonably, unreasonable shakes right, go to the top of my building. Where I go to the top of my building and then drop my phone, it's not a reasonable perturbation to my phone. If I'm sitting here, drop on the floor, that's a reason perturbation that shouldn't break. So you have to understand the purpose, the process, and use domain knowledge to define what's reasonable perturbation. And you write a document about it. So the stability principle really unified the traditional and the untraditional, like what I heard the last talk, like adversary texts and like protection. And like perturbations, argumentations, all of that, that is all can be unified. And similarly, you can talk about model perturbation, right? You use domain knowledge and prediction checks, and then you aggregate all the different results after the prediction child reality check. So, how do you choose perturbations? Right? Reasonable domain knowledge content. Right, reasonable domain knowledge context and also constraints of computing human resources. So, you usually want to probably perturb the pain points which you worry most that might break. So, we are not removing human judgment marks. We're just removing arbitrary human judgment, or at least explore the consequences of arbitrary human judgment because things you're sure you write a document about it. There's only one way to do it, make a case, and that's allowed. It's very much related to this forking has been. Forking has been around from Gauman and his co-authors. And we really insist on documentation. So models are not realities. And you want to say your model capture reality, you have to do the work through documentation and build quantitative and qualitative evidence. And we have a template for people to use from my friend page. So PCS is really developed. So PCS is really developed. I'm a lot more confident about its usefulness than say 10 years ago when I started on its path. It's a pretty ambitious research program. It's philosophical, conceptual, practical, and really stand on basic principles. It's a systems approach. Integrate, look at things different steps together, not just, oh, let's just do modeling, which is important, but it's not enough. And with expanded uncertainty quantification, and we have to document. And we have to document. That's a big part of PCS framework, and we need to discuss what's reasonable perturbation in context. There are many success stories that my group has been able to do guided by PCS. And every time we have to do something a little different, it's not like a cookie cutter. So there are many, many things. And this framework has been extended by other people, like Ken Jeng and also Peter Caljen. Peter Caljan, a geographer, and also Suzanne Murphy's group to reinforcement learning. So, I'm really happy to see colleagues finding this interesting and work more on this. So, let me give you an ongoing case study of using PCS. So, I've been engaged with National Cancer Institute Early Cancer Detection Network for the last four or five years. And the reason was about before the pandemic, I gave a talk at Hutchinson and Zi Jing Feng, who is the validation. And Zi Ding Feng, who is the validation center director, really liked the framework because he was facing a challenge. The replication rate of early cancer detection methods have to be validated by his group, and the replication rate was very low, less than 10%. So he saw this framework could help, but he had to convince his PI. So I started working with MD Anderson. We have another case study. And recently, before my keynote for the network PI meeting, I said, I want to bring in another case study. To bring in another case study, so he introduced me to Professor Chen Naiyan at Michigan, and we worked with Yuping Zhang most closely. And really, all the hard work was done by Tiffany Tan, who was my former student at Michigan and going to be assistant professor at Notre Dame, and Anna Kenny, who is assistant professor at UCR1. So, for this, they already had the ICAR paper. So, we served a stress test. So, if we re-analyze the data, Stress test. So, if we re-analyze the data with PCS framing, do we reproduce the result? And can we do more? So, prostate cancer, as we know, is a very common disease and happened about 12-5%, similar to breast cancer for women. And about 2-5% people actually die of prostate cancer. And PSA is a commonly used clinical procedure, it's invasive, a lot of false positives. Invasive and a lot of false positives, and there's a lot of over-treatment. So, can we do better? So, this is the overview of Shenany and Group. So, they use gene expressions, 54, and prior outcomes like most severe prostate cancer, grade 2 or higher. And then they also consider clinical variables. They have a validation cohort, which is held in the Hutchinson Center, so they couldn't touch, right? It's not our machine learning. Not our like machine learning validation. This is real different people. And they have this MyProcess score two because they have a one, and they end up with like about 17 genes and really lift the performance from the clinical use by 21% and something else by 15%, their own method earlier by 7%. So this is significant improvement in the external cohort. Hard. So we came in, tried to stress test this method, MP2MP. It's another version, pretty similar. So we look into how they process data, talk to Yuping about there's something called CT. This is a thresholding. They deal with a gene expression. So we try different ones and there's different ways of cleaning pole quality samples. And do you include some variables? And do you include some variables or not? So, all of this we considered. We end up with four, with the time we have four different pipelines. And their different models, they use Elasticnet. And there's no reason why you couldn't use the SU array to random forest. And we have something called random forest plus. So that's all things considered. Because the VCS mindset is really considered multiple plural possibilities which could have happened and we didn't consider. So the stress. So the stress-trade recipes identify human judgment by talking to Yiping, and she said, yeah, I could have done that, and to find reasonable perturbations. And then we performed perturbation. We did four for the pre-processing pipeline. And then we used prediction as the prediction check. And then we did the importance ranking. And then we were able actually to reduce the number of genes from 17 to 5. So this is. So, this is assessing the four different pipelines, like pre-processing pipeline we did, and with different methods labeled on the right. So, RF plus is a new method from my group. I'll tell you a little bit later, but let's just not worry. You can see get a little bit better performance than random forest. And they use LoSECNET. It's pretty up there with RF Plus. And you can see auto-panel logistic regression just a little for all the different functions. For pipelines of processing is a little less, right? You can see a shift. So we use prediction check. We just, at this stage, you're not going to worry about LoJF3L2. But it's interesting, we're going to come back to it after we do the selection through stability. Okay, so they did pretty well, right? We see 1 to 2% change and their improvement, the minimum from their own method, was 7%. So they basic passed with flying colors. Basic paths with flying colors. Unlike my class studies. Oh, sorry, this got a little blocked. So then the three, we look into a little bit the different pipelines, and we end up with four different combinations, right? You have processing methods and inclusive, inclusive. So we end up many complete pipeline for the prediction accuracy. And QGS3, pretty much on top. And they actually start with 54. They actually start with 54 genes, they end up with 17. They couldn't go further. And it's a judgment call. You can see that 17 genes, we basically validated their going for 17 genes because they start with default genes. So we kind of validated not just the pre-processing, but also the final result in terms of prediction. And the random forest plus and MDR plus is very much a PCS guided, but more than that, Guided, but more than that, development. So, we want a more flexible framework bridging random forest and linear models. Right. So, what we did is that we actually, for all the trees in the random forest, there's a nice connection between the trees and the square master if you just turn each node into a feature. And then we just line them up. Each node becomes a feature. Each node becomes a feature, and we also add original features. We have a whole block of features related to a particular gene. So, we take all the nodes that split on a particular gene and take them out and group together. And then this way, we have an importance measure, not just taking into account linear effect or non-linear effect, as MDL does, right? MDI really looking at the non-linear in the trees. So, put them together and use something like generalized cross-validation type of thinking or leave one out. Type of thinking or leave one out, and we get the importance measure. And then for this framework, after you have this block of feature related to a particular gene, you have different choices. You can do LOSU, you can do LOSIS regression. You have many choices. So we have overlay of PCS to aggregate. And that's what MDI Plus and Random Forever Plus is about. So it's really trying to combine the importance of nonlinear and linear impact. Linear impact. So check it out. The paper is on archive. So after we have screened out the L2 panel like logistic regression, we end up with four methods, right? Logistic listed. And then we rank all of them, all the genes by four methods. And we look at which genes are stable or the average rank very high. And then we now look at the top genes. Now, look at the top genes. So then you can see that the different threats, the top genes are pretty stable, the five genes. I have a list of 17 genes, not as stable. So we end up with the five gene panels. And we actually went back L2 regular logist regression. Remember, if you use 17 genes, L2 regular logistry regression doesn't work as well. So we didn't use that for the ranking. But now actually. For the ranking. But now, actually, we discover that just random forest plus get very good results, like 80%. And if you look at the original 17 gene model, it's 81%. Remember, we have pre-processing variability 1 to 2%, right? So this is why we think 80 and 81% are very comparable. Remember, we did the assessment. If you just depending on how you stretch out. Assessment: If you just depend on how you stretch out your data, you have one to two percent variability. So, this is basically within that uncertainty. So, now we have a much simpler method. We stress tested their method. They pass with flying color, and we have actually a simpler method and save a lot in terms of the genes expressions you have to collect. And we retain similar proponents. But this is not validation cohort validated yet. This is through a test set. We are talking to. Test set. We are talking to the Michigan group on Monday, actually, and after that, we're going to go to use the cohort test. And check out our random Forest Plus and MDI Plus paper on iCive. So how can PCS help your projects? Right, so I give you an example. You basically consider multiple, even you don't want to consider multiple data processing. Sometimes you don't have the accessibility for some UK buyback, we don't have. For some UK buyback, we don't have access. But at least at the modeling stage, it's really ready for you to try. You consider multiple methods and use P-Check to screen out the bad ones and you aggregate. We got a lot of mileage out by taking this approach. You can have new methods, you can have new pipelines, you can stress your existing work or other people's existing work. And this gives you more release and certainty quantification. So the book I'll tell you about actually have prediction intervals. Prediction intervals really taking into both pre-processing and also model choice uncertainties. And this, we also have a success stories, we have a paper with cardiologists at Stanford. You get more reliable genes to do experiments on when you add stability on top of predictability. And as a result, we believe you have more reproducible scientific results and domain knowledge. So to help unpack To help unpack the principles, we have actually finished a book with my former student, Rebecca Barton, of Utah University Medical School. And it took us eight years. And there will be a free online copy sometime this spring. I've been saying that for a while, but it's really, we're going through copy editing now. And the hard copy will appear this year. And MIT Press already has a website. Apparently, it's $80. I, you know, we it's out of control. We want to make as slow as possible. Out-to-pop control, we want to make it as slow as possible, but it's $80. And it's in their machine learning series. So, hopefully, this will have a whole chapter on data cleaning. So, really try to bring that into the formal consideration of statistical analysis and data science. So we have some software if you want to go this route to make the PCI-style data analysis easy. It's a Python package called vFlow published in JAWS. That's an open source journal. That's an open source journal. And we also have a template, as I mentioned. We also, on the C side, I haven't talked too much, we really encourage people to do multiple data simulations. Use real data. Usually you have multiple models that fit your data just equally well. Then you simulate according to all of them and look at the coverage and things like that. So we make simulation easy, and we're finishing a paper on the guidelines. So called Sim Shell, that's in R. So, called Sim shell, that's in R. So, hope you can try different models much more easily than code up from bottom up. And the documentation is something you can really co-opt for your problems. It's written for actually the MD Anderson group pretty generically, but you can opt it and you just, especially for beginners, I think very helpful. You have a new student in your group, it's really help streamline. I was just visiting J. Streamline. I was just visiting Genetech and people there thought this is a great idea they can use. Everybody, I think a lot of good research labs do something like that, but this is just systemize it to make it standardized. So to summarize, PCS has multiple roles and expanding. It's not a static framework. It's an evolving framework because it's a philosophy. How do you realize the philosophy? Context really makes a huge difference. So you do internal validity. The result is more trustworthy. The result is more trustworthy. And you can recommend do recommendation for external causal validations. Combined with domain knowledge for the cardiology, after our recommendation, we did a lot of annotated database search. And we had 80% success. We did five sets experiment. We also analyzed the data with them. They redid the experiment with our feedback. So it's a whole pipeline. It's not just modeling. If you want to have the impact, we have to go there and bring things easier for people and work with people for the downstream tasks. With people for the downstream tasks. I showed you a case stress test and also improvement. We have iterative random forest, many, many methods that is adding stability to unstable algorithms. And as I mentioned, right, other people have extended to other fields. So I'd like to thank the EDIN colleagues, PECING FENG, who really started us this path and NSF. And my hope is that some of you will try out. Is that some of you will try out some ideas? We don't try, we don't do everything either, just because the constraints. And as our collaborator from MD Anderson said, at the dawn of AI era, it's extremely valuable to have a uniform language and framework to talk about stress tests in the analysis pipeline. And we're holding a Berkeley-Stanford joint workshop on virtual data science on May 31st. So the website is almost ready to go. If you go to my website, you'll see it and you can register. And you can register to come. And we'll have some spotlights or lightning talk spots for people who register. Thank you very much. That was a lovely talk, and I'm really, I think this framework is really great and really wonderful for helping. For helping improve reliability. My question is: anytime we layer on additional expectations to a data analysis, do you get any kind of, I'm curious, is there any kind of pushback about the extra time or the extra effort that it takes to do this? And maybe that's kind of like taken care of by some of the software you developed? Or what are some of the challenges you face when getting folks to try and use this outside of your own? Well, I think the challenge is Well, I think the challenge is a lot is actually calming the paper pressure. Right. People feel like they're pressured to write papers, and this is definitely make the paper. It takes longer. So my pushback, that's why we try to do the software, to put the more idea into the software. We have a project with Microsoft to put into some of the Excel sheet software. So we will make an effort to make software, make it easy to do. And I think one I think why is people worried about whether they'll slow them down? But I think the tide is changing. I see that people are more open-minded now than, say, five, 10 years ago when I started on this path, especially with the talk of safety AI. I think I feel like the time is more ready. And one question actually people ask often is: what's the incentive? And I kind of, for a long time, And I kind of, for a long time, I would say that, so I could sleep at night. And I took that as my like professional, personal responsibility. Now I'm up the ante. And I'll challenge you to think that now, knowing that there's this other source of uncertainty, before, I mean, I didn't know and say 20 years ago, just, you know, did what we did. But now you heard a talk, and you can check whether you agree with me. Check whether you agree with me, there's uncertainties. But these papers by these hundreds of scientists really make the case for PCS. I think Jos is probably there. Jos sent me that paper. I kind of saw that already. But now there's the ecology paper. So I think it's almost like we imperative now, because we always own, we care about uncertainty. And there's this hundreds and hundreds of scientists basically saying, hey, you know. Basically, saying, hey, you're not doing enough uncertainty. So, this has become a professional responsibility for all of us. And I think it's doing our job. So, I'm up the ante. I think our salary is our incentive because that's what we're supposed to do, right? So, this is really a stronger push than I have ever done. And you guys can tell me it's too strong or not. But, really. Thank you. That was great. Oh, yeah. Yeah, another question here. Yeah, I think a great point. I just longer, uncertainty is important, but in many cases, uncertainty comes from the fact we don't really understand the data where we will. We don't have a tools to really figure out the signal of interest. And I know this is a great framework. I know this is a great framework. How much this could contribute to that level or think about how that could help us with data annotation in what sense? Well, I think that's a great question. I think this is really, it's really like a habit forming to encourage to be critical and worry about stability and give you a few kind of nodes to turn. But then the hope, that's why we wrote the book, it's really become a habit. It's really become a habit, and people will even do more and take this forward. I definitely don't see this as like a done, right? This is a beginning of getting all of us to think more about these issues and in a way that past experience of my lab and a lot of people I know, like people do stability. So, this is really a formalization. A lot of things happening already, but just do it systematically, even in my own group, has been really helpful. So, if you think about So if you think about, you know, so here's a recommendation from the book. You should worry about different ways of thinking about how you clean the data. So that will encourage you to think about, oh, how's the data collected? And then that might help you in context, talk to data domain expert to uncover the uncertainties we didn't know. So it's really like, here are a few guidelines. How can we be better detected? How can we be better detective? Right, I think applied statistics are good detectives, and collectively, we need to work with others. So, I think will help us uncover a known source of uncertainty too. I hope you agree or you don't agree. That aspect, because at the end of the day, when you think about any major challenges in science or anything. Challenges in science or any specific human beings. The most important thing is you have to go back. And certainly there is one aspect, but it's not the most important aspect for solving the big problem in science. I think that this is a basic requirement that this kind of procedure would make the science better in terms of the other aspects. But if you think about in many cases, when we have the data. When we have the data, why do we really develop all kinds of different pipelines to process the data? Because all these existing pipelines have some issues. It's not good enough. Well, my angles, I'm not a cursing your approach. I just in general, and when I think about the problems related to data science, that's one aspect. I think I agree. What I do is great. That's another aspect. Well, I mean, this doesn't remove. Well, I mean, this doesn't remove human judgment call. So I think it's like you still have to access your judgment call, but this pass down or share some of the experiences that worked for us and for many others. Hopefully then the startup cost for younger people will be shorter, right? So, you know, all good labs have lab books, right? Old-fashioned paper and pencil, but this is kind of a grand kind of This is kind of a grand kind of, it's kind of a computerized lab book in a way that you should record this, you should worry about this, you should perturb this. And part of a dry lab, if I just do this perturbation, what will happen? And would that concern you? And it's constrained by the human resource. So, for example, I mean, this mindset, even for the traditional experiment, with you and Ashley's collaboration, right? So they have done these experiments and they already had the pipeline. And they already had a pipeline, and we asked to look at through their pipelines. And there, we just look at a lot of videos, we visit their lab. So that's all kind of, and then you just discover things that there were some of these huge things, like cells that they didn't know there were these huge cells. So it's not that different from the old-fashioned going to the factory and look at the floor and do quality control. In my mind, I highlighted the same thing. It's just like now. I highly level the same thing. It's just like now the factory is our own home, it's our own lab. Yeah, so for me, it's really common sense by now. It was not common sense to me when I started on this journey. It was not at all, because it was different from how I was educated. And I taught it different versions of this. Sometimes I just flip back to the original framework, which we're used to, because that's Framework, which we're used to, because that's how I was trained. And it took a while actually to streamline all the thinking in the book. It was now in hindsight, it seems pretty simple after 10 years, but it was not when I started on this path. Well, okay, I think let's thank our speaker. Thank you for having me. Enjoy your bath. 