Okay, thank you for inviting me to here. I'm really novice in the deep learning field. And today I'm just going to show a very simple story from a student of mine, Uyghur, and that she pretty much had been learning alongside her to see how things unfold. So this is, I said, it's a very simple story about how to use deep learning to help us to uncover these so-called genetic gene expression programs from the single cell. Programs from the single-cell data set. So, this is all of my presentation. So, it's talk about what archetypal analysis is. It's a pretty straightforward concept why we want to use this deep learning thing to the single-cell RNAC data set to uncover or categorize this archetypal analysis to do this. Analysis to do this. I discussed the linear, non-linear options, and I'll just introduce this very again a very simple auto-encoder-based non-linear archetypical analysis. Basically, I'll say, give some introduction about the modifications, extensions we did, and show some simulation results, and I'll give you one real data application. In our published paper, there's a couple other examples that you can really be more. On the first, Really be more. On the first, so what is ARCI type of analysis? Some of you may know, but some may have, may have not heard. It's just very simple, like the supervised learning, similar to clusters, but they seeks to some extreme points. It's called archetypes in the multi-dimensional data. And so a given data set or think about a given single cell is assumed to be like the superposition. Assumed to be like the superposition of various archetypes where you think about the convex combination. So each data point, single cell in our case, is represented as a convex combination of archetypes. And it's valid to apply a different field. So these are just like very simple illustration that in this particular case, there are like three different extremes, and you can really categorize someone's facial expression by sort of doing some linear. Sort of doing some linear combination or combination of different on different kinds of things. So it's that is how you define it's more often about the colors, et cetera, it's more along that line. So in the case of single-cell data analysis, these archetypes, they represent nothing but sort of, we think there may be multiple gene expression programs. And typically, when we do cluster analysis, for example, I'll just talk about the difference. I'll talk about the different cell types. They are distinct clusters. But also, I think about like Jing Shu mentioned, other people talk about the trajectories. It's a continuum, right? You just think about like we anchor all the single cells according to these different archetypes. And as each cell, you can think of as like a complex or combination of these different things. And that may give you a little bit more flexibility to categorize. To categorize or to define the single cells you observed. Especially for immune cells and also the things under development, and it's very hard to pinpoint a particular cell to a given cell type. And also, it gave us some maybe a better opportunity to characterize the underlying heterogeneity and also allow to find genes that are associated with the program, not necessarily the distinct cell types. So, this is just like a single cell, you can immune. So, it's a continuum. It's hard to say not continuum, but this is very hard to pin down to a particular cluster. And this is the original paper where this archetypal analysis combined with the deep learning framework was proposed by Smita IEL. And so, this is one application, but also they have many other applications in our paper. But it just will show the single cells. But it just will show the single cells. These are the six archetypes they identified. And you can really look at different archetypes and identify the markers associated. So that's how they are anchored in this figure visualization. So if you back up a little bit, I'm not talking about the deep learning framework, but saying let's look at linear archetype. Think about it more like matrix decomposition, right? Matrix on decomposition, right? So, like non-negative matrix factorization, you have X, you have A and Z. And so, you can think about these Z as archetypes. You think about all these things and A as sort of the combinations there, but here they are constrained to sum up to one because they're linear combinations of different archetypes. But also, you can think of also each archetype also is a combination of the different data points. So, that represents some kind of extremes on Suzanne. And some kind of extremes on to some extent. So the interpretation X can be factorized into A times Z. So Z is archetypes, A is wrong to think about the loadings on these archetypes. But also the archetype also become important and also is a linear combination of the data points. So there's two size constraints for this one and this one as well. And so in our case, the archetype corresponds to so-called gene expression programs. Gene expression programs, and A is the cell usage of different gene expression programs. So, that's all very clear, but the reason we go beyond linear, we try some single-cell data set, we use some linear contrast linear decommission versus non-linear ones through the deep learning. And we did see some benefit, that's why we pushed it a bit more. So, it may not always be possible to really identify this meaningful architecture. Identifies meaningful architects in the given feature space. So we want to go beyond linear to non-linear case. And there has been some examples on that kernelization, but the specified kernelization also has implicit assumptions about the transformation that may not really correspond to give the best categorization data set. Therefore, this is a maybe potential application area that the neural net can be really applied. That can be really applied to find some like undiscovered or meaningful patterns there. So, this is the many times that the auto-encoder. So, you have the initial data set and use the encoder on this latent space. And so the decoder, we get it back. So, this is the loss between the initial data and also the encoded, decoded data set here. So, this non-linear version, but like, oh no, it's non-linear version PCA. And the autoencoder, also again, you'll see that has been utilized to perform non-linear archetype analysis on the images. This is the initial paper there, David Van Dak, also the faculty now at Yield, and also the Deep A. But also has the overall auto-encoder also has gains general success for single cell. It's an ICBI vibrational imprint and the DCA. So a lot of people in the audience must be familiar with all these applications. Must be familiar with all these applications. So, this is a setup for the initial paper on this topic about three years ago. So, this through the code AAMAT. And this represented the initial data output, the input and output. So, these represent the four like the anchor point archetypes. And so this, through this, the whole processing, every single data point here may be represented as a convex or a combination of these different. Or a combination of these different archetypes. So that's basically the structure for this. So there are other, I mean, I mentioned there are other, like the neural nets apply to single cell. This is a SVAI. In the original AA night, so the loss function they use is just mean square error. So this doesn't really respect the nature of the count data nature of the single cell data set. So I will show you later on that you use such a standard like the mean square. You use as a standard, like the mean square error. I mean, that's not as good as we take into account the current nature of the data. So, this is the SCVI where you can remodel this, the observed count data as like a negative binomial or zero-inflated negative binomial distribution there. So, you can all learn is kind of the dispersion parameters and drop-out proportion as also part of how the neural net thing. And also, there's a follow-up on trying to give a little bit more. Up on trying to give a little bit more interpretation of this one. So instead of using the decoder here, we put a linear, linear form so that this is to be more interpretable, like say, since I just factorize this thing into this latent space, but also is with here. So that's the L D V A E. So we'll see some results later on. Okay, so that's basically an overview of the background that where we are trying to push a little bit more. I'm trying to push a little bit more. And I think there are some limitations. The current ones, the first one is for the current AA night on the approach there, this part is somehow respected. So each data point is sort of the complex combination of the latent or archetypes here. But this one is not really constrained, saying that each archetype may not be a complex combination. Not be a convex combination of the data points. So this is just a data point example here. Ideally, you really want all the archetypes to be at the extreme of the data space. But on the other hand, a lot of times, I'll show some examples like actually what is found in the archetypes, they are pretty far. So this may or may not be an attractive appealing feature of the algorithm. And so we make the archetype tight and close to the data set. And close to the data set, so we add a penalty in the overall objective function. We try to incorporate this idea. And so, the other one is we really model this as negative, zero-inflated negative binomial distribution for the single-cell data set. So, this is a part trying to really make sure that the archetype is not too far away from the data space. And this is why I try and try to say, oh, this is the count data, so we better use nightly binomial or zero-infinitely binomial. or zero infinite activity binomial to model data set. So these are the two major two major improvements that we made on to and also we add some like SCVI we have some kind of decoder for the dropout dropout proportion dropout parameter as well as the dispersion parameter from the data set. So this is overall so this is the overall architect of the this the ICA net approach. So we have a raw data counting So we have the raw data count count data going in the encoder, and this is the A. Think about this more the loadings onto the architect, and this is a B, so transforming back to this original space. And then we have the library size, and we all know it's important for single-cell data set, coupled with the dropout, and as well as the dispersion parameter, and also the mean nerd that's gave us some way of really handling. So, this is the RQ type of loss, trying to see that the Z is not going to be. The Z is not going to be too far from the data set. And this is the second reconstruction loss using the negative log likelihood. This is based on the zero-infinite binomial, but also in our program. And I will show later on that we have four different options depending on your choice. As you see, the zero and infinite binomial is pretty general, and the natural binomial is maybe pretty good for the next data set. Data set. This is a zero-inflated Poisson, and also we have the Poisson option in our package. And so there's two main outputs from ICAN on our algorithm. One is the GEP matrix that corresponds to the gene expression program. That's like think about three or four or five. You think about thinking of the major program, you can think maybe the cell types, et cetera, but it's more continuum. A cell actually maybe a combination of different programs. Combination of different programs and the cell usage matrix there A. And think about if I have very distinct the clusters, this usage A will just be a vector of zero or one. That's only one for this one. So to see how it works, we first did some simulations and also compare the results by applying different algorithms to the simulator sets. And so we simulate from the splatter on this single cell simulator, and we consider about 3,000 cells. And we consider about 3,000 cells and about 2,000 genes. We consider archetype, about four archetypes. And across different archetypes, we say there are different genes that we define a different archetype, about 10% of genes showing different expression levels across ones. And also we consider different degrees of differentiation, like 0.5, 0.751, just in terms of the expression, the mean differential ratio using a lot of normal transformation. The using law normal transformation is a scale parameter one and the location parameter 0.5, 0.751. And so for each level, we generate an assets using random season to see how the algorithm works. And also we consider like two different data distributions, either the zero inflated 90 binomial or 90 binomial. So see the results. So this is the Hobby similar data set. Either we use a 90 binomial or zero inflated 90 binomial. or zero inflated 90 binomial. And so this just gives you the mean differentiation for different changes and also the proportion of zeros, just depending on this give you an idea the dropout. So lambda 0.01 is almost no dropout. And so this is another other thing. So let's just look at the visualization, the archetype. So if we apply this one to different, like with different algorithms that we consider to use an archetype analysis. So this is the Analysis. So, this is the ICA net. So, that's how you visualize using this multi-dimensional scaling from the output. And you see that this is capturing the sort of nature of data. It's four different GEPs, and each data point is a combination of four different programs there. And you can see the problem for ANAT without a constraint that each archetype is sort of a combination. combination of the data points, architect can be very, very far away from this. This data is very concentrated here. Architect can be quite far away. And the ICVI is okay, but this little bit distance and the LDV and VLDC impose some linear structure there. I think that the pattern is a little bit less clear. And then also we look at how well, I mean, the different algorithms are able to recover first the First, the usage of different programs, and also how well that we can really infer the things different between different programs. And also, we look at the reconstruction error, looking at the different rates between how well we configure the data set. So this is look at the negative binomial distribution. This is look at zero-inflated negative binomial. Let's just focus on the second row where we consider a zero-inflated negative binomial distribution. By normal distribution. For the cell usage, so there's a six different algorithms we consider there. One is the ICA net using the mean square error. This is not using the 90 binomial loss, but more like the Gaussian thing. And you can see this doing very, very badly as compared to the zero inflated 90 binomial. You can see significant improvement by modeling the data distribution more properly. Yeah, so this card from the AA night. So this AA night somehow is. So, this ANIT somehow is doing a better job than this night using MICE, but not as good as the zero-infinitive binomial. These three different panels correspond to the different levels of the signals, right? The larger this number is the more distinct these different programs are. As you go from left to right, you'll see a better, better recovery of the cell usage. And also, in terms of this is a Pearson correlation, look at it for each gene expression program. at the for each gene expression program right so we can we have you know the ground truth and also we can see how well we can recover that uh from from from from different programs and as you can see there using the ica init actually we better recover uh the the back can really better recover the gene inspection program despite the usage is a pretty big uh sort of the difference whereas i see uh the a net in terms of the the the the loading cell usage is doing better job but Cell usage is doing a better job, but in terms of reinforcing the underlying gene expression program, it's not doing as good a job. And also, based on this usage, and also we can identify genes, right, and to show that there's a difference across different programs. Because we don't need to discretize each single cell into one of the four or five different cell types or gene programs. Actually, we are able to return a little bit more information about genes, which genes they have different expression lines. Which genes they have different expression levels because every single cell is a combination of different programs instead of you have to assign one cell to one or the other cell types. Again, so we look at this similar setup until you can see how well we can really recover identified genes showing different expression levels across different programs. Again, so this looks at the naughty binomial, the zero-inflated 95 binomial with a different drop-out rate, right? So this is a 0, 1, this is a one. 0, 1, this is a 1, this is a pretty significant dropout. And you can see how well we can recover those, right? So this corresponds to the case where we just use the usage as the predictor. And this one user group, we have a higher thresholding. As you can see, in this case, because we respect a continuum of the gene expression program across different cells, we're able to better identify genes showing differential expression levels. So lastly, I just give you one of the three examples. Lastly, I'll just give you one of the three examples we show in our paper to look at microglia. So, this is based on data set. I think this is a part of a well-studied data set from Methods, like about three years ago, looking at the single-cell expression level for 48, prefrontal from 48, the patients, individuals, 24 is AD with AD pathology or 24 without pathology. In total, total, 48, also their sex balanced. Prefrontal cortex. And prefrontal cortex regions. So, in our paper, it's a very, very comprehensive paper, and we all know the importance of the microglia in the Alzheimer's disease on-site etiology. So, we just focus on the microglia on this particular cell type. So, for microglia cell type, in the paper, the authors identify four different microglia, so-called subtypes or clusters. So, we just focus on the 1900 microglia cell. 1900 microglia cells in the analysis. And there's about among all the genes, he focuses on the top 2,000 highly variable genes. And in the paper, we describe how to really pick up four. So basically, it's a basic idea is very straightforward. Look at stability, right? You have, you try to push up on the archetype, number of archetype, until the results are not very stable. And then that probably point where you stop. So in this case, we actually came up with the same number. We actually came up the same number four as the clusters they identified in the paper. So that's what's reported in the paper. And that's what from our archetypal analysis. So there's four archetypes there. And this really just shows you this is the microglia 0, 1, 2, 3 in their paper. And this corresponds to the gene expression program 1, 2, 3, 4. That's how we visualize our results, right? So this corresponds to the gene G. to the the the the gene gep1 where you see the cells on with a brighter color that's where the the the the gep1 is is very active and this is where gep2 is active and this is gep3 is this part and gep4 uh so you can just see um for microglia cells is again it's not very continuous it's a distinct cluster but rather a continuum um and and also um based on based on based on this we can look at on what genes are We can look at what genes are differentially expressed between the GEP2. So, this is GEP2. GEP2 really corresponds to this particular cluster, which is the microglia cluster 3. So, we found a lot of genes, the genes are enriched for a number of categories, right? This regulation for the phosphorus metabolic process, regulation protein, etc. So, this based on the GEP2 and GEP4, and for the most part. And GEP4, and for the most part, they agree well with what's reported. So, to summarize, I think we just introduced this archetypal analysis as something a little bit different from cluster analysis. But when you see very strong clusters, they pretty much give the same thing. But when you see continuum, it's a little bit more generous in just allowing individual cells to be superposition of different. To be a superposition of different programs. And also, you can think about this: it's another way, or more like non-linear way of doing this, the non-native matrix factorization. Just think about that way, you have the archetypes, you have the loadings or the usage of different archetypes. And so in our main contributions, really, we added a constraint so that the archetypes are not too far away from the data points. So we try to reinforce that. And also we introduced. And also, we introduce this reconstruction loss function that's very specific to the common nature of the single-cell data set. And also, we allow different decoders there just to really allow us to estimate the drop-out rate and the dispersion parameters. It's all part of the whole algorithm. And also, based on output, I show that we can really identify more genes showing differential expression levels across different products. Differential expression levels across different programs than just doing a straight cutoff, assign each cell to one of the many different cell types. Just in terms of future research, so as we've seen and we'll see, there's many other data types. This may be helpful just to go beyond the single CRNA-seq to other data sets like the A-TAC-SIG or also multi-omic data set. And also, I said, And also, I said for this particular algorithm that we have there, if you have a major, major different clusters, it may not be a good idea to apply to all these big clusters, but just focus on like a small cluster. You may find some refined structure there. This may allow to uncover some subtle differences or that may be missed by this like the bullforce for cluster analysis. And also, we can allow us to read among the different regulatory layers within the same GEP that gives some kind of. The same GEP that gives some kind of thing to start to think about. And so, this is the paper published in PLOS Commissional Biology, and this is a GitHub repository where we can get the program. Okay, I'll stop here. Thank you.