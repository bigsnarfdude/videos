Mickey Rat. I'll start again. Yep, first up, we have Mickey Ratz talking about correlated stochastic block models. Thanks, Alex, and thanks to other organizers for organizing this wonderful workshop. It's really been fantastic so far. So today I'll tell you about correlated stochastic block models and in particular two natural statistical inference problems. The one is community recovery and the other is graphic. Is community recovery and the other is graph matching? And I'll explain how these are kind of come together in this problem. And this is based on joint works with Julia as well as my PhD student, Anish Schruder. So yeah, let's get right into it. So Julia already gave a great overview of the stochastic block model and motivation for recovering communities in networks. So I really won't get into this, but I think in this crowd, I don't really have to. So let me just start with the stochastic block model. Let me just start with the stochastic block model, and I think all of us are familiar with the stochastic block model, especially from Julia's talk yesterday. But let me just clarify some notation I'll be using. So yeah, so the stochastic block model originates from the work of Holland, Laskin, and Leinhardt from about 40 years ago. And then there's been a ton of work on this, especially over the past decade, including by many of you here in the audience. So that I absolutely cannot go over all of that literature. That literature. So the goal in this problem is: we're given a graph where there are some latent communities, but we only get to observe the graph without the community labels, and we want to recover the communities in a certain sense, either exactly or almost exactly or partially, or just have something that is better than a random guess. And so in this talk, I'll just focus on the most basic version, which is you have two balanced communities. Okay, so I will have end nodes. Okay, so I will have n nodes. Each node i has a random community label, sigma i, which is either plus one or minus one, iid, uniformly at random. And then given the community labels, the edges are drawn independently. So if two nodes i and j have the same community label, so they're in the same community, then there's an edge between them with probability p. And if they're in different communities, then there's an edge between them with probability q. So this is the stochastic block model with parameters n, p, and q. Okay. Q. Okay, so we're all very aware of this model and all the work that has gone into this. And today I want to talk about a different kind of setting where we observe not just a single network, but multiple correlated networks. So, you know, this occurs in all kinds of settings. For instance, in social networks, and there are various social networks such as Facebook, which are primarily about personal connections, and others, such as LinkedIn, are more about professional connections. As LinkedIn are more about professional connections, and because our personal and professional lives are intertwined, there's a lot of overlap in these two networks. But on the other hand, there's also complementarities. So one natural thing you might want to do is ask, well, if you have access to both of these networks, can you synthesize the information present in them in order to obtain better statistical inference procedures and algorithms? And in particular, for the task of recovering. And for the task of recovering communities in a better way. Okay. And just as a quick aside, so this perspective was actually already present in the pioneering work of Han, Lasky, and Leinhardt from 40 years ago. So when they define the stochastic block model and introduce it, they talk about M relations between the nodes and they talk about a stochastic multi-graph. So really, this is already present in their perspective. But of course, since then, the bulk of the literature has focused on just the Of the literature has focused on just the single relation graph setting because we should first understand the simplest settings first. But you know, now this has been really very well understood. So I think it's worth studying this setting here as well, especially since there's more and more data available in real life. Okay, so let me tell you about a simple model, a simple probabilistic generative model for two correlated networks where both have some underlying communities. Okay? Communities. Okay. So we started with a stochastic block model, G, and this we can think of as a parent graph. And then what we do is we subsample it. So we subsample all the edges independently, keeping them with probability S and throwing them away otherwise. And non-edges here in the parent graph remain non-edges in the child graph G1. Okay, so this is a graph G1. And then we subsample independently a second time. We subsample independently a second time to get a second graph G2 prime. Okay, so now we have two graphs. And note that both of these graphs inherit the node labels from the parent graph as well as the community labels. So the color is red and blue. Now we're going to do one more thing here, which is that we're going to take a uniformly random permutation pi star and randomly permute the nodes in the second graph to get a graph G2. Okay. And this is. Okay, and this is because to model the fact that we don't necessarily know how these two graphs are aligned. So, this could be for many reasons. Maybe the node labels are anonymized, or maybe they're missing, or maybe you don't trust the node labels, or maybe even if you know the node labels, they could be different in the two graphs because they just correspond to different features of the same individual. So I might use my nickname in a personal social network, but my full name in a professional. Network, but my full name in a professional social network. Okay, so if G1 and G2 are generated in this way, then I'll call them a correlated stochastic block model. And there are three parameters here, N, P, Q, and S. So N, P, and Q are just the same as before. And S is this subsampling probability, which measures the correlation between the two graphs. So note that G1 and G2 are marginally. They're both stochastic block models. They're both stochastic block models. Now, the edge probabilities are now p times s and q times s, just because of this subsampling with probability s. But the corresponding edges are correlated. That's because they're both subsampled from this parent graph. Are there any questions about the model? Yes. So just to get it straight, if you didn't re-permute these labels, then it would sort of just be like you could put them together. Exactly. You could put them together and have a double edge properly. Yeah, well, yeah, not almost double. Yeah, exactly. So, yeah, we'll exactly get to that. And so, yeah, in fact, if you look at this original paper by Holland Lasky and Leinhardt, they consider this setting with two correlated block models, but without the randomly permuting the node labels. And they call that a pair-dependent stochastic block model. And they actually study that at length in this paper, the original paper. This paper, the original paper. Okay, so this is the model I want to be studying. And so then the natural question is just as before, given the data, which now is two graphs, G1 and G2, which are correlated stochastic block models, but without the community labels, can we recover the community labels in some sense? And in this talk, I'll be focusing on exact community recovery. And in particular, you know, Okay, and in particular, you know, the interesting question is: can you do more than if you had just a single graph? Okay, and exact means, I guess, with both graphs, you have to recover the recommended. Yeah, well, okay, let's suppose that maybe you care about, you know, one of the graphs, say the first graph, and you're using, you can think of it as using the second graph as kind of auxiliary information. Okay. Okay. Any questions? And yeah, so again, this exact recovery means that up to the flip of the color, so really what you want to do is recover this partition between the two communities. Okay, so let's just recall what we know about exact community recovery in the case of a single stochastic block model. So in that case, you can't have isolated vertices. You have to be in the logarithmic average degree regime. Average degree regime. So, in what follows, I'll just have P be A times log n over n and Q to be B times log n over n. So, these are the within community edge probability and the between communities edge probabilities. And so we know from the work of Abiye Bandera Hall and Mohsan Sai, they characterized what is the information theoretic threshold for exact community recovery. And it's given by this formula. Don't worry too much about the exact formula. But yeah, and then. But yeah, and there's a polynomial time, even quasi-linear time algorithm that works all the way down to the threshold. And so don't worry too much about the formula, but let me just give a bit of an intuition, what's behind this. So Abbeys and Sanden, they then drive this threshold for general stochastic block models. And the intuition is really given by a genie-aided estimator. So suppose you knew the color, the community labels of all nodes except one, this node V. One, this node V here in the middle, then what you know, what problem would you have left? Well, if you look at the number of edges of number of neighbors of this node in the different communities, then this behaves like a multivariate Poisson distribution where the average degree is like log n, where there are different constants in front of log n for the different communities. And the different constants of this multivariate process distributions depend on what community label this node B has. So it essentially boils down to testing multivariate Poisson distributions for which you can really characterize the error probability, which is given by what's now known as the Chernoff-Hellinger divergence. And so you want this error probability to be rough. You want this error probability to be roughly one over n because you have essentially you have n of these problems to solve if you want to recover the community label of each node. And so by setting this channel-Palinger divergence equal to one, that's this information theoretic threshold. And in this simple click case of two symmetric communities, two balanced communities, you get this formula here. Okay. Any questions? Okay. Okay, so just from the fact that marginally the first graph is a stochastic block model, we know exactly when exact community recovery is possible from just G1. And that's given by this formula here. So now, you know, suppose, how could we use the information from the two graphs? Suppose we actually knew how to align the two graphs, we knew this latent permutation, then exactly what you just said, Will, we could just overlay the two graphs. Overlay the two graphs. And what we get is a picture like this: where there are the common edges in green, then there are edges in G1, but not in G2. Those are the yellow. There's edges that are in G2, but not in G1. Those are the pink. And altogether, we have the union, the correctly matched union graph is also a stochastic block model. And here, you know, there's this factor one minus, one minus s squared. That's just the probability that an edge survives in either of the two. An edge survives in either of the two subsampling procedures. Okay, so if we knew this latent permutation, then we know exactly when exact community recovery is possible or not. And it's under this condition. And in particular, this is a weaker condition than this one above. Okay. So what this means is that if we knew this latent permutation and the parameters were in this intermediate regime, Where in this intermediate regime, then exact community recovery is possible from the two graphs combined, but not possible from just a single graph. Any questions? Okay, so then this naturally leads to the question of recovering this latent permutation in pi star, which we assume to be unknown, from the two graphs. And this is a problem that's known as graph matching, and this has been very well studied. And this has been very well studied. So, here I motivated this from this question of community recovery, but really, this is a fundamental problem that has been very well studied independently of this. And in particular, over the past decade, there's been lots of work on trying to understand the graph matching problem on correlated Erdogani random graphs, which is a model that was introduced by Petarsani and Grossblauser in 2011. So, this is exactly the same model, but with no community. So, if P is equal to Q. So, if P is equal to Q. And there's really been a ton of work on this, especially in the past five or so years, and by also many people here, especially you can see Jia Ming and Yi Hong in a bunch of places here, and Zhao with Zhao, with Cheng Mao, who will speak later, and others as well. Okay, and yeah, I think Jiangming will also speak about this on Thursday. Speak about this on Thursday. Okay, so there's really been a ton of work on this, and so this is a natural question here as well. Okay, so there's two main questions here. The first one was community recovery, and the second one is graph matching, and that's what I'll be talking about in particular in this correlated stochastic block model setting. So, in what follows, I'll first fully answer the second question, which then gives a partial answer to the first question. Partial answer to the first question, and then I'll fully answer the first question, at least information theoretically. Okay. Now, I do want to mention that there's really much more related work, you know, just in general in this space. And in particular, there's two natural types, two natural lines of work that are kind of bigger lines of work that are related to this. One is considering a setting where you have multi-layer networks or know multiple multi-layer networks or multiple networks in some sense and so there's a long literature along these lines in a lot of these you know you don't have this unknown alignment problem so that's what's kind of really a crux of our work and is not present in this literature and there's really a different line of work that considers a setting where you have a single network but then you also have some additional covariates Have some additional covariate information, covariate information. So these are generally known as contextual block models, and there's been lots of very exciting work in this space as well. And I believe tomorrow we'll talk about the graph matching problem where you have additional features. So it's also related to this general line of work. And there's also been a few works actually which kind of combine these two strands of literature. Transit literature. Okay, but let me continue with what I want to talk about. Are there any questions? Okay. So the first result I want to tell you about is characterizing when you can recover this latent graph matching exactly, this latent permutation pi star. So this is joint work with Annie from last year. So the claim is as follows. So we're is as follows. So we consider this estimator pi hat, which is as follows. So for any latent permutation pi, you can look at, you know, if you align the two graphs according to this permutation, you can look at the number of overlapping edges. Okay, that's what's known as alignment score sometimes. And you can look at the permutation that maximizes this. Okay, so this is a natural estimator to consider that in that natural estimator. Consider that in that natural estimator, that people consider it even in all kinds of settings where you don't necessarily have a generative model. And so, what we show is that if s squared times a plus b over 2 is greater than 1, I'll comment on this in a second, then this estimator is correct with high probability in the limit as and goes to. Okay, so this estimator is a very natural estimator. It's also the Very natural estimator. It's also the map estimator in the correlated Erdogani model. It's not quite the map estimator in the correlated stochastic block model just because of the different probabilities, but you know, it's almost like that. And in fact, it works all the way down to the information theoretic limit. So this is the, as I'll say in a second, this is the information theoretic limit for recovering the slatent permutation. And so in the Erdoshini case, this was first shown by Pulina and Kiev. Was first shown by Pulina and Kirash, and then also Jaming, Yihong, and Sophie Yu, they have some results on in the dense regime, where actually there are some further subtleties when you want to understand this question. So what is this condition? So this is simply the condition that the intersection graph is connected with high probability. So let me explain that. So if you take the two graphs and you correctly align them, and then take just the edges that are present. Just the edges that are present in both. So that's what's called as the intersection graph. And this condition is simply that, remember that we're in the logarithmic average degree regime. So this is simply just the connectivity threshold for this intersection graph. So that's this condition. And previously, so Onar and Gargan and Erkip, they studied the correlated block model previously. So they derived the same. So, they derive the same conclusion under some stronger parameter assumptions. And also, they assume that the community labels are known, which is an issue for two reasons. One is really, if you want to use then this recovering the latent permutation for the downstream recovery of community labels, then you can't really use the community labels to begin with. And the other thing is that if you assume that the community labels are known, then really the problem. Community labels are known, then really the problem reduces to an edit training setting, which is uh simpler. So, and that's already known. Yeah, can I ask a question? So, here the condition is the same as if this stochastic model, like the same as the other chains. Exactly. Yeah, that's it. But can you comment a little bit in the analysis? Yeah, yeah, I will. I will. Yeah, so exactly. So, this is the connectivity threshold. And so, there's a matching converse. So, if you're below the connectivity threshold, So, if you're below the connectivity threshold, then in prior work by Pulina, Singal, Tiavash, and Mittal. So, they showed that no estimator will succeed exactly. And the basic idea is that this intersection graph will be disconnected. And in particular, there will be many isolated vertices. And what is an isolated vertex in the intersection graph? It means that, well, it has some neighbors in one of the graph, and it has some neighbors in the other graph, but these are different. graph, but these are different. They're disjoint. So there's somehow no common information to understand where these nodes should be, where these nodes should be mapped to in particular. You could switch two and six here or nine and 11 here and you'd get a different permutation with the same Paseri probability. Yes, so this is what I said. Okay. Okay. Okay, so I'll explain a bit about the proof ideas for the positive direction, but let me just explain how this then applies to exact community recovery. So under these two conditions, exact community recovery is possible. And the proof is exactly what I already told you. Under this first condition, that's the condition for when you can recover. That's the condition for when you can recover the latent permutation. And once you've done that, you can just overlay the two graphs and use a community recovery algorithm as a black box on the union of the match practice. And then as a converse, if you're below, if you don't have this condition for community recovery, then exact community recovery is impossible, even if you have the two minutes. Okay? So, and here, you know, even if you could. And here, even if you could knew the permutation and you overlaid the two graphs, you can't do it. Okay, so let me actually show these on a plot. So let's look at this picture on the left here. So here, there's three parameters. A is really the parameter for the within community edge probability. B is the between communities, edge probability, scaled appropriately. Probability scaled appropriately, and then there's a sub-sample sub-sampling probability s, which let's set as say a quarter here. And then there are four regions here. So in the green region, that's easy. You can just do it with a single graph, and that's well understood. In the red region, even if you have both of the graphs, and even if you knew the latent permutation, you can't exactly recover the commutance. Now, there's this line here. Here, this is this s squared times a plus b over two is equal to one. And above here, you can recover the latent permutation pi star. Okay, so in particular in this cyan region, you can recover pi star and then overlay the two graphs and recover the communities exactly, even though from a single graph, you wouldn't be able to do so. And then in this yellow region, And in this yellow region, there you can't recover from a single graph, and you also can't exactly recover the latent permutation. But in principle, you could still recover the communities exactly. And I'll talk about that kind of in the second half of the talk. Any questions? Is it possible to think about the scaling window for when that estimator gives a good thing? Because there's like a very precise scaling window. Precise scaling when the right, right, right, right, right, right. I mean, I think it would be the same. Um, I mean, we just didn't try to look so so, meaning like when there's a constant number of isolated vertices, the total variation distance converges to a constant? And that'd be so, so yeah, I'm not sure totally about the argument, but so if you were like log n plus something diverging over n, then you know, then you, then this problem. Then this probability goes to one and then of recovery. And then if you're this log n minus something diverging over n, then this probability will go to zero. So yeah, in between this probability of recovery should go. Yeah. Okay. Yeah. I should I'd have to think a bit more about it, but we yeah, I didn't try to focus on that. Any other questions? Any other questions? Okay. So, yeah, there's three parameters, so you can visualize this in slightly different ways. Let me not focus on that. So, let me just tell you a few words about this graph matching proof and at a high level. So, this is the estimator we want to analyze. So, A and B are the two adjacency matrices of the two graphs, G1 and G2. And then, what we want to analyze. And then, what we want to analyze is this estimator that maximizes the alignment score between the two adjacency matrices. And so, you can see here that, you know, what we care about is permutations on the vertices, but really in this estimator, what comes up is really permutations on vertex pairs. So there's really a one-to-one correspondence between permutations on nodes and lifted permutations. Nodes and lifted permutations on node pairs. Okay, so then if we want to understand this quantity, this alignment score, and let's look at some lifted permutation tau. So I'll be denoting these lifted permutations by tau instead of pi for the corresponding permutation. And so if we look at this alignment score, the alignment score of the true The alignment score of the true alignment minus some the alignment score of some other alignment tau. Let's call this x tau. So this is a quantity that if this is positive for every tau that is not the true alignment, then the estimator succeeds. So really what you want to show is that this quantity is non-positive for all tau that is not the true alignment. That is not the true alignment. Okay. And, you know, if you do just a simple kind of calculation, you know, in expectation, this all works out very well. But what it boils down to is some kind of large deviation calculation. So let me say a few words about this. So first of all, yeah, so at a high level, what's going on is there are some permutations that are close to the true permutation. Those have a bigger probability of having a higher. Probability of having a higher alignment score, but there's only a few of them. And then there's many permutations that are sort of far from the true underlying permutation, but then there's a much lower probability for them to actually have a higher alignment score. So these are kind of the two competing factors. And so suppose that there's you have a permutation that mismatches K1 nodes in one of the communities and K2 nodes in the other community. So then So then you have to understand how such a permutation then behaves as a lifted permutation on node pairs, and how many node pair violations you have compared to the true lifted permutation. So that's something that is relatively easy to understand. So I won't go into the details here, but this is a relatively easy, you know, combinatorial argument. And then what things boil down to is this kind of What things boil down to is this kind of bound, where here. So let me parse this. So this indicator is just that the communities are approximately balanced. So this just holds so you can forget about it. And then you can do the analysis for by fixing both the latent true permutation and also the latent communities. And then this is just the probability that the estimator actually makes k1 and k2 mistakes in the two communities. And the claim is. Communities and the claim is that this the probability of this is at most n to the minus delta times k1 plus k2 okay and once you have this you know you can just sum overall k1 k2 such that their sum is greater than one and you get the result so you know on the one hand you have this um union bound factor which is there's you know n to the k1 plus k2 You know, and to the k1 plus k2 permutations that make k1 and k2 errors in the two communities. And then suppose you were asking, like, what is the probability that this estimator actually is a particular permutation that makes k1 and k2 mistakes in the two communities? And so, okay, let me not go into this in detail, but effectively it boils down to a kind of large deviation calculation. So you have to. Large deviation calculation, so you have to understand some generating function of this quantity of the alignment score compared to the true alignment score. And okay, maybe I won't go into the details, but so this is what you have to do. And let me just explain at a high level how this differs from the Eider-Sch√∂ni case, which is so one thing that is So, one thing that is very useful is that if you look at some Ligit permutation tau, and then you look at how that differs from the true latent Ligit permutation, so you'll get some Lipped permutation. And if you decompose this according to cycles, then what you have is you have, because this model has independence across edges, here you will have independence across cycles for these quantities. For these quantities. So that's very nice. So you really have to just understand these generating functions for a cycle. And in the erosion case, you actually have explicit formulas for these generating functions. And so that's very nice. So here, actually, in the block model setting, things get a bit more complicated because you don't really have explicit formulas for these generating functions. Really, the generating functions really depend on. Really depend on not just how many of each community you have along a cycle, but even where those different community labels are along the cycle. So essentially, what we do is we prove some kind of recursive bounds for these generating functions, which allow us to analyze what's going on. And so this technique could perhaps be useful for other kinds of models where you have independence across edges, but say you have heterogeneous probabilities. Heterogeneous probabilities. Yeah, can I answer the question? Yes, so like in this colour in the auto shrine group, I remember you actually got for a long cycle, somehow the contribution can be always bounded by a short cycle, two cycles. Do you still understand like this or right, right, right, right, right. Two cycles to the appropriate power. Yeah, yeah, yeah. Right. So, okay. I don't know off the top of my head, but I think. I don't know off the top of my head, but I think you. Yeah, I'd have to look at that. But I think you have something along those lines. Yes, but it kind of doesn't come out so easily. But yes, I think so. But yeah, I don't have to go to it. Is the analysis of producer to the case where you have the same probability within each community? Which community, like, does it work if you have, yeah, different communities in the two communities, different publicities within two communities? I think that should work. Yeah, that would work. Any other questions? Okay, so yeah, I'm happy to chat more offline about this. Let me ask another question. So all these conditions right now, your condition on the user variable freedom. Use available statement. Yeah, so eventually you need to remember. Yeah, but basically, if you look at, you know, on the event that the two communities are approximately balanced, then all the balance kind of work out and you don't, you aren't using anything about the. The balance codes for almost all of them are balanced to see them. Yeah, yeah. So the only thing really that you need is that the communities are approximately balanced. You know, it may be that one of the communities just... You know, if one of the communities just is abnormally large, then the balance wouldn't work out. But if, if, on the event that the communities are approximately balanced, you're not using anything more about the community labels. So, just to clarify this high-level message, so it means the threshold is the same as if it is editing, yeah. So, this connectivity threshold. I think that's a natural guess, probably, for other kinds of models as well. Suppose they, I think you mentioned. What supposedly, I think you mentioned the previous work. Uh, consider the case where the label is actually null. Yes, does it just change the value? Um, and no, no, it doesn't change. So, yeah, so in the comp so in the commerce arguments, you can give the community labels. So that it doesn't change the threshold, uh, but it does make the proof easier because you know, if you know that if you know the community labels, then yeah, then. I'm just lucky that you said colours. Yeah. Okay. Let me move on to the second part, which is understanding this bit here. So, you know, just to recap, in this yellow region here, you can't, if you're just given one of the graph G1, you can't. Just given one of the graph G1, you can't recover the communities exactly. You also can't recover the latent permutation exactly. So, the question is: still, you know, maybe you can still do something for exact community recovery. And so this is what we did with Julia and Annie. And so here, what we did is we understood this region here. And so, okay, before giving you the formulas, let me just explain on the picture. So on the picture, Explain on the picture. So, on the picture, there's two regions: this dark blue region and this pink region, which in this picture is a bit small. So, in the dark blue region, you can actually exactly recover the two communities if you're given the two graphs, even though you can't do it from just a single graph and you can't recover the permutation exactly. In this pink region, you actually can't recover the exact community. You can't exactly recover the communities, even though if you knew this. The communities, even though if you knew this latent permutation, you would be able to do so. And okay, so this and this is the picture, and here is kind of a description of what's going on. So in this region, this condition is just that we're not in the red region. So in this region, this equation gives this threshold between the dark blue and the pink regions. And so here this And so here this takes on the following form. So there's two terms: this s squared times a plus b over 2. So you can see that this is exactly this connectivity threshold that comes from graph matching. And then there's another term here, which is s times 1 minus s times this function, which is this Chernov Hellinger divergence. Okay, so this second term exactly comes from community recovery. Okay, so this is, I think this is I think this is a nice kind of equation because it clearly shows the interplay between these two problems: the graph matching and the community recovery problems. So if perhaps some of you know this result on contextual stochastic block models, where you have the block model information and you have the covariate feature vectors, there also there's for community detection, there's this nice interplay between these two different kinds of Between these two different kinds of data, and it's captured in an equation that is kind of analogous to this. Okay. Any questions? Okay, so again, you can, if you plot these phase diagrams for different values of the sub-cycling probability S, then you know you can go deeper into this region or not. Okay. So let's. So let me tell you a question about the formula you put up. So would there have been any way to predict kind of a priori that it would split up in this nice way? Is there some like explanation for why these things should kind of add? Yeah, so I'm not quite sure a priori, but let me tell you the algorithm that works here. This will be not a efficient algorithm, but an algorithm that works. Efficient algorithm, but an algorithm that works information theoretically. So, yeah, this is an information theoretic subscription. And from the algorithm, it's a pretty simple algorithm, and from that, you kind of see where this comes pops out from. So, yeah, I can explain that, and maybe that will answer your question. So, just not this is one PyStar. Is it all possible to recover? Right, right. So, you can have to recover PyStar exactly. Okay, so let me tell you about the algorithm that So, let me tell you about the algorithm that works in that dark blue region. So, you can start. So, here the average degree is logarithmic, so you're still in the regime where you can do almost exact recovery of the communities. And also for graph matching, you can do almost exact recovery of this latent permutation pi star. So, okay, so you can start for G1, you can start with some kind of almost exact community labeling. And so, we use the algorithm. And so we use the algorithm of Muscle Neiman inside. And then what you want to do is you want to use the second graph in some kind of way. You want to somehow match it to the first one and use information from the two graphs combined. So what we do is we use the so-called K-core matching, which is defined as follows. So for any permutation, what you can do is you can match the two graphs. is you can match the two graphs, look at the intersection graph, and then look at the k-core of that. So the k-core is simply the largest subgraph where all the degrees have, all the nodes have degree at least k. Okay, so intuitively the k-core kind of captures some well-connected central subgraph of the graph. And so if you look at the k-core of the intersection graph, and then, okay, you can do this for any permutation. This is for any permutation pi, and then you choose your estimator, will be the permutation pi that maximizes the size of the k-core of the intersection graph. Okay, so that's the k-core estimator. In the paper, I think we use k equals 13. You know, it's some small concept, but not really small. Can we find that in your first step? You actually almost exactly recover the commission. Almost exactly recover the community label that you want. Yes. So, really, the challenge is to then boost that up to exact recovery. Sure, but I'm saying that when you use the like in the second step, you're finding the K-core. So, do you actually use this approximately recovery community level, which you want? Yeah, yeah, yeah. So, let me, well, not for the. So, these are two parallel steps. Yeah, these are two parallel steps. Yeah, yeah, yeah, yeah. Yeah, yeah, yeah. These are two parallel steps. These are two titles. Okay, so this caker matching was studied for the Andrish-correlated energy setting by Kulina Kiavash-Mitalan-Poor recently. And so there are two nice features of this matching. One, I mean, one of them is that, so when you do this, you can think of this also as giving just a partial matching because. A partial matching because you can match the nodes that are part of the k-core of the intersection graph, and then for the nodes that are outside of that, you can just say, I'm not going to do a matching because I'm not sure. Okay. And so one thing that you can show is that all the nodes that are actually, you match them, they're actually correctly matched. And so this is a very nice property actually that you might want to have if you can't match exactly. You, you know, you can't match exactly, you want to make sure that those that you match, you can't sure of the matching, because you know, especially if you want to do various downstream inference tasks, like here in community recovery. And so the other nice property is that, you know, the number of unmatched nodes, those, you know, that's, they have an optimal, optimal size. So, you know, in this setting, you can't do better than you're for any almost exact matching, you're going to make. Exact matching, you're going to make this many errors. And this is some polynomial advantage. It's n to the one minus s squared times a plus b over two. So, you know, you see this connectivity threshold popping up here. Okay. You know, one downside of this matching is that it's inefficient, but I'll come back to that later. Okay. Take the intersection, which label permutation are you using, or are you trying all the ways? Yeah, so you do this overall permutations. Oh, we want to take the intersection. Yeah, for any permutation, you take the intersection graph, and then you look at its k-core and see, look at its size, and then you pick that the pi that maximizes this. Okay. Okay, once you have this estimator, what you can do is you can overlay the two. You can overlay the two well subsets of the two graphs according to this estimator, the Korea estimator, and take the union of those two graphs. So just like previously, but now you're taking the union of two subgraphs. Actually, you know, we slightly modified this estimator for some reason, but I'll get to that in a second. So then the next steps are: well, so. Well, so you have this G1 where you have the labels are almost exactly correct. And then you take the union with G2 for almost all of the nodes. And then here, what you do then is then you just take the majority votes in this new graph with the existing labels that you have. And this, it turns out it works. Now, there's a couple of things. Now, there's a couple of things that you have to make sure in order to make this work. One is that you want to make sure that, you know, in this initial almost exact labeling of the graph, you don't have clusters of nodes that have the wrong label. You want to make sure that the nodes that have the wrong label are kind of just sprinkled throughout the graph. And it turns out that certainly this algorithm gives you that. Gives you that. And I imagine many others might as well. The other thing you don't want is: well, if you look at a node, it'll have a bunch of neighbors in G1 that are matched with nodes in G2, but it might have some neighbors in this set that is kind of thrown out, that is not matched. Okay, so you want to make sure that you don't have any node that has too many neighbors in this set. So actually we We kind of expand this set of unmatched nodes a little bit by a technique of Wuchack so that in what remains, every node will have at most one neighbor to these unmatched nodes. Okay, and you can do that by increasing this set by a size of like a constant factor, so it won't hurt you. Hurt you. Okay, so these are basically the two technical things that you want to make sure in order to make sure that this step actually works. And then you have to still look at the nodes that are unmatched. So these nodes here. And for those, you know, now all these remaining nodes have the correct label. And then for these nodes that you haven't yet matched, for those, you just take a majority vote among the neighbors in G1. Okay? G1. Okay. So, you know, for this majority vote step, what really comes up is this community recovery threshold, this chain of Kalendru divergence here. So that's the relevant quantity. We know that very well. And so let me explain these two factors here. So there's a factor of S times one minus S here. So the factor of S is just because the G1 was subsampled with probability S. So that's this factor of S. And that's this factor of s. Now, this factor of one minus s is a bit less obvious. That's because for these nodes here that are unmatched, they're unmatched really because they're not really well, they don't have many neighbors in the intersection graph. So that means that actually, you know, they don't really have connections in G2. They only really have connections in G1. And so that's why this effective one minus S factor is there too. Okay, and then so this is the relevant quantity, but again, the number of unmatched notes here is not n, it's n to this power, right? And then if you put these two things together, that's where this threshold comes from. So Tim, I don't know if this answers your question. You know, it's not, I don't know if it's a priori populace, but from the algorithm, you can kind of see where it pops up. kind of see where it pops up as a follow-up ask is there a way to interpret like what would it make sense to interpret s times one minus s as the variance is there any reason to think of it that way um not really right like s times one minus s is like the edge probability in g1 but not in g2 yeah yeah that's that's really the interpretation any other questions um okay so Okay, so yeah, let me not say too much about the impossibility argument sketch. Essentially, you know how many, roughly how many singletons you have in this regime, you know, roughly know how many singletons you have in the intersection graph. It's n to this power. And then, you know, you can give a lot to the estimator, you to the map estimator, you can give all the community labels in G2, you can give this set of singletons, and you can give the whole. And you can give the whole permutation outside of the singletons, and basically, still, you know, on the singletons, you won't be able to do much. And so that's why you fail. And it pulls from a second moment in process. So you said map estimator is not the minimum. So what is the map estimator of the file? No, I mean it's not. So you just give So you just given the two graphs, what's the so you can right? So you can you can you can write down the map estimator. And yeah, so so think of it as you want to recover the community labels in G1. And so I can give to the map estimator all the community labels in the second graph. I can give it the matching outside of the singletons. I can give it Outside of the singletons, I can give it the singleton set. But just because I won't know the matching on the singletons, it means I won't be able to recover the communities either on those. Just because there's not enough shared information for those notes. Okay, so let me end with some open problems, future directions. So one natural question is what about efficient algorithms? Is what about efficient algorithms? So, here, you know, the community recovery algorithms, you know, in this two balanced community setting are efficient, but the graph matching algorithms here that we use, you know, either computing this alignment score, you know, or computing the permutation that maximizes the alignment score or this k-core estimator. These are not efficient algorithms. So, a natural question is: can you do this with efficient algorithms? And actually, that has Efficient algorithms, and actually, that has been something that has really been a focus of the graph matching literature. You know, a lot of the papers have been trying to come up with efficient algorithms for graph matching, even in the correlated editorial case. And in fact, in the past year or so, there have been a couple of very nice works. And I believe Jaming will talk about the second work on Thursday. So, you know, this is very much developing, and I think there's Developing, and I think there's lots of hope that some of the ideas present here could perhaps also translate to this correlated block model setting. But there's much more to be understood here. So, this is a natural open problem. And then another natural direction is just to go beyond exact community recovery. So, what do I mean by this? So, you can ask about almost exact recovery. What are the, you know, can two graphs? The, you know, can two graphs improve the error rate compared to one? What are the optimal error rates you can achieve? Partial recovery and community detection. So, in the single graph case, we know that the Kesten-Stigum threshold is the threshold for community detection. And here, presumably, if you have two graphs, then you can do better. So, with Julia and Annie, we're working on some of these questions. Some of these questions. So we, so, and this is work in progress. So, one is about the optimal error rate for the almost for almost exact recovery. So, you know, in the work I just described, essentially we characterize when you have zero error rate, but you can kind of see from the algorithm and proof outlines that they also characterize, at least in the logarithmic average degree setting, what the optimal error rate is. And so, we're working on understanding that. So, we're working on understanding that for whenever the average degree diverges to infinity. And also, we're also working on beating this Kesten and Strigum threshold with two correlated block models. So, here, you know, the argument is roughly as follows. So, suppose that you have two block models, so marginally, the two block models. So, you're in, suppose that the average degree is constant but large, and suppose that the two block models are marginally. Suppose that the two block models are marginally below, they're just below the KS threshold, but the correctly matched union graph is above the KS threshold. Now, in this regime where the average degree is constant but large, you won't be able to align the two graphs. The best you can hope for is maybe aligning 99% of the two of the nodes. And then what you have is effectively, you have something where You have something where, you know, on the 99% where you've correctly aligned, you have a block model, but then you have 1% of the nodes that are kind of matched arbitrarily. So it's kind of like you can kind of think of it as a corrupted stochastic block model. So this kind of relates to these questions of robust detection and robust recovery in stochastic block models. So there's been a good amount of work on this, including by Alex and This, including by Alex and others, a recent paper by Liu and Moitra characterized these kinds of questions in various settings, such as when you corrupt edges or when you corrupt nodes and all the edges adjacent to some subset of nodes. So, here, you know, it's not so this is certainly a very interesting literature that kind of really ties into this. Literature that kind of really ties into this problem. It's not clear to us, so it's certainly not obvious whether you can just use these existing results on robust SPMs as a black box. So it seems like you have to work further because those results were not really designed for such settings, but they're definitely closely related. And so one, so these are some things that we are working on at the moment. I think a much more challenging approach. A much more challenging open problem, but something that I'd love to take your input on, because I think many of you have lots of expertise related to this: just try to even just predict, have a guess for what the threshold is for community detection from two correlated block models. So, you know, I'm not saying, you know, prove it or anything, just have a guess for it, a heuristic. And what does this tie into? So this ties into several things. This ties into several things. It ties into weak recovery of the matching. So, one thing that needs to be understood for this is even just let's go back to the correlated Erdogani setting. You want to, in the constant average degree regime, you want to weakly recover this latent permutation. And, you know, there, even the information theoretic threshold is not known precisely. There is a conjecture, and there's, you know, I think it's almost. I think it's almost proven up to a factor of four. Is that correct? No, I'm talking about if you look at the for graph matching, recovering the latent permutation in a weak sense. So here there's a conjecture for the information theoretic threshold, but like this is not proven yet. It's only proven up to a factor of four. Right? Is that it's still at? Yeah, almost. I guess the threshold belief is like. The threshold belief is like NPS squared, right? So it's like the intersection is the return to the search for the emergence of the joint. Yes, yes, yes. So that's the conjecture, I think. So, okay, basically that we don't cheese the algebraic. Yeah, yeah. So now, I mean, yeah. So, of course. I mean, yeah, so of course there will be communities, but you know, I think to understand this question, you have to even understand questions in correlated data. And we have to first, right? So you have to understand this threshold. And then also you want to understand what is the fraction. So what is the fraction of the permutation you can actually recover when you're above the weak recovery threshold? I don't even know if there's a conjecture for that. But then, you know, even I guess if you look at it, you call it in the stone trust model and just fall back to matching them. So transmotion just for matching, then why then a gas is just using this notation ALA? It's just it must be over to the right, right, right. Yeah, so definitely that's yeah, yeah, I would imagine that that's the case. But then I think if you want to understand the threshold for community detection, you want to understand also like above the threshold, what fraction of the permutation you can recover. And then so that's a function you want to understand. And then on top of that, you want to understand some kind of, you know. That you want to understand some kind of thing, you know, if you want to align things, and then you have some kind of like robust SBM type of question on top of that. So, you know, I think there's several layers to this question, but it could be great. Sorry, I think I missed something. What is what do you mean by community detection? Like, what's the goal? Yes, sorry. Suppose you're in the same setting with two correlated block models, but now the average degree is constant. Okay. Constant. Okay. So not logarithmic. So the best you can hope. So like if you just take a single stochastic block model, then you know what you can hope for is some estimator that is just better than a random guess. Say, you know, yeah. And so you want the same thing, but now you have a second block model that is correlated. Okay, any questions? Yeah, so I think, you know, I think this question has many layers, but I think maybe, you know, just thinking like roughly about heuristics could be quite helpful. And maybe that could be great for such a workshop setting. And really, the challenge in these questions is that you have to understand an interplay between these two problems. So, how come physicists didn't come up with their usual reduction for this? I don't know. Somehow, physicists haven't looked at this model yet. They had actually looked at the question. Oh, yeah. They've looked at the correlated Erd≈ësini model. And I think they're now aware of this model, but they haven't worked on it yet. But this conjecture for the correlated emission is... For the correlated dimension rules, what you said, that's p square equal to one, or something. Yeah, yeah, I think I that's natural. I would imagine that, yeah, yeah, yeah. Um, are they making this conjecture based on some kind of belief propagation gathering that needs to be something else? Yeah, that needs to be something else. Uh, because I think I think here we believe that there may be a computational gap, yeah, yeah. So, if you do the usual specific physics. If you do the neutral specific physics calculation based on local algorithm, then now it will be able to computation of switch, or now it's like a threshold. That's the slightly different. Yeah, like in the choose for Kaspar, like choose the metrics for Caspar or not with the point. Yeah, exactly. So it's like the observation is like analog with the KS search. Yeah, but it's different from ITK. So it's not the approach. So it's not there. Right. But I guess something that I want to comment is that I remember actually in this sparse end, I think this, they actually, I think Alasani and then they actually show that they can possibly align all the nodes in the chat for that. Oh, really? Oh, so then, aha. So then maybe this function would be the exact same thing. The this function would be exactly the same as for the size of the giant components, yes. Okay, yeah, that would be cool. Cool. Yeah, at least that they have this conjecture. Maybe they have a team if they go through. Yeah, yeah, yeah, yeah. Okay, great. That's great to know. And then I imagine it would be similar for the block model, too. Yeah, although I don't know how to combine this too. Although I don't know how to combine these two. Yeah, yeah, yeah. Okay. Yeah, I guess I should wrap up. So let me just say that you can also, we here we just studied the balance two community settings. So obviously you can consider this for more communities, general parameters, etc. Yeah, so let me just conclude. So thanks very much for your attention. 