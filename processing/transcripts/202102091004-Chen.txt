The scatter wave in order to not be detected. So, the last example is about like the stellarator optimization. So, I want to design electromagnetic coils and also the magnetic field inside for plasma confinement in order to achieve this magnetic confinement fusion, which typically like a tech place was. Like a take place with very high temperature, like hundreds of millions of degrees, you have to confine it with magnetic field. Here, the design is electromagnetic coils. It's quite complicated shapes. So, I want to design these different shapes. Subject to this PD constraint, this is magnetic equilibrium model with a force balance equation. And here we have the magnetic field from the coil. And now, the uncertain parameter is a coil shape. And also, our design is also the coil because, you know, like also the same argument, like the manufacturing error or some other uncertain operation environment. So, objective functions to minimize some physical, key physical properties like equilibrium of the magnetic field, and also some properties of the coil itself. This is a This is ongoing research in collaboration with Floria and Gorg here online. And they have a recent paper on a single-stage optimization in the deterministic setting. And I think they're quickly they're going to push out a stochastic optimization version. But this is a simplified model in 1D. Well, there are some commonalities here for this different example. This through different examples. So, first of all, we have like the PD constraint and we have also the uncertainty, typically in high dimensions or infinite dimensions. And then we have certain objective function subject to like a statistical measure. Here is the abstract formulation. We have PD constraint we denote as this R. Here use a static variable, and M is our random variable. And M is our random variable, and Z is a control or design optimization variable. And this V here M and Z, and they're suitable like a separable Banahart space. For instance, this random variable M, causing a random field is in Hilbert space. And we have a weak formulation. So we apply this durality pairing with a test variable, joining variable here. We'll get this abstract weak formulation for the PDE. Weak formulation for the PD constraint. Now, our objective or the constraint function Q and F, F is constraint, for instance, here, the implicitly depend on M and Z through the state variable. That could also be explicit. So the risk and statistical measure we consider here, first is a risk neutral measure, which is just expectation. And a risk adverse measure could be the expectation plus variance or semi-deviation. Or semi-deviation or the conditional value at risk. So, Ju gave a very nice presentation on different risk measures yesterday, especially the coherent risk measure. So, here we have the optimization on uncertainty problem. I want to minimize this cost functional with additional penalization on the control variable, subject to the PD constraint and possibly additional chance constraint here. This F. I want to mention here this F in this work is. In this work, it is a scalar function. So text values as well, number, for instance, but M and Z are high-dimensional. This is quite different from Gorgias' talk just now. And so the random parameter or random field that we consider for this M could be Gaussian Markov or conditional random field. At each special point, it is a random variable. Special point is a random variable, and there are some special correlations, particularly type is Gaussian random field with mean m and covariance. And this covariance is a operator from L2 to L2, and it could be given by a covariance function k here. This is an integral form for any function file in L2. And this kernel is called covariance kernel, one of the quite common classes of a Merten covariance kernel. Merton coverance kernel. And to represent the uncertainty, Sukark just said that we can use a Kakuna law, which is one of the common representations. So it basically expanded in eigen basis. Lambda J and Mj are the eigenpairs of the covariance here. So expanded in the eigenvectors or eigenfunction mj here. And yj is our ID and Gaussian random variable. And another And another common representation is stochastic PDE. So, here, for instance, if our covariance is modeled by this sort of like inverse of a fractional elliptic operator, where here this is a Laplacian, and then we can sample this M, this random field, by solving this PD, where on the right-hand side is a special Gaussian-white noise with a unique variance. So, for each of the something, we need to solve. Uh, you know, something we need to solve a PD, but however, here this is a elliptical PD or fractional elliptic PD for gamma equals to two or you know, four, this would be easier to solve. Well, this gamma basically determines the smoothness of the operator, and this kappa determines the correlation lengths and also the variance of their random field. All right, so to solve this kind of problem. All right, so to solve this kind of problems, there have been many different methods developed over the last decade, I'd say. For instance, like the common MC, MC, I'm sorry, the Monte Carlo or the multi-level Monte Carlo or quasi-montecal, high-order quasi-montecalo. And another type is sparse grid, stochastic collocation, and generalized polynomial chaos. And there are also model reduction techniques like the ROM Gorges. Techniques like the ROM that Gorg just presented, and also low-rank approximation for the low-rank approximation of their state variable with back to the parameters. And recently, we've been working on this Taylor approximation and the leading paper here given by Alain Alexander and also a collaborative Zakar and Omer a few years ago. And well, a special, very interesting. Very interesting paper is this machine learning framework for solving high-dimensional mean field again, which is a bit different setting. However, as it shows, like the machine learning or deep learning has also been applied in this stochastic optimization problems. This is given by Lars, who is also online here in the audience. Well, I'd like to start with the meme very So, you know, start with the mean variance optimization on uncertainty because it's simpler and as a demonstration for our Taylor approximation. So, here, we have a given control or design variable z. Here we omit it for simplicity for all these Q's. A Taylor expansion is given as follows. We have a Taylor expansion at the mean, and this is gradient term, a first-order derivative. This is a second-order derivative. This is a Haitian. Haitian is an operator from the prime. From the parameter and spaces to its dual, and those are their first and second order fresh derivatives. Here, by this duality pairing, it's a simplification for this M and M dual here. And now we can truncate this Taylor expansion up to linear term or quadratic term. And the reason that given this linear quadratic terms for a Gaussian random field, there is explicit or analytical form for Or analytic form for this expectation and also the variance for linear, post-linear, and quadratic approximation. In particular, for the quadratic approximation, it involved this stress of this edge. And this edge is given by the covariance square root. And this is the Hessian term. This is a covariance square root evaluated at the mean here. This is so-called like a pre-prior covariance or the covariance pre- Covariance precondition Hessian. And the variance here for the quadratic term is given by the stress of the H square. So to compute, well, we have the analytic form, but however, still we have to compute this stress. This is typically the challenge. So either we can use a randomized stress matter, or here we use the sum of all the eigenvalues, but we truncate it at certain. Truncated at a certain number n for the dominant eigenvalues. Well, if the eigenvalues lambda j decay pretty fast, then this number n here is typically quite small. And for the trace of the edge square, it's given by the square, sum of the square of the eigenvalues. And this eigenvalue is actually the dominant generalized eigenvalues of this Hayden and covariance inverse. And so we can solve this generalized eigenvalue problem to get the The eigenvalue problem to get the eigen pairs. And well, in order to get the dominant generalized eigenvalues, so the computational complexity actually just depends on this capital N. If you use a randomized algorithm or some other like, you know, algorithm, deterministic sort of algorithm by extracting their leading eigenmodes. So a modes. So a randomized SVD here is given as follows. We're going to draw a Gaussian random matrices here of size n plus an additional oversampling factor, which is quite small, typically five to ten. And then we're going to apply this Hessian acting on these vectors and also the covariance acting on these vectors. Perform QR vector radiation here and we form a new matrix which is quite small now, it's n plus p by n plus p. plus p by n plus p and we can compute this eigen decomposition and this eigenvalues are extracted from this eigen decomposition and also the eigenvectors given by the q times this s and as n other eigen pairs here for this smaller scale eigenvalue problem so the computational costs are given by four times n plus p linearized pd solves once you PDE solves once you performed their state PDE solve, which could be non-linear at the mean. And then all the additional softs are linearized. Now the error is proportional to the remaining eigenvalues. Again, if the eigenvalue can fast, this error could be small. So now we can plug in our tail approximation into their cost functional. And we have this form now. So basically, everything is a computer. So, basically Laborations are computed, and the gradient is also well. Let me introduce how to compute them explicitly. So, first of all, we define a Lagrangian for our objective function Q here and also our PD evaluated at the mean. And then the state problem is given by the variation respect to the joint variable. And so, by solving the static problem, we can compute their objective. We can compute their objective. Now, if you take the variation with respect to the state variable, then we're going to get the joint problem. So after you get the joint variable V here, we can compute the gradient by the partial derivative of this Lagrangian respect to M. This is sort of like we're knowing. And it requires when linearized the PD soft. And for the second order information, basically to solve generalize the eigenvalue problem, we need a Hessian action. And we form an additional And we form an additional Lagrangian here. Here is a static variable and the adjoining variable and also the gradient term. And then I'm going to take their variation with respect to their, you know, V had sort of like the incremental joining problem and also tech variation with respect to their incremental state variable. Then we obtain the incremental state problem and joining problem. Those are the linearized problem where here this operator is a you know the second order You know, the second-order derivative of our PDE operator with respect to v the state variable and also the u here. And then the hashing action can be computed by the partial derivative of this Lagrangian. And so this basically requires two linearized PD solves. And all right, so after we obtained how to solve for all this different quantity, compute these different quantities, we're going Computed these different quantities, we're going to formulate our like sort of mega Lagrangian with all the constraints. So essentially, we have the cost functional here and the state equation and the joint equation and the eigenvalue problem with the join variable of side and star and the orthogonal condition here for the eigenvalue problem and incremental state problem, incremental joint problem. So now so we can use this Lagrangian to So, we can use this Lagrangian to compute the gradient respect to our control variable z. It turns out that this joint variable for the eigenvector here, Poseidon star, is given explicitly. And also, all the joint variables, u head star, v head star, are given explicitly for this particular setting. And we just need to solve this u star and v star. Of course, like it looks Of course, like it looks quite complicated for all this formulation, but however, if we sit down and be patient enough, we can write all these derivatives. You can see that all these derivatives, they involve like a third-order mixed derivative respect to the parameter m, the static variable u, and also the joint variable. So this derivatives can be performed by symbolic differentiation. For instance, we use Phoenix for this high-order derivative. For these high-order derivatives. And to compute one coordinate here, so we need to solve one state PD and also this many linearized PDs. It's essentially 6n linearized PDs. See if n here is small, then the number of linearized PDs of quite small. Here is again the example. Well, we can write down the weak formulation for this problem. Formulation for this problem for the state variable, the velocity pressure, and their additional variable for the turbulence viscosity. And this is a test variable. And it includes the model term and the stabilization term and also a niche method for the boundary term. As you can see, there are many, many weak terms here. If you check third-order mixed derivatives, it's quite daunting. So, fortunately, we have a symbolic differentiation here. Here. So now here's the numerical result. Here's our computational domain. This is an inlet boundary and this outlet boundary. And this is the wall here. And this is the center line, it's a cross section. And here are two random samples from their random field. Here's the initial state for their velocity magnitude or velocity absolute value, the norm. And here's optimum. Value, the norm. And here's optimal state. You can see that we effectively enlarged the cross-section as our objective. And here's a decay of the eigenvalues. As I talked about like the first decay of the eigenvalues, as you can see here, for the like the first 100, you can see several orders magnitude, like four orders magnitude decay. If in log log scale, it's proportional to i to the minus two, or even a slightly higher. Or even slightly higher. So, this is the decay of the eigenvalues, the blue and black. And the green is the estimate of the objective functions, that is the mean covariance by this truncated version. You can see that it has also quite fast compared to the Monte Carlo estimate. All right, so this is a So this is the decay of the eigenvalues with increasing parameter dimension from a few hundred to a few thousand to about one million. And you can see that the eigenvalues essentially they decay and pretty like similar to each other. And also the decay of the number of iterations, like gradient-based optimization, the number of iterations decays similarly for different parameter dimensions. And so essentially, the total computational cost is the number of PD solves per iteration times the number of optimization iterations. So here we see that the number of PD solves per iteration depends on the decay of a generalized eigenvalue. That's scalable respect to primary dimension. And also number of iterations depends on how well the BFGS Hessian approximate the true Hessian. Essentially, the number of gradient iteration is scalable with respect to the parameter dimensions. With respect to the parameter dimensions. So basically, the total computational cost is a scalable respect to the parameter dimension. This is about the parameter dimension, but however, we didn't show the control dimension. But the control dimension is a finite dimensional problem. It's not high dimensional. But in other work where we have the acoustic metamaterial design, it's also scalable with respect to the design variable. So next, I like to So next, I want to present this chance constrainted optimization. So here we have the cost functional. We only consider the expectation here. We have the chance, the f larger than zero, and the probability of that should be smaller than a given alpha. And now this probability can be evaluated as expectation of the indicator function in the parameter domain, parameter space. Parameter space. Now, well, so we're going to apply this like a smooth approximation in contrast to Grog's spherical radial approximation because we have a nonlinear F here with respect to the parameter and the control variable Z. Here is the smooth version. We can see here as the parameter beta increases. Under beta increase, the approximation is closer and closer to the indicator function. If we define this indicator function at zero as one half, then we have the convergence of the smooth approximation function as well as its gradient to the indicator function. The next, so we're going to apply the quadratic Taylor approximation here for the constraint function. So expand it the same as the objective function. Now, As an objective function. Now, so there is no expectation but the probability that we need to evaluate. So, in order to use this quadratic approximation, we're going to first compute a low-rank decomposition of this Haitian term by the eigenvalue decomposition. So now it becomes, you know, this explicit term. It depends on the eigenvalue and eigenvector, which you can compute as a cost of an F. And once you have this. And once you have this surrogate, the quadratic Taylor approximation, you can evaluate the function without solving any PDEs. So you can evaluate their probability now by a sample average approximation with samples drawn from the measure of M. Now, if we combine all these different approximations, what we're going to get is the probabilities given as F depend on parameter beta. As f depends on parameter beta, the smoothing parameter mf and the number of monocal samples and mf is the number of eigenvalues and the quadratic approximation. So this would be their evaluation or the approximation of our probability here. Now, again, the total number of PD solves we need is an F or like a six NF. So, if this NF is smaller than the number of Monocolo samples, then we're going to gain something here. And so, for the nonlinear constraint, we use external penalty method with penalty function given as S gamma, which is given in this form. And we have explicit gradient here. And now, if we enlarge this gamma, increases gamma, basically to controls the significance. Basically, it controls the significance of the penalty. You can see that it goes higher and higher. Basically, once it violates their constraint, then the penalty is pretty big. So, now we change the chance-constrainted optimization problem. We formulated as an unconstrained problem. But of course, there is additional PD constraint. But for the chance constraint part, it becomes unconstrained. It would depend on the parameter gamma. The parameter gamma and the parameter and beta. So we use a continuation optimization strategy. So basically gradual increase in parameter beta and gamma to solve the problem. And so to compute the gradient with respect to the control variable Z, again, we formulate the Lagrangian and includes all the terms for Q and the terms for F. Then we can sort of like attack all the variations to get all the required variables and compute the gradient here as a partial derivative of R with back to the control variable Z. Again, the total number of linearized PD solve is this number is like a 6NQ, 6NF with one additional stat PD solve. So we tried it for a flow problem. This is an application manager. This is an application in management for groundwater extraction for agriculture irrigation, for instance. Here's our velocity field, and we have extraction well. So we have a homogeneous duration boundary condition. Now, the source term here represents the water extraction. We have HK as a modified function, causing modified function, a small function, and ZK is fine to make. Is a finite dimensional control variable in this range 0 to 32. Now, the weak form of this PD can be formulated as follows. This U here stands for the pressure. And our objective is to achieve certain extraction rate, a given extraction rate, ZK bar, which we set as 18. And the chance constraint is that so we want our pressure. So, we want our pressure. So, essentially, this F is given as a sort of like a pressure. It depends on the pressure. So, essentially, it means that we wanted the pressure not to drop too much. Otherwise, the aquifer could collapse. If you extract too much water, then the aquifer might collapse. So, we want to prevent that. This is the average value instead of a pointwise. So, more reasonable formulation would be the point wise, but however. Would be there and point-wise, but however, here we can only handle this sort of like the scalar function right now for this paper. Well, here's the optimal control result. So we use the Taylor approximation of the constant term, which is deterministic approximation, the linear approximation and quadratic approximation compared with the sample average approximation. So on the left, you can see here, this is So, on the left, you can see here this is our target for Z, and this is a deterministic control. We get this number. And for the linear approximation, Taylor approximation, we'll get the black squares. And for the quadratic sample average approximation, by sample average approximation, I mean there is no Taylor approximation here. We use a sufficiently large number of samples in sample average approximation. You can see that the Taylor approximation in quadratic. Approximation in quadratic forms is pretty close to the sample average approximation. And they're evidently different from the linear and deterministic approximation. Here is initial pressure. It's quite a big drop here in the middle. And here's optimal pressure. Here's the extraction red at different locations. So now this is our result for the continuation approach. So essentially, Approach. So essentially, we enlarge this small signal parameter at each continuation step in four steps. So it's quite a compact figures. Let me explain a little by little. So here we have the indicator function in solid lines and the smooth function, smooth function are better depending on better in dashed lines. And we have the deterministic approximation. The deterministic approximation in green and in red is a linear Taylor approximation. In black, it's a quadratic Taylor approximation. And in blue, it's a sample average approximation without using any Taylor. So essentially, at the very beginning, in the first step, we can see that at optimal step, the deterministic approximation is quite different from all the others. And also the Different from all the others, and also the dashed and the solid lines they're evidently different from each other. That basically means that the smoothing approximation is not as good yet. And as you increase the smoothing parameter, you can see the solid lines and the dashed lines come closer and closer. That indicates a certain convergence in their smoothing approximation. And then we have We have the penalty of gamma as gamma becomes bigger and bigger. So the chance we compute it here: this is the chance. We have a targeted chance here as 0.05. And now, so you can see like there at the beginning, the competition is quite different from the target here. But as the gamma becomes larger, this becomes This becomes closer and closer to this 0.05. But still, the linear approximation gives an over-conserved estimate. And the quadratic and the sample average approximation, they give a pretty close value to 0.05. And this is the decay respect to different number of samples used for the sample average approximation. What this What this result sort of indicates their convergence respect to their small parameter and a penalty parameter. But here we don't have any theoretical analysis for this convergence. Well, here is a teleapproximation error estimates. This is for different number of samples. We can see that here, this is the teleapproximation errors with the black indicating the quadrature. The black indicating the quadratic approximation can see evidently they're smaller than the other approximation. And also, we have a histogram to show that the quadratic approximation is much better than the others. And then here we have a chance estimate with 1024 samples. This is estimated for alpha itself without any approximation, with a smooth approximation, but without Taylor approximation. This is estimated of this. Estimate of this chance for different steps. The step goes to 4 here. We have 0.05. And this is our sample average approximation bias. So essentially means how much error we committed for this many sample average approximation is given by the square root of a mean square error divided by the number of samples. The magnitude is about like something six times. Samson six times 10 to the power minus 3. And here we have the use of the deterministic approximation, and the error is quite big. And as we go to a linear quadratic approximation, we see that this error committed by the Taylor approximation is smaller than the sample average approximation bias. So essentially, this error is smaller than what we required. And also, we see that. And also, we see that so this quadratic is much better than the linear deterministic approximations. And then again, this is the decay of the eigenvalues respective numbers. Can I ask you a quick question about that, Paul? Yeah, sure. Did you ever study how this changes for events that become less likely? So, as you don't pick like 5%, but 1% or something? It's a good question. It depends on how well. It depends on how well the teleapproximation is for the wear tail, right? Because for smaller chance, then the teleapproximation might not be a good approximation, especially for parameter and far away from the local, the mean here. So we didn't study that, but we can imagine like as an alpha goes to be smaller, this teleapproximate might become. This tele approximation might become less accurate. Okay, that makes sense. Thank you. Thank you. So, yeah, this is the decay of the eigenvalues respect to the different steps and respect to different parameter dimensions. And this is a gradient norm for different iterations and see a similar scalability result. Well, yeah, as Floria, I think it's Florian asking, right? About the chance of R. So, yeah, so now we use the local and quadratic teleapproximation for our evaluation of the chance and also our objective. But what if the local and quadratic teleapproximation becomes less effective, becomes less accurate? We have several remedies here. The first one is using a variance reduction by Taylor approximation. And the second one is a mixture Taylor approximation. This is essentially Taylor approximation from local to global. Tele approximation from local to global. So essentially, we build a teleapproximate at many different locations in a structured way. And the third one is a higher order teleapproximate. If the quadratic is not good, and then maybe high order can buy us something. But this is maybe a different setting. But however, to compute the high-order teleapproximation involves high-order tensors. So we developed a randomized tensor trend decomposition. A randomized tensor trend decomposition using only tensor action, which basically means the PD solves without the excess of tensor entries. So let me briefly go through them in the next maybe 10 minutes. So yeah, this is a variance reduction. So here we know that for Monte Carlo estimate of a function here, the error is given by the variance of a Q. Is given by the variance of a Q divided by this number of samples M. Now, if we use the Taylor approximation as a control variant, so we can build this expectation by the two terms. This is one term, this is another term. And then for the first term, we can evaluate it similar as we did. We can compute the stress by randomized SVD algorithm. And then for the next part of this one, we can use a sample average approximation. Use a sample average approximation. Of course, here there might be a coefficient that you need to optimize, but if the Taylor approximation is highly correlated with the objective function, then this could be a good approximation. So essentially now the estimation error becomes the variance of a Q minus Q quadratic term. So this could be smaller than the original function here. So we have a demonstration. Have a demonstration. So, here, this at a different control, like the control obtained at using linear quadratic intelligence approximation or deterministic control, and also the different number of samples that we show for the estimate of the objective function and the mean square errors. As we can see here, this estimate for the Monte Carlo objective function here, and this is a mean square root, a square error, square, a mean square error. A mean square error. And so this is essentially indicating their variance here. And here we have the mean square error for the difference, tail approximation errors. As we can see here, there is about two orders of difference here. So that basically means we can achieve about 100 speed up in terms of the number of PD solves with a quick. Of PD solves with a quadratic approximation and use it as a control variant. This is for the mean here, and this is for the variance here. And well, the second one is mixture Taylor approximation. So here is one-dimensional Gaussian density in black, but we can approximate this density by a mixture, a Gaussian mixture model. Model. So with three components, three Gaussian, we can approximate the black by the red dashed line. This is a mixture model. But if we increase the number of components, we can see that the approximation is more accurate. And of course, for each component, the variance becomes much smaller. This is in 1D. So in 2D, here we have a Gaussian in 2D. We can also approximate it by the mixture model. Approximated by the mixture model, and this is approximatic Aussian, which is very close to the original one. And for each of the components, the variance becomes much smaller. So now we can decompose or approximate our original distribution with high variance by the Gaussian mixture models with small covariance. We have a mixture model here given by weighting the sum of each of their. Each of their much lower variance Gaussian distribution. And we perform the teleapproximation for each component here. So essentially, for each component I, we can perform quadratic decomposition. And then for each component, we can also evaluate as we did before. And then the total will be the sum of this weighted of this expectation of a Q using a quadratic. Using a quadratic tail approximation here. So essentially, this is sort of like we want to reduce the variance first before we approximate it with a tail approximation. And we can allocate our teleapproximation in many different locations. But this will depend on how to find the optimal locations. And what if in high dimensions, how do you determine these locations? This is ongoing work. Is an ongoing work. The leading contributor is DC Law here, a PhD student in our group. Here we have a simple test example. We assume that the random field has a large variance, and we use different mixture models for Taylor approximation. You can see that this is for the both of them are for expectation. We have using a linear approximation. Approximation by Taylor with different components, different splits, mixture splits. You can see that, well, because we use a mixture model and Taylor approximation, and there is an analytic formula for this estimation of the expectation, the error does not grow with the number of samples. So we don't have the samples here. But we can, well, this is compared with the Taylor approximation with With Monte Carlo estimate, well, essentially the variance reduction by using Taylor as a control variant for variance reduction, we can see that as the number of samples grows, then the error also decays. There's different trials. Well, for the quadratic approximation, we see that you need to use about like 10,000 samples to achieve about using this Gaussian mixture. Using this Gaussian mixture models, pretty similar error here. So this number of components is quite small, first of all. We hope that by using this decomposition and Taylor approximation, you can achieve certain speed up compared to this variance reduction by Taylor approximation. So, here for this example, it's about like 10 times the speed up. Well, this is for the variance. We have similar behavior to evaluate the variance. So, this is about the mixture model from local to global optimization. And then we have the high-order Taylor approximation. So, again, here Q is our objective function. We have a constraint R, and we can perform, we can consider Taylor series up to high orders. And again, this Q now is not a scalar function as we. A scalar function as we considered before for the chance constraint. This q, both q and f, the chance constraint could be high-dimensional. And then we can perform this Taylor series. For instance, like if Q is high dimensional, this becomes a Jacobian matrix instead of a vector. And then for this, Hessian would become a third-order tensor and can go on for high-order terms, which involves high-order derivative tensors. And by the way, Tensors. But we have a problem that we can only access these tensors by their action sort of tensor product with certain vectors here. And well, this work is by Nick Elger, and the code is available online here. Well, so how to like, first of all, compute the tensor action with a PD constraint. Now, here, suppose we have this. Now, here, suppose we have this case order derivative in different directions m alpha with m alpha 1 up to m alpha k. And then we need to solve a sequence of PDE problems to get the variable intermediate variable u alpha and the adjoint lambda alpha by solving the incremental state and incremental adjoint problems. Those are linearized problems. And for instance, if we alpha equals to ABC, Alpha equals to ABC, then the number of PDs we need to solve is about 2 to the k is a 3 here. But here we have the Taylor approximation where you see this tensors are all in the same direction. You can evaluate all these directions in the same direction. So in this way, the number of linearized PD we need to solve is 2 times K instead of this exponential growth. So they're different like So there are different like tensor decompositions, like the Tucker canonical decomposition, but however for those decompositions, the complexity is too high. And here we use tensor trend decomposition. And we cannot use like the classical algorithm because we can only access the tensor by the tensor action. So we developed a randomized version and also with a pitching of certain elements. So essentially, for instance, So essentially, for instance, like if we perform up to the tensor trend decomposition up to here, then we need to get the next call. To get the next call, so we use randomized random vectors here, 5, 6, 7, 8, and also the dominant vector here we computed. And then we solve for certain vectors to satisfy certain equations here by least squares. This is a bit more complicated in detail. In detail. But essentially, the trend here, the core, is of size R by N by R. This R is quite small, and this N is a parameter dimension. And this R, so this is scalable to high order tensors. The number of tensor actions is proportional to the K R and squared. It's linear in K in order, in the derivative order. So we have a test here for different tolerance of the truncation. We get like for different We get like for different orders, k and goes from one to five. And this is a with a different measure size. We can see that the rank it doesn't change much. And also for smaller tolerance, we use higher rank, but the scale of the rank is relatively small. And that don't change with respect to different orders. And this is a teleapproximation error for different orders. You can see that the teleapproximation error becomes. Error becomes smaller and smaller until it becomes saturated here. Yeah, so this is about the high-order tensor approximation. So essentially, we constructed the quadratic teleapproximation with respect to the random parameters for both a control objective and a constraint, for both mean variance and chance constraint optimization uncertainty. And we explored that the Hessian of the parameter to observable to object. Parameter to observe objective map is compact and it has a fast decaying eigenvalues. And use a randomized algorithm to compute the low-rank approximation. And so the PD and eigenvalue problem constrainted optimization is formulated by Lagrangian and solved by Grading Best approaches. We demonstrated the scalability of the number of PDE solves up to a million random parameters. And we extend the teleapproximation to variance reduction mix. Approximation to variance reduction, mixture, and high-order approximations, and their further research on their analysis of the Hessian spectrum decay and error analysis of teleapproximation. And we have performed these different teleapproximations, but haven't applied it to optimization on uncertainty. And then we can consider more general risk measures like the C-var, the joint separable chance constraint. So that's it for today. I want to acknowledge that the I want to acknowledge the supporting grant here from NSF, Simons Foundation, and DOE and DARPA a few years ago. Thank you very much for your attention. Okay. Oops. Come back into this. Thank you very much, Peng, for a very interesting computational talk. I'm sure there's some questions. So I'll just open that up to the audience. Anybody? Yeah, I mean, I want to ask actually a question. I know that there is peng, so I how much can you actually, does it ever make sense to go beyond the third order tensor when you do these expansions? I mean, is there enough? Because I mean, it becomes huge, right? Is there enough? I can see that there is. I can see that there is third information in that third-order tensor, but does it make sense to go beyond that? Because it also becomes very expensive, right? Yeah, it does become expensive. But if you can, you know, here you can see the error, like it decays exponentially, right, depending on the order. And if there are a computational cost grows polynomially respected, order, but the error decays exponentially, then Then there is a trade-off that we can exploit. Maybe we can still go high order, especially if we perform this mixture model where the variance becomes small for each Taylor expansion, and then we can go to high order. But of course, this will require quite a sophisticated tensor actions and randomized tensor decomposition algorithm. I would say the curve is quite high. Curve is quite high. So, I would have more of a theoretical question. So, you start with these rather complicated optimization under uncertainty problems with PDEs and non-smooth functionals and risk functionals and so on, and then you do an approximation about the mean. I mean, what can you say about the solutions that you compute and their relation to the solutions of the original problem? I mean, is there any kind of Original problem. I mean, is there any kind of theory that says you will approach the solutions of the original problems if that expansion tends to infinity? A good question, theoretically. Well, I cannot say because, you know, like this, as you said, like the PDs are quite lonely and complex. And we might not be able to explore their nice property of convexity, semi-lower continuity. And also, their untail approximation is. You know, also their untail approximation is just a surrogate, it's not a convergent algorithm, let's say. So, but however, like still computationally, we can observe the certain convergence. Like, for instance, we showed for the quadratic tailor approximation, it's the optimization variable obtained is pretty much the same as if you only use a sample average approximation. But this is like a computational confirmation. Like a computational confirmation, not okay. So you do like a brute force solve of the original problem and then take that as the true solution, and it seems and it seems to work. Yeah, for instance, this is a sample average approximation without Taylor. And there is the green essentially sitting on top of a sample average approximation. This is like a sort of computational confirmation of the conversion. Yeah, I mean, it would be good to know. Yeah, I mean, it would be good to know if it works. And it kind of like to link to Georg's question, right? Like, do I really need to keep going with the massive tensors if I know that everything works with a second order or third order? Yeah. Or with a certain error, that'd be a nice result. Okay. So we have seven minutes until our next event. I would suggest we take a quick break and then we come back and we have our first panel of the workshop on. Of the workshop on computational methods, which I think would link up nicely to this previous talk. So thanks again, Peng. Thank you, John. So I guess we'll just wait here. Hi, Peng. Can I ask you a question now over time? Yeah, sure. I was just very curious, did you, you know, both gamma and date are kind of parameters that needs to be driven? That needs to be driven a certain way, and they inherently produce ill conditioning, right? As you push them further and further. I wonder in optimization, underlying optimization, did you notice that the optimization start getting slower when gamma and beta got pushed far up? Yeah, so if we start off directly with quite a big beta and gamma, then Big beta and gamma, then the number of iterations we need is quite high. So that's why we use a continuation approach. We started with the small ones and then we increase them gradually. And so from the numerical result, we can see that the number of durations that we need is pretty stable. But I'm thinking, because typically, I really is your warm start. I realize you're warm-starting it in this way, but typically, if I make my underlying non-linear optimization problem more and more ill-conditioned, you know, being it Newton's method or whatever that is doing the hard work on a non-linear problem, it's going to become less and less effective, if you like. So steps going to become smaller and smaller and things like that. So, in a way, to make an epsilon progress, You know, an epsilon progress with a high gamma and beta, it's very hard compared to a low gamma and beta, and that seems to be kind of very difficult to tune to some extent. I don't know if you expect that. Yeah, that's right. So this essentially depends on how the objective depends on the control or the regularity of the approximate with beta and gamma objective, which depends on their control. Control. So, as we can imagine, like if the beta and gamma goes higher and higher, then the regularity becomes worse and worse. Then it becomes more ill conditioned. But here we performed their tail approximation. So tailor approximation essentially helps to sort of like stabilize the problem a bit and to give you a smaller condition number, I would say, because the teleapproximation is. Say because the teleapproximation it essentially extracted the dominant information and for the high order terms or the complement subspace and you just like sort out those information. So in this way, it's sort of like a truncated problem. Let's say imagine like you have a big matrix, then you're going to truncate it at a certain low rank. Then the condition number you get is better than what you have for the large. Better than what you have for the large metrics. So, this way, the optimization problem, if you solve it like with larger beta and gamma, it's still relatively stable. So, but like, don't beta and gamma have an effect on still on the, on, let's say, the matrices and the matrix vector products and the linear algebra that's sitting behind all the fancy algorithms? Like, those gammas and betas, whatever matrix you're using, however you generate it, I mean, they should be having some kind of effect on the. Effect on the miracles performed. Yeah, they have the effect on their accuracy. On the accuracy. Teleapproximation. Well, I shouldn't say this. So teleapproximation is sort of like independent from this beta and gamma. But remember, if you require that this beta and gamma goes to be quite big, then you have quite a sharp inner face. And for the sharp inner face, you require more samples to resolve that. But teleapproximately. Samples to resolve that, but teleapproximation is limited by its accuracy at the interface. Yeah, I would say like the accuracy is one consideration. But the condition number we observed, at least from the numerical result, it's relatively stable in terms of the number of iterations for getting the best optimization. I mean, you're using tracking type objective functions, right? Mostly, complicated objective functions. Objective functions. Yeah, well, for the chance constraint, it is a tracking type. We have both objective and also a chance constraint. Oh, yeah, okay, you have a probability in there as well. You have a probability, that's right. Okay. So that is quite complex. Yeah, I mean, I'm a bit as surprised as Johannes. I mean, Drew, if you saw Drew's talk yesterday, he didn't mention, well, he mentioned, but he didn't show one part of the American results in our paper, were basically. Where basically we compare, let's say, penalty smoothing with this primal dual approach. And at one point, that penalty parameters get very long.