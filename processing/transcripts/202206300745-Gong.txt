So, today I'm talking about my recent work about partial tail correlation coefficient applied to extremal network learning. This is a joint work with my colleague Peng and Professor Thomas Opis and Rafael Husser. In the context of modeling the extremal dependence using graphs, there are recent works in the literature. In the literature. For example, Huang 2019 provides some explanary tools such as KiNet Work for extreme dependence modeling. And they apply this method in analyzing the maximum precipitation during the hurricane season in the US. And Angel Kohl and Hin 2020 introduced the new notation of conditional independence adapted to the multiple right pareto distribution. To the multiple right parietal distribution and the proposed parametric graphical models for streams. Gisibo and Kumberg and Crowley, they proposed several max linear model-based frameworks using graphs. And Tran 2021 proposed the Q-3 method, which is a simple and efficient algorithm to solve the latent river problem. Latent river problem. For the important case of three models, Angel and Vogu Chef 2020 develop data-driven methodology for learning the graphical structures. And Root Ger 2021 propose Husband-Reised graphical models under the multi-valued total positivity of other two, which they call it as MTP2 framework and angle. Framework and Angelkov and Ivanov has a very nice review paper on the recent development of the graphical models for extremes. The existing graphical models for extremes rely on asymptotically justified models, but sometimes this could yield some dimensionality issues or the lack of general graph structures. So the goal of our work is to complement. Of our work is to complement these approaches by defining the new concept of the partial-tail correlation by analogy to the usual partial correlation. And we define a new coefficient which can be used to estimate the general extremal networks and minimal modeling assumptions. Although the partial tail anchor relatedness is a weaker assumption than the conditional tail Assumption and the conditional tail independence, it can still provide some interesting insights into East Remodendent structures and guide modeling choices for the East Planary data analysis. So our work is hugely inspired by the work by Professor Kuli and Tibot, 2019 paper. Thanks to his talk yesterday, he already introduced the transform linear framework. The transform linear framework. Here, we will just introduce briefly some of the essential elements we used in our paper. So, the transform linear algebra framework is within the regular variation framework, and then it utilizes a transformation to define a vector space on the positive odd. The transform linear operations applied. From linear operations applied to the regular varying random vectors can preserve the regular variation, and they also proposed a metrics which will summarize the tail dependence. So the inner product space is constructed with the help of a transformation or a bijection T is from the real space. From the real space onto an open site X, and by defining the whole product space with addition, multiplication, and other notations, they particularly utilize the one operation, which is priced as the t function, and it will have a negligible effect on the large values. Large values. So, with the help of this transformation, the regularly varying random vectors can be constructed on the positive option. If we denote z as the IID regular varying random variable with tail index alpha, we can construct a p-dimensional random vector x in this way, and x will have the same tail index as z, which is the alpha. Which is the alpha. So, this construction guarantees the regular variation preservation, and according to another paper, we also define the inner product of two random vector xi and xk and further introduce the distance in the inner product space. Space. So, also, as mentioned these days about this matrix, it's called Tail Pairwise Dependent Matrix. It's called TPDM. So, you can understand the TPDM as the covariance matrix in the non-extreme setting, which will summarize the dependence information. If we choose alpha equals to 2, and you equals to 2 and utilize the L2 norm when making the radial or angular transformation. The TPDM is defined in the following way for each element. So the TPDM will have useful but implete dependence information for the random vector. There are very nice properties for the TPDM. For example, the diagonals describe For example, the diagonals describe the scale and the sum of diagonal elements is equal to the total mass of angular matter. If the element of the TPDM is zero, it means asymptotic independence. And in our framework, we further request the TPDM of X to be positive definite, which will guarantee the existence of the inverse. Inverse. So now we are going to introduce our partial tail correlation coefficient, which is the PTCC. So if we remove xi and xk from the random vector x, and we use the remaining p minus two-dimensional vector to predict for x, i, and x, k. The bias linear and biased predictor predict. bias predictor predictor for Xik can be expressed as following and using the projection theorem and our notations we can derive the formula for B height and we can also calculate the prediction error which is the E and the TPDM of the E will have the analytical expression in formula one. Expression in formula one. So we define the PTCC of two random variables xi and xk as the TPDM of the residuals with all the linear dependence of other random variables removed as expressed in the formula one. And also, given the rest of the random vector, xi and xk, they are partially. and xk they are partially tail uncorrelated if the ptcc of xi and xk given other rest equals to zero so notice that the residues of two partially tail uncorrelated random variables they are necessarily asymptotically independent in the following proposition we prove that the following two statements are equivalent statements are equivalent even in our case. And in the colour, we summarize our result. So if we denote the inverse matrix of the TPDM sigma at Q, if the element of Q ik is zero, it means the PTCC of Xi and Xk is also zero. Xk is also zero. So zero PTCC means partial tail uncorrelatedness. Also, we exploit our framework in the graphical model formulation. And in our study, now we are just studying the undirected graph. So this is the standard definition of undirected graph. undirected graph and our method can handle all different cases for the different types of undirected graph phonel. So if there is an edge missing in the undirected graph, it means the variables xi and xj are partially tail-uncorrelated given all the rest of the variables in the graph. And we write the notation like this. The notation like this. So, our method works for general and directed graph structures, for example, a tree, a decomposable graph, and a non-decomposable graph. To learn the sparse structure of high-dimensional extremes, which are the desired properties because of the parsimony and the interpretability, we use Protability. We use two different methods for learning the graph structure, which are the extremographical assault and a more state-of-the-art method, which is called structured graph learning method using Laplacian spectral constraints. And first, for the extremographical ISO, it's basically. So it's basically the extension of the traditional method and we have one parameter lambda where if we increase the value of lambda it will increase the sparsity level. So choosing the ideal value for lambda is very essential because on the one hand we want to enforce sparsity in the graph. Sparsity in the graph, but on the other hand, we also need the estimated theta should be well defined and estimated stably. And another method is called structured graph learning using the Laplacian spectral constraints, which will enforce further structure in the learned graph structure. But you can understand this as You can understand this extension of radical associates, but here we have one more parameter which will control the connectedness of the graph. So if you similar to like so if you increase the value of lambda, the sparsity level will increase and if you have a bigger value for beta it will enforce the level. Will enforce the level of the connectedness for the graph. So, in the simulation, we run several cases and here just to show that we can learn consistently the true graph structures. Here, we just show the result for the extremal graphical Lasso method and by changing the pinal T. The final t parameter lambda, we can change the level of the sparsity. And actually, in all the cases, if we know the correct number of the edges in the graph, it will both methods will recover 100% sure the true graph structure. So, to showcase our method, we Showcase our method, we apply our model to two data sets, which one is in environmental risk analysis and another one is in financial risk analysis. So for the environmental data study, we use the benchmark model of the real-world discharge data from the upper Danube basin. Where the true underlying physical reward flow network is available. And the second one, we apply our method to exploring the historical global currency exchange rate data for different historical time periods. So we include two different economic cycles and the COVID-19 period and the most recent. period and the most recent 2020 Russian invasion of Ukraine period. For the first data application, there are 31 stations as shown in the map and here are the true physical river flow connections and you can also see it's actually directed graph shows the direction of the river flow. So, using our method, first, we estimate the TPDM of the river network and we use the graphical assault. And here, the heat map just shows the volts of edges using the graphical method by tuning the parameters. And then using the second estimation method, the spectrograph Laplacian method, we We have different choices under the parameter settings. As explained before, there are two parameters to tune, which is alpha and beta. So under different settings, we also calculate the voting map for the edges. So like the darker the color, it means these edges voted most of the times. So here are the results of So here are the results of our method. The first one is using the extremographic collaso method, and the second one is using the spectrograph Laplacian method. So the thickness of the edges represents like the number of volts, which is proportional to the number of volts it was counted. it was counted and the the result for the from the sgl method is quite quite good because it basically recovers most of the true connections and there are like more edges for the is true more graphical so method so we draw the all the edges until no nodes is left Until no notes is left isolated. In the financial data application for the first two economic cycles, for example, in the first one from 2009 to 2014. So here basically we retrieve the historical currency exchange data and we fit an R gut. Fit an R gauch model to remove the temporal dependence and we obtained the we get a negative log return first and we remove the temporal dependence and we fit the standardized residuals to the model. So we can see, so this represents the risk network of the currency exchange rate. Currency is change rate. And at a different time period, you can see, for example, the British currency and the Turkish currency were in the middle of the risk networks. And in this 2015 to 2019 period, the Australian, yeah, Australian and this is Saudi Arabia. They were in the center of the Center of the risk network, and they were quite connected to others. And for the COVID-19 period from the beginning of 2020 until this year earlier, you can see like this is Argentinian currency. And also from the news, we know that they were experiencing economic. Economic depression and it's not recovering yet. So it is reasonable, it's in the center of the risk network. And for the most recent event, we were expecting like because of the sanctions and other policies, the Russian currency would be isolated from the global currency network. Network. And that's all. So thanks for your attention and welcome to any comments. Thanks, Jan, for your talk. Are there any questions for people on the side or people in Zoom? Yes, there is one here. Yeah, thank you. Yeah, thank you for the very interesting talk. Could you comment a bit more on what this voting was about that you had in those graphs? I'm sorry if I missed the details, but you were saying that this is sort of the that there's some voting in this heat map here. So what exactly do you mean by voting here? So for example, in this method, we have different options for alpha and beta. Options for alpha and beta. So this like maybe 400 different sightings. And under different settings, we have results for each sighting. And we just count in each result how many times the IJs are calculated or returned. If it exists one. It exists one edges or not. Okay, so example: here's like 60 percent of all the results confirms one connection like here. So the both works like this. Okay, thank you. And do you have any theory? Do you have any sort of results on recovering the true graphs or anything along those lines? Or is it still in progress? You mean if we prove the consistency of the of recovering the true graphs? Yes. Actually not. We have some hints from like the traditional graph theories using these two methods, but we haven't proved in theory about the consistency. Only we run a simulation to see if we would ever recover any. Would ever recover any wrong graph, but actually, not. But we haven't got a theory yet. Okay, thank you.