Thanks, Renee, and thanks everybody for coming. Before I tell you what we'll talk about, I just want to thank the organizers for putting the workshop together and inviting me. Here are some pictures I took yesterday. I went for a run in the morning. So this was a gecko. I don't know if you've noticed, but there are geckos around. So this was hanging out just above my hotel room door. And it was unexpected and quite nice. And quite nice. Then there's all this street art that I think a lot of you have seen. This was actually my favorite from my run yesterday. And then this is something more classical. So that's a candle, but I tell myself it's a camera and a selfie flash. So it's like a, you know, it's like a 500 years old and a five-year-old version. And then this is, I think it's called an agave, right? Is that what this is? So, this here, this extends, it goes up many more meters. It's going to quadruple in height. If you haven't seen this plant before, that's how it dies. Yeah, it goes up very high, and then it collapses, and those are all seeds, and that's how it spreads. So, it's pretty awesome. I didn't know those grew here, and I was pretty happy to see it. So, yeah, thanks for putting together the workshop. And for, I think many of us have been to Banfin Canada, but this is my first time at the Mexican. In Canada, but this is my first time at the Mexico Bin. Okay, so what I'm gonna, what I decided to talk about also because some of the people here have seen me speak about versions of this, was a bit of a retrospective because I might change the way I do gradient descent analysis. So I'm gonna talk about work from five years ago and from last year. So it's called feature learning. I'm gonna compare and contrast some results where no feature learning happens, where the computation is. Happens where the computation is very good and some results for feature learning happens and the computation is very high. That's sort of the theme. It's not really a formal computational statistical trade-off that people are used to. Usually, when people use that term in a talk, then the talk is just about statistical query. Just for some reason, there's this bijection. This will be a talk about upper bounds mainly. Okay, so this is the motivation. This data, this looks a lot like what Renee talked about. This looks a lot like what Renee talked about, but this is the two-dimensional projection. This is data in D dimensions, where actually the correlation between points and clusters is, for many points, a lot higher than within clusters. And this is the two XOR data or the two parity data. I'll give the formal definition of this data set in a second. But the amazing thing is that gradient descent, as it proceeds, all the neurons collapse and they go in these four directions, which in a certain rigorous sense, I'll tell you in a second, is actually the optimal classifier for this problem. Classifier for this problem. So it seems to be the case, and the knowledge that people have built up, or the intuition people have built up, is that with nice data, gradient descent is able to learn nice features. In this talk, I perhaps mistakenly tried to focus on what happens for general data. And the story is that it really looks like Gradient System has trouble in general data and has to pay either in computation or just fails. So the talk will have essentially four parts. Essentially, four parts. First, I'll tell you about the setting. There's going to be two settings: one which has no feature learning, but we get a pretty strong result with an easy proof, to be honest. So this is the old work that when it came out, I thought, well, this doesn't have feature learning, so it's not a big deal. But now the proof is so clean and the computation is so low that I kind of like it. Then I will talk about something where feature learning happens, but again, we're going to have to. Something where feature learning happens, but again, we're gonna have to pay a lot of computation. And then, based on time, and you know, feel free to ask as many questions as you like and treat this as a buffer. I will do some proofs. That's the outline. So ideally, it'll be something like 20 minutes, 20 minutes, 20 minutes, but it'll probably end up being like 25, 35, and then no time for questions. Okay. Okay, so it'll be a little bit more clear, and especially contract. So, it'll be a little bit more clear and especially contrast with Renee's talk in just a moment. So, the first thing I want to say is that the setting I'm in is the zero training error setting, zero training error reached. And for many classification tasks, this seems to be reasonable. For a long time, the way people practically, I don't know if everyone's familiar with this, but for a long time in practice, the way people did deep learning was modify everything about your problem setup until you get zero training error. That's step one. Do whatever you can. Error. That's step one. Do whatever you can, tweak the architecture, tweak the data, get zero training error. Step two, then tweak things to, while maintaining zero training error, get zero, get near zero test error, and then you're done. I don't know if everyone's familiar with this. This used to be the industry methodology for deep learning. That's not true anymore. And to be clear, what I just said motivated a lot of things like the interpolation concept and double descent and a lot of this work. That's not true anymore. For LLMs, they do not reach. They do not reach zero training error. In particular, if they had the choice between spending more money doing more training on the same data and perhaps getting zero training error, or spending more money collecting more data and running on the further data and doing ablations on other data sets, they choose to not get zero training error. Everybody follow what I'm saying? For the budget they have, they could choose to also get zero training error. They do not make that choice. They choose to get positive training error. They choose to get positive training error. So it is a fundamentally different paradigm where we could choose to do what we did before, perhaps. Okay, so this is fundamentally a wrong paradigm for LMs. So that's just something to keep in mind. You can treat this talk as being classification problems purely and not the conditional probability estimation of LMs. Okay, I'm going to do binary classification. If you're curious about multi-class, I can tell you in which parts of the talk it does go through and which parts it doesn't. Which parts of the talk it does go through, and which parts it does not go through. For most of the talk, it goes through. But I'll just do the logistic loss. And I'll be doing gradient descent. And yes, it does keep me at night that Adam actually is qualitatively different. Not quantitatively different, but it's qualitatively different. And we had one talk with Adam, but yeah, the jury's out what it's actually doing. Okay, that's the basic setup. I think the key thing is this. This is not an LM relevant talk. Okay, cool. Okay, cool. Okay, so let me talk about the first model. This is the model with no feature learning, and it's the slides kind of busy, but this setup is quite clean, and the proofs are very easy. Let me just remind you what regular linear separability is. So again, I'm in the zero training error setting, and the concepts I use to analyze low training error will be based on this linear separability notion. Notion. So, linear separability, to my knowledge, originates with the work of Novikov in 62 with the analysis of the perceptron method. And the basic picture is this geometric one. You have these two classes. They can be separated by a linear predictor. A linear predictor gets zero training error. And amongst all the different choices, you choose the direction which maximizes the minimum interpoint distance, okay? Minimum distance to the separator. So you try to maximize this distance. You try to maximize this distance. You look over all the data points, find the closest one, try to maximize this. The classical notion of linear separator from Novikov, pretty incredible proof that he wrote for Perceptron. Here's a formal definition, a unit vector, and then its margin is this minimum over the points. Okay, so that's a linear predictor. So our first notion of Of our first baseline notion of some kind of complexity measure of a linear predictor of a predictor, sorry, which we'll use to analyze the setting of no feature learning, is basically linear separability. So let me just tell you the punchline first. So it's going to be linear separability in the initial RKHS. So let me just explain this. You start training, you freeze your network, and you can define features based on that network. And you look at the Based on that network, and you look at the corresponding RKHS and the corresponding sublinear predictors. Okay, so that is a non-linear predictor. You can get zero training error for essentially any training set. So it's kind of in the deep learning regime, but it has no feature learning. Okay, so again, I'm defining for you my margin notion. All of our theorems will use this margin notion in the theorem. The algorithm will still be gradient descent. It'll still be learning features, but the guarantee will be against this frozen. Guarantee will be against this frozen feature notion. Okay, is that clear? I'll explain the right column in a moment, but just so the intuition is clear. I'm defining a notion of margin, which is with no feature learning. And I'll use this in the analysis, but the algorithm is allowed to change features. This will be a baseline. Okay. So, yeah. Let me. Oh, sorry. You as the. So, again, there are two things: there's the definition. Two things. There's the definition of the margin, which is a complexity measure effectively we use to measure the complexity of the problem. But then when we do the learning, it's going to allow things to change. Okay, so our predictor in particular is going to be this thing. Okay, I'm going to have a two-layer ReLU network, and I'm only going to train the inside. So by training the inside, it's a non-convex problem, and it can do feature learning. Okay, so I'm training the inside. Training the inside Gaussian initialization. This should actually be a this is supposed to be one over root m. Sorry, it's plus or minus one over root m on the on the outer layer, but that's fixed. It's not trained. We're only training the inside. So the parameters I'm training are these right here. Okay. And this is the initial feature mapping and just the gradient with respect to the parameters I can change. And then linear separability. Then linear separability is defined as: I have this u and I interproduct it with those gradient features. Just a technical note: so typically when people construct this space, they put a Frobenius norm constraint on U. For this talk, I'm going to put a 2-infinity norm constraint. For interesting technical reasons that aren't fully understood, the 2-infinity is basically equivalent to the Frobenius norm for this geometry. That's extremely technical, so maybe just ask about it at the end if you're curious, but this is not something that people really understand. But this is not something that people really understand. But the bottom line is that it's it we have these initial features defined by these gradients, we have a corresponding RKHS, and we talk about linear separability over that, which basically separates any training set. Okay, so no feature learning, and here we'll be able to get a very, very clean result with a super trivial proof, like a 10-liner. Right. Right, so it's a little sophisticated because I didn't even specify the dimension. But before I explain the definition, does the intuition make sense? The intuition is I take the initial features, I freeze them, and I just look at linear predictors over those features. Is that fine? Okay, then to make, to parse this entire expression, so this thing is an m by d dimensional object. And so what it's doing is for each of these RELU, if you take each individual ReLU, If you take each individual ReLU and take its derivative, then I have a different vector for each one of those. Honestly, it's very hard to parse the exact expression. It's easier just to think about it as linear separability in the RKHS. And it's also better to think about it that way, because then this part of the talk has nothing to do with two layer. You could use a friendly architecture. Question. So when you say So, when you say learn till some time and then freeze, is that does anything depend on the time till you learn? Yeah, sorry, I didn't, I didn't, just like an audio thing, I couldn't parse your sentence. Can you just say it again? Ah, sorry, I just said that you said that learn till some time and then freeze the layers. What is that time? And does the result, do the results depend on the layout? Okay, so I'll give you, this is there's no theorem on this page. I'll give you a theorem in a couple slides. I'll give you a theorem in a couple slides. When I give the theorem, it'll be clear that we just run gradient descent on the neural network, and there's no freezing. It's just gradient descent. There's no cheating. Okay, okay. Thank you. Yeah, it's just gradient descent, no freezing, no variable learning rate, just gradient descent. But we're going to compare against the network that has the initial frozen features. Okay, got it. Yeah, maybe just re-ask the question when I give you the theorem. For this setting, I'm only telling you the For the setting, I'm only telling you the like the two margin definitions. So, this is the no-feature learning. You freeze the features and you define an arc HS. Okay. Oh, why does it make sense? Yeah, that's one of the secrets of this fancy term over-parameterization, but one of the concepts. Parameterization, but one of the consequences is that as you take the width of the network to be very large, it turns out that the feature mapping stays constant longer. Yeah, so let me put it another way, which is kind of interesting. If you look at the, if you take M growing and you look at the Frobenius one-norm ball in parameter space, okay, okay, sorry, I have a Frobenius one-norm ball in parameter space, and I look at the induced ball in function space, the ball grows. The ball grows. So it's m goes to infinity, and if I look at a constant for being this one-norm ball in parameter space, eventually I fit every function. So the NTK in particular is m goes to infinity as a universal approximator. So yeah. Are you satisfied or just like nodding out of more of attrition? Okay, so that was the first model. So, that was the first model, fancy linear separator. Now, we want to find something for feature learning. So, here's how we're going to do it. And here's a little bit more detail about that data I showed on the first slide. So, this is the 2xOR data. This goes back actually the papers by Leslie Valiant on the PAC model have something like this. So, this is when the problem became popular. But the version that became popular here is. Yeah, the version that became popular here is from Colin Wei and Tung Yi Ma and Jason Li and Cheng Liu. They have a nice paper from, I think, 2018 where they popularize this problem. And the way it works is you have data on the corners of the hypercube in D dimensions, normalize the unit ball, and then the label is the product of two of the coordinates. Okay, so if you compare this to this orthogonal setup that, or this separated, well-separated setup that Renee had, if we look at two data points, different clusters, as Degos infinity, or different clusters, At degauss infinity, for different clusters of different labels, we can find points whose correlation becomes one with degaost infinity, whereas within a cluster, they become more foggy. But there are exist such pairs of points. This is really a pretty nasty, nasty problem. And yeah, so that's just one. And if you study this problem, if you study what gradient descent does, it actually just picks four values. And what I'm going to motivate for you here is an objective function, a notion. Is an objective function, a notion under which these four values are the optimal solution. Just be clear: there's as many solutions as you want to this problem. There are uncountably infinitely many solutions to the training error problem. So we have to find some notion of optimality which is more useful. In particular, it gives us good statistical notions. And that's where we have this second model that I'm going to describe on the right. Okay, so as a first start, what I'm going What I'm going to look at is, I'm going to look at the margin defined in a nonlinear way. Now, now I have no gradients, I just look at kind of the regular margin. Okay, just letting all the V's vary and all the A's vary. Okay, now one problem with this definition is it's sensitive to M. So as you change M, you start ending up with weird things. Like I told you, this problem, the solution is for ReLUs. So if I, you know, if I. If I pick a number, if I pick m, which is congruent to three mod four, you get all these weird issues. So, to smooth that out, the way that this second notion of margin is usually defined as an integral, it's a little bit technical if you haven't seen something like this before, but it's a maximum over signed measures over the sphere. So I look at all possible ReLUs, the uncountably infinite set of different ReLU directions on the sphere, and I make a signed measure over it. And I make assigned measure over it. You can think of this as being a fancy, uncountably, infinitely wide network. I just assign all the things. And so for this RELU over here, or this 2xOR data, the solution ends up just being to pick four RELUs. So you pick a measure on the sphere, which puts mass equally on four different ReLUs, two of them with plus one and two of them with minus one. Okay, so it's a signed measure. It can put positive or negative mass. Can positive or negative math. It's just to avoid technicalities. Like I said, because this here, yeah, we just don't want weird issues like you pick a number which is too small, for instance. So here are the four values, then we make M3, whoops. You know, you give me a problem where the natural optimum has 100 value and I picked M equal 99. And Ethereum would have to deal with these artifacts. And Ethereum would have to deal with these artifacts. But these problems are the setting of my opening slide perspective of A and B and the algorithm, the medical development is exactly growing the number of neurons that you get. Yes, but by doing it this way, we don't have to depend on M. We don't have to say that there exists an amp M or anything. We just define this for any problem. So saying it another way. Saying it another way. Yeah, I don't yeah, I'm not sure who the exact definition is a little bit sensitive because this is sort of related to two homogeneity, so I'd have to see the exact definition and like the norm you put on the measure matters here. So but. But yeah, just to summarize, another way to look at these two different models is this is sort of an L2 notion, and this is sort of an L1 notion. This one promotes sparsity, and that's the way to think about it. Let me summarize what we've covered so far. Also, let me pull my phone out because the Zoom notification is occluding time. So, I don't actually know how much time we've used or have left. Okay, so. Okay, so I've well, maybe I should scroll down a little bit. But I kind of predicted that the last two slides would be a little bit too dense. So let me go over the two models again. So again, what is my goal? So my goal today is to summarize for everybody here two fairly standard models and ways to do analysis. Ways to do analysis that we can use for these computationalistical trade-offs. One is no feature learning. And I wrote it as this, here I write it as an integral. Doesn't really matter. It's just the RKHS from the initial features. There's no feature learning. And this is another, well, another way to refer to it is other than the NTK is that it's the so-called F2 space in box breaking cruiser dimension in neural networks paper. Paper and the feature learning setup is boxf1, and it's the thing I just told you about. So we have these. So if you give me, let me say it another way. So you give me a set of data points, and there are all these different ways that we can get zero training error. These two define two different particular choices of small training error, and we'd like to compare them and compare them to what gradient descent does. And here's a little bit of a punchline. So if you look at the minimizer, if you look at the solution you get over F2, for that XOR problem, this is the optimal sample complexity. So as long as you search over F2, you need to use D squared over epsilon samples. And if you search over F1, you need D over epsilon samples. So the bottom line, and in Bach's paper, he formally points out that F1 is a superset of F2 in the sense. Superset of F2 in the sense that it always minimizes over more things. And so, what this means is that you always get sort of more powerful representations. So, we should sort of expect this reduction in general. Okay, so what have I told you so far? So I've told you two sort of implicit function spaces attached to a two-layer ReLU network. And the question is, what does gradient descent actually do? Does it stay within F1? Does it stay within F2? Stay with an F1? Does it stay within F2? Like, what does it actually do? Okay, and it seems like we have the following picture. It seems like we have a picture that says that F2 gets not great sample complexity. That's this over here. And it seems like F1 gets great sample complexity, but we expect it to be a trade-off. So in particular, F1 encodes many NP-hard problems. So if you're able to show that F1 is efficiently solved by grain of descent, probably have a bug. Probably have a bug in your proof, but you may also have proof that p equals np, so congratulations. I mean, I'm not kidding, it contains np hard problems because it's a sparsity thing and it can, yeah. Okay, so and an interesting thing for me is exactly how the curve looks. So we expect that the worst case complexity in F1 with gradient descent is very high. Otherwise, you've proof equals NP. And then we know that it's low within F2. That's the first part of the talk. The first part of the talk, but I'm really interested in the curve. So, what's the interesting thing to study is if most problems actually we do something slightly worse than F1. So, let's say you run greatness over polynomial time. So it can't solve these empty heart problems. And maybe it does pretty well for a lot of problems. Maybe for most problems, it does as well as if it was fully given access to F1. Okay, so this is my motivation. I wish I could fill in this curve. This curve. Does everybody follow that philosophical question? I mean, it'd be pretty amazing to nail this down because, for instance, if radiant descent really sat around here, and let's say we could prove this for transformers and stuff, where the game starts being that transformers implement algorithms, not just predictors, but they implement algorithms. Then, for many problems, if you want to learn a polytime algorithm, you don't go study an algorithms book, you just run gradient descent on it. If this curve looks like this, that's what it'd be saying. It'd be saying that. What I'd be saying. It'd be saying that efficient algorithms exist just by running gradient descent. That'd be a consequence of this curve. So I'm very interested in this difference. Sorry, maybe I didn't follow. So what is exactly between F1 and F2? What is that function class that you're talking about? Right. So first of all, I do not know. So that's the first comment. But let me just make the statement more precise. So what I'm wondering is the following. My what I'm wondering is the following because my curves here are essentially the worst case complexity. So, what I'm wondering about is if it's the case that for most prediction problems, it's the case that in polynomial time gradient descent does something reasonable and it just fails for a couple of the really bad ones. The sin, gradient descent really is adaptive to the problem complexity. But the x-axis, to be clear here, is just sample complexity. I see. Yeah. And I just. Yeah, and I just drew pointers for F1 and F2. But there's another, by the way, did I answer your question or did they just say stuff? Hello? I'll try to parse it a little bit more. Yeah, I guess I didn't answer it, but yeah. Yeah, there's a question in the room. Go ahead. So you've got these two regimes, F1 and F2. You're also talking about gradient descent, and you're contrasting it to each of these regimes. So my guess is you're not solving either F1 nor F2. Solving either F1 nor F2 by gradient of descent? Yeah, okay, yeah. Perhaps that's part of why I'm confusing everybody. So F1 and F2 are just two function spaces. Meanwhile, gradient descent given data just follows some path. Yeah, and so and so the and the question is, is what gradient descent is doing, is there some well-defined function class where it's actually just minimizing some surrogate objective like a margin over that function class? So is what gradient descent doing? Is it equal So, what the gradient descent is doing is equal to minimization over some function class. It's definitely not F2 because, for anybody that's run an experiment in their entire lives, they know that you exit the NTK in one step, always. So it's definitely not F2. Is it F1? No, because it solves NP heart problems. So it's doing some other thing. It might also not be minimizing a margin objective. From people that attended Fanny's amazing talk yesterday and also saw Renee's comments during it, it turns out that margin is often During it, it turns out that margin is often the worst thing you can do. So there's probably some other objective. But yeah. I don't understand what the blue and green seller are. Yeah, so all I'm, so again, I'm also hedging a lot in this talk because my theorems are going to be for the worst case complexity within a class, but it seems that what, so what I want to capture here is I want to capture the concept that is it the case that for most fairly difficult Is it the case that for most fairly difficult problems, in other words, problems where to get good sample complexity, we need to learn features? Is it the case that for most of these problems, gradient descent does learn pretty good features? Or is it the case that for most of these, it doesn't learn good features? Let me give a more concrete example because I realize people are quite frustrated by how vague I'm being. Wait, let me answer this. Let me just send my sentence and then we'll go on because I think it'll clarify a lot. So for this 2XOR problem, so first of all, it is known. So, first of all, it is known that if you stay in F2, in other words, you do whatever you need to in order to prove that you stay in the NTK, it is known that gradient does match this lower bound exactly. It does get d over squared over epsilon. It's the first part of this talk. The d over epsilon was not proved by me. It was proved by Margolet Glasgow. So she formally violated the SQ model, and she was able to show that gradient descent does learn features for the 2xOR problem, and is able to get miraculous sample complexity. And if you plotted it in this chart, it's down here. She gets very efficient. She gets very efficient polynomial time, polynomial time with a small exponent, and she's doing exactly what you do by minimizing over F1. Here's the thing: just consider the k parity generalization of 2xOR. We take k bits that are unknown and product them together. It is an open question what gradient descent does for that. And people that are experts in 2XOR fight over whether it does the efficient learning. So, I think, I hope that clarifies your question a little bit. That, so for KXOR, we really don't. Or kicks, or we really don't know where the sample complexity horizontal will lie. And so I'm hedging on my curves because I'm trying to capture the concept that I wish we knew for different difficulties or problems. And I don't know how to characterize what it means to be difficult, but I wish we knew exactly what great instant does. Conceptual, yeah. The pot is very conceptual because we don't know how to define all these things. So is there any notion in which F1 and F2 are kind of like the? F1 and F2 are kind of like the neural network versions of Fanny's L1 and L2. The amazing thing is that, yeah, I told her this afterwards, and I told Renee afterwards, so Renee had this question where he was wondering if there was some use in studying this L1 that she does. And so I told her this after the talk, actually. So Margolet Glasgow's sample complexity is one over log D. over log d so to get or sorry the training error she gets is one over log d so to get arbitrary training error you make d to go to infinity we saw also another one over log d yesterday we saw one over root log d in fanny's talk so actually it seems that fanny's l1 is actually very close to what's happening at f1 looks an l1 perpellator in some sense yeah so it's very close there are an infinite number of technical details that might sabotage what we're talking about but this is something that um she's aware of That is a nice observation that you just made. Okay. So are we happy enough? Maybe if I show a theorem, like. Okay. So let me just do GD on F1 or F2. And I think it was Amber Paul who was asking a lot of questions. Perhaps he'll be more, if you're still around, perhaps he'll be happier after you see this slide. Perhaps you'll be happier after you see this slide. But this is an old paper. So, my predictor is, we're back to F2. So, this is the predictor. Okay, and I'm running gradient descent on the inner parameters. Okay, running gradients on the inner parameters. And this is what I'm going to use as a comparator. I'm going to measure my performance using this quantity, this gamma 2, this initial RKHS margin. Okay, so here's a theorem. For all epsilon greater than zero, the probability one minus delta. zero, the probability one minus delta. If you satisfy these conditions, so your width needs to be order at least one over gamma to the eighth, where gamma is this margin. Your samples need to be one over gamma squared. Your time of number of gradient descent steps needs to one over epsilon gamma squared. Then you get test error less than epsilon. Okay, so this is a test error guarantee to be absolutely clear. All the guarantees I give today are test error guarantees. They're not training error guarantees. So if you're on grading descent, So if you're on gradient descent for this many steps on this many, at least this many samples, and your width is at least this, then you get epsilon test error. So to be absolutely clear, we're not freezing any layers. I mean, we're freezing the outside, but we're not freezing the inside. We're just using the frozen features as a comparator. And because the width is sufficiently large, we don't move enough so that we can use that as a comparator. One thing I'll note is that if you look at any NTK paper, That if you look at any NTK paper or whatever, any basically any paper you open, the width of the network is an scales with number of training examples. There's always a number of training examples. It's not here. It just depends on this gamma. Okay, does anybody have any questions about the theorem statement? So you run gradient ascend, the network train on the inner layer. I didn't say the step size, but the step size is actually one. You just see step size one. And And yeah, you get epsilon test there. And you need this many samples. So this looks abstract for a second, but I'll clarify this in a moment. Any question about the theorem setup? This theorem is really no cheat. Yeah, go ahead, Ambar, you have a question? I'm guessing. So does the margin depend on data? Right, because your question is because of the test agreement. Right, because your question is because of the test error guarantee, right? Yeah, I was just wondering because so I think in Ethereum you want the sample sufficiently many samples, but once you increase the samples, would you also change the margin? Right. So for this particular one, I have a typo in the slide. I should have written this not as a minimum over. This margin holds for almost every example in the distribution. So it's a condition on the support of the distribution. So if you want the formal version, So, if you want the formal version, I can just paste it in really quick. The formal version is for almost every x, y pair. So, for every x, y pair in the distribution, we have this. Okay, is that, are you happy? That was your question, right? Yeah, okay, so I was just wondering, you know, so because the margin depends on, you know, the data, and but you're in a theorem, in the premise that you want sufficient. In the premise that you want sufficiently large number of samples. So once you increase the samples, then would you also change the margin? Let me explain the definition. I've corrected the definition. It's for almost every example in the distribution. So now I have the fancy linear separate in the RKHS, and I have the positive class, and I have the negative class. And these are classes. So these are infinitely many examples. There's a support of the distribution, and now there's this margin. So the distribution is sample. Margin. So the distribution is sampled. So with probability one, anything I sample is also going to satisfy this margin. I see, I see, I see, I see. Ah, I see. So I see. I mean, it's my mistake. I mean, it's my mistake. Until your question, I defined everything in terms of a finite sample. So it's my mistake. But yeah, everything. No, no, I think you clarified it perfectly. So now I think the margin is essentially depending on the distribution. It's not on the data. Yeah. Yeah, but still it's my mistake. Thanks for clarifying. And Ambar, you were asking like 50 questions earlier. Do you have another one? Not yet. I can't tell if that's good or bad. Go on. No, so there's a question in the room. The question is: is this for linear separators? No, this is for these RKHS separability. So I can give you a couple examples. You a couple examples. So, in the NTK, it is a universal approximator. So, this separates any data set or any distribution. So, it separates any training set where for every x, you have a unique y. And it separates any distribution where the conditional probability of a label is either full on plus or full on minus. So, it's not linear separability. It's basically anything. Yeah, you need the Bayes optimal to be pure. It needs to be either fully on one class or fully on the other one. Yeah, we have some theorems somewhere where we say that this is universal approximation in a very strong sense. So as long as the distribution is measurable, then I mean Borel measurable. Okay, so some remarks about this theorem. And so again, this is an old theorem, but in retrospect, it's my favorite. It's my favorite theorem I've produced for gradient descent on neural networks, even though it has no feature. So, first of all, the one over epsilon gamma squared for all the k-parity problems, it matches the sample complexity lower bound from the SQ model. Okay, so for those of you who aren't familiar, there's a standard model for getting sample complexity lower bounds in machine learning. It's called the SQ model. And for all these k-parity problems, it gives a certain lower bound on the sample complexity. Certain lower bound on the sample complexity, the only way I know how to meet that lower bound is this proof. I don't know of any other way to do it. And the way you do it is you just grind through this theorem and you calculate what gamma 2 is. You calculate gamma 2 and you plug it in, you match the sample complex lower bound for S2. And again, I don't know any paper that came before or in the five years after that can get the same sample complexity. There are many feature learning papers actually that get a worse sample complexity than this. So that's another reason I'm pleased with this proof. Even though it's just completely trivial. But even though it's just completely trivial, it can get, I mean, this proof is super short. It's like a page and a half with all details. And the concept is trivial. I'll show you in a, I'll find a way to show you the, have time to show you the proof. Okay. Yeah, so there are a number of weaknesses in this theorem, though. So one is that one thing we're really upset about is the width is one over gamma to the eight. It really should not be one over gamma to the eight. So in fact, if you So, in fact, if you truly did learning over the RKHS, so if you froze the features and you just did gradient descent on predictors that interact with that frozen gradient, you could do no math. You could copy paste the Novikov perceptron proof literally, and you would get one over gamma squared is the width you need. So, if you just froze the class and just copy-pasted Novikov's 60-year-old proof, 60-year-old proof, you would get a better version of the theorem. Kind of sad. I should say that I have a follow-up work from a few years later that reduces the width. However, my proof is complicated and the running time increases. So, yeah. So, beating this seems hard. Also, we know that some dependencies. We know that some dependence on the width is necessary. So we were able to show that for the 2xO problem, you need a width of at least one over root gamma two. So there is some fundamental relationship between this margin and the width you need. I should say that when we started this proof, we were just trying to write down some proof that did not depend on n in the width of the network. We didn't expect to meet the sample compound, we didn't expect to meet the SQ lower bound. That was just a shock, a total shock. A shock, a total shock. Oh, yeah, and I said lies here. So there are some lies in what I showed you. So in the paper, we don't actually get this sample complexity for GD, we only get it for SGD. That's a long, funny story, but if you take this paper, if you take the 19 paper and use the Rademacher complexity bound from my newer paper, you get this. So formally in our paper and from 19, you need to use SGD to get this theorem, but it's It should be this, and you just use this generalization bound. And yeah, the proof is just a very easy trick. Okay, so any questions about this theorem? Yes. What's up? Just double-checking. So the F in there. Yeah. That is the Relia network trained with gradient descent. Gradient descent after T iteration. Yeah. And so simply what you have is you actually have a guarantee for gradient descent for the actual network you were training. It's just that the conditions for the theorem depend on a margin with pros and yeah. Yeah. But I should say that the conditions of the theorem or the setup impose fairly strong geometry on what happens. Geometry on what happens. So, for instance, these predictors where you train only the inner layer, they force the weights to actually move slowly, not just in Frobenius, but actually in two infinity. So the max two norm that you move is very small, which is also a shock for me. If we did two layer training, this would be false. If we do two layer training, then single neurons shoot off. So, yeah. Okay. Could you comment a little bit on maybe you don't have the results, or you're going to come back to them later, but since the theorem is really about gradient descent on the network that you want, and the main role of gamma two is really on defining what's the minimum number of samples and duration, then what do I lose by using gamma two? What do I lose by using gamma two versus another measure, I guess? Oh, um, yeah, so my yeah, there is no feature learning, but right, but for the theorem, you are learning. Right, so within the proof, it's sad. So, within the proof, as I mentioned, if you copy-paste Novikov's perceptron on the frozen features, you get a better bound. And within our proof, we just limit the way the proof works is you limit the damage caused by feature learning. So, my belief is that if we had a better margin. My belief is that if we had a better margin notion that allows a little bit of adaptivity, then we would maybe be able to get all these constants down. So yeah, I think even in this regime where you make the width large that you don't move too much, I think that with a better margin notion, we could shave all these other things down. I think it would have really strong benefits. And that's another reason I hedged in that first plot. Like I really do not know what the right function classes are. F2 is clearly wrong even near initialization. So let me tell you about a little bit about F1. Let me tell you about a little bit about F1. This paper is being held up for posting on archive because while it's for two-layer networks, I tried to make the analysis general. So, one of my students is giving the corollary for transformers. It's not going to be, I'm just going to tell you right now, it's not going to be some super strong transformer theorem. It's going to be, yeah, kind of okay. So, I'll just state for you the two-layer version. I have to define some notation. Um I have to define some notation and let me decide how painful I make this given that we have 15 minutes left. So Okay, so let me tell you what I have told you so far. One is that I told you about the F1 margin. It was this integral where we have these sparse like L1 features as we were, we have this sort of L1 constraint essentially on the weights. That was kind of the nicest way to present it. It was a comment in the back. Comment in the back. And I'm still doing gradient and the logistic loss. Now I'm training two layers. And I just saw that I have a mistake. It's actually AJ, BJ, BJ. So I'm training all the layers. And let me zoom in here actually, because I want to make it clear. I'm doing a little bit of slate of hand. So if you look, let me explain what's going on here. Yes, it's true that I wish I could have proved this for a standard two-layer network that did not have the normalization. The normalization. I didn't realize this until recently, but I don't know how to handle that case. So, what this is, is this is an idealization of batch norm. So, for the inner layer, I normalize it by its norm. Okay, so it's idealizing batch norm. And if you look up the code for batch norm, it always has a multiplicative parameter that comes along with it. So, I have two parameters on the outside. One is the batch norm multiplicative parameter, one is the regular outer layer weight. One is the regular outer layer weight. And when the paper hits archive, I make a big deal that all modern networks have this multiplicative structure on the outside. This is actually true. I've checked like 50 GitHub repos. So, I know Lama 3 does this. Okay, so this is the architecture. It has some cheating from what I said before because I normalized the inside. I didn't tell you about that before. And it's crucial for the proof. Okay, so now we can write a theorem against F1. That's the point. The point. And I have two theorems with different computation. And as you notice, even though it's a stronger theorem because it's for F1, I mean, just look at how gross some of this is. So I have two sample complexities. That's how to parse these two lines. So for both of them, I get zero train. I get epsilon test error, but I get different sample complexities. So the first one blows up by a factor D. So the second line is what I had for gamma. Line is what I had for gamma two exactly just replaced the gamma one. The first line I pay a factor d. The second line is morally searching over F1. The first line is doing a slightly more relaxed thing than F1, a little worse. And D for a machine learning problem is like infinity. So D is actually very bad. For both of them, I need the width to be exponential. So disaster. And for if I search over F. And for, if I search over F1, I need the time to be exponential. And if I pay the D factor, I just need the time to be just without the exponent. Okay? And by the way, these are all loose. These are just what I was able to prove. But I really have no idea what these should be. Okay. Uh go ahead. Yeah, is there any notion in which you could try to get like a two-day sort of thing out of this? thing out of this like for the first one over epsilon gamma one squared iterations as long as your sample complexity is this so you have some stronger sample complexity yeah the second line actually is a two-phase proof the first the second line uses the first mess a lemma but i don't know if i need to do that or if it's just an analytic artifact i don't actually know if i have two phases here bad What? Sigma? Oh, these are all RELU. Yeah, sorry. These are all RELU. Yes, it does through homogeneity. If you're saying that this morally looks like just a regular ReLU network, as then like the division by VJ and the VJ cancel, is that sort of what you're implying? It turns out that geometrically, they're very different. So if you run gradient descent on this, you get different dynamics. Ascent on this, you get a different dynamics than if you had done the commute. Wait, did everybody follow what I just said? Or did I just say like random garbage? Did you understand what I said? Yeah, it's shockingly different, actually. So just to be clear to what we're discussing here, if I look at the trajectory of the particles, I can define two particles and two networks. One is I look at the trajectories of these things. Okay, I just plot those in d-dimensionals. Just plot those in d-dimensional space, kind of like Renee's plots the other day. I can also train a network that just says AJ and then this thing. I can train both layers, and I just look at AJ, Bj. These two particle paths have different dynamics. There's a little bit of algebra you can do that reveals it very clearly, but yeah, so unfortunately, this commuting that I believed was going to have some strong consequences is. Have some strong consequences. This is actually kind of a red herring. Everything is learned in this setup. Yeah, I'm learning all three. You can view it that way, sure. Somehow, yeah, the geometry is very different. Well, the geometry is different. Okay, so a few things about this. So for this K-parity problem, I mentioned. This k-parity problem I mentioned, which is the k-bit generalization of the 2xOR, you can just compute the sample complexity by calculating what gamma one is, and you get these two things. Okay, so I wrote the two different versions for based on which line of the theorem you get, but let's just look at this one and just remember that the other one is D times bigger. So, if we do if we do optimization over F2 for these. Over F2 for these k-parity problems that people are devoting thousands of papers a day to, then the sample complexity is d to the k over epsilon. And if you do over F1, then you get K D squared over F. Okay, so this is a ridiculous saving. So again, what I'm showing you here is the output of this theorem. You calculate gamma and you plug it in. And for the people online that had the same concern earlier, yes, I messed up. Yes, I messed up. This should not be minimum over a training set. It should be the minimum over the support of the distribution. So, for k-parity, it's very easy to calculate what that is. And you get these two things. By the way, I should mention that some people think that actually gradient descent gets a better number than this. Some people think that it's actually much smaller than kd squared. Actually, this is a typo. It's not even this, it's actually k squared b. Sorry about that. actually k squared d sorry about that it's k squared yeah sorry very everybody it's even better than what i was saying earlier it's k squared d over epsilon and k squared d squared over epsilon so yeah k squared d over epsilon looks amazing compared to d to the k over epsilon which is what the ntk gets or the f2 gets but many many people speculate that um gradient descent can even do do do better than this This okay, but just to be clear, my theorem is not that exciting because the width is exponential. So I am getting searching over F1, but I'm paying exponential runtime just to do one evaluation of the forward or backward. Okay. Could you comment a little bit on how the dynamics changes when you introduce the normalization? Use the normalization. So, the reason I'm asking is that I'm assuming that the thing that you're gaining by normalizing is that you can control the direction and the scale separately. Yeah, I'll give you the one sentence answer because it makes me a little bit sad. To my shock, it actually significantly slows down the inner rotation by having that normalization. It has to do with, I'm also doing this balanced initialization. So, this is like a huge. So, this is like a huge emotional hang-up for me. All the modern papers, Renee's, everybody's, use balanced initialization. In practice, we don't use balanced initialization. The dynamics are completely different. So this slowing down fact might actually be different if we don't do balanced initialization. But in this proof, the normalization, I mean, I did the normalization for other reasons. I didn't do it to cheat, but later I found out that it does slow down the normalization a little bit. Slow down the rotation a little bit. Down the rotation a little bit. Okay. So the punchline or the way to view this proof is that the way to view it is that it gives us a point on that trade-off curve, which basically tells us that, so I'm just trying to calculate how it's. Sorry, I'm just trying to calculate how I spend the rest of the time of the talk. So let me just finish my sentence again. So, what this gives us is it gives us optimization over F1, but it pays a huge computational time. And I don't know if any of this is optimal or not. It might be optimal in this setup for all I know. And yeah, I should mention prior work. Similar things have been considered, but they always froze the inner layer. They always froze the inner layer. So, this proof allows the inner layer to change a little bit. Okay, so yeah, so to put it in context, but I gave some of this before, the two parts of the theorem I showed you are sort of over here. This is the one where I sort of multiply by d. Then the F2 is over here. And I mentioned a newer paper of mine that sort of Of mine that sort of incomparable. And this Glasgow, it's way down here. Actually, I drew the point in the wrong place because she invalidates SQ. So hers is actually down here. But it's only for one problem. It's not for all of F1. It's for one problem where F1 beats FQ. Okay, so as predicted, I do not have time to show you. I do not have time to show you proofs, but I do just want to write down one thing. And apologies to everybody who's seen me, especially the people that suffered through me giving board talks all semester. A lot of you have seen me do so many calculations based on what I'm going to write right now. But I'll just tell you the punchline. So, a standard gradient descent analysis. You look at this difference of these Frobenius norm potentials, you telescope. Okay, I'm just writing down the way. Okay, I'm just writing down the way standard gradient descent works. And W bar here is a reference point. You just pick any reference point you like. You can call it an optimum if it makes you feel more cozy, but you don't need to use that word. So first one, I telescope. Second one, I expand the square. And the funny thing is that if you do gradient flow, it's the same proof. You do a telescope, but you call it the theorem of calculus and use the definition, the Riemann integral. Definition of the Riemann integral. So it's the same proof. That sentence didn't make sense. Don't worry. It didn't make sense to me either. Okay, so this is pretty awesome. With equalities, we get this equation. And here I can tell you a few things. The standard thing to do is apply convexity to this, but you don't need to. For instance, if you want to analyze policy gradient for L, you can use the, I forget, forget the name, performance difference lemma, and you can get a guarantee. Lemma, and you can get a guarantee for policy gradient from this inequality. What we do is pretty intense. This is a cool idea figured out by Zuwei. If you look at the gradient with chain rule, then you can use homogeneity and that noise. Oh, printer. And what you get is the difference of two losses as if this thing were convex, but you replace. But you replace you use the features at time s with the comparator. So I won't explain this because we don't have time, but I just want to say that the standard gradient descent proof with convexity, it still goes through using only the convexity of the loss. And how does the rest of the proof work? So this is one critical trick, but the rest of the proof uses the following observation. The standard gradient descent guarantee you always see in practice or always. Always see in practice, or always see in textbooks, it has these two terms. And people always delete this term. But what this term gives us is it controls the motion of the weights. So we can use this thing with a bit of algebra to actually control the two infinity norm growth. So the two infinity norm growth is small, and that allows us to use the gamma two as a comparator. So within this proof, we don't have to project or use constraints or any of this stuff or regularization. Radiant descent is already implicitly regularized, and it's given by this term. So that is actually the core of the proof. So that is actually the core of the proof. And I'm not lying, the proof is with full details about a page and a half and uses this fact, the center of the proof. For anybody who's seen me give talks where I highlight this equation, it's actually from this 2019 paper that I got so excited about this proof technique. That's the only proof idea I'm going to show you. I'm just going to conclude instead. So, oh yeah, and this animal here, this is an African spot. This is an African wild dog. This is a very African wild dog. This is a very interesting dog. It's one of the few things that can prevent an elephant to coming to a watering hole because they're so annoying. So it's very good at asking questions. So yeah, the summary of the talk is basically those curves that people felt were not well defined. And people have been pointing out that I seem to think we haven't made enough progress in gradient descent. So I felt it fun to just list a bunch of open problems. So one is, of course, it's always So, one is, of course, it's always sad that every time somebody comes along with a new architecture, we have to write a whole new proof. So, I said general architectures would be nice. Yeah, margins are a very strong distributional assumption. I think we need a new way to do distributional assumptions and to get good results for gradient descent. Because all I see basically are variance of Gaussian assumptions and variance of margins, and we need a lot more than that. I'd like those statistical trade-off curves to be filled in. The real effect of normalization, I really wish I understood. Effect of normalization. I really wish I understood this. Ultimately, a lot of what I did today was linearity. There's pointed out in the beginning of the talk that this L1 was kind of like Fanny's L1 and it had a similar sample complexity. So maybe everything in some sense is linear, even F1, and it's not. Another thing is that I proved test error guarantees, but things aren't IID anymore, so they aren't very good test error guarantees. And in terms of technical open problems, I'll just list a bunch. One is handling imbalanced initialization and large initialization because practice also does. And large initialization because practice also doesn't use mean field that both I and Renee used. Large step size, gradient descent. This is technical, I won't explain it. And yeah, thanks. And if there are any questions, take them now. So I have two questions. So I have two questions. One is the way you're getting these test error guarantees, you said radomarket complexity. So are these all coming from radomarket balance? Most of them. Yeah. For the SGD proof, this is a point of huge pain for me. So for the SGD proof, there's an extremely clean martingale you can use to get that test error bound. And the length of the proof, so if you write out all the steps, is like factor like. Is like factor like 50 shorter than the Rademacher proof if you write out all the details. So maybe this isn't what you're asking, but yes, it's welcome information. But yes, these do use Radamacher. So Radamacher complexity for two-layer networks is magically tight. So we have not tight, but we have a fairly good, the community has a fairly interesting way to do two-layer Radimach complexity. For any other depth, it's just a train wreck. the train wreck and is the is the class you're using some mole or application um for the for the f2 stuff yes for the f1 stuff no but yeah the the the two layer rotamarker proofs are super super clean uh the other question concerned the um this batch norm thing yeah so in the standard pytorch implementation the the scale factor occurs immediately after the Factor occurs immediately after the normalization. You're saying that modern models apply to scaling after the reloading state. Oh, indeed, I'm doing a little bit of cheating here by using homogeneity to pass things in and out. So that's scalar comes negative, right? Right. So in this setting, there's some thing that, yeah, I'm able because I make everything I'm able because I make everything balanced, it's actually not possible for things to flip sign and then switch on the other side because of gradient flow and stuff. Oh, I should mention this. I'm just using gradient flow for the second part of the talk. Just completely lost that, swipe that, put that under rug. So yeah, that situation is actually not. So there's no, to answer your question, I did not cheat in the way you think. I cheated in five other ways. Great talk. Thank you. Ran out of time. Okay, we're out of time, but any other questions?