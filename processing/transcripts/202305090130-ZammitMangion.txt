The speaker is Andrew Zamikmonjeon from the University of Hurong, and he will be talking about deep neural-based estimators for spatial models. You still have like 30 minutes now. 30 minutes? Yeah, okay. Thanks, Rappel. Thanks, utter for the invitation. It's really great to be here. Great company and great talks. So my name is Andrew. I work at the University of Wollangong and I'm going to talk about deep neural-based estimators for spatial models. Estimators for spatial models. This is joint work with my PhD student, Matthew, and with Rafael. Okay, so the slide is generally just about motivation on why we need spatial models and spatial temporal models. I don't think I need to go into too much detail. But the bottleneck, the computational bottleneck, which comes up all the times when using spatial models or spatial employment models and even with extremes is fitting, the model. Is fitting the model. You have some data, you want to estimate these parameters, you want to find posterior distributions over these parameters. And many times you see papers, the applications change, but you notice the main difficulty is trying to fit these models. So now we use neural networks to remove this bottleneck. So estimators are simply functions which take data and give you a parameter estimate. Because functions are very complicated. Because functions are very complicated functions sometimes. But the idea is: can we use neural networks to represent that function instead of trying to come up with the analytic form manually? So there's a history of this, which goes quite ways back, actually. But some of the more recent papers, Jan Gettal is a statistical Sinica paper, where they use neural networks to give a conditional expectation, which is then used as a summary statistic in approximate Bayesian computation. Statistic and approximate Bayesian computation. Papa Macario Sen Murray, this is on using mixture density estimation to find approximate posterior distributions using neural networks. I have a paper here with Chris on using neural networks to estimate dynamics in spatial temporal processes. So you give the neural network mini videos and it outputs how things are moving, like the dynamics. Doug and Florian. Doug and Florian and Amanda both worked then on using neural networks to give you parameter estimates directly in statistical models, which we in this room are interested in. So that really focused on Gaussian processes. And Amanda I think was the first to look at spatial models of extremes using these neural networks. And then with my PhD student and Rafael, we started looking at how to use neural networks for when we. Networks for whether you want to estimate parameters from independent data or exchangeable data. And of course, this is very important for streams where you have many data which you assume are exchangeable or independent, and we want to estimate some parameters from those data. And the solution to that, to I'll explain the framework soon, is using what are known as permutation invariant neural networks. All right, so there was this literature, and I had There was this literature, and I had dabbled around with it. I had seen some papers, and this, why are they doing so well? I think this is the question which was in my mind for a long time. And I know some of you in the room already know the answers to why they do so well, but I think it's so fundamental to neural network estimation that I'll go through this anyway. So, there is a connection between neural estimators and classical estimators, and the link is true. Estimators and the link is true what are known as Bayes estimators. So I'll just recap what Bayes estimators are. So in classical stats, when we want, when what we, but my ancestors probably want to design estimators for, you have a model and you have to define an estimator for some of the parameters in the model. You start off with a loss function and say, okay, but my estimator theta hat, remember, it's a function of the data of my estimator. And if the Estimator. And if the true parameter is theta, there is some loss if I get it wrong. Let's think about these nice symmetric loss functions like mean squared error. So if theta hat is far away from theta, you have a big loss. And if it's close, you have a small loss, right? But and this is our starting point, but we can't just stay here because we don't know what z is in advance and we don't know what the true parameter theta is. So what we end up doing is, okay, we need to integrate this lock. Is okay. I'm going to integrate this locks function over z. I don't know what z I'm going to see in the future, so let's take an average over all possible z. And this is this integral over here, okay, the first integral. And that gives you what's known as the risk function. And different estimators will give you different risk functions, just to fix ideas. We have our risk function that we can have. Let's say one. Let's say one estimator gives you this risk function as a function of theta, and there's another estimator which gives you this risk function. So this would be theta hat one, and this would be theta hat one, two. We've integrated out Z, so these are just a function of theta. And of course, if you have to choose an estimator, this is called an inadmissible estimator, because it's never better than theta 2. You would choose theta hats 2. Has to so that is basically how we could design estimators, but then again, we don't know what theta is in at once. So, one way to deal with that is to take the average under this curve, right? Let's say we have a bounded parameter space, and we'll put a uniform measure or detap on them. And of course, the area under this curve is much less than the area under that curve, so I would choose this as. Would choose this SMA, right? And this is known, this integer is known as the Bayes risk. Now, of course, we don't need an uniform measure. We could have a prior distribution on theta. And that means we really want to minimize the loss in this, the risk, sorry, in this area. We don't really care about the risk outside of that area. So that prior measure on data is actually quite important and it gives us some insight into. It gives us some insight into what to do when training these neural estimates. So, this is the second integral. We're integrating the risk function over all of theta with respect to some prior measure. And then we're going to choose the theta hat which minimizes this base risk that we've got. All right. So, coming up with the theta hat, which minimizes this base risk, is very complicated. There are, of course, simple cases where you can. There are, of course, simple cases where you can do this analytically, but many times we don't know how to do that. So, what we're going to do is say, okay, theta hat is this infinitely flexible function represented by a neural network, highly parameterized, okay, with hundreds of thousands of parameters. And now we're going to estimate those parameters inside the neural network to minimize this laser. So, that's exactly what we do. We're going to denote our estimator as a neural basis. Estimator as a neural base estimator. Now, theta hat is a function of these weights and biases, which are in gamma. And now I'm going to find the gamma which minimizes this base risk. Now, this Bayes risk has got two integrals, and we're going to approximate those integrals using one-to-parallel. So, how do we do that? We sample from our prior measure, Geta. So, let's fix ideas. Let's think of this as a Gaussian process. Okay, so we're going to sample our pig three. So, we're going to sample our pig three, a length scale, a variance, and the smoothness, let's say. And then we're going to simulate data based on those parameters from our Gaussian process, by the Cholesky simulator. And that will be one Monte Carlo sample. If you do that a million times, we can represent the phase risk using this empirical approximation. It was just two averages. Averages and similar to what Abby said, this is a really nice formulation because you can use mini-batch, you can take subsets of these and use stochastic gradient descent. Why? Because if you take a small subset of this, it would be an unbiased estimate of the loss, and the gradient would be an unbiased estimate of the gradient, and that leads to stochastic gradient descent and of the differentiation. So we could use PyTorch, Denserflow, all these machine learning packages straight out of the box to find. To find gamma star. So, why did they work so well? Well, these base estimators, these neural base estimators, inherit all the attractive properties of base estimators. So they are asymptotically consistent, asymptotically normal. The distribution will be exactly the same if once you estimate gamma. And also, we could choose loss functions as we want. So if we choose mean squared error, you get the posterior expectation. If you choose the absolute error loss, you get the posterior median. Newton loss, you get posterior median, zero, one loss, you get the map, so on and so forth. So you can even estimate posterior distributions by taking quantile loss functions. Right, okay, so that was the first question. Why are they doing so well? The second question is, how do we deal with replicated data? Because many times we've got replicates from a process and we want to estimate the parameters. Now, in the past, what people said is, okay, simply apply the neural phase estimator. The neural base estimator one at a time to one of the replicates, each at a time, and then take the average of the estimates. That's very sub-optimal, and I'll show some figures showing that later. And why is that? Well, we know that the optimal estimate based on m replicates is not the same as the optimal estimate of each replicate average, because there's lots of nonlinearities in estimators. So, how do we deal with all these replicates in a neural network framework? Neural network framework. And the idea is to use permutation and variant neural networks. And why is that? It's because base estimators from replicated data are permutation invariant. Just think of a sample average. What is the average of three numbers? It doesn't matter what the order of those numbers are. It shouldn't matter. So theta hat base of z is the same as theta hat base of z phi. And you can prove this, you can actually prove that the base estimate. This we can actually prove that the base estimator is unique under very minor regularity conditions, and this will always be permutation-invariant. So, the idea is to use permutation-invariant neural networks for when we have replication. And I'll start off with the figure. The idea of a permutation-invariant neural network is to take Z1, the first replicate, pass it through a non-linear function, do the same thing with Z2, and the same thing with Zm. So you're all passing them through the same nonlinear mapping, and then you have a permutation. And then you have a permutation-invariant aggregation function. This could be like a sum or a median or a maximum. These are all permutation-invariant functions. And then you do a non-linear function of that aggregation. So you've got two neural networks you need to design, psi and phi, and then your output will be theta hat. So mathematically, it looks like this. Theta hat of z is going to be phi of t of z, and t of z is this permutation invariant function, set function on non-linear. On non-linear functions of the replicant. And this is a very nice form. In classical, when I started reading about this, you see many estimators of the forward form. There's even a lemma called the lemma-Scheffer-Lemmo, which says that all the best and biased estimator for exponential family models has to take this form. It has to be a non-linear function of summary statistics, sufficient statistics for exponential family, sufficient statistics of the day. Of the data. And so you can think of this inner neural estimation as extracting the sufficient statistics from the data, and then the outer one of finding that non-linear function that gives you your best unbiased estimator. So there's lots of potential avenues on how to extend this and how to take advantage of that, I think, as well. All right, so let's look now at some examples. So we started off with a very Some examples. So, we started off with a very simple uniform distribution, and we want to estimate the support of this upper bound of this uniform distribution in this case theta. We know that the ML estimator for this model is max z. So just take the maximum. And this is why the dotted line is really showing you the distribution of the ML estimator for theta equals 4 on 3. When I say the distribution of the estimator, I mean let's fix some. mean let's fix some theta in this case four terms that simulate data and estimate simulate data and estimate and so you get a bunch of estimates and this is the distribution of the estimator but i'm doing some some kernel smoothing this is why i've got these rounded rounded but anyway let's move on to to base estimation i put a prior distribution over theta um pareto for one so we have contributed the posterior one We have contributed the posterior will also be Careto, and we can compute the conditional expectation, which is the base estimator under squared error loss exactly for this model. And that is the green, the distribution we get is the green curve. And when we fit a very simple neural network in this case, we get the red curve. So you can see in distribution, the Bayes estimator and the neural Bayes estimator would be identical. Now, this is actually quite a hard problem. And this is actually quite a hard problem because there's this maximum involved. And neural networks can only be used to approximate continuous functions, really. But we still get a very, of course, reasonable approximation. And the kernel smoothing is kind of helping that. Is the bimodality from your flow? Yeah, it is actually. So this is coming from the prior. This is coming from the likelihood component. We check that. All right. So what is the basic All right, so what is the basic workflow? Define the prior hominid. Now, this is important because when you look at the previous papers on this topic, people many times said, oh, we need to train this neural network, so let's take a bunch of parameters on which to simulate. And many times it's a grid or a Latin hypercube. So you still get a Bayes estimator when you do that, but you can't really be sure. Not, you can't really be sure what the implied prior distribution is at that point. So, it's much better if you can to sample from the prior. This doesn't mean we need proper prior distributions. We need to be able to sample from the prior. Okay, so sample parameters, then simulate the data at those parameters, choose a loss function, design those neural networks, and then train using your paper deep learning package. And then you assess the neural estimator based on testing data. So, the workflows this. Later. So the workflow is the same, and in fact, we've got a package which does this for you, and I'll show you briefly how it works later. Disadvantages, you need to be able to simulate data really quickly. So if you can't simulate data, this is not really something which will help you much. Or else, you've got data from satellite reanalysis data, like hundreds of thousands of images, which somehow you could use, not exactly in this way, but in a similar way, to. This way, but in a similar way to design estimators, but generally, you need a lot of data. All right, so there are some practical considerations. So I'll only talk about one or two. The most important one is this one. The base estimator is also a function of the number of graphic. So as we all know, small sample estimators are very different from large sample estimators. It's not uncommon, for example, to have small samples. For example, to have small sample estimators which are biased, okay. A biased estimator, the best estimator is not necessarily unbiased. But generally, as you increase the sample size, the best estimators will be unbiased. So how do we deal with that? Well, we can train different neural estimators for different sample sizes. Okay, so we've got like small sample neural estimators, medium sample neural estimators, large sample neural estimators. And beyond a certain amount, A certain amount, beyond a certain sample size, you are approaching Gaussianity as base estimators do, and you don't really need to put this too high. But you definitely need to explore small sample regime. Or else, what you can do is create an average Bayes estimator, which is good over several sample sizes. So we modify the Bayes risk a bit by adding this prior over the This prior over the sample size. So, what do you do when you're training this neural network? You don't only simulate the parameters and the data, but you also simulate the sample size. You don't know in advance what is going to be. And this is actually really effective. So, I'm showing you here another example where we've got an inverse gamma on theta and our data are normal zero theta. And what we've got here in the red curve is a neural estimator trained on. Is a newer estimator trained on five replicates. The yellow trade on 150 replicates, and the green is the real, the true one, because we can solve this analytically. And the purple is this average estimator, which is okay for all sample sizes. And you can see that for M equals five, the M equals five estimator is optimal here, but performs very poorly for large sample sizes. Very poorly for large sample sizes. The large sample estimator is very good for m equals 150, but it's horrible for small sample sizes. But if we take this average, this is the purple line, it's never optimal for any sample size, but it's pretty close everywhere. So we can train these neural estimators to be somewhat agnostic to the number of replicates which you're going to have when estimating. This is particularly important for spatial models of extremes. For spatial models of extremes. Just very quickly, pre-training is very important. So when you train an estimator for m equals 5, you can use those starting, those estimated parameters for an estimator for m equals 20. And that really reduces the training time. Another thing is if you can simulate from the model, it's much better to simulate on the fly. So don't reuse data, just keep on simulating. Simulating and that makes the what we call the training loss and the testing loss much more well-behaved. While if we carry on reusing the same data, you start overfitting. This is why the test loss is going, the training loss is going down to zero because we are overfitting. The test loss is actually going up, which means it's becoming worse. If we keep on simulating when during training all the time, it's much easier to train these estimators. All right, so we're going to. We got a few minutes left and then we'll go for questions. I'm not trying to show a small video. So, what we did was we compared this permutation invariant neural network to one which simply takes an average of estimates on independent replicates. So remember, that's all one at a time. You put in your data, your first replicate, you get a theta hat. Second, you take another theta hat, and then you average all of those. And that performs very poorly. And we also Very poorly. And we also compare to maximum posterior estimate in standards. You can't do that in a Gaussian process, not in Schlutter as well, as I'll show later. All right, so these are the risks that we get, test risks, so this is on unseen data, as a function of the sample size. And as we see, the map estimator and the neural base estimators really start, it's got this nice one-on-end behavior, and they go down together. They go down together. While this one at a time estimator, it's okay for small sample sizes, but then it plateaus very quickly. Because why a small sample estimator cannot really compete with a large sample estimate. And if we look at the distribution of the estimators for, let's say, this parameter vector, this is measurement error variance, this is length scale and smoothness. Okay, it's a matter of Gaussian process. What we see is that, again, we've got unbiasedness, we've got small. Again, we've got unbiasedness, we've got small variance, but this one at a time estimator is, of course, very far away, it's biased and way too little variance. Okay, Schlatter is backstable process. Now, here we cannot apply the map, but we can do composite likelihood. So we do composite maximum posteriori likelihood, let's put it that way. And now, interestingly, we see that the neural best neural-based estimator does much better than composite likelihood. Much better than composite likelihood. Composite likelihood seems to be unbiased, but it's got a much larger variance. Why the neural-based estimator is unbiased, seemingly unbiased, and got a relatively small variance. Spatial conditional extremes process. So this is Jennifer and Jonathan's model. It's an eight-parameter model. At least this is the ones that we consider. We already considered the neural base. The neural-based estimator and the one-at-a-time estimator, and again, we see that it's really no comparison. Okay, this permutation invariance architecture really helps things to do well. These are the distributions of the estimator for one parameter vector. You can see that things are roughly unbiased, pretty tight variance around the true parameters. And these are just simulations of the conditional extremes. The conditional extremes models just so that we fill up the white space. All right, I don't have time for the RET C example, but we fit a spatial conditional extremes model to this and with some Turing tests. And probably the most important diagnostic is this one. We are comparing conditional exceedance probabilities from our model against what we observe empirically. And we see that things line up pretty well. So, these neuron-based estimators are really good. And I think for spatial extremes, they seem to be ideal personally. I'm not an extremes person, but it just seems to fit in. You've got these replicates, you can generally simulate very quickly from these models. It just seems an ideal application. Of course, there are many other applications where these are useful. It's likelihood-free, so you don't really care about the likelihood. And I know Mikhail has looked at a talk, which is great. Which goes beyond, and this really capitalizes on this. So, your base estimators are likely to really train for a large set of models and can be used with independent replicates fairly easily. The disadvantage is that you require fast simulation or a large number of data. So, before I conclude, I'd like to show some software that we have created in Julia. And this was really Matthew's. And this was really Matthew's work. Just to show you how easy it can be. So imagine now we have a package, and in this package, there are a lot of pre-trained estimators. So you can just load an estimator for your model. And in this case, we've got a Schlater model. So all we're doing here is simulating. So we've got length scale of five, smoothness of one in this process. So that's for. Process. So that's fairly quick. And then we're going to, in this case, we are actually setting the architecture. And look at how easy it is to set up the architecture. We're going to chain convolutional layers together, flatten, and that would be our psi, and this is our phi. By the way, if you haven't seen Julia before, the fact that you can code Greek symbols and the theta hat is really cool. Makes things very easy to understand. Right. And then what we're doing is we're What we're doing is we're sending z and theta hat to the GPU, okay. And this is the function, and then we just obtain theta hat, and it happened in maybe 0.02 of a second, right? So we obtain estimates from Schlatter's model in 20 milliseconds. And we can do bootstrap because bootstrap is available for free. This is non-parametric bootstrap, so just take out some data, pass it through the neural base estimator, and keep on doing that 100 times. And maybe that takes 800 milliseconds. Takes 800 milliseconds or something like that. And so we get our bootstrap confidence intervals really, really quickly. Yeah, I don't even know what the dimension is. Sorry. You remember? So 156? Okay, so 16 by 16. Yeah, so 156. And how many were puts? And how many were probably 64, I would think. Yeah, I don't remember the details, or maybe if I scroll back, and 150, 150 replicates, yeah, and it's 16, so 16 by 16. Good, right. And yeah, if you're interested in this work, we've got a paper on archives going. A paper on archives currently under review, and also Jordan Richard has got a poster outside where he's using this for censored with sensor data. And I think yesterday there was talk we could do it for up to 500 data points, but I think we're doing it for thousands and again, fractions of a second. So that's interesting because you also need one hot encoded maps, but there are details in the poster. Yeah, okay, that's it. 