From San Diego 68 and Sunny, and he'll tell us about the degree-restricted branding process and how it's smart from people. All right. Thank you very much for introduction, and thanks for giving me the chance to speak here and also to organizers for organizing this workshop. I think it's a great idea to try to bring together these different communities who are working on similar objects, in particular analytic comatorics and probabilistic comatorics. And probabilistic cometorics. And I have quite some good memories of some OFA meeting in 2015. So, everything I'm going to talk about today is with joint work with Mike Merloy from Toronto and Erlang Suya, my PhD student. And the short story is the following. I'm going to tell you the simplest possible algorithm that can randomized algorithm that can potentially generate a graph with a given degree sequence. And then we can ask kind of the strategic question, what distribution does that algorithm have? Algorithm have. Okay, and given kind of the expertise we have, I hope you would agree that these are kind of questions that we should be able to answer. Okay, so let me maybe start for context with the simplest possible way to phrase this. So our goal is, for example, if you wanted to generate a random deregular graph and you give this to some undergraduate student as an exercise, they're most likely not going to come up with this sophisticated configuration model or some other algorithm how to generate them. But I'm pretty sure many of Generate them, but I'm pretty sure many of them would try the following model, which is basically greedy. So you start with an empty graph on n vertices, and then you just try to jam in as many random edges as possible. So you each time add a new random edge to the graph so that the maximum degree always stays at most d. And so this is an extremely natural algorithm. It was already studied in the 80s, going back to mathematical chemistry literature. And in particular, we know that this graph actually terminates with high. That this graph actually terminates with high probability with a deregular graph, model of some obvious parity constraints. Okay, so the basic question one can ask here, and this was already asked by Warmold in 1999, is how similar is the output, the final graph, to uniform random deregular graph? Okay, so we know that with high probability, the final graph will be deregular. So the question is, how similar is it to uniform deregular graph? Okay, in particular, Wormal conjecture, they are similar. Warmer conjecture they are similar, and the similarity notion that used there is contiguity. Okay, and what I want to discuss today is basically a variant of this process or a generalization of this process where instead of insisting that each vertex has up to D neighbors, I allow for different degree constraints. So, some vertices can have up to two neighbors, some other vertices can have up to three neighbors, and so on and so forth. And the main result will be. And the main result will be that, in this case, when you can allow for different degree constraints, that then the output of this process actually differs significantly from a uniform random graph with the same degree sequence. Okay, so we can show a substantial difference between the two, and I'll make that more precise. I'll just say at this moment already that I think one of the key contributions of this work, besides that I think this result is interesting, is actually. That I think this result is interesting is actually the proof technique that we're going to use, which is maybe a little bit unusual here. So, what we're going to do is we're going to use a technique from combinatorial enumeration to establish this result and the so-called switching method. And I'll allude to that more later on. Let's, for the moment, just be sure that we're all on the same page of what this process does. So, here I have a bunch of vertices which have different degree constraints. So, this vertex here can have up to two neighbors. So, this vertex here can have up to two neighbors, this vertex can have up to three neighbors, and so on and so forth. And then we just put random edges down one by one. Okay, and the only thing I want to illustrate here is that something to keep in mind is that as we add more and more edges, certain vertices kind of become inactive. They're no longer in the game. So, for example, this vertex has already two neighbors, so we can never ever join edges to this vertex here in the future. Okay, so this is one of the things that differs. Is one of the things that differs a little bit from the classical Elder Schweni random graph process where we can always attach to any possible vertex. But here it happens that as soon as your degree constraint is satisfied, you're over. Okay, and so this is the process that we want to study. It's very natural. And even if we allow for mixed degree sequences, as long as the maximum degree is bounded by a constant, we still know that this algorithm actually will terminate with high probability with the desired degree sequence. High probability with the desired degree sequence. Okay, so it is a fair model for generating graphs with this degree sequence, but this question kind of is: what is the distribution? Okay, so to phrase that slightly more differently, how similar is the final graph of this degree-restricted random graph process to a uniform random graph with the same degree sequence? Okay, and depending on which community you come from, you can ask the same question in many different ways. Okay, so if you Okay, so if you maybe have background statistics, you might ask, okay, can we actually distinguish these two graphs and can we do it algorithmically? Or if you come more from a comator probability background, you might ask yourselves, okay, so what can we say about the typical properties of these two graphs? Are they actually similar? Or if you come more from an algorithms background, you might ask the strategic question: is this actually a useful algorithm? This actually is a useful algorithm for sampling? In other words, do I get something that's close to uniform or not? And if you maybe have a more background in modeling or physics, then there's always this desire to come up with the simplest possible model that works. And arguably here, this degree is constrained process is kind of the simplest model you can imagine if you want to generate a graph with a given degree sequence. And so this basic distributional question is the one that I want to. Is the one that I want to address here in the case where we allow for different degrees? I should say that whenever there are any questions, please do feel to ask them along the way. I prefer to take questions as we go along. So what is kind of the main result? What is the flavor of the main result? So we want to say that for degree sequences that are not nearly regular, so imagine Nearly regular. So imagine that no degree appears more than 99% of the times. Of course, you could replace that with one minus epsilon. So, for example, you have at least two different degrees. So imagine 50% of the verses have degree two, 50% of the verses have degree three. Then our main result is that with high probability, we can distinguish the output of this degree constraint process from a uniform random graph with the same degree sequence. Okay, so they're really different. Okay, so they're really different. And so, what we can actually do is we can cook up a test statistic so that you can calculate the statistic for each of these two graphs. And by comparing just this test statistic, you'll be able to say with high probability whether your graph belongs to this uniform random graph or the output of this process. Okay, so we have a very, very concrete way how to distinguish them. And just maybe for Just maybe for simplicity, let me tell you how the statistic looks like if you have a lot of vertices of degree one. Say you have at least one percent of the vertices of degree one and at most 99 percent. So in that case, what you can do is you can just calculate the number of one, one edges. So that means the number of edges where both endpoints have both the vertices of your edge, both endpoints have degree one. Have degree one. Okay, so you just calculate this quantity, and what we figured out is that in these two models, this quantity will differ substantially. So they will differ by a linear amount, actually. Okay, so this is one classical example, I would say, where it shows that the degree sequence of a graph does not encode all the information that you want, as some people maybe thought 20 years ago. You really have to understand how did I generate the graph? Have to understand how did I generate the graph because that has a significant impact on the structural properties that you'll see as illustrated by this result. Because just some simple statistic like the number of 1,1 edges really changes depending on whether you take a uniform sample with the degree sequence or use this algorithm to do that. One question? Yes. Is it possible that that one-one edges really differ, but all of the other edges and pairs of All of the other edges and pairs of on this, on both models, are very similar. And you can say that those graphs are similar, ignoring those one, one edges. So we can, okay. So actually, in general, we can show a more general result where you kind of split your set of vertices into two parts, basically, those which have. Two parts, basically, those which have not too high degree and those which have high degree, and even the number of edges that sit on each of these sides, we can show that they also differ. So, I think these models really differ in a lot of ways. So, from my perspective, they're very different because we, for example, also show that the edit distance between these two graphs is like linear. So, to transform one to the other, you need to change a linear number of edges. Your number of edges. Yeah, so they're really quite different. Yeah. Okay. So. Other question? What distance did you say you were comparing? So for example, you can look at the added distance. So how many edges do I need to change to this graph? For example, remove or add to get from one graph to the other. There, you need a linear number of edges, which is like a constant fraction of all. Edges, which is like a constant fraction of all the edges you put in. But our result actually also shows that, if you like, other notions of distance, for example, that the total variation distance is exponentially close to maximum. Okay, so if you're really, really, really close to the maximum total variation distance here. And so they're really quite different. Okay. So one thing I'll talk about a little later here is that we want to use this switching method, which is historically used for studying uniform models or actually for asymptotic enumeration. So a lot of results, particularly in the 90s by McKay and Wormold. McKay and Wormold, they use the so-called switching method to actually enumerate graphs with a given degree sequence. And so that method is inherently used for calculating ratios of certain sets. And so that method is perfect for analyzing uniform graph models, but there is no application so far to stochastic processes. So one contribution of this work is that we figured out a way how to actually use this proof technique from SMT. Use this proof technique from asymptotic enumeration to analyze a stochastic process that is non-uniform. Okay. And I'll explain a little bit more about that in a second. At this point, I wanted to just talk about some intuition why this result might be true. Okay, so unfortunately, this intuition here does not translate into a proof, but I think it makes plausible what's going on. Okay, so suppose we're in So, suppose we're in this situation where in both of these cases, each of these vertices still have certain degrees left. Okay, so here configuration model, how does it work for each vertex? You put on these kind of three potential neighbors, and then you'll have to eventually choose a perfect matching among all of these stubs here. And you can do that in any possible order. This is like an exchangeable model. But now I'll just take in the current situation. But now just take in the current situation. If you're in this current situation, each of these vertices up here has three possible ways that I can connect to each of these vertices. And this vertex down here, because it has only one potential neighbor, has only one chance. Okay, so and we want to now compare what's the probability that they add this edge down here with this D process that we're studying. Okay. The point is in the D process, the probability with which The probability with which you connect to this vertex up here and this vertex down here is the same. The only thing that matters is that this vertex is still active. This is very different from the confreation model, where here I kind of have three possible choices to connect it to this vertex and only one here down here. Whereas here, this has the same probability of connecting to vertex up here to down here. Okay, so it's kind of uniform over all possible choices. And so that means that if you compare the probability that you add The probability that you add this edge in the configuration model with this D process, at least in this one step, then this probability will be higher in the DN process compared to the configuration model in one step. Okay, so that gives some indication that maybe this DN process actually produces more of these 1,1 edges than the configuration model. But of course, this is a little bit misleading because here we're just looking at one step, right? We really have to look at the evolution of these processes and not just one step to make things work. To make things work. But I think it gives some indication that the configuration model has some preferential mechanism going on, whereas the D process does not. And that explains kind of one key reason why certain edges differ in both models. Okay, so how does the main result look like? So that you have an idea how this looks like. So we define some random variable which counts the number of vertices with both endpoints of degree one. Both endpoints of degree one. And then what we show is that with high probability, these are lying in disjoint regions. Okay. So in the configuration model, using standard arguments, you can show that you're very concentrated around the mean. And what we show is that the degree constraint process will, with high probability, sit outside that mean. Okay, so for this talk, I prepared a slide which just says that these two intervals are disjoint, but we can. And these two intervals are disjoint, but we can actually show that with high probability the process lies here on the right-hand side. Okay, so they're really different. I think there was a question. Okay, maybe not. Sorry. Okay, so what are the two things that we have to prove? We have to prove that in the configuration model, we're concentrated close to the mean, and that in the other model, we're very far away from the mean. Okay, so one of these Very far away from the mean. Okay, so one of these two tasks is standard nowadays. So to prove that something is concentrated in the configuration model, it's fairly easy. And that can be done using first and second moment method or Martingale concentration arguments. So our main contribution is how to analyze here the quantity of the number of 1, 1 edges. Okay, and for that, we're going to adapt the switching method from asymptotic enumeration. And I want to explain to you. Symptotic enumeration, and I want to explain to you a little bit about how that works. Okay, so let me try to just illustrate you by an example what switching actually is. Okay, so switching is a very old result. Actually, I think I looked it up the other day. This was introduced in the 1800 to study graphs with a given degree sequence. So, this is a very, very old idea. And here's kind of how it looks like. Suppose you have some graph here, G minus. And what is so special about G minus here is that I want that these two vertices up here, that they have degree one, and that these vertices down here have a degree at least two. Okay, that's the key thing that I want here. And what does switching do? Switching simply does, it removes these two red edges here on the left and replaces them by these horizontal edges. Okay, so that's the only. Okay, so that's the only change. So imagine that G minus and G plus are really, really big graphs, and this is just some tiny proportion of the graph. And the only thing we do to go from the left to the right is do this change. We change these two edges to this one here. Okay, so we kind of switch the edges. That's where this term comes from. And so why is this now relevant? This is relevant for two reasons. One is note that the degree sequence of both graphs is exactly the same. Okay, every vertex has the same degree sequence. Okay, so that's. Same degree sequence. Okay, so that's where this originally comes from, from the 1800. You sequentially transform one graph into another while maintaining the degree sequence. Okay, so we maintain the degree sequence between the two, but there's one key difference. We changed the number of one, one edges. The graph on the right has exactly one one one edge more than the one on the left. Okay, so the graph on the right has more one one edge than the one on the left. Everything else is the same. And so our goal will be to show. Our goal will be to show somehow that the graph on the right-hand side is a little bit more likely than the graph on the left-hand side. That would be good for us, because that would mean that this process kind of prefers the graphs on the right-hand side. And this guy has more 1-1 edges than the one on the left. So that would all be consistent with what we want. Okay. So that will be our goal to prove that the gar from the right-hand side is a little likely than the one on the left side. This is in particular. One on the left side. This is in particular relevant because if you take a uniform random graph, then both of these graphs occur with the same probability. Okay, so that's why we want to kind of get this slight advantage between the two. I should mention that if you try to apply this general switching method to stochastic processes, one thing that happens, and I'll slightly allude to that later, is that if you look at certain sets, for example, what's the probability that your graph lands in a certain set of sets? Probability that your graph lands in a certain set that is just the cardinality of that set divided by all graphs with a given degree sequence. Okay, so in particular, if you look at ratios, then the normalizing constants will cancel out. This is something that does not happen for a stochastic process that's not uniform, because any possible trajectory or a way to reach at the graph will actually not be uniform. They're slightly different. So that's something that we really have to take into account in our argument that we're not uniform. Argument that we're not uniform. Okay, and the question how to do that, in particular, is how to adapt some switching argument to that setup. So the way switching enters is slightly tricky, but I do want to mention is that basically what we do is a combination of two things. One is we actually will do switching on the trajectories of the underlying stochastic process. So instead of perturbating graphs, we're actually going to perturb the trajectory. Going to perturbate the trajectories of the process, which means that we're going to perturbate the trajectories with which your process will generate one of these two graphs. And it turns out that with some extra work, one can actually push this through. But let me say maybe slightly more about the general framework of switching, okay? Because I think it's not so well known as maybe it should be. Be so, so one thing we want to show here, as I said, our goal would be to say that the graph on the right is a little bit more likely than the graph on the left. Okay, that was our goal. So, suppose here in this toy setting, we wanted to show it's at least, say, one plus epsilon. Okay, the graph on the right is a little bit more likely than the one on the left. And so, the way this works is we basically do something which sounds completely ridiculous. Sounds completely ridiculous. So, we basically look at this probability that you generate a certain graph, and then we just expand it out over all possible orderings of the edges in which you can generate this graph. Okay, because we're looking at the stochastic process, the order in which the edges arrive of your graph matter, and they have a significant impact on the probabilities. Okay, so we just write this as a gigantic sum, as a gigantic sum, and it turns out that you can actually then. It turns out that you can actually then do something with all these probabilities here. Okay, so in some sense, what we are able to do is instead of doing switching just on the level of these graphs, we can also pull off some switching argument which looks at ratios of these probabilities, which looks at ratios of the trajectories of the process. And we're still able then to argue that if you look at certain ratios, then you can prove certain estimates. Okay, and so to make that Okay, and so to make that work again compared to classical switching, we have this idea of we need to do switching on probabilities of trajectories. And there's also, for technical reasons, we need to average out over certain sets of trajectories. It's not just enough to compare one trajectory with another. We need to do some averaging argument to make things work. Okay. Just to give you some feeling for how switching also works in the classical sense. Switching also works in the classical setting. Let me explain to you one idea that's used in switching a lot. Okay, so one idea that's used in switching is you look at an auxiliary bipartite graph. And so the idea is up here, you put all graphs where the number of one-one edges is, say, L plus one. And down here, all graphs where the number of one-one edges is L. Okay. And then you connect two graphs from the top and from the bottom. If you can create one. If you can create one from the other by this switching operation, which changes the number of one-one edges by exactly one. Okay, so for each of these graphs up here, you connect them to a graph down here if there's a switching operation, which takes you in this direction. And the goal now of classical switching typically is that if you can estimate the degrees in this auxiliary graph, and you can show that almost all the degrees up here are kind of the same and almost Are kind of the same, and almost all the degrees down here are the same. Then, using some double counting argument, you can convert this knowledge of this hypertype graph here into the ratio, knowledge about the ratio of these sets. Because if you count the edges in this auxiliary graph, you can count them using the degrees from up here or from the degrees down here. Okay, so what switching basically does is it reasons usually about this auxiliary graph, and then that allows you to. Graph and then that allows you to get some information about these ratios of these sets. What we managed to establish is that in certain regimes, we can do a very similar argument using this basic switching lemma. So we start off with this basic switching lemma, and then by suitable averaging, we're able to lift this knowledge about these probabilities of certain graphs to these probabilities. Certain graphs to these probabilities for these sets. Okay. And for that, we crucially use that these graphs here are very close to regular. Let me emphasize this again. For switching, this argument is much, much simpler. Because if you look at the probability that the uniform random graph is in this set, well, that's the cardinality of this set divided by all graphs with a given degree sequence. So when you look at this ratio of probabilities, the normalizing constant just cancels out. Constant just cancels out, and you're back just to comparing sets. This does not happen for stochastic processes, and that's why we need to work much harder. Okay. Let me just mention is that once you have these kind of estimates for, say, what's the probability that you have L11 edges versus L plus one one one edges, then there's a way you can lift this estimate in order to show. In order to show that you're not close to the expected value, I will not maybe say too much about it. The basic idea is you write everything in terms of telescoping products, and then what happens is that if you just go far enough away from the mean, that then your probabilist decay exponentially. Okay, so this is maybe not the main point here. The main point here. The main point is you need to be able to compare these ratios of probabilities. And once you have that, then you're in very good business to prove what you want. Okay. And our dream would now be to prove the same thing in general. What would be a generalization of this degree one case? Well, beforehand, we said, say, between 1 and 99% of the vertices should have degree equal to 1. Now the idea. To one. And now the idea is that you want that, say, between 1% and 99% of the vertices have degree at most s. And then we want to count small edges, which means that both edges, the end vertices of both of these small edges have degree at most s. And then our goal is to show that the number of small edges differs in both models. And we can prove that, but the story is significantly harder. It was actually very surprising for us that it's much harder. That it's much harder, and so the reason is that we wanted to originally prove that if we do the switching, that we're more likely, right? So, if we do the switching, we have more of these edges. And so, we want that the graph on the right is more likely than the one on the left. But that's just not true in general. Here's an example of a graph where, if you look at this ratio, the ratio is not even close to one. It's 0.82. So, we have no chance of proving it. Of proving it using this direct adaptation of the approach I told you for degree one case. You have to, instead of comparing just graphs, you have to compare sets of graphs, you have to use averaging. Let me not go into the details here, but rather wrap up. So, the main story here was that we were looking at this degree-constrained random process, which is the simplest possible way to generate a graph with a given degree sequence. Graph of the given degree sequence. And what we showed is that as long as these degree sequences are not close to regular, then we can actually distinguish the output of this degree constraint process from a random graph with a given degree sequence. And the proof, at least for us, seemed surprisingly difficult to prove this. And so that's why we had to end up adapting the switching method, which is classically used for asymptotic numeration, to the analysis of stochastic processes. Analysis of stochastic processes. And so I think it would be really interesting to see further applications of this idea, which I think is quite promising. And I also want to finish with one question here in this context, which I think is very interesting, namely, what can we say about the two regular case? So when all the degrees can have two neighbors, can we then say that the graph that we generate from this degree constraint? That we generate from this degree constraint process is actually similar to a random two-regular graph. So I think the answer is yes, but and I can prove some similarities between these two models, but not on the level that would say that these two graphs are, say, contiguous or some very refined notion of similarity. I think that would be a very interesting question, and it really feels like something that we ought to be able to do. And with that, I'll stop here and open up for questions. Questions. Questions for holes? So for the opposition of a graph and the solution that's yes, thank you. So for all the vaccines you think that may. The bottom layer, like it's an inclayer degree for the vertices that are all three, and for all the top vertices, the infilayer degree is two. So to be honest, the connection was not so good. I do not understand the question 100%. I think your question was: what happens if we do this switching argument in the case of regular degree sequences? Yes, I'm thinking. Sequences? Yeah, something like that. Yeah, so this entire approach that I sketched here will not have a chance to work for regular degree sequences, because what we use is that there are vertices of different types. So there are vertices which have degree one as an endpoint, degree two or three, or so on. But if you look at a regular degree sequence, then all these vertices are of the same type. So a simple statistic. So, a simple statistic of this form that we presented here will not work. And moreover, there's a general feeling that you need to look at a very non-local property. So if these are different, then you have to look at something non-local, because you can show that if you explore from a vertex up to, say, distance like little o of log n, then you just see a regular tree. So it's really the uniform. The uniform random graph and this process look locally basically the same. So it's very hard to imagine what kind of local property could distinguish them. I wanted to ask, when you, so you, at the bottom of your talk, you were talking about the statistic around 1-1 edges. And then just at the very end, you talked about the more general result and said that it requires significantly more techniques. More techniques. Does it break down immediately when you move away from one, one inches? Or does it have to get reasonably large before the simpler argument? Here is a graph on eight vertices where it would already fail. Oh, yeah. So it's really honest, it was very unexpected. Like we first did a lot of work for the degree one case, and we thought this is a Degree one case, and we thought this is a complicated enough toy thing to develop a technique, but it turned out to be much more complicated in general. So, in some sense, I think we were lucky that we tried the degree one case first, because that really helped us later to understand the difficulties in the higher degree case and how to circumvent them. But it really required, as I said, some more notions, including like averaging and doing these kind of ratios that I talked about earlier. These kinds of ratios that you talked about earlier, you have to kind of do them over sets instead of over graphs, and so it really becomes much more complicated. But I really, we tried to put a lot of intuition and sort of in the paper. So if someone's interested, I really recommend to look at it or maybe shoot me an email and I'll happy to chat about it more. If there are no more questions, that's I can't see them. Thank you. Bye. 