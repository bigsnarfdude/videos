I think it is. All right. Welcome back, everybody. We're very happy to have Jared McKean from Google to give the next talk. And as you could see on the program, it's a very exciting talk about centered around the question what quantum computer science teaches us about chemistry and quantum advantage in learning from. Advantage in learning from experiments. I'm very much looking forward to it. All right. And thanks so much to everyone for organizing the conference and inviting me to come speak at it. As I mentioned yesterday, I'm quite interested in the topic. And I actually, I am going to try to tie in, even if it doesn't seem like early on, how we connect to these sort of ideas of randomness or rather perhaps illusions of randomness and what a quantum computer. And what a quantum computer can actually do to help us tell the difference between the two. So, hope, stay tuned for that connection if it's your primary interest. And I hope to tell many stories during this talk, one of which is something that's been a change in my mind of how I view science, maybe in learning as quite different than maybe rote computation. And I hope to make a little bit more clear what I mean exactly by that. And some of this perspective that we gave. And some of this perspective that we gained over time, we actually had fun recently trying to roll that into a sort of halfway between science and pop science perspective that the first part of this talk is based on called What the Foundations of Quantum Computer Science Teach Us About Chemistry. And that perspective was meant to be a little bit of an opposing view, perhaps, to this traditional narrative that if you open up news articles or many of the papers that have been written about using quantum Papers that have been written about using quantum computers for chemistry, that you're often told, say, for example, I'm going to digitize some molecule, I'm going to put it into my quantum computer, and out the other side is going to come some solar punk utopia where photovoltaics are free, you know, the world hunger is solved, et cetera. So we wanted to, you know, maybe push back against that a little bit tongue in cheek and say, okay, even if we don't have quantum computers, maybe they've already taught us something about chemistry and what might that be? About chemistry and what might that be? And so, typically, when we say we want to simulate chemistry, what do we mean by that? So, there's often a kind of specific sub-part of chemistry that's being referred to in quantum computing, and it's the sub-part of electronic structure where I often give you some idea of where the atoms and molecules, and like a small molecule or protein or material, are sitting. And just from that structure, I'd like to then go and gain some amount. Then go and gain some amount of understanding through computation. So, that might be how a molecule absorbs light, that might be how different species complex with each other along a reaction pathway or in proteins or RNA, or it might be how molecules complex with an interesting metallic surface. And from that understanding, I'd then like to take that model and develop some level of control. So if I know why things absorb light, maybe I can make informed decisions about new photovoltaics. Decisions about new photovoltaics. If I know why things complex in different ways and what their transition states are, maybe I can inhibit misfoldings that cause diseases or things like that. And if I really knew about surface interactions, maybe I design new catalysts and get platinum out of my catalytic converters. And so already, what the frame of mind that I actually want you to take from this slide is that I've said a lot of things that are not directly about simulation. I've mentioned intermediate models, I've mentioned transition states. Models, I've mentioned transition states, I've mentioned this process of design, which naively doesn't come just from a bare computational model. We've actually put a lot of knowledge into that. And to kind of drive the point home, I want to think for a moment about simulating chemistry without the last, say, 100 or 200 years of what I would call theoretical chemistry. And in this setup, I'm going to imagine that I have some collection of reagents and reactants. So this might just be, you know, a few base molecules. Might just be a few base molecules that I can order from a standard chemical supplier. And for all intents and purposes, I have an infinite supply of them. And then I have some desired output product, like a small molecule or drug for treatment of some disease or ailment. And I want to start asking questions like, how long does this reaction typically take? Or, for example, does this ever happen with this set of molecules? Is it synthesizable from my starting products? And if I imagine that I do. And if I imagine that I don't know any chemistry, but someone taught me a little bit of physics, so what I have is just these models of, say, if chemistry existed in a classical world, I have F equals MA on the little atoms, or if I can do the time-dependent Schrodinger equation, I kind of know the Hamiltonian of my system and I can evolve it for it in time. And what would I do with such a thing to answer these questions? Well, basically, the only thing I can do without any other knowledge is wait. And I wait and wait and wait. And I wait and wait and wait for a very long time. And then maybe I saw the product come out and maybe I didn't. And if I didn't see it come out, maybe I can't conclude anything about it. I can't say that it didn't ever produce that. And that's going to be a common theme here. But you might say, well, this isn't really how chemists do chemistry. They know a little bit more than this. And you might be tempted to say that, well, that's just a limitation of the past. Aren't quantum chemistry? Limitation of the past. Aren't quantum computers supposed to do a lot for us? They seem to accelerate some things that we'd like to do in chemistry by an exponential amount, assuming certain complexity theory assumptions. So for example, if I take this Hamiltonian of a, say, excitation that starts over here, and I want to ask how it evolves under a real quantum Hamiltonian to over here, I've now developed very, very sophisticated techniques in quantum computing. It's a rapidly developing field where numerically Developing field where numerically exact solutions appear to be possible in sublinear times and basis functions when I compare this to, say, a classical computation that hits a hard exponential wall due to entanglement truncation. So you might ask, does this help me? Can I just march things forward in time and learn something about a system that I'm interested in? But this is kind of where I would target a first contribution of, say, quantum computer science back to what we should be. Computer science back to what we should be thinking about in chemistry, which is that we know from things like the no-fast forwarding theorem, which I'm caricaturing a little bit here, that basically the simulation time of any physical process done in enough detail has to be greater than some constant times the physical time. Otherwise, you could just recursively simulate your own simulation and go to zero computation time. And so, if you say, okay, well, if I can only afford one instance, even this exponential speed up in simulation. This exponential speed up in simulation of a quantum system, if I can only do one realization, there are chemical reactions that take on the order of hours or months to happen to an appreciable amount of many, many molecules, much less one. So this would kind of suggest to me that, well, maybe I can't do anything for the simulation of chemistry. But any chemist in the audience or anyone who's intersected chemical simulation will say, wait a minute, but we know more than that. We're not that dumb. We don't do things that way. We don't do things that way. So, what is chemistry adding? So, if you've gone to many talks, they're often, even though they use dynamics as a subroutine, they're talking about this particular problem. In fact, this tagline that's used, the underlying physical laws necessary for the mathematical theory of a large part of physics and the whole of chemistry are thus completely known. And the difficulty is only that the exact application of these laws lead to equations much too complicated to be soluble. So, they're often talking about this time-independent picture. Talking about this time-independent picture where we map things in and we read out these particular states. And you know, if you just take a glance at these, nominally speaking, these basically have the same information content as far as computational problems go. So in both cases, the Hamiltonian is an input. In the left case, there is some notion of an initial state, whereas the right is more objective. But if you take another perspective, we often say that given an initial state, dynamics is in BQP. State, dynamics is in BQP, while these stationary states or this eigenvalue problem is in QMA. So one seems much harder than the other. Why do we go to this one over here? And it's in part because of this problem that I just mentioned. Dynamics, while interesting for some problems, is ultimately limited to physical time scales. It must make very aggressive approximations. While on the other hand, thermodynamics, which is kind of the thing implied over here. Of the thing implied over here is often predicted of much longer times. So, what we use in order to make use of this fact is that we look around our world, we see that it's not actively on fire. We take a closer look and find that many things we're interested in are in fact occupying the lowest energy states. And this concept of energy ordering is perhaps not something you could have pulled out of the map ether, but you look around and observe. And so, we use both the fact that thermal. We use both the fact that thermodynamics, which makes t going to infinity predictions, is often predictive of a t much, much larger than one I actually cared about. This allows me to skip forward to useful processes and make predictions on time scales that are much longer than what I could actually simulate. And of course, you say for this to be true, it can't always be true. And so there must be some systems where the worst cases of this are like state enumeration. And therefore, this is where your complexity comes back. This is where your complexity comes back in. And then, to kind of make this case a little bit further, before I make the more stronger computational case, you can say, well, all we've really talked about so far is this infinite time. If I allowed my system to run forever, where would it settle? And I hope that it's predictive on a day-to-day time scale. And that's kind of what's encoded here, where I often study low-lying energy states and some energy ordering here. But what about cases where I start with? But what about cases where I start with all of my reagents and I find out that one of them has lower energy, and I'm like, okay, great success. I pour everything into the jar and I wait. But what I find is that system never gets created in practice. I waited a really long time. I waited, you know, I passed it down through the generations for people to do this reaction and no one ever saw it. And what's happening here is that it says nothing about the rates itself. So to do that, we introduce extra information. Do that, we introduce extra information about a system. Is that we know that rates are often very closely controlled by these bottlenecks called transition states. And what we do is we lump thermodynamics of transition states into that, which includes extra information from our environment, from models of chemistry that we've known, so we know what calculations to do. And to kind of make this point even a little perhaps more pronounced, you might imagine that a more accurate calculation is always better. So if you haven't Better. So, if you haven't thought too closely about these computations, what's often happening here is that we're using an almost semi-classical model of chemistry. So, the fact that nuclei have definite locations and that we can label them as particular molecules is a bit of a classical concept. So, this kind of born-Oppenheimer approximation we didn't have to make. We could have just written down a nuclear soup, and we should be able to solve things in principle more accurately. But let's think about what. But let's think about what happens when we do that, in maybe a very particular case. So, think about this molecule that's just a diatomic, hydrogen on one side, chlorine on another. If I go and I treat all of my nuclei quantum and my electrons quantum mechanically, then there should be some amount of symmetry here. Basically, there should be a rotational symmetry in the molecule that I should be able to exploit. And what you find is that the proper quantum mechanical description of this thing. Quantum mechanical description of this thing has sort of a chlorine at the middle and a sphere of hydrogen around it. You go and do the computation of the dipole moment and you find that it's zero. And then your friend goes into the lab, does an experiment, and they find that the dipole moment is one. So who's right? Your computation was perhaps more accurate than their even experiment, but in practice, you only ever saw an actual dipole moment. What's happening here is, of course, a version. What's happening here is, of course, a version of symmetry breaking that any small perturbation anywhere in the environment breaks this, and you get a strong dipole moment. But again, if you were just sitting alone in a map cave, you might not come up with this. And so this is just another example of us augmenting our models in the background with kind of these observations. And you'll see why I'm building to this in a second. So you'll say, okay, well, this seems like a very powerful technique to kind of point to these reduced observables that let These reduced observables that let me fast forward in time, that let me sort of make judgments about my system based on other interesting things. And you might start to ask yourself, can all physically interesting questions be answered by some reduced model? Is there always some free energy I can compute on a closed system that will tell me if I'm going to get what I would like or not on some time scale? And interestingly, another recent contribution of quantum computer science, which is to say that Computer science, which is to say that if you view nature as quantum mechanical, what happens here? And you start to ask questions like: as I tile a unit cell, does a system ever thermalize? Or does it have an electronic gap as you get larger and larger? Or will molecule X ever form from constituents Y? Was actually showed that these problems were undecidable. A much stronger version of something that's hard to compute that I think the easiest thing to lean on is this definition of physical undecidability, especially when it comes to. Especially when it comes to physical systems, which is to say, as the system evolves forward in time, there are sudden qualitative changes that cannot be predicted in any way except evolving the system forward in time, seeing if it happens, and nothing you measure along the way will ever indicate that it will never happen for all systems. And this is sort of the physical version of the halting problem, but it's one that you could kind of glance at and say that has a lot of hallmarks of what we now these days call emergency. Of what we now these days call emergent phenomenon: that it's very hard to calculate something on a small system, that when you tile it together, suddenly a dramatically different qualitative behavior emerges. And you might say, well, that's nice, but in order for things to really be undecidable, you have to have this infinite capacity in a model. There can't be a bounded number of states, and we kind of think the universe is finite. And so there shouldn't there only ever be. And so, there shouldn't there only ever be a combinatorial amount that you have to consider? And I'm going to lean on this quote by George Box, which says, All models are wrong, but some are useful. And what I'm going to say is that there are kind of different perspectives you can take on this problem of synthesizability, which is to sort of combine different molecules together and ask, can I ever reach the target from my products in any realistic setting? And if you take the sort of closed or combinatorial point of view, Of closed or combinatorial point of view, the natural thing to do is examine the kind of the stationary states to examine the thermodynamic states and ask questions about the free energy and try to construct these plausible pathways, both of transition states and stability that allow you to reason if it's sort of synthesizable. And I would say the more undecidable view of this is to say, well, we can always borrow molecules from the environment, so sort of an open system. And this becomes really important in catalysis. Becomes really important in catalysis and biology, where sometimes you think something is impossible and a protein or enzyme develops that is much larger than your existing system, and suddenly it becomes the preferred pathway. And I think rather than being kind of a defeatist point of view that you can't actually do anything here, I think you can then lean on data models to help you. And I'll elaborate a little more in a second on that. And I think this is important because real systems, if you want to talk about these problems, have things like side reactions. Things like side reactions, you know, they just go down routes that, for all intents and purposes, you can never come back from. You could have kinetic transport out of a system where it literally just shuttles itself away and reactions become impossible, or you can have these ideas of sort of auto-catalytic chaos. And so you're saying, okay, well, that's nice, but where else have we seen this? And I'm going to, you know, just borrow two examples, I think, that are a little bit hinting in this direction in physics, which is that chemistry is often Which is that chemistry is often finite, but in physics, a great simplification is asking questions about this thermodynamic limit. And sometimes that doesn't always work so well, and we have to hitch in a little bit of data to help ourselves out. And so the most one classic example of this is if you just have a ferromagnet and its ground state is degenerate, you take a thermodynamic model and it makes no particular distinction between whether the system should be up or down. Should be up or down. But as soon as you have the smallest fluctuation, you get what people often then fancifully term spontaneous symmetry breaking, which is another way of saying, well, my simple thermodynamic prediction didn't make, wasn't predictive. So I slapped something onto the side of it and helped it out a little bit. Maybe that's an ungenerous characterization, but you can take this a little bit further and say, well, there's more complicated systems like spin glasses, where when you truly get trapped in particular areas, for all predictive Areas for all predictive purposes, I don't want to consider the thermodynamics of the full system. I want to consider the thermodynamics of my sometimes called pure state or this basin that I'm going to be in for a very, very long time. And you give this a special name called the Gibbs measure, which kind of tells you that you do a local thermodynamics in each of these things. And I think this is indicative of us noticing that these quantities are not predictive on their own and augmenting it with some observation and some clever physics. And so just to And so, just to be a little bit even more flippant, I like to say that, you know, this physical undecidability, I would say, is characteristic of many emergent phenomenon, including life, which is to say, if you just started with base reagents, what computation on a closed system could you have done that would execute in finite time that would have told you that proteins or RNA or DNA form or that they go to a mammalian cell or something like this. Or something like this. And then, for example, you are on Earth, you look around, you look, there's a dinosaur. Is there a computation you could have reasonably done to predict the emergence of, say, humans after another extinction event? And so I just like to use this example to get people thinking about what sort of undecidability might look like for physical real systems, or rather, when it's a decent model to consider that there isn't really a closed computation I could have done, but I can look around. I could have done, but I can look around today and say humans are reasonable because they exist, essentially. And so at this point, you're probably saying, well, this all sounds very pessimistic. I'm not really sure where this is going or why you're telling me that all of these problems that I'm nominally interested in are undecidable. Isn't this just a bit of a defeatist attitude? And my argument is that it's not. And in fact, it's kind of known in computer science that undecidability is formally broken by what's called. Is formally broken by what's called advice in some cases. And what I'm going to argue is that data from your environment is sort of a restricted form of advice. And so let's think just for one more second about chemical synthesis. So synthesizability, I think, is probably one of the hardest problems there is in chemistry because it's essentially a question of whether you can make some program or not. And this is a huge open question. And one tack that chemical departments have taken. Hack that chemical departments have taken is that it's often so hard to take a molecule, look at it, and say someone could actually make that or not, that we look to nature and make it our mission to build things that we already know are possible that's in nature. And this is sort of the field of natural product synthesis. And this is just another example where we look to our environment to help us break out of this overwhelming complexity of whether things can actually be reached or not. And so now I'm going to I'm going to set this on more concrete footing by taking a few steps into quantum machine learning. And so, this is something that, if of course you haven't had a lot of exposure to, you say, wow, quantum machine learning. Did you just smash together buzzwords for the hope of maximizing impact in press? And perhaps that was some of the original intent of people, but I hope to put it on more of a solid ground for you and convince you that this is maybe an interesting field to pay attention to. So, in particular, we often So in particular, we often talk in quantum computing about what I would call computationally limited problems. So this is problems where, or this is problems where you have some procedure. It's a simple input, like a key to factor or a Hamiltonian to simulate. And there exists some known procedure classically, but one that might be very, very expensive, perhaps exponentially expensive. And on the other hand, on the quantum side, the procedure is relatively simple, hopefully linear, maybe low positive. Hopefully linear, maybe low polynomial. On the other hand, there are different kinds of problems. I'm calling them data limited here because I think it's a more familiar phrasing than what some people would call, say, query limited or in other cases, kind of depending on the setup. But basically, now the setup you imagine is I'm trying to study something about an unknown physical system. So maybe it's a sensor giving me information about a quantum state. And in this case, you can have what's called sort of a data limit. Have what's called sort of a data limitation, where if I get some number of copies, and that's the only number of copies I get because it's either expensive to measure this or the thing will disappear, then no amount of computation that I do can actually overcome this gap. If I can find cases where receiving a few copies of a quantum state in both the kind of the quantum setting where I have the ability to manipulate quantum memory gives me an exponential advantage over classical, then there can't be any. Classical, then there can't be any resolution. Even at small qubit sizes, for example, Oak Ridge's supercomputer can never come back and do anything about it, essentially. There is simply not enough data available to do it. And why are we so interested in this first prop, the second problem, is in part because we found out, and this is related to this advice case that I was making before, that there is now such a class also as data-assisted problems, meaning there are known, it has a known computational. It has a known computational procedure like the first case, but if I gave you a few pieces of data from the environment, it actually can change the overall complexity of my runtime. And that's what I'm going to detail a little bit more now. So what's a very caricatured, motivating example for data problems is so imagine I'm given some quantum circuit, u, q, and n, and some input that I'm just going to put into a small number of amplitudes of basis states. So imagine p is fairly small, maybe it's like P is fairly small, maybe it's like order 10 or something like this. And now I'm going to have some task, which is to compute the output of this quantum circuit in some Hermitian observable O on these input states. And if you look at this for a second, so this basically I'm going to allow to be an arbitrary length quantum circuit, and this to be any Hermitian operator that I want. It could be super complicated. And so you can kind of see that even if I made p equal to That even if I made P equal to one, this has to be at least as hard as BQP because I can code, say, shortest factoring algorithm in this, decide whether to accept and reject something based on that measurement. So this is a very, very hard and complicated computation. But now I'm going to imagine that I give it a little bit of data. So I'm going to say evaluate this point at a few points in this x, say, you know, p is maybe 10 here. So it's a small subspace. Maybe 10 here, so it's a small subspace of this exponentially large space, and evaluate a few points. So if I rewrite this function f, you know, it looks from this expression to be very complicated and like it could be an exponential depth quantum circuit. So it must be quite powerful. And if I sort of rewrite it cleverly, what I find out is that all of that complexity could be lumped into this matrix BKL. And this is actually at most a quadratic function of Xi with P squared coefficients. With p squared coefficients. So, in general, from like learning theoretic arguments, I'm only going to need about p squared over epsilon squared data points to generalize on any input in this subspace that I get. And so, what's happened is by giving you a tiny amount of data, I've turned a hard quantum circuit into an almost trivial learning task. And so, you look at this and you're like, well, that's kind of a contrived input. Maybe you shouldn't use amplitude encoding. And we thought the same, and that's why we kind of pushed. And we thought the same, and that's why we kind of pushed farther to try to see, understand how we could more generally classify these types of problems. And in fact, we found there were complexity-theoretic separations where there exists problems where with a little bit of data from your quantum device, you can now solve problems that classical computers in theory cannot do efficiently. Yet it is not the entirety of BQP. You can't do factoring in this way. And it's sort of a subset of this larger Poly advice class. Advice class. And so this work kind of got extended into areas like physical simulation by some of the same authors, Robert Kwong in particular, where they showed that in fact simulation of many ground state problems in physics fall into this class. And so without belaboring this too much, you start to ask the question if the fate of quantum computers is to provide training data for classical models. While entirely practical, it's a bit unexciting for what I would characterize as. For what I would characterize as one of the world's most kind of cutting-edge and interesting technologies we're developing today. And so you rack your brain a bit, you're like, okay, well, what can quantum computing do that kind of goes beyond this model of, say, predictable things from training data? And in fact, the quantum computer can actually help us learn from that data as well, if it can receive different kinds. And there were some follow-up papers that showed that even having a minimal quantum memory, which I'll Having a minimal quantum memory, which I'll describe in a minute. This is distinct from something like QRAM. It just means I have two small copies of a state in my computer. You can, in fact, resolve things and certain questions exponentially faster than if you did not have this sort of assistance. And this is one of these so-called data advantages where if I can achieve it, there's really no amount of classical processing that can shore up this gap. So those particular models. Those particular models, they work in this kind of setup that I'm going to be in more or less for the rest of this talk, which is I'm imagining a future scenario, or it could be semi-present, where I have some interesting part of my world I would like to study. This could be a piece of DNA, it could be molecules, and I have a quantum sensor next to my black hole or something like that. I'll mention some other sources later, but you could imagine this data could also come from another part of a digital quantum simulation. And I'm going to imagine two scenarios. And I'm going to imagine two scenarios. In the bottom, I'm going to say we have conventional experiments. And this is what people do in lab today. You hit an ensemble of molecules with a laser pulse and you read out classical information about that state. And I'm going to contrast this to the quantum enhanced scenario, where what I can do is rather than take my state in, measure, reduce it to classical information, I can take my state in, I can shelve it on part of my qubits, wait till I get another ping of data, collect that, and then Of data, collect that, and then operate on at least two collections of this data at once in a quantum computer. And then I do my processing. And I'm going to contrast these two scenarios. And I'm also going to look at the similar case where I do sort of learning about a physical process where I expose a probe state to a quantum process. And I can either retain that probe state or I can immediately measure it. And in this work, we're going to show that there's an exponential advantage with exactly two copies on. With exactly two copies on two different tasks and efficient classical compute. And I'll mention some fun proofs near the end that may be of interest to people as well. So a lot of what this work was, so there was some foundational work on how multiple copies can assist you in data processing, but a lot of what we wanted to do was break it down to kind of understand where is the base advantage coming from, and can we show that it works on a real device? So we wanted. A real device. So we wanted to break it down to the simplest and simplest task. And part of the challenge is: as you make things simpler and simpler, do you accidentally allow the classical learner to do this operation? And this is more or less the simplest task we came up with. So imagine this is your setup. You're given n copies of a state rho, which is a mixed state, which is a combination of, say, the identity matrix, so like a totally mixed state, with this poly operator. With this poly operator. And so this is just a general non-local poly operator that can be in X and Z. And both the classical and quantum learner are allowed to know the form of the state. So it takes exactly this form, but the exact values of alpha and p are unknown. And you can note above that it's kind of interesting. This state is unentangled, but not factorizable. And so we can realize it in a depth one Clifford circuit. And this is kind of interesting because it was as close as we could. Of interesting because it was as close as we could get to even giving it classical data, granted it's in conjugate bases, where we'll still see a big advantage, which is a bit of a spoiler, but I'm sure you could guess that was coming. So we're going to contrast this scenario of conventional experiments, where in that case, you get one copy of row at a time. And then in fact, we give it a little bit of a leg up. It can do an arbitrary POVM on that single copy. So that kind of means it could do an exponential length single copy. Exponential length single copy quantum circuit, meaning operating on one copy of rho, and then do any measurement at once, and then you discard, or the quantum enhanced case where you get a state in, and then you wait with that state and just collect another copy as this system process produces data for you, and then do operate on two or more at once. And the task is going to be take any measurement you want under the N copies for the conventional and quantum enhanced scenario, collect some classical signature. Collect some classical signature of the state. So eventually you do have to do measurements so we can interact with them. And then I'm going to, after all collections are done, I give you a new polyoperator that could be any of the, you know, four to the n that come out. And I say, predict the absolute value of this for me. And alternatively, which we'll do as kind of a classification task, I may give you two candidate polyoperators and ask you which one of these is bigger than the other. Other. And so, to kind of tell you where a lot of the work went in this paper, I mentioned that basically we were trying to break it down to simpler and simpler tasks. And the hard part of this is, in fact, not the quantum side. You'll see that the quantum protocol is quite easy. It's showing that on the classical side, given any adaptive schemes, given infinite computation, any measurements you want, you cannot come up with an efficient scheme in an information theoretic sense. And to kind of give you a sketch. Sense. And to kind of give you a sketch without too much detail of how we do this, the summary of the result is: the best conventional strategy, which can actually include limited quantum computation, requires order two to the n measurements to obtain this value on your new challenge problem to an additive error of less than 0.25. And the proof strategy is basically you reduce it to a discrimination task. So a quantum, what's happening is you say, I can do an optimal measurement at every single step of this. At every single step of this computational tree that can take many different paths. And I would like to try to distinguish between the null hypothesis: I've given you a totally random mixed state, or I've given you this state. And basically, you can, by hand, find an optimal POVM that's optimal under adaptive measurements. And it gives exponentially vanishing returns with each additional measurement. And this is sort of what I alluded to in the beginning part of the talk and will again later, that it can sort of discriminate between. That it can sort of discriminate between what you would call true randomness and kind of structured randomness in a way that's not possible without multiple copies of your state. So what is the simplest possible quantum enhanced experiment? So that was, I haven't told you about the classical technique, but on the quantum side, there's a constructive technique, which is to say, if I'm given two copies, so you can imagine that these are produced either artificially on the quantum computer as we'll do. On the quantum computer, as we'll do, as depth one Clifford circuits plus randomness, or they could be collected from some quantum sensor you're interested in. And then I'm just going to do, say, basically a C naught between pairs of qubits, also known as Bell measurements in some cases, and collect two in bits worth of information that I'm just going to sort of call a Bell sketch of my state. And it turns out that this will be a good feature, which I'll talk about later. And this allows you to perform the task that I mentioned before with a number. Task that I mentioned before with the number of samples, it's actually independent of the number of qubits, and the computation time is incredibly low. In fact, there is a formula for the exact evaluation of this task that I'll just kind of flash on the screen. It's just kind of simple bit manipulations. But I want to focus on the fact that you don't have to know the right answer for this to work. And before I do that, I'll just quickly summarize the scale of this setting of this problem again for the simplest task. So basically, you're being given Task. So basically, you're being given in copies of a state. You take inner measurements you want under the constraints of conditional versus quantum enhanced. And in fact, in the quantum enhanced in this case, I will only ever allow two copies. I'll give you a new poly operator and you predict this trace row. And in the classical processing scenario, I need an exponential number of copies. And then, of course, the compute, if it does exist, has to touch every copy that you collect. So it's also exponentially scalable. Collect so it's also exponentially scaling. And in this case, I need only this constant number of copies and a compute that scales sort of linearly in the number of copies, which is incredibly efficient. It's quite fast and works quite well. And so you'll often see quantum algorithms, and I have that very much require, I would say, a sense of fine-tuning. You really need to know the right answer before you start. And even small errors sort of blow things. And even small errors sort of blow things up. And what we wanted to ask was: is this sort of feature, given that we can write it down by hand, strong enough that we didn't have to know the right answer beforehand? And so what we're going to look at is also important because a classical learning adversary can learn features of noise and it can kind of boost performance in real tasks. And this is sort of an important distinction to make. And we're going to look both at a supervised setting where answers are labeled. And in fact, Answers are labeled. And in fact, we're going to train on small system sizes. And the reason we use a recurrent neural network is that lets us ask a similar question for any system size if we've trained well. And this is not the real data that I'm going to show except for a second, is that we'll be able to learn in an unsupervised setting as well. And so that's just a preview of what's coming. So before I say that, you might be saying, well, can you put this in the framework of where we expect to go in the future with quantum computers? Future with quantum computers. So, can you tell me where this fits in with, say, a roadmap as to where they're going to be useful? And I would say what we need to do first is imagine some place where quantum data is coming from, these n qubit states. And these could be other parts of a quantum computation. Say on a digital device, they could be analog simulation, like say you have a neutral atom system that can do an analog simulation you're interested in, and then you want to learn more about the alpha. In and then you want to learn more about the output results, and you can do very basic measurements like Bell measurements. Or it's just some natural quantum system that you have the ability to transduce or transfer the state into your computational medium. And once you have this data source, basically you need to get it into a form that you can compute on. And of course, there are many media which we now know can be computed on, and you want to move it into one that we're good at manipulating. So this might be, say, moving something. might be say moving something from an NV center into a superconducting qubit or moving something like that any place you cross it but basically you have a quantum buffer and at some point say you want to do more sophisticated computations that I'll talk about at the end you probably need to do a logical encoding but as far as we can tell these steps are probably always going to be roughly speaking unprotected from a quantum fault tolerance point of view there's going to be some finite amount of errors some failure rate Finite amount of errors, some failure rate as you're moving things into your computational media and as you're boosting it into some code that lets it sit in your memory for as long as you'd like. And once you've encoded it, so now you've say injected it into the surface code, that came with some cost that was ultimately your fidelity of this whole process. But now it sits there in its mixed state form and it's amenable to longer-term quantum data processing. So now I can protect it to any distance, I can do interesting things like. I can do interesting things like quantum PCA. But given that this step is sort of always unprotected, it's actually kind of interesting to ask: well, if we ran these things on devices today, their imperfections are kind of mimicking the imperfections of this whole process of data collection, operation, et cetera. And proofs can only go so far in some cases, farther than others, about telling you how sensitive this advantage is to imperfections in your state. What if imperfections? State? What if imperfections always reveal to a classical learner the answer? Or what if your ability to see these things break down immediately when there's noise? And so we wanted to say that the ultimate test is empirical. And so this sort of motivated us to go out and use the chip that we have at Google. And we basically mimic. So our model in this case, as we do experiments, is that we pretend this transduction step is blind as it would be in real data. In practice, we know the circuits that In practice, we know the circuits that actually make it, but we'll kind of cover our eyes, prepare the state, and hand them off to the quantum computer. And this is sort of the layout that we get. And so now we'd like to look at, say, an experimental demonstration of this kind of advantage. So we're once again in this setting where I have n copies of my state row, and I'm going to give you two challenge candidates, Q1 and Q2, and you tell me which one is. And you tell me which one is actually larger on this unknown state after you've collected data. And so, what we do is we go out and we collect data and train our supervised neural network on this kind of both bell sketch data and any conventional measurements that we know how to do. We train it onto noiseless data up to a subsystem size of n less than eight. So that's 16 total qubits because we're using two copies of the system. And this kind of shows you that. And this kind of shows you that there is a lot of signal here. These recurrent neural networks train quite well. And there's a bound on the classical accuracy set by information theory. And then we go to look at the results on how well they predict. So let me break this down for a second. So basically, we vary the system size of the state row. The challenge problem we keep the same. We keep rotating the poly operators, though. And what we've plotted here is the number. And what we've plotted here is the number of experiments required to achieve a classification accuracy, so is Q1 or Q2 bigger, of greater than 70%. And you can actually derive the information theoretic lower bound for any single copy experiment, and that's the dotted line. And then we wrote down the best known classical algorithm that actually tries to do that. And so that gives you a sense of the separation between the lower bound and how well it works. And then this is real data from the experiment device. Data from the experiment device where we went up to a system size of 20 on each system, so 40 total qubits. And you can see that by the time you reach 20, you're needing roughly four orders of magnitude less experiments. And actually, because this is a data advantage, there's no amount of computation that can overcome it so long as you have this assumption that I've blinded myself to the preparation of the initial state. So, in that sense, it's a sort of four orders of magnitude quantum advantage in learning about this. Quantum advantage in learning about this task in practice on the system since you can't do really anything to change your data collection in this case. And so you might say, okay, well, those states you just prepared, weren't they unentangled and kind of uninteresting? So you used 40 physical qubits, but your experiment didn't really impress me. So how about we change the task a little bit and say we've learned about states. How about processes? So in this situation, we're going to do a similar quantum and classical study. A similar quantum and classical setup, and we're going to say, Okay, well, now instead, I'm going to be given access to an unknown process, so I can expose my qubits to this process and take them back. And my task will be determine if the process you've been exposed to is a random time-reversal symmetric evolution, so kind of like a har random unitary, but all real entries, or the second one is more like a random unitary that can have complex entries. And this will be a very similar setup. In fact, what we'll do is just kind of do the mirror image combined with what we did before, which is you do a bell preparation of your probe state, you expose it to your physical process, and then you do bell measurements on the resulting state. And once again, you get two in-bit strings out. And this one's a bit harder to lay out because you have this process where you both need bell pairs between the copies and to do a kind of complicated randomizing unitary within them. Randomizing unitary within them, and you have to fit them all on one chip. So, this instead of being constant depth, looks something more like 1300 gates on 40 qubits, which you can imagine is kind of hard to get going with finite precision, basically. And this is the part that sort of excites me the most. Like I said before, there's often times on quantum algorithms where you run into a situation where you kind of needed to know the whole answer to get anything out. And even slight imperfections of your phase. Slight imperfections of your phase refocusing or something like that makes it feel like discovery of things is very, very difficult. And so, what we wanted to do in this case was take these features and really show that something naturally emerged. And we didn't give any labels to anything and we could have just seen that there was a difference in nature to discover. So, what I'm going to do is take these exposures to these processes, and I'm going to take my kind of bell sketches of these states, and I'm going to average And I'm going to take about 500 in the case of the quantum enhanced case and about 1,000 in the conventional case. Take the expected value of each entry and the variance of each entry. And I'm going to call that a feature vector for the process that I've been sampling, so BI. And then I'm going to do something very standard in machine learning, which is take these features and put them into a squared exponential kernel. This will just tell me some similarity between the different measurements that I've done when repeatedly. Measurements that I've done when repeatedly looking at one process. And then I'm going to do kernel PCA, which is an unsupervised technique that just kind of tries to tell me, do these look good? Is there different patterns here? Are there clusters here? And you look at the conventional measurements and you look at the first two values of the PCA for we do this in kind of 1D scrambling and 2D scrambling. And in the conventional case, I try to distinguish if there's anything to learn here. And all I see is a big block. learn here. And all I see is a big blob. I kind of conclude that, well, based on my current measurements, I think all these processes are probably basically the same. And now, if I use the quantum enhanced measurements, of which there were fewer, but about the same bit strings, you immediately get separation between these two processes without even sort of trying. So this is something that's exciting to me because you could be an experimentalist looking at data that came out of this very simple bell measurement processing for this kind of complex processes. For this kind of complex processes, and see, wow, there's absolutely two qualitatively different processes going on here. Now there's something for me to dive deeper onto subset A and subset B. So it actually feels like you could discover something rather than giving it the right answer, which is that was kind of the most exciting part to me. And we, of course, followed up a little bit on the kind of quantitative difference here, but basically this is just more of the same that if you do quantum enhanced experiments, Quantum-enhanced experiments, your accuracy increases much, much faster than conventional, which is ultimately limited in these cases. So, kind of the last note that I'm going to touch on here before I close up is just some of the parts we couldn't talk about or we couldn't do in an experiment, basically, but connects nicely to some of the work done by people here. For example, Seth Lloyd, which is to say, we kind of are imagining a scenario we were when we We were when we receive quantum data, it's always going to be corrupted. There's no engineered symmetries that we can break in to give exponential protection to some black hole that we want to look at. We kind of have to take the data as it's given. And one interesting method for taking corrupted data and trying to purify it is to kind of model that data as roughly speaking, and this is more close to what happens in a random quantum circuit than perhaps. Happens in a random quantum circuit than perhaps any given circuit, but you have some amount of uncorrupted data and some amount of orthogonal errors. This is not a perfect model of how data is corrupted, but imagine you're in this scenario. Then you can actually start to get better answers if you can sample from things like higher powers of rho. So like rho squared, rho to the fourth power. And I want to just recall for a second this paper we had on virtual distillation that gave a technique for measuring from rho squared, or at least. For measuring from rho squared, or at least expectation values on it. And curiously, it used a destructive swap operation, which has exactly this form, which you probably remember from earlier in this talk as to how we actually constructed these learning features. And you look at this for a second, you say, okay, well, that does seem like maybe quantum computers are using this primitive in order to extract data more effectively. And if one swap gets you virtual distillation, what if I go to the extreme case and use If I go to the extreme case and use the swap as a generator, this in fact gives you the quantum PCA, which is what allows you to sample directly from this principal component in even more general situations. So this is sort of like a linearized version of the quantum PCA. And in fact, there's a proof in our paper that in a conventional scenario, an exponential number of copies are needed to learn about the principal component versus only a constant number of copies in the quantum enhanced setting. So this is kind of the situation where you're starting to see. Kind of this situation where you're starting to see that quantum computers have a unique ability to process quantum data and to purify quantum data coming in from external environments that can be incredibly hard without them. And maybe that's something you would know intuitively, but I think this is giving us a better sense of the scale of that separation and that advantage. And so you might ask, what about dequantization? Well, I'll tell you about that some other time as I'm out of time. And maybe if people have questions, I'll regale you. And maybe if people have questions, I'll regale you with a little bit of that information. And so, what is the punchline that I want you to take away here? So, the punchline to me is: if we could find a suitable data source, and I would say look around at how quickly quantum sensors are rapidly progressing and doing things like gravitometry and things like that, then cloud devices today allow us to learn things that are otherwise inaccessible. And remember that we're talking here about computational advantages versus data, and there's no sense in which. And there's no sense in which, even though I'm using 20 subsystem qubits, Oak Ridge can fix the fact that it can't process quantum data directly. And so, in this work, what we did was we showed proofs of advantage in state learning, process learning, and quantum PCA, an experimental demonstration that even noise doesn't interfere in this demonstration, up to 40 physical qubits and 1300 gates. And what I'm hoping that people take from this work is that it inspires more work on quantum data sources. More work on quantum data sources, sensors, and transduction, and that we kind of get out of our mind with this idea, even though it's powerful in many settings, that quantum advantages in sensing and metrology are limited, for example, to quadratic speedups. For specific cases like learning phases, that can be true. But if you define a more interesting learning task, oftentimes you can boost this advantage to much larger. And so we should think about cases where this is applicable. Here we used features. Here we used features that were about the most basic two-copy features you can have. What more interesting processes can we do to learn even more interesting things about these systems? And can these proof techniques tell us something about existing quantum technologies or techniques? And with that, I'd just like to thank everyone involved, especially Robert Huang for a lot of the hard work of proving things and a lot of the hard technical work of catching stuff. Michael Broughton for helping make the experiment happen. Helping make the experiment happen, and Brooks for his late-night efforts to keep everything up and running. And of course, all of my other collaborators here that made important contributions to the papers and proof, and the whole quantum AI team. And with that, I'd like to thank you all for listening, and I should have ample time for questions. Thanks. Thank you very much for this fascinating talk. I'm sure there are many, many questions. Yes, please go ahead. Hi. So I have a question about the case where you were distinguishing between the symmetric random unitaries and the what was it random unitary itself without symmetries. Yeah, could you show the results? Real versus complex. Sorry. Yeah, I don't recall seeing a graph of like how the results looked like. How the results look like, like, or what you said about it. Can you go back there? Sorry. Oh, sure. Let me. Sorry, my computer is not very Zoom compatible, so that'll mean you'll lose me for a second. Here we go. Let's go to window. Yeah, so the distinguishing processes was the ones we started with here on the unsupervised setting. So this was kind of the more qualified. Setting. So, this was kind of the more qualitative result. So, this was the general unitary. This is time reversal symmetric or all real. And this kind of this is a qualitative picture of how they actually naturally group into two pictures. And this is the more quantitative picture, whereas as we scale up number of physical experiments, how quickly does the accuracy increase for the quantum enhanced learner versus the conventional one, where the task is to say, was the unitary is to say, was the unitary I gave you time reversal symmetric or not? Yeah. Ah, okay. Thanks. Sure. Other questions? Yeah, Michael has a question. Yeah. Um Yeah, so this rigorous lower bound you listed for this task, is that? I guess I'm curious where that comes from and whether that's across multiple strategies for separating these two tasks or separating these two. The maximally entangled with the poly guy, that guy. Yeah, so here, let me back up to that slide. Slide. I guess I'm asking whether this holds for many classical algorithms, whether this is for one specific one that is commonly used for this task. Good question. Good question. Yeah. So a lot of the work here was actually to rule out all possible classical algorithms, including ones that adapt based on the measurements that you've already done. And the thing that allows us to do that, so there's a reason we chose the form of state that we did. So this I plus alpha P. Did so this i plus alpha p, because these are polyoperators, the algebraic manipulations are somewhat more possible. And so, in the bounds, we can come up with an optimal discriminating measurement at every step, show that nothing else could be done even with an infinite amount of information. And so, anything you do to them can only change the total variation distance between the null hypothesis and alternate one by an exponentially small amount if you have a single system. Small amount if you have a single system. So it rules out every possible classical algorithm. Wow, that's really cool. Thanks. We have another question. Yeah, at the end, you mentioned how quantumly you only need a constant number of samples to witness this. To witness this, what was it that you called? The part where you say that the noise is sort of like global depolarizing. It reminded me a lot of what Google does with random circuit sampling. Can you say whether this means that if you adapted your measurement techniques, you can somehow compensate for the exponential loss of fidelity. I suspect that you can't compensate for the exponential loss in fidelity. What it does allow you to sort of do is differentiate between, say, if I gave you a totally random process, like a sort of totally depolarizing channel versus a har random channel. And this is not really in our work. This is in follow-up work that was done by Robert and Jordan. There are strategies using multiple copies. Using multiple copies to distinguish between many hierarchies of different randomness and sort of scrambling that's only possible with multiple copies, because what the quantum interference is sort of allowing you to do is to pick out commonalities between them. And the har random case kind of has a strong pure state that can be identified with this method, whereas the randomness does not. And there's actually other symmetry subgroups they work on for this, say like skew symmetricly general. Skew symmetrically generated, so orthogonal group par random unitaries. But I don't think it would allow you to do some kind of exponentially improved error mitigation that says lets you sample from the same har random state with higher fidelity. It could be, there might be some contrived setup I'm not thinking of where if you learn the sketch of the state, you do very well. I think the problem you'd run into here is that if you notice, Into here is that if you notice, a lot of my bounds obviously scale. Let's see if we find one here, polynomial in the precision. And so this allows you to define some course task. Yeah. Basically, whether it's yes or no. But if you needed to get probabilities right in a har random matrix, the entries tend to be very, very small. And so that precision you need scales exponentially. And so I think you'd need many samples for that task. Need many samples for that task. Yeah, that makes sense. Thanks. Any more questions? I have a question. Is there some intuition on why the classical strategy is so difficult? A strategy is so difficult for this one for which you have the quantum strategy that works well, which is separating the expectation values of absolute values of polys? Yeah, good question. So I don't know that I'll be able to give perfect intuition, but depending on what you're familiar with, I can maybe get a little bit closer to a kind of coarse-grained, simple explanation, which is the simplest feature we needed for this separation to exist. So we got rid of entanglement. To exist, so we got rid of entanglement, but what we couldn't get rid of are multiple incompatible bases in the state. So you need to be able to measure X and Z at the same time to some extent. And this is sort of forbidden by fundamental quantum mechanics that if you measure X, you destroy your information in Z. And this is sort of what the classical is bound by. It turns out that you can bend the rules a little bit with multiple copies. And this is the trick that we used. And this is the trick that we used, and I think is in need of generalization to larger systems. But basically, for polyoperators, there's a trick where if you append a copy on a second system, now the duplicated version becomes completely commuting. So by doing that, we can measure all of them at the same time. But what we've done through duplication is remove sign information. So we can, in one set of measurements, I can tell you what. I can tell you whatever values of absolute value of polymeasurements that you want in a very short number of measurements, but I lose sign information. And in an earlier paper, they showed that you can finish that by doing a clever majority vote kind of union bound scheme to get the sign back. And the question that we asked was, say I stop without a sign. Is this still hard classically? And that was kind of the nature of the proof. And it turns out that it is. But that's the essential trick. We do a copy. We do a copy of non-commuting operators to reach commuting operators. In doing so, we lose the sign, and by squaring it, we go over one over epsilon squared to one over epsilon to the fourth in our precision requirements. Yeah, that's pretty helpful. Yes. And is there some complexity class in which the classical thing resides, which separates it from the quantum one? Is there some intuition there? Quantum one, is there some intuition there? I don't know that in this case, so I'm not a complexity theorist by training, but for a lot of the ones we often talk about in quantum computing, it doesn't fit so well because this is more of like a query separation or a data advantage separation. It in turn has a consequence of a needing to touch all the data computationally. But I'm as a non-complexity theorist, I don't know how to best fit it into these. I don't know how to best fit it into these classes. Probably someone here knows more than me. Thanks. Thanks. Are there more questions to the talk? I somehow don't see anybody speaking up, so Speaking up, so I would suggest to thank Jared again for this fascinating talk. Cool, thanks, Jared. Yeah, thanks, everyone. Thank you, Jared. I'll stop the recording.