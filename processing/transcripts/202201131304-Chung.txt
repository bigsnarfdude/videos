There's geeks. Is there a benefit to that? And so the short answer is yes. And in order to explain why, I'm going to step away from the color modeling task, the delayed response task, and instead talk about an orientation version. So really what was being described when they were discovering, when they were discussing the idea of having these attractors is if we think about the landscape. is if we think about the landscape for this particle, we can kind of have two forms: a flat landscape where only diffusion is going to be a component, or something like this where we have our wells that we can fall into. And so if we think about this in terms of orientation, we have now these wells at particular orientations or locations that matter instead. And so if we think about this and take it to the point of a neural The point of a neuronal network, as has been discussed quite a bit now, we can think about this in terms of excitation, where we have preferential excitatory neurons for particular orientations. And so now there's the selectivity for a particular orientation, and we have other neurons that are obviously going to overlap. And then this broad inhibition. And so what Zach had done previously with Bard and Brent Oran. And Brent or on is to show that you can build a model where we can consider just this relationship. So here we have different preferred orientations. And so for a particular position, so here we're going to talk about this orientation. You have your excitatory connectivity that is going to be sort of locally described, and this broader inhibition. And so what happens now when you're shown a cue? Now, when you're shown a queue and you have this working memory task, is that you get a bump of activity. So, if we now have the same sort of neuronal network model and we activate, we have a bump of spiking activity right at the cue. But thanks to this diffusive wandering, we're going to have the bump of activity that's going to just drift in time or diffuse in time. And so now the question comes: well, how do we? Comes, well, how do we minimize this wandering? And so this is where those biases, right, and this heterogeneity can actually come in. So if you have this homeostatic plasticity and you really build in that whatever this local orientation for your excitation, if we really build that up so there are preferences, we can actually minimize the bombs wandering. And so what they found was that when we have these potential That when we have these potential wells, the bump position is retained much closer. So, in those attractors, as compared to this much more broad-spanning wandering. But there's a trade-off. So, the issue here is that while this stabilizes the diffusion, we have these discrete states. So, you have the sort of lower resolution, right? You have a trade-off between how many states you can actually retain and Actually, retain and this wandering itself. And so there seems to really be a sweet spot where we have the bump variance that's going to increase in time. So if we just have a particularly flat potential landscape, it's going to increase substantially. And it's minimized when we have instead these wells of n equals four. But if we increase that, where you think, oh, maybe this will actually make it better, that isn't necessarily. That isn't necessarily true. And so there's sort of this trade-off we have to consider. So, what this really tells us is that there is a benefit to having this network heterogeneity, and that this is obviously something that humans have, right? So I've now shown you an example of color bias from the Panocello paper, but if we take this one step further, it's known that there's also these colours. It's known that there's also these cardinal direction biases. And so, where is this coming from? We're only considering in these last cases a homogeneous distribution of inputs. So assuming every orientation where every color is equally likely. But what I want to talk about today is a much more natural version of this, which is that in the real world, in natural scenes, there's actually heterogeneity in the Actually, heterogeneity in the inputs that we're going to observe. So, the example we're going to use today is the idea of cardinal directions, where we're going to say that if we're sampling all these different orientations, it is more likely that we are going to sample at a cardinal direction than on any of these obliques. And so we can describe this as just this probability function where the peak probabilities are going to occur at the cardinal directions. So now So, now that we have this heterogeneous distribution, we can go back to the original question: what's the best potential landscape for a particular distribution? We've already seen that that may or may not actually be corresponding to what the particular distribution looks like. And so, the way that we can go about this is first we're going to describe our actual particle models landscape. And so, here we have Landscape. And so here we have a very simple model where A is going to describe the amplitude and N describes the number of wells or the frequency. And so we see that we can have these different landscapes. And the dynamics for a particle in this landscape are going to be described in two components. Just like we saw before, there's the drift, which is going to draw us into these wells, and the diffusion, which is just the noise component that's going to be associated. That's going to be associated with wandering. So, the other side of this is we need a metric by which we can actually measure how well we're doing. How do we actually compare these two? And so, in this case, what we're going to use is a metric of distortion. And distortion in this case is across all of these possible theta values. And so, what we're considering in this case is not just how far a particular response is from its original input, but also what is the result of the response. Original input, but also what is the probability of a particular input? So we can incorporate that heterogeneity. Then what is the probability of a response given an input? And as well as what those actual distances are in terms of the input. So if we use this metric, we can start with the homogeneous distribution once more and just get a sense for what this metric actually tells us. And so if we compare the distribution If we compare the distortion for a various potential landscape using a homogeneous distribution of inputs, we get curves that look like this, where we see a drop in distortion. So we're minimizing our error as we increase n, but only to a certain point. And so as we actually increase the number of wells past a certain point, we see that we're actually increasing in distortion again. And that these effects are much more obvious. And that these effects are much more obvious the longer our delay period is. Going from one second to 20 seconds, we see this is increased. So we see that this is a dip in itself, but what this doesn't tell us in absolute terms here is how does this compare to this homogeneous landscape? How does it compare to a flat potential? And so, just very simply, we can adjust this by saying, okay, so what is the distortion? Okay, so what is the distortion for a particular n versus this n equals zero case? And so what we see is that almost all of these situations, our distortion is well below what we would expect if we were actually using a homogeneous landscape. So again, we see that improvement that has already been described. So this gives us the tools now to interpret what happens if we actually have a heterogeneous If we actually have a heterogeneous distribution, if we're not sampling uniformly. And so again, we're just going to use our cardinal direction potentials here. And I'm going to show you that same sort of plot, but for this situation. So here, we're again comparing to what happens if we have a flat landscape. And we see again the same trends where our distortion is below what we would expect. And with time, we see this. Time we see this trend to be even more reduction in error. So let's just focus on: well, what happens if we consider specifically the situation where we have the same number of peaks in our probability as we have wells in our tractor model? So, in this case, where m equals n, so n and m, what we're really seeing is that We're really seeing is that we have for every peak, so at a cardinal direction, this peak is going to associate with an attractor. So here we have a well, here we have a peak. So this seems like it would be a pretty straightforward answer. You have attractors at the points that are most frequently selected. But what we actually find is that it's not so simple. And indeed, what we do is this is assuming This is assuming that this amplitude for our potential landscape is pretty small. And so, what we actually find is that as we increase the amplitude, AP, we end up needing more wells in order to still find the lowest amount of distortion. And that this is varied as well by our delay period t. So, here I've just plotted some heat maps. Plotted some heat maps at various time delay periods. And so, what we see is that at these higher values of A, we actually jump. So, it's not just that we're incorporating, let's say, one more well, but we actually jump in what seems to be harmonics. So we've doubled. We go from four to eight. And then this trend continues. But again, just like we had seen before, there's this trade-off. And so there seems to really be this balance between. Really, be this balance between where this optimal is between drift and diffusion. And so we see that as we increase our delay period, we actually can retain that four-well solution for much longer. And so this really brings up an interesting point, which is just saying that when that amplitude is large, our distortion is minimized actually when n is greater than that. So this is really getting at the idea of what the landscape itself. Getting at the idea of what the landscape itself should look like. But I want to take one more moment and just consider the contributions of drift and diffusion separately and how those are really incorporated here. So remember, we were discussing how the dynamics could be breaking into these two pieces. And so I just want to look at drift first. So now what we've done is if we ignore the noise component for a moment and just consider drift, what we see is that as we Is that as we increase the number of wells, there's this pretty time-invariant beyond, I guess, five seconds or so effect where more wells are obviously going to reduce distortion. And just to give some intuition as to why this is, we can take an easy example right at the end here and say, if I have two wells, drift is only going to draw me downward into these wells. And so I may. Wells. And so I may be drawn further away from my initial response if I'm going to a well that is further away versus this higher resolution. So again, we're back to sort of the quantumization of this. Now, on the other hand, if we consider diffusion, and in this case, we can consider diffusion by itself by only sampling at these attractor points, right? So only selecting. Right, so only selecting inputs at those points. We see that there's actually two different options here, right? There's two different features and dynamics that we can get out of this. In a low noise case, we see that there's a drop in distortion that's associated with the number of wells, but it's pretty time invariant as well. And in the second case, moderate noise, actually, we're going to talk about that in a second. Actually, we're going to talk about that in a second. So, in this low noise case, again, if we just look at a really easy example, we can understand this in terms of where, how far can we really move, right? We have low noise. We're obviously going to have to overcome these peaks in energy. And so if you have higher resolution with larger N, they're closer together. In the moderate noise case, though, we're back to this trade-off effect. And so if we actually Effect. And so, if we actually look at this one, in this case, we have to think about it in terms of you have more attractors, but you have enough noise that you can possibly hop into a separate well, right? Versus you get this stabilization in this particular case because it takes you longer to escape from this well. So, this is just to give us a little bit of intuition as to what's going on when we have these particles themselves. Now, Now, this assumes, this makes a very big assumption, which is that we already know we have a heterogeneous distribution. But the reality is that that's not necessarily true. So how can we learn this and how can we describe what learning looks like? And so what we're going to describe in this case and propose is the idea of an experience-dependent learning model. And so if we have this heterogeneous distribution, This heterogeneous distribution, we're going to assume that going in, we know nothing about it. So we start with this estimated potential landscape that's flat. And what we need to do there is we're going to approximate Bayesian sequential learning for this particular angular distribution. And so the way to do this is to update iteratively with experience. So for every trial we go through, we're going to have to update this and we're going to update using sort Using sort of a flavor and a von Mises distribution, which allows us to sort of produce this Gaussian first circle. There's two elements I want to just draw your attention to in this. The first one is that we are using this as a negative because we want to develop these troughs, right? We want to develop our wells up. And the second part is that kappa here is going to be what's really controlling our spread. How much are we learning and how... How much are we learning and how deep are we making these wells on each iteration? So let's just get a little intuition as to how this works. So initially, let's say that we have our first queue at the orientation theta equals zero. So what's going to happen is we have this update that's going to build a well initially. So this is retained into our second trial. And now again, we have a new piece of information. We have a new piece of information. We're going to build this. We see that we're starting to build only at the experienced locations, right? But we're building this map. And so by three, we start to get really a more heterogeneous particle landscape. And so that by the time that we have t that's much greater than one, we've recovered something that's very similar to what we were describing before, where we have these attractors at the most likely. At the most likely at these peaks. So, keeping this in mind, now we can say: great, can we actually look at the error and how rapidly can this be learned? What are the effects here? And so we'll start with the situation again where we have our cardinal directions. And if we look at over the course of 500 trials, we can see that as we started with this flat landscape, we start to see that we have these peaks. We start to see that we have these peaks and these wells. So that by the end of 500 trials, we've really recovered what we would expect. And in terms of distortion, what this looks like is that we see this was very rapidly learned. And so here, each point is a different trial exposure, a different experience. And we've kept a running average of what the distortion looks like. And we see that it plateaus pretty rapidly within about 50 trials to this lower level of error. However, we asked the question. However, we ask the question: well, can we learn something like n equals 8 instead? So let's consider a situation. As we saw before, there were times where it was more optimal to have more wells than n equals 4. And so in this case, let's say that we have the probability is distributed such that you have the peaks at cardial directions and the oblique directions. And what you find is that this is. And what you find is that this is much harder to learn. So, within the same number of trials, we can't recover that same sort of really nicely replicated landscape. But even with this amount of poor learning or less ideal learning, by the end of these trials, we've still managed to come to a plateau that is below the distortion level of a flat landscape. So, we are seeing. Landscape. So we are seeing some improvement just with any amount of learning. So all of this helps us to understand sort of the dynamics from one perspective, but we want to take this back to the neural mechanisms. And so in order to do that, we're going to return and use a neural field model. And so at its core, we have this neural field model, which is going to be described both in times of space, which is X, and time, which is T. And we can break Which is T, and we can break this into a few pieces for those who may be less familiar with it. So, first, we're going to consider obviously the average neural activity at a particular location in time. Then, as we discussed previously, we have to consider synaptic coupling. So, in this case, we're talking about what are the weights and sort of the relationship between this. We have to have a firing rate function. And now, as we've discussed, there's this diffusive noise. There's this diffusive noise that needs to be incorporated for the wandering of the information in that delay period. So we incorporate a noise component. And finally, to make this emulate the task that we had talked about at the beginning, we have to have an input. And so this input is going to correspond with the queue that we're being given. And so walking through this, now we're really interested in what happens in the delay period. And so in this delay period, we And so, in this delay period, we've retained this same equation, but our input now goes to zero. And we can look at the dynamics of what's happening to the neural activity. And so, this is predominantly described by the noise here. But there's one key element that we're missing. We still need a way to incorporate that spatial heterogeneity. So, the way we're going to do that is by having this particular function included in, which is going to be the periodic. Included in, which is going to be the periodic spatial heterogeneity. And so here, what we're really describing is that we're going to have these peaks where we're going to have stronger potentiation. It's going to be recurrent. And then through homeostatoplasticity, we can have more of these ditches in between. And so let's just get a sense again of what this means for our actual model and what this means in terms of dynamics. So first, Terms of dynamics. So, first, let's recover this original model. So, setting this equal to zero, we can consider again what happens to the neural activity through time in that delay period. So, here we have an input. Our input is going to have this peak in activation right at the cue point. And we'd imagine that if there was no noise or wandering, we would follow this trajectory through time and retain the memory perfectly. But as we see, there's some amount of diffusive wandering. See, there's some amount of diffusive wandering, and so this is due solely to the noise. But if we instead incorporate this spatial heterogeneity, so in this case, again, n equals 4, so now we have this attractor really including that potential. So excitation is much stronger at this point. We see that it's going to really hug this line and minimize that wandering so that we have a stronger memory. So, that we have a stronger memory and correspondingly lower distortion. So, in this particular case, this is as was described more in the 2013 paper, there's assumption that we already know that we should use this spatial heterogeneity, right? There's an assumption that we know that there is a heterogeneous distribution. So, what if we're back to that? So, what if we're back to that same situation where we need to learn? So, remember, this was the function that we were using to describe the periodic spatial heterogeneity, and this h of y, we described as cosine and y. But what if instead we take this and instead we make it a function that's in terms of time, that changes with experience? So, in this case, what we're going to do is very similar to what we described in the particle model. We described in the particle model earlier, and so we can walk through it once more. So, first, we're going to have a Q yet again for our first trial, but we're using just a flat, there is no spatial heterogeneity. So, we're just using the original neural field model that we were discussing. It's only after we have this experience that we're going to update in this sequential method, just as we did before. Now, the difference is, unlike before. Is unlike before, so we're still updating using the version of von Mises distribution, but now we have to think about this as sort of these peaks rather than these valleys, right? Where do we want this potentiation to be localized? And so we assume that this potentiation is experience-dependent. So, as we observe more locations, so again, if we have a second location, we're now going to include this previous. Include this previous potentiation, right? So incorporate our previous knowledge, and then update it once more, such that as we go through, after many, many trials, we're going to end up recovering something that is very similar to this cosine ny. And so we're sampling because our inputs have been sampled heterogeneously, we're going to have this experience dependence that all Have this experience dependence that also is going to update to look heterogeneous. And so, just returning back to what was described in the 2013 paper, the mechanisms behind this can really be attributed to the idea of long-term potentiation and just long-term plasticity in general, along with homeostatic adjustments, right? So, we're modulating with this experience dependence. So, now if we just take a look at, well, what does this really mean? A look at well, what does this really do? How does this version of the neural field model compare to some of those others? We can again consider this distortion in time, where we now have each point is again a trial. And we see that pretty rapidly, compared to the homogeneous version, the learning model is going to rapidly start to trend with this heterogeneous model. So by the end, the average distortion is very similar. Distortion is very similar. And if we actually look at what those functions look like at the end, we see that we are recovering the same sort of peaks and valleys that we would expect. And so just to put this sort of in a summary version, what we're really saying then is that this learning model is reducing distortion, though maybe not exactly to the same level as the heterogeneous model where it was completely known, but we're really trending towards that. Completely known, but we're really trending towards that. And so, with more experience, we're really improving our error here. And so, just to wrap up, I just want to summarize what we've discussed today in our new work here, which is that these biases that have been observed in humans may actually be attributed to the fact that there's environmental heterogeneity. And so, today we talked about an example where we had cardinal directions. Had cardinal directions. And so this may explain sort of the heterogeneous working memory mechanisms that are being seen and how these two relate. And then in terms of learning this, we have discussed how environmental heterogeneity is learned through experience-dependent update roles. And then from the perspective of a neuronal network, we can discuss how network connectivity can be modulated via long-term plasticity and homeostasis. And homeostasis. And so, moving forward, some of the things that we're interested in doing is asking questions such as: well, what about a less structured neural circuit? So, if we have a combination of both random and structured connectivity, how does this work? And then returning back to the original task that we were discussing, how do the models we're proposing here compare to both the behavioral data, as well as what happens in a situation? What happens in a situation where we have multiple samples or multiple items retained at a time? So, in the case today, we only discussed if we were retaining one queue at a time, but if, for example, you do have three samples, we're dealing with three bumps, how do we actually tune the recurrent connectivity so that we can both minimize the bump interaction and the bump wandering? And so, these are some of the questions that we're going to focus on moving forward. To focus on moving forward. So, again, I would just like to thank my mentor, Zach, for all of his help with this. And I will take any questions. Thank you, Tara. Any questions? I have a couple. I could get us started off. So, I guess that maybe in some ways a little bit hung up by you. That maybe, in some ways, a little bit hung up by your motivating example, which I found to be really intriguing. So, if this distribution and color vision memory has these bumps, are they consistent across people? Yes. And they're consistent even if you do give them multiple samples. So, like, it's retained and it's in multiple. And it's in my brain. Is it a function of where we grow up or is it a function of our biology? The sort of the experience dependence thing makes me think it would depend. Like if you grew up in the far north, you might have experienced a lot more white than somebody who grew up in the tropics, for example. Yeah, that's a good question. I'm not sure. Sure, but my assumption would be: right, whether or not you grew up in the far north or you grew up in the tropics, if you read children's books, aren't you going to be exposed to like, there are certain colors like that we're all sort of exposed to. It kind of makes me think of, you know, the pseudo-synesthesia idea where everyone had the same toy growing up. And so they, in essence, all became synesthetic, but Became synesthetic, but it was really just that they knew all the colors from the toy. Oh, that's funny. No, I hadn't heard of that. But I guess my point being, right, that the colors that it's really focused on that seem to be mainly selected are things that I think everyone would have exposure to. And we're talking about such, I mean, I actually, I don't know, it's probably a bit of both, but I would assume that some amount. I would assume that some amount is experience dependent, that you learn it's easy to just answer certain things. Could it be language dependent? Would you have names for the colors? Sometimes that differs, right? Like, in English, we have blue, but in like other languages, there's like a blue and a light blue that are different. Maybe that would make a difference. Yeah, I mean, they do seem to be pretty like, um, Pretty like periodic. So, like, there are peaks that are pretty evenly spaced, too. So, is it just because that makes life easy, right? Because then you're binning. I also wondered if the diffusion was completely symmetric, like in practice. Rather, I mean, This rather, I mean, I see in your theory it is Svetric, but do you think that drift might have favor the direction from which it, no, diffusion might favor the direction from which it drifted? You know, like you wouldn't fall quite so far down the hill. Yeah, that's an interesting question. I haven't really thought much about it. I guess in my mind, I'm thinking about the diffusion as not really strategic. Strategic, you know, like it's not motivated, but um, but yeah, it's a cool idea. Yeah, cool. This is really interesting. Oh, I see a hand. Sorry, Priscilla, please go ahead. Thank you. At the beginning, I liked your talk very much, and I'm really glad I met you at the gathering. A gathering place and found out that you were talking now. So I managed to hear it. At the beginning, I thought that the distribution with the four maxima was going to actually allow a better estimation because of being a less Because of being a less complicated distribution than one might have had. And at the end, I got the impression, well, this is just the beginning. You want to get finer and finer. But what about the first thought that there might be an efficiency in having a more discrete Discrete distribution to be aiming for? So there, I guess there's two sides to this. So I think if you look at the early data that I presented also, right, there seemed to be the sweet spot. There really is like a sweet spot where there's a certain number of attractors, right, like the four, which seems to come up a lot, where you get Where you get a nice balance between really minimizing that diffusion and not making things too difficult for yourself. But what I think is really interesting and sort of I'm not sure where to go with it yet, is the fact that there are times where you want to actually double it, right? Like instead of four, there's eight troughs that were. Eight troughs that were better. But I'm not really sure how you would implement that in the brain because, obviously, if you're learning from your environment and there's four maxima, right, then you're more likely to obviously also build in these four maxima. So I think, you know, there's a lot of work done on how many. On how many items you can even retain in working memory. And so I think this complements that in some ways, asking the question of, well, is it because we've now discretized, right? And so there are certain places where you're going to fall. And so that kind of plays in. I don't know if that fully answers your question, but we can always talk more about it later. Yes, thank you. Can I ask a question following up on the point that you brought up at the end of your conclusions about multiple objects? How might your neural model or some variant of it account for effects of load on memory accuracy? Like if you have to remember multiple objects? I can kind of see how the drift diffusion model could be modified to account for that. Model could be modified to account for that, like having a higher drift rate for more objects, but it seems a little more difficult to reconcile with the neural model. Yeah, this is an excellent question. So this is just something I've only recently started to think about. Because if you think about it as simultaneous cues, right? So now you're getting these multiple bumps, right, or these bumps of excitation. So the question is really, Is really, how do you differentiate your bump and you have these interactions? And so I don't know if the answer is sort of modulating the potentiation. So rather than having these really strong rules, right, retained. I think the answer is, I don't really have a good answer yet. But this is exactly, this is where it starts to get really. Is exactly this, is where it starts to get really hard, right? Like, at least with there's sort of this attraction and repelling that's known. I think Zach can take this one. He looks like he's ready to jump in. Well, yeah, I was just, you pretty much said it. So, I mean, the main sort of bump attractor models that consider kind of multiple items or multiple item directions would consider. Rather than just like a ring model with one bump, you could have a ring model with multiple bumps that then interact. And there is evidence. So Albert Comte and others have looked at models of this and then matched them to psychophysics data on like, you know, increasing loads to like three, four, five items can partially be explained by these sort of interacting bumps. So each item is a bump. Interacting bumps. So each item is a bump, and then they sort of run into each other and extinguish each other. And if you have two items that are close together, then they become more difficult to distinguish. For multiple features, like if you're trying to store like orientation, position, and color, oftentimes people have represented that with like type models rather than just like a single ring. Ring. And then, yeah, you can also account for multiple item interactions there. So, yeah. Okay, thanks. Yeah, that makes sense. Yeah. Okay, cool. Thank you very much, Tara. We'll move on to our next speaker. Su Young. Is that how I say your name? Forgive me, please. Yeah, hi. So let me share my screen. 