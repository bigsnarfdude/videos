The title of his talk is Elliptic Equations with Degenerate Weights. So, thank you very much for the introduction, Igor, and it's a pleasure for me to give a talk here, although it's only online, but it's a pleasure to be here partly in Canada. So, this is a talk about elliptic equations similar to maybe the one of Dominic, but I will talk about weights, the weighted case with digital weights. Weighted case with digital weights, and you can see here these are a few of my co-authors: Anna Balci. You can see already still on the video here, Antonella Passerelli di Napoli, Rafaela Jova, Sunsik Bjun, and Josik Lee. And these are all somehow contributing to the parts of the talk here. So let me, okay, I have to move this one thing here a little bit to the side that I can see more. So I. So, I want to talk here about weighted equations and what is the most simple basic weighted equation? That would be the Laplace equation. I think you had this already in the talk of Tadele as an example of a local equation. So we have here this Laplacian. U would be a function, scalar-valued or vector, this would be a scalar-valued function here. So, this is your vector, and you multiply it by a matrix A of X. The matrix would be symmetric, it would be positive definite. Symmetric, it would be positive definite. But the important thing is that it's possibly degenerate. So at some points, it can be going to zero, some points it go to infinity. And this would be here, this weighted Laplacian. Sometimes you can write it here like a little f, but I prefer to write it in divergence form because then it's better to formulate the results. And even it's even better to change the right-hand side F into the following form that also on the right-hand side, I write the matrix time. I write the matrix time g, which is a simple consequence of that A is pointwise invertible, so we can always just multiply by A to the minus one. So this is the kind of equation here that we look at. A is positive definite and positive degenerate. And the kind of thing that we are looking at is maximal regularity. So we want to transfer the regularity from G to gradient of U. So this would be the most important estimate here for the course of the talk. For the course of the talk, and for some space X. So, if there would be no weight, this would be some kind of this Kaldon-Sigmund theory operators mapping from one space to the other. But now, because it has a weight, it's a little bit more complicated. Actually, we didn't start the project with the linear setting. Actually, we started the project for the weighted P-Laplacian, and then later we realized that some aspects are new in the linear setting. So, that's why I also focus the linear setting strongly more. So, the weighted So the weighted P Laplacian looks like this. Basically, this term here comes from the fact if you minimize the gradient energy but multiply it by some positive matrix, then the Euler-Lagrange equation gives you this operator in this form. There are several versions with different authors, but this is basically the form that I will use. You have here an m squared. If you plug in p equals 2, then this one would drop out and this would by... And this would just be the matrix A, so we recover, or it will be reduced to the weighted Laplacian. So that's a special case. Okay, please, anytime, don't hesitate to just ask a question if I'm too fast something. Okay, let me first talk about Maya's famous limiting example because this gives you exactly the feeling already what is possible and what is not possible. So Maya has found this nice vectorial example, which is, but this is the first component. Example, which is but this is the first component of the vectorial example, so it's a scalar example. Now it's a function in the plane, scalar valued. You have a symmetric matrix which is like this, and the function itself is harmonic with this weight. And if you look at this, this is basically the weight is a perturbation of the identity. So because this is almost the identity and there's a perturbation and here is a small perturbation. Yes? And it's a very nice weight because you have uniform ellipticity. Weight because you have uniform ellipticity. I mean, the upper eigenvalue is one, the lower eigenvalue is almost one. So it's nicely uniform elliptic, and even then, a lot of things can go wrong. First of all, we know that it's a weak solution. That's easy to see because it's L infinity uniform elliptic. Second of all, we see that the function is Hölder continuous because you see this is the exact Hölder continuity. And if you do the Mosa-Nash-De Georgi theory, that's also what you would retrieve. That's also what you would retrieve in the proof, but immediately here we see what kind of regularity is. So, and then if we take the gradient, the gradient would behave like x to the minus epsilon. And you can directly see what is the integrability or what Matsinkiewicz space of the solution. So the gradient is exactly in LQ if Q is smaller than 2 over epsilon. So this is exactly what we see. And you see, this is a perturbation of identity and the kind of oscillation. Identity and the kind of oscillation has the size epsilon, so the integrability depends on the size of the oscillation in this astonishing kind of linear way. Yes? So what is the history of the weighted Laplacean? The results. So we are trying to transfer the regularity from G to gradient U. And what kind of conditions on A allow us to use this maximal regularity or transfer of regularity? Regularity or transfer of regularity. So, first of all, it was shown by Myers. Basically, you don't need anything regularity on A, just uniformly ellipticity, then you always gain a little bit integrability. That's why he invented the example from the previous page. If the coefficients are continuous, this is by now classical. Maybe Gilbert Trudinger, you can look in the book and then you get integrability up to LQ. Well, this was later extended. Well, this was later extended by DiFazio to the VMO, but if you think like the BMO, if you close the function, the continuous function in BMO, then you get basically VMO, but he had got the right proof, certainly. And it's still uniformly elliptic and almost continuous coefficients. And what we want to do is we want to include the case of degenerate weights. Sorry, yes? Just one remark. One remark. The one that you claim is classical is actually, I think, a result of Stampak. Yeah. This could be, but I mean, I'm a little bit vague with this. This is the only reference where I'm a little bit vague. Just because otherwise I forget. I think it's Stampak from some part of the 60s. Okay, I want it here and so it's fixed on the video. So actually, actually, this is what they actually. What they actually did, they reproved the Calderon-Sigmund theory without the singular integrals and by means of interpolation. And so I think that Gilberg and Tülinger, they go back to the singular integrals proof while this was proved before by Stampaki and Campo. Okay. That's a historical reason. Thanks a lot. So, what we want to do is we want to go. So what we want to do is we want to go away from this uniform ellipticity in this talk. We want to have something like degenerate weights. And the degeneracy would be like either it could go to zero or it could go to infinity. This would be maybe the standard example, but the delta has to be small in sense of that some oscillations later are small. So this would be the standard example. You could think of something more complicated, but that's the basic case that that is a limiting one. Okay, this is more complicated than the Myers. This is more complicated than the Myers example because that was not singular, but still the Myers example will give you the limit of the theory. Okay, degenerate weights. Here I repeat just what was the weighted Laplacian. And let me write down what is uniform ellipticity. So you have this matrix A. You can multiply by X and C, and you obtain the highest eigenvalue and the lowest eigenvalue. And they are related. They're both positive numbers. And this is a uniform ellipticity. But what we want to do. Uniform ellipticity, but what we want to do is there's a bound for the upper eigenvalue, and we have a bound for the lower eigenvalue, but it's basically the same up to a factor, which is this lambda squared. It's squared because later I take the root of everything, and then that's why it's squared here. But if you look from this, like from a linear algebra point of view, because I'm also doing kind of numerics usually, this one would be the largest eigenvalue that. Lower in the largest eigenvalue, that's just a spectral norm here. And if you write it now, the this bound here you can rephrase it, and I like that one. It's just that the condition number of the matrix, that is bounded by lambda square. And everybody knows that the condition number determines things in numerics. So that's, I think, so you can think of degenerate ellipticity as uniformly bounded condition number. Okay, what is the kind of space that we are looking at? This behaves basically like behaves basically like something like a weight x although it's it is vector valued it's a matrix but it behaves like this so this thing here would be if you multiply by u would be integrable so the gradient is square integral with this extra weight mu that's the natural energy space okay what has been done the first thing on weighted equations always one looks at the paper Equations always one looks at the paper of Fabes, Kidnick, Sarapioni, very nice paper. What did they do with respect? So they looked into the class of weights which are Muckenhaupt weights. If you don't remember what it is, it's just for A2, it's very simple. It says that the average of mu, the weight, and the average of the reciprocal, they basically cancel. And so they are basically like a constant or bounded by a constant. So assume that you are in the setting of Muckenhauer. Assume that you are in the setting of Muchenhaup weights, and then you have all the theories from function spaces available, like Poincare's things, and so on. And if the data is nice enough, don't care about this. Then the solution U is polar continuous for some alpha. It is clear that it's only some alpha because Maya's example is the limiting example and alpha can be small if you want. But the nice thing here is that this already includes the case of weights with a small singularity. Weights with a small singularity because these are still Muchenhof weights. Okay, so this is the right setting in the beginning. But we aim for something more. We aim for maximum regularity of the gradient or gradient regularity transfer. So we have to assume more. Muchenhaut is not enough. And then there's this very nice paper of Chao, Mangesha, and Pan or Fan. I don't know. Maybe you can tell me later, Tadela, how to pronounce it. And they proved the following. Assume that the weight is the Mockenhauf weight, but additionally assume that the weight has some mean oscillation which is small, then you have this transfer of regularity. So if G is bounded in LQ with this extra weight, then the gradient is in the same space. And this is for all Q, and the smallness depends on Q. Okay. I think in the first paper they used here this BMO2, but Use here this BMO2, but this is not necessary, and they use later in the other papers this version. So, I put this here. So, what does it mean? It means that you take the average of your weight, but averaged by the size of this mu. This is the upper eigenvalue. So somehow you measure A with respect to its own largest eigenvalues and the oscillation around it. And this has to be small. This is this BMO1 mu space. PMO1 mu space. And if this is small, the result holds. But this is very nice. This is the first result that basically includes these kinds of singular weights on a scale of gradient regularity. What is a little bit unfortunate? I try to browse through all the steps of the proof. There's no explicit dependence on delta of Q, but I think it is exponential, but I will tell later. I have one slide about that. Okay. So, what is our result? So, still, this is the equation here. Same thing. We have degenerate ellipticity written here like uniformly bounded condition number. And now I define omega to be the absolute, the largest eigenvalue, but the square root, because that's just easy. I need the square root to somehow that this is like m squared of x. So if you So if you take any exponent, smaller or bigger than one, and if the oscillation is bounded by, this is some constant, times one over q, or this is like one over q prime, then you have the regularity transfer of this weighted g to the weighted gradient u. And this is why we use the w as a weight. And what you can see is you have a precise dependency on what is the smallness in terms. On what is the smallest in terms of the integrability exponent that you want to achieve. So, this is a kind of linear dependence on the smallness and the reciprocal indices. What we use here is a different condition. We use that the logarithm, which is a matrix-valued logarithm, has a small standard BMO oscillation. And I will say more about this later. So it differs a little bit. But first of all, the results actually are different. Actually, they are different. Because if you look at the result of Taidele, he fixes the weight and changes the exponent. This is like standard if you do Muckenhop theory, but you want to increase the exponent. What we do, we have the weight inside. So when we change the Q, the weight changes, if you in this notation changes also with the power of Q. So it is related, but there's also a subtle difference. There's also a subtle difference in the result. And maybe this is also why we get different conditions here, which are linear or something. I don't know if you get the same linear dependence also for this kind of equation, or if this is here important for this kind of result. I don't know. Okay. Let me reformulate the thing a little bit because this is how we started. We said, like, let's take a look at this from said like let's take a look at this from basically from function spaces so i repeated here the equation the degenerate ellipticity or written as bounded condition number so in the result before we looked at the gradient but with the weight multiplicative so omega times gradient u and you can think of this like um you can write it like a measure or you can just think like here it is a multiplier and this is my notation here for And this is my notation here for weighted spaces with a multiplier. And I learned this thing basically from David Krusuriebel. So, thank you, David, because this was the starting of the project. Because if you do this, then this is much easier with this notation than the dual of Lp omega is just L P prime one over omega. And this is a very simple formula to remember. And also, you have this multiplicative form of Mugenhaup weights. So, omega to the power of P is the Muckenhaup weight if the average and the The one over the average cancels out, and you just have to change it p and p prime. Yeah, this is easier to remember, but this is non-historical way to write it. That's the thing. That was a starting point, basically, to use this natural thing and to work with it. So, what we do now is let's define the logarithmic mean. So, we take the logarithm of the matrix, take the average, and then we take the exponential again. And I call this the logarithm. Exponential again, and I call this the logarithmic mean. So, one second. Okay, so to the logarithmic mean. And this is a nice thing. Why do we take this? First of all, later, we will use exactly this in the comparison estimate. This is the freezing coefficient. We don't freeze it at a point, we don't freeze it use the average, we use the logarithmic average, and which is better for the estimates. And which is better for the estimates, somehow. I have the feeling. And why is it good? It's first of all, we have this nice duality, but this means this is compatible with this duality. If I take the inverse of the matrix and take the average, it's the same like the inverse of the average. And this is, so this is nicely connected to this duality. So that's why we thought this is better to use this also for the freezing equation. What we do then, we use like kind of We do then we use like kind of John Nienberg type estimates telling you if something like the rhythm logarithm of this is small, then you can know that m is bounded. This is kind of this kind of John Nienberg type estimates, but the thing is that you have here this Q and you have here this Q. But this is standard, yes. And it's also if you have a small oscillation of the logarithm, then the weight is automatically Muckenhauf. Is automatically Muckenhaupt, so we are always in the setting of Muchenhaupt weights, and you have that the Muckenhaupt constant is bounded by a very small number, four. So this is always the setting here. So let's go back to my example and compare the result to this. So this was Mai's example. You have divergence. This was m squared and m is a perturbation of the identity with epsilon. Of the identity with epsilon. So, first of all, we see that this is nicely uniform elliptic. I mean, the condition number is bounded by two because it's just a perturbation of identity. Moreover, if you do careful the computation of the logarithm of this matrix, it's for me non-standard, but I mean in the end it's standard. You obtain that the logarithm of this matrix is this matrix, and you see that this is bounded by one, by the spectral norm. It's a projection. By the spectral norm, it's a projection, and this is basically roughly epsilon. So it's bounded by epsilon. So you have exactly, you know, that this oscillation is bounded exactly by epsilon. Now, we know from this, because the gradient of u behaves like x to the minus epsilon, we know that it was integrable if and only if 2 was smaller than 2 over epsilon, which means because this is. which means because this is epsilon here, this small epsilon, we know that this is true if and only if the logarithm of this is smaller than 2 over q. So what is the theorem condition our theorem? We say if this is smaller than some number divided by q, then it is in LQ. And here, I mean, so the result is optimal, except you can maybe still play with a constant here, 2 or something other. But otherwise, this is a very nice linear dependence here. nice linear dependence yeah okay this is for uniform this example is for uniform elliptic and ours works even for non-generate weights or for degenerate so it's sharp for even for non-degenerate okay so let us compare now um the the things that before we thought like we have a different concept of oscillation but now we have a second paper and look at this more carefully and so tadele here measures Now, Tadele here measures the oscillation like this. So the BMO, but the weight here. And we measure the oscillation of the logarithm, but standard BMO norm. And it turns out that this one is slightly smaller, usually, but once any of them is below a threshold, some small number, then they're just linearly equivalent. So, in principle, this one. This one is smaller sometimes, but after a threshold, they're just equivalent. So you can translate these results back and forth. Okay, this is so far. I want also to speak about invariance. If you look at this equation and you scale the matrix by T, the solution by 1 over T, and the right-hand side by 1 over T, you again get a new equation, the same solution and things like this, and you get also satisfy this condition. And you get also satisfy this condition. But now let's look at the smallness. This condition is scaling invariant because if you multiply t by here by t and here by t, it cancels. The logarithm is somehow non-linear, but it still is invariant because if you multiply by t, it will be just a constant plus an additive constant. And once you look in BMO, you don't see constants. So it's also scaling invariant. Both are scaling invariant. But the message here is But the message here is there are many old results where they use smallness of the coefficients in BMO norm or VMO, and this is not invariant. So like in the old papers, DeFazio or something. So that's not the right quantity. And basically, Tadele was the first that they really used a scaling invariant measure for the oscillation. Okay, I have here a long list now for the P-Laplace situation. So let's go to the weighted P-Laplace. Weighted P Laplacian, and I'm only mention mostly the result for maximal regularity here, okay? Um, because it's incredibly many people, so I think it started with that you can transfer the regularity from the data to gradient U for LP to LP. This is a weak solution. Then the next one was for Q bigger than P, and this started by Ivanie. And we had a discussion earlier about there was, but I couldn't find any result in this other. This other earlier paper. So, I think this is the first result for this Q bigger than P. Often, this is not seen. Most people ask always for Caffarelli-Piral, but Ivanic already had it, and it's actually similar techniques. It's a little bit more difficult to read, maybe. Then, Dominic already mentioned the BMO case was settled by Di Benedetto Manfredi for the P bigger than 2. Than two. And then the LQ setting for uniform elliptic coefficients, but almost continuous. This was done by Kinun Zhu and later also up to the boundary. But you see, most other results are just basically constant coefficients, just identity. I have some paper myself with Sebastian Bossusia and Peter Kaplitsky for BMO and C0 alpha. We have this pointwise estimate. Well, we have this pointwise estimate. This is with Tuomo. Maybe he was here yesterday, Andrea and Dominic, and with some pointwise estimates that gives you the estimate in Holder spaces and a lot of other spaces. There's this very nice paper by Andrea and Vladimir Mazia from 2080s, because they are the first ones that really have here this full W12 regularity up to the boundary and things like this. Boundary and things like this. And we also have a small contribution here that we could decrease the value of p. If you look at the vector-valued solution, we lower the bound from 3.5 to 1.19 or something. Then there are also results for the BSOF spaces, or trivializing spaces. This is together with Markus Weimar, and also you have seen in the talk with Dominic. So I do not have to speak about this anymore because we had a whole talk. Had a whole talk. He ends the list. There are many more results. They don't fit exactly in this context, but if the right-hand side is not giving divergence form but F, then there's a lot of paper research potential estimates. It started with Kuziminjono Duza Mali, who unfortunately died very, very recently. And a lot of lists, what is done here. You can also look at weights here, which are in Soberlif spaces, so not Muckenhaup class. This one, so not Muckenhaup class. There are many people that work on that one. And maybe let me mention this: there's also a full theory for Muckenhaup weights for the DiGiorgi for the Mozart iteration by here, David, Muhen, Naibo, Rodney and Moitika. So this is a very rich thing, and I will concentrate on the top part here. Okay, let's go back to the main result, but now for the non-linear setting. non-linear setting. We look here at the this is the P-Laplace with the weights. The right hand side is written such that it has the form that I can just look at g like it would be a gradient and the rest I cancel. We still have degenerate weights, so they can go to zero infinity, uniformly bounded condition number. And this weight is the maximal eigenvalue of m. So now if q is bigger than the Q is bigger than the P from the exponent, and the oscillation is small compared to one over the exponent, Q, then we have this transfer of regularity. I always write locally because these are local results interior. So there should be like a ball and here should be like twice a ball or something like this. Yeah. So what is different in the non-linear case than from the linear case, except that the proof is much more involved. The big difference. The big difference is this one. In the linear setting, we get all q because we do q bigger than 2 and then we use duality and then you get q smaller than 2. But in the nonlinear setting, you only get q bigger than p and there's no duality that is suitable to switch to the other exponent. Okay, so this is the result. And now maybe let me explain the proof a little bit. A little bit. So, at some point, after doing all these things, Kachopoli estimates and Gering and so on, the final estimate is something like this. You take this quantity that appears often for the P Laplace. If P equals 2, V is just the identity. Okay. And so you don't look at the gradient of U, but you look at the V of the gradient. Then there's some localization of U, but basically you can think about that this would be like U. So basically. So, basically. Okay. So, and now the estimates will follow. You look at the oscillations. Basically, it would be the oscillations of gradient u times the weight if we are in p equals 2. And you control it by this maximum function, but something which is small, you can forget about this, plus this measure of oscillation here. This is the estimate. This is not important because you can choose a small. Important because you can choose a small, it just goes as a large factor in front of the data, so you don't care about it. So, the important one is that you get here this smallness in terms of the coefficients. So, what is the next step? Now, you want to absorb this to the left-hand side. So, what you need, you have to need to control the maximal operator here, which is in a power of two, by the oscillation maximal operator. And you have to absorb this into the left-hand side. And for this, we need... And for this, we use this following estimate. If Q is larger than 2, not too close to 2, but it's about anything larger, then you can control the maximal operator by the sharp operator. This is called Pfeffermann-Stein inequality, but this is a qualitative version. So you have this power Q. And it's important to have this qualitative version. Many papers have this with like a 2 to the Q instead of Q. But if you look at this Hardy 1 BMO. This Hardy 1 BMO duality, you see easily that this is the correct dependence. So now we can absorb the right-hand side into the left if this constant we pick up from this inequality and the smallness here, if together they're still small, so we can absorb this whole term on the left side. So this is where our smallness condition comes in. And then this is the proof. We basically end up with a proof there. With the proof there. Okay, so what about the dependency now on the smallest parameter? Let me explain this a little bit because we are often asked, like, what is the difference between this and Caffarelli Peral? So in our approach, we have this that we use maximal operators. The sharp operator is bounded by the maximal operator. I wrote it a little bit simpler here. This is the case, linear case, no right-hand side. Case, no right-hand side, then this would be the correct estimate. Yes. But you see, now we use this estimate here, and we get the product of this epsilon and Q, and this has to be small. This is our condition on the exponent. If you use the standard technique, Cavarelli-Peral, or the same by Ivanić, it's the same technique. You can write it as a redistribution estimate, basically saying that the maximal operator of a smaller level set, associates, can be controlled. It's a larger level, can be controlled by that of a lower level with epsilon. And the epsilon is really the same thing like Heller continuity. So it's really the same epsilon, not this, not a different one, the same epsilon like here. But if you do this and you integrate over the level sets, you get this here. This k goes exponentially. The k is something like two to the dimension. And so you pick up here something exponential. So if you want this small enough to absorb. Want this small enough to absorb? You need that the epsilon is smaller than exponentially on q. So, whenever you use redistributional estimates, your constants are destroyed. And that's the trick how to avoid that. Working here with sharp maximum operators. Okay, I have prepared two final slides because recently we have added a result for the boundary regularity. So, let me first give you this. Regularity. So let me first give you the simple example. It's similar like Meyer's example, but even more simpler for the boundary regularity. So look at this domain. I rotated it, but it's a Lipschitz domain. And with a small, if you rotate it with a small Lipschitz constant. So I mean, it's almost flat. So it has Lipschitz constant epsilon. So what we are looking at, we look at the harmonic function, solar plus u equals zero. And everybody knows there is this is a re-entering corner. The solution looks like this. And you choose. Solution looks like this, and you choose alpha exactly such that the cosine is here maximal and goes down to zero. So you have zero boundary values. Okay. Now let's do the analysis. So first of all, if you look at the gradient of this one, it basically behaves like x to the alpha minus one. And the alpha behaves, if epsilon is small, basically behaves like one plus epsilon. So this is basically epsilon here. So x to the epsilon is irregularity. So, x to the epsilon is irregularity. So, if you want that this is in this space, maybe I forgot a sign here. Maybe it should be like minus epsilon here. I guess so. Let's say it is minus epsilon. Otherwise, it's always integrable. Yes. I will check. Then you get here this minus epsilon, and it's only integrable in this space LQ infinity. This requires this. Infinity, this requires this estimate in the end. So you also need that the Lipschitz constant from the boundary is bounded by one over the reciprocal exponent. This looks very much like the same condition for the weight. So what we, and this is exactly what we obtain. So this is a joint result with Anna. You see her still, Sunsik Bjün and Josek Lee. And just think about all the theorems that we had above. Now, Above now, if you have a Lipschitz boundary and locally the Lipschitz constant is bounded by something, a constant divided by the exponent, then you also have this maximal regularity in the linear and in the non-linear setting. So this is so far everything I have for the talk. So thanks already for your attention. I have one more slide. We have an upcoming, hopefully offline workshop in February. This is organized with Moris Kassman, Anna Balci, and myself. And you can just use this QR code or you write down the long number here, the address. And it will be a workshop on non-local equations. And we try to bring together the experts for analysis and for numerical analysis. And that we will speak together. So that we will speak together. Okay, thank you very much for your attention, and this is all for now. All right, thank you very much for your nice talk. Are there any questions or comments, further comments? I have a question. Yeah, thanks for the nice talk, Lars. Yeah, thanks for the nice talk, Lars. So, in the last result, up to the boundary result, can you also then treat the inhomogeneous boundary conditions? I mean, with according rights. Oh, no, no. So, I should have mentioned everything is U on the boundary is zero. I shortened the slide so much that the most important fact is is missing. Yes. No, but because for LQ estimates, maybe it's possible. Well, for Well, maybe for the linear setting something would be possible, but as you know that for P Laplace, even right inside zero, in homogeneous boundary data, I think the C1 alpha irregularity is open. And this means the regularity of your comparison system is not available. But you can subtract. You can subtract. I mean, there is actually, okay, I mean, this, there, I think, some results in this direction, but we. We were talking about it as the Kapritsky trick. Ah, okay, that's true. It would work. It would work. So, as long as you only look at gradient regularity and not more, then this could work with the Petra Kotplitsky track. That's right. Nice idea. Yeah, yeah. So with Miroslav Bulicek and Bjun and all, we have. Oh, we have and yourself. And myself, yeah, it was for the parabolic, but it should also work elliptic, of course. Okay. No, but I'm looking. So this is already uploaded or? This one is not uploaded. I mean, this is really work in progress. So that's why I just put it here what we get. Josekli is a PhD student. So he's finishing this basically. but it is in the I mean in the proofreading phase so it should be done this year maybe this month I'm not so sure but next month so there was a question by Tadel as well I think I just saw your hand yeah first of all my collaborators were Dad Cow and Took Fan that's fan that's the other is so if I remember I think we did it for Rifenberg flats we have It for Rifenberg flats. Ours was a global resulting. We did it for Rifenberg flats. And I see that you are doing it with small Lipschitz boundaries. What did go wrong if you try to do it for flat domains? Well, that's kind of tricky. I know that for the unweighted setting, there are papers which I did not mention, because also by Sunsik Bjun and yourself, I think also, but with Reifenberg flat. We tried... We tried, I mean, we were, we said, like, we want a theory where in the end we have this, this optimal dependency. And this was impossible to save if we went to Reifenberg flat. If you just say you want exponential dependence, then you can pass, I think, maybe to Reifenberg flat. But since the measurement of isolations in the other result was so nice, we thought like we want to do. We thought, like, we want to do everything to keep this nice dependence here to have it optimal. That was the motivation. Well, thank you. That explains. Okay. I don't know if it's true for the Reifenback flats. Thanks. Oh, sorry. Any other questions, comments? Okay, then many thanks to our speaker and all the people who asked questions and remarks. So our next So, our next talk starts in about seven minutes. Thank you. So, thank you.  Igor, can you hear me? Yeah, so maybe you could keep your camera on, or someone can. I'm having a little connection issues. I'm in my office because it's supposed to be good Wi-Fi, but having a little bit of connection problems. So if somebody freezes, then I know it might be me. All right. Then could you share your screen just to see how it goes? Yeah. See how it goes, yeah. Sorry, Igor, um, do you mind stopping your recording? Yes, yeah, thank you, yeah, thank you for reminding me. Yeah, you're welcome.