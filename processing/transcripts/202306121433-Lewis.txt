This whole thing together. This is a great location and a good bunch of people, so I appreciate being here. So as you can see, my talk title. So let me start out sort of in a very general kind of setup, just talking about composition and superposition. So suppose we have some spaces, X and Y. These could be topological spaces, measure spaces. Topological spaces, measure spaces, and on those spaces I have some collections of functions, f and g, say continuous functions or integrable functions or if x and y are manifolds, functions with some kind of differentiable sort of regularity. And add to this, between my two spaces x and y, I have a family of mappings, which I'm going to call a hemma. And so, for example, in the topological category, this is going to be continuous mappings for manifolds, mapping with some sort of regularity, and that's mostly what we're going to be interested in. So, associated with all of this kind of data are some operators. So, the first operator is composition operator. And this is just pullback. So, I fix a mapping, phi, okay. phi, okay, and then corresponding to phi, I just have the pullback of functions from on y to functions on x just by composition. Okay, so that's the composition operator. There's also the superposition operator. Okay, so the superposition operator fixes a function g on the codomain space, y, and then what my operator And then, my operator is a function of mappings. So it takes a mapping to a function non-x by, again, composition. So I have sort of the same thing on the right there, but the domain is different. One is functions and the other is mapping. So this thing is sometimes also known as the Namitsky operator or sometimes nonlinear composition operator. And then you can just consider the joint. And then you can just consider the joint thing where you consider the operator on the product space. All right, so what are some questions that you can talk about with composition and superposition? Well, if you choose a class of functions fx and gy, and a class of mappings m sub x y, is it even going to be the case that these mappings are well defined? Do they map from Are well defined. Do they map from the space that you want into the space that you want? So that question is typically easy to answer. Continuity. So there's cases where the function spaces or the mapping spaces have topologies and you want to know if those composition operators, whether it's composition or superposition, whether those mappings are continuous. And sometimes they are and sometimes they're not. And there's also And there's also, people also talk about boundedness. I'm not going to talk very much about it today, but that's another kind of thing you can talk about with these kinds of composition operators. And the important thing to notice here is that the composition operator is linear, but the superposition operator is non-linear. And the best way to convince yourself of that is just to notice that the superposition operator is incapable of being Is incapable of being linear for the most part because it doesn't have the right kind of domain. All right, so that's composition and superposition. Now, I'm going to talk about this in the context of differential equations. So, how are we going to do that? All right, so I'm now going to just change directions completely and talk about something different, and then I'll connect them together. Okay? So, let me just first. Um so let me just first talk about something that's kind of classical at this point in time. Um uh series representations of solutions to uh uh ODPs. Okay, so let's just start in the simple time independent case. So I just have a manifold and a vector field on the manifold. Okay, and let's just think for the purposes of this discussion right now, everything is either smooth or omega means real analytic. Omega means real ventilator. Now, in order to be a little bit precise about what I'm doing, I'm going to have some spaces that I'm going to work in, and the space that I'm going to work in is the space of continuous linear mappings between the spaces of functions on M. And a vector field is a member of that space of continuous linear mappings by Lee different. By Lie differentiation. Okay? Alright. So, Volterra series. So, the Volterra series is a rather simple thing. If you think about the flow of a vector field as being, you know, kind of like the exponential map, then this is just the exponential series. And this adjacency of these x's here means concatenation of the Lie differentiation operation. Of the Lie differentiation operation. So each of these x's involves a differentiation. And so this is in sort of series format with t's and factorials. But you can also just notice that this is the centroid of the k simplex. And so you can convert this into an integral like I did on the right-hand side of the collection. And that integral form, it's not necessary here, but it will be useful in the next slide. In the next slide. Okay, so what's true about that series? Well, that series converges in the real analytic case, and moreover, it converges to the flow in the following sense. If I want to know the value of the flow at a function f, then I just take this Volterra series and a few bits of ones. Alright, so again, this is classical. And let me just make it a little bit more interesting here, and let's talk not just about. More interesting here, and let's talk not just about time-independent vector fields, but time-dependent vector fields. And so, now my vector field depends on time, and so the time that I'm going to talk about here is in R. And so I will think of a time-dependent vector field in this framework, which is something I'm going to talk a tiny bit about later, but not a lot, as being a locally integrable mapping with values in the space of vector fields of class CR. Okay, so yeah. So you have to understand what that integral is, and that integral is the Bochner integral, which is sort of the natural adaptation of the Lebesgue integral. All right, and so the idea here is that if you take a fixed time t, then just like on the previous slide, you get continuous linear mapping between spaces of functions by diffusion. And then I kind of just write down the same formulas that I had on the previous slide. The only difference here. The only difference here is that the vector fields are no longer constant, so they depend on time, but the integrals are still the same. You're still integrating over the synplace. This just no longer has the simple t to the k over k factorial time representation that we saw in the time independent days. But nonetheless, that thing is perfectly well defined. And it still converges. Okay? So that's in the time of In the time-dependent case. So, this is what's known as the Volterra series, as I think I mentioned. And so let's think about some of the drawbacks to this approach of series representations for flows of vector fields. So, I told you that these things converge in the real analytic case. Things converge in the real analytic case, they don't converge in the smooth case. And moreover, for the whole development to make sense, everything has to be at least C infinity. And so really what this means is that this is an analytic theory. And so, for example, the standard theory of differential equations, which involves Lipschitz hypotheses on the right-hand side, that isn't valid for this kind of Volterra series representation. Kind of Volterra series representation. So, one way to think about what's going on here is that this series is an infinite series, so let's think about where its partial sums lie. So the first partial sum lies in, it's just a vector field, and so the first partial sum lies in the space of vector fields. The limit lies in the space of mappings. And all the intermediate partial sums, well, so those are the finite k things that I have here. And those things are just some linear partial differential operator. And then all of that whole business sits inside the space of continuous linear mappings on these two function spaces. And so what you're doing with this Volterra series is you're starting with something in here, you're taking it as In here, you're taking a series whose partial sums are in here, and then you're converging to something in here. And so, one way to think about what it is that I'm trying to do is: I'm saying, can we start in here and stay in here? And at the same time, maybe get rid of some of the difficulties here about convergence and regularity. All right? Okay. Okay. So here's a photocopy of the hypotheses for existence, uniqueness, and continuous dependence from Hastine's book on optimal control. And I'm not going to carefully read through this line by line. The point is there's a series of hypotheses. But, you know, so you have this long list of hypotheses, and they're kind of technical and complicated. And at the end of the day, you don't get very much, you get existence. Get very much, you get existence, uniqueness, good, but you get continuous dependence. And what I'm really interested in here is not continuous dependence of solutions on state, but more regular dependence of solutions on state. So we're going to talk about a class of time and parameter dependent vector fields. And so this is kind of the meat of the talk in some sense, is formulating what this family of vector fields looks like. So what are the Of vector fields looks like. So, what are the conditions on there? And in the Lipschitz case, it more or less reduces to that. But it also holds in all kinds of regularity classes. And so let's start with some mathematics. So we're going to do some mathematics for a few minutes here. And I want to present things not just in terms of vector fields, because we're also going to want to talk about functions. And so it's just as easy to work. And so it's just as easy to work with a vector bundle. So I'm going to take a vector bundle's regularity R and I'm going to allow R to be smooth, real analytic, or even complex analytic, holomorphic. So that's what my state variables, that's the regularity of my state variables is CR. And then I have this other variable, new. Nu measures the regularity of my dynamics. Of my dynamics, of the vector field x. And so ν I'm going to allow to be a positive integer or a positive integer plus lip. And so what that means is that, so m plus lip, so if I choose something in here, that means that it's a class CM and the top derivative is local. So that's what that class means. And then I have smooth, real analytic, and a whole function. Okay, so we want to present a frame. Okay, so we want to present a framework which allows all of those possibilities. Okay, so the first thing you need to do is you need to take the space of sections of this vector bundle and apologize them. I'm not going to be talking about that. I will just say this. That in the finitely differentiable, smooth, and holomorphic cases, those topologies are classical. In the Lipschitz case, it's not classical. It's kind of out there, sort of somewhere. Not really in the Lipschitz case, you have to sort of work it out in the locally Lipschitz case, but you can do that. And in the real analytic case, you also have to work that out. But that's been done. Good. So, time dependence. So, I want to now talk about time dependence, and I've kind of already said this. So, there's this symbol that I'm going to use here. Gamma means sections, new. Gamma means sections, nu means regularity, li means locally integrable. So these are locally integrable vector, you know, vector field valued functions of time. Okay? All right, so that's my first sort of major space. And so that's how you encode time dependence. Parameter dependence, you encode in the following way. And so I have some notation here. So gamma again means sections, mu is irregularity. Again, it means sections, mu is the regularity. PLI means parametrized, locally iterable. So it means that I have some parameter dependence there. And the parameter space here is a general topological space P. And then the space of how I characterize these sections is I take continuous functions from my topological space P into this space of time-varying vector fields or sections. Okay? All right. So there's a little fact here which says that you can verify that a vector field has one of these regularities, either LI or PLI, if and only if the lead derivative of every CR function also has that property. Okay, so that's a helpful thing. Oops. Oh, sorry. On my slides. On my slides, but it's my button pushing. Okay, so that's locally integrable time dependence. Now we want to talk about locally absolutely continuous time dependence, because that's the property the flows have. So first we're going to talk about sections of vector bundles and what it means for them to be locally absolutely continuous. And we're going to take the definition of local absolute continuity, which is that locally absolutely continuous means the indefinite integral of something integrable. Okay, so I'm going to take something. Okay, so I'm going to take something that's integrable, that's what this means, and I'm going to integrate it with a variable upper limit. And that's, by definition, going to be what I call locally, absolutely continuous. So again, gamma means sections, nu means regularity, LAC means locally, absolutely. So that's sections. All right. Now parametrized sections. So this is again the same idea. So it's again the same idea. Parameterized, locally, absolutely continuous. I'm going to encode parameter dependence. And I do it in exactly the same way as I did in the interval case. I consider continuous mappings from my parameter space into the space of locally absolutely continuous things. Alright, so we've talked about time dependent, locally integrable, locally absolutely continuous, time and parameter dependent, locally integrable, locally absolutely continuous. Locally absolutely continuous, sectional, so vector fields and functions is what should be written there. We also want to talk about mappings. And so the way you do this with mappings is you say that the property holds for a mapping. So a mapping that's time-dependent from M to N. So M and N are manifolds of appropriate regularity. And you ascribe You ascribe the attribute to the mapping by ascribing it to this composition. Alright, so this is again absolutely continuous dependence, and then you have parametrized, absolutely continuous dependence, and the story is always what you do in this world that I'm working in. When things are parametrized, you talk about continuous mappings from the topological space into the set of time-varying things. Now, this is a very abstract setup, so let me just say one thing: is that if you choose new equals lip, then this basically recovers that long list of hypotheses of the classical hypotheses of existence unique and for ordinary differential equations. But it's more than that. Okay, so let's think about the Lipschitz case for a second. Because the setup is quite abstract, it's worth trying to It's worth trying to see if these spaces have any properties whatsoever. Okay, so here's some properties that they have. And these will seem like differential equationsy properties, and they're meant to. So the first property is that if you're in LIP, then your mappings depend continuously, jointly, on time, state, and parameter. That's again, that's a property that differential equations will have. If you fix, We'll have. If you fix parameter values and state values to p and x, then time is varying, so you have a curve. Those curves are locally absolutely continuous curves. Again, that's a solution of solutions of differential equations. You have a continuous dependence type result, which reads like this, that if I fix a parameter and a state value, p naught and x0, and then allow x and p to converge to x. X and P to converge to X naught and P0, then the corresponding curves converge uniformly on compact subintervals. Again, that's a differential equations property that holds for this kind of abstract set of mappings. This one's more technical, but basically what it says is that if I don't want to change my solutions very much, then I can find a neighborhood of my topological space and parameter space. Space and parameter space so that I don't change my solutions very much. That's what that looks like. Okay? Alright, so this class of mappings has properties, which isn't clear from the definitions. Okay, so what the idea that you'd like to arrive at here is that if you start with a vector field, which is PLI, so it's parameter-dependent and time-dependent, you'd like to end up with a flow or solutions that are parameter. Solutions that are parameter-dependent and locally absolutely continuous. So, these classes of mappings and sections that I've talked about, you should think of these as being vector field properties and flow properties. So, first of all, there's a problem that immediately you run into about locality of solutions, and so you're not going to be able to take an arbitrary vector field and assign to it an arbitrary mapping because flows of vector fields are only locally defined. Are only locally defined. And that whole local thing is a problem all on its own, but I'm not going to think about that today. So I'm going to just pretend that all flows are globally defined, which is just a patently false assumption. But just for simplicity, and you can add hypotheses to fix that up, but I just don't want to fuss with that. So here's our process. So we're going to do something that's similar to what we did at the At the beginning of the talk with these Volterra series, except this is really a lot closer actually to the classical Picard iteration theory for differential equations. You do an iterative scheme. So the first iteration, you choose this mapping. So zero here means the zero step in the iteration. You choose that to be the identity map. And then after having defined the first k of these things, like so, they things, like so, then you define the k plus first thing by this formula. Why is this the right formula? This is the right formula because if you forget about the index here, the k plus 1 and the k, something which satisfies this equation satisfies the differential equation. So if you differentiate with respect to t, this is the differential equation in integral form. And so you're doing the usual business with fixed point theory, except we don't have a contraction mapping theorem because we're not working with odd spaces. All right, so the key thing to understanding this iteration is that. And that, so first this thing has two components. It has the first component, which we've already seen. If X is T L X is PLI, then XG is also PLI. That's one of the properties I said, that these PLI things are. The hard part is that you have this non-linear superposition, which takes the flow, which is the thing you're interested in. Well, it's not the flow, this is an approximation to the flow, the kth approximation, and it maps it to this superposition of this thing composed with that thing. Okay, so this is where the nonlinear. Thing. Okay, so this is where the nonlinear superposition comes in. And so, what you want to show is you want to show that if you start with, so phi naught, the identity mapping, is certainly in here. You'd like to show that phi1, phi2, phi3 are also all in here. So you have to show that this process of recursion here leaves this space invariant. Okay, so that is work. All right, so let's take a look at some. Alright, so let's take a look at some results. Time-dependent case, let's forget about parameterization. So, in the time-dependent case, things are simple and nice. You choose your data to be of regularity type new, and you'll notice here that Lipschitz regularity should be included here. Should be included here, okay, in the time-dependent case. So, I meant to include Lipschitz regularity here, but I just copied and pasted wrong. Okay, and so if you take a mapping, which is a class regularity class nu, and is locally absolutely continuous, and a function, which is regularity nu and locally integrable, the composition is locally integrable. There's an analog of this theorem that sort of is step one in the proof of the existence unique. And the proof of the existence of the uniqueness theorem with measurable time dependence. So, this is a generalization of that. It doesn't follow from that. It's a generalization of that. So, in the time-dependent case, things are simple. Now, in the parameter-dependent case, things are not quite so simple. So, this is the thing I copied and pasted onto the previous theorem, which is wrong. So, here, I do not have Lipschitz regularity. So, I'm omitting Lipschitz regularity from the From the property of solutions of differential equations. But I'm going to have to bookkeep it somewhere along the line anyway, and I'll try to explain why it is that we need to do this. So given a new, then I define this thing nu prime, and in all cases, except finite differentiability, nu prime is equal to nu. But for finite differentiability, nu prime is m plus lips. So you have to add Lipschitz regularity. So you have to add Lipschitz regularity to the top derivative to get this regularity class of new product. And we'll see why that is. Okay, then the theorem says if phi is regularity nu, PLAC, and if g is regularity nu prime, okay, so it has more regularity than phi and PLI, then this whole thing, which is that integral in that iterative equation that I wrote down. That iterative equation that I wrote down is regularity nu and its PLAC. Okay, so that's not an entirely straightforward theorem. Part of that theorem is that you have to consider the composition operator, and you have to consider the joint composition operator. Both of these things are sort of involved here. This X is kind of involved there, and then the flow is involved here. The flow is involved here. And so you have to consider the joint composition operator, and part of what you need to do is you need to prove that this thing is continuous. So this is a non-linear mapping defined on, you know, these things are infinite dimensional locally convex spaces. This is some weird space with a topology, which is an initial topology according to some locally convex topology. So it's kind of a complicated. Overly convex topology, so it's kind of a complicated thing to get your handle on, your hands on. However, it's classical in some cases for people who study differential topology. So for finite differentiability, smooth regularity, and holomorphic regularity, this is well known. So for example, it's an exercise in Hershey's book on differential topology. It's true when ν is equal to omega. So in the real analytic case, this is true, but you have to prove that. Is true, but you have to prove that. And that's not easy. Now, the interesting thing is that it's false in the Lipschitz case. So it's not true that this joint composition mapping is continuous if nu is Lipschitz. And in fact, it's the superposition operator that causes the problems there. So the superposition operator is not continuous in the Lipschitz case. So the correct result is that the superposition operator. Is that the superposition operator f sub g is continuous in the Lipschitz case if g is C1. So not just that G is Lipschitz, but it's C1. So that's why I have this whole business about new prime and adding Lipschitz and all of that. All right. But nonetheless, this theorem does tell you that this iterative procedure that I find, I've defined, Procedure that I find, I've defined under appropriate hypotheses, does leave this space of mappings in red. Sorry, Andrew. Is it obvious to see why it meets QI? No. No, the first proof I know of this is in that paper cited by Greybeck, and it's just a long, complicated, messy calculation. Yeah, I don't have any insights. I can say some things, but it won't necessarily convince you. So it's not obvious. Won't necessarily convince you guys, so it's not obvious. I should say this, though. Counterexamples are obvious. That's not continuous in the Lipschitz case, but that the right condition is C1. Okay. All right, so let me just jump to the punchline here. The punchline here is the turbo type like the tier one slide between this one? This one? Yeah. So, are there examples that illustrate the. I don't have examples. I'm sure they exist. Yeah, it's like that. I just haven't bothered. It's a nicer one for you. The punchline is this, is that if you want Cm regularity with this parameter dependence, then the vector field has to be Cm plus lip. Okay, so that's a little bit surprising. It was to me. So that's a little bit surprising. It was to me at least. I spent, you know, two years trying to improve it for m plus lip, and it's not true. I'm nearly done. And I should just say that our iterative method does not require infinite differentiability. That's the whole point of allowing new to be very general. And it does have this property that the iteration remains in that green blob that I showed at the beginning of the talk. So you have to show convergence. So, you have to show convergence of that iterative process. We've done that in some cases, and I'm not, they're kind of a little ecluggy, but so that's still kind of work in progress. And I'd like to acknowledge the contributions of this work in various ways to my former student, Sabar, who's there, and my also former student, but more recently former, college. Okay, thank you. One more I have one more social announcement to make, and some of you reminded me that I need to do this. The Journal of Geometric Mechanics, of which I was formerly a managing editor, has ceased to exist. And it's being reconstituted as the journal called Geometric Mechanics, which will be coming out in March. So, if you want to submit your geometric mechanics from non-linear control papers, that journal will be back with the same editorial board, but with Board, but with less predatory editorial boxes. Any additional question for Andrew, or perhaps we can ask him over tops. Yeah, I'm happy to defer. Alright, so we'll be convening here in about half an hour then.