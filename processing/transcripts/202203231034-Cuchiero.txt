Uh, that's how I call it, and this can, for instance, correspond to the initial distribution of asset prices, but it could be also a Wiener measure on path space. And then you have some target measure, some output measure nu O, which in the simplest case corresponds again to a marginal distribution at some future time tau that is deduced from option price data usually, or it could, if we talk about If we talk about path-based distribution, it could also correspond to the true market's law on path space. So, this is then, of course, not necessarily fully specified by the option price data that we have. And all these generative models nowadays can be viewed in some sense as a transport map T theta, so a map parametrized by parameters thetas. Parameter status which pushes this initial law μ i to this output law ν o and this push forward is denoted via this usual notation. And this setup includes of course all classical models but also these new generative models which are often based on some neural network architectures. And here to mention only some literature. Mention only some literature, it's certainly not complete. So, this artificial market generation started, I think, with a paper by Kontratif and Schwarz, and then it was taken up by many researchers, Magnus Wiese, Hans Bühler, Beatrice, Gariatowicz, Lukas Brukko, and I'm certainly missing a lot of people. So, yeah, this is certainly not a complete list. So, what is So, what is the goal then? I mean, we parametrized such generative models via this map D, which depends on the parameters theta. And the goal is to find some optimal theta such that this push forward of the initial distribution matches well the target distribution. And for this, one has to define appropriate distance functions and what is the most classical. What is the most classical one? It is just the classical calibration functional. And what does it mean? It means to compute somehow model prices and compare them with the market prices. And the market prices are nothing else than the expected value of this target measure. So the target measures are indirectly specified by the market prices. And the model prices are the push forward of this initial measure under. Of this initial measure under this transport map T theta, the expected value of that for this payoff function g. And one basic criterion is to minimize the square distance between these model prices and market prices and add some certain weights. So this is just another way to write it. And then you can consider more sophisticated versions of a calibration functional version. Versions of a calibration functional where you add an adversarial part, where you parametrize, for instance, these weights with a certain gamma, with a certain parameter gamma, and then the adversary tries to find always the worst weights in order to make this distance worse. And then you play this game between the generator and the adversary. Another example is you can also consider distances of Consider distances of Waserstein one type or radon type. So, what you do then is that you consider such a functional where you have again this push forward of your initial measure and your output measure, your target measure. And here you have functions that are parametric again and And should give you appropriate classes of functions. So, in the case of Baselstein one distance, it would be approximations of Lipschitz functions, for instance, by certain neural nets, but it could be again payoffs of options. Okay, besides the distance that you have to choose, the most crucial choice is also the parametrization of this transport map. And what has turned out to be What has turned out to be a quite good parametrization is to use actually classical stochastic differential equations and parametrize the drift and the volatility by neural networks with parameters theta. So this is written here. So you have a classical stochastic differential equation of etotype driven by some Brownian motion, and your drift and your volatility are now functions of the state and time. State and time, but also are functions of the parameters. So, these are a special case of a stochastic differential equation. But it is somewhat different than the classical ones where you just had a few parameters. Now you have a lot and this gives you this over-parametrized setup. And here, the initial value is distributed with this initial measure, and then the measure and then the solution of this solution measure on par space or if you consider only the marginal one this is then this is then given by this this push forward here and now you have to choose the theta such that it meets the this push forward meets the the target distribution mu O okay what are the advantages it's highly flexible and can also Flexible and can also account for several sources of data. So, if you have time series data and option price data, and it is, as I already said, in the classical setup of stochastic differential equations where you can apply all the well-known theory for mathematical finance, in particular in view of arbitrage hatching and so on. The disadvantages in contrast to classical models is that the parameters are no longer interpretable and that one comes into the problem. Into the problems of robustness, in particular in view of exotic option prices. So you can maybe match your vanillas with several parameter configurations, but this then gives you a range of exotic option prices. So then one is in this setup of robust finance. Okay, so now this new These neural stochastic differential equations work very well in equity markets. This can, for instance, be seen in this paper by Gerjatowitz and co-authors, where they in particular consider this robustness issue. And here our focus lies not on this classical equity market, but rather on energy markets, and they have quite different features. And in particular, they require an infinite-dimensional setup. So, to give you a brief overview on that, so the essential products in these markets, in particular electricity and gas markets, are futures, future contracts and options written on them. And these futures deliver electricity or gas over a certain time interval. So, this I denote by tau one up to tau t. And the price of such a delivery contract at time t. contract at time t is denoted by f of t and then this tau one tau two to denote this delivery period and the t is prior to to the start of the delivery period um now how can one write those those uh these future contracts one can write them in terms of instantaneous forward prices so this is when you have just the delivery at the instant The delivery at the instant u here. So, this is this small f here, and you can write this delivery over the whole period with this weighted integral where this w is some weight function, usually a simple one. And you have this instantaneous forward route. So, this comes from the book of Bent on energy markets. Markets, but I think this is a common convention. And now, what you have on this market as well are options written on these futures. So the underlying are these futures with this delivery period and you have them for different maturities and different delivery periods. So, for one fixed maturity, you can have a whole bunch of strikes and also this different. And also, these different delivery periods. And this potentially, in practice, of course, not, but this gives an infinite-dimensional target law for each T because you have all these different delivery periods. Okay, so now let me give you some economic motivation why we consider measure value processes to model these kinds of contracts. So the usual common well-established approach. The usual common well-established approach is to use stochastic partial differential equations to model this instantaneous forward rate. So, the ones that deliver really at a certain point in time. And then you have an SPDE for this instantaneous forward price up to a certain time horizon, capital T. But for obvious reasons, there is no trade of this instantaneous forwards because you can Instantaneous forwards because you cannot deliver electricity or gas for just a second. And therefore, what we thought is that we could replace this instantaneous forwards by a measure supported on this remaining interval from T to capital T. So the idea is we use a measure when We use a measure-valued process setting instead of this function-valued SPDEs. And to do so, we pass to the so-called Musella parametrization and consider instead of maturity, time to maturity. And then this measure-value process denoted by this mu here is supported on the compact interval from zero to this time horizon capital T. And the meaning is, so if it's Lebesgue. Meaning is, so if its Lebesgue density existed, then this mu t of dx would correspond to this instantaneous forward at time t with time to maturity x. So this is the meaning of that, but it's not necessary that the Lebesgue density exists, but still we can make sense out of that. And in particular, we can write these future prices that I had before just by making this change of. By making this change of variable via this expression now. So, this makes perfectly sense to write now this future prices with delivery from tau 1 to tau t with this expression where this mu comes in now and the weight function from before. Okay, so now some mathematical motivation why measure valued processes. In fact, they are often used for modeling dynamical systems when there is a certain Dynamical systems when there's a certain spatial structure. In our case, the spatial structure is time to maturity. And one important point is that many spatial stochastic processes are not necessarily function-valued and do not fall into the framework of STPDEs. In particular, it's often easier to establish the existence of a measure-valued process than an analogous SPDE in some Hilbert space, for instance, which would, for instance, correct. For instance, which would, for instance, correspond to its density, but which does not necessarily exist. So, for this reason, it's easier. And in particular, there is really a machinery how you can establish existence of measurable processes in a Markovian setup. In fact, these non-negative measures on a compact space, so in our case, the interval from zero to t, if you equip them with the weak topology, so functionally analytic. Topology, so functionally analytic the weak star topology, then, and we denote this. This is for further use by this m plus of e, then this is a locally compact and separable space. And for this, we have this martingale problems, existence results from Etier Kurtz, where you can establish the existence of the solution to the martingale problem via the positive maximum principle. So, this is really. So this is really useful for doing that. And yeah, in other function spaces, you do not get, not so easy get this local compactness and so on. So let me mention that it is important that this replies only to non-negative measures. You cannot do it on the whole space because then you certainly lose local compactness. And now the idea is to re-pro. The idea is to replace these neural SPDEs or STEs now with neural measure-value processes. And yeah, I will show you what I mean with that in a second. Now, coming back to the mathematically finance setup, so our goal is to establish a historical modern approach as it exists already in the SPDE setup, but now in this measure value setup. And what we have one under And what we have one underlying measure-value process, this mu t, which is supported on this interval, to model the basic assets on the market. So, these are the futures. So, very precisely what we are considering are a potential continuum of future contracts given by those prices here. So, these are the traded energy products, as already said before, for a potential continuum of delivery periods. Of delivery periods. And we have a numera asset which we set constant to one and we work in discounted terms. Now, the history of modern setup is used in order to get a framework where no arbitrage applies. And here we deal with a large financial market, so we have to use an appropriate no-arbitrage condition. And what we do is using no artitrabit. And what we do is using no asymptotic full lunch with Maniche, which is an adaptation of the notion of Walter and Fredi to the large financial markets set up. In particular, under this condition, one gets an equivalent separating measure, which in locally bounded cases turns out to be an equivalent local martingale measure. And we deal here with continuous processes, so all these measures. Processes. So, all these measure-value processes will have continuous trajectories. And therefore, we have this assumption: namely, that we have an equivalent measure such that all those elements, these future prices, are local martingals. And the question is, how does this now translate to our measure-valued process setup? And this is answered here in this theorem. So, if we have our measure-valued process and we model our Process and we model our future prices by these weighted integrals here, then this market satisfies this no-asymptotic free lunch condition if we have an equivalent measure Q such that the expected value under this Q of the total mass and the supremum over T is finite, and this is the essential condition. And this is the essential condition if for all phi in this set of test functions, and here one we have to choose a particular one. So we use test functions that are C infinity, restricted to our interval, and the first derivative of at zero has to vanish. This is not actually used for this theorem here, but we need it later on, so therefore I state it already like that. And for those test functions, we need to have. And for those test functions, we need to have this what I call historical Morton condition. So if we pair the phi with the mu, so the pairing is just the integral of phi against mu dx, and we add the derivative of this phi, again, integrated against this mu and along time, this has to be a Q mounting gate. So, in fact, if we have this condition, then this assures that all those future contracts. That all those future contracts are local mounting girls. So, to give you some intuition how this history modern condition comes up, if you are familiar with the usual one for in the SPDE setup, so consider now replace the mu here by a function because this thing doesn't make sense otherwise. Then you have usually this d over the x condition. D over the x condition plus a martingale. So a function value of martingale, in our case, it would be a measure value of martingale. And so this is because the forwards have to be martingales. If you pass through the Musela parametrization, you get this D over DX, this shift term, additionally. And therefore, this is the dynamic that has to be satisfied. So D over DX of the involved. Of the involved function, so the derivative of the involved function plus a martingille term. And this first term now, in a weak formulation, translates exactly to this part here. So this is what is the intuition to get this historical Morton condition in the measure-volute case. So, yeah, I wrote that here again. So, this is what we now get. Get and the question is now: okay, we know how this looks like. Can we establish the existence of measure-value processes that satisfy these conditions? So, in particular, we want something non-negative, a non-negative measure-valid process and satisfying this history-mortem condition. And for this, we pass to a Makovian setting where we We have an extended generator that acts on certain cylindrical functions that have certain regularity properties, and the cylindrical function that we consider here, so this phi is C infinity and vanishes at infinity on our M. And what we plug in into the slots are then these pairs with nu. So the G1 is some element in the set D. And in the set D, so to recall the set D, this was the functions phi and C infinity that are restricted to the interval and that are zero at zero, their derivative is zero at zero. So we plug those in into the slots of our function, and this gives us the cylindrical function that we consider. And on this cylindrical function, the potential generator of our Markov process will act. And for this And for this setup, we also need a notion of derivatives. And here we work with the flat derivative, which is just a directional derivative in the direction of delta x. And this is denoted by this. And if we consider the whole map, so the map from x to this derivative, then we denote it by this expression here. So I will not go into the detail. I will not go into the details of these measure valid derivatives because I think most of you are familiar with that. And now we can, completely analogous to the finite dimensional case, consider diffusion type operators, where we have an operator that acts now on a function with a measure as an argument. And here we have an operator into which we insert the derivative. We insert the derivative of this function depending on measures, and here the second derivative. And we have another slot that depends here on the measure itself. And yeah, these are these operators that act on D. So that derivative of this f is gives actually values in D. D was the set that we considered here. So this function g1 of gn. So, this function g1 of gm that enter in the cylindrical functions here. And here we have this d times that the tensor. This is always the symmetric tensor where we have get the second derivative. And then we need to make sense out of a solution of a notion of a process actually, and we consider the martingale problem formulation. So, an m plus e-value process with E-valued process with continuous trajectories is called the solution to the martingale problem for this operator. If for all f's in the domain of this operator, this expression here, so the usual one with this potential generator, is a local martingale. So this is this notion. And then we have an application of this result of ideques that I already told you before. If we have a diffusion type operator, We have a diffusion type operator with certain conditions on this Q such that this drift part is of this d over the x form applied to the derivative of f. So this is exactly what we had in the HTM condition before. This is what we need. So this part has to satisfy that. And if the q, so the diffusion part satisfies the positive maximum principle. So this means if I take an So, this means if I take an f from this domain here, so this is more or less the cylindrical functions, I take a maximizer of this x and this is greater than zero. Then, if I apply the q to this here, where I plug in the maximizer from that, then this has to be negative. So, this is the positive maximum principle well known that this then guarantees the existence. Then guarantees the existence of a solution to the martingale problem. And in this case, because we suppose that the B is of that form, we also get that the HTM condition is satisfied. And the important thing here is that what we have to guarantee is that this here satisfies the positive maximum principle as well. And this gives actually then the choice of this D, where we had to assume that the first derivative at zero is zero. Derivative at zero is zero. Okay, so this is now, we have such processes now towards tractable examples. So what we can do is we could now start to use the setup of neural SPDEs or STEs. And what would it mean in our case? We could use this covariance structure, this Q here, and to pair. Q here and to parametrize this here as a neural network where the input would be a measure. So it would be a neural network with an infinite dimensional input. And here I mentioned several recent papers where this is considered a paper by Bend, also applied to this history-mortem things with SPDEs. Then the paper by Beatrice, Grazios, and Gudmund, and we together with Philip Schmok. And we, together with Philip Schmokke and Josef, considered also a little bit such a setup. But here, what is easier, we can restrict to certain tractable classes, and then we can use standard neural networks and do not have to pass to this infinite dimensional setup. So in fact, if we restrict these measure-valued processes to lie in the class of polynomial measure-valued diffusions, which I like a lot, then things simplify. Then things simplify. So, to recall what is a polynomial process, this is if this operator L maps cylindrical polynomials to polynomials of same or lower degree. And polynomials, you replacing your cylindrical function, the C infinity function by a polynomial. This is what are the cylindrical functions and the cylindrical polynomials. And then if you are now in this polynomial, If you are in the if you are now in this polynomial class, you can very precisely say how your operator looks like. So you have this part, this corresponds to the drift part of the HTM condition, and then you have this diffusion part, and this has to be quadratic in nu, so in the measure. And now one can write down very precisely what how this has to look like in order to satisfy the positive maximum. Satisfy the positive maximum principle, in fact. So, this q1, the linear part, has to be of that structure. So, you have a function alpha of x. This is now really a function with an argument in E and not in the measures. And this operator looks like this. Then, so if you're familiar with fine processes, this is just the analog of the finite dimensional case. And the Q2 is a bit more. And the Q2 is a bit more complicated. There you have two functions, the pi and the beta, involved to get this structure. And under these conditions, you get an M plus E valid solution that satisfies this HJM condition. And now, instead of having a neural network for this function here as a function of nu, we can just parameterize this alpha, beta, and pi as neural network. And pi as neural networks and get a specific class of neural measure-valued processes. And here I give you some examples. So this would be a Black Scholast type measure-valued HTM model where we have always this drift part, and we have this covariance structure where we just have this beta function. And you see, this is very similar to the Black School's case. So the mu square would correspond to. the mu square would correspond to the s square of uh of the black shuls process where the and the beta is just the um yeah the what is usually called sigma square in the black shows model and this is um yeah a specific polynomial model where we can compute expected values of such polynomials by a system of linear pdes and here you could parametrize this beta function by This beta function by neural network and use then techniques, for instance, established by Akra and Filipovic, certain polynomial expansions to option pricing and internal calibration, or use also some measure-valid extensions of basket option pricing coming from multi-voltage shows models. And another one, which I want to go more in detail, is the F. Is the affine specification. So this would be the case where the q2 is zero and where we just have the q1 with this function alpha. And then you can write this what we had before. You take just the diagonal parts of the second derivatives here. And this alpha is now a positive function on our underlying integrals E. And this here is nothing else than a variant of the Dawson. A variant of the Dorsenwater-Nawve superprocess, if you're familiar with this. So, this is a branching process where usually instead of this D over DX operator, which induces the shift semi-group or the negative shift semi-group, you would have the Laplacian. And now we have this one. And this variant of the Dawson-Water-Nabel superprocess qualify is in particular an affine process where you can apply this whole. Can apply this whole machinery from affine processes to get tractability. And this tractability means, in particular, that you can compute the Fourier Laplace transform for certain functions now, so that take values that are negative and in the imaginary component. And then you have this expression for your Fourier Laplace transform with this psi and this psi. Psi and this psi solves this. Hello? I'll need to wrap up shortly, Christa. Yeah, okay. Yeah. So this is this Riccati equation that you can solve. And again, how do you calibrate? You calibrate by parametrizing this alpha via neural network. And yeah, so let me just show you. So here is the calibration part. The calibration part. As I said before, we will have to calibrate to a family for different strikes and different delivery periods. And using this tractability of FIM processes, in particular, that we can price those call functions via Fourier pricing, where the parameter, this function alpha comes in. function alpha comes in, we can then use the Fourier pricing and directly work with gradients on this side that we can compute explicitly in order to optimize this. And yeah, so let me just show the numerical results. So if we sample from such a Dosen-Water Nabe type process, then we get for one fixed maturity. For one fixed maturity, and here for different delivery periods, we got this kind of prices. On the x-axis, you see the strikes, on the y-axis, the prices, and here we have the different delivery periods. So, this already with a very simple neural network gives quite nice results. And we have also a learning result only with simulated data for now. So, we took a model from Bent and Kruna and calibrated this with this. With this affine model, with this Dawson-Watanabe model, and yeah, this gives already quite good results. So, let me wrap up. So, the idea that we had here is using measure-value processes for energy markets. There are certain mathematically convenient classes, namely FN and polynomial processes, which allow to work in particular with classical neural networks. Classical neural networks because the characteristics are in terms of functions in the spatial variable, and this can then parametrized by neural networks that gives tractable transport maps. And yeah, we get quite high parametric infinite dimensional stochastic models, but which are tractable in particular because one can very explicitly compute the gradients and then that can be plugged in for optimization. And then that can be plugged in for optimization procedures. Okay, so thank you very much, and sorry for having. I'm happy to have for one very fast question if there is one. Anybody has a quick question? You can ask one. Okay, go ahead. Hi, Krista. This is Schoming. Hi. I was wondering if you can use some ideas like normalizing flows to combine. Normalizing flows to combine some of these, like if you have, have you thought like in normalizing flows, you know, they would like combine one, you know, one transformation after another? And does it make sense for you, okay, you have this affinal polynomial processes? Maybe there is a complicated. So I'm not sure that I heard it well. Could you please repeat? Okay. Normalizing what? The normalizing flows. Maybe this is a stupid question, but as I was wondering, you know, there is this normalizing flows that come. There is these normalizing flows that come up in the generating models where they combine a single a simple model and then they like repeat it, they compose it with one another. Yes, you mean these recurrent structures or okay, so it goes well with your polynomial or raffine models? Like, can you do this kind of iterated compositions? This, I'm not sure. I don't know. No, I'm not, so I'm not sure exactly. So, I'm not sure exactly what you mean with the iterative compositions. Are you referring to recurrent neural networks or? Right, so in the generating models, this literature, they would use like a simple model and then they will iterate over it to get more complicated models. I don't know, so I would have to say. I don't know, so I would have to see exactly this iterative structure if this is applicable or not. Yeah, talk later. Thank you. Yes, no. I had a very quick question about how quick this is. So your calibration numerics, how long do they take? Is that half an hour? Is that a day? I mean, so the thing is, so on my computer, it's a bit difficult because we Bit difficult because I have to go high in the Fourier dimension, and so this one took quite a while. But I think I can push it, and it's really just a preliminary implementation. But with the firing processes, is there sort of a hope for having some sort of hypertuning when you learn the map of, you know, you go one step up and you learn the map from the option prices to the parameters of neural nets? To the parameters of neural nets? Is that something that's feasible or is that just too difficult? That you go, that you just learn the map from. I think it could be a possibility as well, to do this classical deep calibration, you mean, or that you have a neural network that learns the map from option prices to parameters. That's right. Yeah. Yeah, so here it was more direct to really permit.