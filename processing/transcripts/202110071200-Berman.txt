So, hi, everyone. My name is Ron Berman. I'm an art marketing professor at the Wharton School at the University of Pennsylvania. And today, I'm going to describe joint work with Ellie Fight, who's also joined us, where the goal of this paper is to try to solve a really big problem for advertising experiments, where you basically, even though you have very, very large sample sizes, the results often have very large sampling variation. Have very large sampling variation, and you can barely tell if an effect is significant or not, or the confidence intervals are way too wide, etc. And what we're trying to do here is we're trying to kind of utilize a few of, let's say, the we're making assumptions that kind of fit a lot of the data that we're actually seeing. And we're also building a model and applying it to this data, which allows us to achieve slightly or more accurate estimations of the average treatment effects. So just to So, just to kind of explain kind of in clear terms what we're talking about, we're talking about experiments which are called holdout experiments. So, holdout experiments are usually these experiments where one group is exposed to a test, let's say a catalog in our case or an email or an advertising, and another, the control group is usually held out. So, usually they don't see anything. We're thinking of other extensions where there might be another treatment. Where there might be another treatment, but basically, we're assuming these are the two groups. The consumers in this case are fully randomized. Okay. And we're also going to assume that the responses, if there are any responses, are going to be non-negative. So that means, for example, sales. They're not going to be either zero sales or non-negative sales, or there's going to be maybe time on site. Either they visited my website, so they clicked an ad and they visited my website for some time, or they didn't. Okay. And it will see. Some time or they didn't. Okay, and we'll see in a few minutes why this is important. And what we're trying to do is we're trying to estimate the average treatment effect. We're going to assume the standard potential outcomes framework. Consumers have a potential outcome under the treatment called YI of 1, a potential outcome under control YI of 0. Our goal is to estimate tau, which is the average treatment effect, expectation of the difference between the individual potential outcomes. And at least in marketing, what we then later do, this is At least in marketing, what we then later do, this is kind of how we use that for decision making. We're going to try to say, is this treatment effect higher than the cost of exposing someone to the treatment? And this will allow us to say, should we target this population or should we continue with this campaign or not? Now, the standard way that we've all been doing that is to do what's called the difference in means estimation. You take the average response under the treated group minus the average response under the control group. The average response under the control group gives you an estimate of the ATE. And we have a formula that we use to compute the standard error or the sampling variance of this, depending on what estimator you're using. And this is kind of an upper bound because we don't really know the correlation between the potential outcomes. And in this formula, what we have is the standard deviation of the responses under the treatment, standard deviation of the responses under the Treatment, standard deviation of the responses of the control divided by the sample size in the treatment, which is N1, and in the control, which is N0. This is like the very start of things everyone has been doing. You can use it. You can also run a linear regression that will give you a very similar estimate. And historically, what we've seen is that even if you have experiments with very, very large sample sizes, when I talk large, I mean 2 million people, 3 million people. People, 3 million people, like in the millions for ad experiments, even then, companies are unable to say if the ROI is positive or negative for an experiment. So they see that they made money, but they actually can say if this is significantly different than zero or not. Like is this due to noise or not? And the reason, there are two reasons basically. Often the customer level data has very high variation in responses. Okay, specifically what we observe is. Okay, specifically, what we observe is that a lot of customers just do not respond to do not respond at all to ads. They don't buy, they don't click, they don't do anything like that. And also, what companies often do is they don't do this experiment kind of split half half. They usually will take a very, much, much, much smaller control group because they're thinking, oh, if the advertising is working, I'm actually losing money by having a control group. So I would like to make it very, very small, which also increases the variation of the AT or the estimate of the AT. Or the estimate of deity. So, what Ellie and I have been doing, kind of, we have these two papers, and we're like pretty upset with this result because this result says no matter what you do, unless you have like the entire population of the planet, you can't really estimate if online advertising is working. So we were asking what can we do maybe to take the structure of online data into account and maybe improve the results, or maybe we need to change the question. So, the first thing, which is I'm not going to talk about today, is a paper. Going to talk about today is a paper called Test and Roll. We got this name from a company that actually utilized this technique. And they said, What we do is we run a very, very small test, an A-B test between treatment and control. If the test is better than the control, we don't wait for significance. We don't compute sample size. We don't do anything. If just the average is higher than the other one, we just roll with it. We just deploy the highest or the better treatment. And we're trying to think: are there cases And we're trying to think, are there cases when this is optimal? And what we did is we wrote the full profit maximization problem of an advertiser. An advertiser basically, this is kind of a very simple two-stage armed bandit. There's a first stage, which is a test stage. Then given the test stage, you pick between A and B, which one is better. And then you roll, you deploy to the rest of the population, A or B. What we do in that paper, we compute the optimal sample size. Paper, we compute the optimal sample sizes. We show that the regret is very, very minimal compared, let's say, to a multi-armed bandit, that you don't really need hypothesis tests. And those experiments can be much, much smaller in the test stage compared, let's say, to an experiment which is designed to achieve statistical significance. But this is not what I'm going to talk about today. This is a published paper I recommend if you're interested in this approach to read. It's already having impact on companies because a lot of them have been implementing that because they realize they will just never have enough. They will just never have enough sample, enough population to try these large experiments on. And then the other approach is saying: no, suppose I do want to have a lower variance or a lower standard error of my estimate. What can we do to decrease the sampling variance of the estimated 18? And this is what we call latent ratification. So, what can we do to decrease the sampling variance of the estimate? There is a well-known method, which is called post-ratification. Well-known method, which is called post-rotification, which kind of the idea says something like that: imagine I could divide the population into different strata. I knew something about these consumers, and within every strata, the variance of the responses of consumers is going to be more narrow. So the standard deviation there is going to be more narrow. But across the strata, I'm going to have higher variance. What I do then is I do an estimate of the treatment effect within each stratum, and then I take the average, and this will give me Average and this will give me a more precise estimate of the AT. So, for example, imagine that I know that populations in the east coast and the west coast behave differently, but they're very homogeneous within the coast. I can do estimates there and then average that. This is kind of a classic approach. Usually, in these experiments, you cannot do pre-stratification. You don't know how good the customers are before you run the experiment, but you do know something about them after you run the experiment. And we have results. Experiment. And we have results that say that the post-ratified estimate is going to be better. But what is important is that you need to have covariates, you need to have variables that are going to be correlated with the responses of the consumers. So if I just take demographics, which are often not correlated with how consumers respond to advertising, actually it might increase my estimate. Sorry, the misprecision of my estimate, so the standard error of my estimate. So if I use pro-certification, Estimate. So if I use pro-certification with the wrong variables or I just don't have the right variables, that might hurt me. So there's some research on this area. There's a method called Cupid, which tries to say, take the same measures that you're going to measure in the experiment, for example, sales, and post-ratify based on that. In many experiments, we actually have past sales of consumers that were collected. Of consumers that were collected in an observational setting. So we know just how much they bought, or we see that they've seen advertising which was targeted, which again was not randomly assigned. But again, we have some information about the responses. We sometimes have third-party data about users, et cetera, and we can apply this method. There's more recent work about causal random forest that says if you do have an experiment and you do have Experiment, and you do have those covariates, we're going to try and identify basically the best covariates for you that will correlate with the treatment effects individually. And this might lead, if you're kind of a bit lucky, to lower sampling variance of the ATE. So this is not what we're going to talk about today. What we're going to try to ask is, can we post-stratify without any pre-treatment variables? Okay, given what I observe in the experiment, can I already achieve? Observing the experiment, can I already achieve a better estimate without starting to collect this data, et cetera? This applies, of course, to the new area of more privacy online, but also applies to a lot of the companies we talk to. They say, later I learn something about the consumers, but much, much later after the experiment. When I need to do this analysis, I actually don't know much about all of these consumers. I just know their address, and that's it. So, I described the motivation. What I'm going to do now is I'm going to talk about. What I'm going to do now is, I'm going to talk about the centrification model and explain it in detail. I'm going to present an application to log experiments. So, these are important because although catalogs seem old school, they work really, really well if they work and they're pretty expensive. So, sending a catalog to your home is about $2, like those fancy catalogs. You want to make sure that if you send it to 2 million people, they're going to be effective. Then, I'm going to discuss about the theoretical benefits of latent certification when it does work. Of latent certification when it does work and when it maybe doesn't give us that many benefits. And finally, I'm going to conclude. So, what is latent certification? First of all, in a lot of our data, what we observe is consumers who just do not respond. So, they either don't buy anything or they don't click an ad or they don't visit a website. So, basically in our data, they appear as a zero or a missing data after they were exposed. Missing data after they were exposed to the ad or they were put under control. Okay. And what we would like to do is we would like to posit this model. And if you know the principal certification model, this is basically that model where we make very slight modifications that apply to marketing. We divide the consumers, or we would like to know if the consumers belong to one of three strata. So we call them A, B, and C. Strata. So we call them A, B, and C. The A is what you know also is always takers. Okay, these are people that will buy the product under treatment and under control. Their potential outcomes are positive under both treatment and the control. The B stratum is what you might know as compliers. They don't buy under control. They do buy under the treatment. And the C stratum are the never takers. They don't buy under control and they don't buy under treatment. Under treatment. Okay. Now, if I had estimates of the treatment effect within the A strata, the B and the C strata, which we call them tau A, tau B, and tau C, and if I had estimates of the size of the strata, or maybe I observe them, like what is the size of each one of those groups, I can compute the basically the weighted average of the AT of this strata to get an estimate of the AT of the entire experiment. Okay, now if I really knew, like that is, I observed for each consumer to which stratum they belong to, actually, I have a really, really good estimate because generally, and look at these strata, in the C stratum, I have zero variance of the AT. The effect is just zero. They don't buy ever. In the A stratum, the only effect is how much an increase I get in the amount they buy. Amount they buy, but I don't get an increase in whether they buy or not. And in the B stratum, I switch them from not buying to buying, and also I get an effect there. Okay. So by design, this is supposed to also not give me kind of the nice estimates that we get from principal certification. It will also lower the variance of the average. But effectively, what we observe is this type of data. So let me describe what we observe here. Okay, for the people that are under treatment and don't buy, we know that they are in stratum C. But if they do buy, we don't know if they're in stratum A or stratum B because we don't know the other potential outcome. So we just know that someone who buys under treatment is going to be either in stratum A or stratum B. For the people under the control, this is the lower right-hand box. We know that the Ardor and Stradum. That they are there in stratum B or stratum C. And in the left-hand box, we know that they're in stratum A. Ellie, can you see my mouse or you can't? Yes, I can stand. Okay, so that's great. Okay, so this knowledge will allow us to estimate both the size of the strata and also the response within a stratum to estimate the ATEs. So, given this, we're going to define the likelihood of the data. For people in this For people in this box, we know that their probability or the likelihood is just pi C is the size of this group, okay? And we can actually estimate it because this is just people undertreated who didn't buy. So the share of people undertreated who didn't buy, this is going to be the estimate of pi C. This nicely gives us an estimate of pi B because we know that people under control who don't buy are coming from group B or group C. We know their share is going to be pi B plus pi C. Be pi B plus pi C. So the difference between pi B plus pi C minus pi C is going to give us a pretty good estimate of pi B. To estimate group A, we know that everyone under the control who bought definitely is in stratum A. So it gives give us identification of pi A. And we can also identify the responses. Sorry, we're going to make the assumption the responses under the control. Responses under the control for those who buy come from a normal distribution with mean mu is zero and standard deviation sigma is zero. And because we know everyone that belongs exactly to this group, we can just estimate it directly. You can think about it as like a method of moments estimate. The only thing we're left here is this group here. Okay, this is the only group that we can't really well identify. And here we're going to make an assumption that An assumption that what we have here is a mixture of two normals. Okay, that people in A respond with a normal distribution in their sales, people in B respond with a normal distribution in their sales. And basically, the likelihood of the data in this group is going to be pi A times the normal with mean mu A1 and standard deviation sigma A1, and pi B times the normal with mean mu B1 and standard deviation sigma B1. Okay, and this all And this, all of these definitions give us a complete likelihood definition of the data, which basically allows us to estimate all of these parameters. So given the likelihood of the data, and let's suppose we maximize the likelihood with an inference method. I'll show you estimation method. I'll show you in a few minutes. Basically, what we will get is we can compute the overall ATE as just the size of group A times the effect. Group A times the effect inside group A plus the size of group B times the response with the mean response to the treatment because we know that under control is zero. Okay. If you know the literature on principle certification with truncation by death, it's literally the exact same thing, but we have a twist. And the twist is that in marketing data, if someone died, someone didn't buy from me, I made zero dollars. So I actually don't think that their outcome is missing or unlike. Think that their outcome is missing or unobserved. I actually know that the company is making $0, which allows us to make this estimate. So, how can you estimate these models? You can use maximum likelihood. You can use Bayes MCMC. I'm going to show you today the Bayesian approach. Actually, in the revision of the paper, we're switching to the MLE approach. The issue we're having is that we have this mixture of normal. We have this mixture of normals here. This is sometimes weakly identified. There are conditions where it doesn't work well, usually, when the two means are very, very, very close or one group is very, very large compared to the other. But this is actually an open kind of research area. We're also going to assume that the standard deviation of the responses within the three groups that actually have That actually have non-zero responses is gonna be the same standard deviation. It improves identification, but actually it doesn't seem to be very crucial in our application. Now, you can also add defiers, so those people who are gonna not buy under treatment and buy under control. In marketing settings, it's a bit odd to say that when I show you ad, you actually buy less, although it is possible. Either you're really upset with the company. Either you're really upset with the company that they showed you so many ads, but a more possible thing is: oh, I sent you a catalog. So you thought, oh, I should buy more clothing. You go online, but actually, as a result, you go and buy from a competitor. But if I don't send you the catalog, you just go online to my website, you don't check anything, and you buy whatever you bought before. So maybe there is this option. But first of all, it creates a second mixture to estimate. It lowers identification and basically makes the And basically, it makes the problem much harder. If you have really, really good covariates, you can probably add the fourth strata of these defiers. So, what are the assumptions that we're making here before I move to the application? Okay, so we're assuming monotonicity. We're assuming that advertising either increases your response or has no effect, okay? But it cannot lower your response. We're going to assume normality, which is if you do have a non-zero response, if you do buy, the distribution is going to be. The distribution is going to be normal. In our data, we're going to log the data. So it's log normal sales, going to make them look normal. We're assuming independence. So we're assuming that people have independent potential outcomes, especially in this group of A, which means that if I know what they would have bought with treatment, it doesn't say anything about what they would have bought with control. Okay. And finally, we're adding to the principles. And finally, we're adding to the principle justification approach. We're adding this assumption that people who are not responding actually have zero outcome. So what is the application? And let me show you. Remember, I tried to, what is the goal? The goal is to lower the variance of the estimate of the AT or the standard error, etc. So we have five catalog experiments. So these are called lift tests. The company tries to send catalogs to people's homes. To send catalogs to people's homes. Usually they do it roughly once a month. It depends on the season a little bit. This is a very large brand that you all know. It's a specialty retailer. They exist in all of the US. They have physical stores. They have an online website. And they ran these experiments where they had about 140,000 customers in each experiment. They split it roughly half, half. Half the people received the catalog, half did not receive the catalog. You can see the The catalog, you can see the dates and the months here. You can see the sample sizes in the treatment and control group, and you can see the average log response. Okay, so if you take this to non-log numbers, you would get that the responses are not very high. They're going to be anywhere between, let's say, two to five dollars. Again, remember, you're sending this catalog is about two dollars. But partially the reason is a lot of people just don't buy. So those who do. Of people just don't buy. So, those who do buy, they actually spend a lot, they spend anywhere between tens to hundreds of dollars. But most of the people just don't buy under treatment and under control, with and without the catalog. So, for example, let's look at experiment two. Okay, so this is the experiment from October 2017, which is kind of representative in our data. The purchase rate is only 16.2% in the control group and 16.6% in the treated group. Okay, so more than 80% of the Okay, so more than 80% of the people don't buy anything. And then the purchase amounts, you can see when you log it, it kind of looks normal. Plausibly remember, these are the people on the left are in the A stratum control, and people on the right are both A and B stratum. Plausibly, there's a mixture of normals here, although it's going to be very hard to see. Okay, there's a slight increase in. There's a slight increase in the purchase rate between control and treatment, and maybe there's some increase overall. So, for inference, there are three approaches generally we found work. You can do maximum just direct maximum likelihood estimation, no expectation maximization, no nothing. And to compute the standard errors, you can use the delta method. And actually, because the formulas are so. Because the formulas are so linear, etc., you can get almost close forms for everything. You can do Bayesian inference, or you can do MLE with bootstrapping. What I'm going to show today is Bayesian inference. This is from the previous version of the paper, which is online. And now we're switching to MLE because it allows us to do a few other things in terms of proving when this approach might give you a better estimate. So let's take a look again at the different. A look again at the different experiments. What you see here on the left is just on the x-axis which experiment. The gray bars are the difference in means estimates with their 95% confidence interval, sorry, credible intervals. This is a Bayesian estimate. And the orange bars are going to be our latent certification model with their 95% credible intervals. And what you see here on the table in the right. Here on the table in the right, is you see the percent reduction in the estimated standard deviation in the variance of the estimate. And what we observe is that the variance of the estimate goes down by 50%, 40 to 50%. The worst is 36%. This is a dramatic decrease. Now, why is this important? Because when you're computing sample sizes for experiments, usually the sample size Usually, the sample size grows linearly in the variance of the responses. And if you're able to estimate them with much higher precision, basically you can reduce the required sample size in an experiment. I was talking straight up and I forgot to ask at this point if there are any questions before I move forward. Yeah, go ahead. A really quick question. So in this setting, is there any reason you can compare this all to? Is there any reason you can't compare this also to the post ratification? Because if you're sending, if you're doing mailing, you should have some idea of who these people are. So you should be able to compare it. Am I missing something there? You're not. So it kind of appears later in the end. Okay. And what we show is that this approach achieves almost everything you would get from post-certification. So the post-certification itself doesn't achieve much more. The nice thing is that it gives you, in this context, when you split to those. In this context, when you split to those specific three strata, it gives you some indication of what covariates are the ones that will tell you maybe to which strata each person belongs to. So it gives you more information. Maybe you can target better or predict the strata of people. But actually, in terms of various reduction, it doesn't add much, which was surprising to us, but it doesn't add much. I see. And then, as a somewhat practical question, I mean, in this setting, like where you're doing direct mailings as opposed to people. Where you're doing direct mailings as opposed to people coming to your website. I mean, you should also be able to sort of have, you know, do pre-stratifications or some block randomization. Does the firm ever do that or not really? They don't. So I think partially the way it worked for them is they kind of outsource a lot of these things to other companies. And this is also why they do holdouts, right? And not something else, because holdouts is just say, don't say the Else because holdouts, just say, don't send it. We're just going to send you a smaller list, right? Um, everything more so. So, the way it works is like even the modeling and who to target and what co-varieties, et cetera, is done by an outsource company, okay? Not by this retailer builder large enough. They basically bought a service from someone else and they tell them who to send. They actually transfer to another third party on who to send the catalogs to. So, for them, it's just much easier to say just hold this off, et cetera. Just hold these off, etc. Okay. Thanks. Can I jump in on that? Also, the answer to that is very context-specific. Like your original question, guys, like, could, you know, could post-stratification do better? And that's always going to be very context-specific. And we weren't trying to solve this retailer's problem. We're really just using their data as an example of how the latent stratification would work. You know, right. It totally. Right, it totally depends on what they're collecting, whether or not post-stratification is going to be beneficial or not. It would be hard to get a universal answer to that. In this case, at least the data that they had doesn't seem to be great, okay, in that sense, which is surprising, but they actually don't know that much about the customers that would tell them if they're responding to the advertising. Let's look a little bit about the parameter estimates because we can learn a little bit from here. Can learn a little bit from here what happens in this data. So, what we observe, and again, this is experiment two. We observe that the catalog does two things, okay? It increases the propensity of people to buy, what we call the extensive margin, but by a small amount, okay? So it's like 0.4% increase. 4% increase in the chance of buying. Okay. But those people actually buy a lot if they do buy. So a lot of the benefit comes from that. And it also increases a little bit the amount that people spend if they were buying anyway with the control. So this is an increase, if I remember correctly, of about like $3 or something like that among those people. Like that among those people. So the estimates of the mean in the end seem very, very similar between the difference in means. So this is an increase of you need to do the exponents, et cetera, but it is profitable. But the standard deviation is much, much, much lower for the latest modification versus the division and means. Just to take you back here, in experiment two, you can kind of tell that this advertising. Tell that this advertising was working. In experiment one, actually, you might not be even able to tell if your advertising was working, and also in experiment five. And this kind of gives you slightly more clarity about whether your campaigns are working or not. So where are these benefits coming from? Okay, so what we're trying to do is not only to say, okay, try this method and it might work. Okay, a lot of the questions and our interest is, can I say in advance under what? I say in advance under what conditions and under what scenarios I'm actually going to get a dramatic improvement in the variance of the estimated AT. And we're trying to take a two-pronged approach. We're trying to take a theoretical approach and also a simulation approach for things we cannot solve analytically. Okay, so for post-certification or generic photosification, we know that what we would like is to have the effect Like is to have the effects homogeneous within the stratum and heterogeneous across the strata. And by construction, the way we built or the way we assume the strata are working, this actually is built by construction. Okay, because in the C group, you have zero effect. It's very, very homogeneous. In the A group, basically, we're saying you're not increasing the probability of purchase. So the only increase is going to be in the amount you purchase. And advertising many times mostly increases. Advertising many times mostly increases your chances of purchase, but not how much you purchase. And a lot of the variance, if at all, would come from the B group. Okay, so by design, you're hoping to get this benefit. Now, if we observe the strata, that is, if I knew each person, whether they belong to group A, B, or C, we can actually compute this benefit and it's always positive, like always a reduction in the variance. And we can actually say under what condition is going to be the smallest, the largest. Is going to be the smallest, the largest, etc. Now, because for this A group who are treated, we actually are unable to exactly say which person belongs to which stratum, what we are doing is we're losing somewhat of the benefit. And to analyze that, we're going to do a simulation analysis. So, in the simulation exercise, what we did is we took representative parameter estimates from our data. We generated a thousand data sets from the model. We generated a thousand data sets from the model, and we're trying now to change the parameters or the descriptives of the population to see when is the sampling variance reduced the most. We're going to do that when we assume either we observe the strata or we partially observe the strata as in our real data. So, what you see here on the left is basically the same chart you saw before. Basically, you have this. Basically, you have the size of the A group going from basically zero to one. So the A group now increases and everything else is fixed. Well, relatively fixed. The gray bar is the difference in means estimator. The dark brown is the latent notification. And the light orange is what we call the observed strata or the oracle model, where you actually know which customer belongs to which strata. Which customer belongs to which strata. And on the right-hand side, what you observe: so here you see the ATEs and their credible intervals. On the right-hand side, you see the sampling variance of the ATE. And what you want to look is basically the ratio between those. So it's a bit harder, but you can see when group A becomes much, much, much larger, the sampling variance increases. It's the highest when group A is half of the population. Of course, the observed. Of course, the observed certification model has the lowest sampling variance and a dramatic improvement over, let's say, the difference in means. And the latent certification in this case is about half of the benefit when pi is pretty small and actually gets closer to the difference in means when pi is large. So, when the mixture model is not identified well, which is this case here, you see that has certification basically lose a lot of the benefit. Same thing you can do with pi b and again. Same thing you can do with pi B. And again, pi B and pi A are complementary to one another. So when pi B is very, very large, pi A needs to be very, very small. The mixture model is not well identified. Again, you see that the benefit kind of gets lost. But generally, you kind of get very flat, let's say, increase or decrease if you just if just the size of the big group changes. What you wonder is the effect within the group to change a lot, but the size itself you don't want it to change. But the size itself, you don't want it to change. What happens when you change the standard deviation of the responses? So, this is sigma. Generally, what we observe is that the standard deviation of the responses is very, very, very small. That gives you a lot of benefit because all of these zeros, you can throw them out and basically you can get a much more precise estimate. And when the standard deviation is extremely large, first of all, the mixture model is not as well identified, so you don't. Not as well identified, so you don't correctly compute the sizes of these groups and also the means in these groups. And second of all, just the data itself is so noisy that it's going to be very hard to say anything. Now we're going to look at what happens when the AT within a group changes. So remember, mu A1 is the mean response in stratum A. So these are people under treatment, and this is their mean response. And μA0 is And μA0 is fixed in this case. So, this is the response of people in stratum A under the control. And what we observe is that when it's really differentiated, so the AT becomes larger and large, not AT, sorry, the treatment effect in Shadow May becomes larger and larger and larger, the latent certification method gets almost the theoretical benefit that you can observe. But when mu A1 becomes very, very close to MuA0, Very, very close to mu A0. Basically, again, you cannot well identify the treatment effect there or differentiate it within the nixture model. Basically, you're going back to the difference in means. Mu B1 is exactly the opposite. Mu B1, remember, is the response under treatment for the compliers. So you want it to be very, very different. Sorry, you want it to be not too different. Different from the control in order to get this benefit. Because when the response becomes very, very high, again, your mixture model becomes unidentified because the mu A1 and MuB1 are too close to one another to separate them. And basically, you're losing the benefit. However, when mu B1 is small enough, and you can tell, oh, A is, let's say, a much higher. Is let's say a much higher response level and B is a much lower response level, then you're getting much of the benefit. Finally, this is the response level of the A group under control. It doesn't make that much of a difference if it increases or decreases. Again, what I'm comparing is the ratios between the numbers here. And the reason is that the control group in the A stratum is pretty well identified. It's its own data and it works pretty well. So just to sum. Pretty well. So, just to summarize: okay, latent notification in our simulation and also theoretical results reduces the sampling variation most under two conditions. When the strata are similar in size, so the best case scenario would be third, or third, okay, and also when the mixture model is well identified. That means usually that mu A1 and mu B1 are going to be very distinct, okay? So you can tell that they're very, very different. Can tell that they're very, very different, and hopefully, the variances either you know them or going to be very, very similar and not too wide or too widespread. So, I'd like to summarize what I've discussed. I skipped a part of the paper where we actually introduce covariates. And what we show there is we show that if you just plug in covariates as they are into this method or even into a standard processified estimator, usually they don't. Estimator, usually they don't predict that well. And actually, you want to bring some economic intuition on what works. So you want a transformation of these covariates to work well. And partially these covariates, because they're not generated under an experiment, they're generated kind of observationally. You want to try transformations that maintain maybe something about the behavior of customers. So just to give an example, if we see, suppose we have a covariate saying how many pass purchases. How many past purchases the person had, okay? But sometimes the person received a catalog, some other person didn't receive a catalog, and this was not randomly generated. So the company thought they should target them or not target them, etc. What happens is that people who see catalog catalogs would buy more. So now if you use pro-certification based on past sales, basically you're losing this correlation into the latent strata that will allow you to do the identification. It will allow you to do the identification. You basically have classic endogeneity there. What we show in the paper is that you can do a transformation on past sales that will rank consumers. Okay, and this is maintained this ranking of who purchases a lot or not, either under treatment or under control. As a result, it actually gives you high correlation into which stratum the person belongs to. I'm not going to go into too much details there, but this is all in the paper. So, just to summarize, okay. Okay. In our data and in most advertising data, when we run an A-B test, we often get imprecise estimates of the ATE. You can tell you made money, but you can't tell if this is noise or this is maybe replicable or not. And what we proposed here is an application related certification or principal certification for advertising or marketing data. And what we need for this to work is we need to know that the potential. To work, we need to know that the potential outcome of people who do not respond is zero or positive. Okay, you can normalize them, but you do need them to respond to ads in one direction. The resulting AT estimator is a smalling sampling variance that the standard difference in means. This will always happen if you know the strata or you have really good covariance to identify them. And this will mostly happen when the mixture model is what when. The mixture model is well identified. What I haven't shown is what kind of past observational data you can use to better identify this strata membership. So what we found is that we took recency, which is just when was recently this customer active in the in basically made the purchase, which is highly predictive. Basically, if they're in the C stratum, they're dead, basically. They wouldn't buy under treatment or control. Basically, they wouldn't buy under treatment or control, or they're in the A and B, they might respond to advertising. So, recency works pretty well. And also, what we looked is we looked at responsiveness to treatment. So, what we did is for each individual, we took their data under seeing a catalog versus not seeing a catalog and just computed the difference there. This is, of course, biased. It's not a correct treatment effect or individual treatment effect estimator because of the targeting. But, what we show is that. But what we show is that actually the ranking on average of customers is maintained. That means people who will respond very positively under an experiment would also respond very positively with observational data under targeting, assuming the targeting targets well, assuming the targeting knows to target people with high treatment effects. So what it does is just amplifies its effects. So people in high remain high, people in low remain low. And then you can use pass research. Can use fast responsiveness to observational targeting of catalogs as a covariant. What we're doing now is basically we're trying to also think of other setups where we think you can apply this method. So again, remember what we want is when you give someone a treatment, you know the direction that the response is going to go. Either it's not going to change or increase. So one thing we thought about recently is pricing experiments. We increase the pricing and Experiments when increased the pricing, I know you will probably decrease your purchases. So, again, we kind of have downward sloping demand. As a result, that means we can apply this approach maybe not just to advertising experiments, but also to pricing and other types of experiments. Thank you very much for listening. I would love to answer any questions if there are any. I would love to hear any comments, suggestions, etc. Questions, thoughts