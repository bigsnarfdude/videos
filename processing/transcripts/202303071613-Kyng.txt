Many years overdue, but the workshop was a good occasion to get it done. So we'll hear about robust and practical solutions of Laplacian equations by approximate elimination and its joint work with Juan Gao, who is an MPI but did a master's thesis with Medi Tech Durich and Deins Bielman. So there should be a little bit of surprises or news for both TCS and numerical analysis people in here, but People in here, but just to kind of gauge terminology and stuff, do we all know what a symmetric diagonally dominant matrix is? Okay. Do we all know what incomplete Cholesky factorization is? Yeah, okay, basically, I see like a little bit of error, but okay, almost. Does incomplete Okay, almost. Does incomplete Cholesky factorization solve SDD linear equations? Like if you use it as a preconditioner? Hands up if the answer is yes. What do you mean by solve? Great, okay, okay, so we're not sure. We're not sure. We're not sure by equation. So we're going to talk about solving equations that look like Ax equals B. I'm trying to find X, and A is going to be S dB. Is going to be SDD basically, except I'll call it Laplacian, but you can think about this as being essentially the same thing. Okay? Symmetric? Symmetric. Yeah. Symmetric. So the S here is for symmetric, not for stricting. Okay, so linear equation solvers, we're looking for x such that Ax equals B given A and B. And we could do Gauss elimination, we could do n to the omega with faster direct methods. Omega with faster direct methods. Of course, we can use structure to do better. So I think on the numerical analysis side, this might mean doing incomplete Cholesky plus multiple method for some things, multi-grid. On the TCS side, Spielman and Teng in 2004 showed that Laplacian linear equations or symmetric diagonal dominant ones can be solved to high accuracy in nearly linear time. In nearly linear time? Provable. Okay. Okay, so what are Laplacian or STD equations? Do they ever come up? So heat diffusion is a problem that numerical analysis people will say we can solve in time nearly linear in the number of non-zeros of the matrix in practice using incomplete Cholesky factorization or multi-grid methods. And for us, the linear equations, so in TCS land, the linear equations that you get here if you discretize things are, at least if you do it right, they'll be symmetric diagonally dominant, and in particular, they'll be Laplacian, which is slightly more restrictive. So I'm sorry, now I don't understand what you mean by incomplete Kulesky, because uh it should not work very well. Uh like it's Okay, but do you do you think this should work, multi-bit methods? Multi-bit methods? Or here? It's probably n blocker. Oh, okay, okay. Yeah, that's what I mean by nearly linear. Yeah. Okay. So now it's worth mentioning that when theoretical computer scientists think about symmetric diagonally dominant matrices, for us they come about for very different reasons. Things like solving a convex optimization program using an interior point method. And so, like, Point method, and so like your geometry is not a flow of heat, okay? It might be like a flow of passengers in a flight network that covers the US. That's a very different shape. Okay, and in TCS, there was a lot of excitement about this Wielman-Tang result. There's been a lot of work by a lot of the people here, Richard, Suchant, Aaron, on improving the Laplacian solver. This started with Kudis, Miller, and Peng and then. And then we got a very nice simple algorithm here from Aaron using a tree-based preconditioner. So, Stephen will know something about that. And then the very fastest algorithm is due to Aaron at the moment, but I won't be talking about that today. I'll be talking about something closely related to an algorithm that Sushant and I developed in 2016. But the general theme of all this TCS research was to try to get simple. TCS research was to try to get simpler algorithms, faster algorithms, and maybe parallelizable ones, but making it work in practice has been hard going. So that's what we're trying to get to now. So we have a new algorithm and an implementation in Julia in this manuscript, this archive. The code is online in laplacience.jl, and we made a benchmark which we called SDDN2023. So I'm hoping that if you So, I'm hoping that if you feel like you have another solver, maybe you'll try and test it on some of the same data. And it's a variant of the approximate Gaussian elimination from King and Satcheva in 2016. So, let's talk for a second about what that means. So, if I have a matrix and I'm trying to compute a Cholesky factorization, To compute a Cholesky factorization of this matrix, okay, and it's symmetric, let's say, symmetric diagonal net. Then I'm going to first eliminate the first row and column of this matrix, right? And this, if there are sort of some non-zeros here and here, then when I eliminate this first row and column as part of my Cholesky factorization, I've got Chalesky factorization. I'm going to get new entries in this submatrix. Are we familiar with this happening in Gaussian elimination? Okay. I could also do this. So okay, so now let me point out that I can think about this sort of in a graph way. So I can think about this as saying there's a variable, the one that I'm eliminating, that has some interactions with three neighbors, which Neighbors, which I'll represent with an edge. Okay? And now, in terms of non-zero structure, if I get rid of this variable, I'll just have the three neighbors. And this is telling me that I'm adding a clique on these neighbors. Is that a clique? You have too many. Too many. Yeah, there we go, right? So it's time I'm adding a clique on the neighbors. All right. Alright. So the plan in King Sativa is that we're just going to downsample this clique. So we're going to throw away most of the edges. And then we'll prove that if we re-weigh the non-zeros that we indeed keep, then this will somehow work quite well as a preconditioner, this Cholesky factorization. Is that statement? Is that statement vaguely clear to some people? Okay. So now the key new idea that we're adding here to make things somewhat practical is that we want to try to maintain connectivity in terms of the non-zero structure when we throw away things. So I'm going to try to make sure that I have this little clique. That I have this little clique. I don't want to break it into two pieces when I'm saying it. So, how can we do that? So, here I have the same example. What is going on? The same example that I drew over there, except with four neighbors. And I just want us to think about the non-zero structure when I'm eliminating things, right? So I have my vertex v that I'm eliminating, or variable. And then in the end, if I get rid of In the end, if I get rid of just one entry at a time, when I eliminate the first entry, I get interactions between that neighbor and all of the other neighbors. So that's drawn here in blue. When I eliminate the second entry in the row for this vertex I'm trying to get rid of, I get interactions between that neighbor and all the remaining neighbors. That's also shown in. When I navigate the third entry in the row, I'll get interactions with all the labels. I'll get interactions with all the later neighbors. There's just one. And the last elimination will actually not create anything more. If I add these three sets of non-zeros together, I get the Gaussian elimination clique, this Virchilesky factorization thing that I added in here as my update. Now, our strategy is just going to be for each of these blue things. Of these blue things that I drew here, I have a star between that thing and some neighbors. Okay, so I have from the first variable to all the other ones, from the second variable to all the later ones, from the third one to the single later one. In each of those, I'm going to randomly pick one of these entries to actually add. And this will be my approximate 2S defined trace each. So here for each. So, here for each of them, I picked a single edge. I'm going to do it at random. I'm going to have to scale it a little bit if I decide to put the entry in to make this an unbiased estimator. But that's the idea. And this will preserve connectivity. Does somebody want to tell me why? I'm claiming that. But I'm claiming that this orange blob always connects all of the neighbors. It's always a tree, right? Each type of a middle. It's always a tree. In particular, one way to think about it is that there's like four neighbors and three edges, or n neighbors, n minus one edges. And also, I can always go to a lower numbered neighbor from where I currently am. So clearly, I can always go to the last neighbor, so I'm connected. So I'm connected, I have m or like neighbors. n or like like neighbors minus one edges it must be a tree okay so now we do some experiments and this is where things start to diverge from other people who think about solving symmetric diagonally dominant linear equations because that is mostly something that numerical analysis people do and they do it because these things come out when you're doing heat diffusion and more generally Poisson problems. Poisson problems. So down here we have our Poisson problems. There's something called the Society of Petroleum Engineers benchmark for connoisseurs of Poisson problems, and there's a bunch of problematically generated things. But are there other STD matrices in the world? Well, okay, on sweet sparse, there's 28 other STD matrices that have size at least like 1,000 logs. Size at least like 1,000 non-zeros. The algorithm also works fine on the smaller ones, but to avoid penalizing things with kind of a startup time, we threw out the small ones. Okay, so there are at least 28 other SDD matrices in the world. Okay, so then we also tried to programmatically generate some other SDD matrices in ways that to us seemed like they would be maybe Like they would be maybe at least a very diverse set of matrices. And we call these chimeras. And we'll go up to 250 million non-zeros in the experiments. And what are chimeras? I'll tell you offline if you want to know more, but it's basically that we're going to recursively try to build diverse graphs. So recursively means maybe you're just going to build two diverse graphs, and then you're going to combine them somehow by, say, taking a graph. By say taking a graph product or just adding some random edges between them. And then the recursion might terminate at some point and give you something like a ring or a grid or a complete graph and so on. Okay, I said TCS people like to think about solving things like flow problems, and we did try to solve some flow problems by plugging, by doing interior point methods for solving a linear program. Here are point methods for solving a linear program, and then using you get some SDD matrices if it's a flow linear program. And we try to solve those linear equations. The IPM is super inefficient, but the solver works. Then we were feeling good about ourselves until Suchant engineered a really interesting graph that breaks our algorithm. Then what do you do? Now, instead of for each of these blue things where we sampled one edge, we'll sample two. This algorithm we don't know how to break. And this might not be an accident, or it might be. So we might need to do a lot more experiments. But two might actually be special. It might be that one doesn't work, and you can prove that two works. I'm not sure though. Shorter. Then we get to the diffusion problems, petroleum engineers, and a bunch of Poisson problems that we got from a paper that was about testing Hyper on really large instances. For those who know what Hyper is, it's another thing that can solve STD problems. Okay, and now you're going to get angry with me because I'm going to show a table and that's really unpleasant, right? And that's really unpleasant, right? If there are going to be numbers in the table. So, oh, first let me talk about what we are going to compare with. I'm sorry about this. Okay, so it's all going to be single-threaded. We didn't get to doing parallel yet. And there are some interesting challenges there about getting a good parallel algorithm. I don't think we have it yet necessarily. We're doing this on a 2.4 gigahertz processor with five. Processor with 500 gigabytes of memory, we have our solver, and then when Sushant broke it, we have a version with two samples. We have combinatorial multi-grid by Kudis. This is mainly just for the people who might recognize something here. So we have Hyper, for those who know what Hyper is, with something called a Boomer algebraic multi-grid preconditioner. And a Kryloff-based solver. PETSI with the same preconditioner. By the same preconditioner, incomplete Cholesky factorization together with precondition conduit gradient and MATLAB. Linear algebraic multigrid, which didn't do so well, so it won't appear in the tables. I also tried Trilinas, which is not fast enough to run on a single processor. So that's like a strictly multi-threaded thing, as far as I can tell. Are all of these PCG or have some of them last? So, comitorial multi-grid, yeah, I think it's also PCG. Oh, oh, okay. So, before we get to the table, I think I just want to keep some numbers in mind. Okay, so I want to claim that when we're looking at microseconds to do a complete solve for non-zero, less than one is going to be great. Okay? Seven is great. Okay, seven is going to be respectable. More than twenty is going to be so-so, and infinity is going to be bad. I hope the last one is not controversial. Oh, okay, so now, okay, so let's look at street spars. So AC and AC2. So, AC and AC2 are almost great. The other ones are so-so or bad. That's the conclusion. The chimeras, bad and respectable. Max flow is somehow fine, so I don't actually yet know kind of how to use large but not completely unhinged, insanely large weights to create. Unhinged insanely large weights to create a bad instance. We haven't tested it with sort of 10 to the 30 weights. Satch diva stars. Okay, so what does this star star mean? It means that you are off from the target relative residual error of 10 to the minus 8 by at least a factor 10,000, so you didn't get that close. That's the star star. Star means you are off by something at least. So here we didn't get that close. So here we didn't get that close. That's when we made AC2, and it does fine here. So somehow that was not a hard case when we sampled twice. And then there's the... It does almost great. It's almost great. Yeah? And then now, so Poisson problems is what numerical analysis people care about, right? And here, hyper is great, right? Can you say Hyperturbo was 2D or 3D? 3D. These are all 3D. Or 3D? 3D. These are all 3. Yeah. So 3D uniform coefficient, high contrast, and anisotropy. And you can tell that we didn't get it completely wrong because we're still sort of close to simple things like incomplete Julespy factorization, but we're not going to get to hyper times easily. Except that, okay, it's actually written in Julia, so maybe if you spend as much time as Lawrence Livermore does on it, then maybe you'll get closer. Alright, so we're low on time. But that's the key message. Okay, one of these columns is not like the other ones. There is maybe a solver that just will solve your SDD instances now. So if you have ones that weren't solved in the past by Hyper, which is sort of the state of the art, you can go and try this one. But if you're a numerical. But if you're a numerical analysis person, maybe this is not so important to you because your problems are Poisson problems. But maybe if you're a TCS person, you'll think about putting this inside an IPM or something. Okay. If Ilse was here, I was going to, or like people who were going to sort of say, you're insane to look at the worst case time. Nobody has said that yet. So let's say, okay, you're insane to look at the worst case time. Let's say, okay, you're insane to look at the worst case time. So we can also look at like medium, 75th percentile, max. Okay. There we can see for this is sweet sparse, so those 28 matrices. CMG doesn't do so well in the 75th percentile. Hyper has some bad cases, but already in the 75th percentile it looks great. And AC is kind of the same, just pretty stable. Just pretty stable. Chimeras, just looking at the largest instances of this diverse family. The main thing that's interesting here is that, okay, they were the worst ones for us. And Hyper is only bad sometimes. And then the rest of the time, it's a little bit worse than us. So I feel okay about it. And yeah, we're out of time. Thank you. Thank you. Questions? Something you said that I'm just not clear on. So, I mean, Spielman Tang is mathematically guaranteed. So you hinted that your strategy is not mathematically guaranteed. So if you write AC log M, then it is provably correct. I see. AC2 might also be. Two might also be provably correct, but we haven't proven it. So it's like somehow your method is more robust, these different types of graphs. So like, I'm sure you looked at it. Even if you just look at the Poisson problems, if you looked at, say, like the 90th percentile runtime, do you get closer to Hyper? Because it's somehow more robust to the hardest case? No, so worst case, Hyper was already beating us on Persona. Hyper was already beating us on Poisson problems. Poisson, right? Oh, this is worst case, not. This is the worst case, yeah. Okay. So Poisson, we're kind of getting beaten, but there isn't a big range. So in Poisson problems, like we're just the same all the time, and hyper is just the same all the time. Okay, I guess I was thinking of this as average or something. So for average, is it what's the how's the head-to-head, like for Poisson problems? So for average, I mean, we're losing. So, for average, I mean, we're losing by a factor five approximately on Poisson colour. Do you remember the number of non-zeros? Yeah, so like these are we're going up to 250 million non-zeros and it's like a seven-point stencil. Final question? Any idea why Hyper does solve the Maxwell IPMs? I think we didn't do that many. Uh I think we didn't do that many. I wouldn't like uh we just picked we just picked like the five cut because these IPMs are super slow. We picked the five worst chimeras for us and then we set up a flow instance on them to see if we could make things worse and we couldn't like the flow weights were actually a little bit easier for now. Cool. Thanks for watching. What machine were you using? Oh, so we had this very briefly, sorry. It was Intel Xeon 2.4 gigahertz with 500 gigabytes of memory. So let's take a short break. We'll reconvene at 50 minutes past. Thank you again. Thank you.