So, today I'm going to talk about some of my recent work in non-parametric mixture model. We discussed a new method for estimation unknown parameter in non-parametric marking value mixture model. So, this talk is based on ongoing work with my collaborators, Dr. Taoyi from NUS. Dr. Tao Yu from NUS, Dr. Jin Ching from NIH. So now let me move to the next slide. My talk has four parts. In the first part, we will set up the problem and then briefly review the literature and then outline what we are going to do next. In the second part, we will propose our new method, discuss the symptomic property we have developed, and then present. And then present EM algorithm for the numerical implementation. And the third part is a numerical study. And then the last part is just a summary. Okay. Now let me move to the first part. So my talk is about non-parametric estimation in multivariate matrix models. So let me start with the problem. Suppose we have NID p-dimensional random vectors from M component. Random vectors from M-component matrix model. So I denote M-component matrix model in terms of the CDF. So this is the form of the joint CDF for the M-component matrix model. Now here, FM, if you're changing the slides, somehow they're frozen. We can see them. So am I still in the first slide or I mean in the slide four now? I mean the slides for now? Yeah, still on the title page. Okay, let me let me check how I can do. Okay, now yeah, now something is moving. Yeah, now it's page four. Okay, I guess I'm not sure. When I use the full screen model, it looks like it doesn't work. So if this one is better, then I will just use this. Okay. So. Okay, so now let me just restart. Fortunately, we are on the problem setup page. So we have an ID random p-dimensional random vector from a M-component mixture model. The joint CDF is given in this form. Now, in this form, the Fm is the joint CDF of the random vector in M subpopulation, and then numb M is the mixing proportion for the M subpopulation. For the M subpopulation. Now, we may agree that if we do not put any further assumption, then the model parameter in this model are not identifiable. Even we, you can see the neighbor switching, they are still not identifiable. Okay, so what do we can do? In the non-parametric mixture model literature, a commonly used assumption or restriction on the component CDF is a conditional independence assumption. Assumption. Basically, for the M subpopulation, the joint CDF is the product marginal. Another way to say this is that if we know the membership of the random vector, then the p random variable in the random vector are independent. So that's called conditional independence. There has been some work under this model assumption. It has been shown that if the conditional independence assumption is satisfied, Assumption is satisfied and p is greater or equal to three. So the dimension of the random vector is greater or equal to three. Furthermore, for each dimension, the capital M marginal CDFs are linearly independent. Then the model pyramid in the mixture model is going to be identifiable up to neighbor's reaching. So the identifiability under this model has been proved in the literature. What we are going to do, we are going to What we are going to do, we are going to follow this conditional independence assumption and then do some estimation work. So let me summarize. So we have ID random vectors from the M component mixture model, and we assume that conditional independence assumption is satisfied. We also assume that P is greater than or equal to 3. Now, our interest is to estimate and the model parameter in this model. And the model parameter in this model: so we have m mixing proportion, we have capital M times P C D F. We need to estimate them. Now, throughout this talk, we assume that the number of component M is given. So we are not going to discuss how to do that. Indeed, we still don't know how to do that. So I will give a very brief review of the three methods that in the literature. That in the literature. The first two methods are closely related to our work. And the last method is the method we are going to compare because there is our package to implement in the third method. Okay. The first method. The first method is the Professor Peter Hall and his collaborate 2003 OS paper. The idea of this method is very simple. So the proposal estimate which minimized the integrated Which minimize the integrated IO2 distance between the empirical CDF based on the data and then the joint CDF based on the assumed model. So we have the empirical CDF based on the metadata, we have the CDF based on the model, and then we can see the integrated L2 distance between these two. After that, then we minimize the distance. So this is a minimum distance idea. The advantage for this method is that they have proved the square root. That they have proved the square root of n convergence rate for the unknown parameter. I should call it a limitation. The limitation of this method is that in this paper, they only consider the number of component m equal to 2. And also, to my best knowledge, there is no convenient algorithm or public available package to implement this method. The idea is simple, it's easy to define estimate, but for me, it's kind of difficult. For me, it's kind of difficult to compute the estimate. So that's the first method. The second method is Professor Petervo and his collaborators' follow-up paper in 2005 by Matrica. This method is called the inversion method in the literature. I will explain the main idea by considering a very simple case, p equal to 3 and m equal to 2. This place we can get some idea that we why we idea that we why we require that p is greater or equal to three for identifiability so in this case we the dimension of the random vector is three we have two components then in total we have seven unknown parameter functions to estimate so we have a one mixing proportion we have three marginal CDF from the first component or first subcombination we have three marginal CDF from the second subcombination so we have seven unknown parametric functions Have seven unknown parametric functions. Now, if we consider one-dimensional CDF on the fourth dimension, then based on the mixture model we proposed, we assumed then we can find a closed form for the CDF for the one dimensional, the first dimension. So we have this form. Now, if we replace the left-hand side by an empirical CDF of the data on the first dimension, then we have one equation. So this Equation. So this part we just estimate by empirical CDF. The right-hand side is the unknown parameter we are interested in. So then after we do that, we get one equation. We can repeat the process. We do all the three one-dimensional CDF. We do all three two-dimensional CDF. We do one three-dimensional CDF. Then in total, we are going to get a seven equation. We have seven parameters here. We have seven equations. Then we have. Equation, then we have enough equation to estimate an unknown parameter. If p is smaller than 3 here, then we are not going to have enough equation to estimate the unknown parameter. So we have to require that p is greater or equal to 3. So that's the whole idea. For me, this idea is very close to the idea for this one is very close to the method of moment. Advantage is that they prove this method, the proposed estimate still. The proposed estimate still has the unconvergence rate. And then they found that this method is much easier to implement compared with the source method. Okay, that's very good. So it's kind of easy to do now. There are also some limitations for this method. First of all, it has been pointed out in the literature that this method is not so convenient to use when the dimension of the random vector is greater than three, and then the number or the number of And then the number, or the number of the components three. Either of them, then this method becomes much more difficult to implement. Again, there is no publicly available R package to implement this method. At least I didn't find one. So that's the second method. Now I move to the third method. The third method is a color-based method. So in this 2011 paper, biometric paper, Navy and his collaborators introduced a Collaborators introduced a color-based EM-like algorithm for estimating the mixing proportion and the PDF. They are not estimating the CDF, they are estimating the PDF. Now, I summarize several advantages I found for this algorithm. So, first of all, they proved that an algorithm is guaranteed to converge. And then they have also implemented this algorithm in our function called mpmsl in the P M S L in the R package mix to us. So basically, there is a public available R function to implement the method. So that's the first advantage. The second advantage I found is that based on my personal numerical experience, I found it's very difficult to beat this method when we're doing the density estimation in their assimilation setup. I found it's really difficult to beat. To beat. And also, they have shown that their method performs much better than the second method. So that's the advantage. There are also some limitations for this method. First of all, the statistical property of this method, such as the consistency and the convergence rate, are unknown. So we still don't know if the proposed estimate is consistent or not. The second one, The method itself is designed for the continuous data. The performance of this method for discrete data or the mixed type of data remains unknown. The third issue is an issue for all colour-based methods. So we need to know how to select the bandwidth for the kernel-based method. Sometimes it's not an issue, sometimes it's an issue. Okay, so I'm going to stop the introduction review and then Problem integration review and then go to my objective for this talk. Now, suppose again, suppose we have the data from the mixture, M-component mixture model. We assume that the conditional independence assumption is satisfied. We are going to propose a tuning pyramid-free method to estimate all and parameters. So, no need to enact the tuning pyramid. We also hope that this our proposed method has a theoretical support and we have And we have a computational effective algorithm to implement our method. The third objective is that we hope that our method can work for discrete data, continuous data, or the data involves both types. So that's our objective. Now, let me just continue to our proposed method. Just record that this is our model assumption. Our method started with defining some indicated random variable. Some indicated random variable. So for any given p-dimensional vector t, so this is a fixed, this is a fixed one, p-dimensional vector. And then our s observation xi, this is also p-dimensional vector. And then we just define a p-dimensional indicated random variable, p-dimensional vector, and each element is an indicated random variable. Basically, we compare each component of xi. Each component of xi with the corresponding component of t, then we get p-indicated random variable. Now, this vector I denote as the capital IIP. So for any given t, then we can define this p-dimensional vector. Based on the model we assumed in the model two, it's easy to verify that the nog likelihood contributed by this p by one vector is given in the L i theta t. Given in the L i theta t. So we just apply the model to so inside the bracket here, that's the likelihood contributed by each indicated random variable. The product here we use the conditional independence. The summation here is considered a mixture model structure. So that's the likelihood contributed by this p by one vector. We have only observation for each one, we can have a likelihood. Then we take the Likelihood, then we take the summation on the likelihood, then we get I al theta t. So that's a log likelihood contributed by m p by one vector. So this is a form of the likelihood. Now we can maximize this likelihood to obtain estimated of lambda and then fm here. But this is in this place we need to specify this t vector. So t is actual vector. So what we are going to do is that we are going to So, what we are going to do is that we are going to choose multiple t vectors and then sum up the likelihood based on the multiple t vectors. Then we need to discuss how to choose this multiple t vectors. So our choice for the t vector is just the only observation we have. So basically for each observation xj, we can define a likelihood L theta xj. So each one we are going to define likelihood. Defined likelihood. Then our final likelihood will be the summation of the likelihood defined based on each xg. So this is our final likelihood. The notation is the L1 theta is the summation of the likelihood defined based on each xj. This is a closed form because you can say that in this form, the majority part is the binomial likelihood. So we call this likelihood is a binomial likelihood. And then also And then our final proposed estimate is going to maximize this binomial likelihood, subjected to the constraint that lambda m are mixing proportion and then f are the CDF. I'd like to provide the first comments. As we can see, this is our likelihood. We just need to maximize the likelihood to obtain the estimate. So the proposed method is tuning parameter three. So we don't need to choose any tuning parameter. So, we don't need to choose any tuning parameter here. In these slides, I just want to share some asymptotic results we have developed. So, we show that under some regulatory condition, we can have n to the power minus one over three convergence rate for the proposed estimate. We hope that we can prove the convergence rate is n to the power minus however, unfortunately, we are not able to do that. Able to do that. So, but we still have a convergence rate here. The second call, so now here I would like to provide a second comment. We would like to see that although our result, when the convergence rate is n to the power one over three, that our result can be applied to any type of data, be applied to discrete data, continuous data, or mixed type of data. So that's the first result. First result. Now, because of the mixture model structure in the likelihood, we proposed, we found it's kind of easy to propose an EM-like algorithm to compute the proposed estimate. We use the theta s to denote the estimate in the s iteration. And then in the instead of the s plus y iteration, we compute z i g m s. So this is IGMS. So this is pretty much similar to the posterior probability in the URL EM algorithm. So we have a closed form to calculate that value here. And then in the M step, we found that we have a closed form to update lambda M, the mixing proportion. We also found that updating the CDF is equivalent to finding the solution of the weightedness. Solution of the weighted weighted isopoly regression. Now, finding a solution of the weighted isopoly regression, this can be easily done by using the poor adjacent wide-native algorithm. And this algorithm itself is available in our package. So, basically, in summary, in E-step, we have a closed form to calculate Z. In M step, we have a closed form to calculate the lambda. And then we have a convenient L. And then we have a convenient algorithm to calculate the updating formula for F, the CDF estimate. Because it is the EM like algorithm, so we can expect that the binomial likelihood will increase, will not decrease after each iteration. So that's a similar property of the URL EM like EM algorithm. We also found that the like the Also, found that the binomial likelihood we proposed is more than or equal to zero. Because it's smaller or equal to zero, then the sequence here is monotonically increasing, so the algorithm will eventually converge. It may not converge to the global maximum, but will eventually converge. So that's the current comments. Okay, now I move to the simulation study. Here, I just we have Here, I just have done quite a lot of simulation to compare our method and the existing method, but I will just show the result for comparing the proposed method and an advanced method, because this method is, that is our function to implement this method. The following are the criteria for the comparison. So, for the mixing proportion estimate, we use the mean square error as the criteria. For comparing the CDF estimate, we use the integrated square arrow. So basically, we can see the square distance between the S C D F estimate and the true CDF after that we take the integration. So these are the criteria for comparison. We present the result for two simulation studies. One favors the advanced method, another one does not favor their method. The first study, we consider the number study uh we can say the number of components is equal to two and the dimension is uh the dimension of the vector is equal to three the proportion for the first subpopulation is 40 percent and the second subpopulation is 60 percent for the first subpopulation and the random variable in three dimensions are iid normal zero one uh for the second subpopulation the run three random variables uh are independent uh the first one follows normal three one the second one follows normal The second one follows normal 41, and the last one follows normal 51. So those are the continuous random variables. It's kind of easy to use the colour-based method for density estimation. So this setup really favors an advanced method. The second method does not favor their method. So the data are generated from discrete rank distribution. We still have two components, three dimension. The maximum proportion are still 40% and 60%. Proportion are still 40%, 60%. For the first sub-population, the three random variables are IID, Poisson, Y random variable. For the second sub-population, the three random variables are generated from Poisson 3, Poisson 4, and Poisson 5 respectively. So the random vector here are discrete random vectors. Now we can see the three sample size: 500, 1000, 2000. The number of replication is equal to 1000. I saw that. Now, table one. This table summarizes the result from study one. So, in the column for lambda one, that's the mean square arrow for estimating lambda one. The column for the CDF are the integrated mean of the integrated square arrows. Because this method favors an advanced method, so we can expect that their method will perform better. But we also notice that if we are just interested in Notice that if we are just interesting estimating number one, then we have similar performance for two methods. If we interest in estimating the CDF, an advanced method is going to be perform better. This is somehow expected. But we also noticed that if the sample size increases, then the difference between these two methods becomes smaller. That's the first study. The second study is for the poison case. On the Poisson case. Now, for estimating number one, our method is the mean square error is going to be much better. For estimating the CDF, the integrated square error is going to be much smaller. This is somehow expected because the advanced method is designed for continuous data. It may not perform well for discrete data. So that's the second study. Now I move to the analyze real data just for illustration. Just for illustration. The real data we are going to consider is the spam-based data set. This data set is publicly available. This data set has 57 features or variables. And then the number of observations is 4,600 emails. So the emails are classified as normal and spam. We know which email is spam, which email is normal. Email is spam, which email is normal. For this 57 features or variables, the first one is called the percentage of the word in the email that matches the word make. So the percentage of the percentage that the make appears. The second one is the percentage that address appears. We have 57 variables here. Now, I just want to emphasize that for this data set and the The membership for each observation is known. Now, if we just ignore the membership information or do not use the membership information, then the data will follow the smart variant mix model with 57 dimension and then two components. Now, I would like to emphasize that it may not be suitable to assume that all the features are continuous random variables, just because if we just simply check the first variable, If we just simply check the first variable, we found that around 77% of the value from this 4,600 emails is going to be zero. So for the first variable, we have around 77% zero. So this variable is highly zero inflating. This also happened for the second variable and some other variables. Now, let me just share some experience that I've Let me just share some experience that we have played with an advanced method. So, if we just simply apply this method to the data set, it will produce some errors because we found that the range for the 57 variables are quite different. So, after we try several things, and then we found that if we rescale all the variables so that they have roughly the same range, then the advanced method. Range, then the advanced method is going to work. Their method gives an estimate for lambda one, that's the proportion of the spam emails. The estimate is around 32.9%. Now, if we just simply apply our method original data without doing any rescaling, our method will give estimate 0.341 for lambda one, the proportion of the spam email. For this data set, we know that which one is spam. set we know that which one is spam which one is normal so the actual proportion is 0.394 so both of them are not that close but our one our estimate is closer for estimating lambda one now because we know the membership for each observation so we can somehow compare the accuracy of the CDF estimate so we first we calculate the empirical CDF of First, we calculate the empirical CDF of the case variable in the spam email group, and then we calculate the empirical CDF for the case variable in the normal email group. So we are going to treat them as the true value for F1 key and F2 key. After that, then we calculate the integrated square arrow for an advanced method and the integrated square arrow for our proposed method, and then we case the difference. We use the EMK to denote a difference. E m k to denote a difference. Now, if the em k is greater than zero, that means our method needs to smaller integrated square integrated square arrow than the advanced method. If it's smaller, that means the advanced method is going to be better. Now, this plot gives the box-cox plot for the 57 integrity, the difference of the integrity square arrow for the normal group. This is for the spectral. Group. This is for the spam group, the box cocksplot. Now we can see that for normal group, except for two variables, our method produced like smaller integrated square error. For the second one, I did not count how many, but for the majority of time, our method produced a smaller integrative square error. So I guess the major reason is that, as I explained, Is that as I explained, the data has highly zero inflation for pretty much almost all the variables. Okay, now let me give a very quick summary about what we have done. So in this talk, we propose a maximum binomial likelihood method to estimate the CDF of the sub-population within the framework of market-varied matrix model. I would like to emphasize that our method is tuning parameter-free, has some theoretical Has some theoretical support and is easy to implement by using the EM algorithm. Also, our method do not require that the CDF is continuous. So basically, it means that our method is ethical to discrete data, continuous data, or mixed type of data. We have done quite a lot of simulation study although we just presented two. We showed that our method is very stable and the performance is very stable because Performance is very stable because we don't need to choose any tuning parameters. And then, in many situations, our method outperforms the AC method. This is our last slide. Thank you.