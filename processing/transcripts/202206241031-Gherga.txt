Here this morning with Adela Garricka, who's going to talk to us about efficient resolution of two-a-malar equations. Thank you for the introduction, and thank you, of course, to the organizers for putting together this fantastic conference in the middle of the mountains in LAS. And lastly, thanks to all of you for sticking around for the last day of talks and showing up in person. It's nice to not be speaking to an empty room. So I'm going to be talking today about two Mahler equations and an algorithm that we developed to solving them. Developed to solving them. This is joint work with Samir and will hopefully be published in the next few weeks. Right, so to begin with, I should probably tell you what a two-mahler equation is. It's just a Diogantine equation that looks like this. Here, F is a binary integral form of degree at least 3. A is some integer that you fix. The PI are a bunch of primes, which are rational, of course. And your unknowns are x, y, and the z i, which are all. x, y, and the z i, which are all integers. And the only condition that we impose on this equation is that x and y are co-prime. Otherwise, you'll just get multiples of the same solution. Okay, so this is a two-maller equation, and we would just like to solve these things. And I don't mean theoretically, I don't mean one particular equation, I mean like all of them, and we would like to do it efficiently. So, in other words, we want an algorithm that we can program that will just do it as quickly as possible. And, well, why do we want to do that? Okay, well, if you've seen my talks before, then you might know where I'm heading with this. And I promise you, this is hopefully the last time I'll talk about this because once we get this published and running, we probably won't have to keep talking about this. But essentially, there is a connection between two molliforms and elliptic curves over Q. And in particular, okay, there's this very long theorem. This is page one of three, so maybe don't actually read it. The basic just It. The basic gist of it is that if you want to move to curve over Q with conductor N, then there are a finite set of two-maller forms of degree 3 specifically whose solutions you could just take and chuck into this form here, and you get all of the elliptic curves over Q of conductor N. These things aren't important. They're actually on page 2 and 3, I think it's the Hessian to Copian discriminant. This D is defined later. It's not important. Later, it's not important. It's very straightforward and easy to read off of the theorem. But basically, we want to compute elliptic curves over Q. We want to do it through these two emolar forms. Okay, so this algorithm lends itself, sorry, this theorem lends itself really nicely to an algorithm, to computing elliptic curves. And it's about three simple steps. Read the theorem, find all the forms that you need, solve the forms, and then just plug in the solutions and get your elliptic curves. Solutions and get your elliptic curves. Okay, now this last step is really straightforward. It's just bit operations, just plug the solution into the formula. So, really, the bulk of the work is in these two steps. And I think the best way to summarize step one and step two is with the following analogy. So, step one is really just read the theorem and just carefully program your thing and make sure that all your coefficients. Your thing and make sure that all your coefficients are correct. And then run some code. And in fact, we've already done step one. We did a one-time computation that took about a week, and we've computed all two molar forms of absolute discriminant up to 10 to the 10, which is now 120 gigs of data. So if you ever need a specific conductor, we could just pull out the respected two molecular forms from this database. Step two, solving them, a little harder. And I'm going to sort of explain why. And I'm going to sort of explain why. Okay, before I go on to explaining why, I should probably also mention that we know that there are finitely many solutions for two Molly form. This is proven by Mahler in 1933. Unfortunately, this proof was not effective. In other words, it didn't tell us how to find those solutions. However, in 68 and 69, Sprinzo, Knogadov, and Coates showed that there is an effective way to bound the number of solutions. And then, of course, in And then, of course, in 1989, Snakasundeweger published a practical method for solving a general two-muller form. And in their paper, they actually went through and they gave a really nice example and effectively created an algorithm for doing this. This algorithm was then implemented in Magna by Karl Hamburg in 2011. So, an algorithm exists. Okay, which brings me back to my point of, well, we are going to use this to compute. Well, we are going to use this to compute elliptic curves. An algorithm exists. So, what's the problem? Well, if you want to compute, for instance, all of the elliptic curves over Q with conductor up to 10 to the 6, there are about 6 million two molar forms which you need to solve. And if you're in this happy little perfect world where each one takes five seconds, that would be about one year on a single core. So, this is like great, fantastic, if that's the case. Spoiler, it's not. It's not. So, this is a nice example of a two-muller form that will show up just a few times. If you run this on the Hambrook implementation, it will take about four minutes. So it's a bit of a far cry from our five seconds. Here's a slightly less nice example where the only difference is that I've got a leading three. This now gets you 1.6 hours. Here's another one with a few more primes. This is four hours. And then there's this. Hours and then there's this really really bad example which I don't know how long actually it will take because it ran for several months on my computer not my computer someone's computer it ran for several months and then it just maxed out the memory and magma crashed so I have no idea and I see I've a tribute to Philip's talk here with a funny face we don't know we don't even know how to do this so the idea of computing elliptic curves over Q up to 10 to the 6 conductor is kind 10 to the 6 conductor is kind of a far cry with the you can't really do it with the current implementation. So we need a new two-maller solver. But also, I don't just care about two-maller forms for elliptic curves. There's also a tied to Gormative equations. So a Gromativ equation, I'm just going to very briefly mention this, is an equation that looks like this. Everything here is an integer. And if you fix n, and in particular, you take n to be 5, then you. You take n to be 5, then you get two molar equations of degree 4. And so, if you want to find solutions of the Gormatic equation in this particular case, you have to solve a bunch of two molar forms of degree 4. Okay, and because degree 3 went so well, surely degree 4 would be better, right? Okay, well this one was actually not so bad. This one was only 20 days, which is somewhere between not great and also not bad. But moreover, you can also But moreover, you can also say, well, what if you're interested in the Reminusian tau function? Well, if, for instance, you wanted to say something about tau at some power of a prime and wanted to prove that it was not equal to some other power of a prime, then you can get two molar forms of degree for a function of m minus one over two. So for instance, if you take m to be forty-one and p to be eighty-three, you get a degree twenty to a model equation. Degree 20 to a molar equation. So again, on the 2molar solver, the implementation of SmackSpit Vager by Hamburg, again, no idea how long this will take. No idea if it's even possible. It probably isn't. So the general gist of it is that the current implementation isn't quite up to speed for what I would like it to do. Okay, but with Samir, we now have a brand new tool. Samir, we now have a brand new 2Molar solver. And to demo this new 2Molar solver, I'm just going to give you this quick example here. So this is a degree five 2Molar equation, which from Sweden-Tannakis we know has no solutions. And we know this because Seydman-Sanakis ran this on a handbrook implementation and it took 72 days. And then they said that was too long, so they created their own code which took Code which took 2.3 hours, but it required manual input, and it was very specific to this equation. So we went ahead and we tried this in our algorithm. Got many instances of magma open. So this is 0.0 seconds to solve. Okay, this sounds like maybe something's a bit fishy. Sounds like maybe something's a bit fishy. I promise it's not. This is actually just some silly trick that we do that happens at the very, very beginning of the code. And I wrote 0.1, it's 0.6. Anyway, it was fast, basically. So this is a huge improvement. Okay, so from the way that our two-modeler solver does this, we actually conclude even further with this trick that we actually also don't have any solutions to this equation, where n is just some integer cosine to 3. Ramta 3. So that's pretty great. Some of the other examples, I think this is the degree 3 Tumala equation at the beginning that I said is impossible to solve. And here's some degree 11 equation that someone sent to me because it raised an error and they sent it to me via a screenshot. Not the way that you send an error report, but let's see how this goes. So I'm going to launch this one, and then I'm going to launch the other one. This one, and then I'm going to launch the other one. Okay, so you can see it's just going to go through. Okay. Okay, so this is good because already we are solving the Already, we are solving the equations that before we couldn't. So, 21 seconds and 30 seconds. So, these are things that we couldn't do before. But we're pretty happy about that. Okay, so how do we actually do this? What are the steps in which we solve the two-moller equation, and how does this differ from what Senakis and Neveda do? Generally speaking, it's as an overview, it's not that different from what Senakis and Neveda do. We follow the same steps, but we just sort of deviate. Steps, but we just sort of deviate along the way. So, the same overarching theme is that we generate a very large solution on the bounds using linear forms and logs. We successively reduce this bound using Diamond approximation computations, and then we search below this reduced bound with some clever sieve. This is exactly what Zenakis and Reveger do. We've just sort of taken it to an extreme. Okay, so to start things off, here's our two-month form that we want to solve of some arbitrary degree of bigger than three. Some arbitrary degree of bigger than three. There's some normalizing that we do, but it's not super important. Basically, we just create a monic irreducible polynomial whose root we then just chuck into Q and get a number field. And then in this number field, we basically can say that solving the two-moller equation is equivalent to solving this norm equation. Okay, so starting with our norm equation, we can show that there are finitely many. We can show that there are finitely many ideal equations of the following form. So for every prime, for every prime ideal showing up here, these are the ideals whose ramification index and inertial degree are equal to one. And for each rational prime in our norm equation, there's exactly one prime ideal in S. Okay, here, everything else, all the other prime ideals above each rational prime are. Each rational prime are, they have a bound, so we know what their exponent should be, and we just sort of chuck them into this ideal A. Okay, from these ideal equations, we then... Yes. So I lost track of which things are fixed and which are the unknowns. Oh, sorry, yes, good point. Thank you. So the things that we know, we know A0, we know theta, we know the random integers, of course, and we know everything here except for the exponents. Everything here except for the exponents. So we don't know x, we don't know y, and we don't know what the ni are. Okay, all right, thank you. Okay, yeah. Okay, so from this family of ideal equations, we want to work in the number field, so we principalize this sort of one way or another, and we end up with a bunch of equations that look like this. So, again, here, just to emphasize, the unknowns are x, y, and bi. So, here, tau is more or less the generator of the ideal A, and the Of the ideal A, and the delta I are the basis of the ring of S units modulo torsion. Okay, so as an example, let's look at this degree 11 equation. So here there's two possibilities for the ideal equation. If we look at one of them, this is what our ideals are. This is one of the generators of the ideal. Maybe not the best choice. Definitely. Yeah, sorry. It's just this ugly thing. Okay, and then we can say even for that exact example what our equation of k looks like. Here are all ten of our deltas. Please don't verify this. Please don't verify this. Okay, so for each one of these equations in k, we're going to generate a height bound and we're going to start reducing this bound. So I'm not going to say too much about this. Essentially, if we let b be the L infinity norm of our solution vector, then using linear forms and logs, we get a bound. This is using results of Gujot-Giori, Viv, Yu's-Lemma, et cetera, et cetera. Lemma, etc., etc. The reason this is C20 is because that's how many coefficients it took to get there. So it's not a nice-looking result. Well, it is, but it's not fun to read. Okay, so another thing, just for the sake of notation, I'm also going to write V2 for the bound on the L2 norm of our solution. And of course, if I have a bound on V, then I have a bound on the L2 norm. So going back to our degree 11 equation. Degree 11 equation. If I run through this bound, then I get something to the order of 10 to the 222. So obviously, from here, I can't just brute force my way through at least. That's just not possible. So I need to reduce this boundary. Okay, and here's where we begin to deviate from the work of Senakis and Nive. In this case, this is where they would begin to work in p-iadic fields and extensions of p-iadic fields. And that's where things get a little bit messy, especially when you're dealing with precision errors. Especially when you're dealing with precision errors and just general complexity of the algorithm. So we avoid that completely. We only work with pedic fields theoretically to generate the upper bound. And then from there on, we don't. We don't really. Okay, I'm going to let epsilon be the product of the delta i to the vi. And I'm going to suppose that I have a bound on my L infinity arm. Initially, we start off with C20, as I said before. Okay, so when we generated this initial upper bound, we actually This initial upper bound, we actually used somewhere along the way this inequality. So here, mk is the set of all places of k, and 17 is some constant that comes out of that computation that I think is generally less than 1. And what we want to do here is for each place of k, we're going to compute some bound on each of these sumins. Now, when I say each place of k, I really mean the infinite places and the primes. Places and the primes showing up in our ideal equation. If the prime is outside of our ideal equation, then this sum end is going to be zero. So we don't care about it. Okay, so to actually find these bounds for the finite places, we're going to find some integer k that's going to bound our evaluation of our ideal equation. For the infinite places, I'm not really going to talk about it because it's slightly annoying. For the complex places, it's actually Places is actually pretty straightforward. You get a fairly tight bound at each place, but for the real places, it's kind of annoying. So, I'm just gonna say that and leave it there. If you want to know more, we can chat about it after. But okay, so essentially, the new thing here is this, how we're bounding our final places. Sorry, the summoned on the final places. And once you have a bound for each of these places, you just chuck it back into this formula, and then you get a new bound, and then you. And then you get a new bound, and then you can iterate and start again. So you sort of go back here and start the process over. And you do this until you no longer get an improvement. Okay, so let's focus on the finite places. So if I choose one of the prime ideals in my set S and some integer k, I suppose for contradiction that our evaluation is bigger than this integer k, then of course my ideal equation was zero mod that fine power. 0 mod that 5 power. And because the ramification index and the inertial degree of this prime is 1, then I can show that there's some integer that is congruent to theta mod this prime power. So of course, if I plug in theta into here, this is still true. But now these are all integers. So because they're all integers, this equation is true modulo the rational prime exponent. Okay, so recalling our ideal or equation in k looks like this. So now if I just subtract away this thing, which is congruent to zero, I get this difference modulo, all of the primes above p to an explanate k. Okay, now if I divide out the prime in s, then I'm left with this congruence. This convergence. Okay, so from here, I'm going to define this map theta, sorry, this map phi, which basically just takes each vector and it raises it to the product of these exponents, the product of the delta i raised to the xi. And then if I have a solution vector b, then under this map, b has to be a z. Then under this map, v has to equal theta 0 minus theta over tau. So this is just because the y goes away in this image. And then I just divide over the tau, basically. So in particular, this quotient has to live in the image of this map. And if it doesn't, we get a contradiction on our initial assumption that our valuation was bigger than k. If, however, it does live in that image, then there has to be some vector that maps to it. So I'll call that vector. to it. So I'll call that vector w. And in particular, if I let L be the kernel of this map, then I know that my solution vector has to live in this translated, if I let L be the lattice, I let the columns of L be the, be defined, yeah, define a lattice, then my solution vector has to live in this translated lattice. Okay, and recall, we know the length, or rather, we know a bound on the L2 norm of our solution vector. And so, because I also know that it has to live in this lattice, Because I also know that it has to live in this lattice, then all I have to do is search for vectors in that lattice whose length is less than this bound. And if there's no such vectors, then we have a contradiction. So I can conclude that actually my valuation has to be smaller than or equal to k minus 1. And I do this for every prime. And when I do it, after I finish it for each prime, I'm able to update also the bounds on the infinite places, and then I can get an update. places and then I can get an update on the bound on the L2 norm and the L infinity norm. So here it is as an example on our degree 11 equation. So this is the bound we started off with, this massive thing, and here we are for each prime ideal reducing the valuation on this ideal equation. So you can see right away from order of 10 to the 222, we're getting quite small numbers. And if I do this again, I get a new bound on the L infinity norm, and I get a new normal normal norm L infinity norm, and I get again new bounds on the phi at the valuation at the prime power. And I keep doing this until basically I don't get any improvement. So here's where we stop. So then we can conclude that the L infinity norm is bounded by 179, L2 norm is then bounded by 567. But at this point, if you're looking at this, you're like, okay, well, these are pretty small numbers for the exponents on the I. Pretty small numbers for the exponents on the ideal equation. Surely you could just brute force your way through here. But if you go through every possible combination of these exponents, you are then dealing with a bunch of two-way equations that you still have to solve. So you're kind of in the same boat. So it doesn't really save you anything. And if you are just going to use this 179 to iterate through all the possible combinations here, well, keep in mind the BI are integers, so your search space is. Your search space is 2 times 179 to the 10 in this case, which I think is like 10 to the 25 is your search space. It's pretty big. And you're also dealing with an equation in a number field, which isn't going to be, it's not going to be a fast search. So you don't want to do that. So what you want to do is you want to search below this bound using something a little bit more clever. And for that, we've got a sieve, which is, there is a sieve as a. Is there is a sieve as well that's a maximum data algorithm, but it's the idea is that their sieve is, well rather, our sieve is like theirs, but on steroids, really. So again, if we start with some bound and we pick some prime whose support is co-prime to all of our tau and delta i, then defining this map, which is very Defining this map, which is very similar to the previous map for the deduction, but now with primes outside of s. Then I can say that a solution vector has to live in this set RQ. So RQ is defined the following way, which is just a subset of this guy over here. And the reason that our solution vector has to live in RQ is, well, if Q divides Y, X and Y are co-prime, so Q can't divide X, then Then the image of our solution vector is going to be a0 over tau, so it's over there. And if q doesn't divide y, then we end up in here where x over y is now some integer between 0 and q minus 1. Okay, so our solution vector has to live in RQ. And RQ is pretty easy to define, pretty easy to calculate. Its size is... Pretty easy to calculate. Its size is roughly about a cube. And if I look at the kernel of this map, then I take a set of pre-images under the image of theta q of the elements of RQ. So in other words, for each element here, I look for something that maps to it. Then I know that my solution vector has to live in WQ plus LQ. Q. So if I do this for several primes and I intersect it, then in particular I know that my solution vector has to live in, again, taking this as a lattice, another translated lattice, but this is rather an intersection now of translated lattices. So basically, I have a bunch of lattices that I know the solution vector lives in, and I do it for a whole bunch of primes. So each time I do it, I intersect these lattices more and more, so my search space gets. So, my search space gets smaller and smaller and smaller. And because the search case gets smaller each time, it makes it really easy to actually find B, our solution vector. And we find B using something like thinkypost, and from there, we're basically done. Okay. Well, we can do something I mean the the this of course interpretatives has since cube plus one or something. Q plus one or something. And then so if you do these intersections, I mean, you get you have to multiply the size of these things unless you are careful with what juice you take. Similar to the more evasive, I guess. So are you, I mean, yeah, I mean, I just, yeah, I mean, just to acknowledge the great people in the room. I mean, this is the GM equivalent of the Mobile Vases. Yeah, you know, so it's making. Making careful choices of the bias to get contradictions. You want to have interactions between the multiplicative groups and then, yeah, exactly. Okay. But in this case, it's easier to figure out which points work well together than there's heuristics that one has to make. Okay, so as a bunch of other examples, so we're gonna do the degree 11 one that we've been Degree 11 one that we've been sort of talking about this entire time. And I'm going to do something controversial here, which is I'm going to assume GRH, but only because I don't want to wait the three hours it's going to take for it to otherwise complete the classroom. So we're just going to assume GRH for this example here. And I'm going to do, I don't know, some other degree five thing. I think it takes roughly the same amount of time. And I'm going to assume gerage for that one. Amount of time, and I'm going to assume Girage for that one as well. And my obvious one too, I don't know. Okay. So while these things work away, you can see they're going through, they're iterating, they're going through each ideal equation. So while that's happening, let's just have a quick announcement, actually. So I'm actually about to launch this code to compute elliptic curves over Q of conductor up to 10 to the 6th. And for that, I need a lot of cores, and I need a lot of cores with storage and magma. And I mean a lot of cores with storage and magma. So if you have cores available with magma on them, ideally with version 2.26, and you would like to donate them for a few months, then I would greatly appreciate it. You could probably email me about that at adelegerga at warc.ac.uk. And hopefully, within a few months, we'll be able to add all of our data to the LMFDB. So I know that John Cromo has already sort of So I know that John Common is already sort of set to have this data added to the database. Okay, so with that said, let me just return back to our examples. Okay, so this one finished. It took 35 seconds. This one was... Okay, so this is a degree 4 to a molar form, so 35 seconds, pretty good. This is our degree 11. This is already eleven. I know this is going to take three minutes, and I think that this one is also going to take three minutes. So perhaps I'll say thank you here, and then while we answer questions, if there are any, we could just watch this finish. Questions for the speaker? Questions for the speaker? Do you have an estimate how much CPU time you need for computing all the electric curls of conductor per building? On one core or how many cores? Yeah, of course in 2004. So I think right now I have I have 48 cores available to me and I think that would take about six months. About six months. Could I just ask, like, for you to clarify, like, at the Sith part, you have to choose these primes so that the thing actually shrinks. So how do you prove that you can guarantee that it shrinks? Or is it just, you don't prove it, you just, you know, in practice always shrinks? Yeah, so I think so. Here we know the size of RQ and we know that the yeah, so we know the index of each lattice and so each time we intersect it Yeah, sorry. I'm I'm uh maybe ask me after I'm not at the front of the room. Sorry. Yeah, so clearly in practice this is a big improvement but it would be nice to have some theoretical Have some theoretical confirmation of that as well. So, if you'd be able to make some statement about your expected complexity, perhaps put some heuristics or conjectures in it, whatever you need. I think it would be very useful, especially if you can compare it to sort of the original method, so that you can show where where the improvement actually is. So hopefully that will be clarified when we ideally submit the paper in a few weeks. But yeah, you're right. And I should also mention that the examples I'm doing, oh, this is our degree 11, I think about three minutes. The examples I'm doing here, there are still examples that will take several hours. Like there's a few examples that take about 58 hours. I obviously didn't click those because we can't see them live. Those because we can't see them live. So there is still some stuff that happens with certain cases that it's hard to account for. Sometimes. Or I mean, just estimate what this does, assuming you have your password information. I'm hoping I'll get more information when I just launch this code for the degree three elliptic curve stuff, and then we can get that sort of better picture. Are you going to start from conductor 11 just to check that was so far 110 is correct? That actually is the idea, but John sort of sent me a test case where we could just compare a fairly smaller range of conductors. Range of conductors, but I can, in fact, start from the smallest and work my way up because I have all the forms ready for that. Yagi? Sure. I mean, so one thing it sounds like is like you do need these magma-enabled cores to run like key components of your algorithm, but there could be things like the class group computations, which are non-trivial but take a while. Is there a way that you would be able to sort of like restructure your code so that, you know, suppose you run like class group computations on You run like class group computations on Julia or Sage or whatever, compile some massive database for the things you actually need, and then somehow just feed that data into the magma relevant components of your procedure. Yeah, that's a good idea. I think that's actually what there's a student solver of Benjamin Mashka that kind of does exactly that idea, where he does the classroom stuff outside after and then just imports the information that he needs. So that's a good idea for that. I think for the degree 3 case, I don't really care. For the degree three case, I don't really care that much. Generally, the class group was not that high, but for everything else, one could definitely do that. I was actually going to comment that I think your request for generosity there is really just for the cores. If you explain to Magma Group for what kind of high profile event you're using this, I think the licensing can probably work. That's good to know. Okay. I'll I'll send an email after this. Okay, I'll send an email after this. Questions for the speaker? For the worst cases that you found for a computation, taking aside the problem of class groups, etc., do you have a, for the very bad case, an explanation? Why it happens? So there is one, I think the one that took 20 days, the reason that it took so long was because the fundamental units The fundamental units were massive. Like the number of digits in there, like it filled up my whole screen and it went on for 20 minutes just printing the fundamental units. So and always there's like things that you can't really account for. So even when the classroom is so small, there's other stuff that kind of just you don't necessarily expect. You can work with these kind of follow-up rollout simulations that might I mean you're probably moving the probably Runtimes are somewhat relative to the hardware that you run them on. So I'm curious to know, you have these quite fast runtimes, but could you say a little bit about the hardware that you have on your laptop? This is not my laptop. This is actually at work. Okay, okay. These are, I think these are the servers there. But I don't know. I think it should be roughly equivalent. I think it should be very equivalent. The code is on GitHub, so if you want to pull it in and try it yourself, then let me know. That would be great. I don't have an idea on my computer, so I can't quite tell you. All right, why don't we wrap up there and thank the speaker again? Let's talk the target starting with pickputs. 