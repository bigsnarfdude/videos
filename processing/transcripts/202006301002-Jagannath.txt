All right, so today hopefully it'll be sort of a little more relaxed and we're going to sort of completely change gears from sort of From sort of, you know, yesterday we were, you know, largely focusing. Let's just do a quick recap. So yesterday, you know, we were largely focusing on trying to at least explain at an intuitive level our understanding of. Understanding of replica symmetry breaking. You know, as we understand it now in the math literature. And we went over a very simple example of one RSP system, namely the random energy model. And then we spent a while talking about the Mazard-Parisi. while talking about the Mazard-Parisi-Virasoro picture for RSB for RSB systems. And we sort of, we ended with a bit of a discussion about, you know, how we expect this hierarchical decomposition into peer states and sort of the results we know in this direction. And sort of the results we know in this direction, such as the ultrametricity theorem of Pinchenko and sort of a pure state decomposition at large but finite n. And then sort of the last part of it, we started talking a bit about the phase diagram for spin glass models for the statics of spin glasses. And sort of one thing I sort of was mentioning that I want to sort of start to get back. Mentioning that I want to sort of start to get back to it'll sort of at least show up again was this sort of conjectured phase boundary that you know at least we know is more or less the correct phase boundary in the SK model was connected to this this thing called the Dale-Meda Taulis line where a certain Where a certain quantity, which we called the replicon eigenvalue, changed sign from being positive, sort of at high temperature, to being zero on the AT line. The Almaitalis line. And so you have this picture for the Sherrington-Kirkpatrick model. For the Sherrington-Kirkpatrick model, where you know we had this DL-made talis line, and we know now here we have replica symmetry by the work of myself and Ian Tabasco, and down here we have replica symmetry breaking due to the work of Toninelli. But then, you know, what I had said was, you know, when Ian and I were studying this problem, we thought about the generalization to P-Spin models. Generalization to P-SPIN models. And we found something very curious, which was that the Diel-Mated Taulis line analog, the zero-replicant eigenvalue curve for the P-SPIN models looked like this. So this is P is equal to 2, and this is sort of, broadly speaking, the picture for P greater than or equal to 3. But here, the x-axis is the temperature, and the y-axis is the external field. And so this is kind of funny because, you know, okay, so. Funny because you know, okay, so we still know that the DL-made to Talis curve is the correct phase boundary at sufficiently large external field. But, you know, sort of at moderate temperatures, there's sort of a question about what's going on. And what we discussed was, you know, the conjectured true picture should be. Picture should be: you know, there's some other phase boundary, and in fact, an intermediate regime of one step of replica symmetry breaking before you have full replica symmetry breaking. And so, you know, this change in the shape of the element DLMA to Talis line, you know, at the level of statics seems to be not, you know, super important, but at the level of dynamics, it has sort of really profound implications. Profound implications when it comes to trying to understand the performance of various dynamical questions and algorithmic questions. And so that's going to be sort of the focus of most of today's talk. So it'll be sort of most of the first lecture and a part of the second lecture. So the real focus for today is going to be on dynamics. Focus for today is going to be on dynamics and algorithms. And so the first lecture for today, as we said, is going to be on what I call refutation. So what do I mean by refutation? You know, we think about this from an algorithmic perspective. We want to say that some algorithm fails. To say that some algorithm fails at doing some kind of task. And so I'll be focusing on two kinds of refutation results. So the first will be about sampling. The question we want to ask here is: well, consider the Gibbs measure. For the system. Remember, it was of the form e to the minus beta h on z. And the question is to show, there's this question of can you actually produce a sample from this Gibbs distribution, right? From this Gibbs distribution, right? There's sort of, you know, there's different motivations for this, right? At low temperature, from a physics standpoint, you know, the question is: can you, you know, we have this sort of really developed equilibrium theory of spin glasses. Can you actually sort of sample from the equilibrium Gibbs measure at low temperature? If you like to think about it more from an algorithmic perspective, you could think about generic Gibbs. About you know, generic Gibbs sampling in high dimensions, I want to sample from you know a function in high dimensions, so we'll focus on the simple classes of homogeneous polynomials. And then the question would be, you know, average case for a typical polynomial, could you sample from a Gibbs measure of this type? Now, on the other side of things, there's a question of optimization. There's a question of optimization. We started this sequence of lectures with the question about what is, can we understand the optimization of homogeneous polynomials on the sphere? Or more generally, now can we understand sort of more discrete constraints? Suppose now I know my solution. Now I know my solution is going to be binary, so its entries are either plus or minus one, say, or zero and one. And so the question here we want to ask is: you know, can I produce can I efficiently can I produce A near optimizer of these sort of random homogeneous polynomials with an algorithm that's stable. And the precise notion of stability, I'll sort of at least give. I'll sort of at least give a more precise notion in just a bit. But what do I mean by this? Sort of heuristically, I'm just saying if I change the noise a little bit, right, if I change just a small fraction of the entries of the matrix, the solutions I put out shouldn't change too much. And so the question would be: can I produce a near-optimizer with a stable algorithm? And so the idea of the first part of today's talk. Today's talk is: you know, how and when does replica symmetry breaking imply refutation So, this is, you know, a somewhat complicated question. And in fact, we're going to have some really exciting lectures soon by, you know, there's one lecture by Eliran Subag on, you know, efficient. Oh, sorry, I forgot to say I want to produce near-optimizers efficiently. So, by here, what I mean is, you know, most of the algorithms that we'll hear about throughout the course will be actually order one-time algorithms. And so, efficiently, I'll give myself even a little more room, I'll say, you know, in log n time. And so, you know, and I guess on the second, we'll hear a talk by Eliro Subag on, you know, efficient algorithm for a class of full replica symmetry breaking spin glasses. Spin glasses and then later Andrea's course He'll also describe sort of another class of a fairly general class of algorithms that also perform well on a, you know, many full replica symmetry breaking models. There's also work by By, I believe Bubak Omarangel Sebastian Bubeck, Yuval Perez, and one second. And fun way on sort of an optimization problem that's very closely related to these spin glass models, specifically producing local optimizers. So these are all going to be related to replica symmetry breaking problems. And so the question is: you know, does replica symmetry breaking actually have Replicosymmetry breaking actually have interesting ramifications in terms of reputation for sampling and optimization? And it turns out that the answer is going to be yes. But the issue is going to be the sort of fundamental obstruction isn't replica symmetry breaking by itself, but sort of one other obstruction, which has to do with the positivity. Which has to do with the positivity of this replicon eigenvalue. So today, you know, I hope to explain at least sort of the picture for this and how it applies to both sampling and optimization. And I'll start with sampling just because it sort of most naturally connects to this discussion from the Connects to this discussion from the previous lectures, and then I'll sort of move towards optimization, sort of with the mind towards the future lectures that'll be coming up in this sequence of courses. So, okay, so what's the question I want to think about for sampling? So, the question for sampling is: you know, I want to sample from a given is, you know, I want a sample from a Gibbs distribution defined by a random polynomial, a random homogeneous polynomial, or more general, a random power series. So here, you know, h of x sort of in general could be a random power series, which we can think of as a linear combination of p-spin models. And there's sort of a precise sense in which the Piecepin models are sort of the most general class of random Gaussian power series that make sense in all dimensions. And that's sort of, that's due to a work of Schoenberg in the 40s. So the piece, see, the mixed piece bin models are, in some sense, the broadest class. Of rotationally invariant-centered Gaussian processes on the sphere that's defined in all dimensions. And so, the question would be then for a typical power series that sort of makes sense in high dimensions, and where by typical we mean say the entries are Gaussian, can I actually efficiently sample from this Gibbs measure? And so, well, you know, the real goal, you know, Well, the real goal, since we're interested in refutation, the goal we're gonna have is in this replica symmetry breaking phase, when you have phase coexistence is the mixing time or the spectral gap. Um exponential, or the inverse spectral gap, rather exponential in the dimension, which would say, which would lead to a sort of exponential slowdown. And so the story I'll give, you know, it'll apply equally well to both settings with essentially no change, to two settings with essentially no change. And sort of I'll leave it up to you which you want to think about. To you, which you want to think about depending on what you're comfortable with. Sort of one perspective is the discrete perspective, where we're thinking about, you know, our state space again is going to be the discrete hypercube. And on the discrete hypercube, we'll be running, we're trying to sample by using a Markov chain. And we'll acquire sort of firstly Sort of firstly that this Markov chain is nearest neighbor and it'll satisfy detailed balance on the other side of things if you like to think about If you like to think about dynamics in the continuum, we'll think about dynamics on the sphere in dimension n of radius root n and sort of the natural normalization from a probabilist perspective. And on this, we'll consider Langevin dynamics. And so, what do I mean by that? We'll consider the solution. solution dx of t, which is, you know, it solves the following equation dB beta minus beta grad H dt with some initial data. We're here by dB in By dB in grad, I mean spherical. So this is spherical Brownian motion, and this is the spherical covariant derivative. And if you're not used to thinking about spherical Brownian motion, you should just think, take full space Brownian motion and then project it onto the sphere. So, okay. And we'll be interested in, we'll be thinking about this from a spectral perspective. So, one thing that'll be So, one thing that'll be important to introduce is the infinitesimal generator for these processes. So, in the discrete case, you have, you know, it's the identity minus the transition matrix. And in the continuous setting, you have, you know, there's the Laplacian minus beta H beta grad H dot grad. Okay. So, the first question I want to think about is spectral gaps for these operators. So, what do I mean by that? You know, okay, so we'll think about the spectral gap, you know, lambda one L, which is sort of the second eigenvalue. Of this infinitesimal generator. I say second because, of course, zero is going to be an eigenvalue in these cases. And why am I interested in this? The spectral gap is a measure of mixing in these systems. So System. So firstly, it's a measure of L2 mixing. It's a measure of mixing in an L2 sense. So for example, if you sort of look at all functions that are square integrable with respect to the Gibbs distribution, and you look at how far they are from the, you know, evolved at time t under the natural semi-group from the true. Group from the true sort of sample average, and you look at the sort of squared error with respect to the Gibbs distribution. Now, this will be a mouse e to the minus lambda t. And on the other hand, in the discrete setting, you could also try to ask yourself questions about total variation mixing. And a thing to keep in mind, at least. And the thing to keep in mind, at least when we're interested in exponential slowdown, is that on the exponential scale, because you're on the discrete hypercube and the Hamiltonian is at least somewhat well behaved, the mixing time is essentially on the exponential scale going to be equal to the spectral gap up to lower order corrections. So, the first sort of part of today's talk is: I want to explain a bit the following result about the spectral gap of these systems. So, what I'll do is I'll define the following quantities. So, there's going to be first, there's beta RSB, right, which is which we came across before. We came across before, which is, you know, the smallest temperature, or the smallest inverse temperature, rather, at which the support of the Gibbs measure, the support of the Parisi measure, which was again, so this here is the Parisi measure. Which minimize the Parisi formula. And remember, replicosymmetry breaking was defined as when the support has cardinality at least two. And so I'll define beta RSB to just be the first temperature when this happens. And then another thing I'll do is I'll define another temperature, beta D, which is You know, the smallest temperature, smallest inverse temperature at which the limiting spectral gap is exponentially, is exponentially small. The result I want to sort of at least explain the At least explain the key ideas of is the following result, which is joint with Gerard Benerus. And also building off of work of joint work with Reza. Is the following, which says that for all mixed P SPIN models, well, let me just state it for the P spin models for now, and we can talk about the mixed P SPIN models in a second, but for all P greater than or equal to 3. Beta D is strictly less than beta s, beta RSP. Which is to say that, for example, if you're in the replica symmetry breaking regime, you have exponentially slow mixing. And in fact, the onset of slow mixing happens a little bit before the static replica symmetry breaking. Replicosymmetry breaking sort of picture. And sort of the mechanism for this, I'll try to explain is connected to what are called free energy barriers. So, okay, so let's think about, you know, how would we show a theorem like this? You know, what's. You know, what's different from sort of more classical systems like the Curie-Weiss model? One of the first things you can think about is, well, okay, the Hamiltonian doesn't admit a simplification in terms of sort of a natural 1D observable that you can track. So there isn't sort of an effective 1D dynamics a priori that you can think of. A priori that you can think about. You know, sort of a bigger issue, though, is if you want to try to understand where this transition is going to lie with respect to replicosymmetry breaking, you realize, well, wait a second. This beta RSB, this glass transition, at least from the perspective of statics, was defined. Was defined in terms of a quantity that depended on two independent copies of the system. So somehow I need to probe a question. I have to sort of ask a question about slowdown of the dynamics running in the environment, but somehow it needs to be asking related to a question about two. Related to a question about two identical copies of this environment. And so the idea is going to be: what we're going to do is we're going to think about sort of a key obstruction that happens in these problems, which is called free energy barriers. The idea of a free energy barrier, from a learning perspective, you can think about it. I have some observable. Or you can say you can think about it as some sort of concept that my dynamics is trying to learn. And so the idea of a free energy barrier is that there's going to be sort of entropic, there's going to be an entropic entropic obstacle to learning F and sort of the key point is that you know you know in some systems like we'll see here the key the key sort of concept that's going to be hard to learn isn't going to be a concept that you know it's not going to be That it's not going to be sort of a query that you make of a single instance of the dynamics, but instead it's the kind of query you make of many independently evolving sort of non-interacting copies of the dynamics. So it's as if you have many different sort of learners walking around, not talking to each other, and then you query something of their group. And what we'll find is that sort of these group questions can also. Questions can also have entropic obstacles, and these entropic obstacles will lead to sort of bottlenecks for the dynamics of each individual learner. So, in the interest of time, what I'll do is sort of talk about the specific obstacle that will appear in these problems, but perhaps now is a good time to take our first break since it's been about half an hour. Since it's been about half an hour. So please feel free to ask questions through our cushion in the chat. Seems like he's been pretty clear. Any questions? Oh, there you go. It's coming out. So you have a question about uh entropic obstacles, how that's uh if it's different from the notion of uh entropic barrier, and then a question what double mixes. And then a question about tomics is so let you answer those. So it's still a different notion from the notion of entropic barrier. So I'm not quite sure what you mean by entropic barrier. So the answer could be yes or no, depending on your definition of entropic barrier. I would say probably no. And I'll be extremely clear what I mean in just a sec. Extremely clear what I mean in just a second. I think that'll clarify your question. And sort of the question on tau mix: it's the mixing time. That's sort of the what you do is you look over all possible starts, the distance between sort of the evolution of the distribution of that start from the true Gibbs measure and the total variation distance. In the total variation distance, and you want to minimize that over all possible starts or maximize that over all possible starts. And then the mixing time is sort of the first time at which that distance is sufficiently small. It's like a little less than a half. And if you want a precise definition, you know, please just take a look at one of the books that was the prerequisite for these lectures, the book by Perez, Levin, and Wilmer on Markov chains and mixing times, which is available for free. Chains mixing times, which is available for free online. But at least for the purpose of these talks, I would just think about the spectral gap for now. That was kind of the point I was getting at. Any other questions? Looks like you're good to go. Okay, so okay, so getting back to this question: what do we mean by an entropic obstacle/slash barrier in this case? We'll be interested in free is what we'll call free energy barriers. What we'll call free energy barriers. So, what we'll do is, right, so remember the sort of key order parameter in these systems was this overlap distribution, right? Which was, I'll write down the sort of random version of it, right? It was the, I take two independent draws. I take two independent draws from my Gibbs measure. I look at their normalized inner product. All right, so again, remember R12 is just the inner product between these two copies. All right, so there's this overlap distribution. Which is connected to the sort of Parisi measure, but you know, it's not exactly that in the case of, say, P-Spin models. And the point is going to be that, you know, is the following. So our starting point is the following theorem that says the following. You know, this sequence of You know, this sequence of random measures has a large deviations principle with rate n and good rate function. Grade function, which I'll call i for now. So this is sort of so the good rate function i is in terms of what's called the two replica potential. Which was first sort of introduced by Corchan, Parisi, and Virasoro as they were sort of investigating the dynamical phase transitions in these systems. And the explicit formula has been proved in the discrete hypercube setting for this deviation rate function. It's a consequence of a more general result. Of a more general result of Penchenko, his free energy for vector spin models in the case of the discrete hypercubes, and Justin Coe, more recently in the spherical setting. And its explicit formula, you know, it's a bit complicated. I'll just say in a couple of words, if you take this formula and add to it twice the free energy. Twice the free energy of the system, which was given in terms of the Parisi formula, which is a 1D variational problem over the space of probability measures, or is a variational problem with the space of 1D probability measures. This itself will be given in terms of the variational problem that's a little more funky. It's you minimize over three parameters, there's going to be a measure sitting around. Going to be a measure sitting around. There's going to be a Lagrange multiplier term, and then there's going to be a Q of T, which is a non-decreasing path of two by two positive semi-definite matrices. That start at the zero matrix and end at the matrix which is one on the diagonals and q on the off-diagonals, where q is the point at which we're evaluating the wave function. And then you have the sort of complicated two-replica potential functional. And so again, it's going to be related to sort of now a two-dimensional Hagmilton-Jacob E. Bellman equation. Jacob E. Bellman equation, where now not only the coefficient of the nonlinearity is going to be a variable, but now the ellipticity of the PDE will also become a variable. But sort of rather getting sort of a bit lost in the hardcore technicalities, let's get back to this question of free energy barriers. So we'll say the following. We'll say that there is a free energy barrier for the overlap distribution of height. Of height age if when you look at the maximum over three points A less than B less than C of the rate function at B minus the minimum of the rate function at A. The minimum of the rate function at a and c this should be bigger than or equal to h, which is bigger than zero. So, this is sort of a an n tends to infinity notion if you want, sort of a finite n Version of this whole story of free energy barriers for general functions, you can see our paper and how you know it's free energy barriers at finite n and how they connect to spectral gaps inequalities. But let's just try to understand in a picture what I'm saying. And you've already seen this picture if you were following the course of Den Hollander earlier on metastability. On metastability. The idea is if you plot the large deviation rate function, you'll have, say, some zero, and there'll be this other secondary minimum, which may or may not be a zero. So there's this point A here. There's this point C here. And then there's this point B here. And the height, this area here, this. Area here, this height here is going to be the height h, and the free energy barrier is exactly this: right, the obstacle is going to be going, you know, from this metastable state toward the equilibrium value, or you know, in the case of RSB, C will be also a zero of the rate function, then it'll be a question about sort of going from one equilibrium state to another equilibrium state. Okay. And so, what you can show that you know, there's the following theorem that on the exponential scale, if you look at the spectral gap, it's bounded above. It's bounded above by minus the height of this free energy barrier. Okay. So, yeah, there was this question about what A, B, and C are. So, again, what I'm plotting here is this large deviations rate. In here is this large deviations rate function, right? So i of x you should think about as basically being one on n log the probability the Gibbs probability that the two states the two draws have overlap that is basically x And then A, B, and C are the different values that the overlap is saying. And so there's a free energy barrier if basically you have two values A and B that are significantly more likely than another, A and C rather, that are significantly more likely on the exponential scale than some other value B with respect to the Gibbs distribution. And so, just sort of a quick word on how you would prove an inequality like this. So, just the idea of the proof is as follows. So, we said there's this issue, which is that the overlap, which is the quantity. Overlap, which is the quantity that's sort of defining what it means to be in the low temperature phase. This order parameter was sort of funky in the sense that it was an observable of two copies of the system instead of just one, right? I'm not just, it's not like I can take a single instance of the spin glass and probe a question like what's its magnetization. I have to take two independent I have to take two independently prepared spin glasses and ask how similar they are in terms of spin alignments. And so then the idea was: what we're going to do is we're going to look at what we call replicated dynamics, which is, you know, or you could also just call it product dynamics. And the idea of replicated dynamics is what I'm going to do. So consider a larger dynamics, which is just two copies of the original dynamics not interacting with each other at all. They're two independent copies of the original dynamics, but in the same Environment. So it's the same H, but two different copies of the Markov chain. And so then the idea is, well, okay, you can look at the generator for this object and a simple sort of tensorization argument, you know, just by definition, actually, it's going to be, you know, the tensor product of the generator, the first one with the product of the generator of the first one with the identity and then the identity times the times the generator of the first one and so by sort of an elementary tensorization argument what you find is at least the spectral gap of the generator for the replicated dynamics is the spectral gap for the original dynamics so if i want to bound the spectral gap for Spectral gap for the underlying dynamics. If what I can do is I can lift to sort of two or maybe k copies of the system and study the spectral gap of that sort of higher order dynamics. So, this is at least the idea of the proof of how a free energy barrier can. Of how a free energy barrier connects to slow mixing. And so then there's the question of how you even show that there is a free energy barrier. And so what I'll describe here is at least some sufficient conditions. For the existence of a free energy barrier. So, the starting point is what I want to do is I want to first remind you: what did it mean to be replica symmetry breaking, right? The definition of replica symmetry breaking, sort of the working one, was that the minimizer of the Parisi formula, which we called the Parisi measure. Has at least two points in its support. But one thing to watch out for is that in general, it's not true that the limiting overlap distribution. So I know yesterday I said you should think about the limiting overlap distribution as being basically the Parisi measure. But when I said basically, you know, you should really take that with a bit of a grain of a grain. Really, take that with a bit of a grain of salt. You know, actually, making that correspondence exact is a little complicated. But, nevertheless, what you can show is the following. The first thing you can show is that the support of this Parisi measure is contained in the zero set. In the zero set of the large deviation rate function. And the second thing is, actually, no, before I get to the second thing, let's just say, okay, so what does this tell us? So the first thing would tell us that if you have RSB, then when I'm looking at the large deviation rate function, I know that there's There's going to be at least two zeros. And in fact, you know, the prediction is that there's going to be a zero, and then there's going to be some interval of zeros. But this by itself isn't going to guarantee a free energy barrier. So you need one more thing. And so the next thing is to observe that if you have that quantity, the replicon eigenvalue, well, it turns out the replicon eigenvalue, you can define. Eigenvalue you can define for any point in the support of the Preezy measure. And if the replicant eigenvalue is positive for some point in the support, say some Q naught, then what this implies is a locally quadratic lower bound for the large deviation rate function. Right, so what this would tell us, this sort of second condition here, I'll call this PREV. This is the condition I mentioned earlier. So, RSB tells you. So, RSB tells you that you have these zeros, and then PREV is going to tell you that around one of these zeros, you have a locally quadratic lower bound. And so in particular, you know that the large deviation unit wave function, okay, it's going to do whatever it does, but it's going to lie above this quadratic envelope, hit zero, and then go back up again. And so, in particular, there must be a free energy barrier. A free energy barrier somewhere here. Right here, you would get a positive lower bound on the spectral gap inequality from this height here on the spectral gap. So just a quick corollary of this is that if replica symmetry breaking holds and this one of the replicon eigenvalues This one of the replicon eigenvalues is positive, then there is a free energy barrier for the overlap and Let me just sort of, before we move on to the next part, let me just sort of give some examples of, like we said earlier, the P-SPIN models are models which satisfy both of these conditions. More generally, if you have these mixed P-Spin models, right, if I have a more general model, then sort of if Then, sort of, if the temperature inverse temperature multiplied by the coefficient of the quadratic term is sufficiently small, say zero, then you also get PRAV. So, if you have these polynomials and there's no second-order term, or the second-order term is really small, you'll also have these conditions. All right. I guess it's getting close to the end for the first lecture, so maybe I'll stop here. Okay, great. So maybe so we'll stop the recording and live streaming on YouTube in just a second. So can Sarai, can you unmute everyone? So can Sarai, can you unmute everybody and we'll give a round of applause for this lecture for Okash? You should be able to unmute yourselves now. We can stop the recording and then you can feel free to ask questions either in the chat or with the mic directly. Mic directly, and for those who want to get something to eat, we'll reconvene in 35 minutes for the last like