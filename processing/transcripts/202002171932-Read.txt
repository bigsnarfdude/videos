Thank you, Andreas, and Michael and Elijah for the invitation. It's really nice to be here and see some old friends, and a good opportunity to meet people that I've admired from afar. So thank you. And I changed my title a little bit from the one that I submitted, which was a bit aspirational, I think. And so I'll be talking today about computational approaches to stochastic gene regulatory dynamics. And I'm going to be talking about this from a I'm going to be talking about this from what I'll call top-down and bottom-up perspective. And by that, I just mean top-down being a bit more data-driven, data-constrained, bottom-up being models that are certainly informed by data, but where there's maybe more freedom to do the theory, to explore the consequences of hypotheses. And what we would really like to do is try to have these meet in the middle. Have these meet in the middle. Okay, so the models that I'm going to be talking about tonight are discrete biochemical kinetic models of stochastic gene networks. These were already beautifully introduced this morning by Jay Dewby. And so this is just a cartoon of the schematics for the types of models that I'll be discussing, that we've studied. And so the idea here is that this is starting with just two genes. The network diagram here is supposed to indicate that these two genes can interact. That these two genes can interact either positively or negatively. They also have the possibility of self-activation or self-repression, or maybe they don't interact with the dashed line. The way that we're actually encoding those interactions is here in this stochastic model. And so this is showing two genes with regulatory elements. We're considering four possible binding states just because we're considering the possibility that on the gene that encodes for Y, that transfer. Codes for Y, that transcription factor can bind itself, or the other transcription factor X can bind, or neither or both. In this cartoon, it's showing homodimers. That's just a very general way of trying to incorporate cooperativity into these types of models. Depending on the model, we take into account fluctuations both in mRNA, in transcription and translation. And then degradation, which is not shown here actually. There's not, should be a degradation reaction on the mRNA. There's not a, should be a degradation reaction on the mRNA as well. And then the way that the different regulatory logics are encoded is in the impact of these different binding states on the gene transcription rates. So the rates, the birth rates for mRNA copies. And so for example, down here, if we want to encode a so-called mutual inhibition self-activation, which is a common network in development, then this is one way, it's not the only way, but this is one way to encode that, making use of only two different Making use of only two different sorts of activity levels or transcription rates. So the idea is that this is symmetric. So for the binding states, these four possibilities on gene X and the four possibilities on gene Y. If you have, when nobody's bound, it's low. When it self is bound, it's high. And then the other one, it's low. And then when they're both bound, if we call that low, for example, that's one way to encode this mutual inhibition self-activation network. And of course, these types of model systems, there's, you know, Model systems. There's many, many examples of previous literature on this. This is just a few examples of some of the papers that we've been inspired by in studying these models. Okay, and so then this is trying to now extend. Yes, please. Briefly, these arrows mean different things here. What is the dashed one here? I see in the top right corner is you trying to. What is, I'm sorry, what is which one? Yeah, so I understand the division, but what is the dashed one? Oh, dashed is just if there's. What is the dashboard? Oh, dash is just if there's no interaction. Oh, yeah, yeah. So later we'll be looking at classes of models where we try to explore all those possibilities. Yeah. And then I should have mentioned also these types of models incorporate kinetic rates for binding and unbinding. That's shown here with these H for on rates and F for off rates as well. Okay, and so then if we extend that same type of model framework to a larger network, this is an example from the literature. This is an example from the literature of a network that is thought to be controlling pluripotency in mouse embryonic stem cells. So these people have done the hard work of combing the literature and trying to synthesize information from many different types of experiments to put together these types of network models. And if we build a model like this in the same kind of discrete stochastic framework, it becomes very quickly intractable in terms of its master equation. So it has a huge state space. Its master equation. So it has a huge state space. We're dealing with the cursive dimensionality here. So, for example, this is eight genes, and depending on the parameters, if we assume copy numbers for the proteins, in this case, the transcription factor is on the order of thousands per cell, then of course we have then an absurdly large possible state space. We also have many different potential gene binding states, which are not really sort of fully shown here, but for the different combinations of regulators bound to a regulatory region for a gene. Regulatory region for a gene. So it's an impossibly huge state space. Of course, in reality, the dynamics are going to inhabit a very small sliver of this huge state space, but we don't know a priori what that is, what that looks like. So for us, anyway, it's impossible to directly solve a master equation. It's also potentially inefficient to simulate this. So in principle, it would be straightforward to simulate this kind of network using Monte Carlo simulation. Carlos simulation, stochastic simulation algorithm, or in this field often known as the Gillespie algorithm. One thing that challenges that is the possibility of rare events, which was also discussed this morning by Jay. And so because of the kinds of feedback that we have in these networks, we have the distinct possibility that there's very, very long switching times between metastable states. And so I want to summarize in really one slide some work that we did and trying to look at what are sort of numerical tools we could use simulation. Sort of numerical tools we could use, simulation tools we could use to address these kinds of questions. And so the idea is that if we take as an input a transcriptional network model like this one, we can approach that with stochastic simulation using rare event sampling methods. We have been primarily using a class of methods called weighted ensemble sampling. And I'm skipping over many algorithmic details in terms of how do you bin the state space, and I'm happy to discuss that. Happy to discuss that later. But at the end of the day, what we did then was build, based on sampling trajectories from stochastic simulations with these rare event methods, build a network graph here. So this network represents the colored circles each represent some sort of metastable cell state, or just a cell state where that state is representing different copy number levels for the different genes in the system. In the system. And each of these is connected by an edge that represents its transition probability that we then quantify. Sorry, how do you know all the kinetic rate parameters of these circuits? In the model itself? Or you mean the kinetic rate parameters in the model or the kinetic rate parameters of the transitions here? No, no, no, the model. Of the model. We don't really. Yeah, so that's one big challenge in this field is that you could argue that maybe the networks You could argue that maybe the network topology here is fairly well known, or at least reasonably well known. The kinetic rates are really difficult to know. They represent individual reactions. Elementary reactions in this kind of system actually probably represent many, many complex processes. So yeah, so that's one of the challenges, I think, with this bottom-up modeling. That being said, I think there's been some really nice theory that's been done on sort of what these, you know, how those rates, what they may be, how that corresponds to stability of cell states. But really, at this point, But really, at this point, kinetic rates, we don't have direct information about those. But then, what did you do to simulate it? You just picked some things that you thought were recently? Yeah, I mean, we used, we were, we, again, we sort of pulled this as a model system from previous literature, but we use rates that have been estimated to be sort of in line with experimental observations. So, you know, maybe order of magnitude we can try to estimate. But really, this is really a vast simplification, and I think we really don't know in detail. Don't know in detail what these rates are. Yes? So I was just wondering whether you tried to use some sort of ensemble approach to not only to the number of variables or the system size space, but also to a metric space. Later in the talk, we'll do some stuff. We did not do that with this particular system, but we're doing that in some more recent work that I'll talk about. Yeah. So, yeah, and then So, yeah, and then once we do the sampling, we also applied something called a Markov state model analysis, which has been popular in the protein folding literature. This is a way to define what are the metastable states in the system. I think it's a very pragmatic. It's purely time scale separation based. We know about the time scales in the system directly from the transition matrix that's encoded here in this network. And so, if you have good separation of time scales in your If you have good separation of time scales in your system, it may be a reasonable approximation to express your dynamics in this very coarse-grained way with a small number of states with Markovian transitions between them. And so, and in that picture, we can also look at things like dominant pathways for transitions, for example, in differentiation and de-differentiation. And so, with these methods, the idea is that if you give me a network model with parameters and so forth, we can And so forth, we can output this. And so we think that there's sort of nice simulation tools available now to really produce this kind of intuitive and kind of human-readable picture of what the dynamics of this complex system are and define the metastable states of the system. That being said, as people already alluded to, there's really no closed loop here with data. And so it's very unclear how to sort of directly incorporate Of directly incorporate experimental data into this approach, although it is now possible to measure time scales for these transitions, for example, with single-cell microscopy techniques. And so that's what we wanted to do after this, was to try to take, again, the sort of more top-down approach and try to approach this from the data side a little more directly. And that's, I'm going to switch gears now and talk about that. And so our approach to this has been through single-cell transcriptomics data, single-cell RNA-seq data. Cryptomics data, single-cell RNA-seq data. And so I'm showing here just some representative picture of what the data looks like. This is from a frog, Xenopostropicalis. This was published a couple years ago by Long Klein and Mark Kirschner Labs. And so this is a hugely high-dimensional gene expression state. So this is look space. Excuse me. So this is looking at tens of thousands of genes and measuring transcript levels simultaneously in tens of thousands of cells. Thousands of cells. So there's been a lot of excitement about data science approaches to this kind of incredibly rich data. We're taking a different kind of approach to this data. And over here, I'm showing a low-dimensional gene expression space. So there's no dimensionality reduction here. This is literally just looking at the bivariate distribution of two genes. This is SOX2 and T. brachiuri, which are both important fate regulators. And this is just, if you go digging into this data, Digging into this data for any two genes you might be interested in, you can output these sorts of distributions. And this is just saying here the number of mRNAs for SOX2 and the number of mRNAs for this gene T. You can see that for these two, they look pretty anti-correlated. The number up here indicates the stage during development. And so we're going to be looking at this kind of low-dimensional gene expression space. Space. The question that we're trying to address here is: to what extent does the noise in this distribution or the shapes in these distributions tell us things about the underlying gene-gene interactions? And so we're taking a very different approach trying to look at these low-dimensional pictures instead of these high-dimensional pictures. Something that we're trying to do, just in general, with a couple of other projects in the lab as well, is to try to integrate statistical. Integrate statistical inference and more data-driven approaches with more mechanistic or biophysical models to try to maximize the insight that we get from omics kinds of data. So another project where we're trying to do that right now is in epigenomics DNA methylation, where we're trying to combine sort of mechanistic enzyme biochemistry models with statistical inference on omics data. I won't talk about that today, but this is sort of generally the Today, but this is sort of generally the approach that we're trying to take. So, going back to this model, back now to the two-gene model, and we're going to focus on two-gene networks for two reasons. One reason is that these are, these gene pair interactions are the building blocks of more complex networks, and so this is kind of the fundamental unit, perhaps. Another more practical reason is just that it's much easier for us at this point, with the approaches I'll talk about next, to do. With the approaches I'll talk about next to deal with two genes in a time, we really have not yet addressed trying to look at three-body interactions and so forth in these networks. If we can, say, solve the master equation and get the stationary distribution, for example, from a network like this, it might look something like this. If they're mutually repressing, you can get, for example, this is showing an example that's tri-stable, appears to be tri-stable. To be tri-stable. And then we can just log transform that and look at the quasi-potential. Again, these are non-equilibrium systems. We have birth rates that are not connected to the death rates in these systems. And so we have a quasi-potential, which is basically just representing the negative log of the stationary probability. Sorry, I said another little question before we go on. How do you deal with the dropout problem with single-star in ACP? Yeah, we're not. It's a short answer. Yeah, yeah. I'll mention that a bit later, but. I'll mention that a bit later, but we're not yet. And so at this point, when we look at it in this kind of landscape picture, it starts to be reminiscent of the well-known Waddington genetic landscape and make us think about stable cell states and dynamics among those states. This is what it looks like when we solve the master equation, the stationary master equation for a small 2G network. For a small 2G network. So we can do this. The master equation is very tractable if we first of all focus just on the mRNAs and limit ourselves to these two-gene interactions. If we do that, we have state spaces that are on the order of tens of thousands or ten thousand, let's say, different states. So even these really tiny models already have pretty large state spaces. But still, it's very tractable to solve this as a linear system. And we're doing this currently with just assuming a finite state space with reflecting boundaries. If we wanted to be a little bit more rigorous about the error involved in that approximation, we could do like finite state projection types of methods that Brian Muncie has pioneered. So in this, I'm going to argue that this picture here, now this bivariate distribution, is a fairly direct analog to what we're pulling out from the data. Pulling out from the data, at least under a number of assumptions. And again, what we're plotting here, the color is on a log scale, so it's analogous to this quasi-potential landscape for this particular network. I didn't say which one it is. This is just a representative example. Yes. Are you looking at just the network in which these two nodes interact in some way? Yes. So you're ignoring that it's embedded in some larger network where all kinds of other things are also infringing on these two guys? Yes. Yeah. Right now we're looking at only edging parents. Right now we're looking only at gene-pair interactions. So it's a very different approach to what I presented earlier, where we can have a much more complex network model and simulate the whole thing, but here we're relegating ourselves to a very small piece of this and gene-pair interactions. But the benefit is that it's straightforward to solve the master equation. So are you still incorporating all the interactions as if they were sort of? As if they were sort of elementary type chemical reactions? Yes, yeah. So it's the same kind of. Because one could argue that if you allowed a little bit more phenomenological representation of each step, since it's a stationary, whatever they're doing directly on each other would be sort of captured by the model anyway. Potentially, yeah. I think we're trying to look at that as a possibility, but I don't know right now. I can't say that's true. That's true. Are you saying that in some ways maybe we can effect kind of capture effective interactions? Yeah, yeah, we'd be there to direct interaction than interactive interactions. Yes, yeah. I mean, I think that's true because we can call these elementary reactions, but we know that they're not, right? Yeah. This is simply, this is just the degree of resolution of the model, and these are the kinetics that we've encoded in the model, and they could represent many, many different things. Yeah. And so my student Cameron Galavan has been looking at, he just wanted to see kind of what was sort of all the behavior that you could get out of this class of model. And so he tried to study it quite comprehensively in the sense that, you know, for this model framework that I showed, that one has these four different binding states for the promoters. So if we look at symmetric interactions, we basically have 16 different combinations of regulatory logic. And then also varying these kinetic parameters. Also, varying these kinetic parameters. Those kinetic parameters can represent different things. So, for example, if we make them fast, the idea is that that's maybe more representing simple transcription factor binding and unbinding, which might be a fast reaction. Whereas if we make them very slow, that may somehow capture phenomenologically some much more complex and slow-scale event like chromatin remodeling in eukaryotes. And so, these are the things that he's looking at. Looking at and looking at on the order of tens, now kind of 100,000 different model variants, changing those different things. And this is just a representative pictures of the types of different shapes that you can get in these landscapes. So this is again plotting these, I'm calling them co-expression landscapes or quasi-potential landscapes for these bivariate distributions. And then the next thing that he did was he tried to delineate the shape space, calling the quote-unquote shape space here, using just straightforward. Shape space here using just straightforward PCA. So this is taking all of these, I think here, 40,000 models, and plotting them in a PCA space. So here, each one of these dots represents one of these model variants with a particular set of kinetic parameters and a particular logic encoded. And then the color of the dot is just colored by the Shannon entropy for that particular landscape for the two genes. And then each model then is now projected. And then each model then is now projected into this PCA space of the first two components. And you can kind of see, perhaps, if you follow over here, you can start to see what kinds of features they're pulling out. So if I'll start down here in the bottom left corner, these are very high entropy, High-Shannon entropy landscapes. That C over here, so this is high coexpression. If we go up here to this vertex of the triangle, these ones are the ones that have very anti-correlated, sort of bistable type of landscapes. And then down here in this. Landscapes. And then down here in this corner, E, that's the one that's nothing interesting is happening, they're just off. And then in between, we have combinations of those shapes. And so this picture captures a large degree of the variance. I can't remember exactly, around like 96% or something like that. Yes? Could you project the master equation onto these as basis functions for a reduced master equation? I don't know. I have to think about that. No, I had to think about that. Could we project the master equation? Right, so you could project a linear system onto a lower-dimensional linear system. This could be a basis of that particular projection. Oh, yeah, yeah, maybe. Six-dimensional OD. Yeah, that's kind of interesting because actually, maybe, maybe, I think I haven't really thought about that directly, but I will say that when we look at the eigenvectors that we get from PCA, they look, in some ways, maybe you could say that they look related. In some ways, maybe you could say that they look related to the eigenvectors that we got way back here when we were doing okay, I went over it, but the sort of transition matrix Markov state analysis. So, yeah, so I think there is kind of a relationship that could be explored there. Although, I haven't thought about it. You just need the vertices. It's almost like a colored gamut. What's that? You just need the vertices, like A, E, and C. Yeah, I mean. Yeah, I mean. I'm sorry, but these are stationary states, right? Yes. Whereas the networks also have dynamics. Yes. So you're projecting away the dynamics, for sure. Yeah, here we're looking only at the stationary solutions. That's true. We could project the dynamical system onto the dynamic lower-dimensional manifold. And it could capture, potentially, it could capture all of the, it could reproduce the full transient probability distributions. Probable distributions if you project it onto the right. But finding the right manifolds is very hard to do. That's obvious to me. There could be many different states that have the same stationary state but different dynamics, huh? Well, it depends on the. You won't capture all time scales. Yeah. I think you went forth, but no, this is cool. Is there any intuition about what these principal components represent? Yes, I'll show some pictures later. But this one, I guess going this direction, is basically whether it's, the x-axis is basically whether it's off or on, essentially whether there's gene expression or no gene expression. And the vertical one here is the difference between something that's very anti-correlated and something that's very co-expressed, roughly speaking. So, yeah. So, yeah. These networks have, do you have fixed topology and just varying sampling in the parameter space? Or do you incorporate? We're changing the topology in the sense that these can be activating or repressing or not interacting. Yeah. But we're doing that in this very constrained way. We just have right now 16 different logic combinations here that represent the network topology. And that's overlaid with variation of parallel. And changing, yeah, yeah. Here, this is a Yeah, yeah. Here, this is a parameter suite. More recently, we're just kind of doing some more random sampling of parameter space to get pictures that are a little more filled in, but basically, the story's not. And how do you sample? What is your sampling algorithm? Oh, well, this one is actually just a parameter sweep. Yeah. Yeah. We're just trying. My student was looking at an ivor cube or something like that? No, I mean, this is a relatively limited. No, I mean this is a relatively limited parameter space at this point because we're making a lot of constraints on the symmetry and things like that. And so we're just trying to kind of sort of do like through log space of these parameters and with, we think, reasonable biological parameter ranges, but that's it. Yeah, of course. Yeah, so no idea. Yeah, so we're not doing any kind of parameter sampling here. Yeah. Okay, so. Okay, so let me talk about some of the model assumptions. Many of these have already come up, but we are right now only looking at at least direct gene-pair interactions. We're assuming that each gene has very few gene expression states. So either just low and high, like the examples that I showed, or maybe three, like low expression, like off basically, or basal and high, but a very small few expressions down. Right now, for this data that I'm showing tonight, we're assuming symmetry. We're assuming symmetry in the networks, but we do have some asymmetry in the kinetic parameters. We're only assuming intrinsic noise at this point. And then we're also right now assuming that the protein is linearly proportional to mRNA. So we're ignoring all fluctuations in translation and protein degradation. That, you know, that's probably a pretty bad assumption in some ways, although looking at this picture, depending on your perspective, maybe it's okay or not. Depending on your perspective, maybe it's okay or not. But this is looking at quantification in mammalian cells of mRNA copies versus protein copies. So, sure, they're correlated, but this is not a great assumption, but we're doing it for practical reasons right now. On the data side, which I'll show a little bit later when we start to apply this to data, we're also neglecting technical noise in the single-cell RNA sequencing data. And the reason for that at this point is just because. One, I'm new in this area. Second, I'm not sure that I trust some of the sort of imputation methods that people are using right now because I don't think that many of them, at least to my knowledge right now, are informed by things like stochastic gene expression. So right now we're sort of treating the data as pretty much raw and hoping that over the years as the data gets better and better that it will become more justifiable. So we're taking the data as is. Okay, and so in these pictures, this is trying to overlay different types of network motifs onto this PCA shape space. And so let me know if this is hard to follow along, because I know it's hard to have all these pictures in mind at the same time. But if you remember the triangle with those vertices, this is showing network logics that all sort of share the character of being in some way mutually activating. And it's not too surprising, those ones show up in the bottom. Those ones show up in the bottom of the triangle, which means that these are the ones where we have, you know, high and correlated expression of the two genes. On the other hand, the ones that are mutually antagonistic or mutually repressing kind of occupy the top of the triangle, although there's overlap. Up here in this corner was where we had those really bistable looking landscapes. When we don't interact them at all, the genes can be self-interacting but not interacting with each other. They're actually carving out this very sort of narrow. Carving out this very sort of narrow region of this shape space. And then we have complicated, nasty ones where they have both repressive and activating character, and they're just all over the place. We really can't see what those are. And so this is telling us that there's some ability to discriminate, at least some ability to discriminate these regulatory interactions based on their shapes. That's not too surprising, but this shows us, I think, a way to do it. But this shows us, I think, a way to do that, a way to approach that. Are the PCA vectors the same in all cases? Are the vectors defining those coordinates the same in all cases? Yes, yeah, yeah. Right now I'm still projecting these on sort of the same two dominant components that we had, which I haven't shown yet, but I have a picture of them on a later slide. Well, why do you have concentration about the boundaries of those domains? It seems that you have high density of data points in certain areas. Yeah. Is that normal? Yeah. Is that the normality section? I think that it has somewhat to do with the fact that we were a little bit like over here, these are basically just everything is off. So even though we technically have this network topology, based on the kinetic rates that we have, it's really not very strongly mutually repressive. It's basically just nothing's on. So we could have been a little bit more careful about how we set the kinetic regime so that we don't have, I mean, really we should be able to neglect some of these interactions based on their relative. Neglect some of these interactions based on their relative rates. But right now we just have sort of everything. And so it's technically all these data points have this topology, but they have it with very different kinetics. And so the rate constant may be very different. Is that the case? Yes, yeah, yeah. And so maybe this is, yeah, maybe it's effective network logic is not really this because of its rate constants. And so that's something that we are working on. And so here what we did was we And so, here, what we did was we tried to plot. So, again, this is the same triangle and the same two PCA components as before. We wanted to look at what happened when we overlaid different pairwise metrics. So, this is Shannon entropy, which we were already showing in the previous color plots. Now we've just sort of interpolated and tried to fill this in to get a sense of how the Shannon entropy depends on these two shape-space components. And then we've done the same thing over here with Pearson correlation coefficient with mutual information. And this is something called a co-expression index, which is Index, which is another measure that is used. And so, what we're finding here is that some of these kind of overlays are fairly intuitive in terms of the way that these metrics line up with the PCA space. But that being said, we do also, we can certainly find examples, let's put it that way, in the data, which I've tried to highlight here. For example, if we pull out some representative landscape. out some representative landscapes that have very similar mutual informations when we they can they can be in very different regions of the shape space and that's kind of clear why that's true right mutual information here is not discriminating between things that are either very positively correlated or very negatively correlated but either way have high mutual information but we see that as well for some of the other metrics as well and so then the idea here is you know we're wondering how much can we you know maybe there's more we can learn from just simply looking at the shapes. More we can learn from just simply looking at the shapes that's maybe not clear from these single metrics. And maybe that's not totally fair comparison because here we're looking at two PCA components and we're comparing it to these just single number metrics. But overall, it looks like some of these sort of traditional pairwise metrics for random variables seem to be relatively insensitive to the landscape shape. But when you look at the shapes of these distributions, it's sort of, you know, after looking at this stuff for a few months. After looking at this stuff for a few months, it's sort of very clear to us what kind of network motif gives rise to those shapes. Okay, so now I'm going to talk about trying to incorporate some real data into this. This is just public data set. It's the same Xenopus tropicalis frog embryonic development data set that I mentioned earlier. So, what I'm plotting down here is three representative gene pairs. There's one that's boxed here in blue, one that's Boxed here in blue, one that's boxed here in red, and one that's boxed here in yellow. And so these are the raw data for these gene pairs. And the numbers here represent the developmental stage. So this is essentially different time points during development. Now, in the top here, we have overlaid them on the model space. And I know this is actually a different model training space than the one that I showed previously. We changed things up. We are focusing here only on mutual inhibition, self-activation. Mutual inhibition self-activation network. We also changed some of the kinetic time scales before we were really comprehensively sampling, or not sampling, but sweeping kinetic parameter space. Here, we wanted to sample parameters that were reasonable with respect to the time difference between these snapshots because we're still using the stationary distributions to try to look now at evolution and time. So, we're using sort of quasi-steady state approximation. At any rate, At any rate, so what we've done here is use the model set of stochastic models to train PCA, and then we're applying those same components, that same projection to the data. And so that's shown here. In the blue is looking at, you know, over developmental time how this gene pair evolves. And then yellow, over developmental time, is how this gene pair evolves. And then the red one is the really interesting one because that's the one that. Interesting one because that's the one that, oops, I forgot to show the components, if that's helpful. That one is interesting because it shows initially some increasing co-expression of these two genes, but then later they kind of simultaneously turn off and become more antagonistically expressed. You can see that also, maybe, by eye, if you follow along. So these trajectories in the PCA space, we think are doing a good job of representing what we can see with our eyes. Of representing what we can see with our eyes in terms of the shapes and evolution of these. Here I'm plotting, now I'm finally showing what these eigenvectors are. So the first three PCA components. This is the first one is basically just showing whether it's expressing at all or not. The second one we think is really interesting because it's picking up the difference between co-expression where they're both expressed or antagonistic by stable expression. And so that's PCA component two on this y-axis. And so this is the legend here. The legend here that essentially we can describe this behavior in terms of these axes. And then this is when we just look at the dynamics on those eigenvectors. This one is clearly pulling out asymmetry, and we think it's doing a good job. So we can see here that these gene pairs, two of them are going one direction in terms of the asymmetry, and the third one is going in a different direction. And I think I can see that. Maybe you can too. Can too. So then we wanted to look a little bit more comprehensively, and this is just trying to basically cluster a list of 1,300 gene pairs. The caveat here is that we haven't really been, I'd really like to do this more comprehensively with sort of the full transcription factor space here for this animal. And we haven't really done that here. We had sort of an incomplete list. But nevertheless, with the list that we had, we took those gene pairs and We took those gene pairs and projected them in the same way on this shape space and tried to look at their trajectories in development. And what I think we're seeing here is that there are distinct kind of classes of trajectories. I'm going to zoom in on a few of them just because maybe it'll be easier to see the colors. So, for example, if I start up here, this is just a few that are clustered together and they go, and in principle, if this were larger, you might be able to. In principle, if this were larger, you might be able to follow along with kind of the relative shapes here. But these are, or here, these are turning on and being co-expressed, and then they're coming down and back around and turning off while simultaneously being more and more antagonistically expressed. This is a phenomenon known as multilineage priming. So this kind of characteristic shape to us looks a lot like what people describe as multilineage priming. Multilineage priming is the idea that in multipotent cells, you have simultaneous co-expression of two transcripts. Two transcription factors first, and then in the commitment to cell fate, those become one of them or the other turns off, and you get then antagonistic expression in different cells. And so, this is thought to be really characteristic of a cell fate decision. And I think that's the kind of behavior that we're seeing here. We saw something kind of interesting, which I don't really know what this is, but to me, it looks like anti-multilineage priming, which is the idea that these genes are kind of turning on in a more and more antagonistic way, becoming more and more bistable first, but then Becoming more and more bi-stable first, but then later they're co-expressing more. And I don't know what that is. And then we have some other ones that just kind of become more and more antagonistic, or this one has kind of a little bit of a cycle here. It's a little messy, but there's sort of a small, maybe mild multi-lineage priming. So we're trying to explore these further, but I think right now we can say that maybe this is sort of a useful and intuitive way to pull out these different kinds of co-expression trajectories during development. Development. And finally, I don't know how much time I have, but I think this is my last slide. Two minutes. Great, okay. So, this is just answering the question: so, you know, do we need the models? What if you just sort of did this on the data? What if you just ran PCA just to look at all the different kinds of pro-expression shapes that you could get? We did that too with this frog data, and we're trying to do it now with some other animals. And unfortunately, I only have the first two components. It's kind of interesting to see the other eigenvectors that are pulled out. What we do see in general. That are pulled out. What we do see in general is that from the data, we see at least to our eyes what looks like sort of reasonable similarity in some ways between some of the eigenvector components that we get from just running PCA directly on the experimental data versus doing it on our sort of model training set. The ordering flips. So here, this one, the second component was asymmetry clearly. We had one further down the line that was, I think, number three or four that was pulling out this kind of like code. That was pulling out this kind of like co-expression, antagonistic expression thing as well. And so now this is leading me to wonder if we can not try to do inference or, we're not really interested in sort of fitting parameters for one particular gene pair, but can we somehow sort of holistically fit or learn about all the different sort of classes of transcription factor, transcription factor interactions that we get at a certain developmental stage in different tissues. Stage in different tissues of an animal. So I'll finish there and summarize. This is a little wordy, but in the first part, I focused on simulation methods that one could use given a complex network model. Those methods are very general to any kind of stochastic dynamics. It's not specific to gene networks, but as a way to characterize metastable states and transitions and paths. I think there's now a lot of opportunity in single-cell data. A lot of opportunity in single-cell data, and a lot of it is now public and easily downloadable. But I think there's still more work to be done to try to incorporate what we've been learning over the years from theory of stochastic gene expression and gene networks into these kind of data analysis methods. We see that shapes of co-expression landscapes and their evolution seem to some extent to reveal gene-gene interactions. And we're interested in exploring these kind of simple shape-space metrics as a way to just pull out this kind of behavior. To just pull out this kind of behavior. And then, right now, this is something that we're actively working on, and I'm really trying to quantify in some way the information gain, if any, that we get from these shape measures, these sort of simple PCA kind of shape measures, relative to traditional things, these other kinds of metrics that I mentioned. And so, I think with that, I will just thank the graduate students who worked on this. So, the work that I presented today was primarily by Cameron and Maggie and Brian and Hung Lai. And Brian and Hung Lai, and I'd be happy to take more questions. So at the beginning, the really large metastable landscape that you estimated, are you not looking at the deterministic limit dynamics there? We haven't actually. That's all estimated from the yeah, so yeah, so I don't actually know to what extent the deterministic fixed points would overlap with those metastable states that we're seeing. So you could be picking up like just purely sort of stochastic metastable states. We talk about like crossing over like bifurcations, right? That are noise induced. So you can see bi-stability even though. Seed by stability, even though I think that's really cool. All you care about right there is the separation of time scales. Yeah, yeah. So that's why when I started, I was kind of like, well, how do we even define what is a state in these systems? And yeah, definitely there's examples in gene networks where there's, as was mentioned earlier today, also wants to have a limit on everything derived from that. But this is like more practical probably and more useful. Yeah, well, but yeah, there's definitely been work and genome work showing that there are stochastic men's states that are not deterministic fixed points, right? In your PCA patterns, you had the subcase of the disconnected genes and they down to a one-dimensional manifold in there. Is there an intuitive explanation for that? I'm a little confused by that if I think about disconnected genes. It seems like a boundary. Yeah, it did seem to be happening right along the boundary between these sort of two classes of mutually mutually activating and mutually repressing. So yeah, I think right there it also overlaps with the kind of no mutual information region as well. Yeah, yeah, so it's clear that there must be zero mutual information. Yeah. But you also have far fewer samples because you have less. Because you have less links to sample rates over, right? So you have like just the number of points was much less. So it may or may not be. I think that's true. Yeah. I don't know exactly what number because it's hard to see in these pictures, right? But that's probably true that there was fewer overall. Yeah. Yeah. So this is a nasty question, but coming back to what I asked earlier, so we know that in these systems. Like the pluripotent stem cell network in mice, that there is groups of transcription factors that are highly connected, with each of these factors having multiple inputs for multiple factors. Actually, it's very complicated with enhancers and chromatin organization, all this stuff, right? But even if we ignore that, the kind of output you expect from a circuit like that, I don't understand why you think that you could sort of learn something about that. Learn something about that from looking at outputs of isolated pairs of nodes that interact. Because we know this thing is not built out of isolated pairs, it's built out of highly connected topology. No, I mean I agree. I think it's a starting point, right? I mean, we're starting with the pairwise and trying to expand from there. Computationally, it's daunting. So, I don't quite know. I mean, we can do sort of similar types of analysis with simulations, but we can't do it as sort of comprehensively. So, I guess we're sort of hoping that maybe we can come up with a way forward on that front. What people would do, right, is they would say maybe they look at all pairwise correlations, but then try to do something like partial correlation analysis or something to see which ones do I really divide up. Yeah, I think so. Yeah, I think so. And I mean, maybe it could be true that these sort of shape measures could then be built into like a network inference algorithm that then has other interactions as well. But we haven't done that yet. So could one conceptually think that the other interactions are changing the parameters of this small module that grew up? Yeah, so I think that's related to what you mentioned earlier. So I um yeah, so I would like to explore that in a kind of a rigorous way. Explore that in a kind of a rigorous way, but yeah, we haven't done that. This might provide a framework to reduce that 10,000 by 10,000 master equation to something. Yeah. That you could understand how the outside influences affect the parameters in this lower dimensional space, which could get to connect data to everything else. Yeah, I think at the end of the day, it's possible that we already, yeah, that maybe we're still characterizing and. Characterizing, and certainly from the data, we can also see sort of what is the whole space of sort of transcription factor co-expression patterns, right? And so, from that, we could then maybe figure out if we could isolate what those external effects have on those individual pairs. But there's also sort of like a conflict, right, in terms of saying what, like, maybe those are important, but maybe the two-dimensional model captures what we will call the essence of the dynamics of the system. And I think there's a bit of a conflict. System. And I think there's a bit of a conflict in terms of the technology-driven data collection analysis to measure many more variables that we say are significantly different from instrument noise. But that doesn't mean that they have any biological significance at all. It just means that they're more than noise. So I think we have to be careful about what we want the mollusk to do. Do we want to have reduce the dimensionality of it? Reduce the dimensionality of it to make it intuitive to understand basic correlations and basic causality? Or do we want to be able to explain all the data that our fantastic technologies can produce for us? Like in one ways we're trying to measure everything, but focusing on fundamentals. How do we do that? And so I think we should be careful with getting caught in the details sometimes.  I don't know if I'm doing it. scanning a whole bunch of parameters data somewhere. And then you pick out all the parameters that are fine statements. And then seeing it as we are somehow asked if I ask what is it that makes one cloud circular stable. It's really asking about it. And there's correlations among these. But I'm interested in the way I'm trying to explain to it. What what is the constraints of the process? Yeah. And then I was like, now that's like the bottom of the top. Yeah, we couldn't get the left. Did you publish anything on that? No, no, no, no, no, no. There was nothing there. It's still very interesting. I don't know whether you're going through it. Yeah, maybe talking about the parameters and and and finding the right parameters and probably is a volume. Of volume space where certain qualitative behaviors come up, right? So it's sort of like a rare event, although it's sort of like a code I'm using two kind of thing where you have like some things, like structurally unstable and maybe stable things on the mentality. You have to simulate that. It's just done with the screen simulating the whole thing. I mean you need a certain strength of repression relative to the activation. That's kind of the main thing. This data exactly said with a very simple experience basically. These are not true, but here's why it is not true. And again, the others show my work. It's an interesting parameter. Parameters were insisting that they can do something for it. And then we couldn't find connections, but it's co-bearing. Not just a matter of intellectual importance, it's also a matter of doing quality stuff in the first place. And then we could start to realize that maybe the conclusion of the first work is fine. That's something that I think inspired. That's a good one. That would sort of be confusion, which is not a big problem. But I think still that's a big problem. And how do we, if you have biological observation, like even if it's just qualitative behavior, how do we Behavior, how do we use that as a constraint? You're never going to be able to measure all the costings in the living cells of those be not like that. Right, yeah.