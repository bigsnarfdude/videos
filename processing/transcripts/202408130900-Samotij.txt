And today I'm going to speak about a recent joint work with Marcelo Campos, which actually is scheduled to appear on archive today, so very recent. And I will not assume that you have ever visited the container method, so I apologize for the title, it's just that we have revisited it, but it doesn't mean it doesn't necessarily apply to you. So, what is the hypergraph container method? I mean, if I wanted to be a To be a salesman and sell it to you, I would say that this is a fairly general method for dealing with large union bounds. And in problems that could be described using some system of local hardcore constraints, you've already heard maybe the buzzword yesterday in Matt's talk, who said that it could be used, say, for estimating the probability that G and P is triangle-free. G and P is triangle free, kind of above the threshold it were. Every edge in a typical sample belongs to many triangles. Now, the two motivating examples that I would like to base the first part of the talk on come from combinatorics. There will be one example from Ramsey theory and one example from extremal graph theory. The example from extremal graph theory will be somewhat similar to Matt's example about probability of GNPP in triangle three, but slightly different. But slightly different. So, the two theorems that I need to introduce or recall: one is a theorem of Turan. Let's make the following definition. The extremal number for a graph H relative to a graph G is simply the maximum number of edges of a graph G prime, which is a subgraph of our host graph G and doesn't contain And doesn't contain the graph H is a subgraph. So you should think that G is a large graph and H is a fixed graph. And the second theorem is Ramsey's theorem, which says we will write G arrows H in our colors. This means, so this is a definition, this means that however we color the edge. The edges of G using our colors, there necessarily needs to be some color so that when we look at the edges that obtain this color C, this graph will contain a copy of H. So these two are definitions, and what do the theorems say? And the definitions are general and what what I will be talking about could be generalized to an arbitrary graph. Could be generalized to an arbitrary graph, but just to keep the discussion simple, I'll only restrict to the case where h is a triangle, like in Matt Stock. So Ramsey's theorem says that for every R, the complete graph on n vertices arrows the triangle in R colors if n is sufficiently large. And to run theoretical, And Turant's theorem determines the asymptotic value of this extremal number of the triangle relative to Kn. And this is roughly half of all the edges. I mean, this is the precision that I will use. Much more precise results are known. And as you may know, the largest triangle three-star graph of Kn is a balanced compete by part graph. Compete by bike and graph. Now, what's kind of the goal or a problem or a group of problems that people were interested in since say the mid-80s and it had some motivation in the study of Ramsey's theory, namely people wanted to know whether there exists a graph which arrows K3 but doesn't contain a complete graph on four vertices, and kind of this initiated the study of a Ramsey theorems relative to a random graph. So our goal. Graph. So our goal would be to understand when can we replace Kn with the random graph G and P? Or more precisely, what conditions do we need to assume on P so that in these two statements I could replace Kn with G and P and still have this hole with probability, which is. With probability which is close to 1, or it tends to 1 asymptotically as n tends to infinity. So this is the goal, and kind of many such theoretics are already known, and I don't want to give a full overview of the field. I just want to highlight the main challenge in addressing this question. And the main challenge is overcoming Large union bonds. So, at the first attempt, how would one prove this kind of statement relative to the random graph GNP? So, to prove a lower bound, namely that we can find in a sample of GNP a large triangle 3 subgraph is easy because we just consider an Because we just consider an example in the complete graph, we just intersect it with the random sample, and we get kind of the right proportion of the edges. So the difficult part is to prove the corresponding upper bound. We would like to prove that the random graph typically doesn't contain a triangle-free subgraph, which contains significantly more than half of the entry, so say 51%. So in the setting of trans theorem, let's set L to be 1 plus delta. Delta times the number of edges we have in the largest charge with free subgraph of Kn, and we kind of expect to keep exactly p proportion of it. So the same proportion as the number of typical number of edges G and P to the typical, so to the number of edges in Kn. Now, as a first guess, one would say, hey, let's just calculate the expected number of triangle V subgraphs of this size and see if that works. So the expected. So the expected number of triangle free. By the way, everyone is fine with the definition of triangle free? Okay, triangle free sacrators of GMP. Now, I would like to now bound it from below because it will not work. So since the property of being triangle-free. Since the property of being triangle-free is a monotone property, then any subgraph of this graph will also be triangle-free. So we get a large family by considering the largest triangle-free graph and all subgraphs of it, which contain n edges. And then each of them appears with probability p to the n. And this is about e over 1 plus delta to the power n. To the power n at least when p is smaller. Because the binary coefficient A choose B is roughly EA over B to the B. So the kind of naive union bound blows up. Similar in the setting of Ramsey's theorem, what is the expected number of colorings of Of G and P with no triangle. And here, what we can do, we can say, okay, I don't want, so the only edges of GNP that I want to see are the edges crossing some fixed partition, which means that I pay for it. This factor, I want. Factor, I want all these edges inside not to appear, but now the graph is triangle-free. Typically, we'll see about this many edges crossing, and we can color each one of them with any color we please. So, we get a factor of roughly r to the power k and k3 times p. So, this is about this is exponential in p. A p and choose to minus the extremal number of k3 in kn times, sorry, this is with a minus and a minus and one plus log r. And as you can see, r is maybe at least three, and this already blocks up. So, kind of these naive unit mounts do not work, and kind of the idea of this container method is to try to do something smarter. Now, this smarter thing is to kind of group Kind of group these triangle-free subgraphs that could potentially appear in GNP together. And here's a statement, which is maybe the kind of most basic example of a container lemma in the setting of triangle-free graphs. And I'll show you how it could be used to solve this question. This is maybe slightly more. This is maybe slightly more delicate, not much more difficult, but I want to also talk about other things, so I will not do it today. So a container lemma for triangle-free graphs, Fn of K3 will be the family of all subgraphs of Kn. Which don't have a triangle. It's a statement that says the following: For every epsilon, there exists some constant k and a family C of subgraphs of the complete graph, which has the following three properties. First of all, each of Each of the triangle three graphs is contained in one of the elements of the family. Number two is that this family is not too large, namely the log of the size is exponential in roughly n two, three lattice. Roughly n2, three halves. And the third condition, which is the most important because so far I could just take c to be the singleton containing just the complete graph, where it's just one graph, and then every triangle-free graph is contained in it. But we would like to cover triangle-free graphs with something, ideally, maybe just maximal triangle-free graphs, but ideally something which resembles a triangle-free graph, which would maintain some of the information. Maintain some of the information. And one such condition is each of these covering graphs contains fewer than epsilon n cubed triangles. Why n cubed? Because n choose 3, so order n cubed. This is the number of triangles in Kn. So in other words, maybe the graphs that we use to cover the triangle 3 graphs are. Use to cover the triangle-free graphs are not triangle-free themselves, but in some sense they're close to triangle-free. And this is measured by this parameter epsilon, which only affects this concept. Now, one might ask, why don't I cover everything just by triangle-free graphs? The thing is that every maximal triangle-free graph, meaning a graph which is triangle-free but every edge that you add to it would close a triangle, this family is pretty large. Its size is exponentially in n squared. Exponentially in n squared, and this does significantly better. This is exponentially all the entropy graphs. So now, if you see it for the first time, then I can imagine this could be a little confusing, why this statement is useful. So let me show you how it could be used to prove this random analog of Turang's theorem or Turang's theorem relative to GN. By the way, if you have any questions, please feel free to interrupt me. I'll be happy to answer. So let me keep the M and now instead of calculating the expected number of m-triangle free graphs. M-H-triangle free graphs that appear in GMP. Let's do something slightly different. What's the probability that there exists some triangle-free subgraph which is contained in our random graph GMP and it has at least m edges. So this is kind of the event that we would like to rule out. That we would like to rule out. Now, instead of calculating the expected number of such graphs, let's now use this family C. So if this G is contained in G and P, it has at least M edges. Well, this G is a triangle-free graph, so it has its own container. Now, if G is contained in G and P it has at least M edges. In G and P and has at least M edges, then it means that G and P must intersect at least one of the containers in at least M edges. So this is at most the probability that there exists some C in C, so that the size of the intersection of G and E with C is at least M. Now we can use the union bound, sum C and C. What's the probability? C and C, what's the probability that G and P intersect C in at least n edges? Now, why would I expect that this probability is small? If C was a triangle-free graph, then by Turin's theorem, it only contains this many edges. So the expected This many edges. So the expected size of the intersection is precisely this quantity, and we want to exceed it by a multiplicative factor larger than one. So this would have exponentially small probability in this expectation. However, we only know that this graph contains few triangles. However, there's an easy-to-prove generalization of Turan's theorem, which says that since C contains that since C contains fewer than epsilon n cube triangles implies that the number of edges of C is no more than say 1 plus delta over 2 times the largest number of edges in a triangle 3 subgraph and the quantifiers would be for every delta there exists in epsilon so So, first you ask me for some precision delta. I calculate the epsilon according to this theorem. I get this family C. And now C has this many edges, so already this is an exponentially small probability. So it's C times something which is exponentially small in N squared V. And now the size of P is exponentially. Now, the size of p is exponential and n to 3/2. So, this tends to 0 is p, say, greater, greater, than 1 over root n times log n. So, kind of, this is maybe the simplest application of the method. And the interesting thing about this is that this is essentially the best you could do. So, if I change this aside so if I lower p below one over root n, Lower P below 1 of root n, this statement is no longer true. GNP contains triangle-free subgraphs, which contain almost all the edges. And one can also get rid of this log n, but let me not get into it. It's slightly technical, but this can also be achieved. And a similar spirited argument would also establish this random version of Ramsey's theorem under the analogous condition. Okay, but I didn't come here to talk only about triangle-free graphs. This is maybe kind of some simple motivating illustration. I wanted somehow to talk about what a general container lemma would say in a more general setting. Perhaps one of these settings could be interesting for you. So, the general framework is that we have some hypergraphs. Uniform hypergraph, meaning it's just a collection. Yes. Will you tell us how to prove the container? I will say something at the end. So we have a universal hypergraph on V, meaning a collection of subsets of V, we all have the same size. And we'll be interested in the family of Independent sets of this hypergraph. So these are all subsets of V that do not fully contain a set of H. Now, for example, triangle-free graphs are independent sets in a hypergraph, which is defined by taking V, the set of hyper vertices, to be the edge. The set of hyper vertices to be the edges of the complete graph, and eight are just all triangles in Kn. So subsets of V are just sublars of Kn and this condition for every triangle, triangle is not contained in the graph means that the graph is triangle three. So this is kind of a more abstract way of describing the set of triangle three graphs. Set of triangle three graphs. And now in this more general setting, a hypergraph container lemma is a statement of the following form. So it asserts that there exists some family of subsets of the vertex z. Of the vertex set, which satisfies the following three conditions. Kind of analogously, every independent set in our hypergraph is contained in one of the elements of C. Two, the family of C is small, and three every Each of these containers is almost independent. So note that I deliberately wrote small and almost independent kind of in quotation marks because this doesn't have any precise meaning. And in the remainder of the talk, I'd like to discuss kind of some potential ways of. Some potential ways of quantifying what almost independent means and how it relates to small. Of course, there would be some trade-off here, as you saw in the case of triangle-free graphs. If I, as a definition of almost independent, I would choose independent, then the smallest such value would be exponentially n squared in the case of triangle-free graphs. And if we sacrifice triangle-freeness to containing just, say, 1% of the triangles, then it becomes exponential in n to 3 halves. Exponential n of the pass. So there is some trade-off here, and there is some freedom in choosing what algorithms are dependent on. And this is what I would like to talk about in the rest of the talk. So, the first time such a statement was proved in this general setting was in 2015 and this is due to Georgi Vallo, Rob Morris, and myself, and independently by David Saxon and Andrew Thompson. So So the work of David Saxon and Andrew Thomason was inspired by the work of Sapujenko, which we've heard of yesterday, whereas our work was inspired by a method due to Kleichman and Winston also about enumerating independent sets in graphs. Now to state the theorem I need two definitions. The degree The degree of set L in some hypergraph H is simply the number of hyper edges that contain this given set. So this kind of generalizes the notion of degree in graphs. The degree of a vertex is the number of edges that contain it, but here the edges could be larger. So we could consider degrees not only of vertices, but pairs of vertices, triples, and so on. And And the maximum a degree is just the maximum a degree of a set of precisely L vertices. And the theorem says something, says the following. The following: Suppose that for some p between 0 and 1 and and d we have the following condition the maximum L degree is at most t This is roughly the average degree of a vertex. And these degrees, as the set L grows, should kind of decay exponentially with L. So L equals 1, we just assume that the maximum degree is not much larger than the average degree. And then as we increase the set, we want to, each time, shrink by a factor of p. So this says something about the dispute. So this says something about the distribution of edges of the hypergraph. The edges don't concentrate too much on small sets. And this is enough to prove the existence of a family of containers with log of the size of C to be roughly P times the size of the vertex set. So 1 for mu will always mean the same, meaning every independent set is. mean the same, meaning every independent set is captured. 2 log of c is at most some constant, which depends on R epsilon, and the only times V P log 1 over P and 3 the notion of almost independence is the notion of almost independence as we saw in this triangle 3 example, meaning. So in this triangle, free example, meaning for every container C, if we look at the hypergraph which C induces, so these are all the edges that fit entirely in C, this is at most an epsilon proportion of all the edges. So before we had edges, cube triangles, and we wanted only to have And we want it only to have epsilon proportional. And this implies the container lemma for triangle three graphs, because you can check that this condition is satisfied where you take p to be one over square root of n. Is that r uniform? R is sorry, r uniform, yes. Yeah, thanks for it. Now, in fact, kind of as we realize later, there is a better way to phrase condition number three. And one And one can get rid of the assumption star. So one can just kind of bunch up three and star. And the idea, so let me call it three prime. Would say that for every C in C, it's not true that this condition holds inside of C. And kind of the reason that it's true, suppose we have the opposite inequality, then I could just replace the number of edges of h in here with the number of edges in here, just suffering a constant penalty of one of epsilon. Penalty of one over epsilon, which then could be absorbed by this constant k. Now, this is implied by the following stronger statement, so let me call it 3 double prime for every C in C. I don't necessarily want to have this condition for just for the hypergraph itself, but also for each of its. But also for each of its sub-hypergraphs. So I want to kind of call it in a robust sense. So it's not true, there does not exist a hypergraph which lives inside which satisfies the same condition. Now, if instead of if I now allow to put weights on the edges, so if I allow weights on the edges, then it turns out that this is equivalent to the following statement, which let me call it 3 triple prime. For every C and C, there does not exist a distribution on On the edges of the hypergraph. So take some distribution, I take, have some random edge according to some distribution, and now the probability that my random edge contains a fixed set should be at most dpl minus 1 divided by c. So if you're familiar with the Kantali conjecture, With the Kalcalai conjecture or the fractional version of it and the notion of spreadness of a measure. So, this is kind of the same flavor. We don't want a measure on the edges of the hypergraph so that the probability that our random edge contains some given set kind of has this exponential force. So, kind of this is very similar to the work of Frankston, Kahn, Nariana, and Park. And part, if you're familiar with the work, then I encourage you to make the comparison. So I dragged you through the mud here because I wanted to motivate the next statement. If you're familiar with the work, there's an observation of Talagrand which says that the existence of such Of such a spread measure or non-existence of it is equivalent to the existence of a small fractional cover of the hypergraph, which is small p-wave. So using linear programming duality, there's a different way to describing the same condition in terms of covers of the hypergraph. So we want to have a collection of sets such that every edge of our hypergraph contains one of the sets in the collection and the expected number. The expected number of edges in our collection that appear in a p-random set is small. So let me recall. The upset generated by a family G is the family of all subsets B that contain one of the elements of G. So we take all sets in G and all their supersets and the weight or the P weight. But the p-weight of G is just the expected number of elements of G that appear in a random subset of V. If we keep each element independently, we'll probably get now the theorem that Marcelo is. That Marcel and I proved is the following. So it's a new version of the hypergraph container theorem, where condition 3 is expressed in terms of the existence of a cheap cover. So kind of a dual condition to the condition 3 triple prime, which is stronger than the condition from the original theorems. So to state it, suppose H's are unit. H's are uniform, and let P again be this density parameter. And the theorem postulates the existence again of a family of containers. Condition one is the usual one, every independent set has its container, and condition two A condition two, a log of the size is again exponential and roughly the number of vertices times p. But the third condition is now that every each of the containers makes a cover a A G, meaning some G whose upset contains all the edges of the induced hypergraph and the weight key weight of discover Is its most B times C. Sorry, and one important condition: every edge in G is size at least two. You were not allowed to use the empty set or singletons. Now, why is this condition important? If I was allowed to use singletons, then I trivially have a cover of size P C, namely I take all the vertices. Take all the vertices. Now, suppose I have a cover which uses only large sets, and now if my Q is slightly smaller than P, then the Q weight of G is at most Q over P squared times the P weight of G, which is now at most delta. Delta times Q times C. So the fact that we're covering not with singletons is stronger than the fact that we have a cheap cover. So cover whose P weight is only a small proportion of QC. Let me call this theorem A because there is also a theorem B which states the following. Following for all P between zero and delta, there exists a family of containers, and now h is just some hypergraph. I don't assume anything about uniformity. Just that it's a non-empty hypergraph. Now we have a family of containers covering all of its independent sets whose size is at most exponential in Vp. And third, the notion of almost independence is that every Every container satisfies the following for every vertex in C, the probability that it appears in a random subset, if we condition that this random subset is independent, is at least one minus delta time. 1 minus delta times p. So we sample from the hardcore measure here, we sample a random subset of C and condition it on being independent, and then each vertex appears almost with probability. So this is the measure of almost independence, and know that in particular this implies that the typical size of such a sample The typical size of such a sample is at least one minus delta times the sample if we didn't condition it on anything. And a cute fact, this implies that the probability that an unconditional That an unconditioned random subset is independent is pretty small, it's pretty large. So it's at least one minus p to the power delta c. So we had three theorems. The theorem which expressed the notion of almost independence in terms of Almost independence in terms of the existence of some measure that satisfied the spreadness condition. So, this was the original theorem, slightly strengthened. We have theorem A, which expressed the almost independence in terms of the existence of a cheap cover. And finally, we have theorem 3, which expresses the notion of almost independence either by this occupancy condition or by the fact that if we sample just a random sample, Just a random subset of the final container, we're quite likely to get something which is really independent. So, let me try to compare these three theorems in a diagram. So, first, we have the covering condition. There exists a cover whose weight is a small fraction of Pc. And it covers the final induced hypergraph. So this was theorem A. Theorem B says that the probability that a random sample is independent is somewhat large. So it's exponentially small only in delta B C. In delta B C. And the final theorem is that, or the original one, sorry, there is no measure such that for every subset, the probability that a sample from this measure contains this given subset is at most p to the size of the set minus one divided by one. By sorry, the d times size of the set, p to the size of the set, divided by delta PC. Now, this by L P duality is equivalent to this statement, but in a fractional version. So there is no fractional cover. Cover G whose weight is at most delta B C. Of course, if we have a cover, then we have a fractional cover. And in fact, this is the easy direction of the L P duality. This is the more interesting one. If you have a small covering hypergraph, what does it say about the probability that you're independent? Well, you could use Harris's inequality and say, I'm certainly independent if I don't contain any of the elements of G. So a lower bound on this probability is not containing any element of G, and this is roughly exponential in the p-weight of G. The P weight of G. So by Harris's inequality, A implies B. Now, interestingly, this condition implies that condition. And one can prove it using Janssen's inequality. So if you take a hypergraph. Take a hypergraph, and I tell you that the probability that a p-random subset is independent is fairly large. This means that we must have a cheap cover or a covering hypergraph with relatively small weight. And since I think this is probably kind of an interesting statement to explore further, let me state a conjecture which is stronger. Conjecture, which is stronger than what we can prove, and hopefully, I can interest you in thinking about the conjecture. I'll mention some of the implications of it. So, what we can prove is the following. Suppose we had some p between 0 and 1, and also a hypergraph whose all edges have size at most are. Have size at most R. So you have some upper bound on the size of all the sets, not necessarily uniform. Then there exists a G which covers H in the sense that every set in H contains some set in G. And importantly, Importantly, G doesn't contain the empty set because this would trivialize it. And the probability that a random set of vertices is independent is at most exponential in the weight of G divided by 8, but unfortunately, But unfortunately, we here need to pay a penalty by dividing p by r by a factor of r squared. So if r is a constant, somehow this doesn't mean much. And know that Harris's inequality, so this is Harris, gives a lower bound, which is roughly this. So if this proposition says that up to changing the density, says that up to changing the density by a factor of r squared, the probability of being independent is determined by the smallest p weight of a cover. Now the conjecture that Marcel and I propose is that you can replace this R factor of R squared with an absolute constant. Conjecture. There is an absolute constant C such that for all P and all R and every hypergraph is such that which doesn't contain any edges which are larger than R, we can find We can find a cover, again, non-trivial cover, such that the probability that VP is independent is most exponential in minus weight of the cover, but now it's P over C plus. In the two minutes I have left, let me kind of try to sell this conjecture to you. Uh do you know the Kancali conjecture? I mean some of you know, some people sitting in this room even solved it. People sitting in this room even solved it. So the conjecture says that if every weight, if the p-weight of every cover is at least a half, then if we increase the density by a multiplicative factor of log r, the probability that we enter the property, meaning we're not independent, is close to one. Close to 1. So let's take a suppose that a cover is the cheapest cover at P is already larger than, say, a half. Let me increase now P by a factor of log R. Then the cheapest cover now has to have weight at least some large constant times log R. So this is minus C log R or minus C. or minus 2 C log R plus C log R. So this is small. So this conjecture would imply the Kancali. Moreover, in the Kanakali conjecture you have this logarithmic factor in certain problems it's unclear whether it's necessary or not and perhaps you have a logarithmic factor in a different power. Say if you want to study the property of If you want to study the property of containing a triangle factor, then the right power of log is one-third. Now, we argue in the paper that this also implies the Johansson-Kahn-Wu theorem, because the cheapest cover for triangle factors will comprise triangles. So, to beat this factor of log r, we only need to increase p by a factor of log r to a third because everything kind of Because everything, kind of all the terms that appear here, are in the third power. And so this conjecture would also imply the JKV theorem and possibly some other interesting things. So if you have some time, we would both be very happy if you think about it and kind of think about it seriously. So thanks so much for your attention. Something that I had a little trouble digesting in your theorem B is this extra primary delta to choose. Could you maybe give an indication if you were to try to use theorem B to prove your GMT triangle through the statement? What you would, how should we think about this delta? So you would again choose delta to be Choose delta to be roughly epsilon. Because then you say, okay, the probability that if you take, so you have some graph C, and your assumption is that if you take a P random subgraph, then the probability that it doesn't contain a triangle is somewhat large. So it's exponentially small and minus delta n squared q. Now, if this graph contained more If this graph contained more than delta n cube triangles, then you could use Jansen's inequality to get an upper bound on this, which is better than the lower bound of you. If it's easy to describe, how do you prove the upper bound of this composition? The upper bound. Sure. So you take the hypergraph and then you just keep the And then you just keep the minimal edges. And now the idea is that you want to apply Janssen's inequality, but sometimes the factor delta would kill you. So the sets that will contribute a lot to blowing up delta, so delta means it calculates the expected number of pairs of edges that intersect, then you replace intersect, then you replace all the edges by the problematic intersection. And kind of in doing so, you lose this factor of 1 over r squared. There are edges that bunch up on a small set to replace by this set. Could you maybe give an example of something you can do with this strengthened version? Strengthened versions A or B? No, not the precise. Strengthened versions A and B that we do not do with the previous. So one thing that I didn't mention, because me is more for the experts, is the bound on the log of the size of the family of containers has this constant k. And I said this k depends on epsilon, on d, and on the uniformity. So in kind of the original papers, the dependence on the uniformity was Original papers, the dependence on the uniformity was kind of unfortunate, it was factorial in R or kind of exponential in R squared. And in both the theorems A and B, it's only kind of R squared, so it's polynomial in R. So if you're dealing with hypergraphs that have large edges, the size of the edge is kind of growing with the size of the vertex set, then this could be beneficial. So unfortunately, Beneficial. So, unfortunately, kind of, we don't have any statement that we could prove using theorems A or B that we couldn't prove using some earlier versions, but kind of perhaps this point of view of considering different notions of almost independence is... I mean, if the best thing that comes of this work is that someone saw this conjecture, then I would be very pleased. And just one last question for a more user's guide to containers. Is there a good heuristic for guessing what a rough size of a set of containers should be? In other words, to indicate whether or not Ethereum is going to be useful or not. So for instance, maybe even just the triangle-free case, is there a very good heuristic for guessing the correct size and how fast I contain this? So kind of it's the perhaps. Perhaps kind of this condition could guide you. That instead of kind of looking at the final container, just try to convince yourself that taking V itself doesn't give you a good container. So calculate the probability that the random substance of the vertex as is independent, if it's not exponentially small in the expected size of the set, that you're not yet in the container, actually. And as long, as soon as As long as soon as you get that, then you can kind of have more efficient coverage than just be itself. So maybe we'll leave discussions for later.