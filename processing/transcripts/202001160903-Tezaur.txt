I think this has been a really productive and interesting workshop for me and everyone else too. So, I'm Irina Tazaar from Sandia National Labs. I'm going to be talking about our strategy towards achieving a performance portable Landis model using the Trulinos and Cocos libraries, kind of to kick off the software engineering and performance sessions. And this is joint work with some of my collaborators on the prospect. Collaborators on the Prospect landized modeling project funded by the DOE: Jerry Watkins, Rachel Minaro, Marlow Perigo, Andy Salinger, and Steve Price. Okay, so I thought I would start by talking a little bit about our motivation for performance portability and also kind of defining what exactly I mean when I talk about an application being performance portable. So, in recent years, there's been a big push to solve ice sheet and climate models using. Ice sheet and climate models using higher and higher resolution grids. In order to do that, you need machines with additional computational power. So, HBC architectures have been advancing to provide this additional computational power. In particular, they've been becoming more and more heterogeneous as we move towards exascale. And so, I would say that a big challenge in terms of software engineering for climate models in recent years has been how to adapt. How to adapt to execute correctly and efficiently on some of these new heterogeneous HPC architectures. And this is often far from trivial because the new architectures have very different memory models than some of their predecessors. So if you have a code that was written 10, 15, 20 years ago, getting it to run correctly or to run at all on one of these new machines can be far from trivial. So this is where kind of performance portability comes in. So what I mean by comes in. So what I mean by portability, an application is said to be performance portable if it runs on and achieves a consistent level of performance across a variety of computer architectures. So from traditional CPUs to CAN L's to GPUs to some of these hybrid machines. So when we talk about portability, it's kind of useful to look at what are the trends in HPC architectures, what architectures do we have now, and where is this heading in the near future. This heading in the near future. So, the first part of the slide here shows you kind of the current and near future advanced computing machines we have in the US that are kind of open science machines that climate models are going to be targeting. So, currently we have a variety of different CPUs out there: IV Bridge, Haswell, Skylink, ARM platforms. There are also some large KNL machines. So, these are also known as the ZNFs. These are also known as the ZMTs. These are many core type architectures. We have the Cori machine at NERSC, the Theta machine at ALCF. Let me move the mouse out of the way here. But really kind of, if you go beyond this, we're really kind of limited to GPUs and heterogeneous CPU-GPU architectures. The follow-on to the CAN L, the Knights Hill architecture was recently cancelled. So we have currently Summit, the world's most powerful supercomputer, having NVIDIA Volta GPUs. Computer having NVIDIA Volta GPUs. NERSC Perlmutter is going to be another GPU machine that will come out later this year. And then the first two XScale machines next year are supposed to be Frontier and Aurora, which are both GPU machines having AMD and Intel GPUs respectively. So really kind of, you know, whether you like it or not, the GPUs are coming. And even kind of as Steve mentioned in his presentation on Monday, even if you're not that excited about it or keen on GPUs, if you want to get an allocation on one of these big architectures. Want to get an allocation on one of these big architectures, you're going to have to probably have some way to get your code to run there since those are the big machines we will have. So, one thing with GPUs is what's expensive and what's cheap is very different from CPUs. Computations are cheap on GPUs. What is expensive is memory transfer. And when you have a model like this, usually MPI alone is not enough to exploit all the available parallelism that the machines can provide. So, one approach, and the one I'll be talking about. So, one approach, and the one I'll be talking about, to get portability to be able to take advantage of all this parallelism is the MPI plus X approach, where you basically have two levels of parallelism. You have MPI for intranode parallelism for your CPU, and then you're going to have your plus X for the intranode parallelism for your accelerator or GPU, where basically you're going to launch many, many threads to speed up your computation beyond what you would have with just MPI alone. So, listed here are some approaches for going from. Approaches from going from MPI only to MPI plus X. The most obvious approach is probably this first one. You can have an architecture-specific approach. You can rewrite your code using CUDA for an NVIDIA GPU or HIP for an AMD GPU. And there's been some really nice work recently by Ludovic Gruss and Jenny as well on a model that was written using CUDA called Fastice. It had some very good results on NVIDIA GPUs. NVIDIA GPUs. The issue with these architecture-specific approaches is they're not portable. So the same code won't run on a CPU usually. And if you want to even run a different type of GPU, you need another implementation. So in terms of approaches that are portable, we have directive-based approaches so you can write your code using OpenMP and OpenACC. And there's been some level of success porting the Home Atmosphere Dive Core by Matt Norman to OpenACC with some good results. So what I'm going to be talking about today is a So, what I'm going to be talking about today is this third approach. So, relying on abstraction layers of data and task parallelism through libraries that are designed to provide portability. And so, specifically, I'm going to be talking about our approach, which is using a library called COCOS, which is a C ⁇  library developed at Sendia that's open source and provides portability. And it's really looking to be kind of the path forward for the DOE E3SM or system model under the SCREAM project for atmosphere and our project process. For atmosphere and our project prospect for land ice. Oh, let's see. So, outline: so, I'm going to describe some of our efforts in creating a performance portable implementation of our model called OLLI, Albany Landece, using COCOS and Trelinos. I'll give kind of a very brief overview of our project, and then I'll kind of break up the main part of the talk. I'll talk about final element assembly for half of it, and the linear solve for the other half, which are the main two. For the other half, which are the main two operations that we have. And this is mostly about kind of what we've done. You know, I'll try to present it as things that may be helpful to others as well, but I will kind of end with some discussion points that we could talk about to kick off some of the discussion here. Okay, so a little bit of background. So the work I'm presenting is being done as a part of the PROSPECT project for land use modeling. You heard a little bit about this from Steve on Monday. PROSPECT stands for Probabilistic Sea Level Projection. Probabilistic sea level projections from ice sheet and Earth system models, so a little bit of a mouthful there. It's a SAIDIAC-4 project that began in 2017, in which Sendia has been one of the key players. And so our role at Sendia has been to develop and support a robust and scalable Landeis solver based on the first order-Stokes equation. So we call this OLLI or Albany-Landece. Some of the requirements for OLLI are listed here, so just to highlight a few of them. Being able to use unstructured grid meshes was a big reference. On structured grid meshes was a big requirement to allow you to concentrate the computational power where it's needed. We do that using finite elements. The portability, of course, is a big requirement, hence this talk. And then also being able to do some of the advanced analyses that we've talked about earlier in the week. Mauro Perrigo talked about the deterministic inversion calibration work, and UQ is another big aspect of that. And this OLI code is hooked up to the DOE E3SM through MPAT. E3SM through IMPAS. So you've also heard about MALLI this week. That's basically IMPAS coupled with OLLI, which is our hook into the E3SM, where OLI is the steady velocity solve, and then currently IMPAS does the thickness and temperature evolution for dynamic simulations. So since this is a software engineering session, I wanted to say a little bit about the engineering behind OLI. And this actually amounts to really talking about the engineering behind a code called Albany, in which OLI designed. In which OLI resides. So, OLI is implemented in a Sandia open source C multi-physics finite element code known as Albany. It's available on GitHub at this URL. It was started about 10 years ago by Andy Salinger at SEDIA. And it has a lot of different equations currently. We have the Landec application. We also have a lot of other applications, fluid structures, quantum device modeling, also other climate applications, notably atmospheric and permafrost modeling. Atmosphere and permafrost modeling. And the strategy here in terms of engineering, Albany, is what we call agile components. And so, in this strategy, mature modular libraries are glued together using template basionic programming and abstract interfaces to basically give you code with a lot of capabilities that you can just access at runtime by changing an option in your input file so you can swap out different preconditioners, solvers, things like this. And most of our components are Trelino's. And most of our components are Trelino's libraries, which are also open source libraries developed at Zydia. And so, you know, just to comment a little bit about this approach: there are pros and cons to using libraries. The cons would be if a library doesn't have a capability, you have to wait for those developers to put it in or put it in yourself. It can happen that a bug gets introduced into the library and so your code slows down 10 times overnight without you making any changes. But overall, we've had, I think, a very positive experience with this. I think a very positive experience with this where it's allowed us to leverage years and years of RND and algorithm software, more recently, performance portability. And we really wouldn't have been able to build up this model so quickly with so many advanced capabilities if it weren't for all these libraries. So we're using here the first order Stokes model. I'm not going to say a lot here. These are the equations. I think you guys are very familiar with this. The main point here is that Ollie does the diagnostic solve. So this is the volume. Does the diagnostic solve, so this is for the velocities. This is a nonlinear system. We're going to use a Newton method nonlinear solver here with automatic differentiation Jacobians to solve this. And underneath here, we need a linear solver, and we use preconditioned Karylov iterative linear solvers from Trulinos. And so this being implicit, we have two main operations, the finite element assembly and the linear solve. And so, as I said, I'll kind of split the rest of the time on each of these. Okay, so I'm going to start with the finite element assembly. So, as I said earlier, our strategy. So, as I said earlier, our strategy for the portability relies on Cocos. So, what is Cocos? Cocos is a C library that provides performance portability across a variety of different architectures using the MPI plus X programming model. So basically, it provides automatic access for various plus X, OpenMP, CUDA, P threads, relies on templated metaprogramming. Really, you know, the high-level kind of what's really nice about Cocos is you write your algorithm once using Cocos. Your algorithm once using cocoa's functors. And then you can change a template parameter to get the optimal data layout for your hardware. And this is accomplished under the hood through a memory layout abstraction. So I'll show this more specifically in a couple of slides what I mean by this. But this is really nice for application developers like myself because we don't really need to deal with architecture-specific programming. We learn COCOS, we write the code using COCOS. Usually writing the code using Cocos forces you to write better code that is more performantized. Code that is more performant on CPUs as well as some of these advanced architectures. And basically, you can compile the code for your different architecture. You get the optimal layout for each one. And if a new architecture comes out that doesn't exist right now, the idea would be that Cocos developers will modify Cocos to work on that architecture. So, again, people like me are kind of shielded from having to deal with that. Okay, so let's look at our finite element assembly. So, this is specific to OLLI, but I think. This is specific to OLI, but I think this is kind of fairly general to other codes as well. You probably can recognize some of the operations for your code. This picture here shows the main parts of the finite element assembly in blue and in yellow. The lavender color, that's the salt, which I'll talk about next. So basically, we have these, the blue boxes here is what we call shared memory parallelism. These are routines that are dominated by computation. So there's a lot of loops there over cells, nodes, quadrature points. Points. We have a gather that will fill the element local solution. We interpolate the solution in its gradient from nodes to quadrature points, evaluate the local residual in the Jacobian, where the Jacobian comes from automatic differentiation, and then do a scatter to fill the global residual in Jacobian. So we're going to give that to the plus X. And then there's also the yellow, which is distributed memory assembly. And these are operations that have some communication between processors. We have an import and an export of the solution, Residual Jacobian, and we rely. Rizzo Joel Jacobian, and we rely on a Trilino's package called TPetra to do that. This is an example of one of these shared memory kernels that I showed from Albany. This is just meant to show you kind of how easy it is to switch a code from MPI only to MPI plus X. So this routine computes the gradient of a vector. So you can see there's a lot of loops here, cells, quadrature points, dimensions, nodes. What you would want to do, you can do all these things kind of massively in parallel. So you would want to give that to plus x. So any one of these loops or all the loops. So, any one of these loops or all the loops. In this example, we're just giving the cell loop to the plus x. So, we remove the loop, we add this code plus parallel 4, and you can see it's templated on this blue execution space, and that's the parameter that tailors the code to the device. So, it's set here at compilation to be the layout for your device to open in PCUDA serial and BMP only. And you can do this for, in the results I'll show we're just doing this for the cell loop, but you could also do this for any number of these loops or all the loops. We call that multi-dimensional or hierarchical. That multi-dimensional or hierarchical parallelism. Okay, so some results. So these are all for Greenland, looking at two different resolutions: one having 1.51 million elements, one having 14.4 million elements. These are all unstructured tetrahedral element meshes. The times I'm going to report are wall clock times averaged over 100 global assembly evaluations of the residual and the Jacobian. And the notation is as follows: RMPI plus JX, R is. MPI plus JX, R is a number of MPI ranks, J is a number of OpenMP threads or GPUs per rank, and then X is your architecture for the shared memory parallelism, so it's going to be either OpenMP or GPU for us. Does number of elements mean 3D elements or 2D elements? Oh, these are 3D elements. Yeah, good question. Okay, so I'm going to be showing results on six different architectures, and these really kind of span the gamut of the types of CPUs and GPUs that are currently out there. So one of them is Cori, a NERSC machine. One of them is Cori, a NERSC machine. This has Haswell and KL nodes, so I'll show results for both of those. We have two machines at Sandia that are CPU-based, Blake and Mayer. Blake has Skylake processors. This is a very fast CPU. Mayer has ARM processors. This is a very low power architecture that's coming out. And then we have two GPU machines, Ride and Waterman. Ride has the Power 8 and the P100 GPUs, and Waterman has Power 9 and Volta GPUs. So Waterman is the one. And voltage UPU. So, Waterman is the one that's kind of very similar to SUMMIT, one of our larger targets. And we're going to be looking at MPI only, MPI plus OpenMP, MPI plus CUDA, and MPI plus OpenMP, the MPI ranks are mapped to the cores, OpenMP threads are mapped to hardware threads, MPI plus GPU, the MPI ranks are assigned to a single core per GPU, and we're relying on CUDA UVM unified virtual memory to manage the memory management between the manage the memory between the Manage the memory between the host and the device. And these are kind of the optimal layouts that we've, or the optimal configurations that we found for this. So I'll show a couple of results. The first ones are looking at node utilization. So this is basically if you take a single node of each of the six architectures, you max it out, and you look at MPI versus MPI plus X. So these are these six clusters of graphs, these are the six architectures. The third one in each one is the GPU, so RIDE, P100, Waterman, Volta. And then AC. Volta and then A, C, and E, and each of these, these are going to be MPI only, and then B, D, and F are MPI plus X. Okay, so since I don't have a lot of time, just very quickly, kind of main takeaways. So we get a speedup across most of the execution spaces. The largest speed up is on the GPU machines. So the P100, we get a 12.6x speed up. The Volta, we get about a 2x speed up. So how does this compare to what we would expect? Well, ideally, you could get up to a 16x speed up if you have a mass. Up to a 16x speed up if you have a memory-bound application. For us, we're actually not memory-bound because we have these export-imports that are this yellow part of the curve or latency-bound. If you look at the blue, which is the memory-bound case, that actually is about a 16x or even more in this case, speed up. But one thing you'll notice is that the yellow, when you go from the E to F in each of these, it actually gets bigger, and so that's not good. That means that we're slower with the plus F. That means that we're slower with the plus X. And there's a couple of reasons for this. One is something we have control over, one is something we don't. So, what's slow here after profiling is this Teepestra export. The Toledo's team has been working on refactoring this to make it more performant on the GPU. So, we're using the old version that isn't so performant, hence the slowdown there. So, that's something we need to switch the code over to fix. The other issue is more of a hardware issue where there's a no. Is a more of a hardware issue where there's a known GPU direct issue on the power system, or basically CUDA doesn't play well with MPI. And so that may get resolved at some point, or certainly with the AMD GPUs, that would go away. Also, very much of interest here is strong scaling. So again, we have the six different architectures. The main point is that we get reasonable scaling across all the devices. This is running the same code without any machine-specific optimization. And the GPU results are not scaling as well as they could because of. Are not scaling as well as they could because of some of these issues that I mentioned on the previous slide with the export and the GPU direct. Okay, so that's my whirlwind overview of the Fine Element Assembly, kind of main points. Cocos is easy to use. We have the same version of the code running correctly on all these architectures, and we were able to get a reasonable performance across all these devices without any specific code to each of the architectures. So, let me now switch gears and talk about the linear solve. Since finally, Since you know, finite element assembly is only half the story, we also have the linear solve that takes about 50% of the CPU time. And so, here the portability work is not as mature as for the finite element assembly. I'm actually going to spend kind of most of the time talking about algorithms we've developed to make the linear solver very scalable. And I think this may be of interest to other folks here who are having similar issues to solvers as we are. But actually, the solver algorithm that I'll describe is portable, and I'll kind of outline. And I'll kind of outline the path towards portability at the very end. Okay, so motivation for our linear solver work. This is a scaling study for Greenland and Antarctica with some off-the-shelf linear solvers from Trelino's. These are for an ILU solver. The red line is the linear solve time, so you can see it's not scaling very well. It should be flat, and for the case of Antarctica, we're also actually incredibly slow here. Incredibly slow here. So that's not good. So a lot of work that we've done has focused on kind of trying to understand why the scalability is so bad to try to kind of remedy this. There's three reasons. Reason one, ice sheet geometries have bad aspect ratios due to the meshes being very thin. In the case of Antarctica, ice shelves can generate problematic linear systems, which can also cause solar failures. And for both of these, islands and hinge peninsulas of certain kinds can also cause a lot of. Of certain kinds can also cause a lot of problems for the solvers. So, I'm going to focus on dealing with the first two issues here through the development of a preconditioner we developed of the algebraic multi-grid variety that uses an idea called aggressive semi-coarsening. So, I'm running low on time here, so I'll try to speed up a little bit. So, the solver I'm talking about, this is a multi-grid solver, just a little bit of a review of how multi-grid works, just to explain what the challenges are and how our solver differs. Basic idea of multi-grid is to accelerate, convert. The basic idea of a multi-grid is to accelerate convergence of an iterative method on a given grid by solving a series of cheaper problems on coarser grids. So, main ingredients, you're going to have coarse approximations to your grid shown here. You're going to have a restriction and a prolongation operator that let you interpolate between the grids. You define smoothers that are applied as you kind of go back and forth between the grids, which are usually very simple numerical methods like Jacobi and Gauss Seidel that have the effect of smoothing out various components of the air so that they don't propagate. So that they don't propagate. And then, so, what you're going to do is you're going to go from the coarser grid, you're going to the coarser grid. On the coarse grid, you're going to solve the problem, which can often be done with a direct solver since it's so cheap. You go back up to the fine grid. It's called a V-cycle. You use this to precondition your problem. And here we're doing algebraic multi-grid, which basically means that this coarsheading is done at the level of the graph of the matrix. You don't actually need to construct any of the coarser meshes. The coarser meshes. So, why is this algorithm bad for these problems? Well, we have very thin structures for ice sheets, as you very well know. The ice sheets are about maybe five kilometers thick compared to thousands of kilometers in the X and Y direction. And so these bad aspect ratios of the mesh ruin classical multi-grid convergence rates. Basically, the issue is that in the approach I described on the previous slide, you coarse them equally in all three coordinate directions. That's a bad thing to do because there's very small horizontal coupling terms and it's very Very small horizontal coupling terms, and it's very hard to smooth the horizontal errors in this approach. So, the moral of the story here is solvers need to take the aspect ratios of the mesh into account. And so, we developed a solver that does this based on an idea called aggressive semi-coarsening, and it's illustrated here. Basically, rather than coarsening equally in all three coordinate directions, you're going to coarsen only in the vertical direction, which is what this shows. And then, once you're down to a single level, you can coarsen further since you've gotten rid of all the bad aspect ratios, and you can do that using an unstructured coarsening approach. Using an unstructured core setting approach. And so this is implemented in the ML and Mulu packages of Trilino. So these are available for others to use open source if they're interested in trying them out. So some results very quickly. This is on Greenland. This is a new preconditioner. This is an ILU preconditioner. We get a much more scalable solve, up to 16,000 cores, 1.12 billion unknowns. And where you really need the new solver, as it turns out, is Antarctica. So Antarctica is different from Greenland. It has very long. Different from Greenland. It has very large floating ice shelves shown here. And the thin floating ice is particularly problematic for solvers. You can see this by doing some analysis involving Green's functions, where basically these Green's functions are related to matrix inverses and preconditioners. The Green's function for the floating ice looks like this. It's about constant in the thin direction. And this is a case where ILU is known to not be effective. And so as a consequence, you get really poor scaling. You know, really poor scaling, and you're actually not able to even converge on the finest mesh resolution, whereas we're able to get a very scalable NPAS level. I'm almost done here. So, of course, we want to make this portable. We've been intentionally kind of holding off on the portability because we've been waiting for the Trilino's AutoStack to mature to integrate COCOS for performance portability. So, we're at a point now where kind of all the necessary pieces have been ported to Cocos. The semi-coarsening algorithm does not need to be redesigned for GPUs. Does not need to be redesigned for GPUs. So we're starting to evaluate the portability of this kind of now that all the necessary pieces are there in Trelinos and they've been demonstrated to be scalable on GPUs for several other applications, Maxwell equations and compressible flow. A little bit of an advertisement here, so we're going to be looking for a summer intern to help with some of this work. So if you have students who may be interested, we can definitely talk about this offline. Another little bit of an advertisement, I've almost done. Advertisement, I'm almost done. So, if you're interested in this topic of the portability, Mauro Perigo and I will be organizing a session on computational methodologies for next-generation climate models at ESCO 2020, European Seminar on Computing. It'll be in Pilsen in June. So if you're interested, we can make sure you get invited to that as well. So, summary: so, we're making progress towards portability using cocos and trolinos. And these are some of the discussion points that I think we could talk about. Today and tomorrow. Of course, COCOs and these similar libraries are not a magic bullet. Some algorithms need to be redesigned for GPUs. You're not going to get around that with COCOS. There's a question of how feasible it is to port non-C ⁇ D codes to COCOS. There's obviously more of a challenge there. You're always going to have some trade-off between portability and performance. So, really, to get the best performance on a GPU, you may need to do some architecture-specific optimizations, even with Cocos, if you really. Even with Cocos, if you really want to optimize that. Relying on libraries can be both a blessing and a curse. As I said before, your code can speed up 10 times overnight or it can slow down, and that's something you have to deal with. Regression and performance testing is very critical to maintain performance. We've spent a lot of time doing that. And we are potentially interested in other solvers for GPUs besides the multi-grid. We've done some work with Professor Eric Darv at Stanford on hierarchical solvers and looking. At Stanford on hierarchical solvers, and looking at those. So I'll stop here. Sorry that I went a little bit long. Yes.