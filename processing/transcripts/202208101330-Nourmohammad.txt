I'd like to thank the organizers for inviting me here. The talks have been quite stimulating and these are things I haven't been thinking about for a while, so it's great to be back in sort of information theory community. Yeah, so I'm gonna tell you a little bit about some of the work we're doing currently in the group, sort of thinking about immune system in a very sort of high-level In a very sort of high level. And given that the organizers kindly give me quite a bit of time, I wanted to first tell you sort of the broad pictures we are interested in and then somehow that segue to what we're going to talk about today. Yeah, so we're sort of a theory computational group in the Department of Physics at the University of Washington. And in the past In the past five or a bit more years, I've become very interested in studying the immune system and so thinking about it from an evolutionary point of view. So we use statistical physics methods to study evolutionary processes and in particular these days with a lot of focus on immune system. And so the adaptive immune system, I'm not going to teach you too much biology, but I'll give you some sort of points because I don't expect many people. Points because I don't expect many people know about TA, but it's a fascinating system. The main actors in our adaptive immune system are B cells and T cells, and probably everyone heard about them by now after all this pandemic. So the unique thing about them is that in our body, we have a diverse repertoire of these cells, very diverse receptors. So basically, every cell has its own receptor. Well, crudely speaking. Well, but crudely speaking, so this diversity of receptor that is generated throughout our lifetime allows us to counter the diversity of pathogens that we see around us. So as a result, we can mount the specific responses to pathogens we deal with, and we also keep a memory of previous infections to act more efficiently on future imparties. So, what we are interested in is more or less three passes to understand the system. Passets to understand the system, to understand how immune repertoires are organized and encode information about pathogens, looking at immune pathogen co-evolution because this is a very dynamical system and it evolved over our lifetime as the pathogens around us evolve. And also using this understanding to build control approaches to use immune system, harvest immune system, to control quality pathogens. Just to give you some recent examples of the works that we've been doing. So I said with both theory and computational group, more or less every sort of topic we touch upon, we do both modeling and also data-driven work in that area. So for example, in the organization part, so to understand the immune system. We've been looking at sort of sulfate decision-making in immune system, so still looking at these things, how memory and affector cells are written silent. And so we've already seen the one. And so we've already seen the Waddington landscapes for cell-based decision making, and so we are particularly interested in that for immune cells. And on the data-driven side, we've been looking at learning basically immune and protein shape spaces and how to learn these things from a lot of sequence data that we have these days, and particular structured data, many other things, and developing sort of physics-based machine learning, interpretable machine learning methods to. Critical machine learning methods to get at these problems. Oh, I learned finally option down here. So on the covolution side, we're interested in immune pathogen co-evolution. So that's a good example of Alice and Red William running after each other. And you can think about immune pressure on pathogens. So something is driving pathogenic evolution. And then the escape of pathogen results in changes in the immune system. Immune system. And on the data side, we look at dynamics of immune repertoires and building statistical inference techniques to understand, for example, in the context of different diseases, acute and chronic infections, COVID is acute, HIV is chronic, how immune repertoires change within individuals over time. And on the control, so we heard a beautiful talk today in the morning about controls. If you're also interested in the theory of adaptive control for generally Theory of adaptive control for generally evolving populations in a stochastic process. And so, this is an example of sort of one of the recent work we did on building optimal control protocols to drive populations to have pink cows that would otherwise will end up getting white cows. Natural selection, so how to sort of encode artificial selection by monitoring the population and adding feedback to it and sort of steering evolution. And sort of steering evolution. I've been using these sort of control approaches to design therapy trials for HIV. So that's very collaborations with clinical groups that are actually doing broadly neutralizing HIV trial design. And today, oops, well, typical. Alright, so today I'm going to talk about mainly theoretical work and mainly self-aid decision making in the immune system, but Immune system, but on a very sort of top, sort of high level, so not many details of the immune system. So, before that, the work I'm going to present was basically done by a PhD student in my group, Oscar Schnock, who graduated recently, and the collaboration with a good friend who caused it. All right, so please do I ask questions throughout the talk? I think that makes it. I feel that there's some perception here. Alright, so let's start with memory and let's go sort of beyond immune systems. I'll start from general biological system. So the first place we know about memory is cortical memory. So our brains, they generally have memory. And so we deal with immune memory, in biological systems, generally we use memory to sort of operate. To sort of operate efficiently in biology. And so we also have physical memory, for example, in allosteric networks of proteins, so they keep a memory of interactions with peptides and various things. And so inspired by memory in biological system, we have now all these artificial neural networks that use memory to actually do computation. Now, it's all encoded for memory, but they have different strategies to do that. Different strategies to do that. So, there are different encoding strategies, and sort of the question we were trying to ask was: why different systems use different memory encoding? And so, to be specific, I'm going to sort of highlight two of these systems. One is the olfactory cortex. So, basically, when you smell something, olfactory cortex is a bit different, it's evolved differently than the rest of your cortex. So, there's a direct connection between your nose and your brain. Between your nose and your brain, because it's very old. And when you smell something and you try to encode the associated memories, in this case, you smell a red wine and you kind of associate it with a sunset view, there's quite a red wine there. And so it's sort of encoded in this sort of synapses of the neurons in a very distributed way. And so you get distributed associated memory in your brain. So that's a good way of encoding memory for robust. That's a good way of encoding memory, more or less. Again, very simplistic description, so there's a lot of details here. I'm not going to go into it. And then immune memory is a bit different. So I mentioned these B cells and T cells, and you can have memory version of B cells and T cells. And so if you encounter a pathogen and you have a good receptor against that pathogen, so you sort of keep that receptor and the cell with that receptor, and we call it memory cell in that case. Call it memory cell in that case. And so, generally, if you have a memory against influenza, you're not going to do very well with that memory against SARS-CoV-2, and that we all know quite well. So, there's some cross-reactivity across different variants. So, the original COVID vaccination was working somewhat against Delta variant, but then the memory veins. So, Omicron was a different piece. And so, but generally, so you get different variants that you can. Generally, so you get different variants that you can encounter, but across diseases, it's very unlikely. So, this kind of memory, I would call it compartmentalized memory, and it's more specialized, not distributed like all types of context. Yeah? Do you have any avoidance? This is actually sort of a visual platinum as opposed to a stroke. That's the topic of the colour. Alright, so why do we have these different memory encodings? So, the hypothesis. Memory encoding. So, the hypothesis that we had was that evolution of the pathogen is actually the thing that makes mentalized memory more optimized and distributed. So, it's actually driven by the fact that we need to deal with moving targets. So, that's the end of the story. So, you can just make a suggestion. I think the word memory has actually gotten us in a lot of trouble. I think we should use experience or encounter or something. Decounter or something because you know, immunologists are trying to explain to the public, and for that matter, to physicians, why somebody who's been out vaccinated four times still gets COVID two or three times after that. So the use of the word memory makes it sound like people don't think of memories as something that changed. So we probably should use a different term. I maybe agree with you, but I'm going to use memory because if I start using experience, I'll mess. Because if I start using experience, I'll mess up my talk. My memory is not that great. Yeah, so yeah, alright. For the purposes of this talk, we all agree that memory can also fail if there's a new pathogen that is quite different from the original one. We fail. Even with the original one, the the sort of memory vanes over time and it's different against different pathogens. Against different pathogens, so and that's a complicated story if I would chat about it over dinner. Alright, so that's our hypothesis, and so let me just build to it. So what do I mean by evolution? So on the left-hand side, you have this sort of abstract antigenic space. A pathogen is evolving, that's what I'm going to claim. And on the right-hand side, you have abstract sort of odor space, so for a specific odor. Specific order. And what data kind of me, so things move. And what I mean by evolution is if you look at antigenic space, you have this sort of directional change and things get far from each other. Whereas in other space, my claim is like things just fluctuate around a point. And this is sort of supported somewhat by very limited data that we have in both spaces. Have in both spaces. So, for evolving packages, this is a multi-dimensional scaling map for influenza, H3 and 2 strain of influenza. And the different colors are basically corresponding to different strains. And then you see the years, and the distance on this map is really antigenic distance. And so, as you can see, as years proceed, you know, things get further up further. So, there's some directionality there. Whereas in other space, you take the smell of musk. You take the smell of musk and lily and grape, and there's some variation in that, and that's sort of spreading the space, and there's not much directionality. That's the idea behind it. There is fluctuation, but not necessarily directional. And we're sort of simple-minded physicists, and we wanted to build a model that we can understand with pen and paper. And we already heard about Hofffield in the context of kinetic proofreading. In the context of kinetic proofreading, and so this is a different celebrated paper by John Bopknow than Hoplin Networks. So, the idea of Hoplin networks, which was one of the first networks that led to car neural networks that we have, is let's say I'm showing, so it's inspired by a nervous system, but I'm going to modify it to move it to immune system or whatever. So, I can, I have a network with connections. A network with connections between its nodes, right, called Jij. And I can show patterns to this network. And so here is n patterns, and these patterns are binary, plus one and minus one, each pattern, and they're on average orthogonal to each other. So the n different classes. And what happens is that you can train a network, right? You show a pattern to the network, you update the network with some learning rate lambda, and the learning rate, the way you And the learning rate, the way you update it from the previous time point to the next time point is by just changing the connections according to sigma i, sigma j. So i and j are just the positions in this array of patterns. So the pattern is of length L, I and J go from 1 to L, and alpha denotes the sort of pattern class, right, sigma 1 to sigma, different classes that you have. So that's how you update your network. This is one way of doing it. It's called heavy and learning. This is one way of doing it. It's called heavy end learning, if you will. And so that's you update your memory as you encounter new patterns. Now, the beauty of this model is that it results in associative memory. So you can define an energy, which is basically an overlap between a pattern and your network. And you can use that energy to then collibrate in this energy landscape. And what we see is that the memory of these different pattern classes are stored in energy minima. Places are stored in the energy minima of the landscape. And if I now show a slightly different version of this pattern, maybe with some flips, I can equilibrate using whatever equilibration method you want in equilibrium, I can equilibrate in this landscape doing flips and then fall into the attractor that was the original pattern. And so the performance would be the overlap between the attractor I fall in and the pattern I'm showing, you can quantify. You can quantify. So, yeah, so this is sort of a physics kind of way of thinking about storing memory in order. Now, I told you about evolution, so these patterns generally are static, you can show them one at a time. So, we just added the simplest way you can add evolution to this picture. As you show patterns, the spins can flip plus one to minus one. Now you have classes that are on average orthogonal to each other. Average orthogonal to each other, but within a class you can have variation. These variations are directional because mutations happen on top of each other. So you do this with some mutation rates. Now the new version of the original patent, class alpha, may have superscript alpha, may have a flipped fit, right? And so what you need to do for learning is now you need to also increase your learning rate to follow the evolution of patterns. Learning rate will follow the evolution of patterns, so to be fair, right? And so you can sort of show analytically that the optimal learning rates should scale with the mutation rate that you're putting in, the square root of mutation rate that you're putting in. And I call mu effective as mu n, which is the number of patterns. So n would be basically the time, the characteristic time you see the same pattern classifies because you randomly are showing these patterns to the network. And so this is basically an. And so this is basically analytics versus numerics of how this lambda star should behave as a function of mutation rate. But then if you increase your lambda according to this optimal protocol, you still end up getting a reduced performance as you increase mutation. And the performance is this thing I defined. The attractor, the overlap between the attractor and the pattern you showed. So this was puzzling, right? So the dashed line here is the optimal performance you could. Line here is the optimal performance you could potentially get because as you increase mutation rate, you're always lagging behind evolution, so you're not going to completely recover that. So that's basically the optimal you can get. And the lines are basically what you get if you follow these protocols of an optimal learning rate for different number of patterns for the network. All right, so some investigation later, what we Investigation later, what we figured out, and I just so mu effective is mu times n, and this is, for scaling purposes, would be square root of mu divided by n. So the optimal goes as 1 over square root of n, n is the number of categorical masses. Okay, good. Right. So some investment. Right, so some investigations later, what we figured out was actually what's happening in these networks. As I'm increasing the learning rate, I'm distorting this energy landscape. And what I'm exactly doing is I'm connecting these energy minima to mountain passes. So instead of equilibriating in my energy minima, I could actually just walk on the mountain pass, you know, stochastically, and just equilibrate in the other one. And so that's the thing that tends to happen. Tends to happen in these patterns. And it's basically by increasing learning rate, you're interfering in the memory. So the different memory could interfere with each other. So that was a key thing. So what do we need to do? We need to separate these memories, these energy minimas, so that they don't interfere with each other. Well, there are many ways of doing that, but the simplest way to do that is to really compartmentalize these networks. So instead of looking at the whole sort of general Instead of looking at a whole sort of distributed network, I can have now blocked diagonal network in a sense that each sub-network does not anymore interact with the other sub-network. And now the equilibriation or sort of finding the attractor is two-step decision. First, you find which compartment you belong to, and then you equilibrate in that compartment to the attractor. So now we have two different temperatures. The first one is associated with immersed temperatures, beta. Merse temperatures beta. The first one is associated with this specialization or finding the right subnetwork. And the second equilibriation temperature is the helpful temperature where you equilibrate in your sub-network. Now let's see what happens. So here in different colors I'm showing, this is sort of looking at patterns, 32 patterns that we have, and you can Patterns that we have, and you can start off with completely distributed networks, so a simple compartment, and then you can go all the way to have 32 compartments, so one compartment per pattern. And what you end up getting here is this is now performance as a function of mutation rate for a different number of compartments, and so you can achieve the optimal for a one-to-one network. So everything else is basically inferior in the regime of high mutation rate. For low mutation rate, all networks are equal. Right, all networks are equally good, so we don't actually care. Is there a cost in the sense of like you have to have more neurons when you do this one-to-one compartmentalization? So we do the, yeah, so we do a scaling so that you keep the number of neurons the same for when you're doing the sort of compartmentalization. So you only compare the large networks with the sub-networks, so you do this scaling appropriately. So this is taken care of. Improperly. So, this is taken care of. We are a reviewer. So, alright, good. So, what do we see? So, we can now look at some sort of a phase diagram basically for these processes. In the static pattern situation, right, so again, the colors up here are different number of compartments for the network. In the static pattern, there's no evolution, and what I can see here is And what I can see here is this is basically a heat map showing the performance as a function of the two temperatures: the spatialization temperature, finding the right compartment, and then the hot field temperature sort of going down. So inverse temperature in both cases. So high one, higher accuracy in both cases. And so what I see here is that in a sort of static pattern situation, for all network compartments, we've got Network compartments, regardless of beta s, I have a very accurate memory, I can find the right energy minimum. And then, as I reduce the sort of inverse hopping temperature, I go to partial memory and no memory. And the solutions here, the optimal strategies corresponding to these memory strategies are, you know, if both beta S and beta S and beta H are high, all memory, all types of compartments are equally good. Types of compartments are equally good. For small beta s, we only use distributed memory, so c equals 1, and then everything else we have the grade. Now, if you look at evolving patterns, what you end up getting is the following. So the phase diagrams look a bit different. You can only get accurate memory in large beta S on beta H, right? And so that's basically, you need, and that corresponds to one-to-one. And that corresponds to a one-to-one specialized memory. So you need one compartment for one pattern class. So that's sort of the red part. And then if your beta is small, meaning you're very inaccurate in finding the right compartment, then you can only store a partial memory. And in that case, you don't use intermediate compartments, but rather you go to a fully distributed network. So this transition is quite sharp. So you're either using So, you're either using highly specialized memory or you're just using the secretive memory. So, this is more or less our speculation in a way, or our postulate in a way, so that why immune system and olfactory cortex are different from each other in terms of encoding of memory. So, the smells that we encounter contain rather static molecules. Contain rather static molecules in them and they're not evolving. And so distributed memory is a perfect solution for dealing with this kind of information. Whereas for immune system, we deal with constantly evolving pathogens and we need to sort of track them. And so once you store these things in recipient memory, they start interfering and nothing works. So this is sort of a short story and I kind of go to more focus on immune system. It's more focused on the immune system in a minute, but if anyone has a question about this part, please ask. Otherwise, we'll move on. Yeah? Wonder the difference between the associated memory and the one-to-one. Is there another factor that should come in here that is the associated memory, we can pass the threshold of the number of things to memorize, there will be a catastrophe. Exactly. So we are below, so that's the capacity of the network, right? So Hopper network is not. Right, so Hopper network is not, you know, it gets glassy when you're above capacity. So everything here is below capacity. In principle, everything should work. So we're not in the spin glass. Yeah, again, more like a biological question, but how would it, in case this was now not a moving target, can you kind of explain of how the immune system would look like then? If then you would predict right then they would also kind of have that. So how would that like biologically Have that. So, how would that like biologically look like? Good, right? So, that's good. You can imagine everything. So, you can think about having all like cell networks, right? So, the immune system does have cell networks, cells communicate and cooperate with each other. But you can have a very strong, so instead of having single cell to find a pathogen, you can have a network of cells that would then encounter many pathogens, like whole nervous system and now for immune cells, why not? And it's pretty robust in a sense, right? So, if you think about it, So, if you think about it, our memory, our brain memory, works much better than immune memory. So, it's like electric car versus like you know, old-fashioned cars, right? It's like when you deal with a lot of chemistry, the way that immune system works, things get messy. So, if you have a way of doing things digitally, why not? That would be pretty optimal. So, uh I know you used the example of olfactory cortex you just alluded to the brainwork. You just alluded to, Brainware. Barely well-known examples of people who've had head trauma or tumors or whatever, and they lose the ability to speech and then they regain them. The question is, how much of that is new learning and how much of that is distributed memory? I think model of distributed memory, the way I described it, you know, how it corresponds to how really the memory in the brain, sort of. The memory in the brain, so learning works is questionable, I think, or debated, so to speak. So, I don't know the answer. The closest connection is olfactory cortex. So, biologically, if you think about it, this is distributed memory. I think the closest example is olfactory cortex. But how real memory, memory, remembering works, I don't know. I don't think this is a good model for it, actually. Are there examples in the brain where you are learning? Where you are learning a varying environment, and as a result, you will grow compartmentalized in groups. It's not a fully distributed network in the brain. You have compartmentalized regions in your brain, and things are really sort of stored in different places, and you don't use all the yeah. So it is there. I I just don't know enough neuroscience to actually give any any wise uh anything wise then, yeah. Anything wise then? Yeah? Yeah, I hope it's related. So, for example, our brain is integral of HTTP columns. That would be the border for 100 of HTTP. When it comes to learning, for example, if you have a certain number of patterns, is there a way to find an optimal number of learners that the rate of learning is optimal based on it, or the bigger the better? No, it's not always the bigger the better, right? Because things can get lost because you have to retrieve it, right? Get lost because you have to retrieve it, right? That's a very good question. I don't know the answer to this. I suppose there may be an outcome. Yeah. All right. No more questions, so let's move to the immune system. So we're going to look closer at the immune memory. I'll teach you a bit of immune system now. So we're not going to do the whole hand wavy. This is olfactory cortex. Okay. Okay, so if I give a typical talk that I would give, these slides will be sort of the first slides of my talk because a lot of work we do is trying to make sense of a new repertoire sequence data and so building statistical inference there. And so let me tell you how the repertoire is formed first. We have these receptors, so this is let's say a B cell, and this is a receptor on the surface, and so you encode. The surface, and so you encode the receptor sequence in your DNA. But you do that by doing a thing called BDJ recombination. You take a, we have a whole series of V genes and J genes and D genes in our chromosomes, and you basically take a single V gene, a single J, single D, you put them together. So that's combinatorics. And then you add some pieces of DNA in between at the junctions, so insertions and delete some. So, insertions and delete some, insertions and deletions. And so, that produces a whole repertoire of sequences that you can have. And that process is called BDJ recombination. And so, these receptors are pretty random. A lot of them are non-functional because things go out of frame. And then you select the ones that are in frame. So, those become potentially functional cells. But then, what they go through this process of selection. Selection, so this is generation, and then you go through selection. For T cells, this happened in thymus, for B cells, happens in bone matter basically. And what they are tested against is they're tested against cell peptides or cell proteins. And so if they bind really strongly, they're gonna k get kicked out because that would induce autoimmunity. And if they bind if they don't mind anything, they're probably useless receptors. And so you also throw those things out. So, you also throw those things out. And so, you basically throw out 90% of the things that you generate up here, and you kind of keep things in the middle box. And so if you look at sequences, you can learn models of generation and selection of these receptors. So, doing the inverse inference. And so, you get, you can show that you can generate up to like 10 to the 18 different sequences in your immune repertoire. Our B-cell repertoire is about, you know, we have about. Is about, you know, we have about 10 to the 12 B cells, so we can generate way more receptors that we have cells for. So that means if I compare my repertoire to someone else, if you compare two identical twins to each other, the overlap between their immune repertoire is very small, right? So it's highly understandable. So that's just an interesting fact. Now, B cells on top of that, so this happens to both B cells and T cells with slightly different numbers. B cells on top of that, V cells on top of that, if you have a V cell that marginally binds well to a pathogen, and so you get infected with that pathogen, you take the V cell, take the antigen or pathogen to the lymph node, and starts a Darwinian evolution inside your lymph node that can last for, you know, for COVID actually, this was sort of surprising. With COVID vaccination, this process lasts for six months, but generally it lasts for a few weeks. It lasts for a few weeks. And that's a process called somatic affinity maturation. So you can mutate the receptors and then test them against the pathogen you're dealing with. And you sort of only select the ones that are increasing the binding affinity. And with this process of affinity maturation, you can increase the affinity up to 10,000 whole. So it's pretty efficient. And you basically produce these genealogies of B cells in your body that are related to each other through this. Related to each other through this affinity maturation process. So these are sort of the sort of genealogies, and you started from a naive cell and you produce different types of cells. So the pictures down here are multi-photon microscopy from Gabriel Victoria's lab. So this shows a lymph node and an infection with some immunoglobulin, so chicken immunoglobulin, which is a model antigen for BCL studies. And as you can see, so what they do is this rainbow experiment. Is this rainbow experiment? So they basically color every receptor somehow with some recombination, magic. So each cell is basically colored differently, and this color is heritable as they replicate. So as you can see, is that you start out with a complete mess, right? So there are many different colors here. That's a day tree after infection. And what you see there are these regions in the lymph node that are formed, and then different cells basically take over, right? Yep. Right? Yeah. Is this only point mutations, or this is also Vitamin? This is only point mutation. Vitiger recombination is ended. So sometimes you get larger indels in it, but primarily it's VDJ recombination. Point mutation, sorry. Right, so then you get these sort of clones that win, right? And now you also get reinfections, right? So day 11 after reinfection of this mouse. After reinfection of this mouse, you get some of the old cells coming up, but also some new cells. So you have a sort of mixed memory and a novel response and competition between cells. And that's a fascinating topic that we've been looking at as well. But generally, out of this process, you produce some memory and some plasma that would then go and react. Okay, so this is more or less demonology we're going to talk about. About. So, the question we were interested in is: how do we store immune memory? What would be an optimal way of storing a new memory if I were to select somewhere in this progeny, some of these cells become memory, which ones I should choose? So, what's unique about memory? You know, the point is you get vaccinated to act better next time, right? So, why is it like that? So, there's some chromatin modification that happens in this memory. That happens in these memory cells, that what it does is it makes them respond faster, right? And also, they proliferate to a larger amount. And so, if you put numbers together, is that if you compare a novel response to a memory response and ask how long it takes for a novel response to reach a level memory it would reach, it's about, you know, two to five days. So that's a delay, you can think about it, or a time lag time lag. So genetically memory is just faster. Energetically, memory is just faster and more effective. And energetically, that's the puzzling part. So, if you think about this affinity maturation, this progeny that you're forming, initially your cell is a naive cell, so it's generally have low affinity and at the same time is also kind of cross-reactive. It's a floppy receptor, generally. And then, as you add mutations, it becomes more rigid and it becomes more specific. So, if you think about the binding affinity. Think about the binding affinity of the cell to different antigens on the x-axis. First, everything is of low affinity and broad, and then things become more specific and more focused. You can store a memory anywhere along the line in principle. And generally, plasma come quite at the end. Plasma are the things that are antibody factories and would actually go and battle the infection. And what we have seen over years is that memory, on average, is of lower affinity. On average, it's of lower affinity compared to plasma. And so you can ask why you want a low affinity memory if you can choose anything here. So there has been two hypotheses in the field. So the first hypothesis, this was the long-standing hypothesis, that there's actually a selection for plasma cells as the affinity maturation falls. So you want high affinity to plasma, but there's not much selection for memory. So on average, memory is a lower affinity. But now, Worth it. But now, more sort of experiments come out and they have shown actually there is active regulation in memory formation. I'm not going to talk about this regulatory network, but there's active regulation that makes sure memory is of low affinity. So you're actually selecting for memory to be low affinity, and that seems to be something beneficial. So why is that? So here is a sort of our very heuristic model of the system. Heuristic model of the system, and we were trying to basically answer this from an evolutionary point of view. So, you start, you get infected with a pathogen, you go through affinity maturation of your B cells, let's say, that amount to a few weeks of, you know, one or two weeks of sickness until you get your plasma cells that produce antibodies, attract the infection. Somewhere along the line, you produce M-memory. Then, maybe one or two years later, you get reinfected with the. Years later, you get reinfected with a slightly different variant of the same virus, or these days, maybe a month later. So, you get a reinfection, and so you can choose between using a memory and mounting an immediate response against this immune pathogen, or mounting a novel response and go through the one or two weeks of signals. So, there is sort of an in-between thing, but these two extremes, we looked at the two extremes, so I didn't do this or this. Extremes, so you either do this or the other, and so doing the in-between doesn't cause the change, sort of. So, the good thing about memory is that it's fast, the bad thing is that it will be of low affinity, right? Because first of all, the pathogen has evolved, so it's a different pathogen. Original memory was centered around the original pathogen. Now, it's not only for one reinfection, in general, you want to optimize things over lifetime of the organism. Lifetime of the organism. So you want to basically get the largest utility you can get out of your memory over a lifetime of an organism. And that becomes an evolutionary sort of arms race with the package. So this is sort of optimization problem that we set out to solve, right? Okay, so how do we find a 3D? So as I said, for receptors, what happens is during affinity maturation, like if you look at their protein structure, Like, if you look at their protein structure, salt bonds form, things become really rigid. So, as you go down here, initially you are cross-reactive and low affinity, and as you get affinity maturation, you become very rigid, high affinity. And this is not always true, so there are receptors that don't do this, and they remain cross-reactive, but generally true. And so there is some sort of a trade-off between affinity and cross-reactivity at the receptor level. Reactivity at the receptor level. And the exact shape of this trade-off is not important, but we just assume that there is a trade-off. And that's basically the only assumption in the model. So we have affinity versus cross-reactivity, and a memory that lies on a line, basically high affinity, low crust reactivity for both five crusts. Alright. Now to model this sort of To model this sort of trade-off, we assume, in a simplest model you can assume, like something like a Gaussian function. Basically, the height of this function would give you the maximum affinity that you have, and the breadth gives you cross-reactivity in this sense. You can do it a bit more generalized, so instead of power 2, you can have power theta, so you go from a bell curve to like a top hat, right? It doesn't really matter. And what you end up getting is basically a curve that would reflect. That would reflect how you change in the shape space, pathogens, and the sort of variance, one of a variance would become specificity inverse of cross-reactivity. Okay, so let's put together the model. So, first of all, you get infected with a pathogen. You have this curve that shows how you're interacting with pathogens nearby. So, the x-axis. Nearby. So the x-axis is kind of a weird axis. It's called antigenic space or antigenic distance. And the idea here is we have this dual space between antibodies and antigens or pathogens, immune-shaped space. So if you're far in pathogen, pathogenic or antigenic space from each other, then you can't find an antibody that blinds both of you. And if you're close, then you have common antibodies that react. Have common antibodies that react. So you can go forth and back between the two. And so that's sort of the space we operate in called shape space. And so the variance or one over the variance would basically decline the specificity of your response. Okay, so that's our affinity character. Now I get reinfected with a new pathogen at the antigenic distance delta. So if I use the same memory, which has the broken. Same memory which has this profile, right? My interactions with the memory, my memory will have an affinity that is a bit lower. That's this Em. And so that would be whatever response I get. Or alternatively, I can mount a new response, and that would be just a new curve centered around the new pathogen. And so we can define a memory cost, which is the sub-optimality, energetic sub-optimality, so the delta E. Optimality, so the delta E in my response. So that's just you can calculate it. So that's one cost. The other thing that you get is on the kinetic side. So I said memory is mounted faster. So you can characterize the probability of a memory response to a pathogen. If I have, you know, this, I said this deliberation time of two to five days until a novel response kicks in. So the probability that the amount of memory So the probability that you mount the memory is one minus the probability that you don't mount the memory, and that you can say should be exponential of some integral that up to that deliberation time of two days, you encounter a virus and get activated. So viral encounter rate, call it gamma, right, and depends on the number of viruses you have in the body and they replicate and they grow, so it grows over time during infection. And there is this binding affinity that basically tunes your Affinity that basically tunes your recognition. And so if you have a very long deliberation time, even a very crappy memory would mount a response because this integral would still get bombed, right? So then higher chance of memory response in longer deliberation, but if the memory happens to fail, you're giving the virus a lot of time to replicate during that time, and that comes with a cost, right? So a longer deliberation. So, a longer deliberation, if memory fails, it results in a high cost for mounting a novel response because the virus grows, so virus replicates, so a naive or novel response should be monotonically costly as a function of this deliberation. So, these are sort of the ingredients of the model. I have a cost for memory response, which is sub-optimality, and a cost of naive response, which is due to accumulation of the pathogen during an infection. Pathogen in an infection during an infection. Any question on here? Yeah. What was EM delta yet again? What was the last term? Em delta. This. This. In the integrated. E sub m. Delta, so it's this thing, right? E m of delta. Okay. So there's this very interesting mechanism that we weren't talking about viruses, we were talking about virus. We weren't talking about viruses, we were talking about bacteria. Do you know about mitosis? This fascinating thing where if you get infected by Staphylcoccus aureus or other bacteria, and there's actually, the neutrophils actually condense their DNA, break it up, and then secrete it and form neutrophil extracellular traps, which are negatively charged and bind to positively charged bacteria. Charged bacteria. So they concentrate on the pathogen. So you talked about, you know, time for recognizing the pathogen. So we've evolved separate mechanisms that actually can influence some of these parameters. Yeah, I mean, okay, so these parameters are actually coming from real data. So it's effective parameters that take into account whatever neutrophils are doing. So it's not that pathogen is diffusing your body until you encounter it. Your body until you encounter it. You're right. So you're concentrating pathogens and focusing immune system on the concentrated pathogens. Otherwise, it takes way longer than that. So are you assuming that naive response will not start while you deliberate? So in the model I'm presenting here, yes, but if you assume that the, I mean, naive response always takes a bit longer to actually, but if you, but, you know, it's not But if you, but you know, it's not zero during that time. So if you have a small naive response or a slowly counting naive response, result doesn't. But then deliberation time doesn't matter, just starting up. If it's needed, it will count it. It's not interfering. No, so they interfere with with each other basically what happens. So your memory is now much higher at that point and they can interfere with each other. So it'll move the time a bit shorter. Move the time a bit shorter, but naive is very slow, basically. That's what people can say. All right. Okay, so these are sort of ingredients of the model, the cost factors in the model, basically. And this crowd, we wanted to sort of map it to quantities in thermodynamics. And there is sort of a There is sort of a natural mapping, in a sense. You can define the utility of these memory and naive factors, and the utility would be basically: our memory would be the goodness of memory minus the badness of naive, and the utility of naive is just the negative omega. Omega was this cost, this monotonic cost function that I showed. And then, probability of mounting in memory would just be some sort of sigmoid function with a factor of energy factor that is. Factor, energy factor that is delta u, the difference in the utility of the two. You can think about sort of free energy as expected utility, probability of mounting a naive times utility of naive plus probability of mounting a memory times utility of memory. Then you can define something like dissipation, which is the sub-optimality of response. So if you go back to this picture of mounting response against original versus secondary infection. Secondary infection. You can have some effective response, which is just the weighted sum of the two responses. That's the average response, we call it epsilon effective. And you can have this novel response that is pretty optimal for the second infection. And the Kolbach-Libber divergence between the two, these are not quite a distribution, you can think about distribution in the antigenic space. The Kolbach-Libber divergence weighed by a factor beta, which relates to Data, which relates to your deliberation time, quantifies the situation. So, this is sort of interesting mapping that you can do, and it is useful when you're thinking about sort of decision-making in a non-equilibrium setup. So, you know, theory of decision-making goes back to John von Neumann for equilibrium systems, so in the context of gambling, and then more recently, the group of recently the group with Brown and with Graham Moya so they did sort of non-equilibrium version of this sort of mapping these ideas through thermodynamics. And so in non-equilibrium decision making that we have here, it's non-equilibrium, it's driven by evolving pathogens. You basically you make a decision in at each point to mount a memory, basically exploit your existing memory or mount a novel response or explore a new space. Response or explore a new space. And these are the factors that would factor in: free energy, the average utility, and the dissipation, these callback laboratories. And so the maximize, so what we tend to do with basically, you're trying to maximize the net utility over time, which is your free energy minus dissipation, plus extracted work. And by optimizing this over the lifetime of an organism, you can The lifetime of an organism, you can find the optimal specificity and optimal deliberation factor. So, these are things we sort of did mechanically to get to that. And so the problem with it is we looked at various levels of antigenic divergence. So, the working hypothesis is that organisms basically encounter pathogens around them and they gather statistics about the pathogenic evolution and they tune their biochemical machinery to be optimal. Machinery to be optimal or close to optimal to deal with the pathogens they expect to encounter in their lifetime. So if I see that you know I tend to encounter pathogens with some divergence from each other, so that's the factor that you put here in some normalized way. So the larger, the more, the faster evolving pathogens. And so what you're seeing here is a heat map of net utility, and these are the optimal solutions. These are the optimal solutions. Specificity basically drops as you increase the antigenic divergence. So you want less and less specific receptors. You want things to be cross-reactive. And you also increase your deliberation factor because as you become cross-reactive, you become a worse receptor. So you need to wait longer to actually utilize your memory. So you give it more time. So basically, your strategy should depend on anticipation of pathogenic evolution. Anticipation of pathogenic evolution. And if you then sort of use that dissipation factor, but take it a bit too seriously, you can then say, okay, in a very small evolution, divergence of pathogen, I generally have no dissipation. The memory I use is pretty perfect because it matches the next pathogen. So I would call that non-dissipating strategy and it's equilibrium memory. But if I look at this sort of intermediate pathogen, In sort of intermediate pathogenic evolution, my memory is dissipating. I'm using this time that I gain to actually use a bad memory to mount a response. So you have a large dissipation, and that's this region. And that's what we call non-equilibrium memory. And if pathogens evolve really fast, basically there's no point on storing memory. And so the optimal strategy against evolving viruses to use dissipating sort of memory or non-equilibrium memory, and we are Memory or non-equilibrium memory, and we are dealing with basically a speed affinity trade-off in this problem. Okay, so this is sort of the gist of it, and there are now sort of predictions based on the model. So, first of all, you know, we don't deal with one type of pathogen, we deal with a whole spectrum of pathogens. So, there's a difference between chickenpox and influenza, as in chickenpox doesn't evolve that much, whereas influenza evolves quite fast. So, now if I give an immune system, So now if I give an immune system basically a whole distribution of pathogens, a flat prior on pathogenic evolutionary rate, what kind of strategy would be optimal? And the solution is interesting. It's not that you have to match every single evolutionary rate. Rather, you have a bimodal solution. So these are these purple ones. So you store a fraction that is very specific for basically non-evolving patterns. For basically non-evolving pathogens, so the equilibrium memory, and they also store somewhere in the middle an intermediate sort of non-equilibrium memory. And the non-equilibrium memory really doesn't need to match the rate of pathogens. It's very flexible in a sense. And so this seems to be sort of consistent with how immune system operates. So every time you get infected with something, you don't know a priori what type of pathogen in terms of evolutionary rate it is. Pathogen in terms of evolutionary rate it has. And so you actually store two types of memory in sort of jargon called IgM and IgG, class switch memory. And so the IgM memory are the ones that are quite cross-reactive, and the IgG ones are the ones that are very specific and very focused. So that's what we get. The other thing that we get out of this analysis is the strategy that you have should also depend on Have should also depend on the life expectancy of the organism. So, what we're seeing here is that if your life expectancy is sort of long enough that you expect to see evolution evolving pathogens, so ignore this part because this is too short to have evolutionary pathogens. Life expected antigenic divergence is below one. So, if you're above one, what you end up having is basically for short-lived organisms. For short-lived organisms, you need to store cross-reactive memory, more cross-reactive memory. And the reason for it is, you know, the pathogens are evolving, and before they evolve too far, the short-lived organism is going to die, so it's not going to care about it. So it's better to use a cross-reactive memory that amounts of rapid response rather than having to redo memory storage. And so, in this case, fewer pathogenic encounters. Fewer pathogenic encounters lead to optimal memory that should be cross-reactive in your lifetime. And so that means that if I line up a bunch of organisms with different life expectancies, I should see that cross-reactivity of their memory should go down. That's something we're trying to look into this later. And the other consequence of that is if the organism's lifespan just changes dramatically, as it did for humans. Humans. Then, in a way that we don't have, we haven't had enough time for our immune system to evolve its biochemical machinery or biochemical program. So, what do you expect to happen? You expect that our memory is more cross-reactive than it should be. So, if you have very cross-reactive memory, you mount responses to evolving pathogens but of low affinity. And so, that does happen, it's called original antigenic sin, and it's a very Original antigenic sin, and it's a very big deal for influenza vaccinations. So, if you're older and you already seen a version of influenza, your memory could interfere with mounting a novel response and basically make a very ineffective. Well, influenza vaccines are generally ineffective, but make a very ineffective immune response as organisms called age. So, that's something sort of relating to antigenics, original antigenics. Okay, thing with this, I'd like to conclude. So, we talked about. I'd like to conclude. So, we talked about distributed versus compartmentalized memory in the static and evolving environments. And memory strategies, in a sense that they should be actively regulated depending on anticipation of evolving pathogens. Specificity should depend on organisms' life expectancy and lifespan. Elongation of lifespan can, in principle, reduce the utility of your memory. And this And for this, I'd like to thank all the people in the group. So there's now a slightly different configuration of these people, but so especially Oscar here, who has now gone on to do more exciting things, and also Luca in particular. Great collaborate. Thank you. Great, we have time for some questions. This idea that you mentioned at the very end of, you know, perhaps this lifetime dependence is really cool. Do people know in general whether the rate at which you encounter pathogens is kind of, you know, is a roughly constant rate? So let's say you're a tortoise, live 120 years, you're encountering the same, you know, pathogens at the same rate as a human or a shrew or something. Does it really depend on the environment? Depends on the environment, people don't know anything. The baton environment, people don't know anything about these things. Basically, what happens is everyone has their own organism, and generally people study human and mouse sometimes. And tortoises, no one studies tortoises as far as I have surveyed, basically. So encounter pathogens varies across organisms. So yeah, so there is an inherent time scale that you need to normalize and you need to sort of put things in dimensionless scale. And that's the complicated part. That's the complicated part. We have some ideas of how to do it based on the other types of time spells that you have in the organism, but yeah, so we basically have two good data points, humans and mice. And it could have been up or down, and it's so far consistent. So I don't know what we get because we add more organisms, so I will be excited. Um the slide you showed, which was a solution, where you had a small fraction of highly specific and then fairly broad histograms. I'm just kind of trying to understand what the quantification means. See if I go the right direction, of course. Okay. That one, yeah. So the purple optimized. So why is it not wider? Why isn't it not narrower? What are the proportion of the peak at the right end really scaled? I mean, can we say something about the fraction of the immune system that is so? I think, well, okay, so the fraction of immune system, this to this, the height, right, or the integral, right, the integral. That is sensitive to the parameters you put in. The thing on the x-axis is normalized. So it's a specificity normalized by antigenic divisions. So it's a quantity that one is meaningful by sort of some scale of antigenic divisions. So it's sort of zero to one. It's normalized how to quantify it in real data. So when you change your parameters, can you change the integration of the P? Of the peak in the middle relative to the one on the right? How do they depend on the parameters? I think you can sort of change it a little bit, but not huge. So that's kind of comparable in the integral. That's what it is. But yeah, you can say, you know, I showed this theta parameter, for example, that's one of the parameters. Whether your specificity is like a bell curve or a top hat, right? So that's a trade. Top hat, right? So that's the trade-off. So that's basically the only three parameter we have in this whole model. Everything else is optimized self-consistently. And so that changes a little bit, but not too much. So it's a model that doesn't really have many free parameters. Well, it's just interesting because if you think about other systems that we've evolved to deal with threats, not just pathogens, but other types of threats. Types of things. We've shown that they're not very specific. There seems to be some common circuitry that handles everything from radiation to throw both systems. So obviously the question of the shape of that histogram distribution's breadth and so on is kind of relevant and also relevant to other responses that we make. Yeah, absolutely. I mean, yeah, so. Yeah, so as I said, the only parameter we have that we put by hand is this trade-off function, right? And of course, if you change it dramatically, you get slightly different things. So as long as you have trade-off, you get a quality that will be the same shape, of the same weight or this. What happens if you change the prior? Do you get the same quality? No, I mean, this is basically, you know, we start with the flat prior, right? So it's computer most. So it's going to be the most but uninformed. So if you really put a strong prior and adding very specific ones, then you have more very specific ones. So we want it to be unbiased as much as possible. But I mean if you try yeah I understand that part but if you tried like different flyers would you get different I'm just wondering about the dependency positions. Yeah so you would get a different solution right but we don't know what I mean there is no reason to think that you need to have a different well Well, if your biochemical machinery has some inherent, you know, there's a biophysical constraint, then sure, then you can actually move this. Right. So you talked about this human and mouse scenario at the end. I was wondering with the mouse.