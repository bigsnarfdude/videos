I got a lot out of this. I'm glad I could be here. Yeah, so I don't need to convince anybody in this room that biology is replete with information, and this is because living things are complex, structured, non-random systems. Living things generate, integrate, store, and use information themselves as well. Therefore, there are all kinds of ways that we can apply information theory to these kinds of systems. Basically, information theory is a particular Information theory is a particular flavor of describing variances and correlations, and given that life is all about variances and correlations, we can apply this tool in all kinds of ways. What I'd like to ask today is what kind of information is actually useful to organisms themselves? So, we can measure the information content of a genome in terms of its base pairs in various ways, chrysodomics. Its base pairs in various ways. Chris Adami gave a very interesting talk yesterday about doing that. We can measure information in signals in various ways. We can measure information in structure, et cetera, et cetera. What is the informational currency that is actually relevant to organisms and driving the dynamics at the organismal or population level? So, the questions I'll be discussing today is what information is adaptive? What is the value of adaptive information to organisms? To organisms? How is adaptive information acquired? And how good is natural selection at acquiring adaptive information? The first part of this talk, getting at the first three points here, will be more of a perspective talk. I think to people in this room that think a lot about information biology, this perspective will be fairly natural. But I think it's important to explicitly ground information thinking and biology in evolution and what is adaptive information. And what is adaptive information? And I think some points along the way will hopefully stimulate some discussion. And then the last part about the fourth point here will be a bit more meaty. But in the interest of time and also being the last talk on the third day, I will actually gloss over most of the details in math and some philosophy. But if you want to dig into that, we can talk theoretically. So just to set things up, I'm actually going to return to the primordial ooze of pre-life. Ooze of pre-life to think about what selection actually is fundamentally. So we have various different types of things that spontaneously assemble, and these things have different inherent persistences in the environment. So as we see over time, things degrade, and we get changes in the frequency of things that are existing. And the way that this works out, if there's some characteristic persistence to these things, then this is actually natural selection. So nothing's replicating here, but things have characteristic. Here, but things have characteristic growth rates that are negative, if you like. And we see the types that are more likely to persist are those that are more likely to exist. That, to me, is natural selection. And what life is, is a particular innovation where something figures out how to replicate itself and the information that is pertinent to its type. And it turns out that if you're able to replicate your type, that's a good way to keep your type persistent in the environment. Once you get this innovation, everything is about. Once you get this innovation, everything is about optimizing growth-free. So, we can think about there being multiple different kinds of environments represented here by colors. And an organism or a type will have different fitnesses in different environments. And this matching will look different for different types. We can think of the fundamental problem that lineages are trying to work out is figuring out a matching between the types of. And matching between the types of offspring they generate or the types that they themselves adopt, if they're plastic, and environments that they are experiencing. So if you get this matching right, then you have good fitness and you will tend to persist and increase. Here's just a framework for thinking about how organisms do this. There's some environmental conditions, the organism itself has some development and behavior that generates a phenotype, and the fitness of the organism. And the fitness of the organism is some mapping, matching between these two. And so, getting that matching right, you have some information about the environment, this can be quite helpful. So, I think almost all the talks during this meeting have been thinking about various kinds of, what I'm calling here, sensory integration, of environmental cues that can then be leveraged to change any phenotype or behavior. There's another source of information, which is the information you inherit from your parent through your genotype. You inherit from your parent through your genotype. So, this information channel is the one I'm going to be focusing on in this talk. But I also think and work a lot in the sensory domain as well. So it's been very interesting to discuss that with all of you. Right, and so between these two information channels, if you can use that information to match your phenotype, then you will be well off. All right, so thinking about that genetic channel. So thinking about that genetic channel, in super pervasive terms, including in pop culture, the Jurassic Park, everybody knows that the genome is information. Here, Blueprint for Building a Living Thing. We talk about the genetic code that has instructions that are transcribed, translated, copied, proofread, edited, and go on and on. Basically, everything that we think about the genomes as the modern synthesis has to do with information. But our concept of what examples. But our concept of what exactly that information represents is still a bit nodulous. So, what is meant by genetic information precisely? In particular, what is meant by adaptive genetic information? And how much adaptive information is carried by the genome? And how is adaptive genetic information acquired? So generically, information is that which reduces uncertainty. So, really what I'm trying to ask here is what is genetic information reducing uncertainty about in some sense? Reducing uncertainty about in subsides. So we can think about the genome carrying correlational information. The genome or alleles can reduce uncertainty about some phenotypic trait that we observe as of correlation between the allele and the trait itself. Alleles or SNPs can be correlated with geographic regions. Barcodes can be correlated with various observations in a lab population. Lab population and alleles can be correlated with disease. So we might say that the genome is carrying information about any of the things I just said. And in some sense, to us as scientists, that's true. We could also say that the genome carries causal information. That is, the sequence of the genetic information has information about, say, phenotype or any of these steps along the genotype to phenotype pathway because there's some chemo-physical dependence or causality. Physical dependence or causality between the sequence itself and these other things. Critically, correlational and causal notions of genetic information do not necessarily refer to adaptive information. We could have correlational information about something that is explicitly maladaptive, for example, and we could have the same amount of sequence information corresponding to proteins that have different levels of function. Have different levels of function, including maladaptive protons. We could think about some of the things that Gristami is talking about to get at this, but that still is not necessarily getting at explicitly adaptive information. There's another sense that the genome carries information, and that is a transmission sense of information. So, critically, the genome is transmitted from one generation to the next, and there are strong teleological arguments. Strong teleological arguments you can make that this is one of the core purposes of the genome and the way, the reason it is the way that it is. So, Shannon information theory formalizes exactly this transmission sense of information, where you have some sender that encodes something about the state of a system X and sends this encoding to a receiver, who then observes this as a signal. And information being that which reduces uncertainty, we can say that. Uncertainty, we can say that, we can ask how much does the signal Y reduce uncertainty about the system X? So, just as a simple example, you guys are familiar with this. We could say that the sender is observing some playing card and sends a message that this card is a black suit, sends that to the receiver, observes that message, and the uncertainty on the part of the receiver is now reduced by having observed that message. Observed that message, and in this case, that gives one bit of information about the suit of this card. So we could say that because observing y reduces uncertainty about x, we could say that y provides information about x. And we could say that y is a representation of x. So, in the genetic context, we have a cell that's transmitting a message in the form of a genome. In the form of a genome. So parent and daughter are the sender and receiver. And we could talk about the mutual information between the genotype and some variable, some system X, that is the referent of that message. So the question here is, the genome provides information about what? We could say that maybe phenotype is what the genome encodes information about. But there's a couple things to note here. One is the genotype to phenotype mapping is. The genotype to phenotype mapping is notoriously hard to pin down. There certainly is some causal information as we discussed, but to the extent that the genotype encodes information about phenotype, it doesn't encode necessarily a lot. More critically here is this interpretation gets the causality of the transmission sets of information backwards. The sender cannot be encoding something about the child's phenotype because the arrow of time is going the wrong way. Going the wrong way. So genome teleologically must be providing information about something else. I argue that the genome provides information about the environment. And to explain what I mean by that, we can think about the genome in the context of natural selection. So here we have four different counterfactual environments with four different genotypes in different colors, and we can Colors and we can first ask: okay, here's some cell that has not yet chosen a phenotype, and so it has some uncertainty about which environment it is occupying, but it receives some genotype. And the question is, how much does receiving that genotype reduce uncertainty on the part of the cell about which environment it has been born into? All right, so over generations, natural selection acting in these populations is going to create Populations is going to create a special correlation between the genotypes that exist in these environments and the environments themselves. By doing that, now if you draw a genotype that happens to be green, given the conditional probabilities of these things, this actually does provide some reduction in uncertainty about which environment you are occupying. And the extent of that reduction in uncertainty will be dependent on the correlation that is or is not established by selection. Selection. So we can make a similar argument about a mouse that is trying to choose a light or dark phenotype based on some allele. And we can think of inheriting a light fur allele as a message that the environment is light. So girl, light fur coming from your mother. And the environment is light. This is conditioned on the correlation between alleles and environments that is established by selection. By selection, so Bro Lightford is integrating the interpretation of the genotype as having correlational and causal senses of information as well. All right, so here is a definition of adaptive genetic information that I'll put forth, that this refers to the contents of the inherited material that encode representation of the population's environmental history and thereby reduce an organism's uncertainty about their current environment such that its expected fitness is greater than it would be by chance. Expected fitness is greater than it would be by chance. And we can measure this, as we've said, with the mutual formation between genotype and environment. And this has a couple of nice features. So, actually, many nice features. But equivalently, we can express this as this expression here. And we see that there, it's basically similar to what is saying, yes, that we can talk about the reduction in uncertainty at the level of individual, as this term here is the conditional entropy of uncertainty. Entropy of concern about the environment given that you inherit a particular genotype, then the information is an average of that over the population's genotype composition. So that is a nice way of linking the individual and ensemble scales. Also, we can express dimensional information as the KL divergence between the actual joint distribution of few types of environments that we observe, which is established by natural selection. Which is established by natural selection, and how that deviates from an assumption of independence between those two things. Lastly, for this section, adaptive information has fitness values. So this is something I think many people here are familiar with, and there's a bunch of literature about this, but basically, for every bit of information between genotype and environment in this case, that you obtain, that corresponds to up to one-fold increase in yearly. Up to one fold increase in your lineage's long-term growth. We can ask how this information changes over time. So we can look at two counterfactual environments in this case. So the same composition is put into these two environments at T0, and then natural selection occurs in these two environments, and we get two different distributions at the end. We can measure how much this genotype composition. Much of this genotype composition changes over time in each respective environment, in both of them. And we can build up an expression for how these individual changes correspond to change in time of this mutual information, this adaptive information. So we can start by having this term here that has the information at time t, assuming this started as zero, is equal to the expected change in each environment. In each environment. And then we can take these two counterfactual cases and ask what if they were pooled together, the change that we observe in both of them, and ask how much change occurs in this pooled metapopulation. And then we can finish this expression I was building up here by saying that adaptive information is the average change in each common factual environment minus the change that is common to all environments. In other words, adaptive information refers to genetic change that is specific. Refers to genetic change that is specific to particular environments. So, interpreting that in terms of this cartoon, what we see here is that in both of these environments, these first three genotype categories basically die out. So you think this is maybe different protein alleles that simply don't fold in your non-function. And then we see in these two cases that this environment favors this allele, this one favors this allele, but the other ones do stick around. So we can say that. Around. So we can say that really these changes are representing the adaptive change in these populations, but specific to their particular environments. And other things are actually relatively convergent in their outcomes. So we wanted to say how much of this information, how much of this change is adapted to particular environments that is actually captured by mutual information as shown in this conversation. This relationship which you said is adaptive in particular. Like that of information and fitness. Are there some underlying assumptions that you have basically stable environments for a long period of time? I mean, what do you have a very rapidly fluctuating environment? Would you necessarily want to commit to all the rapid fluctuations in some sense and reflect that in your genotype? Yeah, that's an excellent question. I'll actually get to that in some detail in a second. So, a lot of the fitness value of information stuff is actually dealing with the sensory channel, and in that case, you With the sensory channel, and in that case, you have some heterogeneous thing and you're designed to bet hedge or use cues. In that case, the information is, you know, what, if you have a fluctuating environment, how much of that fluctuation is actually encoded in the signal, and the fitness value is proportional to that information. In this case, what we're talking about is environmental histories. So, if you have a fluctuating environment, typically this is also about counterfactuals. If you have one environment that Counterfactual. So, if you have one environment that fluctuates in one particular way, and a different environment that fluctuates in some different statistical way, then selection will respond to those fluctuations differently. And to the extent that those responses of selection are different in those two cases, that'll show up here. I actually have a visual that will make that clear later. Uh yeah. Uh I was just wondering if there's a rate at which this has gathered over the dynamic process of selection. Is that analogous to selection pressure in a sense? Selection pressure, in a sense? That is a good question. I think so intuitively. That's actually something that I've been meaning to get into analytically, but I don't think I have a slide for this. You could think of this also as some measure of the fitness landscape similarity of different environments, for example. If you, for example, office go to mutation selection balance, then ask about the information at that state. Than ask about the information at that state. That is a good representation of the adaptive differences in two environments. And then some preliminary stuff is that does correlate with the PopGen type selection approach. Are you familiar with the cost of the fifth box and that's basically if you have a final If you have a time-dependent environment, you can calculate the rate of human fitness momentarily, but the vision of the environmental errors, so it's interesting. And it seems to me, so that measures the amount of adaptation relatively amount of time, that's the amount of what uh it seems to me that this won't be Quantity axis related to that flux. Yeah, yeah, I think from what I understand what you said, like in spirit, they sound like very much the same thing. So yeah, it would be interesting to dig into that. I think sort of a macro. Okay, cool. I'm already running a bit behind, but yeah, so a couple points here. Yeah, so a couple points here. So if we're talking about mutual information and this talk's going to be all about discrete stuff, then something that seems like maybe a quirk of this metric is that you have to have bins and the value of the information depends on the bins and the distributions over those bins. But I agree that's actually a really conceptual feature of measuring things this way. So we're talking about adaptive information specific to particular environments. We're really asking what Asking what? How many different environments can the genotypes in a population resolve? So, how much can you actually reduce uncertainty about things? So, if you have a simple situation with two environments, two genotypes, you can have at most one bit of adaptive information in the system. If you increase the number of environments, it seems like some people think it's weird that this still can only encode one bit of information because we're limited by the genotype. Because we're limited by the genotypes variable in this case. But that is entirely intuitive: that genotypes, even though there's four or many environments, having two different genotypes can only resolve up to two different classes of environment. Of course, if we have more variation that we can work with, the amount of information that, the amount of environments that we can resolve can potentially increase, but the fitness structure here also matters, where the colors aren't great here, but these are the optimal. These are the optimal. This is the optimal in this environment, this environment, this environment, this environment. And we see that the same type is optimal in two of these environments. So if you play out selection, you will still have one bit happening. So to have some sense, you know, maximal potential for adaptive information, you need to have environments that favor different variants that are in your genotype variants. That are in your genotype variant space. There's somebody who wants us here. I'm going to move on in time, but we can talk about that more later. So I think we already have the intuition that adaptive genetic information is acquired by selection. The selection is creating these environment-specific correlations with genotypes. And so this is the decomposition I was showing on the slide, but Board just plotted. So you can see in these four different environments, we have these KL divergences. Have these KL divergences of just the change in each environment, and this is then the change of the pooled meta-population. And the adaptive information is the difference between those two. Similar to what I was just talking about, if we have a convergent evolution case where we have four counterfactual environments, but two of them are favoring this same type, then we see that there's basically the same amount of change in each environment, but the amount of change that is common across environments is high. Change that is common across environments is higher in this case, and that's reflected in accumulation of less adaptive information overall. Arguments that I was just describing. Adaptive genetic information is degraded by mutation. So a point that I'll make is that selection is the only thing that increases this in expectation. So for example, mutation degrades this. So if we have four populations that initially start with eight different genotypes and they go to mutation selection, Genotypes and they go to mutation selection balance. The mutation part of that mutation selection balance is actually preventing them from differentiating each other in environment-specific ways much as they otherwise would. So we see that the start line is the adapter genetic information, mutation selection balance, and this dashed line that it's hard to see here is how much information you would have, you didn't have mutation at all. So you can see that mutation is degrading in some sense the amount of information you could have. Amount of infrared could have. But critically, mutation can generate new variants that represent potential information that the population could filter to establish adaptive information. So in this case, we start with one genotype, and then mutations come in and sweep, and we see that in the adaptive information, we have this repeated kind of increasing of the asymptote that it's working with. There's some interesting stuff that we can say about drift, but basically, drift and expectation doesn't create environment-specific change, so it doesn't accumulate information. In the interest of time, I'm going to move past this, but the sentence that I'll put out here is that drift, you think it was just selection operating at counterfactuals or resolutions that you don't care about. So, information due to drift is information about those resolutions. All right, so. All right, so basically we've established the definition of adaptive genetic information and argued that this is acquired by divergent selection but not by other processes such as drift mutation and gene flow expectation. So quickly, I'm going to pivot to asking now, so this is kind of the picture we're talking about, we have different change in different counterfactual environments, but what are the dynamics of actually getting that information gain in the first place? Getting that information gain in the first place. So we can think of this in the simplex as we have some initial frequency distribution, the natural selection will move that through the simplex to some evolutionarily stable state. And this is the basic picture of the core thing that natural selection does. And along the way, we measured the information gained as this divergence that we were talking about. So basically, how much change occurs along this trajectory. I'm going to change the notation here for simplicity, because we're going to stop thinking about. Because we're going to stop thinking about different counterfactual environments and just focus on one environment at a time. And another quantity that I'll point out is: so, this is the information gain that we have so far, and this is the potential information that we still have, potential gain, which is the divergence between where you are and the evolutionary stable state. And we can show that the dynamics of selection have a Lyapunov function in terms of information such that the potential information is always decreased. Such that the potential information is always decreasing with respect to the essence. So we could think about: you know, if you left out a bowl of candy in your office, over the course of the day, people will come and graze off of it. At the end of the day, you see the distribution of things and say, okay, these are the types that are most persistent in my office environment. So you learned something here, but you did so at the cost of losing a bunch of your candy. So this is a feature of selecting. Candy. So, this is a feature of selection that's true also in like the mouse situation. So, the reason allele frequencies are changing here is because some of the mice are selective deaths in a crude example. You don't need to have selective deaths, but that's just a data illustration. So, in that kind of example, we have information gain through allele change, but we also have this buildup of cost over time. This is something, I'm going to skip very fast through some stuff that people. Stuff that people in population genetics have thought about and figured out ways to measure. They call it substitution loads, basically the potential population growth that is not realized due to selection. So in this case, the red type is favored, the blue type is, say, being killed by a drug. So this is the population size you could have had if you had information, you know, full information about the saving environment from the beginning, but because you had to gain that information by going through. But because you had to gain that information by going through this seemingly pre-process of selection, you don't actually realize that potential growth. So a key result is that the information gain and substitution load are related to each other with this relationship, that for every bit of information that you acquire by selection, there is some amount of load that you must pay to acquire that much information. So we have a So, we have a picture that looks like this for the core trajectory to a BSS situation that we were considering in this section. We've done some experiments. You can measure this and that the relationship is robust experimentally. We can also extend this to consider environments that fluctuate over time. So, this is something that we are motivated to do in dealing with experimental populations. So, basically, we So basically, we extend substitution load to something we call mismatch load, which is asking basically what is the difference between the growth rate of an optimal type in whatever condition exists and the growth rate of the actual type that exists in that condition. And if you integrate that over the composition of the environment and the population, that basically gives you that substitution load, that unrealized growth cost. So, this is something. So, this is something that Oji Kimura, who first thought about substitution load, called a cost of selection. But what many have pointed out is that even though you have this cost of selection, your population is actually getting better off in terms of its mean fitness increasing over time. So is it reasonable to call this a cost of selection? How do we think about this? How costly is selection relative to other processes? To answer these questions, we turn to learning theory, which is kind of the domain. Learning theory, which is kind of the domain of considering processes that learn or acquire information and comparing them in cost or efficiency terms. Very short on time, so I'll just summarize all this that we could think of, again, the problem that populations are facing as a game against the environment, where the environment has some number of conditions that it can take, and the environment, or the population has some number of types that it could field. And so you can think of this as like a tabletop. So you can think of this as like a tabletop game where the environment lays out a bunch of conditions, the population puts out a bunch of types, and then these types experience the revealed conditions that they occupy. We can write a game about this and measure basically what is the potential, the loss of potential fitness that each of these things have due to being mismatched. And so we could talk about. So, we could talk about what are optimal algorithms for learning a good type distribution to play this game. And natural selection happens to be one instance of a very big class of algorithms. And we could compare, for example, a simple follow-up leader learning algorithm with that of selection. And we basically find that a learning theoretic measure known as That a learning theoretic measure known as regret is actually a more appropriate measure of the cost of selection, which takes basically what is the loss that selection realizes relative to the optimal possible loss that any algorithm could have had, or any strategy could have had in this situation. And there's more I can say about this, but I'll stop there. I tried to get an hour talk down to 30 minutes and didn't quite succeed. But yeah, thank you for your attention. 