So today I'm gonna talk about I mean I'm gonna focus on stochastic derivative optimization algorithms and I will describe some tail bounds for function estimation that will hopefully help us actually help us use a little bit the analysis of those algorithms in the end. And this is a this talk is based on a joint work with Focus based on a joint work with Luish Vicente and my PhD student, Damiano Cefil. Okay, let's start with a brief outline. I will first focus on the problem we're going to deal with. I will give some detail about it, and then I will also give a brief overview of the existing methods for stochastic theory-free optimization. The ready-free optimization, and once I'm done with that, I will try to highlight the contributions that we gave with this work, this piece of work. In the second part, I'll focus on our tailbounds. I will try to give an idea of how these tailbounds work and I will try to explain how they help us. They help us in guaranteeing, easily guaranteeing the convergence of those methods that we analyze. Then I will focus on the methods, actual methods. So we have two different methods, a very simple direct search scheme and another very simple trust region scheme for possibly non-smooth problems. Problems and we'll see how by using these two tail bounds we can get a sharp and easy analysis of those metals in the end. Finally, draw some conclusions and highlight some future works we would like to carry on. So here is the problem we're gonna, you know, we try to deal with. We want to minimize functions, right? It's an unconstrained problem. functions are an unconstrained problem. f of x is a locally is a locally Lipschitz continuous function and as I said it's possibly non-smooth and of course it's bounded from below and it is given by a stochastic oracle that basically produces an estimate FTLL of x for any x that we have so this is how the stochastic oracle looks like we have we feed the stochastic oracle with the point x and it gives us back f tilde And it gives us back f tilde of x, where we can imagine f tilde of x as this guy in here, f of x x c, where basically x is used to parameterize our function. And then the oracle is basically given by sampling over the next variable. And here I give two last, I mean, two. I mean, two nice applications that lately are widely studied by different communities. So we have statistical learning problems. In this case, the f of xc is basically the loss of a decision rule that is parameterized by our x variables. And c is simply a data point that we somehow pick, usually at random. Usually at random and comes from some given application. Otherwise, we can also have some simulation-based engineering applications where usually f of xc is some noisy and comfortable version of the original function. In this case, xc is basically a random variable that induces some noise on the original one. Classic example is Monte Carlo simulations. simulations of course if f of x is exact exactly an expected value then things get nicer and we can replace f i mean replace f of x with the expectation with respect to c with respect to c of f of x c and this is a classic problem that some of you in the room uh studied in depth and you got very nice results on those problems there's On those problems, there's no need to discuss about this right now. And let's switch to the overview of the methods. As usual, we have two different classes of methods. And also in this case, we have two classes of methods and some papers on those two classes of methods. First of all, let's focus on model-based methods. And what we found out is that mainly the papers in the literature. Mainly, the papers in the literature are of a trust region type, and the main features are the ones that I listed in here. So, F usually has a certain degree of smoothness, like the example I reported in here. So, the gradient of F is my Lipschitz continuous. And usually, we have a probabilistically accurate gradient estimate that is available, like a probabilistically fully linear model. And there are very nice papers also in this case. I listed here some of them dealing with the model-based methods. And of course, we also have very recent papers about direct search methods, both for the smooth and non-smooth case. And the smooth case, there was this nice paper recently published by Zahini on co-op related to the convergence rate of the Rate of the simple direct search scheme. And the nice and sharp analysis, also in this case, was obtained by extending some results for another nice paper by Blanche and co-authors from 2009. And the non-smooth case is what we are concerned with in this case. And actually, we try to study in depth this second paper related to the stochastic version of the Stochastic version of the math approach. And this is also a nice one. And in this case, the analysis was carried out by using, by extending, nicely extending some of the results reported in the Paget and Scheinberg SciOpt paper from 2020. So let's now focus on the contribution after this quick overview. As I said, I will first focus on Tailbank. said I will first focus on tail bounds and I'll try to analyze in depth analyze in depth those tailbounds and compare them with some the other theoretical tools that were actually used the other papers that they were analyzed in the Nosmoth case and once we are done with that we will We will try to use those tail bounds in order to get a sharp analysis of our stochastic TFO methods. What is the main idea? And roughly speaking, what we do is considering some reduction estimate. So the estimate I report in here. So we have the objective function value calculated in XK, our iteration K. And then we have the minus the objective function value. Minus the objective function value calculated in xk hat, well, xk hat. You can imagine it as a trial point that we analyze in the present iteration. Of course, what we want to do with this tail bounds is suitably bound the probability that the error in this estimate is large. And I mentioned now is why focusing on this reduction estimate? Well, the answer is easy. Estimate. Well, the answer is easy because we basically use this reduction estimate in the acceptance test of both our algorithms. So let's focus now on the algorithms. As I said, we have two different kind of algorithms we're going to analyze: stochastic, very simple stochastic strategy and a very simple stochastic trust region scheme. I mean, one of the main features of these schemes is that we replace the half with the estimates in the acceptance test. I mean, this is a key feature that we need to keep in mind also when we will deal with the analysis later on. And the algorithmic schemes are, as I said, are very easy. And they mainly have three main steps that we focus on. We basically choose a direction over the unisphere at each iteration. Direction over the unit sphere at each iteration. And then we generate the new iterate by either moving along the direction in case of the stochastic direct search strategy or by solving a trust-region sub-problem in the case of the stochastic trust region scheme. And of course, once we are done with this, we get a new trial point and this acceptance test to accept or discard a new either. Keep in mind that the acceptance test is built up based on the function estimates. Function estimates, not the real function, of course. Okay, about the convergence analysis, as we will see, we have two main steps. First of all, we analyze convergence of the step size or the trust region radius to zero, and we get this almost surely in our results. And once we are done with this, we will use this result in order to prove park stationarity at limit points of the sequence of unsuccessful iterations. And of course. And of course, now we give some notations. I mean, there's no need to spend too much time on this. Just what we want to highlight is the notation that we will use in the algorithm. So GK is a shorthand for GKW, where W is an outcome of the sample space. And of course, GK is a search direction realization. Delta K is a step-size realization. FK and FKG are respectively the function value estimates realizations for function values f of xk and f of xk plus f of xk and f of xk plus dk gk and of k of course fk minus one is the sigma algebra events up to xk uh here are our tail bounds i mean uh as you will see they both play a role in our analysis and in two different steps and so we will see how uh they're gonna be used inside our proofs at least i will give you try to give you an idea of how things work and then if we have enough time How things work, and then if we have enough time, maybe we'll switch to the appendix where we have detailed results. About the first assumption, as you can see, we have a conditional probability in this case, while in the second assumption, we have a full probability. And the second main difference between those two is that the coefficient we have on the right-hand side in the second case has a square root in here. But the main But the main part, the main fact that I want to highlight is that in both cases, we assume error bounds for the estimate of the difference between two function values. So this is the key trick that we're going to exploit to make those proofs easy. And just to see clearly that actually we are dealing with, I mean, just to be sure. I mean, just to be sure, they're all on the same page, but I'm pretty sure it's an easy step. We can just set alpha equal to epsilon f divided by p with alpha greater equal than epsilon f. And we actually have clearly now our tail bound, like from our undergrad courses. And the nice part is that we now have a clear bound with respect to this alpha. Clear bound with respect to this alpha, as you can see. And I just rewrote this with the bound this way to highlight the fact that we have a bound that is big O1 over alpha in this case. Okay. And this one is going to be used to prove the almost sure convergence of the step size or class division radius to zero. While in the second case, what you end up with is a new bound that is big O1 over alpha alpha square. So we have a tighter bound in this case, and this one is. Tider bound in this case, and this one is needed for the Clark stationary proof. Now, I will quickly compare with the, as I said, with the conditions that are used in the paper, and the Stomach's paper for proving the 3D results. We have the KF variance conditions, and these nice conditions are actually some adaptations of the gradient-based condition. The gradient base condition in the Packet-Scheinberg paper. And of course, in the Packet-Scheinberg paper, those assumptions were actually also combined with some probabilistically accurate credential estimate. And we were, of course, in the smooth case. This case, this estimate is not available to us. And what we can prove, we did prove in the paper, is that if you get this condition to hold. This condition to hold, then this condition is going to imply our two assumptions for suitable choice of the two parameters epsilon f and epsilon q. So another nice thing that you can get from here is that if you have a finite variance or oracle, then of course if you also have a large enough number of samples, then you know that this Samples, then you know that these conditions are going to be satisfied. This is proved also in the Stoman's paper. And then, since these assumptions imply ours, then also our assumptions are going to be satisfied. Actually, we also try to work in order to get some weaker assumptions, like dealing with some different oracle. And we actually were able to get some nice results also. Get some nice results also in this case. But I mean, I think I wouldn't have time to talk about this, but if you want some videos, I can talk about this later on. And of course, I gave some other comparison with the other conditions that are normally used to carry out the analysis in the other papers that deal with the stochastic DFO methods. And just I want to focus on one important paper. Important paper that is one by Larson and Phillips, where they actually had a tailbound, a little bit different from ours. And in the tailbound, in this case, anyway, they were dealing with a different problem, kind of problem. They had some smoothness on the function. And of course, this assumption was combined with some other assumptions like the fully linear local model. Like the fully linear local model, in order to prove the convergence. So, this result that I will give now is just to highlight what is the relationship between those two conditions, but it's not a big deal in the end. What I want to highlight in this case is that if this assumption holds, what you can have once you probably choose this beta, eta, and theta, then a modified version of our assumption still holds. Version of our assumption still holds a modified version that, anyway, can help us proving the results that we have. So, there is some relationship between this tailbone and our first assumption, the first tailbone that I showed you before. Okay, I start with the direct search method. So, we have a very simple scheme, and I will show you the scheme, how it works. As I said, we have step three, we search iDirection GK. We search a direction gk in the unisphere, then we calculate our estimates. We check if this test is satisfied. In this case, we have a successful step. We update the iterate xk plus one. We update our step size. We actually increase the step size. Otherwise, we got a failure. We got a failure. Point stays the same when we try to shrink the step size. And of course, first question is: how to generate GK. First question is how to generate GK. GK is going to be dense in the unisphere on some properly chosen subsequence to get convergence. This is a classic trick. We will talk about it later on. As I said before, main step in here is that you're replacing the objective function value, the original objective function values with these estimates in the end. What you need to keep in mind is that actually the most complicated things is dealing with the same Complicated things is dealing with this case from a theoretical point of view, which is the so-called what we call bad case, a bad successful step, which you actually have an increase of the objective function on the trial point with respect to f of xk, but due to the fact that your estimate is a bad one, you actually get for your acceptance test a success. And this is the bad successful step, which you don't like. We don't like at all. Okay, we need to deal with it in the theoretical analysis. In the theoretical analysis. Let's focus on the theoretical analysis now. We, of course, as I said, is basically a two-step kind of analysis. In the first step, we prove text to the first assumption. And by properly choosing the theta in the acceptance test, we prove that this guy holds. So the series of the squared step sizes is bounded. Is bounded almost surely in omega. And this basically means that almost surely we get the delta k goes to zero as well. Just to give you an idea, we use some classic tool, the function of phi k that is basically measure on one side the improvement of the function and on the other side how things change with the step size. And we need to probably choose the eta value in order to satisfy. In order to satisfy the result as well, so the eta is theta divided by four theta, four tau, sorry, and of course, what we will see, I mean, what we see in the proof is that in the end, we are able to prove that this condition holds, this inequality holds, and this basically gives us our final result. And once we're done with this, we need a technical lemma. The technical lemma. We focus on the unsuccessful iterations and then we include the new assumption. The new assumption, the other tailbound. So we now have both tailbounds plus the hypothesis we had before. This is needed for guaranteeing convergence, almost sure convergence to zero over the step size. And then we have almost surely that this condition is satisfied. So this guy is going to be greater or equal than zero. And just to give you an idea, what we end up proving is that this guy Proving is that this guy is always greater equal than this guy on the right side, where m is any natural number. And finally, we get to our final result. What we include here is a further assumption related to F. We need F to be Lipschitz continuous. And then what we know is that if L is a subset of indices, Subset of indices in K, where K is a subset of indices related to unsuccessful iterations. And we can guarantee that this condition holds. And also that we have a sequence of points convergence to a point x star, then the point x star is going to be clear stationary. And in this case, what we do is focusing on any direction D in the sphere, then choosing a subsequence of indices L prime that is containing L that guarantees this condition. That guarantees this condition to be satisfied. And this is, of course, satisfied thanks to our assumptions. And the rest boils down to proving that this condition holds. And then we are done. We get calculation everything. Let's quickly switch to the trust-revision method. I won't spend too much time on this because more or less the idea is the same. I mean, the structure of the proofs is the same. There are, of course, changes in the proof depending on the fact that we are dealing with the DFO method. And here is the scheme. In this case, And here is the scheme. In this case, we again start by selecting a direction gk on the unisphere, and we build also a symmetric matrix BK. Once we are done with this, we compute an estimate fk of f of xk, and we solve this trust region sub-problem. Once we are done with this, we get the new point SK. We calculate our estimate related to the point xk plus sk, and we, of course, calculate the Of course, calculate the ratio rho k in here that, of course, will help us with checking if we have a successful iteration or not. In fact, if this guy is greater or equal than one, then we get a successful iteration, we update our point, and we, of course, also update our step size. What I want you to know: this is the net case, in this case, what we need to have is an upper bound on. Bound on the step size when we perform the increase. And in this case, this is needed to guarantee that the first result that I will show you holds. If this is not the case, of course, we get an unsuccessful iteration. We just let the point stay the same and we shrink the step size. Sorry, the last region radius in this case. Sorry, if I call it step size. Call it step size. I am, of course, wrong. So, of course, we need some assumption on the matrix BK. This is an assumption. This is the assumption that we have. In this case, this assumption is the same as the one in the 2019 paper I mentioned before related to the deterministic trust region scheme that we just described. Of course, this assumption is a bit weaker than. This assumption is a bit weaker than the usual assumptions that we have in transvision methods. And this is a nice thing. But we want to focus on tail bounce once again. So we see now how to properly modify the tail bounce we described before in order to deal with the trust region scheme. Okay. And of course, it's very easy to understand how to properly modify those tail bounds because what we do Those tail bounds because what we do in here is simply replacing gk with the normalized vector sk, so sk divided by the norm of sk, and delta k is going to be the norm of s vector, sorry, or the norm of sk. So here are the two-tailed bounds, very similar to the ones we had before. And also in this case, we can prove basically same result as before. Of course, the proof is a bit trickier. The proof is a bit trickier in this case with the trust region because we have to deal with two different cases when the solution is on the boundary, when the solution is not on the boundary, and we need to be careful about that. And this is also the reason why we need delta max that I showed you before. And the nice part is that you can get the exact same result in the end with the similar tricks and by using the same tailbound, the assumption number one. Of course, modified in order to consider. Course, modified in order to consider the new framework, algorithmic framework, but still same idea as before. And what I want you to notice is that in that case, this m bar shows up both in the theta definition, in the definition of the bound for theta and in the eta that we have in here. And m bar is this guy, so depends on delta max. Of course, we also have this further proverb. Further property that we also had in the other paper, this basically says that the solutions tend to step along the negative gradient almost surely in our case. And this can be easily obtained as shown in this proposition in here by considering that the assumption tree holds, the assumption that we made on the matrix PK, and that we solve all the trust region sub-problems up to optimality. I know it's a little bit. I know it's a little bit stronger, but this we can work out in order to maybe get rid of this second assumption somehow. Anyway, in this case, what we have is that the algorithm number two then generates a sequence that satisfies this property in here. Why do we need this property in order to prove the last result? To prove the last result, the clock stationarity, this is very important in order to guarantee the clock stationarity in the end. So, again, a technical lemma similar as the one we had before. We have the second tail bound that plays a role in this case, and we are able to prove again that a similar condition as the one we had before holds. Also, in this case, there are some technicalities that make things a little bit different. Things a little bit different in the proof, and this is due to the fact that we are dealing with a trusted region approach. But in the end, the main idea is always using probably using the tail bound in order to get the result in the end. And finally, you have your conversions to classification points. Same result as before. We again include the fact that the function is going to be linked is continuous with the proper constant. And of course, the proof sketch is going to be a little bit different in this case because we first select the direction in the sphere, then we get some set of indices L prime that satisfy this condition. And thanks to the results we had before, what we can prove is that this condition holds. And then we're basically down because thanks to the other result we showed before, everything. We showed before everything was down to proving that this holds. So, and it's very easy. I mean, some classical steps that you can easily imagine. So, let me conclude. As I said, we proposed some new tail bounds for stochastic DFO in this presentation. What we found out is that they are weaker than the usual assumptions that. Assumptions that are considered for the potentially non-smooth case, and there is some nice relationship also with the other conditions that we have in the literature for the smooth case. We obtain thanks to those tail bounds in a very easy way convergence for both a simple direct search and a simple trust region scheme. And just to summarize, what we did was proving that the series of square step size already is almost surely in. Already is almost surely finite. And then we use the result in order to get combined with another tailbound. So we just need one further tailbone in order to get our convergence result. But future work, we actually plan to do something. After this, we would like to use some nicer non-smooth model to improve the trust region scheme, like some piecewise linear model. Wise linear model. We would like to analyze maybe some special cases like Max functions. And of course, we'd like also to extend the results to some other case like constraint case, maybe starting with some simple constraints. And of course, we'd like to use these tricks also for probing convergence for other schemes like line search schemes. And of course, we'd like to start the performance. Start performing some numerical tests. Maybe when we have also a line search scheme, that would be nice to try and see what's going on with the three schemes. But before doing that, we need to suitably modify the proofs in order to deal with the line search and with the expansion step, which is no piece of cake. Actually, I think we already found a way. Probably we should be able to get some results very soon about that. Very soon about that. So, here is the paper. We posted it on archive and optimization online. And of course, thanks for the attention. If you have any question, feel free to ask.