The breakout rooms are closed. Okay. Be back. Great. Well, welcome back, everybody. We're very pleased to have Emily Barnard from DePaul University talking about the Crevera compliment on the Lattice of Torsion classes. All right. Thank you to the organizers for inviting me. Can you guys all hear me okay? I can't see too many faces, but I can see a couple. Faces, but I can see a couple of you. So if you can nod and just let me know. Okay, great. Thanks. All right. So yeah, thank you for inviting me to speak today to share this work. I'm excited to be able to be a part of this conference and it's really nice to see so many familiar faces in the audience. This work is joint with Gordana Todorov and Shiji Zu. It started well at It started while I was at Northeastern University working as a postdoc with Gordona. So, hang on one sec. There we go. Okay. So, the goal of this project was to study really a purely combinatorial object that I'm going to call the Kappa map. And in the context of representation theory. So, I'm a combinatorialist. Gordona and Xi Gier both do representation. GGA both to representation theory. And this was us kind of coming together and like having a conversation about these things. And I think you've seen a couple of talks now where there's this really beautiful conversation between representation theory and combinatorics that I'm excited to share with you. And some of the terminology that I'm going to use, if you were there for Al Garver's talk, he had defined some of these things. So, the basic outline is: I'm going to define for you this mysterious Kappa map, and then we'll do a bunch of kind of combinatorial stuff. I'll make a connection to the Crevora complement, and then we'll shift focus to more of the representation theory side. And the kind of take-home message, what I hope to be doing in the talk is like building the case that this map that I'm going to describe, this Kappa map, is like an analog to the Kreverett complement. So, I'm going to try to give you. Compliment. So, I'm going to try to give you like a bunch of evidence for why you should believe that. Okay, so let's go ahead and get started. Nathan's talk set this all up really well. So he's already reviewed for you what's a lattice, but that's the world I'm working in. I'm working in lattice posets. Every element has a joint and a meat, and I'm focusing on the joint irreducible elements and the meat irreducible elements. Let's see. All of my last. All of my lattices in this talk, I'm like looking at the last bullet point, all of them are going to be finite. So I have defined for you these two notions, join irreducible and completely join irreducible, but they're the same for finite lattices. Occasionally I'll say one or the other, but in your mind you can think those are the same things. In the project that we worked on, back Gordana and Chiji and I worked on, the lattices we were thinking about in general were not finite. Thinking about in general, we were not finite, so this was like very much an important distinction for us. But just to keep things simple for this kind of shorter talk, I wanted to like kind of focus on the nicest possible world. So just like in Nathan's talk, right, joint irreducible elements were very important, and he was identifying them by saying, oh, they're the elements J where there's only one way to go down. It covers a unique element. We're going to call that unique element J star. And then similarly for meat irreducible. Similarly, for meat irreducible elements, right? These are the guys where there's only one way to go up, and we're going to call that unique element that covers a meat irreducible element M. We'll call it M star. Okay, so that's our setup. And right, here's a little running example is this little Tamari lattice that I'm realizing as the subpost of 312 avoiding permutations. And if you're not super familiar with pattern avoidance, that's fine. With pattern avoidance, that's fine for this little S3 example. It just means that I've thrown out 3, 1, 2, and I have all the other permutations ordered in the weak order. The highlighted yellow ones are the joint irreducible elements, right? There's only one way to go down, but they're also all of the meat irreducible elements. There's only one way to go up for each of these. Okay, so I'm ready to define my mysterious Kappa map. This is a map which takes This is a map which takes a completely joint irreducible element, in our case, that's the same as join irreducible, to a completely meet irreducible element. And here's how it works. You hand me J, and I find that unique element that J covers, J star. And then I take the max of this set. I take the max of the set of elements in my lattice that are greater than or equal to J star, but not bigger than or equal to J. When such a thing equal to J when such a thing exists. So that's the definition of kappa. Now let's do a little example. Okay, I'm going to kind of tilt my screen and write on this as we do it. Hopefully it works. So my J is the permutation 2, 1, 3. It covers J star 1, 2, 3. And so I want the largest x, which is greater than or equal to 1, 2, 3, which is easy because that's everyone, but not bigger than or equal. But not bigger than or equal to 2, 1, 3. So I'm not allowed to pick these ones. And so I'm looking at this set, and I want the max, which is 1, 3, 2. So that is kappa of J. Okay. Let's do another example. Since I'm talking about the correct compliment, it's the title of my talk, right? So we should have non-crossing partitions. So here are non-crossing partitions on the Partitions on the numbers one, two, three. J is this non-crossing partition. I hope it's big enough for you. I know when you're doing this side by side, sometimes the slides aren't super big. J is this guy where one and two are in our black and three is by itself. And then J star is the non-crossing partition where everyone's in their own black. And here you can see there's two maximal elements in this set of things that are greater than or equal to J star. Things that are greater than or equal to J star, but not bigger than or equal to J. So kappa of J is, it's just not defined. So this is kind of meant to demonstrate that this kappa map that I'm looking at doesn't always work. You have to look at sort of special lattice posets to make this work. In fact, there's a really like a class of lattices that are characterized by whether or not. Characterized by whether or not kappa is defined. So if L is finite, then kappa will work, kappa is defined if and only if L is what's called semi-distributive. And I'm going to define that in a couple of slides. Okay, but Tamari lattices are an example of semi-distributive lattices. Non-crossing partition lattices are not semi-distributive. And actually, the little modular lattice we were just looking at is like kind of a minimal example of something that's not semi-distributive. That's not semi-distributive. But there's still a connection to the Creverer complement. So I'm calling, I think many of us are familiar with this. There's a bijection from the elements of our Tamari lattice to the elements of the non-crossing partition lattice. And even if you're not familiar with that story, right, you can just see there are five elements in both sets. So there's a bijection. But there's a really nice bijection, which for the purposes of this talk, I'm just calling the bump diagram map. Calling the bump diagram map, which is not what it's called in the literature, but it's just the cute name I'm giving it today. So, on the right-hand side, if you look at the non-crossing partitions, I've drawn the classic like bump diagram for these. Basically, I took that circular representation and kind of cut the circle and stretched it out on a line. On the left-hand side, I've got the Tamari lattice, and I have a kind of bump diagram for each of these. kind of bump diagram for each of these two. These are called non-crossing arc diagrams. I'm going to zoom in and try to tell you how this works. So if you wanted to draw this diagram for a permutation, you look at the descents in your permutation like three and one. These are numbers next to each other and decreasing in order from left to right. And you connect the corresponding nodes in this diagram. So I know my handwriting is not great in this picture, but three. Not great in this picture, but three and one are connected. And you see that that arc, I specifically drew it so the node labeled two is on the left side. That's because the number two in the permutation is on the left side of this descent. That gives you kind of a sense of how that map works, how you can draw one of these diagrams for every permutation. Now, this bijection just takes the bump diagram, the little non-crossing arc diagram, and A little non-crossing arc diagram and rotates it and gives you a classical bump diagram for a non-crossing partition. So, what we're going to do is we're going to map an element of the power army lattice to a non-crossing partition and then compute the correvora complement and compare that to what happens if we do the kappa map first and then do this bump bijection. Okay, and that way we'll get a kind of direct comparison of the kappa map and the Krebra complement. Kappa map and the Krevor complement. Okay, so I'm going to go back to writing my non-crossing partitions around the circle because it's easier for me to demonstrate the Krebora complement that way. So here's where I first do this bump map. And I'm going to be, as my example, I'm taking the permutation 213. That maps with this bump map to the non-crossing partition where one and two are in a block together. One and two are in a block together. And then I'm doing the crevora complement. And let me just zoom in a little. So when I do the crevora complement, I write draw an extra copy of the numbers one through run around my circle and then put in like the coarsest partition that I can so that it doesn't cross that red partition that I had originally. And that's where you get this black three and two are connected, and the black one node is all by itself. All by itself. So the Crevora complement is sending this non-crossing partition where one and two are together to the non-crossing partition where three and two are together. Now let's compare that with first doing the Kreverick, sorry, the Kappa map and then doing this bump bijection. So we did that Kappa map computation together. 213 goes to 132. goes to one three two and then the bump bijection sends one three two to the non-crossing partition where three and two are together in a block so maybe not surprising right what i'm saying is i have a little commutative square here so in that sense i can really say that the kappa map and the correvora complement are the same at least in this small example so the key point right Example. So the key point, right? I want to think about the kappa map as an analog to the Crevora complement. But now I'm not acting on uncrossing partitions. I'm acting on this class of finite semi-distributed lattices. That's where kappa makes sense. So let's talk about semi-distributive lattices. Here's the definition as promised. A semi-distributive lattice satisfies a certain weakening of the distributive law. If this is something you're used to looking at, I think you might be. To looking at, I think you might be able to see the distributive law here. And if it's not, then it's probably not worth you trying to memorize these two implications. They're not important for the purposes of the talk. Okay. What is important are the examples at the bottom of the page. So, some important semi-distributive lattices. I said Tamari lattices, but more generally, see Cambrian. Lattices, but more generally, Cecambrian lattices are semi-distributive. The weak order on a Coxster group is semi-distributive. Posets that come from hyperplane arrangements are semi-distributive. And then lastly, the kind of topic of this talk, these lattices of torsion classes are semi-distributive. And I think that Nathan mentioned that actually there's this really recent work of Hugh Thomas and Nathan Reading. Thomas and Nathan Reading and David Speyer, that kind of shows that these semi-distributive lattices like really are somehow composed of data that comes from a kind of torsion class paradigm. Maybe I won't say more, but you can always ask me about that. So we're going to focus on torsion classes now, which I'm going to just assume we're not really familiar with torsion classes. We're not really familiar with torsion classes. Here's the setup: you've got an algebra, finite-dimensional, basic algebra. It's over a field. You can assume that field's complex numbers if you want. This algebra lambda, if you want to be really concrete, you can think this is like the path algebra over a directed graph. Okay. Now we're going to look at all of the modules over that algebra and a torsion class. And a torsion class is a class of modules that's closed under some operations. So I like this. This is a kind of very combinatorial way to think about it. It's thinking about closure spaces. So what are we closed under? We're closed under quotients, isomorphisms, and extensions. Okay, so let's do a little example where we're going to look at all of the torsion classes you can build for some very small algebra. Very small algebra. Okay, so I repeated the definition at the top of the slide here: what a torsion class is. I'm going to look at the path algebra over this little quiver where I have an arrow from the vertex one to vertex two. And there are only three indecomposable modules. Everything else is built from direct sums of these. So there are two simples, which by definition have no subs or quotients, which is nice. And then one projective. And then one projective, which is an extension of the two symbols, in the sense that there's this little short exact sequence. So S1 is a quotient of the projective that I'm calling P1. All right, so let's write down all of the torsion classes. Okay. So one of them, I could take the zero module. That's a perfectly good torsion clash. It's closed under these operations. Closed under these operations. Another easy example, I could take all of my modules. That's fine. That's also closed under these operations. Those are the easy ones. Now I'm going to try to basically take one module at a time and then generate a torsion class, like one module, and then think about closing under these operations. So if I were to take, say, S1, there are no quotients of S1. Are no quotients of S1 because it's simple, and there are no non-trivial extensions or self-extensions. The only kinds of extensions are direct sums. So if I kind of generate from S1, all I get are all direct sums of S1 that you could write down. And then the same thing works for the other symbol. S2. Now, if you generate using this prediction. Using this projective P1, closing under extensions doesn't give you anything, but closing under quotients does, right? Because S1 is a quotient. So that gives us a torsion class kind of generated from the projective, and it has S1 and P1 and all direct sums of these. And if you try to take two pairs of these two modules. Of these two modules and do the same kind of generating game, you won't get anything that we haven't already written down on the list. So, for example, if I were to take S1 and S2, when I close under extensions, I automatically get P1, and now I have all of my modules. So, there's only one, two, three, four, five guys. And now I want to organize these into a postet. And this is the lattice of torsion classes. So, it's just what you would do. So it's just what you would do. You order them by containment. And here, I mean, I didn't write the little add symbol because, you know, in our heads, we know we're taking all direct sums of these to get a class of modules. But this is what this postet looks like, which is hopefully familiar from our other example. I want to say one thing before I move on to the next slide, which is, remember, these are our special elements, the join and meet irreducible elements, and each one is kind of generated. And each one is kind of generated by a single module. For the simples, that's obvious, but remember, this one is kind of generated by just the P1. And that fact turns out to be true in general. Okay, so let's start doing some results. So the first main result is saying that for lattices of torsion classes, the Kappa map is, you can do it. Is you can do it like you can compute it, and it's actually quite nice. So, the first bullet point in this theorem is saying that what I had just said in our last example, every joint irreducible torsion class can be written as basically being generated from a module M, and this module is a special kind of module called a brick. This filt gen, you can think of that as saying, okay, I'm taking my module and I'm closing under quotients and extension. Under quotients and extensions, just like we had done in our example. And then the kappa map just takes that M and is applying a perpendicular operator. So you want to take all of the modules that don't map to M. And that's quite nice. So I actually want to say a really brief remark. I had mentioned before in this project with Shiji and Gordana, we really think about We really think about infinite lattices. And so, actually, what makes this result kind of more interesting is that, you know, even though kappa is only defined for finite semi-distributive lattices, it works for these infinite, for infinite torsion classes. So this lattice of torsion classes is often not finite. Okay. Now I'm supposed to be building the case that kappa is a kind of analog. Oh gosh, I need to really. Analog, oh gosh, I need to really speed up. That kappa is an analog to the Kreverer complement, and the crevora complement acts on all non-crossing partitions. So I want to let kappa map, the kappa map act on all of the elements of my lattice. And I do this using a special property of semi-distributive lattices. So every element in a semi-distributive lattice can be written as a join of join irreducible elements, which is called the canonical join representation. Showing representation. And what's nice about this is that, here we go, the Kappa map kind of respects canonical join representations. I maybe actually didn't say this on the previous slide, but there's an analogous canonical meat representation where every element has a unique way to be written as the meat of meat irreducible elements. And Kappa sends these canonical join representations. These canonical join representations to canonical meet representations. And so we use that to define a kind of extension of kappa to all of L. So we say kappa bar of x, what is that? I take the canonical join representation of x. That gives me a bunch of join irreducible elements. I apply kappa to each of those joint irreducible elements individually, and then take the meat. And that is the canonical meet representation for some elements. Presentation for some element. I know I'm throwing a lot of new words at you, and it's probably way too much to digest. But the takeaway here is that this gives me a bijection from L to L. And now I can start studying dynamics. So I'm just going to kind of briefly say here for torsion classes, canonical join representations are easy to compute. You're basically looking Easy to compute. You're basically looking at orthogonal sets of these special modules from earlier, and that means that computing this kappa bar map is also easy. It follows immediately from our other theorem that to compute this kappa bar, you're just taking an intersection of these left perp M modules. Okay, so here's the next bit of evidence in saying that. bit of evidence in saying that look the kappa maps an analog to the crevra complement if you have a lattice of torsion classes that's finite and you let r be the number of vertices in the corresponding quiver for this algebra lambda thinking about lambda as a path algebra then if we take the size of a torsion class not to be the number of modules but to be the size of this special canonical join representation then taking the Then, taking the average of this statistic over our orbits, you always get the same number, and it's always r over 2. Now, you can think of r as like the analog of rank of a Coxster group, and then this computation completely matches what happens for non-crossing partitions. The same sort of thing happens. Okay, apparently I have a slide that's missing. That's too bad. So, in my very last, I know I'm a little bit over, what I want to say is that just as there's a kind of representation theoretic analog to the Tamari lattice, which we can think of as these lattices of torsion classes, there's a representation theoretic analog of non-crossing partitions. And these are wide-scale. Non-crossing partitions. And these are wide subcategories. And this connection here has been studied by Hugh Thomas and Engels and Ringel. And they compute, they basically compute the kind of representation theoretic analog of the Krebret complement, which I'm going to call epsilon here. And just as there's a bijection, which I called this bump map from the Tamari lattice to non-crescent. map from the Tamari lattice to non-crossing partitions. There's also a map from torsion classes to wide subcategories. And the last theorem is saying that for special algebras lambda, these are called hereditary algebras, the same square, this square we had for the Tamari lattice and uncrossing partitions, is a commutative square when I'm talking about torsion classes and these wide subcategories. These wide subcategories. Okay, so I think I'm quite over. So I'll stop there. Thank you all for listening. You're not over at all, Emily. Oh, really? I thought I only had 20 minutes. Okay. 25. Oh, well, then I can tell you my last slide. Perfect. That's perfect. Thank you, Tom. All right. So the last slide is saying that this is maybe a little more representation theoretic. So for those More representation theoretics. So, for those of you who know a little bit more about the representation theory of quivers, when you do have this special case where lambda is hereditary, you can interpret the kappa map in terms of this special Auslander writing translation. That was important also in Al's talk. So, if you are in Al's talk, you kind of see some similar words showing up. So, applying the kappa map twice corresponds to Corresponds to applying this Auslander-Wrighten translation, the inverse of this Auslander-Wrighten translation. Maybe I'll just say a little bit about the terminology. So what I'm doing is I'm taking kappa bar of a, this is a join irreducible torsion class. So it looks like it's generated by one of these modules M. And when I apply kappa twice, what I'm getting is Apply kappa twice, what I'm getting is another joint irreducible torsion class, but the M that generates it is now tau inverse of the M I started with. Okay, so I'll stop there. Thank you all so much for listening. And I see that there are lots of questions that I cannot read your questions and talk at the same time, but I'm happy to try to answer them now. All right, thanks, Emily. Questions that anybody wants to pose in person? I had a small question. When you're back in the distributive world, you've got these two distributive laws, but they're equivalent. If you assume one, you get the other. Is the same true in the semi-distributive case? You can assume one of the semi-distributive properties and then you get the other one for free? Properties, and you get the other one for free, or do you need both of them? You do need both of them. There are lattices. I'm sorry, I'm going to try to go back to the slide that Bruce is talking about. So he's looking at the definition of semi-distributive lattices. Those two conditions. And there's these two conditions. Yeah, so you can think of this one as having to do with the join, and this one having to do with the meat. And there are lattices that satisfy one without the other. Okay. So do you get anything interesting if you? So, do you get anything interesting if you just assume one? That's so the thing about it is, is actually I'm using, I really need both. I think, I think I need both because I'm using both the join and the meet here. This Kappa map, especially to be able to extend it to all of the elements of the lattice, I need both of these conditions. Both of these conditions. These conditions guarantee that I get this special representation of the elements of my lattice as both a join of join irreducibles and a meet of meat irreducibles. Gotcha. Gotcha. Okay, thanks. Yeah, thanks for asking. I also had another question, if it's easy. Sure. In your slide, where you have In your slide, where you have both the Creveras complement orbit behavior and the Kappa map orbit behavior? Yeah, let me go ahead. Sorry, I know it's terrible watching somebody like scroll through things, but there we go. Okay. Yeah, so here, are you implying that you can use the theorem for the Kappa orbit behavior to prove the one for the Kreveras complement? So this is for the non-crossing partitions, right? So the size. Crossing partitions, right? So the size of P is the number of blocks? Yes. Oh, so the second statement is supposed to be a relatively trivial one? The second statement, yeah, this is like historical. And then the first statement is independent of that one. Like the proof is different. It's not terribly difficult to prove. And you could imply, you could sort of say, because I have. Because I have, because I have this commutative square here, this you could use to imply that the same kind of orbit behavior has to happen for non-crossing partitions. I read it as if it was the average for the size of the anti-sane, which I think is a more difficult theorem. Oh, okay. So, Emily, is it your for top theorem? Is that generalizing Panushev's conjecture or not? Yeah, that's where I was kind of pulling. That was what I was the, what I was thinking of, yes. I see. And so how broad is the class of things to which this applies then? Is it a few finite examples of post-sets or lattices, or is it much broader? Or lattices, or is it much broader than that? These are, this applies to any lattice of torsion class that's finite. So these are the condition on the algebras is something called functorially finite. It's a large class of algebras. And actually, I think what maybe Nathan Williams and Hugh Thomas might have a like a theorem about. Like a theorem about they are also thinking about some of this representation theory and combinatorics. And maybe something that I meant to say, but I didn't say, which is not directly related to your question, Tom. But the way that Nathan was doing row motion in his talk, where he was labeling the edges and like swapping down labels with up labels. With up labels, if you recall that, you can also compute this kappa map in the same way. There's a way to label the edges of these lattices, and then you're just swapping up down labels. I think I'm doing up labels with down labels. I see. Well, the MC is telling me that I'm out of time to ask more questions, but I'd love to chat with you more about what the concrete examples that manifest this are. That manifests this are maybe in the afterword after discussion, but we should probably move on to the next talk. So let's thank Emily again. Thank you.