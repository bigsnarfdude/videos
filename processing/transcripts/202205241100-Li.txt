So today I'm going to talk about randomization inference beyond the sharp null. And in particular, I will talk about how to infer quantiles of treatment, individual treatment effects. So randomized experiments, like since Fisher's adaptation like 100 years ago, have become the gold standard for drawing causal inference. The randomized experiments can help balance all potential confounding factors on average. Confounding factors on average, like, and it now is widely applied in clinical trials, field experiments, and A-B testing. Moreover, these randomized experiments allow randomization-based inference for causal or treatment effects. In particular, we can use only the randomization of the treatment assignment as the reasoned basis. And there are at least two types of. Two types of randomization-based approach. One is Fischer's randomization test, which I will introduce in detail later. And another one is Neyman's repeated sampling inference. Fisher's approach typically involves constant treatment effect assumption in the sense that the individual effects are constant across all units. And Neyman is mainly focused on the average treatment effect. So there are some. So, here I want to say there are some limitations of the current approach, the current randomization-based approach. First, Fischer's approach, as I said, you need to specify, Fischer's approach works only for SharpNAL, where you need to specify all individual effects, such as all the individual effects are the same and are the same as a constant C. In practice, how the treatment effect vary across individuals are generally unknown. Because individuals are generally unknown. So, Fisher's approach can be limited. So, Neyman focused on this inference of the average trim effect, which allows effect heterogeneity. But Neyman's approach requires large sample approximation. And more importantly, sometimes the average may not be the most preferable summary of treatment effects. So, here I just illustrate using a very tall example. Suppose I can Tall example, suppose like the pandemic or certain public policy decrease the income of most people in the population, but increase the income a lot for a tiny proportion of the population. Then it is possible that the overall, like the average income is increased. In this case, simply concluding that the pandemic or certain policy is beneficial for the overall population can be misleading. And the reason is that. Be misleading, and the reason is that the average is sensitive to outliers. So, in order to avoid such sensitivity of the treatment effects to outliers or to make the treatment effects more robust, people have used quantiles to characterize the treatment effects. For example, in econometrics, people use a lot about this, use this. About this, I used this quantile treatment effects a lot, which is say the median of the treatment potential outcome minus the median of the control potential outcome. And in this talk, I will instead focus on the quantiles of individual effects, which is the median of the difference between these two potential outcomes, okay? Which I think is a more direct measure of the treatment effects. And moreover, this can immediately tell us the number of unit authority. The number of units, sorry, the proportion of units with effects greater than any threshold C, or equivalent for, for example, the proportion of units that actually benefited from the treatment. So here I want to emphasize, compared to this blue quantity, the average effect or the median effects, this quantiles of individual effects, as well as the proportions of units with effects greater than C. These two red part, red as. two red part red estimate compared to the blue one they are not point identifiable the blue ones are point identifiable because they are they are the comparison of the marginal distribution of the two potential outcomes but this red quantity they depend on the jointed distribution of the two potential outcomes but in however in practice for each unit we can observe one of these two potential outcomes so their joint distribution is not identifiable such as the correlation between these two potential outcomes The correlation between these two potential outcomes. So, the inference for this quantity will be more challenging, but as we will show later, we are still able to get confidence intervals. Although we cannot get a consistent estimator, we can still get a valid confidence intervals. So, throughout the talk, I will use this potential outcome framework. So, here for each unit I, I will use y1, y0 to denote a treatment. Y0 to denote a treatment and the control potential outcomes. So each unit will have these two potential outcomes. And if a unit receives treatment, we will observe y1, otherwise, y0. I will use this tall i to denote the individual treatment effect. So I will consider the difference between these two potential outcomes. And then I will introduce the ZI to denote a treatment assignment indicator. So ZI equals one if a unit receives treatment zero. Units receives treatment zero otherwise. And the observed outcome will then have this expression, which is one of these two potential outcomes, depending on the treatment assignment. So if a unit receives treatment, observed outcome is Y1, otherwise Y0. Moreover, I will, as I mentioned before, randomized experiments allow randomization-based inference. So that's the inference framework I will use throughout the talk. In particular, we will. In particular, we will view all the potential outcomes as fixed constant. This is equivalent to conducting conditional inference conditioning on all the potential outcomes. So, compared to the so by viewing them as constant, this randomization inference framework has the advantage of imposing no distributional or model assumptions on the potential outcomes. For example, we do not need to assume this potential outcomes. We do not need to assume these potential outcomes are independent across your units or are identically distributed across units. They can be arbitrary constant. They can be generated from arbitrary distribution. And moreover, like this, we do not need to assume the units are ID samples from a larger population, from a superpopulation, which is often hypothetical because in practice, many randomized experiments, they just enroll convenient samples. In raw convenient samples instead of actually doing random sampling from a bigger population. So, in this randomization-based inference framework, we will mainly focus on the units in hand or the units actually in the study. And the randomness in the data will come solely from Z because only Z is random. The potential outcomes, individual effects, they are all constant. So, therefore, understanding a treatment assignment mechanism is crucial for inference because it governs the data generating process. So, here I also want to emphasize the observed outcomes random simply due to zi. It can either be y1 or y0. So, in case you are not familiar with Fisher's approach, here I will. With Fischer's approach, here I will briefly review a facial randomization test. So, here I consider a simple example of six units. It's a completely randomized experiment. Three units receive treatment, the remaining three receive control. And for the treated units, we observe their treatment potential outcome 350. For the control units, we observe their control potential outcome 401. Okay, and these are the missing potential outcomes. And these are the optimal. And these are the observed ones. So Fischer considers this Schrafnon hypothesis that the treatment effect has the treatment has no effect for any individual. Here it's like the two clinical outcomes are exactly the same for all the individuals. So these types of null hypothesis are often called sharp null hypothesis because it speculates the treatment effects for every individual. So here the treatment effects is zero for every individual. Is zero for every individual. So, like a usual statistical testing, we will choose an appropriate test statistics. So, here, for example, we can choose a function, which can be a function of the observed outcome, observed treatment assignment. So, in order to get a valid p-value, we need to know the non-distribution of the test statistic. And fortunately, in this case, for under 50%, In this case, for under facial sound of no effect, the null distribution is known exactly. The reason is that based on this observed data and the null hypothesis, we can actually impute all the missing potential outcomes. So we can actually know all the potential outcomes. As you can see here, the red ones are imputable and they are the same as the observed one because of a fisherious null hypothesis. The two outcomes, the two potential outcomes are exactly the same. Outcomes are exactly the same. So, therefore, for any possible assignment Z, we can get from this, because we know all the potential outcomes, we can immediately get all the observed outcomes. We can then get the value of the test statistics under this possible assignment to Z. And because it's a randomized experiment, the distribution of Z is known exactly. So, the distribution of a test statistics is also known exactly. Is also known exactly. So, in practice, we often approximate the distribution by a Monte Carlo. So, here we just generate a random assignment following the same distribution as Z, and then we get the value of a test statistics. And the histogram of these values of test statistics will actually approximate the non-distribution of the test statistic. So, hypothetically, like So, hypothetically, like intuitively, it's just like running the experiment multiple times and see how extreme the observed value of a test statistics is compared to the histogram we get. And the whole procedure, so the tail probability evaluated at this observed value of test statistics will be a valid p-value. The whole procedure is quite similar to a usual permutation test, but the reasoning is slightly different. Here, the validity is Different here, the validity is guaranteed by the randomization, but usual permutation test is the exchangeability of the outcome of these units. So, Fisher's approach actually works for all general Sharp non-hypothesis, not necessarily the non-hypothesis of no effect. For example, people often consider the null hypothesis of constant effect. So, here's a constant effect of one. So, in this case, Of one. So, in this case, the Fischer's approach still works. The procedure is almost the same, except that now the imputed potential outcome changes. So, the blue one are the imputed potential outcome under this null hypothesis of constant effect one. So, the treatment effect for all individuals are one. And in practice, we often invert a test for a sequence of constant effects to get. Sequence of constant effects to get an interval estimate. And this will be a valid confidence interval for the average, for the treatment effects if the individual effects are indeed constant across all units. So Fisher's approach works straightforwardly for any randomized experiment as long as you can simulate the assignment. And it works for any test statistics and it works for any sharpener. Sharp noun. However, like Fisher's approach has received a lot of critics. The main reason is the sharp null is too restrictive. Like the usual interval estimate, assuming constant effects is also too restrictive. So here, as commented by Gauman in his blog, like the so-called facial exact test, which is a randomization test, almost never makes sense as this test of an uninteresting hypothesis of exactly this. Hypothesis of exactly zero effects or worse effects that are non-zero but are identical across all units. Essentially, we're saying that the null hypothesis is too restrictive. And this facial sharp null has been viewed as like uninteresting and academic. There are also defenders of facious approach, like for example, in Inverse and Rubin paper or in Inverse and Rubin book, they show they come. They commented that a fisher's approach is still useful for assessing whether the treatment had any effect at all, and it can be actually a preliminary step to determine whether further analysis is warranted. Others have argued that facious approach can be more preferable to shortcomings of parametric models, which can be sensitive to assumptions about heavy tails and also may not be able the usual. Uh, may not be able, the usual parametric model may not be able to account for complex treatment assignment mechanics. Okay, so here the question I want to ask is, is Fascia's approach, the randomization test, really so restrictive? Okay, is it really so restrictive that it works only for the like non-the sharp non-of-no effect for any individual? So now. So now I will show you Fischer's approach actually has a broader justification. And I will show, illustrate our results using a study from the from using an educational study, a study from education. So the goal is to study, it's a randomized experiment, and the goal is to study the effects of professional development on elementary school teachers. So a study involves about 200, more than 200 elementary school teachers, and this teacher will be randomized to treatment and control. So teachers in the treatment group will take courses on electric circuits. And the outcome we're interested in is the gain on test scores, content test scores. And for illustration, we consider it as a completely randomized. Consider this as a completely randomized experiment. The actual one is stratified. It's a stratified randomized experiment. So here I first do inference using facial approach. I test the facial strapnol of no effect. I get almost zero p-values. So this indicated that the treatment has the effect on this, like on the test scores of these teachers. I further invert. I further invert a test for sequence of constant effects, like I get a 90% confidence interval, which shows, which is from 16 to infinity, showing that the treatment benefit these teachers at a magnitude of at least 16. So they increase the test scores by at least 16. So both of them indicate that the treatment has a strong beneficial effect. Strong beneficial effects on the teachers. However, like people can criticize these results first, the p-value for facious null is not informative because the sharp null is likely to be false. Like the treatment is likely to have effect for some of the teachers, right? It's not exactly zero for all teachers, not likely to be exactly zero for all teachers. So, second, the 90% confidence interval is not likely to be. Percent confidence interval is not likely to be a valid or meaningful interval since the individual effects are likely to be heterogeneous. Like generally, it's not constant across all teachers. So here we first, in the first half of the talk, we will give a broader justification of facial analyzation test. First, we show that the almost zero p-value is actually valid for testing the null hypothesis that all the individual effects are non-hypy. The individual effects are non-positive. So, therefore, given the p-value is so significant, we can know that at least some teacher benefited from the treatment. We will reject this now, indicating that some teacher will have individual effects greater than zero. Second, we will show that this interval estimate is always a valid confidence interval for the maximum individual. confidence interval for the maximum individual effect. So there's at least, there exists a teacher benefited from this training, benefited from this course or this professional development program by at least 16%. So increase the teacher's test score by 16%. So here, we essentially show that the facial randomization test results from features approach have a more broader meaning. Broadening even when the treatment, even when the individual effects are heterogeneous across units. However, people can still argue such results may not be informative enough. The reason is that the almost zero p-value indicates that there exists one teacher benefit from the treatment, one teacher benefit from the treatment, and this interval is only for the most impacted teacher. So essentially, the inference is. So essentially, the inference is about one teacher, which is the teacher that is most affected by the treatment. The people may still be, like, seems the results may not still not be informative enough. So actually, we further generalize Fisher's approach to infer all quantiles of individual effects. So, here, because we are considering this randomization-based inference, we focus particularly on the unexpected. Particularly on the unexperimental units in the study. So ranks essentially can be essentially the order statistics of the unindividual effects. So besides the interval for the maximum individual effects, we can also construct intervals for the 41st largest effect. And it's the intervals from 10 to infinity. So we are confident that at least like 41 teachers benefited from the Teachers benefited from the program from the cost by a magnitude of at least 10, and at least 89 teachers benefited from the treatment. So this equivalent implies that we're 19% confident that at least 17 benefited by a magnitude of 10, and at least around 40% of teachers benefited from the cost or from the treatment. So here I only list like two individual effects like at two like here I only list the 41st largest one, 89th largest one. Actually, the approach works for all quantiles of individual effects, all equivalent individual effects at each rank. So we can infer all of them simultaneously. And here's a figure showing the lower confidence limit of individual effects at each rank. Individual effects at each rank. So the first one is a confidence interval for largest effect. So the first interval is actually the usual interval from features approach. Like this one is for 12 largest effect. And if we are interested in the number of units or proportion of units actually benefited from this program or with individual effects greater than one, you can simply count the intervals that do not cover zero. And moreover, all the And moreover, all these confidence intervals for the quantiles of individual effects, as well as for the proportion of units with effects greater than any threshold, all of them are simultaneously valid. So there is no need for like multiple analysis adjustment. So indeed, this figure actually gives an undimensional confidence set for the whole individual treatment effect vector. But unlike a usual undimensional set, the set the confidence The confidence interval of the confidence set we get here is easy to visualize and interpret. And it's best summarized in such a figure. So now I'm going to summarize our main results. So we think the critics on Faisal's approach are largely correct as far as they go, but they may be overly restrictive. We show that many randomization tests can actually be valid. Tests can actually be valid for a bounded null hypothesis. So, a sharp null can be valid for the null that the treatment effect is bounded by zero. And for a general sharp null that specify the individual effects as delta i, the test for this sharp null can also be valid for this bounded null. And the usual interval estimate can always be a valid confidence interval for the maximum individual effects. individual effects. And we further extend the facial approach to infer all quantum of individual effects. They are simultaneously valid. And it equivalently tells us like the proportions of units with effects greater than any threshold. And the results is best summarized in this figure. So here the figure stops around 100. The reason is that all the remaining confidence intervals for the smaller quantiles of individual effects are actually like minus. Actually, like minus infinity to infinity. They are not informative. So I cut the figure at about individual effects at rank 100. I think this slides basically summarize the main message of this talk. So I want to stop here to see if there are any questions. Okay. Okay. Actually, I do have a question. Okay. So this is all based on a randomized study so you can work out your Z variables, right? Can one extend this to observational studies where one estimates the C's from something like a propensity score? Actually, as I illustrate As I illustrate later, so the inference for quantiles will require some exchangeability in the treatment assignment. So we have extensions, it's not like universe perpendicular weighted approach, but based on matched observational study, we can extend it to match observational study. We can also conduct a sensitivity analysis. So you can do it for matched things, but not in general for inverse propensity scores. Is that the Is that the answer? Yes, you are right. We can extend to matched observational study, but inverse preventative score, the quantile approach, our inference for quantiles will not hold. At least, that's our current understanding. Yeah. Thank you. Thank you. Okay. So now I will continue. I will continue. So I will first give the details for the broader justification of feature analyzation test. So I will first introduce this notation. I use y1, y0, this tau z to denote the potential outcome vectors, individual effect vectors, and the treatment assignment vector for all the units. The observed outcome then has this form. This form, right? So, here this small circle denotes the element-wise multiplication, but the observed outcomes like one of these two potential outcomes, depending on the assignment. So, we consider a general Sharpen hypothesis that specify the individual treatment factor vector as delta, as delta. So, here, delta is some predetermined constant. So, like, as I described, the first step is to As I described, the first step is to impute of the feature realization test is to impute the missing potential outcome. And this is the way we impute them. So for the treatment of potential outcome, it's the same as the observed outcome of treated units. And for control units, we'll yeah, the hotel cut my internet. I have to log in again. Yeah, sorry. Yeah, I should be able to share. Yeah. Okay. That works. Yeah. Thank you. Yeah. Sorry for the interruption. Yeah. So here, I just want to emphasize, so these are the impuity of potential outcomes, and they are uniquely determined by the observed assignment, the hypothesized individual effects, and the two potential outcomes. So here I use Z delta. So here I use Z delta, the subscript to emphasize the dependence on the first two. So like after we impute a potential outcome, we then need to choose the test statistics for the features approach. So that's an important component of like facial analyzation test. So here the test statistics can be a general like function of the assignment vector and the outcome vector. So here we can So here we consider a general form, which is a function of the observed treatment assignment and the observed outcome. So because we imputed the potential outcomes, so under any possible assignment A, we can then get the corresponding observed outcomes, which has this form, based on the imputed outcome. We can then get the value of the test statistics at each possible values of A. And then the And then here A is just a random vector following the same distribution as the actual treatment assignment. Then the distribution of this one is the imputed randomization distribution of a test statistics. It's an imputed non-distribution. And this one denotes the tail probability. So we evaluate the tail probability at the observed value of a test statistics. We get a facial randomization p-value. So when the So, when the null hypothesis holds, this imputed observed outcome will be the same as the true observed outcome if the treatment assignment was indeed A. So, therefore, the distribution of the imput, so the imputed non-distribution will be the same as the true distribution of a test statistics. So, therefore, this guarantees the validity of the facial randomization test for the Sharp null. However, like when a null hypothesis However, like when a null hypothesis fails, this one may not be a valid p-value. And here I also want to emphasize the p-value is uniquely determined by the observed assignment, hypothesized individual effects, true potential outcome, and treatment assignment mechanism. Because these three are fixed, although the potential outcome are unknown. So I just emphasize the dependence on the first two, the assignment, observed assignment, and the hypothesis. Observed assignment and the hypothesized effect. Okay. So, what I just described, this is a figure, summarizes the procedural facial analyzation test. We chose the test statistics, right? We imputed the potential outcomes, then we get a non-distribution. This is an imputed non-distribution. And in practice, it's often approximated by Monte Carlo. We just repeatedly do the experiment many times. And then we evaluate a tail probability as the observed value of the test statistics. Statistics. There's another approach, which is like, so this form of test statistics is used in this Inbans and Rubin paper. There's another form used in Rossenbaum's paper, which instead of using observed outcome, uses the imputed control patch outcome. The whole procedure is the same. Test the statistic, get imputed, and none, get a p-value. So for simplicity, I will call the first one as the inbuilt Rubin P-value. The inbounds Rubin p-value, the second one as the Rossenbaum p-value, just to distinguish them. And both of them are valid for the sharp null. And I will show you, like when the test statistics satisfy certain property, they are also valid for the bounded null. So here I list the property, like the first one is effect increasing, in the sense that if we increase the outcome of treatment units, decreases the outcome. Of treatment units decreases the outcome of control units, then the value of a test statistics will increase. The second one is differential increasing. So, here A is just assignment vector. It's a binary 0, 1 vector. So, if I increase the outcome of a subset of units corresponds to A with elements with being one, then the increase of the test statistics is maximized when the Maximize when these units happen to receive the treatment, when the A is a treatment assignment. The last one is called distribution 3, which says that the distribution of test statistics no longer depends on the outcome vector. So different from the previous two, the last one also depends on treatment assignment mechanism. And the definition may look abstract, but they actually hold for most commonly used test statistics. For example, The statistics. For example, here's a generalization of a difference in means: summation of certain transformed treated outcome minus summation of certain transformed control outcome. If the transformation is monitoring increasing, then the statistics will be both effective increasing and the differential increase. The second class is a generalization of a rank-sum statistics. When phi is identity, this is the Wilson rank sum. The Wilkeson rank sum. So it calculates a rank sum of treatable units. And this statistics will be effective increasing as long as this phi is an increasing function. Most of the time, we always choose phi to be an increasing function. And it will be distribution-free if the assignment is exchangeable and we use random method for types. So by exchangeable, I mean the treatment assignment for The treatment assignment for all units are exchangeable. So, two examples are the completely randomized experiment and the Bernoulli randomized experiment. In Bernoulli randomized experiment, all the ZI's are ID, Bernoulli random variable. So here, I want to emphasize to deal with ties. So to rank units with equal outcomes, we have to use this random method for ties instead of the classical, like taking average ranks. Because if we take average ranks, the statistics will no longer. Ren, the statistics will no longer be distribution-free, it will depend on the outcome factor. And to implement a random method for time, we can just first permute the order of the units in the data. Then we just rank them based on their indices. So for units with equal outcome, we give the one with greater index, with greater index the larger rank. So now I'm going to present the main results. So first, if the test statistics is either differential increasing or effect increasing, the Rawson bomb p-value for sharp null is valid for bounded null. Similarly, if the test statistics is effect increasing, then the inverse looping p-value is valid for the boundary null. And this bounded null, I closely related to. And this bounding now is closely related to monotonicity in instrumental variable analysis, which I will have an example later, as well as Pareto efficiency. And this, also I want to mention this bounded norm with like this with constant treatment effect assumption, like is equivalent to that the maximum individual effects is bounded by C. So the usual test for usual For usual test for non-hypothesis of a constant effect C, like by these two properties we just listed, can also be valid for test null hypothesis that the maximum individual factor is bounded by C. So therefore, the usual interval estimate by inverting a test of inverting a sequence of constant facts can always give us a valid confidence interval for the maximum individual effects. And here I just want to mention the confidence. Here, I just want to mention the confidence that is often an interval, has a form of an interval. So, here is just a simulation study. I just want to say the usual difference means Wilkinson-Rank sums has no power to detect positive maximum effect when the average effect is non-positive, because they mainly focus on the average effects. So, here is a new class of rank sums. is a new class of rank sum statistics which is the wilkson rank sum so basically it take it to emphasize more on larger ranks it's the original rank to a certain power essential original rank to a certain power and they can give us a larger power to detect positive maximum individual effects uh i think i only have five minutes remaining so i will quickly go through uh the the Through how to do inference for quantiles of individual effects. So, as I mentioned, we will first, the quantiles are essentially the order statistics of the individual effects. So, here we will sort them from the smallest one to a largest one. And we will consider this null hypothesis that the individual effects at the rank K is bounded by C. So, here I introduce this HKC to denote a set. To denote a set of possible individual effects under this non-hypothesis. So, in order to get a valid p-value, one intuitive way is just to take a superior of this valid p-value for sharp null over all possible, like, over all possible values of the individual effect vector under this null hypothesis. We take a superior over delta possible value. Over delta, possible value, delta, possible values of tau under this null hypothesis. However, such an approach is generally computationally infeasible. So notice that the imputed distribution also depend on delta. And such a dependence can be quite complicated. The imputed distribution in practice is often approximated by Monte Carlo. There's no closed form of it. So such an optimization is almost Such an optimization is almost infeasible because of the complicated dependence of this histogram on delta. So, here we will use Rossenbaum's p-value, and in particular, we will choose a distribution-free test statistics. In this way, we avoid the dependence of the impurity non-distribution on delta. So in this case, the impurity non-distribution will just be a fixed function, no longer depends on delta or no longer depends on the observed assignment. Depends on the observed assignment z. And in this case, the superior of the p-value reduces, computing a superior of a p-value essentially reduces to computing, essentially reduces to minimize the test statistics over possible values of individual effects. So the test statistics often has a closed form. So the computation is much simplified. But however, like general but however like general is for general statistics still difficult the reason is that under the null hypothesis we're essentially hypothesized that k individuals like for k individuals their individual effects is bounded by c but we do not know which k individual is bounded by c right and there are in total unchose k waves so brute force enumeration is actually mp half MP high. So, here I want to give some intuition on how to minimize the test statistics, which has this form. So, oftentimes, we will choose T to be effective increasing statistics, like a rank-sum statistic. Intuitively, in order to minimize this test statistics, we want to make this term as small as possible, equivalently, make delta, the hypothesized effect, as large as possible. As possible. And under our constraint, we can at most let k elements of delta to be c and the remaining n minus k elements of delta to be infinity. But the question is, which k? So notice that delta olympics play a role for treated units. So our intuition will be we let the n minus k treated units with the largest outcome to have infinite delta. The delta. That's what we allocate all the largest delta to those treated units with the largest outcomes. In this way, we hope we can minimize the test statistics. And actually, this intuition leads to the correct solution for the rank sum statistics, as long as phi is an increasing function. So here we also emphasize we use a random method for time because the test statistics needs to be distribution free. And for simplicity, we will call these statistics with increasing transformation and the random method for ties as the REC score statistic. So here I'm ready to summarize the results. So basically, in this way, we get an almost a closed form p-value for testing non-hypothesized quantiles of individual effects. Here, I just summarize the results. We have a p-value, which almost has a closed form for testing this non-hypothesis. For testing this null hypothesis, inverting it, we'll get confidence intervals for quantiles of individual effects. And equivalently, we can test like the number of proportions of units with effects greater than C. The reason is that number of units with effects greater than C is bounded by minus K is equivalent to that the individual effect at the rank K is bounded by C. And this P value can also be valid for this one inverting. Also, be valid for this one, inverting it. We can also get a confidence interval for the number of units with effects greater than c yeah. Since the times are, since it's already 50, so I won't mention these sites, just showing you, I just want to tell you these intervals, they are all simultaneously valid. So there is no need for a multiple analysis correction. We can infer all the quantiles simultaneously. House simultaneously. This one is just the simulation just shows difference and rent. It's also beneficial for infer number of units with effects greater than zero. Since I don't have time, I will skip the examples and directly go to our conclusion. So the rise of like non-parametric causal inference, like randomization. causal inference like randomization based inference in the traditional Neyman Rubin in the Neyman Rubin tradition has received a lot of attention recently like in quantitative social and biomedical sciences because like also because the popularity of field experiments and the random randomized clinical trials and we show that the current view of facial organization tests is too restrictive they can have broader meanings and we further extend the facial And we further extend the Fischer's approach to infer all quantiles of individual effects, as well as number of units with effects greater than any threshold. This moves beyond Fischer's constant effect, as well as Neyman's average effect. So we think the randomization inference deserves a secure place in quantitative applied scientists' toolbox. So that's all for my talk. And this is the reference as well as a package for inferring quantitative. as a package for inferring quantiles of effects.