First I want to thank the organizing committee for providing me this opportunity to present the work here. So today I'm going to talk about a knockoff-based approach for identification of putative causal loci in whole genome sequencing studies. It seems like I cannot move to the next slide. Oh, maybe I start sharing first. Okay, so let's first look at some results from the From the conventional association test. So, for the conventional association test to control the type 1 error rate, a standard approach is to control for the family-wise error rate by applying a bonfirernic correction. So, a potential issue with this type of approach is that bonfireric correction can usually be very conservative and only very strong signals can pass the threshold. So, like in this example, so this is Like in this example, so this is a study for Alzheimer's disease and the sample size is about 5,000. By using conventional association test, most of the rejections comes from crops on 19 around the APOE region, which is a well-known loci for ozimus disease and corresponding to very strong signals. So, beyond those, this set of rejections, we're not able to claim any other associations, even with Other associations, even we know there are some other weaker signals identified by some other studies with much larger sample size. So, another challenge for GWAS is the genetic variants are usually highly correlated. So, this problem can be alleviated at a certain level if we fit a joint model. But in practice, so in order to make the method computationally scalable, so we use a scan status. So, we use a scan statistic or a marginal test. So, then, like, kind of the same problem has already been described by Shoning. So, this is also the analysis for Alzheimer's disease. We zoom into the APOE region. So, we know that in this region, most of the causal variables come from the APOE gene or APOC1 gene. But now, like we see for association test, all signals. Test all signals above this monk threshold, all the grief ones, will be rejected. So we not only see the causal variables, also we see a set of rejections simply due to correlation. And also in our days, the cost for sequencing has become much cheaper, which implies that there will be more and more data set, even at the biobank scale, becoming available. And another consequence. And another consequence of the low cost of sequencing is that the sequence coverage becomes deeper and the result for rare mutations becomes more reliable. So it is not meaningful to simply exclude those rare variants from our analysis anymore. So today I will try to introduce a new approach for whole genome sequencing study that attempt to address those challenges and To address those challenges and provide a partial solution. So, first, in order to reduce the number of false rejections due to correlation, so instead of testing for marginal associations, so we may need to switch to a set of non-hypothesis corresponding to conditional independence. So, this is also mentioned by Xiaomi. So, actually, we would like to test this set of hypotheses. We want to know whether each Hypothesis: We want to know whether each variable is independent of the trait conditioning on the other variables, as well as a set of covariates. So, those covariates are introduced to adjust for potential confounding effect, such as gender, age, batch effect, and population stratification. So, it is also for genome sequencing studies also common to perform the window-based analysis. perform the window-based analysis. And then we can also write down the null hypothesis for conditional independence in this case. So the inference is performed by using knockoffs. So the knockoff is a very recently introduced method for false discovery rate control. And also in recent three or four years, there are more work about the knockoff-based inference for genetic or genomic studies. For genetic or genomic studies. So, here, like, I only list a very few of that, so there will be more indeed. And also, this session is about the knockoff-based inference. So, let me first provide a brief overview of the method. As Shoni mentioned, like the knockoff-based inference can usually be finished within those three steps. So, first, we need to construct a set of the synthetic variables as the next. Synthetic variables as the negative controls. So if we have p original variables, so then we need to construct p synthetic variables as the knockoffs. So those knockoffs are constructed to satisfy the so-called exchangeability condition. So that is, if for any variable, any g variable, so if we swap this variable with its null cough, so the joint distribution for all variables. Joint distribution for all variables still remains the same. So, this exchangeability condition implies that indeed we are trying to construct a valid control group. And also, I want to emphasize kind of the this exchangeability condition is the key for conditional inference and valid type 1 error rate control. As long as this condition, Control. As long as this condition is satisfied, we have some freedom to choose different types of test statistics. So then the natural question is that how to construct those variables. So I provide an example for multivariate normal. So if the original variable and the knockoff variable jointly follows a high-dimensional multivariate normal distribution with this specific covariance matrix, then the exchangeability condition. Then the exchangeability condition is satisfied. So the next step is to calculate the feature importance statistic. As I mentioned before, the choice can be very flexible as long as this set of statistics satisfy the so-called flip sign property. So for example, we can run the soul jointly with the original and the knockoff variables and define, oh, sorry, and then define the test statistic by contrasting the coefficient for original. The coefficient for original variables to that for the knockoff variables. This is just one possibility. And the key property is that if we look at those statistics, so the distribution for non-features, so the distribution for non-features should be symmetric around zero. But for the non-known features, it can be positive with a higher probability. And if we look at the signs, so they are simply. Look at the slans, so they are simply Bernoulli IV Bernoulli random variables. So they are some valid p-values, but those p-values can only take two discrete values. So we call that one-bit p-value. So the last step is to run a knockoff procedure. And we define the threshold is also shown before. So this threshold is actually this ratio is an estimate of FDP. So if So, if we look at the denominator, which is exactly the set of rejections, and if we look at the numerator, because the signs can be either positive or negative with equal probability, so the numerator is the estimation of the number of false rejections out of that set of rejections. So, essentially, we would like to control the FDP at the target level. So, in practice, we may make this structural slightly more conservative. structural slightly more conservative by adding a one to the numerator and find the selection. So because this is an estimation of the FDP, so similar to the other approach targeting at FDR control, so we can introduce a Q value for each variable. Okay, after this brief overview, let's look at our knockoff screen. So our knockoff screen exactly follows those three steps. Folllows those three steps. So, let me provide more details for each step. So, in terms of the construction of the non-cuffs, so we apply an approximation to the correlation structure. So, we'll assume the LB matrix can be approximated by a block diagonal one. And also, we'll assume the variables follow a normal distribution. So, here you may ask the question, so the genetic variables essentially So, the genetic variables essentially take discrete values. So, why we can apply a normal approximation. So, indeed, like in our work, we realize the marginal distribution for variables is not that important. So, in order to make knockoff-based inference, it's more important to find a model that can capture the correlation structure, to capture the correlation structure jointly for all the variables. So, later, we'll provide. So later we'll provide some evidence to try to convince you that essentially normal approximation is quite robust. So in terms of the construction, it's essentially an adaptation of the original sequential conditional independent pairs algorithm proposed in the 2018 paper. So we need to sequentially sample from this conditional distribution. So with those previous model assumptions, actually Model assumptions actually, we have some more information about this conditional distribution. So, first, the form of the conditional distribution is still normal. And then, like, if we look at the variables that we're conditioning on, so by making that block diagonal assumption, so this set of the variables can be greatly simplified. So, we only need to condition it on the nearby variance coming from the same block. And also, the conditional mean is a little bit conditional mean is a linear combination of the nearby ones. So that is the motivation for the algorithm. So we're going to sequentially fit a linear model. So in this linear model, the nearby variables are used as the explanatory ones. So we use this linear model to estimate the conditional mean. And then in order to sample from that conditional distribution, we do not estimate the variable. We do not estimate the variance. So instead, what we did is that we estimate, so we calculate the residual after fitting this linear model and then permute the residuals. So it turns out that by permuting the residuals, it works better for rare mutations. So this algorithm can also be generalized to construct multiple groups of the knockoff variables. So for a single group of So for a single group of knockoffs, we can think like we have a one-bit p-value. So if we construct a multiple group of knockoffs, then this p-value has a higher level of precision. So this is some empirical verification of the exchangeability condition. We try to check the quality of the construction. So here what we did is that we calculate the pairwise. Is that we calculate the pairwise correlations and we calculate the pairwise correlation for the same pair, but with the variables comes from either the original group or the knockoff group. And then like we plot them against each other, if the exchangeability condition is satisfied, then all the points should lie on the y is equal to epsilon. So this is the visualization for common variants with minor allele frequency. Common variants with minor allele frequency larger than 0.01. So then we see the exchangeability condition is well satisfied. And also we have the result for rare mutations for the rare variance. So for rare variants, it's generally more difficult to generate the exchangeable variables, but we see that in this case, it's still reasonably well. So in terms of the model, So, in terms of the multiple knockoffs, there are some other motivations for constructing that. So, first, it can help us to overcome the difficulty imposed by the extremely sparse signals. So, like what we mentioned before, this ratio is an estimate of FDP. So, if we would like to control the FDR at the level 0.1, for example, in order to afford for one thousand. To afford for one false rejection, so we need at least 10 true signals to do so. So, but sometimes we do not have that many true signals. So, now like if we construct multiple group of now curve, because the p-value now has a higher level precision. So, reflected in this estimate, it means the numerator can be multiplied by a coefficient 1 over m. m is the number of groups. So, if we construct, for example, 10 groups, We construct, for example, 10 groups. So, now to afford for one false rejection, we only need one true signals. So, another motivation for that is multiple knockoffs can help us to stabilize the selection result. So, the construction of the knockoff variables is random. So, the selection result in some in it's also it also has some randomness. It is possible like on the It is possible, like on the same data set, we construct the knockoff twice and we'll end up with slightly different selection result, which is not desirable. But if we construct multiple group of knockoffs, the selection result will be more stable. Like here, like we repeatedly sample knockoff 100 times and visualize the selection frequency. So the blue line is the selection frequency for multiple knockoffs. Say for multiple null curves, and the orange one is that for single null curves. So we say it will be more stable. So, in terms of the choice of test statistic, sorry, and also we compare the computational efficiency. So, this construct, this sequential generator is indeed quite efficient because we only need to sequentially fit linear models. So, in terms of the choice of text statistic, so we in knockoff for knockoff screen, like we integrate multiple type of test statistic into this framework. Actually, this is a very long list. So, what we did is that for each test, we can obtain a p-value and then we apply a Cauchy combination to get the combined result. So, what I want to So, what I want to highlight here is that if we take a closer look at those test statistics, so we will realize they are still marginal test statistics. But we have constructed knockoff variables that satisfy the exchangeability condition. So even here, we are using marginal statistics, so we can still provide a valid inference for conditional independence. So, actually, I think this is a very unique feature. This is a very unique feature for knockoff screen. So, like in future, if we have some more sophisticated tests available for association, no matter whether they are marginal or conditional. So we can simply add that test to this list and then run the knockoff-based inference. So then we can have the valid result for conditional independence. So, this is the simulation result. So, we see the blue curve corresponds to the performance of Knockoff screen. So, actually, we see kind of a lot of power gain. So, on the left-hand side, this power gain is mainly due to the construction of multiple knockoffs. And on the right-hand side, I think it is mainly due to the integration of multiple different types of tasks. Type of tasks. And as I mentioned before, because we are performing conditional inference, so the properties of the conditional inference are naturally inherited by knockoff screen. So the first property is that knockoff screen can help us to reduce the number of false rejections due to correlation and can prioritize the causal ones. They causal ones. So, again, like this is some simulation study. So, from the simulation, we see if we look at the signal, look at the window identified by knockoff screen, it is more likely to overlap with the causal ones, and also the distance to the causal ones is smaller. So, the second good property is that, again, because we're doing conditional inference, so knockoff screen can reduce the number of. Screen can reduce the number of false rejections due to shadow effect for rare variants. Again, this is a simulation study. So, in this simulation, we set the causal signals to be the common ones. So, all the rare variants, they are nouns. But if we apply the association testing, we can see some rejections of the rare variants simply due to the shadow effects. But if we're not But if for knockoff screen, like this number has been greatly reduced. So finally, like we apply this approach to some real data. The first case is the study for optimal disease. So we see like by using FDR as a more liberal measure of type 1 error, so we are able to claim more associations. More associations. And on the right-hand side, the low side in bold corresponds to some signals that have been verified before. So we see most of the rejections by knockoff screens, so they are reliable. So the second example is a treat related to lung function, which is a more polygenic treat. So we also like by running knockoff screen, we can have more rejections, more. More rejections, more discoveries. So, on the right-hand side, we can see most of the discoveries have been verified before. So, for some further extension of this framework, so first, as I mentioned before, so in terms of the choice of test statistic, it can be very flexible. So, we may consider some other form of the test statistic for them or can apply some very sophisticated. Apply some very sophisticated machine learning algorithm to derive a test statistic. And also, we hope to see whether we can make this make the generation becomes more scalable for later scale data set. So the last point is that here, like in terms of the construction, we apply a block diagonal approximation. So this approximation may lose, so we may lose some long-term. So, we may lose some long-term dependency. And also, we have not considered a non-linear dependency among the genetic variables. So, it can be further extensions of the generation. So, in the end, I would like to thank all the authors that are involved in this project. So, in particular, I would like to thank Sufai Huo and also thank Juliana. So, they have made a lot of effort and without their Of effort, and without their input, I won't be able to present here. So, that will be the end of my talk. Thank you.