Models don't do random walks, but actually make jumps. And so we'll see where this takes us. So this actually starts much earlier and it starts with a variation with a class of models that are quite rich and have very interesting behavior. And generally they're called A plus B models. So what is the idea of the A plus B models? The idea of the A plus B models. You have some graph, and for this talk, you can just think of Z D as the graph throughout. And you have particles on the graph. And the particles do random walks. And these models are interesting with nearest neighbor random walks. They are already quite complex. So there are two types of particles. We have type A and type B. And the two particles do random walks. And it doesn't have to be the same random walk. Maybe one of the types is faster. Same run block, maybe one of the types is faster than the other, so they have some rate at which they jump to the nearest neighbor, so DA for type A and D B for type B. Okay, yes, so uh yes, so uh the particles uh jumpers and uh the okay the fully Okay, it was just this, then you have some particles diffusing, and this is relatively straightforward to analyze. But there is an interaction between the two types. So, particles of each type do not interact with others of the same type, they can be at the same place, there's no exclusion, and nothing happens. However, if particles of the same type occupy the same vertex, so you have A and B in the same vertex, then the particle of type A converts to B. So A plus B becomes. B. So A plus B becomes twice B. So that's the of course, depending on where you start your particles and depending on the rates DA and DB, you can get several different types of behaviors that can happen. And then, okay, there's one more thing that in some of these models we do is that particles of type B might revert to type A also just randomly. So you don't need to, you don't need anything that you. Needs to, you don't need anything that has to do this on their own. And of course, if you have two particles of type B at the same vertex, if one of them reverts, then instantaneously you have A and B in the same vertex instantaneously it goes back to B. So they can only revert if they are alone. And there are many different initial configurations that people are interested in. You can start with a Poisson process of particles of each type and ask for so the whole thing is stationary. thing is stationary is translation invariant and you can ask what happens over time do most particles convert to type B or does type B disappear you can consider the case where we have a single particle of type B this is this is a model for an infection so if you have some individuals moving around one of them is infected with some disease that's the type B and when they meet they infect other people sometimes they get healthy and so then you have a CD And so then you have a single particle of type B, and maybe the infection makes someone a bit tired, so they don't move as fast. So you have a different rate for diffusion. Does meat mean be adjacent or they have more than one particle? At the same vertex. So you have more than one at one site. Yes, there is no extrusion. You can have as many particles as you want all at the same site. And the interaction is within a single site. And of course, you can consider the variations where if you Where if you can infect if you are nearby, and for many things, you expect a very similar behavior. I think there's a lot more papers about the interaction at the same site, but certainly many of the techniques with some extra difficulty can be pushed to these variations. Okay, and then so you have quite a lot of different possible models depending on these parameters and These parameters and the starting configurations. I should mention the two rates dA and dB. So, if the two rates are equal, then things become much nicer and easier because really you can think of each particle as having some trajectory that they are going to follow, and it doesn't depend on their type. And if they change type at some point, they just keep following the same trajectory with the same jumping types. But if the rates are different, if the A is not equal to dB, things become. So, if the A is not equal to DB, things become more complicated. And one of those grades can also be zero. So, it's also perfectly relevant to say what happens. So, if the particles of type A never move at all, that's perfectly fine. You have a particle of type B that moves around and activates. So, this will be a this will fall into a model of activated other mocks, which is the form model that we will get to. Get to. But uh yeah, so uh this has been studied by many people over the years. Uh I'm not going to attempt to uh do a complete list, but certainly uh some key works by Hester and Vladas, Leo Rola. So there are many things to study, whether the particles of type B survive at all, maybe they become extinct, if they do survive, how does the set of type B grow, is there some limit shape and what can you say about it? So And what can you say about it? So, okay, so many things to do about these general A B models, but in complete generality, they are quite difficult. Even in one dimension, things are not trivial here. But we're going to specialize to the frog model. And as I said, the frog model, you think of the two types. So, type A is a frog that is asleep. And naturally, initially, all frogs are asleep. So that they don't move. So that they don't move, they just sit in one place and nothing. And then the type B is the wake-up and active folks. And the folks of type B just jump around. So since the rate is, we only write one rate may as well let it be one. So folks of type B do a random walk, a discrete time, either discrete time or continuous time random walk. Choose whichever you like. You like, but in order to the model that the specific model the fog seven is finite lifespan. So each fog that becomes active, it jumps around for some finite amount of time. And again, there are variations. You can either make it a fixed amount of time or a fixed number of steps, or you can make it that it has some rate at which it stops. So it moves around for a while and then it cocks. And I should mention this is a joke. I should mention this is a joke, but for the non-native English speakers, since folk also means dies, it's not just the sound that the folk makes. It dies. Yeah, it cooks. So uh Okay, and again uh you can ask all of the same questions. Ask all of the same question. You can ask: Do the active forex persist or does the process terminate at some point? And if they do persist, if you do this A on Z D, is there some limit shape that you can describe for the space occupied by the active fogs? And how fast does it go? If you look at finite graphs, people have studied questions like how long does it take to, what is the cover time? So, how long does it take? Time, so how long does this take because before every vertex of the graph is visited by some frogs, so that all the frogs will walk in? You can ask about heating time, so how long does it take to activate a specific vertex? So there are many different questions and many different answers have been done over the years. And again, I'm not going to attempt a complete survey of everything that's been done, but I'll mention a few results so Few results. So, Alders, Mercado, and Kokov proved that there is a phase transition to this. Ah, so I should have maybe did not say explicitly, I think this was on the previous slide, that you start with the Poisson process of sleeping frogs and then usually a single active frog. So there is a parameter lambda for the intensity of the post-trum process of sleeping frogs. Sleeping fogs. At each point, you have a Poisson number, Coisson lambda sleeping fogs at time zero. So if lambda is large, there are lots of fogs everywhere, each fog will wake many others, and you can expect that the process will blow up, there will be many, many fogs and it will survive. If lambda is very small, then typically a fog will walk around a bit and not find any other fog and die. And if it does, then that fog is also unlikely to find others. Is also unlikely to find others, you get something subcritical. So the subcritical case is basically what I said. It's very easy. The supercritical requires a bit more work, but there is indeed a phase transition here on ZB. So if you have sufficiently large intensity of forks initially, then the active forks do survive, persist, at least some lambda c, and below it it dies. Yes, so there is monotonicity here. And yeah, and this process, there are many different ways to work with this process. It has all sorts of abelian properties. So you can consider, for example, that each fog is assigned its trajectory initially. And then you have when a fog is active, it doesn't have to start immediately. You can just think of it as active and only let it start working later. And the set of things that you reach will be the same. So That you reach will be the same. So there are many different things you can do here. If you are trying to study things like the limit shape, then of course you want to do things so that all frogs use the same time. Okay, so there are works from Hoffman, Johnson, and Jung analyzing the frog model trees. And the phase transition has been generalized to other kinds of Generalize to other graphs. So, Cosygina and Terrezerner have a zero-one law for transients, so it's almost surely in fairly general settings. So there is a limit shape on ZD for the set of active fogs. Herman, a few years ago, analyzed the cover time for finite trees if you have a deregular tree of some depth, how long it's taken for everything. Some depth how long it's taken before everything is reached. And there's a question for a general finite graph: do all vertices become active or not? And so there are thresholds for that in terms of lambda. So, okay, so there's many other papers that are not listed here, but I said we're going to start focusing down on the work that we are doing here, which is on lambding folks. So we are going to On numbing folks, so we are going to from here on the graph is always going to be either Z D or a D dimensional torus. So if you want a finite graph, you take a torus or infinite graph, you have all of Z D. And we have some jump rate. So the frogs and then there's a jump kernel. So the frogs jump at rate 1, but when they jump, they jump passively to a faraway vertex. So there's a heavy tail polynomial decay. So we assume that the frog jumps from x to y with probability q. Jumps from x to y with probability q of xy, which only depends on the difference between x and y. And we assume that this decays polynomially, like some power of the distance. So the distance to minus s. And this is up to constants bounded above and below. S needs to be larger than b, otherwise this is an observant. So s is just a parameter and Yes, and S sometimes people write S as D plus alpha. So, if in some of the slides you see an alpha, that's just the difference between S's. And the theorem that I'll tell you about is about how long it takes to activate farmways entices, how long it takes to cover the toggles. How long does it take to cover the toggles? So, if you define this AT of X as the activation time of X, so again, so the starting of the equation, we have a single active fork at zero and Poisson lambda process elsewhere. And well, we need the lifetime to be large enough. If the lifetime is too small, then the process is subcritical. So we need the lifetime to be large enough, and there's always a positive probability that the And there's always a positive probability that the process dies out, so we need to condition on survival. Then the activation time of the vertex x, it turns out, I don't know if this was your first guess, but it turns out it's a power of log x, and it's log x to the delta plus little of 1, where delta is this number, it's a log base 2 of 2d over s inverse 1 over s. Intersects one over. So it's an explicit number. This is if s is sufficiently small. So small s means you have a heavy tail for the jumps. If s is large, then you get a linear distance. So if s is larger than d, then even though you have fairly large jumps, you generally evolve d path. Any questions about any of that? Yes. What happens if L is infinity, if they don't die? Is it the same result? Yes. Yes, I think this doesn't change the exponent. The lower order terms might change, but the the linear I think would become sublinear. Again, with x to the one plus little of one. But we haven't done the analysis for this. Yes, so there's a second theorem also on the topus. On the torus, you can ask about the cover time, how long it takes before every vertex is activated. And again, if you have a torus of side length L, then you get something very similar. So you get load. Very similar, so you get log L to the delta, same delta as before. So it's the same as the activation time for the typical vertex in the topics, up to lower order terms. Okay, so I guess before I start changing, there was a complaint yesterday that MathTalks don't have enough. That masdogs don't have enough change of costumes, so fix that. So, this exponent delta with this log. With this log base two of two d over s inverse. Some of you would have seen it before if you've been following the long-range calculation literature. I'm not sure how many of you have seen this exponent before. Okay, fewer than I thought. So uh so what is long age percolation? Uh this is another model with a with a longer delastium's history in percolation uh theory. In percolation theory. So suppose you take either Zd or the torus, and you could consider a percolation model on it. So you have edges open and closed, but it's not just the nearest neighbor edges. Any two vertices have some probability of being connected by an edge. So the nearest neighbor edges may or may not be there, but an edge of distance X is open with probability. of distance x is open with probability q of x which decays like some power of of the distance. Same power as before, same shape, same form as before. And okay, and again there is a long list of papers and many people have worked on this. This was studied by works of Newman and Schulman and Eisenman and Newman in the 80s. And they proved that when you change the parameters, the connection processes The parameters, the connection probabilities, there is a phase transition. So, if the connection probabilities are small enough, then even though you have very long edges, you'll have only finite clusters, whereas if you make sufficient connections, then you start getting infinite clusters. And there were many other works analyzing this phase transition. And when you come to understanding the metric structure, so the metric structure of percolation clusters in general. Clusters in general is quite a difficult problem that, in most settings, is still open. But there are some results. So, this was studied in the long-term speculation. Benjamini and Berger started the study of this for one-dimensional models. They gave some bounds on the distances, not very char, but there was a long list of improvements since then, many works of Marik Biscuit and more recently, Biscuit Berlin, and Wisconsin. Recently, Wiscopen Lin and Wiscopen Riegel from a couple of years ago. And eventually, the result is that if you look at the supercritical long-range percolation model and you look at what is the distance between two vertices x and y, then in probability it's of the order of a log of the distance to the power delta. And yeah, so you'll notice there's no little of one here. You'll notice there's no little of one here. So, for this model, they've been able to get a more accurate statement. But you get this polylog distance. And maybe I'll just on the board say in a few words to give you an intuition of why you might get this kind of log behavior. So, suppose you Behavior. So suppose you have some vertex X and the vertex Y far away, and you want to find the shortest path between them. Well, okay, so finding the shortest path might be difficult, but if you want to find some short path, then you will say, well, let's look for some vertex near x, let's call it u, and some vertex v near y, so that you have a single jump from x to y, from u to v. So you have a single edge that will cover much. That will cover much of the distance. And depending on the probability of edges and how far u goes from x and how far from y, at some point you're going to start finding such edges. And it turns out that the distance that you need here is going to be some power of the distance between x and y. So the distance between x and u and the distance y v will be much smaller than the distance edge. So you find one edge that is going to take you. So you find one edge that is going to take you most of the way, so from almost from edge to almost to Y. So this takes you from Vancouver Airport to Calgary Airport. But you still need to get from home to Vancouver Airport and you still need to get from Calgary Airport to BS. So then you break down a bit easier and you find a smaller edge that takes you most of the way here from nearly X to nearly U and another edge from here. So this is Here, so this is maybe a taxi, and this is maybe a shuttle. And you keep going, so you recover. So, each time you break the problem into two smaller problems, you have something that solves most of the way, but then you have two smaller problems. So, of course, the number of problems grows exponentially, as we have in ZigMath, but they become smaller and smaller, which is not that frequently math, I guess. But yeah, eventually we. But yeah, eventually you get to a nearest neighbor's and you find the connection. So eventually you just walk outside and so on. So then you ask each time you look at a scale like this, the distances that you care about decay by some power of the previous distance. This power depends on the dimension and on s. And on the other hand, the number of things you need to do at Tscal doubles. You need to do at each scale, it doubles each time. So you have one thing that goes exponentially, the other thing decays exponentially, and if you combine the two things, you get so this log of x minus y is the number of scales that you need. So the number of scales you need is log log. So if you have some scale, and each time you go to a power of the previous scale, then after log-log, many iterations, it becomes order one. But the number of things you need. But the number of uh things you need is exponential in the number of scales. So you get e to the constant log log, so so it's the power of log. So this is a very crude picture and these papers make this much more precise. So this is a few words about distances in long-range percolation. Any questions before we get back to the frog model? So, okay, so this is where things start to get a bit more complicated. So, if you feel lost, then now it would be a good time to just dive into your email. So this is eventually it's a multi-scale argument and so this is talk about multi-scale arguments, the sort of thing that generally don't really go well in talks, especially on Fridays. I'll give you some of the some of the ingredients of the proof, but Some of the ingredients of the move, but the details are quite thorny. But okay, so how do we proceed? So we want to try to activate some vertex and in order to get started, we are going to talk about smaller boxes and consider what happens in the small box. So there's going to be some scales, there are going to be boxes of many scales. Scales. There are going to be boxes of many scales. The smallest scale is going to be some parameter R, which will depend on everything, on the jump kernel, and on lambda and on the dimension R we are going to assume R is large enough. And we take a box of side length R and we are going to study the frog model where we have frogs only inside. So we have the same correspondence process of frogs inside the box, but there's nothing outside. So the frogs can still go outside. So, the folks can still go outside and come back, but they are not going to wake any other folks outside. And for such a box, let's say that the vertex is good if you start with an active rogue at that vertex, then it's going to cover at least a quarter of the box. And of course, this doesn't have to be the vertex itself that can. Be the vertex itself that covers the frog itself. That's very unlikely, it only has a finite lifespan. But the frog that starts from V is going to go around and maybe activate a few more frogs inside the box, and those will cover some more. And we want this process, this whole process, we want it to cover at least a quarter of the box B. And we are also, we said there's the lifetime L, which is the amount of time. Lifetime L, which is the amount of time that each frog lives. So L is going to be related to the parameter R, in that L, you should think about it as the order of magnitude of how long it takes our frogs to exit the box. So this depends on the parameters. If you do things, if alpha is less than 2, then the random walks are dominated by the very large jumps. And the time to exit the box of size r is roughly r to the alpha. It's basically how long it takes before you make a single jump. Long it takes before you make a single jump that takes you macroscopically outside the box. Okay, if alpha is larger than two, then this is roughly r squared. Then you are essentially a Rean motion microscopically. If alpha is larger than two, so so you have finite second moment for the jumps and the time to exit the box of size R is on the order of R squared, then in the critical case you get some logs. In the critical case, you get some logs. So, okay, so we have several parameters, and maybe I'll just put them also here. So, L is the lifetime, and R is the box. And we assume that this is the same as the exit time from a box of size R. Size up. And you might complain at this point: well, if the flow model is given to you, L is fixed, so how can you choose R to be larger? And we are doing this for R for a lifetime that is large enough. So we need the lifetime to be large enough so that R will be large enough. So we are still working on this and we hope also to push the results all the way to the critical lambda, but at the moment we assume, at the moment, this is probably. We assume at the moment this is for lambda large enough. So, if so, by sorry, yeah, so yeah, so we need so we need the lifetime to be large enough at the moment. So, yeah, lambda we don't need to be large, but of course L needs to be also large enough. How large L needs to be also will depend on lambda. Um right, so we have this lemma here, it's the same lemma here. So uh if you take the box and you look at uh some subset S of the box, we claim that it's very likely to contain good vertices. So I'm not only saying that the box will have good vertices, but that there will be good vertices are everywhere. So every subset S of the box is exponentially unlikely to have no good vertices. And in particular, it's exponentially the probability there is no good vertices in the box at all. If you take S to be the whole box, then it's E to minus the volume of the box, E to minus R to the D. But we need this for any for more general sense. And as I said, we assume that R is large enough, so the whole thing is super critical, in particular. Critical. But it's not even close to the critical. And what's the idea of the proof of this? Well, the idea is very straightforward. You start with the vertex in the settings and you check is it good or not. And as we said, the process is super critical. So the frog will typically activate more than one other frog. It will activate several frogs. And each frog moves around. So it reaches So it reaches so when um so if you uh so if you do a random walk inside the box uh you have L steps and uh a good fraction of them will be new vertices because these walks are transient so the number of vertices you cover is some constant times L with high probability and so typically the number of frogs that you activate is C times L times lambda at least and you can make this large enough you can make this at least two for example so each frog At least two, for example. So each frog on average activates two other frogs, and those on average activate two other frogs. You get a supercritical binome mega altruism tree. This is supercritical. There's a good chance that it survives and you get exponential growth and you will activate quite a lot. And if that happens, you reach most of the blocks. But if the vertex doesn't happen, so suppose the first fork dies without hitting any other fog. Dies without hitting any other fog, or maybe it's one and that one may also dies. Okay, then just take the next vertex in S and try again. So each time you take a vertex in S there is some positive probability that it will activate, it will get a surviving process and you reach lots of vertices of the box. And there's S vertices in S, so you get this exponential. Sorry. Yeah, you get this exponential in the Yeah, you get this exponential in the size of S. The difficulty here is that there is negative information. The reason this is not, you can quite take this and expand it into a move, is that there's negative information. If I tell you that the first vertex from S did not actually activate a large part of the box, then it tells you that maybe there are. Then it tells you that maybe there are fewer other frogs in other places, or that those frogs somehow have trajectories that happen not to hit frogs. So you get negative information, and the more you try and fail, the harder it becomes to still cover a large part of the box. So you do need to somehow keep track of this negative information. And so we have a sub-lema for this. A sub-lema for this, and this is maybe, and again, these are lemmas about random walks in boxes. But the lemma is that if you have the box, so if you have this box here, you have some set D here, and the set D we assume is not too small. So D is at least epsilon the volume, and you do a random work, you take any value. And you do a random walk, you take any vertex inside the box, maybe this one up here, and you run a random walk for L steps. And this is this Heavytail random walk that we are using. So you run a random walk for L steps. Then the claim is that there is at least some constant to all of these constants depend on all of the parameters here, but just numbers. So there is some constant probability that the random walk hits at least some fraction of that, at least some fraction of the fraction of the fraction of Of that, at least some fraction of the random walk is inside D. So you get epsilon is the fraction of the box that is in D, and the random walk X has roughly C times L delta s, so we want C times L times epsilon of them to be inside D. So the random walk, yeah, so we have this random walk from X, and we want it to visit many vertices in D, and there's a good chance that this happens. So it's not a not necessarily So, it's not necessarily high probability. Maybe these, the right half of the box and the random box completely avoids the right half of the box. So, this the probability of this does not vanish. But there's at least some constant probability of this. So, this is if alpha is less than 2 and this is larger than 2. And I'm going to just focus on this case. But if you get some critical dimension or critical, you get a log, and if you also have the critical alpha, you get another. Also, have the critical alpha, you get another log, so you have a low block there, but but uh let's not worry about those uh things. But uh essentially you get that's the one you should think of. And the proof of this is relatively uh straightforward, it's just first and second moment, you calculate so you want to calculate uh how many on average do you hit, you can calculate second moments, or estimate them. Calculate second moments or estimate them. There is a key observation that if you ask how many vertices of the random walker inside D, and suppose M is the median of this, or actually it's the ma, you look at the median depending on the starting point and you maximize this overall starting point. So if you look at the median, then the probability that you have a large multiple of this median. A large multiple of this median declays exponential. Because you can just run the random walk until it hits M vertices inside D, and then just ask what happens after this point. So now you have less than L remaining steps, and again you're asking, you have probability half that you reach another M before you die. So this means that this rand this this random variable of the size of intersection of X D has a nice enough tail that if you can control the moments you can If you can control the moments, you can also show that there's a good probability that it's not the same. So the expectation doesn't all come from the heavy tank. So this is really getting into some specific, some very direct computations we find on oxygen and the heating care of them. So that's the sub-namer. So, how do we use it? So, going back to the previous idea, so we start with So we start with some vertex in S and we start running the exploration process and this is a supercritical process. So it might survive, in which case we are going to cover quite a lot of the pores. But if it doesn't survive, if a supercritical process doesn't survive, it actually dies very quickly. So as long as the process hasn't covered, as long as this exploration is starting at different births to face As long as we haven't covered a decent fraction of the box, as long as we have at least an epsilon fraction of the box that we still haven't touched yet, then each new time that we try, we are going to find new flocks to activate inside this epsilon volume, and we can still be super critical and have a good chance to cover a large part of the box from that. So, as long as we cover at most some epsilon function of the box, this is likely. This is likely, and okay, and then this gives you enough attempts to find the superheat, to find the vertex, that you are extremely likely to find this good vertex somewhere inside the settings. So these are the main ideas of finding good vertices inside the box. So at the moment, I just I just say to a pizza, so and I'll just also remind you, so a good vertex activate much of the box that it's in. And the next step we want to uh bootstrap that and uh Bootstrap that and go from this to activate it even more. And if you boot vertex, so we said it activates at least a quarter of the box that it's in. But I'm saying that in fact it's likely to activate all of the box and also all of the adjacent boxes. So you have a box, you have other boxes nearby of the same scale, and this is extremely likely. And the proof of this, we go back to Roman's talk. This is basically using sprinkling. So what's the idea? If you are not familiar with the sprinkling, I'll just remind that the idea is we take all of the frogs, we actually split them into two parts. So we use some fraction of the progs we use for this first model. We define good vertices only. We define good vertices only using this fraction of the forks, forgetting that as if the other forks don't exist. And now we throw in this other forks. We say, ah, we actually have another lambda over 10 additional forks on our regid vertex. And now we have this box. We activated the quarter of the size of the box. So there's going to be quite a lot of new forks that we have not yet used inside here. And those forks are going to go around. And those forks, there's going to be a lot of them, and there's going. To be a lot of them, and there's going to be so many of them that they are just going to swamp everything. That was another joke, I guess. So they just cover everything inside the box, and they'll also cover all the vertices in the adjacent boxes. So, at this point, what we said is that if you take a box, then it's very likely to have some good vertices and active many good vertices. And active vertex, many good vertices, in fact, an active vertex will activate. If you activate that vertex, a forget that vertex, then it will end up activating all the vertices in the box and the adjacent boxes. And that was the base scale. As I said, this is a multi-scale argument. And so we're going to partition the space into scales. So the first scale, we have boxes of size R. The first scale we have boxes of size R, which is this parameter that we chose based on a lot of things. And then, so this is this L0 is R. And then each Li plus 1 is a power of the previous one. So it's a power Li to the 1 over Zeta. The reason we use 1 over Zeta is that we are using notations from the long-range percolation literature. So the letters that we use there. So each time you have li to the Each time you have li to the, each one is a power of the previous one, so the boxes grow very quickly until at some point you get to the so if you get to the scale that you care about. You have some large scale L that you care about either because you care about activation of a vertex at this, so you're looking at the toes. So, yeah, so zeta is a parameter that needs to be chosen with some care, satisfying a bunch of inequalities, but Inequalities, but it's doable. And in fact, it's even a bit more complicated than that. So you need zeta to slightly change. You use some value zeta for a while and then you use a slightly different exponent. But I said it's Friday, so right. So if you have, so you make these boxes, yeah, so you have this. If you are looking at the torus, then you might say, Looking at the togus, then you might say that it's not exactly divisible, and you might need to make the boxes almost the same size, but not exactly. That's not an issue. So you get to this scale L after K2 steps and there are two intermediates. Yeah, there are two intermediate uh steps, scales uh K zero and K one that come up in the analysis, but I'm not going to go into too many details, so it doesn't really matter what they are. And then we're going to look at the boxes. So we said we had some good vertices, so now we're going to have nice boxes. So what is a nice box? So at the smallest scale, boxes are automatically going to be nice. That's just the definition. That's just the definition. But the idea of a nice box, so if you have a so you have a box on some scale Li plus one, which is made up of many boxes of size Li. And what do we want? First of all, we want most of these boxes of the previous scale to be themselves nice and up to some small fractions. Up to some small fraction. There are also some parameters here to play with. And we want that if you have two nice sub-boxes, maybe this one and this one, we want that there is some frog from here that jumps here before dying. And also in the other way. Some other frog. So maybe we have some boxes here that are not nice. Not nice, a small fraction of boxes here. We don't worry about them, but most of these boxes are nice. So, between almost any box here and any other box, we have some form that is jump. And above a certain scale, we don't want most of this, but above a certain scale, we actually require that all of the boxes are going to be nice. And the lemma is that boxes are. The lemma is that boxes are very likely to be nice. So you can calculate the probability. So at the smallest scale, boxes are automatically nice. And then each time you go up to a scale, and you need to find, so you have some estimate of the probability that each of these boxes is nice. These are independent. So the probability that you have many not nice boxes is easy to control. And you know that the frogs, you know the jump rates of the frogs, so it's easy to estimate the probability that you have a frog jumping from here to here. So you can show that. Jumping from here to here, so you can show that. So, by induction, you get that the probability that the next level box is nice is very high. And each time you keep going, so you might fail because there are too many not nice boxes, this is very unlikely. Later on, even a single bed box will be a problem, but this will not exist anymore. And these frauds are going, you're going to find such jumps everywhere. Jumps everywhere. So, this is just a careful induction of the probabilities. So, boxes on all scales are going to be very likely to be nice. And once you've done this, so that's the heart of the multi-scale argument. After you choose all these parameters carefully and you get that the boxes are nice, and now you actually uh get the activation, and this is the picture that uh that I erased here before. That I raised here before. But okay, but it's not just the long edge percolation, so a lot of this is similar to things that have been done with long edge percolation. But for the frog model, things are different because if a frog jumps into another box, it's not enough to connect, it needs to actually reach the next place where you go to the next stage. And this is where the good vertices come in. So if you have a right, so the next ingredient, and I think I'm going to just given the time, I'll just very briefly discuss this. The claim is that we are going to find in a large box, we can find a fairly large set. So the set A will cover most of the box, not necessarily all. The box, not necessarily all, and it's going to be spatially homogeneous, so it's going to be reasonably dense, so it's not going to have any large holes. So any ball will include some, and will have some fraction of the ball inside the set A. And for this set A, we're going to have this large set so that activation time for any vertexes x and y in the set, activation time from x to y is 2 to the i at most. To the i at most up to constants. So, and the idea is this picture that we had before. So, if you have some vertex x in the set, so you want to take the box of the previous scale around x, the box of the previous scale around y, these are going to be nice boxes. That's part of the condition for being in the set A. And then inside X, you can find, you can go to U into the I minus 1 steps from 1. minus one steps, from v to y you can go into the i minus one steps. So it's not, so these steps means that it involves that many frogs. So if you convert this to time, each frog has a certain lifetime, so you get some constant of L in there. And then you keep iterating down. So you get so that's so this is where we iterate. So, this is where we iterate the fog. And the key thing is: so, we said we have this fog from this box that hits, that jumps somewhere into this box. We don't know where it hits. It jumps into some vertex, and we don't know anything about this vertex, but it hits this, and in fact, it probably hits this, and it still has some time left to live. So, it's going to walk around a bit. So, it's going to cover quite a lot of vertices inside this box of order, maybe some constant. Order maybe some constant times L, and one of those vertices is very likely to be good. We go back to the earlier lemmas. So once the frog jumps here, it's very likely to actually start a supercritical thing here that will activate quite a lot of things and go up and eventually you'll be able to reach to reach once you go from to this V, you'll be able to eventually reach Y because you need two vertices. So that's how things are being put together at this point. And this is where I'll start. Questions for all of us?