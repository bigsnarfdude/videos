First, I just want to thank the organizers. I can't think of the, and all the participants, beyond it just being so pleasant to be back in a workshop of this kind, I can't think of the last time I went to a conference where I learned something new from so many different people. I just have had a really diverse and fascinating set of conversations with people here. So I'm really grateful for the opportunity to be here. For the opportunity to be here, and of course, for the opportunity to speak. I'll just say it again. I ask Sri, if I'm writing too small for the Zoom, please jump in. And I'll encourage questions, any and all questions. I've tried to keep things elementary at moments, but of course, sometimes that can be overly vague. And I'm always happy to give more confidence. So, for the algebraists or the topologists, jump in with questions at any time. Let me start by talking about what a matrix factorization is. I know many of you have seen this, but maybe not everyone. So, matrix factorization of a ring element is going to be a pair of pairs. To be a pair of matrices AB, where A times B is F times the identity matrix. That's also B times A. So they should be square matrices. You multiply them and you get F along the diagonals. F along the diagonals, and that's it. I mean, this is one of these definitions that's only understandable, I think, through an example. So let's do one right away. I set this to be F. Then I can choose A to B. Let's see if I can use this one in my notes. Let's see, I go X zero, Y zero. Y zero, get zero. And I think this is it. Instead of checking my notes, let's check with our fingers, right? We'll do it. So, you know, if I multiply these, do I get, on the diagonal entries, I get F and zero, zero, and then on the other diagonal, I get F. Okay, good. So that's a matrix factor is a. So that's a matrix factorization of F. If you haven't seen these before, it's surely not obvious why they should exist in general, where they would come from, things like this. But at least just from the definition, we can see that it's somehow correlated to the complexity of a polynomial. So let me do a second, somewhat trivial example. Example which motivates the name, which is well a one-by-one matrix factorization is what? Well, I want a pair of one-by-one matrices. There's only one inter we call it, G and H. And when I multiply them, I get F times the one-by-one identity matrix. Matrix. So in other words, a one-by-one matrix factorization is a factorization of f. So it's a notion which generalizes the idea of, so you have a one-by-one, a non-trivial one-by-one matrix factorization if and only if you're reducible. And having a two-by-two non-trivial matrix factorization is like some general. Matrix factorization is like some generalization of that, and 3 by 3, 4 by 4, and so on. So it's a way to kind of measure the degree to which you fail to be reducible, or some sort of higher measure of irreducibility. The definition arose in work of Eisenbud from 1980 on maximal Cohen-Macaulay modules over hypersurface rings. I'll say more about that in a moment. But they show up all over the literature. I mean, I know of significant mathematical connections to topics like free resolutions, moduli of curves, string theory and mirror symmetry, K-theory, I mean just many more. I actually was counting before this talk, and I think the majority of the people in the room right now have a paper where they mention matrix factorization. So I think I counted right. Counted, right? So they show up all over. So many of you are aware of this, and they show up all over. Oh, fun fact. This Eisenbund, the paper he introduced this definition, is his most cited paper. It's been cited, according to Google Scholar, by almost 800 times, but the bulk of those are from physics, not mathematics. So, a lot of citations. Okay. All right. The connection with maximal Cohen-Macaulay modules is. Maximal Cohen-Macaulay modules is going to be especially relevant. So, let me just say a few words about it. So, if R is regular local, or the case I'll be interested in, a polynomial ring. Then it's a good question. You're following up already. It's a good question. Your polynomial rings are over fields. Yes, yeah, yeah. I'll make that notation more precise in one second, but yes, they will all be overfields. Then you can extract a module by taking the co-kernel of A, or the co-kernel of B, but the co-kernel of A, and this is a maximal Cohen-Macaulay module. Oops, that was a good plan. Oops, that was a good planning. On the hypersurface ring R mod F. And in fact, every Maximal Cohen-Macaulay module arises in this way. So I won't say too much about the properties of Maximal Cohen-Macaulay modules, but they've shown up in many they've shown up in several of the talks this week. They're kind of a fundamental thing of study for us algebraists. Study for us algebraists. And so matrix factorizations arise in that context. All right, let me focus a little, let me get a little more notation to get this more close up. So I'm going to throughout, I'm going to work over an algebraically closed field. That's a bit of a red herring for some of the results, but it doesn't. It's not going to have a major effect. So it makes things cleaner. So it makes things cleaner. I'm going to be focusing on the situation where I'm looking at the polynomial ring. And I'm going to, so that's going to be my ring, and I'm going to take an element f in there, which is homogeneous. So all the variables. So, all the variables will have degree 1. I'll just take the standard grading. And I'm not really interested in one-by-one matrix factorizations, so I'm going to assume that f is also irreducible. That just gets rid of some trivialities. And so I'll note this gives me a hypersurface as well, V of F inside the projective space. Reactive space. So that's the first bit of notation. Always over an algebraically closed field and looking at homogeneous polynomials. The second thing I want to do is define the invariants that are going to show up in the main rank conjecture for this talk. And so the main ones that they'll call MCM of F. So the main ones that I'll call MCM of F will stand for the minimal rank of a MCM module, the hypersurface ring. And I left a little gap here because the free module is MCM. MCM, it's colin-Macaulay, and we want to avoid that. So I want to choose a non-free maximal Cohen-Macaulay module. So that's what MCF will stand for. There's another invariant I'll need. This one's a little uglier to define, but short up in a Reynazov. E of F will stand for the following. I take half the codimension of the singular logo. Dimension of the singular locus. And just to be clear, I'm measuring the codimension within the hypersurface. So not within the, it's not the codimension of that locus in projector cells. It's the code itself. It's the codimension. It would only change it by one, right? Okay. Then I subtract two, get my parentheses right, and I take the round one. So it's roughly. So it's roughly half the codimension of the singular locus. That's the way to think of it. Alright, so those are the invariants, and now I can state what I'll call the BGS conjecture. Calling it that because the conjecture was made by Buchweitz Royal. Royal and Schreier. This is from 1987. And it says that MCM of F, the minimal rank of a non-free maximal colour-Macaulay module, is at least 2 to the E of F. So that's the conjecture, and it has a 2 to the n quality to it. Just very quickly, let me just do the example that highlights the 2 to the n quality, or maybe the 2 to the n over 2 quality, is if v of f is smooth, then e of f is E of f is, I think I got this right, the round down of n over 2 minus 1. And so the BDS proposes, in this case, that the lower bound is roughly two to the n over two. So it's it's similar in the speed of it. Similar in speaker. Make some remarks on the conjecture and how it fits into the other conjectures we've heard about, but maybe now's a moment also to see if there's a question. So let me say later I'll give more precise, you know, I want to rewrite this. Later I'll give more precise versions, but the theorem I'm going to talk about is that the conjecture is true for generic polynomials, but I'm not going to state it precisely yet, because I want to wait to be able to say more about what I mean by generic polynomials. What I mean by generic polynomials, but that's something. What's that? Do you connect it to a lower bound? You said equality. Do you? Absolutely. Yeah, yeah, sorry. Yes. Like that. Did I get the inequality in the right direction? Yes. Thank you, Sri. All right, so let me give some remarks. So, first of all, There hadn't been much work done on this. So the conjecture was from 1987. And it was really only known in two cases. So one is the degree two case, quadrix. This was Norr from 1987. Actually, in a companion paper to the paper where this conjecture To the paper where this conjecture appeared. So it's kind of, that was part of the motivation. I'll actually, we'll talk more about this case in a minute. And then the second case that's known is if E of F is small, the most recent result in this vein is with E of F is almost two, and that's due to Ravindra and Tropathi from 2019. So there were very few cases known. The second comment is a very subjective comment, but I think it's a beautiful conjecture. I mean, this is fairly obviously what I think. But because this MC. This MCM definition, this invariant, I mean, it's about, it's very algebraic. It's about either these maximal Cohen-Mercalli modules, or even more concretely, it's about breaking down a polynomial as a product of these matrices in this funny way. It's just a very algebraic idea. And the singular locus, at least to me, is more geometric in spirit. And the idea that somehow the And the idea that somehow the polynomials which have nice geometry, which determine smooth hypersurfaces, have this complicated matrix factorization expression, it seems to me is surprising. I think it's a lovely or a compelling conjecture. I mean, it's not evidence that it's true, but there's reason to doubt. But I think that that's for me what makes it interesting, is the way it mixes. It's not obvious. It's a non-obvious conjecture. It's a non-obvious convector. But Daniel, just going back to the equality versus inequality, so remind me what BGS did. They showed equality for certain kinds of equality. Yeah, that's actually literally the next comment I'm going to say. So what BGS showed was they showed that for, so I'll say BGS constructed examples Examples where MCF is at most S. And now it's not for every F, right? So for some specific F, they did it, for instance, I think the simplest ones they did it for are like the Fermat polynomials. You take the sum of xi to the d for various d's. Because it must be equal, though, if they conjectured there was a lower value. They conjectured that it was a lower value. Well, they didn't prove it was a lower value. I mean, they. Oh, sorry, sorry, you're absolutely right. Yeah, yeah, sorry. Sorry. Although, I agree with what you had first. They constructed specific modules. Oh, yeah, that's right, but we don't know that it's actually... That's right. Because this is the minimum. They constructed modules that they constructed. Screw me up, Greg. Yes, so they did this. So their paper did that. And in some sense, these came from Kazool. And in some sense, these came from Kazool complexes. And so in some sense, I'll say the conjecture is roughly them saying, and I was going to say I don't want to put words in their mouth, but I think I'm literally about to put words into their mouth, that the matrix factorizations that come from Kazul complexes. Or the smallest possible So it's I'm saying it that way in that loose terms because I want to emphasize the way in which at least I see it as parallel, the conjecture of May, you know, as parallel to the books composing their construction, obviously, the above depends on the bazooka. That's right. Depends on the Kazool complex. That's right. What they did is they built certain Kazul complexes and used those to construct their matrix factorizations. And then they conjectured the smallest possible matrix factorizations are the ones that come from Kazoo complexes. And so at least thematically, there's a connection between this conjecture and like the Buxbaumizing La Horace conjecture. That makes sense? Does that make sense? I'll note here, I'm aligning there's another rank conjecture that would maybe be a little more natural for our context. You could also ask about, instead of bounding the rank of the Cohen-Macaulay module, you could ask about bounding the size of the matrices. And the conjecture implies that the matrices would be at least as big as this with a plus one. Big as this with a plus one, right? That the conjecture would be. Because there's a way to relate the rank of the MCN module to the size of the matrices. So they didn't make any conjecture in that vein, but that's a little closer to the things here. All right, and I want to make one other connection, which is to get us even closer to things we saw yesterday. So here, I'll call it the danf root two. Root 2 rank conjecture. I named it this because Mark Walker maybe kind of conjectured this yesterday. I don't know. But over coffee, right? Or did his talk slash over coffee? We were talking about this. I'll phrase it as a conjecture, just for a problem, because it's probably more of a question. But if we let F be a comp, a free Be a comp a free complex of SN modules. And we want it to have finite length homology and B, I've learned from the other speakers to be sure to include this, it's not trivial. Trivial. That. Then the conjecture is that the rank of f, well, we know it's not at least 2 to the n, but maybe it's at least root 2 to the n, which is, you know, 2 to the n over 2, whichever you prefer. So that was a conjecture/slash question that came up yesterday. Actually, I think it's a really interesting question. Can you get any sort of explanation? Interesting question. Can you get any sort of exponential lower bound or middle? And this one is reasonable for several reasons, but maybe one reason it's compelling is that the VGS conjecture would imply it. So this arrow was kind of also worked out over coffee, the implication over the coffee grades, that if you could prove the BGS conjecture in its full form, you would get something like this, as a lower value. So this is maybe a very simple question, but I guess I'm sort of curious about that minus two. And I realize you pointed out there's those examples over there, but still... Is there a good reason why that's the right number? I think it's, if you think about the bound on how big the matrices should be, you would drop the minus two. So the matrices are basically two to the n, but if you have Basically, 2 to the n. But if you have a matrix factorization of size 2 to the n, then one of the two Cohen-Macaulay modules has to have rank small. I guess one has to be a... It has to do with the discrepancy. I'm going to screw it up. It kind of shows up there. So if you think of the actual conjecture as being about the size of the matrix factorizations, it would go away. This is a stronger conjecture, though, because the rank of the Cole-Macaulay module. That the rank of the colon-Macaulay modules is harder than the rank of the size of the matrices. Does that answer it, Elizabeth? All right, let's do some examples. Really the only examples we're able to do. So I'm going to do the next clock. Okay, so I'm going to take F is this guy. And I want to show you what happens because I think it's interesting. So I'll give you the name now. All right, so let's see. So How do we do this? We take so I'm writing this in a particular pattern, because this is how I remember it. And maybe if you were taking notes, I had to erase it. Notes, I had to erase it. Recognize these blocked matrices. So these were the matrix factorization I started with kind of wedged together. You take the new terms here and get the signs right. And let's see, I think we have minus signs here. Maybe minus signs here. All right, let's check it. Okay, at least the first entry's right. I'm not going to check it. All right, so this is a matrix factorization of F. Check it, it's size 4 by 4. And you can kind of see this was the matrix factorization of the smaller class. The smaller clock. So you can kind of see where the exponential thing is happening. I'm getting a new matrix factorization from the old one via this block structure. And in fact, this works more in general. So if I wanted to say what A3 is, so if I wanted to get a matrix factorization, if I moved on, said I wanted to get one for the next guy. What I could do is take a2 x3 times the identity, negative y3 times the identity, and b2, and do basically the same thing over here. And if you check again, you'll see that you get this because A2B2 is a matrix factorization of the first part, this kind of works out. So if you go in this way, you'll see this doubling effect. By Knoor, that's known to be optimal. Yes, exactly. That's exactly. This is, in fact, this is Knoor's, I'm kind of hinting at part. I'm kind of hinting at part of Kenora's theorem. So let's just do it. Not only known to be optimal, it's known to be basically all there is. So Kenora's theorem says quadrics over algebraically closed field. Algebraically closed field. And here I need characteristic not equal to 2, though there's another theorem. I forgot all the authors involved to do it in the characteristic 2 case. They have essentially a unique matrix factorization. So I haven't told you the equivalence relation on matrix factorizations, and it's not. And it's not so important. But there's essentially only one matrix factorization. It's the one you see, right? The one I've kind of hinted at and was built up in this way. And so if F, if you write F, your quadric of even rank, so in the even rank case, if F is this, then then the invariants you get are E of F is S minus 1, the number of terms that appear here minus 1, and I mean the number of terms you started counting at zeros, S plus 1. No, I have this right. Sorry, what I said was wrong. It's S minus 1. The rank of the quadric needs to be even for me to every quadric over an algebraically closed field of characteristic not 2 of even rank can be written in this form, right? And if it's odd rank, you have to make one of the terms like a perfect square or something, right? But there's kind of a standard form for quadrics in characteristic count with two. So it's enough to do this case. And there's a similar theorem in the odd. There's a similar theorem in the odd rank case, but it's not especially important. Alright, but the point is, you've totally classified what happens for quadrants. It's the one case we understand. We understand it perfectly well, you know, everything that happens. You've motivated the conjecture. But that's yeah. As I erase, let me go on a tangent, which is, I mean, the theorem, actually, the theorem. I mean, the theorem, actually, the theorem as I stated it is only part of what appears there. There's another, the method of proof really involves studying how a matrix factorization changes when you go from something like this to adding on just a term, like a UV like this. And there's a sort of periodic phenomenon that arises. And that's really the main result called NOR periodicity. Periodicity. And in the theme of this conference, there's a sort of algebraic periodicity that he uses to prove it. And it turns out, and this is Michael Brown's thesis, I think, is shows that the algebraic facts about matrix factorization that Kenora used to prove this theorem are essentially an algebraic analog of Bot periodicity and an algebraic topology. So the topologist might be interesting. Might be interesting. It's like a totally separate algebra topology connection than what we've been talking about. Did I characterize that correctly, Michael? All right, where was I going? All right, so let me try to say the theorem a little more precisely. So the theorem. So the theorem is that the BGS conjecture holds for a generic polynomial. And I need to pause here because it doesn't really make sense to talk about. Sense to talk about a generic polynomial with no adjectiv, no more specificity, because there's not a single parameter space for all polynomials. And even within there, there's not a kind of generic value for E of F and things like that. So it'd be hard to know what it kind of means. So as usual, we want to break down the space of polynomials into discrete parameter spaces and kind of understand it there. There. And so the statement is going to be: it holds for a generic polynomial of any degree. And the usual thing would be to fix the degree and the number of variables, but I'm going to fix a slightly different invariant called strength. So the statement is, holds for any generic polynomial of any degree in strength, and this we should think of as a proxy for the number of variables, and I'll define it like that. So if I have a polynomial, the strength of F is the minimal S for which we can decompose F as a sum like this. So I want to decompose it as a sum of lower. It as a sum of lower-degree polynomials. So the key point here is I need these guys to live in the maximal ideal. I guess I'll just say I need them to be positive to rule out f equals 1 times f. I want it to be a non-trivial decomposition. The definition goes back to Ananian and Hoxter's work on Stillman's conjecture, and I'll say a And I'll say a bit more about it in a second, but let me just do a couple examples. Let's try to fix this. So let's do a quadrant. So the how about we do the one over there. So the strength F is this guy. I mean, this is almost the model for something with strength 2. I could start counting as 0, 0, 1, and 2. Counting as zero, zero, one, and two. So this has strength two. I mean, from the way I've written it, I've only demonstrated the strength is at most two. Maybe there's some crazy way to write it, but don't worry. It's like the strength of the state. Here's a funny one. What's the strength of this guy? Linear form. Yeah, that's the right look, man. You look very suspicious. You can't decompose it, right? You can't decompose it, right? So it's conventional. It's infinite structure. There's no way to decompose a linear form in this way. It's stuck. Okay? How about, let's do one more. Let's let F be in, again, this minus what the ring is. And let's rule out the linear case. Case. Let's say it's at least 2. Well, then you can always write f as a sum, you know, like this. You can always write it as x 0 times z's. What's oh, z's? It was supposed to be z's, thank you. And you mean homogeneous. Yes, everything's homogeneous. So I want everything homogeneous. Want everything homogeneous. Everything homogeneous. Okay, so what you see is that the strength is then never more than n minus 1. And so the strength is kind of paralleling the number of variables you need to define a polynomial. Anani and Hoxter, in their work, were trying to prove things that they wanted to prove Stillman's conjecture, which famously involved. Conjecture, which famously involves coming up with bounds that don't reference the number of variables. And so they wanted a more stable proxy for something like the number of variables you need. And they introduced this notion as a proxy for it. So n minus 1 or n? 0 through n. Thank you, Mike. I think at some point I added Z0, and then minus 0. That screwed up. Is there an effective algorithm for computing this? That's a hard no. So there's no effective algorithm. I mean, it's what it amounts to, if you want to get fancy, the only known algorithm would be like determining which secant variety, there's some secant variety construction you could say if you strength is like, do you lie on the third or fifth secant variety of some thing? So, I mean, if you could write. There's an algorithm that's just not effective. There's an algorithm that's just not effective at SharePoint. Yeah, yeah. I mean, there's an algorithm that hasn't been implemented because it's actually Justin Chen tried implementing it, and I think it didn't terminate for like a cubic and three variables or something. I mean, so there's not a good algorithm. Is there an expected strength given the number of variables? Great question. So the question was: Is there an expected strength given the number of variables? And the answer is yes. This is so. Is so let me think. It has to do with so the expectation is that the best decomposition, the most efficient way generally for general polynomials, should be if these are linear and these are degree d minus 1. And in that case, writing f as linear times this is equivalent equivalent to saying that V of F contains a linear space. The space spanned by both the linear forms, right? Because if F is this, then F belongs to the ideal generated by the Li's, and then you do algebraic geometry 1, V of F contains. And there's a lot of classical algebraic geometry that says when does a generic hyper surface contain a linear space of a certain dimension? A linear space of a certain dimension. And so I'll screw it up, but the generic strength is not n. It's something like n minus n to the 1 over d or something. It's the general strength is not linked. It's sublinear. There's a paper, right? Oh, there's a paper. There's a paper, right? Bic, this is Maya's first thesis problem was to basically answer this question and then she got stooped. What was the number of cutscenes? I don't remember. I want to say it's like n minus n to the 1 over d-ish, but I'm sure I'm off, right? But the point is it's not n. It's smaller than n. Oh, a a note on this this um definition of strength is that uh it's a kind of elementary definition when you look at it. And in later years, me and others, I mean, people have realized it's shown up elsewhere in the literature. So it showed up in number theory, in Schmidt's work on counting rational and integer solutions to polynomials. And integer solutions to polynomials, this turns out to be an essential invariant. And it showed up in the work of Kausden Ziegler in algebraic combinatorics and algebraic geometry over finite fields. So it's kind of curious that the same definitions show up in the explosive. All right. So I need two additional notions. Let's say the collective strength. And a set of polynomials is the minimal strength of a non-trivial k-linear combination. So it's a way to assign a strength value to a set. A strength value to a set. You look at non-trivial, like I said, I want homogeneous wedged in there somewhere. I want to wedge homogeneous in there. So I want homogeneous k-linear combinations of the F's. And I take the minimal strength. Meaning you can just only pick F's at the same degree. Exactly. So if the F's had different degrees, then it's just the minimal distance. And then the second definition I want is if. If F has strength S, then the secondary strength is say at most n if we can write F like this. Like this, where the collective strength of the G's and H's is at least 7. Or sorry, is that on staff? So collective strength is a way to measure the strength. Is a way to measure the strength of a set of polynomials. And the secondary strength is saying I look over all these decompositions and I ask about the kind of strength of the sum. So I overcollect the strength of this sum. All right. All right, so let me do an example and then we'll let me let me not too let me explain why I'm talking about this. Let me explain why I'm talking about this. So, the idea here is the actual theorem that is the following. And it's that, you know, if there exists a constant depending on DNS, where if F is any polynomial of degree. Of degree D, strength S, and secondary strength, at least N. Then, in fact, I was gonna say VGS holds, but you actually get something stronger. What you get is that the MCM value is on the nose. is on the nose 2 to the s minus 1 which is at least 2 to the e of s. So you've written strength at least at secondary strength. So strength is yeah I wanted equality but wait you can only technically define secondary strength most at. Oh whoops that's right I think I want me to at least Let me think a sec. I think I want it at least in both places. Sorry, the definition is that it's at least signed. It's it's actually factoring. It's a little fun. It's the max of these things. Yes. Okay. And so then the theorem is, I mean, the key point here is that you have to understand that this is the genericity condition. High secondary strength is an open condition and appropriate for another state. So that's the idea. So that's the idea. So I don't want to talk about the proof. The proof involves ultra product methods, and that's not going to be great in five minutes, right? So maybe we won't do that. What I want to talk about, though, is how, why this is, I mean, probably looks like a very technical statement, but I want to try to talk about where it comes from and why it's somehow a natural statement. And so for that, you rely on what I think of as one of the key ideas about strength of polynomials. It's the Ananian-Hox principle. So this is the following. It says, if you take If you take polynomials of very large collective strength, then they act like independent variables. So it's kind of an intentionally vague statement. Statement, but it captures the idea that was at the heart of their paper. So, at their paper, they proved a million theorems, they proved a lot of theorems to the effect of if you take forms of very high degree, then they'll determine a prime ideal. They'll define a prime ideal. And so, in that sense, they kind of act like variables, which independent variables define a prime ideal. If you take thing again backwards, independent variables, Thing again backwards, independent variables form a regular sequence. So you might say, oh, is it true that high, you know, forms of high collective strength form a regular sequence? And yes, that's another theorem of an Einhamstead. So there's a whole bunch of these. This is kind of a meta-principle for predicting how forms of high collective strength will behave. And by and large, I mean, it led to many, many theorems of this game. Even some and kind of some geometric theorems that had appeared. Theorem that had appeared earlier and elsewhere. But so, the overarching picture I want in your head is that what this says is that if you have elements of high collective strength and you kind of squint, you can pretend that they're independent variables. That's somehow the intuition behind their principle. And so, what does this have to do with this? Well, the idea is if I write f as As in this form, and these guys have high collective strength, the G's and the H's have high collective strength. Well, then they're kind of like independent variables. And so it's kind of like a quadrant. And we know about matrix factorizations of... And we know about matrix factorizations of quadrants, right? And so the idea is: what I want to do is take the intuition from this principle and say, well, as long as these guys are somehow generic enough, as long as they have nice behavior and you get a decomposition like this, then it really ought to act like a quadrant. And that's where we want to go. All right, so questions on the intuition? Do you want to do too much more? That's right. There's there's very intentionally no reference to the number of variables. Yeah, that's so what Carol's getting at is kind of how do you make this all precise, right? And that's where I'll be like, oh, I'm out of time, hopefully. The basic idea, so I mean the big idea in the proof. So the big idea in the proof is we want to make, we can make the predictions of the Ananyan-Hocher principle into theorems via a certain framework that we developed. We developed along that I developed with Stephen Sam and Andrew Snowden several years ago. And what it did is it involved a certain graded ultra product ring. So it's this graded ultra product ring, which is you take, a little dub hint of what it is, so what you do is you take the polynomial ring in infinitely many variables and you start by just taking And you start by just taking a direct product of it, an infinite direct product. But direct products of rings are kind of bad. You start with a domain and you take a direct product. It's not an integral domain anymore. And so what you do is there's a certain equivalence relation that comes from model theory, from an ultra filter. Let me just say it's complicated. But there are certain equivalence relations that if you mod out by the direct product, you get very nice properties for that quotient. And this is what we call the ultra-product range. And what it does is an element in the ultra-product ring is a sequence of polynomials, right? Sequences of polynomials determine elements, and everyone's determined that way. It's many to one. Is determined that way. It's many to one. But what the construction allows you to do is imagine the theorem were false. And then you'd have kind of a sequence of counterexamples of ever-growing collective, or sorry, of ever-growing secondary strength to the theorem. And you could take all those counterexamples and kind of mash them into a single element in the ultra-product array. And then what we did was develop. And then what we did was develop the algebra of the ultra product ring, which it turns out is actually very simple. And so we kind of use that to take these sort of potential counterexamples and rule them out and make these sorts of things precise. I don't have too much time for that, but I'm happy at grades or whatever to talk over that. But in particular, I assume you have very little control of the game. Little control of what they get. That's a great question. It's absolutely inexplicable. So it's very much, yeah, it's absolutely inexplicable. It's like Ethereum, it's like what you get from Ethereum, right? There's no, you get finite generation, but no balance. There's no, and there's no refinements of it that would, at least the way we do it, that would, that would help us out. So it's totally explicit. All right. All right. Alright, so this basically gonna end here. I want to actually end though with two questions that I thought were interesting from this week that have been occurring to me. So one is, you know, we saw in the BGS conjecture that we had this exponent of n over 2 popping up, give or take 1. And I'd be curious to understand a better heuristic for where that comes from. I mean, that was. From. I mean, that was, you know, that was somehow one of the questions I got, and I don't have a great answer. But the construction that they use, I actually think, is somewhat interesting. So I mentioned how it uses a Kazool complex. It also involves a sub-variety. So you've got, so the BGS construction, you have your hypersurface. It also involves a sub-variety of roughly half the dimension, which is like The dimension, which is like just about where the Lefschetz hyperplane theorem doesn't tell you what kind of homology classes are. And I've always wondered if that was a coincidence or not. I mean, I can't say anything more precise, but I've always thought that was interesting. Where does the n over 2 come from? And is there some geometric meaning about that? That's where you get kind of surprising sub-variety. And another question. And another question that I think is interesting for folks in the audience is the connection between the BGS conjecture and what I was calling the Banff conjecture. You know, if you could prove BGS for certain polynomials, you might be able to then also get cases of that sort of conjecture for certain classes of complexes F. And so it might be interesting to even prove new special cases of the Bookweight's Royal Schreier conjecture. Royal Schreier conjecture, and then see what those would convert to on the rank conjecture side. That I think is an interesting avenue that at least I'd never thought of until the last few days in the conversations we've had. All right, that's all. Thank you. Is there anything by the other if you know the random conjectures? Do you get some PGS? Does it go back? Does it go backwards? Is that your question, Sri? Yeah. I don't think so. I've tried to make it go backwards, and I just haven't seen how to get it to go backwards. But I mean, you know, I don't know. So I always had the belief from conversations with people over the years that there was this view that this was kind of in the same family as the other rank conjectures. Rank conjectures. But I've never seen someone write down relationships among them. And when I actually tried to do it over the last few days preparing for this talk to make sure the implication was true, it was a little harder than I thought it would be. So I just don't know that, you know, I mean, I think heuristically people believe they were connected, but I don't know of anyone who really can do that. I guess Mark was telling me that, you know, the Betty degree stuff, the Betty degree conjecture had that in. Betty degree conjecture had that in mind, and then Raina and David's work connected this to the Betty degree sequence, but those are all still kind of one-way streets from what I understand. I don't know. Do you know of anything in that vein? Three no? Oh, yeah. Are there other questions? You know, all the other conjectures is a group hanging around somewhere. There's a group hanging around somewhere. Is there a group here somewhere? Is there a group here? But where's the group in the algebra? I mean, in the algebraic conjecture? Oh, in the physics. I mean, it's still a toy. Everything's... Oh, oh. Well, but, I mean, but in the algebraic connection, they're overlocal rings, too, right? Yeah, we haven't talked about those. I know, we pretend they don't exist, but they're, right? I mean, it's. But they're right. I mean, it's I don't know. I mean, it's in the graded case, it's the same group, right? It's for the things I'm talking about, the same there's a hyperservice, right? It's a group that goes with the F. Oh, I was meaning like the C global C star. I mean, what if he's I couldn't hear Max Plumatis was saying that I don't think there's a group? There's no group, but there's no group for where's the group? Where's the group in the total rank conjecture, or even the Booksbaum-Eisenberg conjecture? Oh, we know there's an operator, a linear operator that's acting on the vector space. That's good enough. There's something that's not really good enough. Yeah, okay. I don't know. I don't know. Wait, did you have a question, Ellen? Yeah, so I know you already said that this capital N is not exactly something you can get your hands on. Something you can get your hands on. But this does seem to suggest that if the conjecture was false, then you somehow want to find something of secondary strength, kind of low with respect to the degree in strength. So does that give you any insight into where it looked? I don't know. I mean, I just don't know. It's so inexplicit, right? That I just don't know. Yeah, I mean, it's... I mean, I would if it so the one thing I would say is it's it's if the collective strength the argument doesn't quite give this, but I'm not convinced you couldn't. I think you might be able to if the collective strength of just the G's was high and you had some weaker hypotheses on the H's, then you might be able to push something similar through. Be able to push something similar through. So I would look for something where the strength decomposition is not kind of linear and nonlinear. It's like quadric and quadric. So I'd start with cortex of lowish strength or something. That would be maybe a place to look for counterexamples. Again, it's vain because if I could have proven the strong result, I would have, right? But at least it's, you know, that's where things would really go off the rails, would be if, like, Off the rails would be if, like, if these guys had, if the g's were not linear and maybe the h's had lower strength, something like that. Like sums xi squared, yi cubed, things like that maybe could be, but I don't know. You have to speculate, that's the last thing. Um so i uh is is the theorem making both Is the theorem making both claims about the equality and the inequality, or is one of them somehow? This one's trivial, though. So I actually, the only part of the proof I was going to do was this one, and it's like three lines. So it's this one follows by the product rule for derivatives. So if you have something like this, you can bound the singular dimension of the code. If you take a derivative of this, every derivative will vanish modulo the G. Will Vanus modulo the G's and the H's by the product rule. So the singular locus can't have codimension bigger than 2s plus something. So the inequality of these two is trivial. So yeah, so it is stating it, but it's. That part's true always. That part's true always, yeah. It has is long and that one. This is the interesting part. Yeah. Do you think there's a general relationship between the MCM of F and the stream? Great question. And I think the answer is no. Great question. And I think the answer is no. I think it's a so at one point I thought it was yes, but then I realized that that contradicted something else I believed, which is, so I tell you an interesting example, which is if f is the determinant, no one's ever proven this, but I believe, I would conjecture, that the strength of the determinant is given by the Laplace expansion. You go along the top row. So the strength should be A. That's the most efficient way to do it. So no one's proven that beyond the 3x3. No one's proven that beyond the 3x3 determinant. But if you believe that, then there's a big gap because the determinant has a, the co-kernel of the determinant is a rank one maximal colon-Macaulay module, or rank, wait, but small. I'm screwing up my hand, but it's much smaller than 2 to the n. It's, you know, it's, it's, yeah, I mean, because it's the matrix factorization is it and its adjoint. So it's, it's massive. So it's massively smaller than 2 to the n, right? So you can't hope for any reasonable bound solely in terms of the strength. Get an upper bound in terms of the strength? Yes, yeah, yeah. So just by like, when you multiply matrices, don't easier to name. But it's not very good. I suggest we discuss more our coffee. Afterwards we'll have working groups.  