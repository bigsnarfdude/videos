Good morning, everyone. Yes, thank you for coming. I had like a question that like, yeah, it's in general like addressed to everybody, but like But like maybe specifically to Ben and Rich because of they discussed this editing of EBMs, right? And like and maybe also towards Thomas because you talked about a lot how to okay you have this interpretable model right and then like you actually need to look more into the so you can look at the transparent model but then like you have to contextualize it with the data to understand that actually The data to understand that actually, like the wrong numbers hide something bad, right? And then there are treatment effects that really confound how the reality actually is. So the point is in the end that really like we as interpretability researchers we're trying to create all these tools to explain models, right? But in the end, to actually In the end, to actually interpret the data in the correct way, we still need to do a lot of investigation, right? Like we cannot stop just to add the tools. And my open question is, do you ever think that we will ever be able to get rid of this human in the loop in the interpretability process? Or like yeah, that's pretty much it. If anybody wants to start over here. If anybody wants to start I don't know if the question was to human in the loop not like for for experiencing itself more like to make it less cumbersome for humans if we can automate it a bit more and did you mean that or did you mean the human in the sense of treatment and then so what I meant mostly was um if uh we If we can make interpretability as user-friendly as possible, so they don't have to know statistics so much, let's say. Yeah, I'm not sure how to say it, but also because I'm thinking a little bit on the fly here. But you can also start replying to them. So the question is whether they need us. So the question is whether they meet us. Yes. Okay. The final user anyway. So I guess I can make a comment. The tool that we have built called GamChanger, meant to be a pun on Game Changer, but we're editing generalized edits of models. That tool is designed to be. That tool is designed to be used by end users without having data scientists or statisticians involved in the process. So it really is designed for end users. And it enables them to walk through making edits, corrections to the model. And then because we don't want end users to overrule the model too much, we don't want their own biases to sort of take over instead of the data. Instead of the data and the biases that are in the data, we make sure that it's a slow, careful process to do the edits, that you have to document your edits, that you have to examine the impacts of your edits on different cases, and that the edits you make are time stamped and stamped by the user, and that we can see what it looked like before and after your edits. And that you can unroll edits if you later. Edits if you later change your mind or somebody else comes in and overrules you. So we're very careful about this editing process, and the whole system is designed for end users. And the doctors we work with have had no trouble using the tool balance. Does that mean they always do the right thing? Not necessarily. Some of the edits that we make might be incorrect, but then the data is also incorrect to begin with, so the model is also incorrect. To begin with, so the model is also incorrect. So we're just attempting to make the models better. You know, statisticians have this saying, which is all models are wrong, but some models are useful. So we are strong believers in that. There's no such thing as a perfect model or perfect data. Our goal is just to help make the model as safe to use and hopefully as accurate to use in the real world as possible. And then accuracy on test data doesn't really tell us what accuracy. It doesn't really tell us what accuracy in the real world will be. Sometimes interpreting the model in the first place does benefit from having a data scientist available, partly because as data scientists, we understand things like confidence intervals and error bars, and we understand some properties of the model, such as if you have five strongly correlated features, then what the model will learn will be spread among those five features. Among those five features. And in some sense, you have to think of the strength of what the model has learned as being the sum of the effects on the five features. And these are the kinds of things that data scientists and statisticians naturally understand, but end users don't necessarily understand things like that. So there are definitely advantages still to having users, I'm sorry, data scientists working with users in the loop. But for example, our editing pool really is. For example, our editing pool really is designed for end users without needing a data scientist, as long as they think they understand the model along. Sorry, I don't want to talk too much. Maybe I can add to that. So as long, so Rich, I think you're correct, as long as the data is in a tabular format. Absolutely, yes. Our methods are only really appropriate for tabular data at this point. Yes, so because when you have So because when you have sequence data of any form, I think at the moment I cannot imagine how a loop would work without a data scientist. So for example, if you have time series data, how do you split the data? That's already like a big open question for many time series data sets. And so maybe this will be resolved. So maybe this will be resolved in the near future and then such tools may also emerge. But currently I don't think that this is possible for time series data. Or are you aware of any tool that is also designed for time series? No, I completely agree with you. You know, there's at least three stages to model development. One is training the model, and that's when decisions about train set versus validation set Set versus validation set and all these sorts of things come into play. And there, you can imagine that the data scientist is probably in the driver's seat, but they have to be talking with the human experts or they'll make mistakes. For example, if we're training on medical data from patients in a hospital, a big question is, do we want to know your lab test results when you were first admitted to the hospital? Do we want the average of your lab test results over the entire time? Your lab test results over the entire time you're in the hospital, or do we want your lab test results near the end of the stay in the hospital? And those really depend on the problem. And a data scientist who's not an expert in medicine can't answer those questions himself. So this is like halfway to being a time series, because we often have a time series of lab test results. And we need to select at what point in time, what snapshot we should be using for these things. And that requires a data scientist plus a Plus, a domain expert perhaps can get a blog. Same thing for true time series data. Then, once the model is trained, going over the model, vetting the model, finding problems, debugging the data set, debugging the model, figuring out that there's always treatment effects. Oh, the only reason that's been showing, the only reason why a creatinine of four or creatinine of six looks like it's low risk is because starting at three and then even more aggressively starting at five, they start digging. Aggressive looks already applied, they start giving you really effective treatments. And in fact, having the bad high creatinine looks like you're lower risk only because you're getting really aggressive treatment for high creatinine. Then it takes both a domain expert, perhaps maybe with a little help from a statistician to find those things. And then the final editing of the model, that could be done probably most by the domain expert, because now they've already had the assistance. They've already had the assistance of a data scientist, statistician, both in setting up the model training and in trying to understand it. But now the editing, maybe they can do that more or less on their own. But I agree completely with you. Once things get harder, like time series data, you know, then this is all more difficult to automate. And you need even more expertise in the room. Related to this, to the audience, would you think that training those interpretability models, taking into account the level of satisfaction of the end user, makes sense or would just you know enforce reinforce uh the confirmation bias that one uh that an end user could have. That an end user could have. How how do you see this? Like, you know, ha having like as an extra variable how much satisfied is the end user at the end of the explanation that we are giving. Of course, this would need like a kind of a feedback project, but what's your opinion about this? By that, my perspective, I think that all what we're discussing about, and even the tool. About and even the tool that Breach presented, and also other tools that we've seen both is that the end users need the time to use these tools and to build trust in these tools. And this is like building trust is a process that takes time of usage. And right now we're mostly working with retrospective data, but I think that when, at least for the correction, the way I see it is when we really start seeing the difference that it makes. We really start seeing the difference that it makes on this average treatment effect, if you want, is only on future studies where we will try to remove this bias that there is in the data and see how it ends up. So we need effective studies and patients in the clinic to actually see how users build trust. I think there are also two types of satisfaction, right? Two types of satisfaction, right? And they actually navigate between performance and interpretability. So a user acts what he or she thought should be going on anyway and can then be very dissatisfied with the performance of the model and the other way around. And I think there are, at least for like intensive care, there are a few papers where they try to actually incorporate machine learning based workflow and really work with the nurses and the clinicians. Worked with the nurses and the clinicians, and there they saw that they are willing to give up a bit of the explainability, or I mean, it's now easy to distinguish, like give up a bit of, oh yeah, this is what I expected, for better performance. I think we have to distinguish there between model development and model deployment. Exactly, exactly. So there is this phase of model development where clinicians really want to know what's Where clinicians really want to know what's going on. And then, after some colleagues, the clinician has confirmed that it's okay, I think then they are fine with a more black box approach. I guess. Yeah, no, go ahead. Ah, okay. Then I took it. Sorry. So on that point, I have to reconnect with the confirmation by. Connect with the confirmation bias. I mean, the problem is that, like, again, so I agree that the end user has to build trust with the tool, but the problem is that oftentimes, if the model is not aligned to our own confirmation bias, then we're going to be super dissatisfied for the model in some reason. So it's really a process that takes maybe, I think, a little too much time for it. too much time for uh for it. Like it creates a lot of problem in terms of uh uh deployment, in my opinion. Yeah, but maybe I wasn't clear with what I meant. Uh when I said that we need uh studies with integration in the clinic, it's but only to measure user satisfaction because you might be dissatisfied with the tool because it doesn't reinforce your confirmation bias, but yet using the tool will make you a better physician. And that's why we need these kind of studies where we then we need to see like, okay, AI. See, like, okay, AI helped us detect, I don't know, multiple sclerosis earlier or give better treatment to the patients. But that's in the randomized clinical trial, right? In the end, that's what, I mean, from my perception, that's what doctors at least, if we talk of this group of users, that's what they ultimately trust. And then they really are willing to give up a bit of, I understand why this is working on. I I understand why this is working or how this is working. I think where we should try to go because otherwise we're just talking and it would only be very progressive physicians that we decide to adopt at their own risk our techniques. I think when you're talking about what a model is doing or interpreting a model, there's two aspects to it: it's understanding the model and it's understanding the real world. So there's always a point. Real world. So there's always a conversation between the modeler who understands the model better and the physician who might understand the reward better. And so sometimes when something doesn't match your preconceived notion of what should be going on, sometimes the decision is right and sometimes they are not. So I don't think you can say beforehand whether or not it's good to include them. I think it should be a conversation where both sides kind of agree that they could be wrong. Agree that they could evolve. I think it's important also to distinguish between global explanation, betting the model, which might be done prior to deployment, and then local explanation, which is the ability of the model to explain any one prediction that it's making that a doctor might or might not agree with. And they're very different things. Now, the EVM models are designed to be able to do both. Designed to be able to do both global and local explanation. Some methods are better at one than at the other. One of the issues also is that most doctors aren't that interested even in the prediction of the model, let alone spending the time to understand why it's made that prediction. It's only when the doctor feels that the model is predicting something different from what they thought that they suddenly might get interested in the model's prediction. It's like, oh, why does the model think this patient? Why does the model think this patient is at high risk of the following or should receive the following when I don't think that? And then the doctors suddenly are interested in learning a little bit more. But the truth is, you know, hopefully 90% of the time the model and the doctor agree. So then the doctor doesn't really care what the model predicts or why it predicts it because they disagree anyway. So it's only in the cases where there's disagreement that a good doctor would. Would spend some time trying to understand the reason for the prediction of the model made. I also have a comment like whatever it is that we now have access to many many data points on very many omics platforms, but what statisticians have said to begin with, look for parsimonious model, try to be a Try to be really parsimonious so it's explainable to others easily. So, first, one should try with the lower level models, then make it if the accuracy is not good enough, then you just don't want to feed any data possible in the model and then try to get something out of it. That's a wrong way of modeling, and it is not as interpretable end of the day. End of the day. That's my thinking about it. There is also like a. Sorry, go ahead. I was just going to tell you an interesting story about that. I agree in principle. Early days of COVID-19, body mass index, BMI, was considered a very important risk factor, and patients were admitted to the hospital if they had COVID-19. More often, if they had high BMI, because it was considered a very risk factor, a very important risk factor. The modeling that Ben and I did together in the early days with hospitals in New York City where the pandemic hit hard first in the United States, suggested that once you took all other comorbidities factors into account, and this would be things like hypertension, diabetes, heart disease. Heart disease, and all these sorts of things. Once you took them into account, it turned out high DMI was not important at all. So it turned out that the reason why high DMI was important was because it's correlated with age, hypertension, diabetes, heart disease, and all those sorts of things. So if I can only tell you one fact, it turns out knowing that the patient is high VMI or not is a really important single fact. Single fact. And yet, if I can give you a little more detail about the patient, and I can tell you that, oh, they are hypertensive, they're not diabetic, they're 75 years old, and they do have a history of heart disease. Then it turns out knowing whether they're IDMI or not doesn't convey any extra information. So it's a really interesting problem in that the simplest model would like just know like three factors about a person for COVID-19. About a person for COVID-19, their age, their body mass index, and whether they're male or female. Like that model alone would tell you a lot. It would be pretty accurate. But a little more complex model that told you whether the patient was hypertensive or not, whether they had diabetes, like that model is even better. And then you don't even need the BMI anymore. It's really interesting. So this notion of model simplicity and using Simplicity and using the simplest possible model first, it just turns out to be a more complex issue than we imagined. And we're now doing something that in the early days of machine learning, I did research on feature selection and was a strong believer in using the smallest number of features that were accurate. And now with the EVMs, we actually do the opposite. We try to always start with all the features, and we only exclude features. And we only get screwed features if we actually believe they're hurting us. It's really, it's turned this notion of simplicity in terms of features, at least, on its head for us. It's quite a surprise. Yeah, I would think that the person who is modeling is taking advice from real practitioners, and already there is literature, tons of literature, when you are getting the BMI in what really affects BMI and what. Really affects BMI, and what else do you have? You have to think about it. You just don't randomly pick one variable and put it in there. A lot of thinking goes in to input a variable. If you know that your BMI gets high or low, depending on some other covariates, then you have to choose between them which one is the thing you have to incorporate in the model. You don't blindly just put one variable. Blindly just put one variable in the model. See why it is important in the molecular aspect, you have thousands of genes. If you are doing the network analysis, you don't want to, it will break up if you don't really need some filtering somewhere. So in the network, many a times it is not possible to take the entire network. You subdivide it into different modules. And for the disease, if you think that Disease: If you think that these modules of genes are really important, you keep them in that network structure, you don't put everything over there. So, I'm saying that before modeling, you have to learn a lot about the system you are modeling. And then you put the variables. It's blind modeling is a recipe for disaster. And I know now you can do way more with computing power, but just giving everything over there is. Just giving everything over there, it's not a good idea. Interpretability will be pretty tough in that case. Although our suggestion is the opposite now, we always suggest that we'll start with all of the features and then let the model tell them what's happening with those different features and which ones should be kept, which ones are eliminated, which ones are strongly correlated with each other and therefore. With each other, and therefore, you at least have to know that you either want to keep them as a group or eliminate most of them. Here's a weird thing that happened with the body mass index: which was after we had all the other comorbidities in the model, it actually learned that high BMI leads your risk slightly lower than COVID-19. And the doctors didn't trust the model in the beginning, and they were like, oh, this is wrong. The model's doing the wrong thing. It turned out, because they were admitting more patients, We were admitting more patients who were high BMI than patients who were low BMI because they thought BMI was such a high risk factor. They were on average admitting lower risk, high BMI patients. And the other patients who were not high BMI who were admitted were actually higher risk because if they were sick enough that they still had to admit despite having a normal BMI, they had to be pretty sick. So what the model had actually learned was that once you correct it for all Is that once you correct it for all the other factors, having a high BMI is actually slightly good for you for hospitalized patients, and that's because there was a selection bias to be admitting on average slightly lower risk LBMI patients. So now it took a while to figure that out. It wasn't popular to be in there. But it's not that we think the high BMI actually was good for you. We think it was a selection bias in the way patients were admitted to the hospital and therefore in the training set that caused them to look like they was good for you. That caused it to look like it was good to be. But anyway, but our advice these days is to start with all of the factors and then start to remove factors once you have an idea how they influence the model. Not to start from simple and work your way up, which is boy statisticians would almost always at work, which is start with the fewest number of factors that you have to be in the model and only add factors as evidence requires you to do it. Requires you to do it. With the interpretable models, we go the other direction. We go more towards feature elimination as opposed to forward selection. It really depends on the dimension of the data set. I mean, if you have too many, you don't want to feed them. For tabular data, like what you have, it is possible to do it. But in the omics context, no, this is a very good point. Yeah. Very good point. I mean, you never want to have 10x or 100x more features than data. I think that Thomas raised the hand before. So maybe to add on to that, it might also depend on the purpose that you're training your model for. For example, I'm reflecting on the talk we heard, I think it was yesterday, on interpreting a neural network as a causal diagram, and one of the requirements was. And one of the requirements was that the input of the features are deseparated, causally desparated. So, I think if you want to, for example, analyze the causal structure of your network and the causal effect, you need some kind of input structure from your data. You cannot go with all the data because then your assumptions break down. But again, if you want to analyze the data, then it Data, then inputting it all and just looking at how is the effect of one variable, and how can you design what are the artifacts, the biases in your data. I think each has its own benefits. There's a real chicken in the egg bottom there. I guess it's what Sushmita was saying because it goes a bit against the curse of dimensionality and all of that, right? To say just. all of that right to say just just feed everything you have well it depends on how many samples you have and yeah I guess all of these things still hold so for us in in the genetics field we have 60,000 features and 200 patients it's just you cannot even look at one of them each one of them independently I wouldn't have enough hours in my life to do that so that's I guess something that is still unresolved. Still unresolved. Yeah, I was just wondering if the two are really that much at odds with each other. Because even if you start from many features and you go down to viewer, you're still trying to find a kind of parsimonious model that makes sense and that doesn't add unnecessary features. So I'm not sure that the two are really all that different, except from where you start. Right, yes, there is less bias on when you start. If you start from all of them and then you reduce only after you have. Then you reduce only after you understood what is going on. There is less bias from starting to build a model. I think if you have the possibility to fit a model with all the features and you have enough data, then it could make sense to then have some filtering done afterwards. Just not then. In the context of the year model, you have the stepwise regression. It helps you know that. So either Of you know that. So either you start with all of them and then reduce it, or you start with one, keep adding somewhere to in-between is the real model. So I didn't mean that the two approaches are not different, just that they're not different in terms of that they both try to come to some kind of parsimonious model. Can I make a comment about that? Actually, with the EVMs, we're not looking for a parsimonious model. We're looking for Model, we're looking for an interpretable model that's understandable to the experts. And parsimony, we have found is not necessary for that. In fact, it can be harmful. Let me give you an example. So remember the risk as a function of age in the pneumonia problem. We saw a jump in risk at age 67, 68, 69. That jump in risk is, we're almost certainly due to a retirement. Is we're almost certain is due to a retirement effect in the United States. We see it often in data sets for different medical problems. If we have a retirement variable in the data set, we want the retirement effect to show up on the retirement variable and not be part of the age graph. However, if you go for maximum parsimony, for example, you're going for a feature selection, it will eliminate the retirement variable and it will The retirement variable, and it will put the bump in the age graph. And that's because it can, because there's such strong correlation between retirement and a narrow range of age. So it can basically be fewer features and just as accurate. It can be more parsimonious. However, it makes understanding the age graph more difficult because now you have to do detective work to figure out why is this bump there. Now, in the case of retirement, it's pretty easy to. Case of retirement, it's pretty easy to understand why the bump is there. But there are many other correlations among features where you cannot figure out why the bump is there. And the more features you eliminate and go for parsimony, the more you are exploiting correlation. And you are causing all of the effects that you learn to be cooked into the fewer number of more powerful, highly correlated features. And our experience is this hurts. Our experience is this Hertz model interpretability. So it ends up making each of the individual graphs more powerful, but also more complex. And then it's harder for experts to understand the graphs. And if you need to correct the model, it's harder to know how to correct the model. Instead, we would like to have twice as many features and have what's learned on each of those feature graphs be more independent. So arson of the usual measure. arsenony in the usual, measured in the usual way, is actually something we are not aiming for in the interpretable models that we use. I know this is like shocking. My own experience is that working on this interpretable stuff has changed the way I think about so many things. It's changed the way I think about feature selection. It's changed everything I think about the missing. It's changed everything I think about missing values. It's changed what I think about parsimony. It's just changed a lot of things because all of a sudden interpretability is equally as important to us as accuracy. And what's good for interpretability can be fairly different than all the things we're used to learning are good for accuracy. I align with this because, in my own experience, when you use the In my own experience, when you use the whole feature space and a model that, like IBMs, EBMs, just has a correct spread across the features that is. When you have new data and if you have done feature selection, you are in a superbased way being too greedy about the labels that you are praising. About the labels that you are going to predict. So, when you reduce the field space and new data arranges, and you have this normal in the middle that was not present in the data before, you have reduced so much the feature space that you now have enough resolution with the function that you have learned to see that this new data that is not exactly, for example, doomed. Is not exactly, for example, do more, but it's not exactly normal. If you have a performance feature selection, probably you are going to classify in either way with high priorities, but if you have a good feature spread with relevances, what you can try to do is just to make local explanations about this sample and to see that they are in the middle between the labels that do what. The labels that you are predicting. And I have proved this, I have tried this with metal ionic samples. If you perform video selection, you don't have enough resolution to just emute data this normal and immediate. Especially for metagenomics, and we should have a lot of viability, I guess. So does this mean that you kind of need to redefine for simic? Because in the case of the BMI, you still pick out the BMI, because if you add the other things, it doesn't add anything anymore. So that's also, in a way, it's going to the simplest model that still makes sense. It's just in a different type of parsimony than when you look at accuracy only. I think that we are somehow like underestimating the potential of interpretability for Of interpretability for knowledge discovery. I think that this is something that should be actually included, like, you know, as you said, like different ways of looking at the problem. Maybe the answer is that one, but can include more unexpected variables or maybe a different response, a different solution that is equally valid, but it's including something different. Including something different that probably the clinician didn't consider or is not thinking in the first place. So, yeah, I think this is another ingredient of interpretability that is really important, like uncover unexpected things that are equally valuable. So, I think we're not looking enough at the phenotype. We are very concerned about the model, so especially in the ICU data, we are very concerned about the model. We are very concerned about the model, how we train it, how it performs. But most of the time, I mean, Juliana, you can confirm this, is spent on discussing the phenotype and how to extract the phenotype from the data. And I think when what I'm about to do, and what I think should be done more often, is investigate different variations of the phenotype and look at how does your model change to then. Your model change to then do this knowledge discovery: like, okay, we tune on the phenotype little parameter here, and suddenly things happen with a feature regarding whatever. And this is done far too rarely, I think. About the knowledge discovery, and probably also closer to what you were saying, then my question is: how do we actually know? How do we actually know that what our model finds out is as new knowledge? How do we evaluate what we find out? It's not even anymore about evaluating interpretability for human use purposes or for the user itself or for making decisions. It gives us candidates. So, the interpreting the model gives us candidates about things that we didn't know would happen. And in the case of EPMs and everything that Preach has presented so far, they have Everything that Rich has presented so far, they had domain experts, they had tabular data that were still easy to read and interpret. But if we move to more complex data, sensor data or something, how do we evaluate how truthful is this knowledge that we have generated and whether it's not biased or whether it's not actually showing something else or data? Yes, we can. Did you want to say something, Savannah? Did you want to say something, Selana? But you design experiments to then test. But if it makes some kind of predictive, whatever information you're uncovering, knowledge, if it implies something else, you never know anything for sure, right? You only know that it agrees with what you've observed. I agree. I mean, like, in the end, so I think, my personal belief is that, like, when we do machine learning and statistics, we suddenly forget about the actual scientific method. So, I think we should go back to what Inge was presenting, for example, and reconstruct the cycle of, okay, we have these models, we can generate hypotheses on this model and create a model out of this model, and then we can use the model to do predictions, right? And if the predictions are correct, then like And if the predictions are correct, then the model is most probably correct. And again, we cannot demonstrate that the model is going to be true, but we can find out when it's false. In my case, I'm sorry if I counter, if I go on with this discussion, but it's just like, in my case, it's true when you know that your model is performing really well. The problem is when the model is in between. So when you don't get that ninety four, ninety five, ninety eight percent accuracy 98% accuracy, but you only are in a middle way, and you know that you're better than random or you're better than a human. But how do you actually say that anything your mom is speaking is a new piece of knowledge? No, I see this like in, like, there are two pieces. Now, like, one is corroborating existing knowledge, and this is the ninety five percent accuracy. But the model should be also able to engage Should be also able to engage into, you know, like discovering new knowledge, like proposing new things. Maybe like the result that is 76% accuracy still containing interesting knowledge that, okay, should be tested, should be validated with new experiment. But like, you know, the quality of the predictions should be like kind of a continuum now in the in the type of solution that that model is proposing. Solution that the model is proposing. I always think of, you know, the story of Watson in the Sloan Catherine Hospital. It has been tested for many years, this hospital. And the main critique at the end was that the model was really good, but was not going beyond what an average student in medicine was able to say. And that's the problem of the 95% accuracy. Like you're like you are always sticking to what is already known. Sticking to what is already known and not really, as I said before, like engaging into new knowledge. So I think somehow this component should be also present. Can I make one comment about accuracy? Like most people in machine learning, I was trained that achieving high accuracy on test data was my goal. And that's what I did day in and day out. I no longer believe that. And I distinguish now the And I distinguish now very much between retrospective accuracy, test that accuracy, and prospective accuracy. What would be the accuracy of the system if I used it in the real world? And they're very, very different things. The pneumonia model is rewarded with extra high accuracy for predicting that a 105-year-old with a history of asthma who also had a heart attack three years ago and who now has a lung obstruction and striber is. And Spreiberg is rewarded for predicting that they are low risk. And that's because it's a true statistic in the training data and therefore in the retrospective test set. And yet, we know it's true statistically in the data. It'll be true in every data set we ever collect because asthmatics and heart disease patients healthcare are always receiving high quality care. But its accuracy will not be high if you release it in the real world. If you release it in the real world, and in fact, it will start injuring, possibly killing patients of those types if the model is believed. And if the safety net is not strong enough to prevent those patients from then not receiving care, if the model is believed too much. So accuracy, you've got to be careful. Accuracy on a test set is not a measure of how correct or good your model is. It's a measure of a combination. Measure of a combination of how good your model is and how much it was able to exploit signals which it should not exploit. And you don't know whether most of the accuracy is the good kind of accuracy or whether a large fraction of the accuracy is the exploiting signals it should not exploit. One of the first models that was deployed in radiology was a model to distinguish between calcification in breast images. Calcification in breast images versus tumor in breast images. And when they finally started to explain how the model worked by doing sensitivity analysis, it turned out when it is a tumor, the radiologist uses a tool to measure the size and shape of the tumor. And that's because the size and shape of the tumor affects the treatment that we will receive. And it also means that when we do another image a month later, we'll be able to assess. Later, we'll be able to assess whether the treatment is effective, whether the tumor has gotten smaller. The radiologist only uses that for tumors, they don't use it for calcification. And it turned out the machine learning, the deep model trained on the radiologic images, was cheating. And it was often able to tell that the tool had been used to do a measurement. And if the tool had been used for a measurement, then it would predict tumor because that's when the tool was being used. And once they understood that, they Being used. And once they understood that, they realized that they could not deploy this model clinically and use it because, in fact, it was going to be very inaccurate prospectively, even though it was extremely accurate retrospectively. So, you do have to be careful. Models are rewarded sometimes very much for doing things that we might think of as cheating. The model is just doing what we asked it to do, but we didn't have the prescience, the knowledge, and the understanding of how complex our data is enough in the beginning. Is enough in the beginning to know what data to give them. And often we can never give them the right data because it's not even ethical in healthcare to collect the right data. So it's, anyway, accuracy is a term that now terrifies me in a way it never did 10 or more years ago. The first 25 years of my machine learning career, I happily went about chasing accuracy. And now I'm a little more afraid of accuracy than I used to be. Sorry that was such a long time. Sorry, that's such a long answer. So, with regards to the ethics ascetic. So, if we have a model in which we can detect some signals that we know it comes from problems in the data that we can fix, but there's always to be biases and patterns that we cannot recognize, and they might be either there are things that the model has discovered that we did not know, or they are things that are evil and we cannot recognize them. But we might not. Them, but we might not always be able to tell one from the other. And it's a good technical question to consider: how far are we willing to go to achieve new knowledge? Because at a certain point, you might be able to just let the model run for a while under the assumption that you cannot test, hoping to understand what it's trying to achieve, or how it is trying to achieve, so that you can determine if this is something that you want to keep on your model or not. And this is something that is a very gray area ethically, because you might. Area ethically, because you might do harm, but science eventually has to advance, and you have to cut somewhere to learn what's inside. Yeah, I just wanted to go back to what Rich was saying, because all the examples you've mentioned are basically examples of this scientific cycle. So you don't end with the modeling accuracy, but you then kind of make new predictions beyond your original data, where you ask for a 105-year-old with some kind. 105-year-old with some kind of condition, and then you test that prediction against something, in this case common sense, and then you find out that you need to update your model. So, I think it's exactly this example of the scientific cycle that you're doing to improve the model. I completely agree, 100%. And our goal is just to sort of vet the model and edit it as much as possible before the first deployment so that we have the least. So, that we have the least amount of chance of doing harm as we gather more information. We're never going to get it perfect. As you said, it's going to be an iterative process with scientific discovery as part of the process. There's nothing we can do about that, except we should do it all as carefully as possible. The higher the risk we remain, the more cautious we should be. To reconnect to what Mara was countering, then Mara was uh countering then, like uh yeah, the the problem is that like in many cases we don't okay. I'm I'm a fan of the scientific process, right? But like to be fair, like this is not always possible, right? And like the question is, like, do we have a way to select the model? So if it's not a purity, then like, okay, we pick maybe the 76%, then like, do we are we ever gonna have a measure of how How credible is the knowledge that we extract from the models? Because then, like, sometimes also testing new hypotheses is quite expensive or unfeasible. But we is coming up with examples where you think you know what should come up. So, it could be like easy examples. But I think that's a trick. You can come up with something where you think obvious. But it really depends on data. I mean, in our case, like medical. I mean in our case like medical data like monitoring data you might have if you work on genetic data that's completely different but there I think actually you have more more possibilities maybe of knowledge discovery because also a doctor who looks at genomics data would not know what to do so actually you have a chance On what to do, so actually you have a chance to go beyond what the doctor is able to do. Because we often, I mean, if we predict something super early, then maybe we're better than the doctor in the sense that we were earlier. But in the end, we do use the same data that the doctors look at in order to be the doctor because they are trained. In fact, that's kind of what I was doing. In fact, like that's kind of what I was referring to when when you're working with a genetic there are some some genes that you don't even have antigens for testing it on the tissue. So you might have an hypothesis but testing might be very expensive or require a new approach to the problem. So let's say that you get ten possible hypotheses but you can select only two to actually test. Then how do you select them? Tests, then how do you select them? That's kind of an obviously in between. I'm not saying that we should really skip all the rest and like forget about testing hypothesis. I'm just saying which ones do we test first? It's my problem right now. For high-risk applications, do you think that regulation can be a way of avoiding those harm risks? risks or an obstacle to you know proposing new you know new new models, new ideas. How do you see this? Because it's debated a lot, like if algorithms must be regulated like it's done for drugs, for instance, or something like this. So how do you see this? From your perspective, I think we should start talking regulations and regulators should start talking And regulators should start talking about machine learning if it makes any sense. Because it's difficult to find a sweet spot in between the two worlds. I would never be able to choose a policy for an entire population. But people making policies right now have deep understanding of what are issues and the problems because we're researching on them, so we ourselves are here to learn. So I guess it's an open question and more people need to be actually taught. To be actually taught in both directions. Maybe also with respect to the hypothesis selection, I think there's a lot of merit in unsupervised learning, looking at, for example, if you have three different methods, you have dimensionality reduction, you have clustering, and you have representative selection. So these all measure different things. And I think when you have a couple of candidates, you should Have a couple of candidates. You should then go to the unsupervised setting and explore all the options that you have, that you understand. And I don't think this can be automized. So you automate it. I think you really have to, you have your problem, and this is very specific. And in this case, for example, you have one cluster and you have your 10 things in this one cluster, unfortunately. Then you need to look for representative selection. On the other hand, let's say you want to cross-estimate something, and you go for dimensionality. And you go for dimensionality reduction. So I think this really depends on what your domain is demanding. And I think, again, accuracy is not helping yet. So you have to go to other methods that take structure into accuracy. Yes, some sort of like higher level ensemble, right? Yes, yes. Yeah, that's another thing that I was wondering is if we can use, if it's a true pattern, they If it's a true pattern in the data, but then there's always the same data and how you would like the world to function, right? Complicated than what we think. But if there is something in the data, then multiple models and approaches should agree and pick them up. There could be a way of filtering all methods of checking a model, and let's say you have. And let's say you have a bunch of models together and you have quality control like measures like accuracy being one of them. Maybe there are in the unsupervised context you have a rand index, a just rand index and silhouette view. I mean I'm talking about unsupervised. And run your cluster or whatever model through all of them. So Through all of them. So you will see that there will be a ranking. In terms of one measure, quality measure, one clustering algorithm will give you, a bunch of models will give you one ranking. But when you have other measure, quality control measure, it will give another ranking. It is entirely possible. So in that case, what do you do? Which one do you choose? Number one, One do you choose? Number one is not unique. Not all the quality measures is going to give you number one to be the same. So, what do you do? There are methods, cross-entropy methods, stochastic optimizations. All these things have been thought about. It's just we've got to consult the people who really know what's going on. It's an interesting problem in this context. Accuracy may not be the best measure. Accuracy may not be the best measure, but what are the other choices? You run your model through all of them and see where you are. But I understand your point. What you're saying is it doesn't matter if you're curious your field model. And if you have some hypothesis, you run them through multiple models, you evaluate your multiple models in different ways, get the rankings for your hypothesis, and then you do some statistical analysis on that. And then give you some statistical analysis on that. Yeah, it's an optimization problem. I mean, across entropy or whatever, there's stochastic optimization. You run multiple times and see what is happening for. Where's the ranking? Ultimate ranking. What you get. If you get maximum vote, your money gets maximum vote, maybe first, second and first, second with multiple such quality measure. Yeah, yours is the best. But you do it across. But you do it across all sorts of measurement. The expert is not going to save you. I mean, if you're doing that, it's because the expert could not do it. So in the end, there's always going to be a margin of error, a margin of Yeah, I mean you're risking. You you have a certain amount of risk and and you're it's something that we have to accept and it's something that we have to teach people to accept. Something that we have to teach people to accept because it seems that people can be terrible, they can kill by mistake, and the machine cannot make a single mistake. And it's something that we have to consider. Right, it has been this process has been used for voting when people vote, that multiple people are meeting, you can rank the candidates according to every individual how they voted. And then you have to take the integrated rank, not one. Integrated rank, not one rank. One person, you can rank somebody to be the first one, the best. And another person will rank that person to be the last one. So multiple votes you take. How do you aggregate that all this ranking? So who gets the maximum vote to be the first one or second one? Some rules that the candidate is doing the best. You pick that. I mean, if you don't have any other domain exposure. Don't have any other domain expert or anything, that's what you do as a data person. Even if you have the domain expert, even then, even then, even then. So, that would be a fair way of evaluating anything. One of the ways that I talk. It was very interesting working with doctors, especially in the beginning of code like e analytical data because Analytical data because the doctors didn't know the answers and they didn't have strong prior beliefs because COVID-19 was new. And what was interesting was the models several times suggested things which later were proved to be true, but which they did not accept and did not believe and which we did not pursue because it didn't make sense to them. And later, it made more sense. It made more sense like six months or a year later as we started to see other work. And then for things which they could explain, like they could say, oh, that would be consistent with our understanding of this as an inflammatory mechanism. That would make sense. Those kinds of things they would accept. So it was very, very interesting. There were a couple findings, and hopefully Ben is sitting there shaking his head, nodding his head yes, because he was involved in this. There were a couple findings early on. There were a couple findings early on that the doctors just said, oh, that doesn't make any sense. You know, your model probably is just picking up on something weird. And then we didn't pursue them. And later, they were proved to be right. So there are times when the data actually is telling you something, and the expert doesn't want to believe it. And that can be very disappointing in the long run. As a data scientist, I had no prior belief whether the model was right or wrong. I only knew that I tended to try. Or wrong. I only knew that I tended to trust the model because I've seen it be right so many times. But I've also seen data be weird and messed up in so many ways that I could easily imagine the data is just messed up too. But anyway, it's just fascinating. Experts have biases and they're more likely to believe things which make sense given what else they already know and more likely to disbelieve things that don't seem to make sense. Yeah, yeah. A domain that's a perfect example of that is uh disinformation. A perfect example of that is disinformation. Because in this information, you assume that you have the ground truth, I mean you have absolute truth, and you can represent truth, which is impossible and even dangerous, assuming that you can tell someone what's correct and what's not. And everything that's been done then is, I mean, it's rather dangerous because they are assuming in certain cases what's disinformation, what's not, and it's a field that is completely blocked and polarized and politicized because. Polarized and politicized because every decision and every decision that scientists make is subjective and is political, and that's something terrible in respect to society. And there's little that you can do beyond just characterizing data and trying to understand the different patterns of microbe. Yeah, I was thinking in response to this. Yeah, I was thinking in response to this discussion that there might be actually two reasons to follow up on a hypothesis that your model makes. And one of them is that it makes sense with regard to your background knowledge, but also when it doesn't make sense in a way that's like the most where the interesting stuff happens, because that's where you learn something new. But then you can maybe focus on predictions that are very robust. So if you make variations of your model and all of the models kind of predict the same thing that you didn't expect, then it's still worth following up. Then it's still worth following up on, even if you didn't expect it. So there might be multiple reasons why you would choose something as interesting to follow up, and it's not only because it's aligned with your current knowledge. I think it should be close, though. Like, probably not just, you know, corroborating what you already know or what you expect, but something like different, but somehow related, like a different view on the like a different view on the on the problem. So probably this is you know like a way of accepting even like a different prediction that is different from what you expect. Sorry, no you go ahead. But I would argue that even if it's very different from what you expect, but as long as it's then a very robust prediction on your model, so nothing's just it could be a fluid, but if it's very robust, Could be a flute, but if it's very robust, then it's interesting to check because either you have to update your model or you learn that something is not the way you thought it was. But in both cases, you learn something. So these cases are still interesting to investigate, even though they don't match your current understanding. Well, I was wondering, I mean, that like she showed that physics way of mechanistic modeling, and even in mechanistic modeling, you do. Mechanistic modeling, you do have parameters, multiple parameters you can change, and that will change the simulator. So you build a simulator based on the knowledge of how the data is generated, like for single-cell data analysis. You have a fantastic paper like Splatter. It really, I mean, the mechanism of how the data is generated, they know it very well. So they change all the parameters. You run your model through that. Through that, changing all the parameters, and if it does not work in any situation, then you may be not correct, you have to do something to the unless there are points, I think we're running out of time, but if anybody has a question. I will never. What do you think would be like methods for exploring those what-if scenarios? I mean, I think about synthetic data generation. I mean, I work on this, but it's another model still. But yes, simulation, for instance, it's definitely an interesting framework. But yes, I think that all the knowledge that we are gaining through those interpretable knowledge. That we are gaining through those interpretable models can actually be used for exploring those what-if scenarios and then check if the knowledge that you are acquiring actually makes sense or not. That's the best we can do. What else to do? This is a good time to thank everybody for the active participation. Thank you also Rich and Cynthia for joining online. And Cynthia, for joining on live. Thank you. Okay, thank you. Shall we meet tonight somewhere? Shall we meet tonight somewhere in the bar? Or is this inclusive?