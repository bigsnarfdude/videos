Can they hear us? Okay, okay. So, Brent knows that we're in hot water here. Yep. And they're on the Zoom. Right, but you just want to try to share your screen. You just want to try and screw your screen again. Just go back. They switch it back over. If they switch it back over, then you can see              It's a Newtonian fluid. It could also tell you about the viscosity of the cytoplasm. So the reason we kind of So the reason we kind of hesitate to talk about effective viscosity or viscosity is that we don't really know. In fact, the cytoplasm probably isn't a Newtonian fluid. It has elastic properties to it because it's a very crowded space and there are lots of biopolymers. So we'll just talk about the diffusivity of gems, which is a well-defined thing. So it's basically the variance of those delta x's. To x's, with maybe a scale factor. So we're going to try and spatially resolve this parameter, the diffusivity. And more generally, the approach that we take in my group is to use stochastic models of the motion of something: bacteria, particles, diatoms, and then trying. And then try and connect that stochastic model, the parameters in that stochastic model, to environmental factors that are hidden. Okay, so what kind of data are we dealing with? So I showed you a nice movie at the beginning. That's a turf microscopy video. It's not actually what we're going to use. We're actually going to use 3D videos from a spinning disk microscope. From a spinning disk microscope, and I can say those words and pretend like I know how to use a microscope, but of course, I just know how to use a laptop, right, and a computer in Python. So, this is a max projection. So, we take that 3D video. It's really hard to visualize a 3D video, right? Especially one that's as noisy as this is. Look how much sort of noise there is, and that's because the gems are powered by GFP. Are powered by GFP and they're not very bright, right? And they're very fragile, biologically expressed fluorophores. So we just have to deal with these low signal to noise conditions. That's just part of an inherent part of this data set. So it's ZStack, right, that you're dealing with, and then you've projected it just for visualization, or do you project it for analysis too? Good question. Just for visualization, we're going to. We're going to. So these red spheres that hopefully you can see are that's a representation of our tracking, our estimate of the three-dimensional position of a particle. So I'm overlaying that. That's not, of course, part of the actual video. I'm overlaying that on the top of a max projection just for visualization purposes. Because it's nice to say, oh, is this particle that I've tracked? Is this particle that I've tracked, I get a nice x, y, z number, does it actually correspond to a particle in the video? We like to make sure that these things are working properly, right? So this is, I'm going to use these sort of layered visualizations to try and understand what we're seeing as we move along. So one of the problems that we caught early on, so yes, we can get these particle positions, but XYZ doesn't necessarily tell you. Doesn't necessarily tell you where inside the cell you are. Because remember, the statistics of what we're interested in estimating, the diffusivity, depends on how far between two frames a particle jumps. So I have to connect the dots. I have to take a particle in one frame and connect it to a particle in the next frame. And what if I create these jumps? What if I create these jumps over completely different compartments of the cell? That's going to give me bad data. And it turns out we are actually interested in regions like tips, branches, and these are places in the cell that are prone to get right up next to... So we call these hypha, just these distinct tubes in the cell. So we want to be able to separate our measurements between hypha. But not only that, we want to be able to say, But not only that, we want to be able to say, oh, here are particles that are at the tip of one of these hypha, or these are on a branch point, or how far in terms of the axial distance along this tube are two observations. So we need like a geometry of the cell, right? So that's one of the first things that we had to tackle. So what we did is we took all of our X, Y, E P. All of our x, y, z positions, right? So I did like a time projection. I just put them all in like this point cloud, right? And then we had to segment them, those points, into distinct hypha, distinct tubes. So that's a very hard problem, and the way we ended up having to do it is by hand. And that's one of the first things that Grace figured out how to do. To do, and she worked on this project as a not an undergraduate because she had graduated, but before she started graduate school. So, this is one of the first things that she figured out how to do. So, once we have the points separated out, what we can do is use computational geometry to work out a surface mesh. This is like something you would see in like a video game, right? Maybe like an 80s video game. This is because they figured out how to use polygonal mesh. Game is because they figured out how to use polygonal meshes to represent three-dimensional graphical scenes. So we're using the very same tools. And then, so we have a representation of the surface of the cell with this polygonal mesh. And then we can use skeletonization to get a medial axis curve. And these two things we can basically put our observations in the context. Our observations in the context of a cell. We know where the tip is because we know where the end of the medial axis is. We know where branch points are because it's not shown here, but we actually get a graph with these medial axis curves. And we can take our estimates, right? We're estimating the variance of our delta x's spatially. So we can project that onto the surface. Project that onto the surface, that's what you should see here, or we can project that onto the medial axis curve, and we just get like a spatial profile moving down the central axis, the medial axis of these tubes. Those are the two things we're going to look at. So this was all done with the computational geometry library, which is just a standard open source C library with lots of With lots of really useful sub-modules and packages. Anyway, so I want to talk about the problem of spatially resolving something like diffusivity and the approaches that have been taken in the past, just sort of broadly speaking, with particle tracking. So, this is a completely different data set. I honestly don't even remember what this is. Augur, this is a micro-reality. This is a micro-rheology data set, so it's a 2D thing. But the point is that this is something that we've done in the past with Greg Forrest's group. These particles, they just have their own private little territories. They don't interact, really. Maybe there's one here that kind of gets close to another particle, but I could just estimate the diffusivity of each of these tracks independently, right? And then sort of like sew those diffusiv, like. So those diffusion, like spatially localize those diffusivity estimates and then create like a landscape, right? But that doesn't work for us in these hypha because these gems are small. The smaller a particle is, the faster it diffuses. So they are mixing, right? This is like, you know, it's a crowd in a party. Everybody is just walking all around. Everybody is just walking all around and exploring lots of overlapping territory. So we need a different approach. So what we decided to do is use a kernel regression estimation and maximum likelihood estimation. I already showed you the formula for maximum likelihood estimation of the diffusivity. But remember, it only depends on the delta x's. The delta x's. So I don't take a particle over its entire track and estimate a diffusivity because it's gone halfway down all the way to the end of the tube, the hypha, and back again. I need to spatially localize these. So I just take the delta x's, which they haven't traveled that far, and I pin those down to a spatial location. And then I give each one of those these delta. Of those, these delta x's again, I give each one of those a Gaussian, its own little Gaussian. And so each one gets a Gaussian wherever it happens to be, and sigma is like how far away that Gaussian is going to persist. And then you get a large superposition of these Gaussians, and then that gives you the spatial profile of the diffusivity. So it's like taking my max. So it's like taking my maximum likelihood estimator, which is basically the variance, and just spreading it out with this kind of smooth histogram. A word about how we did the actual, how we localized where the gems are. We get this, like I said, this large point cloud of positions at times. This is based on some earlier work where we Where we developed a convolutional neural network that can pinpoint the centroid position of a particle. So it's a feed-forward network of convolutions that starts with the image data. So this is grayscale integers. Each pixel is an integer, right? The bright ones are larger integers, and the black ones are like zero, right? Like zero, right? And those get fed into these convolutions, and there are non-linearities between these different layers. And you train them, and when you train them, somehow, by the magic of machine learning, they begin to recognize patterns. And we have a very simple pattern, and that is bright blob, right? But it's a difficult problem because we have low. Because we have low. How is this working? Oh, it's already yoda. Okay, I didn't know that. All right. So these are, this is not gems. This is, I think, virus and mucus. It's just a standard example. I show some of the difficulties that you face when doing particle tracking. Low signal to noise, it's much brighter on one side or one corner of the image than the other. Um and uh anyway, so the neural network does a pretty good job of just pinpointing where these particles are. And then there's a task that you have to perform after that, which is get the delta x. We have to connect the dots, go between. Is it giving you a full 3D shell? This is just a 2D video of. Yeah, so we get three-dimensional positions. I'll show you more of the actual You more of the actual visualizations for the gems tracking as we move along. Anyway, the neural network is actually pretty simple as far as the neural networks that are designed to categorize your cat pictures on your phone are probably much more complicated with more layers, but our patterns are very simple, grayscale, bright blobs again. But we do do, I often get the question, do we I often get the question: do we build time into this inference? Like when you watch a video and you let it play, you can see the spots, but if you just stop it, then it's just a sea of noise. So we do what's called a recurrent neural network, which takes temporal information through time. We have these very large videos, 3D videos. 3D videos can be 10 gigabytes, 100 gigabytes, and more. And neural networks take a lot of memory to run. So one of the challenges that we had to do is we had to store this terabytes worth of videos. And we had to deliver those videos to processors, lots of individual processors with their own memory, in order to run the neural network on that data. So we did this with So we did this with this standard map reduce kind of algorithm and cloud is in this case Apache Beam on Google Cloud there. They're designed to do this, right? Take virtually unlimited storage and deliver it to processors. So that's what we used. So we get this pipeline, which involves, again, the particle tracking and these green ones here are The green ones here are hand processing. So we had to segment the localizations. We did that by hand. And then we had to also touch up those surface meshes I showed you. So we get the geometry and the tracking together. That's the point. And then the products are those surface projections and the medial axis projections. So here's some surface projections. Some surface projections overlaid on top of the max projected video with the particles there. So, this is just like a stack, right? A layered stack of results, just so that you can see the different things that are coming together to create these estimates. So, I'm going to come back to this because I think first, it's important to validate what we're doing. Can we trust any of these? Can we trust any of these results? One of the things we do is we look at stuff like this just to see if things make sense with the data and the tracking and the diffusivity estimates. So that's important. But I think what's also important is to do some other kind of validation. And what we like to do is synthetic data where we control the ground truth and we can really investigate the Can really investigate how the uncertainty propagates through the entire pipeline. So, when I say synthetic data, I mean images. I made 3D images. This is a max projection. Notice how it looks a little too perfect, right? Because it's fake, right? I made this. The real hypha curve a little bit, right? Or it's a little bit messier than this. So, what I wanted to test is a really basic question. If I have Question. If I have a region in the hypha that has very low diffusivity, so that it's very viscous, and it's right next to a very high diffusivity region, will my tracking be able to pick up both simultaneously? So this is the medial axis projection. Remember, I told you we could get just curves. And this is the surface projection. And it does. It's able to get that. It's able to get that, but it didn't at first. When we tried this at first, it completely failed at doing this. We could get one or the other. And so we had to actually build in some heavier machinery based on something called the expectation maximization algorithm. So we iteratively created better estimates for ourselves. And I just don't have time to really get into the details on that. On that. Long story short, we can simultaneously resolve low and high diffusivity regions that are next to each other. We underestimate diffusivity if the diffusivity is large. And that's partially because we have boundary effects. And just because tracking is inherently biased to fast-moving particles that move very far. The farther they move, the harder it is to estimate them. To estimate them, this is perfect tracking, fed through the entire pipeline. So, if we did particle tracking perfectly, that's what the answer we would get. So you can kind of compare these and see the uncertainty that just comes from the particle tracking itself. What's the difference between that image and this one? Notice the scale. It's like 0.1 down to 0.02. This is in micron squared. 0.02, this is in microns squared per second. This is 0.02, so this is 10 times less. We wanted to know if we could do this resolution across orders of magnitude in diffusivity. So we can still resolve very small diffusivity differences as well as larger ones. Here's some results. What we find, we were able to quantify. We were able to quantify the heterogeneity that exists within a single cell, which you can see here, these all come from the same cell, and even between different cells. We wanted to see if there are any interesting regions of the cell that have low diffusivity. Remember, our original question was: can we locate any diffusional barriers? Diffusional barriers within the cell. Unfortunately, we didn't get much of a result for diffusional barriers in the interior, but we did see a strong signal for lower diffusion at the growing hyphal tips, where you have a lot of actin dynamics and other things occurring. So, the way that GRACE quantified this is by taking the average. Quantified this is by taking the average diffusivity along the entire hypha and then comparing that as a ratio to the diffusivity of a region of interest like the hyphal tip. And so we see the very strong signal of lower than average diffusivity at the hyphal tips compared to the rest of the hypha. Can I ask a quick question, Jay? This is the one. What about around nuclei? Okay, we got. Okay, we got some unconvincing basically lack of any difference around nuclei. And there's a couple of reasons for this. There may actually be diffusional barriers, but they're too small for us to really pick up with this method. We need to somehow increase the resolution of our spatial resolution, I guess. Resolution, I guess, of diffusivity, and I'll talk about some extensions and things we're thinking about in a bit. But we did find an interesting, the difference is small, 69%, but that small difference is statistically significant. And that's at the SG2 checkpoint, which is kind of interesting, suggesting that you get some increased crowding there. That you get some increased crowding there, but it's also a checkpoint for the cell cycle. That would be probably a nice target to delay or advance the division time if you wanted to, say, control synchrony across the population. We have another quick question in the chat window. This is Joel's question. Can gems be excluded from the regions of interest around the nuclei? Can gems be excluded from the nucleus? Can gems be excluded? Well, they're certainly excluded from the nuclei themselves, and we did consider sort of the volume fraction of organelles. There's been some EM studies looking at that, and we believe that gems should, their small size should be less than any sort of gel pore size, and the organelle. And the organelles in Ashbea are never dense enough to prevent something small like gems from going wherever they need to go. So we're pretty sure that they should be able to access any region of the cell except for inside the nuclei themselves. So just to summarize these results, we're able to quantify and spatially resolve diffusivity within the cytosol. Within the cytosol. We were able to correlate it with cellular structures like the growing tips of the hypha and around nuclei. We used a convolutional neural network to automate the particle tracking. And we used Google Cloud for storage and processing of large video data sets. Okay, so I want to tell you a little bit about some ongoing work. A little bit about some ongoing work that we're doing. So, to address and enhance our ability to spatially resolve properties with this sort of particle tracking data, I've been exploring a different approach, a new idea, and that is based on not tracking particles. Okay, so remember, we have to connect the dots. We need the delta x's. Well, I'm tired of delta x. Well, I'm tired of delta x. I want to do something else because these are very dense with gems. It's very hard to control the density of gems and the signal-to-noise ratio is very low. So what we wanted to, and this is with Max Hirsch, who's an undergrad at Carnegie Mellon, who came out of this like super high school in Chapel Hill, or in North Carolina. Chapel Hill, or in North Carolina, where they apparently all come out as experts at Python. You're getting into your 10 question. Okay, that's fine. So anyway, this is just that same movie I showed you before, but I'm just supposed to say, look how dense it is, right? It's really hard to connect the dots between these, and it is. So instead, what I want to do is I want to count how many particles are in bins. Bins, and then I want to model molecular motion using a stochastic reaction diffusion equation. We saw this in Sam's talk earlier, I think that was yesterday, and also in Tim Elston's talk. He talked about, what did he call it, the spatial Gillespie, right? So the basic question is: can we train a neural network to count and not just count all the particles? Not just count all the particles in the whole image, but to count in like bins, right? So, this is some preliminary results. These aren't published, but here is a very simple training image. And the neural network does seem to still be able to spatially localize, well, this is the neural network output here, right? It does still seem to be able to spatially localize where the particles are. And it counts, but that's not obvious from this image. From this image. So, what Max did is he ran a whole bunch of experiments with differing numbers of particles and looked at just how many particles the neural network counted. And it does a pretty good job, though not perfect. So this is, I think, a great start for being able to extract data from these microscopy videos that we could feed into a reaction stochastic. Into a reaction, a stochastic reaction diffusion model, and then do inference with that model. All right, so another project that's ongoing that Grace introduced yesterday is tracking the position and the lineage of nuclei in living cells. And so, these, I think these are videos we've already seen, but I'll show them again because they're very nice. We have hundreds/slash thousands of these nuclei. Thousands of these nuclei moving in complicated ways, not just Brownian motion, and they're dividing, they're spatially organizing within the cell in fascinating ways. They're doing it in order to grow and to be able to optimize their growth spatially. So there may be different concentrations of food in different places. Concentrations of food in different places. And they're also doing it to, I don't know, remain asynchronous. The synchrony of the divisions has some intrinsic sort of role to play in all of this, but also they maintain a very deliberate spacing, a distribution, and they also maintain a very deliberate number. The number of nuclei to the overall volume of the cell. Now, this is a unicorn video taken by a grad student years ago, and it's really hard to get anything approaching this good. And she doesn't remember how she did it. So we can still use it to. We can still use it to show presentations, and that's great. But they don't like to lay nice and flat like this in 2D. So, Grace has been working hard to get three-dimensional videos of these. And there are all kinds of issues that you have to deal with. Like, look, they're growing over the time scale that we need to actually, because the division time is like 100, what she say, 140 minutes on average. Average. So during the time scale of hours, these cells grow, and you need to be able to sort of aim this three-dimensional region that you're imaging on so that they grow into it and not out of it. Oh yeah, and you have to do this without killing the cell because the laser light that you use to image these things kills the cell. Oh, and it also bleaches the thing that you're trying to look at. The thing that you're trying to look at. So it's a really difficult thing. The way that we're going to approach the tracking, because we want to be able to track these nuclei as they divide, and we want to track the lineages, so that we should be able to track daughters to mothers and so on. In order to get estimates for the phase along the cell cycle, because remember Grace's model. The cell cycle because remember, Grace's model is a curimodal model of phase, coupled phase oscillators. So, we want to be able to connect that to data, and the best way I think for doing that is to come up with good models and then use Bayesian methods. So, we've been developing some work that's been done recently on fully Bayesian multiple particle tracking. So, the multiple particle is important and Is important and the fully Bayesian is important. Those two things together make the problem extremely difficult just to come up with a Bayesian model. Not to mention all of the tools that you would need to actually access the posterior distribution. And there's been some amazing work from, I think this is a father and son team in Perth, Australia. And that's all I have. And I have to take questions. Got time for a couple. If you do the room while the audio switches over So yeah, just a couple of questions first. Can you comment on the regular neural network and how you know in the training I had I think that's usually a common problem is usually when you have like you get these long videos, which you know it's a problem. So something that is potentially a problem for for you. And my second question is about the train, the Rust function that you mentioned for the stochastic model. I just want to learn more about this Rust function that you Okay, so for the first question, um there is a problem with recurrent neural networks um getting long range temporal correlations and Correlations. And we don't. I don't want long-range temporal correlations. I think the temporal correlations we want are very short anyway. Just one frame, really. And so the recurrent neural network already does that quite well. In our training videos, we can make them as long as we want. So we can also limit, like not many people want to limit the temporal correlation, but The temporal correlation, but we didn't have to. If we used something like an LSTM, which is sort of designed to have these long-range correlations, then maybe we would have had to actually limit them. And your second question is the loss function for the neural network? I think for your students, it's the loss function, you know, with this. Oh, you mean for the counting? Yes, yes, yes. Okay. So. Yes, yes, yes. Okay. So are you asking how the the stochastic reaction diffusion model plugs in. Yes. So we're training like a pixel dense classifier and we just use cross entropy to train it. Yeah. And that's just on the image side, right? Yeah. So you've got the high voltage are growing and it looks like the nuclei are kind of getting transported as well. So does that confound, because there's going to be a direct duct delay. So does that confound an estimate of dilution? Oh, that's a quick question. Well, I mean, in theory, though, because we subtract out the mean, so we're basically estimating the variance. So if there is anything out of the mean, So if there is any knockdown flow, then it shouldn't be, it shouldn't affect the estimates. But the flow is the time scale I showed in those last pieces is hours, right? So that's like a very long time. It looks like those nuclei are just trucking at like 60 miles an hour. On the time scale of the Jam's videos, those nuclei are not moving at all. That fluid flow is much slower. Fluid flow is much slower than what we do with the GEMS videos. Yeah, because it may be that there's some spatial heterogeneity to that effective term, which might. Yeah. One of the things we wanted to do was estimate that fluid flow with the gems, but it turns out that's very difficult. The gems are very diffusive, and so you need a lot of signal to be able to. It's basic standard error, right? If the noise is very large, then you need a whole lot of square root of x. You need a whole lot of square root of n to get accurate estimates. So, Ed, you're up next. All right, thanks. Yeah, really, thanks. That's a really, really interesting talk. I'm just sort of thinking out loud, but I'm just wondering, do you think it would be possible to do something almost like an FCS analysis on the discrete GEMS particles just by looking at fluctuations in the fluorescence intensity within small volumes as a way to back out estimates. As a way to back out estimates of the local diffusivity. That would be interesting. I haven't tried it. This is basically what they do with particle tracking ball symmetry. So this is something they do in fluid dynamics with passive tracers. And it's interesting to think about whether we work here. So I'm skeptical because of how noisy. So I'm skeptical because of how noisy, how could that possibly not depend on just the intrinsic signal to noise in the video itself? But I haven't tried it, so I can't say definitively that it wouldn't work. That would be pretty cool to do. But even if you're like combining it with your particle counting neural network, so that you're basically just looking at fluctuations in the number of things, assuming that each. Of things, assuming that each one has equal brightness, wouldn't that to some extent ameliorate the noise, the signal-to-noise problem? Assuming your counting is good. That's a really good point. I guess, in a way, you can say that that's exactly what I'm doing with that neural network and using reaction diffusion master equation. You could sort of think of that as a way of doing exactly what you described, right? Looking at correlations of how they fluctuate through time. correlations of how they fluctuate through time yeah but in that in in in a way that can that can um deal with the intrinsic uh high noise um conditions that we have yep yeah yeah well uh so uh unfortunately we're going to move on we've got a couple of folks that i think are like 1230 us today so we can't go too late um so i think what we'll do is we'll just go ahead and move straight over to Go ahead and move straight over to the virtual. Anybody that needs to stretch their legs in the virtual can to just kind of get us back on time. Apologies to those who didn't ask your question there.