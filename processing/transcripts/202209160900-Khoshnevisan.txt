Thank you, Martha, for your very generous and thoughtful introduction. And thanks, everyone, for being here. I know it's been an exciting, action-packed week. And of course, for everybody who's ever followed a fantastic speaker like Arnaud, I'm put in a position of having to give you a reasonably good talk to end this wonderful conference. End this wonderful conference. I hope I won't fail you. And thanks also to all the organizers for having me. It really is a pleasure to be here. It's been a few years of difficult pandemic-related activities. It's wonderful to at least see that our discipline is alive and very well. I would like to present to you portions of some ongoing work with our fearless leader, Martha Sansoid. Many thanks both to Many thanks both to our respective funding agencies that have made this work possible. The portion that I will present is the completed portion, which is SPDs with additive noise. And maybe sometime in the near future, we will have a story to tell about multiplicative noise. The purpose here is to derive optimal regularity results. Let me see if I can use my curve. Let me see if I can use my cursor. There we go. So to cite a sort of typical example of this type, let's say w dot is spacetime white noise. Hex is a one-dimensional variable, which I think everybody here might know, but at risk of some redundancy, I'll remind you, it just means that it's delta correlated in space, delta correlated in time, centered Gauss. In time, centered Gaussian random distribution. And so let's consider the sort of first SPD, or maybe one of the first SPDs anyone in the field sees, which is just the linear constant coefficient stochastic heat equation with additive noise. This is also called the Edwards-Wilkinson model for a sandpile model. Or a sand file model. And for simplicity, let's say that at time zero, the initial value is zero. And the solution, of course, variation of parameters tells you that the solution is a stochastic convolution with the Gaussian heat kernel. And I think there was some discussion about why Laplace versus half Laplace. So if you use full Laplace, then the Gaussian heat kernel has four instead of two. Any kernel has four instead of two in these two spots. Okay, so that's the solution right here. There's a space-time convolution of the heat kernel with the noise. The noise is the forcing term here. And of course, this is a simple example of a very well-developed theory that goes back to the old work of very important old work of Krilov-Rozovsky and Pardu from the 70s. And Pardu from the 70s, and John Walsh wrote his own exposition of this in a San Fleur course. That's very exciting, and many, many others have contributed to this. The bottom line is that the solution to the stochastic PD, which I've written up here for your convenience, is a random field with continuous sample functions. And it is basically Helder a quarter in time and Helder half in space, possibly with some logarithmic correction term. Some logarithmic correction terms as needed. So, this should be very well known to everybody. And in fact, one can even look more closely at this. This is a very special equation. Just because it's simple, of course, doesn't mean it's not interesting. It has a lot of structure. So, for example, I'll just mention this as an aside. Leon No art proof that if you fix an X variable and watch the evolution. Variable and watch the evolution of the process in time, what you actually see is precisely a fractional Brownian motion of order a quarter plus some other process which is also Gaussian and is almost surely C infinity away from t equals zero and continuous all the way up to t equals zero. And we, for other reasons, we developed the spatial For other reasons, we developed the spatial version of this. So, if you fix a time variable t, not t equals to zero, of course, then as a process in x, what you see is a Brownian motion plus a C infinity Gaussian process. And of course, because we know all sorts of things about regularity of fractional Brownian motion of index a quarter or Brownian motion, which is FPM with index half, of course, you can also deduce results of this type and. Deduce results of this type and other things as well. So, this is meant to be an explanation, a quick explanation of a result of this type. But by optimal regularity, I really mean results of this type, where you try to find sharp conditions that actually ensure that the process is held or continuous locally, either in time or space, or both. So, we will be studying for the purposes of this talk. The purposes of this talk, SPDs that are up here. I don't know if I can highlight it properly. There it is. So you have a heat equation plus forcing, just as before, but forcing here has correlations in the space variable and plus a possible semi-linear term, B of U. I do want to say something about the noise model. I will use the noise model that Dalong has in his exposition of the theory. Position of the theory. So basically, its noise is delta correlated in time. In the space variable, you have correlation gamma, which is not assumed to be a function. So you interpret it the same way as you do the delta of T minus S via convolutions. Gamma is the positive definite tempered measure. So it's a sigma finite measure that's tempered at infinity. That's tempered at infinity, and it's positive. That means Fourier transform is bigger than or equal to zero. And the SPD for your convenience, again, is reminded here, which is just the heat equation plus a semi-linear term plus a noise that's white noise in time and colored in space. This is the most, effectively, the most general colored noise in space you can have. In space, you can have that is spatially homogeneous, except we are assuming that gamma is a measure, not a sign measure, actually. So it's positive. It's a positive distribution. As for initial data, it's just any non-random bounded initial data would work. You can generalize that as well, but this is good enough. So let's say B is also non-random. Also, non-random and Lipschitz continuous. So, this is precisely the kind of SPD that has been the subject of a lot of analysis. And in fact, Balanced theory implies that the SPDE up here, if B is Lipschitz, has a random field solution, which basically means utx is well defined for every T and X non-random as a mild solution to the integral version of the SPVE. And that happens. And that happens under this spectral condition. So, if you take the Fourier transform of gamma, that's the spectral measure of the noise in the x variable, it has to integrate this particular function. And this is optimal in the sense that in the Gaussian case where b is zero, this is necessary as well as sufficient. So, the regularity comes if you ask the next order question. Becomes if you ask the next order question, an important question in many applications of the general theory to qualitative analysis is when is the solution held or continuous? And the answer was, in my view, quite surprising. It came in two papers by Sansale and Sarab. And it's succinctly put as follows. You just take the same condition as the Lung's condition, you just ask for a little more. Just ask for a little more, namely, instead of two, if you put two times anything less than one, then that condition suffices to ensure that the solution is held continuous. There's also information on the sort of Helder exponents, but I just want to concentrate on whether or not the solution is Helder continuous. And so the Stanselet condition is just a slight strengthening of the Delong condition. This is well-posedness, this is Helder continuous. This is held a continuity. This is a sufficient condition. And of course, both theories allow for multiplicative noise models and generalizations thereof. I want to concentrate here on the additive noise case. And to highlight sort of the kind of theory that we would like to show, we want to look at a slightly generalized version of this model. So if you think about the SPD that I showed you. The SPD that I showed you here, for simplicity, let's say B is zero. If you think about this as random, as Brownian motion in random environment, random environment being F dot, you can ask the same question for any generalized random walk model. And of course, this family of random walks, if you close it under weak limits, is exactly the family of Z processes. So the most general random walk in random environment model of that type. Random environment model of that type is the following. You take the generator of a general ID process in Rd, call that L, and solve an SPD of this type. Up here, I can't highlight them, I'm sorry, but there it is. Where F is the same noise model as before. And for simplicity, I've dropped the B. And for simplicity, I'm going to assume that the initial value is zero. So that's the most general continuous time random walk in random environments. Random walk in random environment model of this type. I want to write the solution as I did in the original setting of the Edwards-Wilkinson model. So I need to have transition densities. I don't in general have transition densities. In fact, it's a notoriously open problem to decide precisely when a Levy process has transition densities. There are theorems that describe that going back to many people, Tucker and Many people, Tucker and Hartman and all sorts of famous Fourier analysts, have thought about this. There doesn't seem to be a good answer to this question. And the bottom line is it's not true that every Levy process has a transition density. It does have transition functions, which are measure-value functions of time. And here they are: pt of f is the probability that at time t, x is an f. X is an F. So that's the fundamental solution. This is the fundamental solution for the equation for L. Of course, this is classical. And by a solution, we would like to mean exactly a stochastic convolution, just as we did earlier and everybody else does as well. And H is to remind you that this is the heat equation. Okay, so you have to be a little bit careful because you're taking a random distribution F. Taking a random distribution f, you're multiplying it by measure-valued function p, you're integrating it, you want to get a function h, so you have to be a little bit careful. But this is this is well-known territory in space in the world of Gaussian processes. And in any event, whenever you can do this, that's when you can solve the heat equation. That's the bottom line to get a random field solution. And our question is: I'll talk about when you can do this, but our question. I'll talk about when you can do this, but our question really is: when is the solution Helder continuous? And we want optimal regularity, so best possible conditions to determine when the solution is Helder continuous. So the same question can be asked for, for example, wave equations. It's the same equation as before. You have generator L of LAD process, W is for wave, and you look at a second-order stochastic partial differential equation, you have any reasonable initial value. Have any reasonable initial values here is what we chose to write, and the noise model is as before. And we study dimensions d equals 1, 2, 3 for the reasons that I think you might be familiar with. When L is the Laplace operator, that's when the fundamental solution is a measure. In general, the fundamental solution is not a measure, it is a pseudo-measure. And in any event, And in any event, you want to make sense of an equation of this type or an identity of this type, where phi is the fundamental solution, but it's a pseudo-measure in general. Pseudo-measure means that its Fourier transform in the space variable is a bounded measurable function, is an L infinity. Again, the questions that one wants to ask is: of course, when does this make sense? Wants to ask is, of course, when does this make sense? That means when is the SPD well closed? And the main topic is when is the solution to our SPD held or continuous. As I sort of tried to imply by the title of this talk, we can add nonlinearities to this, Lipschitz continuous nonlinearities to this, with really no big effect. So I will not do that from now on. I'll just look at constant coefficient Gaussian processes. Okay? It's a simpler exposition. It's a simpler exposition, and I don't think you will learn too much more by having the added generalities for the purposes of this talk. Now, the only thing that we need to assume for the analysis that's to come, we need to assume something on L. But part of what we really want to do is have very general statements of the type I'll mention to you in a moment. So the assumptions should be really, really, really mild. And the assumption here is that the underlying Levy process. Here is that the underlying Levy process is what we call genuinely d-dimensional. And really, by that, we mean that if you look at the characteristic exponent psi, so as a quick reminder, if you have a Levy process, the Lady-Kinchin formula says that the law of the whole process is determined by the law at any given time t, t positive, and is given by a formula of this type. The characteristic function of this process at time t must be of the form e to the minus t. The form e to the minus t times of psi, where psi has to have a form like this: it's a rift term plus a possibly Gaussian, a Brownian motion with possibly correlated coordinates, and a pure jump term given by a form explicitly like this, where nu is what's called a Levy measure. Bottom line is has to have a simple analytic property which ensures that of any set F is actually the expected. Is actually the expectation of the number of jumps that the Levy process makes, jumps of size in F by time one. Okay. And the assumption that we're making is this assumption that the only zero that psi has is a zero. So if you look at the psi inverse of the set zero, I didn't put the set for the notation here. That's zero. So the only place where psi is zero is zero. That's a real assumption, of course. Real assumption, of course. But why would you make such an assumption? I want to assume that the underlying process is not degenerate. So let's just concentrate for a moment on this condition. Suppose this condition fails. Okay, that means there is a non-zero number psi, vector psi, such that the characteristic exponent vanishes at psi. So let's take the Levikinshin formula. That's this here. Levikinshin formula, that's this here, and calculate its real part. The real part is this. Please notice the sum of two non-negative terms, this one and this one. So if psi vanishes at this point psi, so does its real part. That means both of these terms must vanish. So of course, if this vanishes, then A doesn't have. Of course, if this vanishes, then A doesn't have full rank, which means that if there is a Brownian component in your Levy process, it's at most D minus one dimensional. It lives on a D minus one dimensional sub hyperplane of R D. So that means it's degenerate. And the jump must likely also be degenerate. If this is zero, that means new concentrates on a very special set of discrete points, namely all points of this form. And in fact, if all those things are zero, you'll see this is also zero. This already we've seen is zero, so this must also be zero. So the drifter must also be very special. The drifter must be exactly parallel, orthogonal to the point where this psi vanishes. And so it's a really, really degenerate leg process. It has degenerate discrete jumps. Jumps, its continuous part is degenerate and lives on it at most R D minus one dimensional part of the subspace of the R D. And if there is a drift, it's in a very specific direction. And so we want to avoid such degeneracies. This is a generalization. So Levy processes include continuous time random blocks. Continuous time random walks. In the case where you have a continuous time random walk, nu is a very special form and these vanish. And in that case, this actually is exactly what Spitzer calls genuine media dimensional random walks in his book and the context of discrete walks. Okay, so we assume that whatever the underlying Lady motion is random walk model is for this SPDE, it's just. For this SPDE, it's just non-degenerate. That's the only assumption. And so we have two equations: we have a heat equation where L is the generator of a genuinely d-dimensional Levy process. And the wave equation, again, for the same L. And the basic theorem, existence regularity theorem, this is the analog of DeLong's result, and there's a very general result about these things. And there's a very general result about these things for abelian groups and so forth. Is that the equation, both equations are well posed if and only if there's a spectral condition just like Dalong's. Before we had norm psi squared being here, and now you have the real part of the characteristic function of psi. Please note that the real part of the characteristic function of psi is non-negative. I explained to you why a few moments ago. So this is one plus. So, this is a one plus a non-negative number. And it's really saying something about the asymptotics of the tail behavior of the spectral measure. The tail of the spectral measure cannot be too large, is what this says. And of course, you all know that if the Fourier transform is not too big near infinity, then the measure near zero is smooth. So, this is a smoothness condition on the correlation. All right. There is a connection to replica symmetry, but polymer measures, but I don't think we want to talk about that. So let's move on. And so our theorem really succinctly put, specialized to these constant coefficient equations, is the following. Fix an x variable, and first of all, Variable. And first of all, the first assertion is you have Helder continuity in T if and only for one X, if and only you have it, if and only if you have it for all X. And then an analytic condition, a spectral condition, is that there exists some number A between 0 and 1 that integrates this spectral measure in this way. So please remember: if A equals 0, that's just well posedness. That's this statement. Okay, so if you strengthen, Okay, so if you strengthen it a little bit along the lines of the original paper by Sansole and Sara, exactly in this way, then that's the optimal condition for held irregularity in time. Please note that upstairs you have the modulus of psi, downstairs you have the real part of psi, and they're not the same thing in general. And so you can actually use this to produce all sorts of To produce all sorts of strange examples. By this, I mean you can produce asymmetric Levy processes that have all sorts of strange examples. And I will invite you to look at these examples in our paper, which is available in Excel. You can also ask about Helder continuity in space and Helder continuity in space. And this is one of the nice things that you observe when you put in more general random walk models rather than the Laplace operator, which is self-hygient. Operator, which is self-hygient, what you see is actually a different condition. So, in space, Helder continuity is ensured if you integrate in a similar way, you strengthen the well-posedness by putting in a little more, but the little more is now in terms of the integration variable raised to an arbitrary power to B. B can be as small as you like, but still has to be positive. Can I ask a question? Of course. For the wave equation, did you look at the similar result for the velocity variable? Or is it not really of interest? I don't know if it's not of interest. We didn't look at it. Okay, thank you. And maybe another question. So I guess you know what the earlier exponents are. Herder exponents are in each case. We know the optimal Helder exponents. Yeah. So if you look at the largest A that makes this finite and the largest B that makes this finite, those are two indices. They're kind of like Blumenthal-Guittor indices. And they determine the Helder exponents, the optimal Helder exponents. Yes. But maybe so what's interesting is that the underlying levy. So, what's interesting is that the underlying Le Dipress is essentially completely general. It's just a genuinely d-dimensional equation. So, you're not adding a d-dimensional noise to a d-one-dimensional object. That's all it says. Barring that, this is a completely general statement. One of the amusing things you have to recall, this is a fact that goes back to Bochner. It might be older, maybe Kinchin. It's from 30s in any event. It is in Bochner's book on stochastic processes. And you can derive it on your own from. And you can derive it on your own from the Levy-Kinchin formula. It says that the characteristic exponent of a Levy process can never grow faster than that of Brownian motion. Okay. If you remember, Fourier transform at infinity means smoothness of the measure at zero. So what this is telling you is that the smoothest Kolmogorov equations you can get is Laplacian, okay, which is a very natural. Okay, which is a very natural statement. And so now let's observe this in light of these two conditions. What you immediately see is if this integral is finite for some b because of this fact, then this integral is finite for a equals b, which means that quite generally, without hardly any conditions other than degeneracy, really, whenever your solution is continuous. Whenever your solution is continuous in X, it's continuous in T, but not the other way around, which is reminiscent of what happens for local times of Markov processes. Local times of Markov processes are always continuous in T, but not always continuous in X. And this is not a random connection I'm pointing out. There really is a connection between solutions to equations of the type I've mentioned and local times of Markov processes. It might vaguely remind you of Dinkin's isomorphism theorem. There really are. Isomorphism theorem. There really are connections here. In any event, the statement that comes from this is quite generally: Helder continuity in space always implies Helder continuity in time. So if you want Helder continuous solutions, you always have to make sure you're studying objects that are continuous in X. And these statements, actually, the only thing I just want to say about additive. The only thing I just want to say about additive noise models is if you have, for example, Lipschitz continuous nonlinearities added, everything here is true. This theorem remains, except instead of if and only if you have to say if, because in general, you can do strange things with V. But these are the optimal Helder conditions. So, how do you prove results like this? Well, we've all proven something is Helder continuous at some point in our careers. Continuous at some point in our careers, I think. I think most people here have dealt with continuous parameter stochastic processes. And I imagine pretty much all of us use some form of Kolmogorov continuity condition. So here's one that I like. There are many different forms, all pretty much equivalent, all use the same idea, which is chaining. But here's a version that I like. So Slutsky wrote a paper in. So Slutsky wrote a paper in 1937. This is in German, where he ascribes this to Kolmogorov. And I think we all call it Kolmogorov continuity condition, at least because of that. Kolmogorov himself never published it. Someone over a coffee told me once he didn't think it was important enough to publish this. Levy independently discovered the method in a series of really incredible works that ultimately led to his construction of Levy's multi-parameter varying motion. Multi-parameter varying motion. And this is the Chenin method. Chensov wrote the multi-parameter version very similar to what I'm showing you here, in a very similar context of Gaussian processes. Dudley realized, oh, this is a very general idea that has to do with metric entropy and sizes of compact sets in Banach spaces. There is a version of this, again, using the same idea, but particularly. But particularly useful in some applications that was developed by Garcia Roden McRumsey. And there are many, many, really, maybe too many results of this type in the literature. It's not too many. They all are useful in different contexts. Here's a version that I find very useful in this context. If you have a stochastic process, an abstract process, let's say it's separable if you're really worried about this kind of thing. But in any case, if you have a real value stochastic process. have a real value stochastic process it has some compact parameter space t capital t and for simplicity let's say it's a subset of rn and now suppose you can find numbers between zero and one not including zero alpha one and so forth and a number k bigger than the sum of their inverses such that the following holds so what i'm really telling you is that the stochastic process x is a mapping X is a mapping from t to the probability sort of the Lebesgue space LK of omega. This means there is a constant that doesn't depend on anything interesting. So suppose this, this is the distance between T and S, because alphas are between 0 and 1. So what this is telling you is view Rn indexed with this distance function. So that's a metric space. Distance function. So that's a metric space. And suppose your process is a quasi-continuous embedding of that metric space into LK. Okay, that's actually what all of this results of this type are saying. Suppose your process is a continuous mapping from a suitable version of one metric space to another one. Then, in fact, almost surely you can make sure this process. You can make sure this process is held or continuous, provided, in this case, that k is bigger than the sum of the inverse of the alphas. And of course, Kolmogorov's theorem in this way also gives you an estimate for the Helder continuity index of your process for the almost sure. It says that, okay, so you can create a new metric, new distance function. Here it is. Any Q in this range works. And almost surely that is actually a Helder. Surely, that is actually a Helder. With respect to that distance, you have it almost surely a Helder continuous process. So you give up a little bit, but now you have almost sure Helder continued. There are many, many variations on this, of course. This is the one that I'd like to point your attention to. And for those who are interested in optimal regularity, I don't know if everybody knows this, but there's a beautiful paper by Molly Hahn and Michael Klass, and there are non-trivial extensions of it by. Non-trivial extensions of it by Ono and Ibragimov and possibly others that shows this really is the best you can do. We're not all using Kolmogorov continuity theorem because we don't know what else to do. This really is the best you can do in the sense that if you have an inequality of this type that holds for a large family for the same constants for a large family of precesses that's optimal, meaning that if the converse holds for enough processes, Converse holds for enough processes, then some of these processes will not be held continuous. Okay, so it really is a very attractive phrasing. In any event, what it says is that bottom line is this is the only generic way to prove something is Helder continuous. Now, the challenge for us is that sometimes That sometimes it's not so easy to find these alphas. And that's the case here. If you can find such an alpha, you're done. And so the question is, how do you find such alphas? In very concrete settings, one can just try to calculate or estimate objects. Here, we don't have enough structure to do that. And so, actually, that is the challenge. So, I want to show you the method that we use, and I'll show you this. And I'll show you this method not for SPDs, which is multi-parameter and you have all sorts of other things running around. I'll just show it to you for one-parameter stochastic processes. This is the method of finding what the right exponents alphas are in a one-parameter process setting. Okay, it's an idea that people have used all over the place. It's related already to Garcia's lemma, but also. But also to ideas that people use in the theory of fractals and others, regular variation and so forth. Unfortunately, I'm stating it in this way because I looked everywhere in, for example, books on those topics. I couldn't find the exact statement that we need. But the idea certainly exists in some ability theory and so forth. So here is the problem stated abstractly for processes with one problem. Abstractly for processes with one parameter. Suppose you have a real-value stochastic process. And what you really want is you really want an upper bound for an object of this type, right? That's Kolmogorov continuity theorem. For simplicity, I'm assuming T and S are one-dimensional. So let's say they're between zero and one. This can be easily generalized. So getting a type of bound that we want, Kolmogorov continuity theorem type bound, is equivalent to getting. I found is equivalent to getting a bound for g of r. Okay, so I've just boosted the object by putting a supremum here. So if I want to show that this is less than constant times t minus s to alpha, that's equivalent to saying that g of r is less than constant times r to the alpha. Right? Because the function r to the alpha is increasing. And let's assume that g is a finite finite function, so it's a real, real-valued function, and so everything is well-defined, measurable, and so forth. Okay, great. So here's a very simple lemma. This is a summability theorem or summability statement. Suppose G integrates a singular kernel of the type r to the one plus a. But I mean, if you divide by r to the one plus a, multiply by g, the integral locally is finite for some a, then in fact, you have the kind of inequality that you want, namely g of r is less than constant times r to the a for that a. And this is a very simple fact. And I love proving simple facts in talks because I can. So here's a quick proof. It's a Calburian proof, maybe Abelian proof or factal proof. So, this is the condition. G is an increasing function, integrates this singular kernel. So, you write it by summing it along e to the n instead. So, that's equivalent to this being finite. Of course, if the sum is finite, the terms must vanish. So, that says that this goes to zero. So, now take a look. So now take a point s between you know e to the minus n and e to the minus n minus one. By monotonicity, g of s over s to the a is comparable basically to a constant times this. That goes to zero. It's locally bounded. So this is bounded by some constant. That's the theorem. G of s is less than constant times s to the a. Okay. So again, the point is if you want to prove helder continuity, you need to find the constant. Helder continuity, you need to find the constant. This is telling you one way to find the constant. If you can integrate this object against the singular kernel, that does the job. Of course, finding integrals can be a smoother operation than finding point-wise estimates. So this can be helpful, and it is actually in the context of SPDs. The other nice thing about this is that it has a converse. Before I mention that, let me just Before I mention that, let me just summarize that. So, if you can, this is your G of R. If you can integrate a singular kernel of the type R to the power 1 plus A for some A between 1 over K and 1, then your Helder continues. So, why all of a sudden I put 1 over K and not 0? Because Kolmogorov's continuity condition has a requirement on how big K should be. On how big k should be before you can get continuity. So if this is finite, automatically you see that the integral, the expectation here, the LK norm, is less than r to the a. But k has to be sufficiently large for that condition to imply continuity. Okay, so you have to be able to find not any positive a, but a bigger than 1 over k that integrates this. So that's one condition. That's a sufficient. So that's one condition. That's a sufficient condition for one parameter process to be continuous. And now let's, so the nice thing about integral conditions is you can try to reverse them. So here's one way to reverse it. Suppose you have a function f, this is basically g as before, that fails to integrate a singular kernel of the type r to the one plus a. Then F cannot be bounded by C times R to the B for any B bigger than A. Okay? That's the statement. So this is a converse to getting a Kolmogorov type bound. And the proof of this one is also simple. So allow me to show you that proof. So first of all, without loss of generality, you can assume Without loss of generality, you can assume that f is increasing. Otherwise, you do this trick that I mentioned, put the running maximum of f, it won't change anything. So now f is increasing. And now you reverse the previous argument. If f fails to integrate a singular kernel, then you have some mobility condition. That sum along e to the n must fail to converge. Now, I claim that if this happens, then f must be bigger than e to the minus n b for any b bigger than a. And that's easy to see. Suppose that's false. Okay, so for infinitely many n's, not for all n, but for infinitely many. Suppose that's false. That means eventually this inequality goes the other way around. Well, if it did, you could just bound f of e to the minus n in this way. In this way, B is bigger than A, so this is finite. That actually contradicts this. This should not be e to the minus n A. This should be e to the minus n. My apologies. It's f of e to the minus n. We're just integrating along between these exponential blocks. Exponential blocks. So the point is that we found a slightly easier to verify condition for Kolmoburov continuity theorem. Namely, instead of pointwise conditions, we give an integral condition, and that can be reversed. So it translates to the following: when you actually plug in. The following when you actually plug in instead of f this object that I showed you, namely, if you have an energy integral of this type that's infinite, then the process is not held or continuous of order beta for any beta bigger than in LK, in LK, not almost surely. Okay, so these are sort of very simple. So it's very simple lemmas that show you when you can and when you cannot verify the conditions of Kolmogorov's continuity theorem. But of course, Kolmogorov continuity theorem is not a necessary and sufficient condition for Helder continuity. It just tells you you can't use Kolmogorov continuity conditions sometimes. So how do you turn that around to show optimality of almost sure Helder continuity? Well, here's Held a continuity. Well, here's the next stage. There is this thing called Paley-Ziegman inequality. And I can't help but say this. If you actually look at the Paley-Ziegman paper from 1932, this is called Lemma Gamma, not Lemma 1, 2, or 3, but Gamma, Alpha, Beta, Gamma. I don't know why, but they thought it was a good idea to name their lemmas alpha, beta, gamma, delta, etc. So this is lemma gamma of Paley-Ziegman inequality 19. Of Paley-Ziegman inequality 1932, they discovered this in the context of boundary behavior of harmonic functions, but it's found uses all over the place. So the lemma says it's just Cauchy-Schwartz inequality, but it's a very useful version of Cauchy-Schwartz inequality. Take a number between zero and one F. Here I've chosen a half. Okay? And you ask yourself, what's the probability that a positive random variable, here this is the positive random variable, is bigger than, say, it's L1 norm times a half? And the statement is you. And the statement is: you can always bound that probability from below by the L1 norm squared divided by half squared, that's a fourth, divided by L2 norm squared. So that's the Paley-Ziegman inequality. Okay, so if you could prove, for example, that the L2 norms is controlled by the L1 norm times a constant, then Then the probability that you're bigger than half of the L1 norm is bounded below by a constant that doesn't depend on s or t. If x is Gaussian, that will generically happen. In fact, if x is a Gaussian process, it doesn't matter what kind of Gaussian process it is, the right-hand side is 1 over 2 pi. It's just saying that the expectation of a Gaussian process. Expectation of a Gaussian process squared is controlled by an explicit constant times its first absolute moment, quantity to the power two. That's all it says. It's actually equal to that. And so if you have a Gaussian process, then there's always a non-trivial probability, a universal probability lower bound for being bigger than half of your L1. There's the statement. There's the statement. I won't embarrass myself by telling you what CK is, but we have calculated it. And it's 1 over 2π, this quantity, when k is 2. And then the other statement that you will need to get almost sure optimality of regular. So this shows you that under some settings, there's some chance that you probably aren't looking like you held a continuous process. Process. If this L1 norm cannot be written, the previous lemma is a condition that does that, cannot be written as T minus S to any power, then there's a like probability one over two pi lower bound that you probably are not looking like a Helder continuous process. But how do you make that into a statement about Helder continuity? Well, there's zero one laws for Gaussian processes. Okay? In this context, there's a zero one law due to. Text, there's the 0-1 law due originally to Kalyanpur, and then you need a version of it due to Kambanis and Rajput, which says that these kinds of processes here, if we have a Gaussian process XT here in this example, either it is Helder continuous or it is not, with both statements happen with probability one. Either almost surely it's Helder continuous or it's almost surely not Helder continuous. So actually put them together with So, actually, put them together with elementary properties of measures. What you'll find is because there will be a probability estimate at least 1 over 2π that you're not held or continuous, then in fact, you're not held or continuous almost surely. So, this gives you a flavor of the optimal result for SPDs that I mentioned. Of course, the challenge then will be then there are still computational challenges. You have to estimate LK norms and verify energy inequalities and so forth. Energy inequalities and so forth. And I think I will spare you those calculations. Thank you for your indulgence. Thank you very much, Davar. There's a clear talk, so it's time for questions. So, yeah, Sami raise his hand. So, Sami, please. I was clapping again. Clapping again, but all right, but never mind, I can ask a question. I was wondering: so, like, for instance, in Roughpath, we use those modulus of roughness, or you have also more precise quantifications of regularities in more classical settings. Can you say something about that in your context? Context? Well, so of course, I've limited my exposition to Gaussian processes here, but nothing here is limited to Gaussian processes. And if you do bother to look at the paper, the objects there are not Gaussian processes only. So in the non-Gaussian case, I would really hesitate to say one can do the kinds of things that I think you have in mind. In the Gaussian case, maybe because you have aprior. Because you have a priori information about the tail of any Gaussian process. The only thing that I do want to warn against is that in the examples that you have in mind, I think these LK norms, I should go back if you don't mind my going back. And this is sort of a subtle point here, but it is important. Think about this in the case where n equals one. So you have a one-parameter process. Once. You have a one-parameter process. This is t minus s to alpha. In the sorts of examples that I think you have in mind, there's a sharp alpha for which this inequality holds. For example, if x is one-dimensional Browning motion, alpha is a half. It's exactly a half. In the SPD setting, this method doesn't produce exactly a half. Because, so this is subtle, but since you asked, I'll bore you with this. Because here, all you can do is produce. Here, all you can do is produce an arbitrary number close to a half to get the lower bound. Okay. Does that make sense? Yes, yes. So if you want further refinements than Helder continuity, you know, with all sorts of logs and log logs and log log logs and things like that, you won't be able to do that here because you don't really know precisely how big. So in the Gaussian case, I know Sannya of course knows this, but I'll mention. I know Sami, of course, knows this, but I'll mention anyway. In the Gaussian case, a very important object is this object when k equals to 2. What I'm telling you is, you can't exactly calculate this object order, even to the correct order of magnitude in the general setting that we're in. This is a method that tells you what the optimal alphas can be, but it doesn't produce the optimal alphas. Does that make sense what I'm saying? And so it would be impossible to get. It would be impossible to get the kinds of refinements that you have in mind. Of course, if you have concrete SPDEs where you can calculate things, there are many examples where you can get the sharp alpha. And then probably in the Gaussian case, you can use, I mean, we know a lot about Gaussian processes. So you can probably do the kind of analysis that you're asking about. Okay. Yeah. Thank you. So more questions So maybe I should end with a 0-1 law. This means either the talk was clear or everyone's sleeping. So either thank you very much for your indulgence or my apologies. I think I'll end on this note. 