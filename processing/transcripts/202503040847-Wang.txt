Thanks so much. So, this tutorial, once again, it is not about the type of matching for this workshop, but this is another type of matching problem. So first I would like to thank the invitation to the organizer and for their wonderful work to putting this workshop together. Putting this workshop together. So, some of the work I'm going to talk about is from my own group, and that is joint work with two of my former students and my one group of collaborators Wayna. So, in this tutorial, I'm going to talk about this graph alignment problem. So, there are many different ways to motivate this graph alignment problem. It comes naturally in Naturally, in many different applications. So, the first application is coming from the need to understand a privacy concerning social network called social network de-anonymization. So, nowadays, everyone uses all kinds of different social networks, and people probably sign in more than one social network, like Facebook and New Phoenix and so forth. So, now, suppose Facebook wants to release some data set for research purposes, and before they release. Research purpose. And before they release it, they anonymize user identity by replacing user by a random number. And they hope this will be enough to protect user identity. However, there might be some issue with that because of the availability of some highly correlated publicly available social network like LinkedIn. And people could potentially identify the user identity in the anonymized data set. Identity in the anonymized data set by comparing it to the linking network. And in fact, there is an infamous data bridge instance back in 2006. There was a famous Netflix prize data set released to assist the research recommendations at Wisconsin. But shortly after that, people demonstrate it is possible to de-anonymize the users in the Netflix prize. In the Netflix prize data set using the publicly available IMDB information. And clearly, to understand when is the de-anonymization possible is important to protect users' privacy. So that's one possible way where alignment problem arises naturally. And here there can be a different application. So in biomedical imaging, oftentimes you get multiple different You get multiple different MRI images of the same object from different angles. The question is: can you synthesize all those images and combine information to help doctor maybe do the diagnosis? And clearly, if you are able to align different parts in these two images, that will help you combine the information. And also, in biology, it is known that proteins with similar structures. Proteins with similar structure might have similar functions across different species. So, this could be helpful because if you want to do experiment on humans, that's difficult. But you could maybe do it on mouse, a different species, and understand the function of proteins there. And hopefully, this will uncover relations and transport biological information or knowledges between different species. Right? With all this motivation, With all this motivation, I'm going to tell you the mathematical formulation of the graph alignment problem. And after that, I will talk about the information theoretic limit. I will define what I mean by that for a few different models. And after that, I will talk about polynomial time algorithms for this problem. And I will end with a discussion of open problems. So it turns out this graph alignment problem can be formulated. Problem can be formulated as a special case of the well-known quadratic assignment problem. So, suppose A and B are the two adjacency matrices, are the adjacency matrices of the two graphs. The quadratic assignment problem is trying to find a permutation that maximizes the sum of the product Aij and the permuted version of B in the pi i and pi j entry. So, this problem wants to recover. This problem wants to recover the true permutation or the correspondence reverses in the worst case, and this is well known to be an empty heart problem. However, in the past 10 years, there has been some new interest in this problem to consider in the average case. So people are asking what if the typical practical networks are not always in the worst case. Can we solve the problem for more? Can we solve the problem for most of the typical practical graphs? And also, can we relax this guarantee a little bit to allow a small but vanishing fraction of error? So, under those new changes, what will happen to this problem? So, in order to answer those questions, we will first need to define what we mean by typical. So, that means we need to give a random graph model. So, assigned properly. So, assign the probability to the graph, and so that you can say what is a typical graph. And also, this new question will correspond to a new performance metric. And this problem under these two variations is what is referred to the graph alignment problem. There are other names in the literature, and here is my tiny connection to the matching workshop. But if you see all those names, it refers to this. It refers to this version of the average case of this problem. So let me start by introducing you the common, the most common random graph model people consider for this problem. So this is the so-called correlated earlier graph pair. So the problem starts with a base graph, and you can think of this as the two friendship connection among people. And from there, and we will. And from there, and we assume this is an early training graph, which means every edge is generated by ID for node P. And then we do a sub-sampling to create the first graph. So you can think of this as the Facebook network. That is almost a reflection of the true friendship connection, but potentially missing some edges here and there, you forgot to connect some thread. So here, the subsampling, we are assuming every edge is preserved with probability S. With probability s and erased with probability 1 minus s. Okay, that's the first graph. And then we do another independent subsampling. So this will be your linking network. So because they are coming from the same base graph, you can imagine these two are correlated. And after that, we use a random permutation on the vertex label to model the anonymization process. So we just permute vertex for example. A permute vertex, for example, 1 to 2, and 3 to 4, and so on and so forth, and call this graph as G2 prime. So G prime is the permuted version of the graph. And the question is, given this two graphs, G1 and the permuted G2 prime, can you come up with an estimator that recovers the vertex correspondence between the two graphs? And we say exact alignment is achievable if you can find such. If you can find such an estimator that guarantees the probability their equal goes to 1 as n goes to 0. And what people mean by achievability region would be to figure out the set of parameters n, p, and s so that this exact alignment is achievable. So people study this question under two different computational Computational constraint. So the information theoretic limit would refer to there is no computational constraint. You can use as much computation power as you want. What is the corresponding feasible region for the parameters? And the polynomial time feasible region would mean you put a constraint on polynomial time Elvison for your estimator and what would be the corresponding feasible. And converse would be, it would be. And converse would mean the set of parameters MPS so that no algorithm can achieve exact velocities. So, problem setup is clear. In the literature, there are also other criterias considered. In order to introduce that, let me define this quantity called the overlap between your estimator and the Q permutation, which is nothing but the fraction of correctly. Fraction of correctly aligned verses. And under this definition, exact alignment would mean to guarantee overlap equals one with high probability. And almost exact alignment would mean overlap is one minus a vanishing fraction with high probability. And there is also partial alignment because in some applications even aligning part of the verses could lead to some privacy. Could lead to some privacy issues. So, people want to understand for some constant alpha larger than zero, when can you guarantee the overlap is larger than or equal to alpha with high W D. But for the purpose of this talk, we're going to focus on exact alignment. Okay, so let me start by telling you the information limit. I will start from the simple. Start from the simplest version, which is when s is equal to 1. So remember, s is the sub-sampling rate. So s equals 1 means there's no sub-sampling. In that case, the two graphs g1 and v2 are just identical to the base graph G. And G2 prime is a permuted version of G, which means it is isomorphic to G1. And for simplicity, I'm going to assume P is less than equal to half. And in this case, there is a Case there is a very nice phase transition in recovering the correspondence. So if this quantity n times p is larger than log n plus any divergent term, so this means a n that is going to infinity, and this can be any divergent sequence, then there exists an algorithm that achieves exact alignment. And not only so, this algorithm can be a polynomial type algorithm. Can be a polynomial type algorithm. But if only opposite mp is less than log n minus some divergent term, any divergent term, then no algorithm can recover exactly into this type of. So a few remarks here. So here there is a sharp transition, unlike in the worst case scenario. Now all of a sudden, so if you plot, this is the probability of miss probability of error that you're Probability of error that your recovery permutation is different from true permutation. So, this probability of error for very small p, no matter how smart the algorithm is, the error will go to 1. But as soon as p is a little bit larger than log n over n, you can design an algorithm that leads to vanishing probability of error. And so, my background is in information theory. So, as soon as you see some of the phase transition phenomenon, you get very happy. Phenomenon, you get very happy, and this reminds you of the channel capacity, and you want to figure out where exactly is the phase transition happening. Is this the same as when there's a giant single giant component? Exactly, so it coincides with the threshold for connectivity in the, if you are familiar with the random graph theory. So, when p goes a little bit larger than log n over n, all of a sudden the graph. Along n over n, all of a sudden the graph becomes connected. In fact, that's exactly the reason why below it it's not possible because below it, all of a sudden you will have a huge number of isolated vertex. And no algorithm, because swapping to isolated vertex will not make a difference in the graph. So no algorithm can tell the true correspondence in that case. And here. And here, I want to point out a key quantity here, which will be helpful for our later discussion. So, see this left-hand side, this MP, what is that? That is roughly the average degree in this graph G, in this early frame graph. And this quantity will be turns out to be important for figuring out the phase transition point. Okay? Okay, so that's the simplest case, what people call random graph isomorphism problem. And now, how about the general case where s is not exactly one? And in this case, the two graphs are not identical but highly correlated. And in this case, the information theoretic limit becomes when instead of having MP, we have MP as well. Square. So if it is larger than 1 plus some positive constant epsilon times log n, then you can show there exists an algorithm that achieves high exact alignment. And if mbs square is less than 1 minus epsilon times log n, then no algorithm can achieve exact alignment. So similarly, there is a phase transition phenomenon happening. And now instead of having mp, this quantity mp as well, what is that? Square, what is that? That, if you think about it, we have two independent subsampling. So, this is p times s square is the probability of having an edge both in g1 and g2, right? And n times that will be the average degree in the intersection graph. Oh, sorry, I think this is typoso. So, it should be g1 intersect g2, the unpermuted version. Okay. And similarly, this. And similarly, this is the threshold for connectivity in the intersection graph. One small remark here is you might notice there is a small difference between this expression and the previous expression. So before it was any diverging term, and here this divergent term has to be of order larger than some constant times log n. So there is still a small room for improvement. And if you want to get to any diverging term, To any diverging term, people show this is possible if you get add some further condition to restrict the graph in the so-called sparse region. But in general, you would need this diversion to be a little faster. So that is a small room for a small gap in this general correlated order strain graph alignment problem. So if you're interested in polynomial time algorithms, how would you Oh, that's a good point. So, here, as you notice, before, even a polynomial time algorithm can achieve that region. And here, I put a parenthesis polynomial time. Right now, there is a conjectured gap between the information theoretic limit shown here and the best known visible region by polymer. Visible region by polynomial time algorithm. I will talk about that in the second part of my talk. Alright, so after this, so this is essentially a summary of the information theoretic limit for the vanilla graph alignment problem. And when I started uh working on this problem with Swena, we are thinking um what do people do in practice? People do in practice. So, in practice, it turns out people exploit much more than just the edge information between users. They kind of cheat and bring additional information to help the alignment. So, what we motivate this attributed graph alignment problem is sometimes the graph is just symmetric in some way, and there is no way to tell. And there is no way to tell, for example, here Alex and David are in completely symmetric position, and this reverses 1, 3, 4 are also in symmetric position. So there's no way you can tell, should I map Alex to 1 or 4 and so on? But in this case, people often just look up the internet and get additional information. So for example, in social network, it's very easy to get people's affiliation and educational background. So for example, Educational background. So, for example, Alex works in Google and Bob worked in LinkedIn and so on and so forth. And with those additional information, now it is possible to align the two graphs. So we model this additional attribute information as additional vertices, and we put an edge between the user and attribute if this user has the corresponding attributes. Attributes. And here, a natural question to ask is: how much benefit can those vertex attributes bring? And that is the question we want to answer when we started this problem. So in order to understand this, we extend the Erich Reiner graph pair model into the attributed model. Now we have three additional parameters. m is the number of additional attributes. And we assume a edge is generated between a unit. A edge is generated between a user and attribute with probability PA, and also independent of each other. And that's how the base graph is generated. And with the base graph, once again, we will do two independent sub-sampling. So each user-user edge is sub-sampled with probability S and each user attribute edge is subsampled with probability SA. We do one subsampling to get one. One sub-sampling to get G1 and another independent sub-sampling to get G2. And then to model the anonymization process, we put a random permutation on the user part. So notice here the attribute part will not be permuted because this is trying to model the public information that is already aligned. So only permutation applies to the user part, and that gives me the G2 prime. And the same question with observation of G1 and G2 prime. Of G1 and G2 prime, can you find an estimator that recovers the true permutation with probability going to one? Good question. Attribute edges, they never vanish with properties of one. Would that become the problem much easier? Or the attributes. Like the edges between the initial vertices of the attributes, if those edges they never draw in the subsamplo? They never oh also meaning as A equals what? Oh, also meaning S A equals one. S A is equal to one. Yeah. If P A times S A equals one, that means every edge is connected. So you probably want initially it's... Oh, I see. So there is a stereotype at the first place between the two subs. Right, right. So P A is nothing but S A is equal to 1. Right, right. So if that is the case, well, it also depends on how large P is. How large p is. So if p is smaller, then it may not be very helpful. But if p is large, then essentially, so we will later come to that p quantity. So it's the product of them. It's the m times p times s squared, the whole thing, that determines how strong the information from attribute is. So here the turbility and converse are defined similarly. Are defined similarly. The only difference is now instead of looking at the three-dimensional region, we are looking at the six-dimensional region. Are there some limits? Like P should not be too close to 1, S should be small, like are there some additional conditions that you need? So, just from the proper formulation, there's no condition, but I agree in practical scenarios, usually the more useful cases may be like, usually, because the social network. Like usually because the social network, although there are billions of users, the connections are very sparse. So the more practical regime might be when P is not going to one, for example. And S, in fact, S, I will discuss, isolate the discussion of S later in the polynomial time already part. So in short, for a long time, people only can do alignment where S is almost one, like Boeing. Is almost one, like going to one. Nowadays, people are pushing the S to smaller, smaller constants, but they at some point they think there is a limit, S cannot be vanishing, S has to be a constant, that's kind of the state of the art for the most time. So where I'm coming from is if my P is one, right, then actually there are many nodes that are indistinguishable because it's a fully connected tractor. So that's excluded from the requirement. Like the goal is to align it only to the extent possible. Just going back to your original model, you give this n be larger than log n as the requirement. But if my p is 1, I cannot align. Yeah, that's a very good point. So that's why I have the assumption p is less than half. So what happens? Yeah, you can ask what happens when p is larger than half for the graph isomorphism problem. So, the most basic version where s is equal to 1, it is symmetric with respect to half. So, if p goes all the way to 1 minus log n over n, you are good. Above that, there's no way to, yeah, exactly, your intuition is right. Yeah, and for simplicity, I'm only talking about sparse region, but you could do some sort of accompliment graph to get the other region. Right? So, what happens? Right, so what happens to the attributed graph alignment case? Oh, before I give you the information theoretic limit, there is one nice thing about considering this model, which is it relates to several other models, several other variations of this problem in the literature easily by specializing in this model. So, for example, if you want to go back to the vanilla artitraining graph alignment problem, you just need to set You just need to set m to zero, meaning there is no attributes, or there is no edge between user and attribute, then attribute will be useless. So that's the vanilla problem. And now if you set the edge probability between user-user part and user attribute part to be the same, and also setting the sub-sampling rate to be the same, this specialized to the so-called seated graph alignment problem. So the original seated So the original seated graph 11 problem is you start from the early frame graph and you assume some of the vertices are pre-aligned as seeds, and you use those seeds to help align the remaining unmatched vertices. There is a small distinction you might notice because if you start from the Earth Reining graph, then there will be also edges between seeds. But for our problem, there's no edge between the attributes. But it turns out, one can show that. It turns out one can show that the information selective limit of the original CD graph alignment problem and the specialized version without seeds between attributes are equivalent. So there is no loss in studying the problem without seeds, sorry, without edges between seeds. And the third special case is if there is no edge between user and user part, then this specializes to a problem. Specialized to a problem called bipartite graph alignment. So, because when there's no edges between users, this is a bipartite graph, and you have one side aligned, and you want to figure out the correspondence for the other side. Okay, now the information limit for the attributed graph alignment problem. So, there is a mild condition I have to introduce. The first two is exactly saying, okay, this P is. Saying, okay, this P is not too dense, P is bounded away from 1, and P A is also bounded away from 1. And this subsampling rate should not be too small. As long as it doesn't vanish faster than this, that's okay. And for this problem, what is the information stereotype limit? So we just, you can see the right-hand side is always the same, and the left-hand side, you just Side, you just need to consider in this new attributed graph pair what is the average degree in the intersection graph. So remember, this quantity is what we are familiar with, M P S square. So that's the average degree in the user part, from user neighbors, and M times P A times S A square, that is the average degree from the attribute part. From the attribute part. So if you plot this feasible region into a two-dimensional region where one axis is the MPS square and the other axis is MA times SA square. It turns out there is a nice linear relation. So if the two quantities sum up to log n plus some divergent term, it is achievable and otherwise it is not achievable. So, in some sense, you can think of this MPS square as the strength of signal from the user part, and the MPA times SA square is the strength of the signal from the attribute part. So, going back to your question, it's not one quantity that determines how easy or how strong the signal is, the product of the three things that determines whether the attribute can be helpful or not. That's also correct that with well scientific and if it is like bigger than log n, then you probably don't need any. If this part is already of order log n, then we don't need anything from this part? Yeah, that's true. Okay, so from here we can easily see what is the benefit from attributes information. So if you want to compare So, if you want to compare when there is no attribute information, so that corresponds to when this axis is equal to zero, so you specialize to this axis. And first of all, this will recover the best known information theoretic limit for the vanilla problem. And from here, you can see without attribute information, you can only achieve the right-hand side, the green part. Or the green part, and with attribute information, you can achieve the additional blue part. So that tells you the benefits from attribute information. And moreover, you can also specialize into this line of slope m by n. So if the ratio of the two axes is m by n, that gives you the visible and invisible region for the seeded graph line problem. Graph alignment problem. So here we have m plus n times the ps squared. And notice that the right hand side is still log n instead of log m plus n. And that is because the m se are already aligned. And there are only n unmatched policies. And moreover, you can do the yeah, another specialization into this access that gives you the feasible region, infeasible region for the bipartite graph alignment. For the bipartite graph alignment. So, one small remark about this is: in fact, this region can be further improved by starting a slightly more general settings called database alignment. But for simplicity here, I'm just giving you this version of the problem of the visible region. So, how about the complexity? Does the C tell to give you a polynomial time algorithm? Oh, yes, that's a Oh, yes, that's a very good question. I'm going to talk about this in the second part. But the quick question is: yes, it does help and it helps tremendously. I think I will give a very brief version of the key ideas in achievability and converse. So here we are using the, we don't have any computational constraint, right? So the estimator we use. Constraint, right? So the estimator we use is just the optimal estimator, the map estimator. And it turns out one can show that the map estimator boils down to just you rotate the two graphs until you find a permutation that minimizes the edge misalignment between the two graphs. So that's the map estimator. So however, for the attributed early framing model, so there the map estimator will boil down to a weighted average of the Weighted average of the misalignment between user-user edges and the misalignment of the user attribute edges. And the optimal weight is described by the parameters from the graph. So this is a detail I can explain more if you're interested. And the error bounding technique is a little bit involved. It uses the probability generating function and it looks at this so-called lifted permutation from vertex to. From vertex to edge, because every vertex permutation will correspond to an edge permutation, and you look at the lifted permutation and you analyze the cycles in the permutations. I'm not going into the details here for the sake of time. So, the key ideas in converse boils down to studying isolated vertex, like when there are more than Like when there are more than two isolated vertex in the intersection graph, then that is when no algorithm can recover it. So in particular, and the reason for that is you can find non-trivial automorphisms induced by the isolated vertex. Yeah, so here I'm trying to say. So here I'm trying to say this probability of success is upper bounded by one over the number of automorphisms. So if as soon as you have one non-trivial automorphism, so this number will be two, then there's no way this probability of success going to one. And in the attribute case, instead of considering the automorphism induced by isolated vertex, you will need a little more to get the converse. So you will need to. To get the converse, so you will need to consider automorphism induced by so-called indistinguishable vertices. So, what are indistinguishable vertices? So, these are just vertices that has the same connection or disconnection to all the other vertices. So, for example, here one and four, it is either connected, both connected to some vertex or both not connected to uh vertices. And and this has to be true for all the other vertices, and in this case, one and four are indistinguishable. So One and four are indistinguishable. So, to do the converse, we analyze automorphism induced by those pairs. Okay, now come to Bruce's question about efficient algorithm. Does attribute information help in developing efficient algorithms? So, before we go there, I'm going to give you an overview of the state of the art for efficient algorithms for the vanilla problem, the vanilla. Vanilla problem, the vanilla Ardifeni problem. So now let me re-describe the information theoretic limit in a different way. So remember, the information theoretic limit is roughly MPS squared equals log m plus minus some omega 1 term. Now I'm going to isolate this effect of S. So remember S is the subsampling rate. So S B1 means the two graphs are. One means the two graphs are identical, and the smaller S is, the less correlated the two graphs are. So, I'm going to isolate this effect of S. And the other axis, I will put it as MPS square. So, when the product is equal to log n, so I'm going to use this curve to denote log n. So, above that, it is achievable, and below that, it is not achievable, roughly, like that. And how about the polynomial? And how about the polynomial time visible region? So, for a long time, people are, all of the polynomial time algorithm will require S going to 1, almost 1, or very close to 1. In 2023, there is the first algorithm that shows you can develop efficient algorithms up to this so-called Alter's constant, which is roughly a point square root of the Altus. Point square root of the Alter's constant, which is roughly 0.338. And then they conjecture that below the Altas constant, there's no polynomial algorithm that can achieve the information at the limit. Shortly after that, Ding and Li proposed a new algorithm that shows in the dense regime when the product MPS is of order n to the sum constant. Some constant, there actually exists a polynomial time algorithm that can go beyond Otter's constant. And this can be as small as any constant, as long as it's not vanishing. So now, below this constant, in the dense regime, it is your conjectured there is an information computation gap. So this part people don't know yet. Can we go beyond others constant or not? In the sparse region, Or not in the sparse regime, which sometimes is the more useful region for practical purposes, it is not known. Now I'm going to answer Bruce's question. What happens when there is additional attribute information? So it turns out with additional attribute information, you will be able to push the correlation, the S to vanishing region. Okay? And we show that with With a very tiny bit amount of attribute information. Remember, this is the key quantity that describes the strength of the information from attribute part. So we show, for example, if the strength of the information from attribute part is as small as 1 over square root log n, it is possible to push the s to something like this, which is vanishing order. And you may say, oh, this is unfair because. You may say, oh, this is unfair because before you have zero attribute information, and now you have additional attribute information. Indeed, it is not fair. But we want to argue that with this tiny bit of additional attribute information, it is not enough to change the information theoretic limits, right? Because remember, information theoretic limit is this community plus this community larger than log n. And compared to that, this is nothing. So it doesn't change the information. So, it doesn't change the information of the limit. However, it greatly boosts the ability to align using efficient algorithms. Right? I have a collector question. Sorry? I have another question. I know those polynomial time algorithms have like x130 or something. How about when you decide information? Can you have some practical, like lower-productive-like, yeah, that's a very good point. So, the algorithm that achieves The algorithm that achieves up to the Attorney's constant has n to the 23 or something polynomial. And this one we have is inspired by the idea which I talk very soon. But it has, you can have a polynomial always as small as n to the, I think, five. That's the smallest we can do, I believe. The smallest we can do, I believe. Which is still probably not practical, practical, but better than to the 23. But you're welcome to improve that, for sure. Yeah, okay. So now I'm going to talk about the algorithm very quickly. I know I'm slightly short in time. So there is a general framework in designing all the efficient algorithm for graph alignment problem. So a general Recipe is to create something called a signature embedding for each vertex. So, what is a signature embedding? So, it is basically information you collect from the graph for some vertex. For example, the simplest embedding could be just the degree of the vertex. And in fact, that is what people use in the very early work for random graph isomorphism problem. They just compare degrees. And, of course, if you can compare. And of course, if you can compare degree, you can compare like the degree of your neighbor. So you can count your neighbor, you can count the neighbor of the neighbor. So that gives you the so-called degree profile. So that means this vertex has a degree 3 neighbor, degree 1 neighbor, degree 2 neighbor, degree 1 neighbor. So that gives you a profile, and you can compare all the degree profile between verses. Now, as you can see, now you can go deeper and deeper and collect more and more information. Go deeper and deeper and collect more and more information about the vertex. The state of the art is by perhaps one of our online, I'm not sure whether Jami is here, but he is listed as one of the online participants. Jamie and his students and collaborators, they created this thing called chandelier. What is a chandelier? It is nothing but a rooted tree where each Tree, where each branch ends with a different non-isomorphic tree. They count the appearance of this kind of gendered layer and use that to create a signature embedding. So once you have the signature embedding, you compute some sort of similarity score between a vertex i in g1 and a vertex g in g2 prime, and you create this quantity gamma ij, which is Gamma ij, which is usually some sort of similarity function. The simplest one can be just the inner product or some sort of normalized inner product between the signature embeddings. And the key observation here is for the correct pair, the average similarity score will be very high because these two quantities will be positively correlated. But for the wrong But for the round pair, all of the round pair, these two signature embeddings will be almost independent and therefore this correlation will be much smaller. And then the question boils down to creating a hypothesis testing problem and show that under what condition the two distribution will be concentrated enough so that by thresholding the probability of error will go to zero. Okay, so that is the general framework for many of the efficient algorithms. Of the efficient algorithms. So, in particular, we are going to consider star graph count algorithms. And yeah, I'm going to skip this part. So, let me just tell you what we are counting. So, we identify a rooted tree. This is inspired by the candelia alpha, by the way. So, we count this rooted tree starting from a user vertex, let's say one, and end with two. Let's say one, and end with two attribute vertex here, AB. So, by the way, here, one, two, three, four, five is the user are user vertices, and ABC are attribute versus. So, we want to count this type of tree that starts from a user vertex and in two halves, in exactly two halves, end at two attribute versus and not only so, we require it to go through a user vertex hop. Vertex hop. So the first hop has to be user versus, and the next hop have to end at the desired attribute. Why can't you count this in polynomial time? Why can I count this in polynomial time? Okay, so first of all, this is exactly two half, so that's a constant. And also, we require the number of branches here. I give the example of two branches, but in general, this can be k branches, but we also require k to be constant, so everything is constant here. So it's still. Here, so it's still polynomial time. So, how many such graphs can I identify? So, this is one possibility. And if you swap AB, this is another possibility. And then this is another one, another one. So, this count will be just 4. And how do I create the signature embedding? We simply start from this vertex 1 and enumerate over all, let's say their M attributes, so enumerate over. M attributes, so enumerate over all M choose two vertex, sorry, M choose two attribute vertices as the leaves. And we do this counting for each of them and use this vector as my distinguishment embedded. That's what we count. And the story after that will be the same. We count the similarity and figure out where to put the threshold so that the correct pattern. To put the threshold so that the correct pair and the wrong pair will be sufficiently separate, you know. So, this turns out will not be achieving exact alignment immediately. We first can only achieve all but a vanishing fraction, and we do a second step to refine and use the already aligned parises to further align the SC's to further align the remaining purposes. So, that's the basic. That's the basically our algorithm. Okay. I mean, exactly. I will have like a let me just briefly comment on the open problems. So first open problem is here. Is there really an information computation gap in the vanilla or is rainy problem? So in the dense region, anything that is not vanishing is polynomial time achievable. How about this part? And in the sparse region, we Spark. I think the sparse regime, which I argue is the practical and more useful regime, is this outers constant really a threshold for the gap, or can we go beyond that? So that's one of the big open problems here. And for all of the information computation gaps in the literature, typically it's based on two types of arguments. One is by reducing it to some well-known. It to some well-known well-established information computation gaps, such as the planted click problem. The other is by analyzing a family of polynomial type algorithms and say, oh, this family will fail and therefore conjecture a gap. So this gap is based on the second type of argument, which shows the failure of certain type of polynomial time algorithm. So I would say compared to the first type, this is perhaps a weaker argument, a weaker argument. Weaker argument, a weaker conjecture. So you're welcome to break them, or you're welcome to show there is indeed a gap. And how about for attributed 1211 problem? Oftentimes when I give this talk, people ask how about this tiny bit. Is it because the analysis is not tight in your algorithm that you can actually achieve more? Or is it there is some fundamental limitation for your algorithm and new algorithm need to be introduced? A new algorithm needs to be introduced to close this part. So, my answer is: we believe it's the second case. We have done the best possible for the analysis for our algorithm, and it doesn't seem to be achieving the full information theoretic measure. Alright, so there are many variations, including some of Bruce's students' work to consider alignment from more. Consider alignment for more than two graphs. All people go beyond the early framing graph to consider more practical graphs like stochastic block models. And in all of the discussion today, we're assuming two graphs are coming from exactly the same set of users, which may not be the case in practice. You might have one smaller graph, one bigger graph, and how to do alignment in both cases, the sub-graph alignment. And finally, in the past, I think it's been a year, me and Wena and other collaborators. And other collaborators are discussing this sampling-based alignment. So, we want to come up with an estimator that samples from the posterior distribution and hopefully create some Markov chain that converge in polynomial time to the desired posterior distribution. And if you're interested in learning more about this, please come to another breakdown session. And with that, I will conclude my talk. Regarding the sampling-based arrangements, so once you sample from the post-figure, how do you use it for solving arrays along? Oh, yeah, so there is one small lemma that shows you so is your question what is the estimator or what's the zero? Then, how do you use that sample to actually solve that energy problem? Yeah, so I just claim the sample is