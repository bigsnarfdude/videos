So, first of all, I'd like to thank all the organizers to give me an opportunity to talk to you here. I'm Kosuke Morikawa, an associate professor at Oscar University, and this talk is joint work with Professor Kim at IR State University. So, today I want to talk on semi-parametric adaptive estimation and the informative sampling. So, this is a brief summary. So, in survey sampling, some data are sampled according to inclusion probabilities. Instead of using all the data from the target population, so this is a very simple illustration. So, we can use the information of the response probabilities. So, in survey sampling, so these three data sets can be highly sampled. data set can be highly sampled but the others data set will be will not be sampled okay so we can use the information of the inclusion probability or weight so weight is its inverse so this weight plays an important role to conduct varied statistical analysis so for example Horace Thompson's mirror is using this weight just multiply the weight weight just multiply the weight we can obtain the consistent and asymptotically normal normal the estimator that has consistency and asymptotic normality so however the classical estimator like horrifystones meter is not efficient so in this talk we propose an estimator that that attains symparametric efficiency bounds by using a model on the weighting By using a model on the weighting mechanism. So, this is the content. So, let's move on to the introduction part. So, this is the setup. So, there are five variables. Y is response variable and x is our interest covariate. And z is other covariates. So, we consider the situation where we are interested in the relationships between. The relationships between X and Y. So, for example, we are interested in estimating regression parameters or to estimate the outcome model. Okay, and we are not interested in the relationships between Z and Y. Our interest is just on relationships between X and Y. Okay, and W is inverse of inclusion probability. And delta is something indicator. Delta is something indicator. So it takes one if the sample is really sampled, and it takes zero, it's not sampled. Okay. And we consider two settings. And one is the situation where X is completely observed. And the second setting is we can use partial information on X. So we can use the information of X only in the sample units. In the sample units. Okay, we construct two settings, but due to time constraint, I focus on the second setting, this setting. And also, even if we focused on the second setting, there are further two scenarios. One is the capital N, this is a size of population, is known or unknown. So there are two cases. So we separately consider the two cases. So, we separately consider the two cases. So, to estimate parameters in service sampling, the sampling mechanism is very important. So, it is often assumed that the sampling mechanism is non-informative. So that means if we are conditioned by the cobalt, the W is independent of Y. Of y. But in this talk, we do not assume the assumption. We do not rely on this assumption. So in this talk, Y may have an effect on W, even given by X and Z. So this is a very simple and clear data and real data written in first textbook. Inferral's textbook. This is the real data set of the Canadian workplace and the employee survey data. So, because this Professor Hura's textbook is very educational, so this real data is very simple and clear. So, we want to know the relationships between payroll and the total employment, so, X and Y. And y. So you can see that it's the joint distribution of x and y might be normally distributed. So it's reasonable to achieve the normal assumption. And the size of population is 2000. And we sampled 142 workplaces out of 2,000 workplaces. And it is sampled according to stratified sampling with three strata. With three strata. And in each strata, so there are three strata. So in each strata, it is further sampled by simple random sampling, but it has some non-response adjustment. So you can see that if the pedal is small, then the people are allocated to high weight. If the pedal is high, the people The people should be low weight. So there are relationships between Y and W. Okay, so here we are interested in estimating the outcome model. Here, the parameter is AB and sigma square. Okay, let me define our target parameter at first. So we consider a G estimator. So that is Estimator. So that is a unique solution to moment equations. So, for example, if we are interested in our expectation of y, the u function becomes theta minus y. And if we are interested in our out-score model, then u becomes the score function that is a derivative of log likelihood. So this is our target parameter. So we need to consider separately these three situations. Separately of these three situations. Okay, so if data are completely observed, we can adjust change this expectation to the empirical one, then we can get a reasonable estimator. However, because it is tampered, there is exist bias. So to remove the bias, the probability function estimator is the most, I think, well-known method. Well-known method to remove the bias. So, by multiplying the weight, this estimating equation becomes, I forgot, unbiased, sorry, unbiased estimating equations. So it can be easily checked. So by taking expectation on delta given or data set, because of the assumption on W. On W, this becomes W inverse. So this term can be cancelled. So this estimating equation is unbiased. Okay, so this estimator has consistency and asymptotic normality. Okay. However, there are some efficient methods. Some efficient method. So let W childa be the smoothing weight. It is defined by conditional expectation given observed data. We want to show that using this smooshing weight, instead of using W, it's more efficient in the context of the regression analysis. So, W2, of course, this is based on conditional expectation. We need to estimate. We need to estimate this smoothing weight. Okay, of course, if we misspecify the conditional distribution, the method will be biased. So there are some trade-off between bias and variance. So we can gain the efficiency after risk of bias. And human skin approaches. And Kim and Skinner propose an optimal weight in the regression analysis in keeping with consistency. So, what I have to say is, so there are possibilities that we can construct more efficient estimator than a Horbet-Sampson estimator. So, before explaining on our proposed estimator, I'd like to prepare for I'd like to prepare for one formula. So let F1 is a conditional distribution in sampled units. So I use one to indicate sampled units. And pi is response probability. So there is a formula to transform from F1 to F. So you can easily check by using the base. So you can easily check by using the basis formula if one can be changed to f by using this formula. And oppositely, F can be transformed to F1 by using this formula. I use this formula many times in this talk. Okay, so at first, I consider conditional maximum likelihood method for outcome model. So assume Outcome model. So assume that we are interested in the outcome model and the response probability is known. So this is not a reasonable assumption, but at first we assume that this is known. In this case, of course, maximum likelihood method is efficient. So we can construct a maximum likelihood method based on sampled units. So because So, because F1 can be written as this form, by plugging this equation here, we can obtain an estimating equation. This is efficient if response probability is known. Okay, so if pi is known, we can compute efficient estimator by solving this equation. So, however, pi is generally unknown. So, however, pi is generally unknown. So, we need to model and estimate this pi. So, how can we estimate this pi? Because we have only observed data, so it's very difficult to estimate pi. So, however, there is very interesting relationships between the conditional expectation and response probability. And Sverkoch and Fauman showed that the conditional expectation on W given Conditional expectation on W given X and Y in sampled units is the same as one over response probability. This can be easily shown by using the formula. So by using the formula from F1 to F, we can obtain this equation. And because of the assumption on W, this term canceled. So the numerator becomes one, and the denominator becomes just integration out with respect to. out with respect to w. So that's why we can obtain this equation. So this equation implies if we want to estimate pi, what we have to do is just take conditional expectation on W, given X, W on X and Y in sampled units. So we can estimate the response probability by using sampled units. Sample genes. Okay, so if the model is correctly specified, we can estimate pi and then we can construct an efficient method. So, of course, if pi is misspecified, this estimate causes bias. Okay. So, this is a summary of what I have talked so far. So, Horbit Stormson's matter has. So, Holby's Thompson Matter has consistency and achievement normality. And the conditional maximum likelihood similar can gain the efficiency by adding information on W given X and Y. So by modeling these relationships, we can gain efficiency. But there is a risk of bias. So we can gain efficiency, but there is a risk of bias. Gain efficiency, but there is a risk of bias. Okay, our goal in this talk is to get an estimator that has all properties, consistency and smooth normality, and efficiency. Okay, all estimator has consistency. Okay, let's move on to the probable estimator. Esmeralda. So, this was a setup. So, we consider this setting. Okay, the key idea is to regard W as a covariate, not a deterministic value. We consider W as a random variable. And so, we treat W as a covariate and construct a symphony model. So, we consider the joint distribution. Consider the joint distribution given sampled unit. So, because we do not know the disk distribution and distribution of x, so this involves some infinity dimensional nuisance parameters. And if our interested outcome model, at this time, y given x does not involve any Nuisance parameters, but if our interest is just a regression model, this has some, this may have some nuisance parameters. Okay, so our goal is to estimate theta that is not affected by these nuisance parameters. So we can use some semi-parametric theory to get efficient estimation. To get an efficient estimator among regular and asymmetrically linear estimators. So, thanks to the great work by Rotonitsky and Robbins, the efficient score is given by this form. So, the first term is essentially IPW method, IPW estimator, and the second term is augmented term. So, this has zero mean. So, this So, this is a kind of decomposition of original score function into two parts. One is one element lies on the tangent space. This has information on nuisance parameters. And the other part lies on the orthogonal tangent space. So, this means this is not. Is not affected by nuisance parameters. Okay, but this is a bit complicated, so I don't know, I don't give the details, but this star is the unique solution to these functional equations. So it's a bit complicated. And this should be obtained according to our target parameters. Be our target parameters. So, for example, if we consider three setups, the first case is just our interest is expectation Y. And the second case is a conditional regression model. And the third case is outcome model. So the form of D star is different according to our target parameter. Okay. Okay. So this is the first result. So when the number of size of the population is known, we can obtain the efficient score, explicitly obtain efficient score. So in usual not missing a random case, the efficient score cannot be explicitly obtained. But because of the formation of W, we cannot be aware of the formation Of the formation of W, we can obtain expressly the efficient score. And D star and C star have this form. So, for example, if our interested is the solution of this equation, then D star is U and the C star has this form. And if our interest is regression parameter, then D star and C star has this form. So, C star takes a little bit complicated. So, I'd like to focus on the final case, the last case. So, we are interested in estimating outcome model, and D star and the C star takes this form. So, here, pi bar is expectation on W given X and Y. So, we use, oh, sorry, we use this equation. We use this equation. Sorry, we use this notation because when E changes to E1, as I explained before, this becomes pi. So that's why I use this notation, pi bar. So the difference is whether E is E1 or not. So this efficient score involves three unknown functions. One is pi bar, we need to estimate or model. We need to estimate or model, and also we need to compute these two functions. Okay, so once we obtained these three functions, then we can obtain the efficient score. Before I talk on the adaptive estimator, I'd like to remark that Z is unnecessary to get an efficient estimator. So, information. So, information of Z does not affect the efficiency of C at all. This is because in missing data analysis, so all the covariates, so in this usual case, all the covariates have effect on indicator delta. However, in this case, W blocks the effect of G. So this means observing W is enough to explain delta. Is enough to explain delta without using Z. Okay, so we do not need to sample Z if our interest is just relationships between X and Y. Okay, so I skip this. Okay, let's consider an adaptive estimator for the outcome model. So, what we have to consider, sorry, what we have to estimate is three. To estimate is three functions: pi bar and its conditional expectation. So, I give a reasonable model in the next slide. So, assume that we can get a reasonable function pi bar, then the two functions can be easily computed because pi bar, the functional expectation can be written as this form. And of course, theta is not estimated, but we can obtain a Horbit sample. But we can obtain a horizontal estimator. This is consistent and assumed to be normal. So we can obtain a reasonable estimator for theta. So by plugging here, we can compute this think function like this. So the value function is the same. So the problem, what we have to do is just estimate this pi bar function. Okay, so to give a reasonable estimator or reasonable model to pi bar, I use these relationships. So if x follows a beta distribution is parameterization alpha better, then 1 minus x over x follows a beta prime distribution is a parameter parameterization better alpha. So this is a fact. And by using these relationships, we assume that W inverse given X and Y follows a beta distribution with parameterization of this. The reason why we assume a beta distribution is because W inverse is just a probability. So that takes variance on interval between 0 and Y. Sorry, 0 and 1. Recurring one. And also, the reason why we use this parameterization is by taking expectation, the conditional expectation is M, and the variance has this one. So if phi is large, the variance becomes small. Okay, and this is essentially the same as the beta regression model. Regression model. So you can easily interpret this model because expectation is just M. Okay, so by assuming this assumption, this modeling, we can show that W minus one follows a beta prime distribution by using this formula, so this equation. Okay, if W inverse follows. If W inverse follows beta distribution, the one minus W inverse over W inverse force beta prime distribution. So W minus one force beta prime distribution. And furthermore, we can obtain the distribution of this W minus one in sample units. Okay, by using the formula for F1 to F again, we can show that. Again, we can show that the distribution of all in sampled units also follows beta prime distribution. But the parameterization is different. So we need to add plus one here. And by using this property of the beta prime distribution, we can show that the conditional expectation of w given xy is one over m and it's And its expectation in target population is written like this. Okay, so in summary, so if we assume that the W inverse given X1Y follows beta distribution, then W minus one follows beta prime distribution, and its sample version also follows beta prime distribution. But the parameter Beta prime distribution, but the parameterization is different. We need to add plus one. Okay, so if we can assume that W inverse force beta distribution, we can estimate the parameter on phi and m by using maximum likelihood based on this distribution. Because we have all data for delta equals one, we can estimate m and phi from this distribution. This distribution. Then we can, once we obtain a reasonable estimator on M and V, we can get an estimator for this conditional expectation by plugging M and V here. Okay, this is our strategy to get adaptive estimator. So at first, I assume a parametric model on M okay, so because this Okay, so because this is just as I explained, expectation of one, sorry, expectation of W given XY in some of the units is same as one over pi. So M is the same as pi, the response model. So it's reasonable to assume like logistic model for M. So assume a parametric model on M, so like a logistic model, then we can Logistic model, then we can estimate this better and free by using maximum likelihood based on the beta prime distribution, so this distribution. Then we can obtain a variety estimator for pi bar just plugging here and take inverse. Okay, then we can get all. Okay, then we can get all the unknown functions. So we can construct a semi-armatic efficient estimator by plugging in the unknown functions and solve this estimating equation. Okay, and so I'd like to. So far, I assume that n is known. So when n is unknown, the efficient score is obtained by letting. Is obtained by letting C star that constant time to be zero. So, for example, if the regression model is of our interest, in that case, C star by letting C star zero and d star can be written as this one. Okay. And this is exactly the same as the result of Tim and Skinner. So So this is the summary of efficient score. So we derived nine settings in nine settings. But in this talk, I focused on this part. So n is known, x is partially observed case. So as I explained in this slide, in this case, n is unknown, x is partially observed. This is exactly the same as the Kiemann-Skinnard estimator. The Kim and Skinner's estimator. So, our contribution is we derive the other eight cases, the other eight cases of efficient estimator. Okay, and finally, I'd like to remark that if the sampling mechanism is stratified sampling, it would be reasonable to assume that W inverse follows a better distribution. For us a better distribution in each stratum. So this is kind of a mixture model. So we need, we assume data regression in each stratum. But additionally, we need a model on the mixture proportion. Okay, once we assume the mixture proportion, we can obtain an estimator for fee and For phi and mH by using EM algorithm. So, but it's a bit complicated, so I omitted the details. But anyway, we can compute the pi over function and the pi function analogously. Okay, this is a large sum property of our proposed estimator. So, under some regularity conditions, our estimator has two property: one is Has two properties. One is if all the working models are correctly specified, it obtains a symparametric efficiency bound. It is the most efficient in a class. And even if all the working models are misspecified, our estimator has consistency and assumption normality. And the variance can be written as Sandish formula. So, the second property is a kind of robustness of the working model. And we can extend the working model to semi-parametric and non-parametric case. So, the semi-parametric means we may keep assuming a better regression, but we can consider a non-parametric model on M. So far, I assume it's some parametric model, linear logistic model, but I think we can easily extend. And also, we can consider non-parametric working model. Because what we have to estimate is this function. By using the formula from f to f1 again, we can show that this is the same as this one. So by estimating both two functions, we can get a non-parametric estimator for. We can get non-parametric estimator for pi bar, and so I'm sorry, this is just ongoing study. So we believe that we can show that these estimates also varied, but we have not finished to prove yet. So I didn't talk the detail anymore on simple metric and non-parametric working models. Okay, let's move on to the simulation part. Part. So I considered the case when X and Z and Y follows a normal distribution and W inverse follows a better distribution. And the sample size of a population is 5000. Okay, and our interest is to estimate A, B, and sigma square as a real data example. As a real data example. And I consider three cases. Oh, it's okay. The first scenario is the response. Okay, response mechanism is simple random sampling. So it means X and Y is independent of W. And the second case is. And the second case is x and y has information on delta or w. So this means this is an informative sample. And the final case is misspecified case because we assume that M follows this logistic model. So in that case, we can cover the first and second cases, but the third case is misspecified. And we compare four cases of four estimators. One is the complete case analysis, and the second is a Holbest component type estimator. And third is conditional maximum likelihood. The last one is our proposed estimator, but we assume the two cases. One is we assume a regression model, and the other case is we assume a normal distribution. Okay, so the assumption is very weak. So, the assumption is very weak in the first case and strong in the latter case. Okay, this is a box plot of our proposed estimator for B. This is a slope. So, this is the slope. So, in the first case, because the x and y does not have any information on w. Information on W so we cannot gain any efficiency. So all the estimator has consistency and the asymmetric variance is almost same. Here, IJ means, I means N is known, and J means X is completely observed. So EFF11 is the most informative case. We have a lot of. Informative case. We have a lot of information. Okay, in scenario two, you can see that, of course, we need more strong assumptions on outcome model. So outcome model, assuming outcome model is more efficient than just assuming regular, sorry, regression model. And also, if by adding information, If by adding information, the assumptive variance becomes smaller. Okay, and also Horbit sum sums is consistent, but assume the variance is a little bit large. And conditional maximum likelihood estimator looks good, almost the same as this one. However, in the third case, when third case when the response model is misspecified the control maximum likelihood is biased because it's heavily depending on the correctness of the response probability but our estimator is keeping a consistency and its performance is almost same as Horace Thompson even if the response probability is misspecified okay Okay, finally, I apply a method to the real data analysis. So, this was a real data. So, there is a relationship between X and Y and W and Y. So, by adding information on this weight, we can gain the efficiency in keeping with consistency. So, because this So, because this is a stratified sampling, we need to assume three response models in each structure. And in each structure, it's simpler than the sampling. So, we assume that in each structure, the mean is constant. The mean is constant. And it has some variance. And also, the allocation of strata depends on y. So, if y is small, these people should be allocated to the fastest strata. So, we assume that the mixture probability of strata is much the logistic model that is dependent on y. Dependent on y. So we assume this working model and estimate the pi and pi-bar function. And this is a result. And we can see that Horbit Sompens estimator and our proposed estimator is very similar. But the standard error of our estimator is much smaller than Horace-Thompson. So, for example, in this case, the standard error is Case, the standard error is one-third. So, this is an effect of adding the information of weighting mechanism. So, in conclusion, so in survey sampling, weights are known, but the information has not been fully utilized. So, our proposal estimator attains information efficiency bound if the working models are correctly specified, but it has consistency. But it has consistency even under misspecification about working models. And we need to extend our working models to non-parametric and symbolic working models. Okay, that's it. Thank you.