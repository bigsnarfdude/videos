Special thanks for having it at a reasonable time of the day for me. So I'm going to talk a bit about some work I've been doing on random plane waves. So random plane waves were suggested by Berry as a model for chaotic systems in the sense that very loosely said that a chaotic system should behave like a random wave. And so this is like a physicist's conjecture. This is like a physicist's conjecture, so it wasn't in terms of really precise statements. But what we're looking at here are random waves, they're functions of this form. So we have a function u, which is a sum of these oscillating exponentials. Now, exij, those give you the direction and sometimes you allow them to have some magnitude as well. So usually the xij either sit on the unit sphere or On the unit sphere or on some neighborhood very close to the unit sphere. Your CJ, this is where the randomness comes in, you choose them according to some random distribution. So some things that people do, you can have Gaussian, so each CJ is a Gaussian and they're independent, or you could say, well, I want to make sure that the L2 norm of these things is always one, so I'm going to pick. Things is always one, so I'm going to pick them uniformly on a large-dimensional sphere. So there's a couple of different things. And it turns out that most of the stuff results that you get, things like Gaussian or uniform on a sphere, you tend to get the same results, even if you set slightly different ways of picking your coefficients. We set our normalization if we're using Gaussian, so that sigma squared is n inverse. So this just So, this just, you know, it allows us, for instance, to say that the L2 mass on a ball of radius one is just equal to that volume, exactly, in expectation. So that's the normalization that we always pick. Okay, so what do random waves look like? So this is numerics by Alex Barnett showing a random wave in a unit box. So Unit box. So you can start to see, and these are the filament structure I was talking about. These are slight enhancements along what appears to be lines. And if we, hopefully this works. Could someone confirm that they're seeing the web page that I went to? Yes. Yep, good. No, sorry. No, no, still see your presentation. Okay. Okay. All right. That's a nuisance. It doesn't seem to just let me see if I can fix it. All right, Melissa, you might have to click on share again, and then it'll give you sort of an option of different things to share. Okay. And then you can like share your web browser or something. Yeah. Okay. How's that? Great. Beautiful. Okay. Sorry. Zoom was not keeping up with what I wanted to do. So, what I wanted to do is show you some of these where you can actually zoom in on them. Okay, so let's go in like this. So, you can see, and when you look from a far field, they look like straight lines, but you can see as you come in that they have a bit of a sort of a wiggly structure rather than being perfectly straight lines. And you also have this property that they develop, so these work. They develop. So, these worms movies. So, what's happening here is we're going to look at when we go from considering all frequencies in a ball to ones that are just on the sphere. So, now we're randomizing here, for instance, over things that are in a very small annulus around the unit sphere. Whereas if I reverse this back, here we're randomizing over things. We're randomizing over things are in a very thick annulus around the unit sphere. So you can see that this filament structure sort of emerges as you tighten that band around the unit sphere. Okay, so I'm now going to go back to the talk. Okay, so this is the kind of thing that we're trying to understand. Kind of thing that we're trying to understand. And those numerics were created as part of an AIM workshop that Alex was attending. And I'm just going to just, because there was a lot of discussion in the AIM workshop about what these things actually were, and here are some suggestions that people came up with, that they were numerical artifacts, that they actually really were scars, or that, you know, we're seeing we're basically hallucinating. That we're seeing, we're basically hallucinating a pattern that isn't there. So I think I'm going to have to do this fiddle around with sharing again. Okay, so the highlighting is mine. So sort of read down, it's all about here, but read this bit. So opinions and arguments, often of the same individual, arranged over the Raged over the spectrum between believing that these could be formalized and observations stated and proven. So it seems, at least from reading this report, that there's wide differing opinions and nobody was really sure. So the upshot of this, this is the end of the AIM workshop. Nobody really came up with a conclusive set of conjectures or a conclusive idea about what these filaments might be. So I didn't actually attend. So, I didn't actually attend this one, but I did hear about a lot about it shortly thereafter. So, yeah, so this is sort of what I'm trying to have a look at. And hold on a moment, I'm just going to go back to my slides. Okay, my slides are not showing up in the share, so give me a moment and I will get them back. That looked promising for Zek. That looked promising for today. Yep, I've got them. Okay. All right, so half the problem with working out what we're looking at here is that if the enhancements exist, they can only be logarithmic because so you have these results that say that the expectation of the L infinity norm of U can only grow logarithmically. View can only grow logarithmically. So, this means that we could never get power growth, which would be very easily visible from the numerical stuff. We can only get the logarithmic growth. And so it's very hard to be sure that you're seeing an actual effect rather than a numerical artifact. Okay, so let's have a look at some of the results that we do know. So, if we fix X and compute the expectation of the size. Compute the expectation of the size of u that's equal to one or you know bound upper and lower bounded by one with very small error. This is really easy to compute. So that's not a really big deal. And what's more interesting is Berkelebeau showed the other side of the logarithmic growth, which is not only is the expectation of u in L infinity bounded above by some kind of square root of log lambda. By some kind of square root of log lambda, it's bounded below by that as well. So, the significance of this is that you expect that there will be some bright spots. So, if you pick an x and then you consider your average over all sort of possible choices of coefficients, then on average for that fixed x, you'll see one. However, if you create all of these random waves and you examine them and say, are there any bright spots? Are there any bright spots? Then, on average, the answer will be yes, there are bright spots. In the other direction, you can show some equidistribution theorems. So, these theorems show that, in fact, there are no scarring or anything that is like that, even at quite small scales. So, I'm combining some results that I had with Hahn and with DeCourcy Island had with as an Of as an individual result. So we looked at slightly different coefficients in each of the studies, but for uniform on the high-dimensional sphere and for Gaussians, if you combine the results of all of our work, you get that so long as the radius of the ball is above the Planck scale, then the expectation is equal to the volume and the variance decays. Decays compared to the volume. So, in a way of saying this, it's like you pick a ball, you check the L2 mass on it, then you expect to see the volume of the ball. So, that's the equidistribution statement. So, you're also able to obtain some stronger results. So, this is coming back to the difference between equidistribution and uniform equidistribution. Uniform equidistribution. So if I just in general look at a random variable x of w where w is a parameter. So for the balls, the parameter will be something like the center of the ball. Now you can say that it's equidistributed if its expectation is equal to whatever the equidistribution value is plus something small and the variance. Small and the variance is small with respect to the equidistribution parameter. But this is just like that where we fix x and we look at how u of x behaves in the randomized thing. So you often want to look at the probability that for any w you exceed your equidistribution value. Distribution value. Sorry, this should be an x there. So if you're looking at something like a random wave, you're not picking a point or picking a ball and then computing its equity distribution. What you're doing is you're looking at the numerical thing and say, looking at all possible walls and seeing if one of them exceeds what it should do. So what you want to get for the uniform statement is something like this. So I take my Like this. So I take my set where I consider the coefficients so that I exceed whatever value that I think I should get by some amount. And I want to take the probability of that and show an upper bound for it. And this tends to be useful when the upper bound goes to zero as h goes to zero. So this is saying it's decaying probability. So in a sense, in the limit, you have no chance of getting this. You have no chance of getting this. You also want m of h to be small as well. So you often want m of h to be going to zero as well, because this is saying not only is the value of the random variable close to the equidistributed value, it is very close. So you're having like some kind of band that is narrowing down. So that's the kind of statement we want. Usually this upper bound is some kind of exponential decay. So it's a very So it's a very strong decay, and it says that effectively all of your things are uniformly gathered around the average value. So that's what was able to be obtained for bulls is that for bulls, you can say that they're uniformly equidistributed in their center, in the sense that the probability that any one of them That any one of them is significantly larger than their distributed value decays exponentially. So now I want to look at this for the random waves. And the first variable I'm going to look at is kind of an x-ray transform variable. So there's the equidistribution on balls. So an x-ray transforms, the idea here is one of the things that some people thought was true was that, in fact, these films. That, in fact, these filaments were honest to goodness scars. So, that there are lines for which you have enhanced L2 mass on that line. So, a natural thing to do would be to set up a random variable, which is the L2 mass of U along a line segment. And then you want to ask, is that equidistributed? And in particular, if it's uniformly equidistributed. Because if it's uniformly equidistribisted, Because if it's uniformly equidistributed, that means that the chances are, and they're decaying chances, that you will not get any coefficients that make a large enhancement on any line. Then that says that these filament structures, whatever they are, are not, in fact, honest scars. So I have my random. So, I have my random variable f with two parameters x and xi. And what I'm going to look at is the L infinity norm of that in x and xi. So if this is much, much bigger than one, that would say that I expect there to be some scars. So that's like the analogous statement to saying that you expect there to be some bright points when you look at the L infinity norm of U. What, in fact, we'll see. In fact, we'll see we prove is that this does not happen, and in fact, we can prove something very strong in the other direction, which is that not only is this bounded, the L infinity norm over X and XI is one, but the probability that you are not one is exponentially decaying. Okay, so there's a very strong form of saying that these scars do not exist. Do not exist. And the way I do this is a technique of combining some information about the random variable, which comes out of sort of Lipschitz norms and concentration of measures, with a class of phi, which these are supposed to make the calculation of expectation easier. So I'm looking for a phi for. So, I'm looking for a phi for which this equation is true. So, I need to make some monotonicity and convexity assumptions on phi. Those are very small assumptions, but things like if phi grows as a power is perfectly fine. So anything like that is suitable for this kind of treatment. So, the idea is that I'm trying, I want to compute E of X. What I can compute is E of 5. Can compute is e of phi of x, and I want to know what kind of error do I have between the two. Now, the parameters here, n is equal to the size of the set that you're randomizing over. So this is size of sort of your basis set. Q is related to phi. So you should be thinking this as this tells you something about the growth of phi. Tells you something about the growth of phi. So my model for this is that phi of t is tau to the q, and that's where the q parameter comes in. Kappa is related to the Lipschitz constant of x. So this is about the random variable itself. So if I take the random variable and I compute it at two values of the coefficient, what kind of L can I put in there? If anything, what kind of Lipschitz? If anything, what kind of Lipschitz bound can I get? And then the Lipschitz bound, sorry, that should be to the half minus kappa. So that n to the half is sort of the worst Lipschitz bound that you could get. That's if absolutely everything goes wrong. And then this kappa measures your improvement over the worst bound. So we want to pick phi strategically to make our calculations easy. So, for instance, we've got So, for instance, we've got random variables that are L2 norms or LP norms of various things. So, that suggests picking phi as tau to the p, tau squared, things like that, so that I can really explicitly write down the expectation of phi of x. So, let's see how this works with f. So, in this case, I am picking tau squared. picking tau squared and I can compute the expectation of F squared very easily. So if I just write this out, this is the sum over J and L, C J C L, and then I'm integrating along gamma. And the only thing that matters are the difference between Xi R J and X I L. Now the reason why I say this is really easy is because we're working Really easy is because we're working with Gaussians. So each of the Gaussians are mean zero. So when I compute the expectation of this, any term in the sum where I have CJ Cl and L is not equal to J just completely vanishes. It's zero in expectation. So I never need to make any computations for those terms. All I need to do is compute when j is equal to L, and then I'm just integrating the function one. So, you know, in this chart. So, you know, in this choice of phi, you really have a very trivial calculation to do. So we can compute that the expectation of the square raised to the power half is one. Now we need the Lipschitz constants. So the Lipschitz constants are going to tell us whether this phi inverse expectation phi of x is going to give us the expectation of x. So that it's really the Lipschitz constants. It's really the Lipschitz constants that drive what is happening here. Okay, that they're the important thing. They tell you how the random variable behaves. Now, in this case, I'm just looking at something, say, in two dimensions to match what Alex has done numerically. So these follow from some hypersurface estimates that I did some years ago. And in this case, we know that. And in this case, we know that if we're on a hypersurface, our function can only grow as h to the minus a quarter. And so the Lipschitz constant is therefore bounded by h to the minus a quarter. So that turns out to be good enough. And if I compute my expectation, I get that the expectation of f is equal to one plus something that is decaying. And in fact, I get the decay rate. And in fact, I get the decay rate. So it's decaying quite quickly. So that's the first part. The more interesting part is the uniform equidistribution. So far, all I've managed to do is show that if I fix my x and xi, I get something that's equidistributed. What I really want is to prove that they're all uniformly equidistributed. That they're all uniformly equidistributed, that would tell me that the actual scars are not, in fact, the answer to the filament structure. So for that, I use measure concentration. So I'm looking at the probability that I exceed, in this case, the median. The way that you prove the measure concentration, you have to use the median rather than the expectation. That's all right, because if you use the measure concentration itself, You use the measure concentration itself, you can relate the median and the expectation. So it's not actually a big deal that you use the median here. So it decays exponentially, and the parameters that really matter are the dimension and this constant L squared. So what we're going to do is we're going to say, all right, I want to say that I exceed it at some point x, xi. Okay, so I have some point over here. Have some point over here in my x express for which I exceed whatever the equi-distribution value is. So the equity distribution value for this one is one. So say I exceed it here. Well, everything that I'm dealing with is oscillating with frequency one on h. So if I put a sort of a blurry region size order h around this, I'm not going to change. This, I'm not going to change my random variable very much. So, changing my parameters can't change my variable very much so long as I don't move within more than a wavelength. So, I'm going to create a grid that is sort of spaced on the wavelength, or sometimes you just space it a bit smaller for convenience. So, the idea is that if I fail at the little blue point here, then Point here, then I must also fail at the little yellow point on the grid. So then I've got a finite set of things that if I fail at one point, I must fail at something in this finite grid structure. Now, the number of points in the finite grid structure is growing polynomially. It's growing as h to some negative power, but my But my probability decay estimate is decaying exponentially. So I can afford a polynomially growing grid very easily because I can pit it off against that exponential decay. So this is just written out here. So this is my exception set S. So these are the f x x i minus one that are greater than m of h. So this is, you know. So, this is, you know, for one XI that I am blowing up somewhere, something has gone wrong at this point, then it's contained in the grid. Then I just say, oh, well, I can beat out the polynomial decay, the polynomial growth of the grid with the decay from my measure concentration. So, whatever is happening with that filament structure, it isn't actually. With that filament structure, it isn't actual scars because they are uniformly equidistributed. Now, what I want to look at a little bit further is the phase space picture. So instead of looking at just restricting in configuration space along a line, I'm going to look at a phase-based picture where I restrict close to a line, but I also restrict. But I also restrict the frequency or the momentum variable in the direction that the line is going. So I have a number of parameters here. H the alpha, alpha determines the sort of the shape that we're doing. Okay, I might, because when you're looking in phase space and you're trying to localize things at Planck scale, you have a lot of choices about what your Planck scale is. Do you make your physical Physical space concentration down on some order H ball and allow your frequency to be anything? Or do you want to control your frequency and give up your spatial control? So alpha just gives you the basic shape that you're working with there. So my Fourier side, I open a little aperture of size H the alpha and my Fourier support lives in here. I'm not going to write Lives in here. I'm not going to write down the symbol for this pseudo-differential operator because it's terrible, but it's just cutoffs onto this region. Now, if you come over to the configuration space, now if I cut off with this aperture of H to the alpha and I took into account the curvature, then I would be looking at an uncertainty box in the configuration space of H to the one minus two alpha. h to the one minus two alpha in the direction of propagation. So that's the direction that I'm localized around here. And h to the one minus alpha in any other direction. So that's my basic shape here. Now I've got this parameter mu, which allows me to control whether I'm at Planck scale or not. So when mu is equal to one, I am perfectly at Planck scale and I cannot do anything further. I'm absolutely up against the uncertainty. Absolutely up against the uncertainty principle limits here. But as I make H large, as I make mu large, I'm moving away from the uncertainty principle. So I'm getting into a more relaxed regime where I've got some wriggle room between what cutoffs I've put on and what cutoffs I could put on via the uncertainty principle. Finally, I normalize everything so that if G was equidistributed, it would be equal to one plus It would be equal to one plus perhaps some decaying term. So now the problem becomes: all right, study G and see if the phase space thing is equidistributed. So it's equidistributed if I can get that you've got the decay, the same sort of gridding argument that I did for F. If I can get that I've got exponential decay for that, then I'm uniformly equidistributed. If, on the other hand, I show Hand, I show that it's large, so g of xi, the expectation of its L infinity norm is large, then I'm showing that I fail equidistribution. And so this gives a bit more of an interesting picture than just looking at the x-ray. This way. Okay, so does it equidistribute? So if you just fix x and xi, it's pretty easy to say, show that the expectation of this thing is one. That's just like what I did for x. One. That's just like what I did for f, and it really is just a property of the fact that the Gaussians are mean zero. So that's not particularly exciting or interesting. What we really want to know is the uniformity is, all right, are there any points x, xi for which I exceed one by some significant value? So firstly, if I stand back from the Planck scale, I stand back from the Planck scale. So I make mu growing in H. Okay, so this is, I've got a nice gap between the cutoffs I've got and the Planck scale cutoffs. So for that, I can follow the same process as I did for the X-ray variable, and I can show that they're uniformly equidistributed. So you get this format where you measure the probability that gx xi is not equal to one, is far away. Not equal to one is far away from one, and you get some kind of exponential decay. And again, I'm not going to write out what delta and epsilon tilde are. You can. They come out of combining various statements about Lipschitz constants and dimension. But again, you know, these are themselves not particularly important exactly what the formula is. What is important is that they are going to zero. So, so long as. So, so long as I have a bit of decay, I'm all right. Much more interesting is the case where you're at Planck scale. And so this is the case where you start to get something that is not uniformly equidistributed. So this result shows that if I am at Planck C. That if I am at Planck scale, then I will expect to see some x and xi that exceed one or exceed their quidistribution value. So if I create a bunch of random waves and I calculated g xi and I sort of searched through all those gxi and see if some of them are large, then with very high probability. And in fact, if this doesn't happen, you're back in that exponential decay. Back in that exponential decay again. So, a very high probability is that I'm going to get something that is large. And what is it going to look like? Well, for the u of x, what it looked like was bright spots, right? Because we're looking at the size of u of x. Now, for g of xi, this is something that lives in phase space. But again, you know, the uncertainty principle. Again, you know, the uncertainty principle is always sort of hanging around. If I've got one GXI which is large, then I will have other GXI which have to be large because they're close. Okay, and again, this just comes down to the fact that you can never really make a phase-space portrait of a function. You always get some degree of blur. So, this is not what I'm showing you here is not a numerical thing. It's just what I would expect to see. So, say I was looking at So, say I was looking at a two-dimensional case, and you plotted the ball of radius one and parametrized this circle from minus pi to pi, you would expect to see some smeared out regions. So if I color in as a representation of how big GXI is, I would expect to see some bright regions. And they would take the same shape as the wave packet that I am using. As the wave packet that I am using to define GXI. And they have to because of the uncertainty principle. So this is what a visualization of this down at Planck scale would actually look like. You would definitely get these bright smears. So that tells us something. It tells us that in these random waves, if you are looking at the face space, there really is honest scarring there. Okay, so it really, really does exist. Okay, so it really, really does exist. But of course, we're not, those pictures that Alex did, for instance, they're not face-based pictures, they're configuration space pictures. So it doesn't completely explain everything, but it does start to suggest what might be going on. And how much time we've got? Okay, good. So I am going to show you a little bit about how we do this. How we do this. So, we should use a very similar argument to what Burke Lebeau did to prove the L infinity norm for U. So, I'm going to compute my expectation of phi of x. Now, phi in this case, I'm going to choose to be tau to the pH. So, the idea here is pH is very, very large. So, it's growing logarithmically. So it's growing logarithmically in as h goes to zero. And once you're growing logarithmically, I can estimate the L infinity norm of G up above and below by the LPH norm of G. So the L infinity one is the one I want to get to, but what I will actually do is compute the LPH norm because that's the one that I've a nice formula for. So I'm going to directly So, I'm going to directly compute this thing. Okay, so this involves some very high powers, so it's not quite as easy as the case for F, but it is tractable. And then I'm going to use measure concentration. So let's just look at what the measure concentration tells us first. So if I plug in, this is my formula for the error. The error that's coming out of my calculating this thing rather than directly calculating the expectation. I can see that what really matters, so I've got this pH to the half. So remember the pH is logarithmic, so that's all right, that's on scale. So if I can make this error smaller than any constant times square root of log, then all I need to do is. Then all I need to do is compute that this thing is bigger than the square root of log. So that's what I'm aiming for. And the answer is yes, that this factor here, one on n to the kappa, you can make this arbitrarily small. And this is coming out of proving Lipschitz bounds about g. How fast can g grow if I put in different coefficients? So that is very That is very hopeful. And now I'm going to talk about how I compute the LP H to the LP to the power pH norm. So firstly, I'm going to assume it's even because G involves an L2 norm. So I want to really be able to expand everything out. So I want to spare a factor of two. I'm now going to re-normalize everything. Going to renormalize everything so that I have a standard Gaussian. So if I renormalize my coefficient C, you should think of C as being a vector in a high-dimensional space, each of whose entries is a Gaussian. So I'm just renormalizing the whole thing. And I write g squared in this format. So it's W transpose A of W, where A is given. A is given by this integral. And so all of the important stuff about our situation, about our basis functions, about p, all of our parameters, that's all contained in A itself. And so the idea here is I can make my life a bit easier by diagonalizing. So I'm going to diagonalize A, okay, to there. And if I take a Gaussian variable and I'm going to take a look at the A Variable and I act on it with a unitary operator, I still get another Gaussian variable. So that's, you know, all right. So I haven't lost anything by doing this diagonalization. What I have given myself is I've given myself a way of expressing this out longhand when I want to raise it to a high power. So if pH is equal to 2m and I raise g squared to the power m, I get all the different combinations. Get all the different combinations that I could get of lambda jk. So these are the eigenvalues of the diagonal matrix times y jk squared. So that's the advantage of going across to a diagonalized form. Now, what I want to try and do is prove a lower bound. You can prove an upper bound too, but the upper bound turns out to be reasonably trivial. The lower bound really depends on the Really depends on the eigenvalues of the matrix A. So, what you're trying to do is find one large eigenvalue. If you've got one large eigenvalue, then you know that you have at least one term in this sum, which is large. So that's what the argument really is about. And the reason why you have one large eigenvalue for D is, so if I come in here, I'm at Planck set. I'm at Planck scale. So at Planck scale, I can't really see orthogonality between the different directions, right? Because I don't have any space to see it. I'm absolutely at the scale at which everything looks the same. So that means that even though I'm taking an inner product here with different Xi J and Xi K, that will still contribute. That will still contribute, right? If I had cutoffs that were a long way from Planck scale, then I can start to see the orthogonality. And if xij and xij k are different, then terms start to drop out. But what is different about the Planck scale thing is that all terms contribute equally. And so you are able to construct that you have one large eigenvalue. And that then gives you the lower bound for this expectation. For this expectation. So that's an expectation at a value x xi, and it's uniform in x and xi. So I can just integrate it to get my LPH bound. So that's basically how you get at this thing, is you come as it sort of sidewards. You know, you're going to, I want L infinity, so I'm going to compute Lp for large P. I want the expectation of Lp, but what I'm actually going to compute is the expectation of Lp. To compute is the expectation of LP to the p, and then I'm going to use measure concentration to kind of rescue myself from that. All right, so I'm just going to finish off by speculating about what the filament structure is. So, we know it's not honest scarring. Okay, we've ruled that one out. We know that there is honest scarring if you look at the face space. Okay, that's the result. Okay, that's the result if you're down at Planck scale. What is it we're seeing in the configuration space? So, what I think what we're seeing is some correlation between the bright spots. So, we know from Burke and LeBeau that there will be bright spots in a random wave. So, there will be some points that are logarithmically larger. Now, if you had something that was completely, didn't have any structure in it, those points should be just sort of randomly. Be just sort of randomly throughout the whole wave. What I suspect is happening is that they are correlating along the lines that are scarring in the phase space. So for any particular random wave, you know that you will have some lines in the phase space that scar. And I would conjecture that what is happening is that bright spots in the configuration space are aligning themselves along those lines. Now remember these bright spots. Those lines. Now, remember, these bright spots are only logarithmically bright. They're logarithmically bigger than one. If I take an L2 norm along the line and I want to pick that up as a scar, I need practically all of them to be bright. Like every single point has to be bright. But I could have like heaps of them being bright and still not pick it up in the alternal, right? That it would still not be a That it would still not be a genuine L2 scar because there are just not enough of them. But if you looked at it just visually, and I bet you would look at it and say, yep, that is definitely some structure going on there. So I guess I'm coming down as somewhere between the camps in the sense that I'm saying that there is some structure. It isn't honest scarring along the lines. It is there. The lines, it is there, but your mind is also helping you along in seeing it in the sense that you are seeing something that is actually happening in the phase space and you're seeing a sort of an echo of it in the configuration space. But obviously, I need to do a bit more work to completely back that up. But here is, I guess, the start of hopefully a solution. So I will stop there and ask for questions. And ask for questions. So the filaments we're seeing are filaments of collections of bright spots. That's what I believe. Yeah. So let me see if I can. And there's just not enough of them to make a genuine scar. Yeah. If you look at this thing, you know, it definitely looks like there are quite a few bright spots. Are quite a few bright spots along these lines, right? But so we know from the calculation that there are not enough of them to make a sort of an honest scar, but you could still have heaps of them. And that would still look pretty persuasive, is what I am, I guess, conjecturing there, is that what you're seeing here is some bright spots that are correlating along lines, not enough of them to make a scar, but enough of them to be visually noticeable. Visually noticeable. And in that picture, do you see this smears also? No, this is just a configuration space picture. But I mean, you sure, but like this. Okay, so there's not some reliable way that the smears project. Yeah, so yeah, there's not, this is just a picture of a configuration space to. To do the phase space, you'd need at least three dimensions just for your to create your phase space, and then perhaps color to tell you something else. I guess my question is, so the smears, they're just pointing in sort of random directions in phase space. So when they're they don't make any, okay, they don't make any predictability about what directions the spheres might fall in. So, you know, you create a random wave and you will get little filaments going in one direction. All get little filaments going in one direction, and if I create one, there's no reason why that they should be correlated in any way, shape, or form. Cool. Sorry, I'm hogging out of time. Thank you so much, Melissa. Let's see if there's any other questions from the audience. Anybody? Okay, maybe I ask a quick question. So you said that you are sort of in between the two camps, but it seems to me that you're. The two cans, but it seems to me that you're mostly along the lines that these don't exist, you know, except there's a small face-based phenomenon. And is this a correct interpretation? I'm not misunderstanding things. But they also exist. Yeah, I guess, no, I would say ISOTA almost exists completely between the two. At least the two camps that were represented to me when I've talked to various people that belong to it. There may be other camps. People that belong to it. There may be other campuses around who had opinions that I just haven't heard. Jared? Yeah, I had a quick question. Melissa, is it an oversimplification of what you're saying to say that in some sense it's that there's enough variance that this thing is large somewhere in phase space? And that once it's large somewhere in phase space, it has to be large along the whole flow out of that point. Whole flow out of that point because of propagation of singularity? So it doesn't have to be large. Yeah, it doesn't have to be large along the whole flow out, but for like some short time. For a little while, there's some time scale along which propagation of singularities. There's some time scale, you know, that once you've got, that's what, that's why there's smears rather than points. Once you've got one, you have to have it's just the geodesic flow out, right? Yeah. At least for a while. Yeah. And even with these configuration space ones, you would have it. Configuration space ones, you would have it. But the smear in this case is just a ball smear. And so, you know, once you float more than time H, yeah. Yeah. Okay. Thanks. Cool. Okay. I don't have any more questions.