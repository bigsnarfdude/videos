Title is some recent progress in mandatory randomization. So, Qing Yuan. Yes. Great. Do you see my screen? Yes. Okay, brilliant. Yeah. Thanks a lot for inviting me to speak in this workshop. Yeah, it's a great shame that I couldn't be there in person. So the talk is about Mandela randomization, and Frank sort of. And Frank sort of already warmed up everyone to the idea. So, if I could use one picture to repeat, basically, the idea is that we'd like to use genetic variants or genetic variations to mimic what we would observe in a clinical trial. So, here's an example that we can use some variants in the HMGCR gene to hopefully in In mimic the drug effect of something like astatin, when we investigate the causal effect of LDL cholesterol on artery clake. So here is the natural experiment that's happening at conception, and here is a randomized control experiment that we may need to spend millions of dollars on in a clinical trial. So the talk is about two. So, the talk is about two recent works from our group. These are sort of quite different in some sense, but I thought it could be interesting for this audience who are thinking about analysis in the biobanks scale. So, the first part is going to be an answer to this question: What are the exact assumptions and potential biases behind Mendelian randomization? So, this is So, this is based on a paper called Almost Exact Mendelian Randomization, which is joint work with Matt Tutbo and George Chevy-Smith. And the second part of the talk, I will talk about the possibility of using MR to discover biological mechanism based on this paper just accepted at Biometrics, which is a joint work with Zring Gala and Trevor Hasty, where we proposed a method called PAST GPS. Proposed a method called PASGPS for discovering shared genetic architecture using GWAS summary data. Okay, so first of all, almost exact Mendelian randomization. So this is basically based on this observation that the first picture I showed you is not quite the natural experiment that happens when genes are transmitted from parents. Transmitted from parents to children. So, in fact, in the 2003 paper by David Smith and Abraham, they already noticed this and said that we think family menditary randomization would be more credible than the sort of population-based MR that is the most popular right now. So, we try to formalize this idea using some Using some mathematical models and some graphs. So basically, let's say we have some data which are genotypes and phenotypes from a trio study, so of the mother, father, and offspring. So I'm going to use M, F, and Z to denote mother, father, or the offspring. And I'm going to use a superscript F or M to denote haplotypes inherited. Haplotypes inherited from the father or the mother. So M Fj means is this is the mother's haplotype at locus J inherited from her father. And if I don't use the subscript, that means the genotype, which is the sum of these two haplotypes at that locus. So this work is basically based on two ideas. Based on two ideas in the literature. One is the transmission disequilibrium test, which is quite popular in the 90s and early 2000s. And then there's a more recent proposal from a Stanford group called the Digital Twin Test. So basically, the TDT, the transmission equilibrium test, the idea is that you should condition on the parental haplotypes when you test. The parental haplotypes, when you're testing the association for a particular locus. And digital twin tests sort of take that to the next level. And they basically use some existing meiosis models to simulate this whole distribution of the entire genome of the offspring given the parent haplotypes. So we can be basically a bit more specific. A bit more specific than that, than a digital twin test. So, the idea of our work is illustrated, can be illustrated with this rather complex graph. I don't expect you to sort of immediately understand it now, but essentially the beauty of these graphs is that you can understand them by looking at different components. So, here is a component where you have the mother and father's hypothesis. Have the mother and father's haplotypes, and they're correlated because of some population stratification. So, whenever this node is in gray, that means that variable is not observed. Okay, so A is not observed, and the second locus is also not observed in this particular example. But the other variables, other nodes in sort of in hollow circles or Circles or they are observed. So, this part of the graph, this is where the mother's haplotypes decides the offspring haplotype through meiosis. And there are some latent variables Q in this process that we don't fully observe. This part of the graph is highlighted. It's highlighted represents assertive mating. So basically, the idea is that there may be some unobserved variables, some unobserved phenotypes of the mother and unobserved phenotypes of the father that jointly decise, let's say, a reproduction event. So that sort of reproduction is always conditioned on because otherwise we wouldn't observe this offspring. We wouldn't observe this offspring. So, basically, whenever these, in this particular graph, these variables in rectangles are variables that are conditioned on. Okay, and this part of the graph represents dynastic effects. These are essentially environmental effects, how parents' behavior, parents' Parents' behavior or parents' phenotype may causally affect offspring phenotype. I think some authors refer to this as the sort of the indirect genetic effect. So that's sort of the main components of this graph. And then there's some assumption such as the Anline causal variant of this exposure variable x. Of this exposure variable X of the offspring is the second gene, and the only sort of direct causal variant of the outcome phenotype Y is this third gene. So a basic question in Mendelian randomization is whether one genetic variant is indeed a valid instrumental variable. So that basically has two requirements. has two requirements. The first requirement is that that genetic variant needs to be independent of the potential outcome given some of the variables you may select. And the other requirement is that that genetic variant needs to be at least associated with the exposure variable given these same variables. So graphically you can test these things using These things using so-called desaparation criteria for a directly cyclic graph like this. So, just to give you an illustration, so in this particular example, Z1 is actually indeed a valid instrumental variable. So, for example, there is this path that's highlighted in red that goes from Z1 to YX, but that is blocked because we condition on Z3. Condition on Z3M in this particular example. And there is indeed this other path from Z1 to X that has no collider and no variables being conditioned on, which means that Z1 is indeed associated with this exposure variable X. So you can check all the paths between Z1 and Y, and Z1 of X is a bit of a tedious exercise. A bit of a tedious exercise, but in this particular case, you can show Z1 is indeed an instrumental variable. And notice that for that to be true, we need to condition on the parental haplotypes at the same locus, which is the intuition in the transmission disequilibrium test. But we should also condition on these sort of direct causal. Sort of direct causal variant of the outcome to block this kind of horizontal paleotropic path. And we also need to condition on the parent haplotypes at those locus. So I can't go too much into detail for this work. It's really quite a substantial article. Article. I just give a sort of a quick summary of what we've done. So we gave a review of the history of the Mendelian randomization and an explanation of different components of this graph and how this may work at a genome scale. So not just three variants, but maybe for a whole chromosome or the whole genome. We have a discussion of various bias-inducing paths. Bias-inducing path and some conditions for when an adjustment set would be sufficient to render an instrumental variable valid. There is an almost exact randomization task, which follow from previous ideas in the instrumental variables literature. A lot of these can be greatly simplified under Hodan's Hidemarkov model, particularly if you assume no mutation. Particularly if you assume no mutation. And I think what's probably the most interesting thing for me is that you can possibly find these perhaps very weak instruments after you've conditioned on some neighboring genetic variants to block horizontal pleiotropy, but you can combine these weak instruments using some multiple hypothesis testing techniques. Multiple hypothesis testing techniques. And we also have some proof of concept real examples. Okay, the second thing I will discuss is PathGPS, which is short for pathway discovery through genome-phenome summary data. So the idea is that we would like to discover clusters of SNPs and traits that correspond to the same genetic pathway. Same genetic pathway. So, here in this sort of toy example, we have two genetic pathways, which is capital M. So, these X are SNPs of genetic variants, Y are some phenotypes. We assume this capital M1, M2 mediates these genetic effects, and this little M1 is an environmental confounder of Y3 and Y4. So, ideally, we would like to sort of Like to sort of rediscover these red variables and these blue variables using statistical analysis. So we assume a simple sort of linear structural equation model for these variables. So M is a linear combination of some genes, and these phenotypes are some linear combination of these latent mediators and some. And some environmental confounders. So basically, how this works is that we would like to basically discover or estimate what U and V in these matrix, what these matrices are. And how we're going to do it is that we're going to plug in M equation into the Y equation. So we see that the sort of effect of X on Y have this. Of x and y have this coefficient UV transpose, and we're basically going to make the assumption that this UV transpose is a low-rank matrix. So there are many more SNPs and trace than a number of sort of important mechanistic mediators. We are going to also assume these UV are sparse, which means that there may be only a few influential SNPs or influenced traits. Or influenced traits in any particular pathway. So, under these assumptions, you can basically show that if you get the marginal sort of GWAS statistics beta had for every genotype-phenotype pair, and you take essentially the inner product between them, then that has this kind of low-rank structure. And if you do a single Structure. And if you do a singular value decomposition, then the right singular values will correspond to the genetic effect V, but also annoyingly this environmental effect W. So one of the first challenge in this work was to sort of tease out this genetic component from that environmental confounder. And the way we proposed to do this was to use some negative control SNPs. Active control SNPs, which could be empirically estimated from some Chiwa's study, and then just assume that they correspond to those that has u equal to zero. And then that will mean that when we take the covariance matrix of these negative control SNPs, the first genetic part will vanish. And then if we take the difference between beta hat transpose beta, Between beta hat transpose beta hat and beta hat transpose, and the same thing for the negative controls will cancel out this environmental part and then left out with the sort of genetic component. The next thing, which I probably don't have much time to describe, is that we also noticed that if we just take the singular value composition of that difference, Of that difference, and then try to sort of cluster the SNPs and trace by the non-zero entries in the estimated V and U matrices. The results are quite unstable if we sort of change the traits slightly or the SNPs slightly. And there are a lot of hyperparameters, for example, number of principal components and clusters. And clusters and etc. And that all give us very unstable results. So, the way we handle this was through a bootstrap aggregating, which basically means that we're going to sub-sample the genes and the phenotypes and get a lot of these sort of estimated clusters and try to then average over these results. So, we tried these on two data sets. These on two data sets. One is a metabolomics data set, more or less just for a proof of concept, because these traits are already well understood and well classified. And we found that this result allow us to sort of recover most of the sort of the existing character categorization of these traits. So, here on this panel, I'm showing you the human. Panel, which I'm showing you the UMAP embedding of these traits and some of the genes that were discovered. And you can get different essentially different clusters that corresponds to liquid proteins with different densities. And then each cluster are associated with, are also associated with some genes that some of these are known sort of functional genes. Sort of functional genes for those subfractions. We also applied this to the UK Biobank. I don't think I really have time to go through this. So as with this kind of large-scale examples, so this type of method discovers a lot of interesting relations. Some of them can be validated by the literature, but a lot of them are just interesting. Interesting things that we don't know if they truly correspond to a causal sort of mediator. I'll just stop there in case there's one short question. Thank you. Thank you, Jimian. We have time for 12 questions. This is really elegant work. I had a question to make sure I clear a little bit.