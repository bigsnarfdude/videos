So, thank you so much for being here. Dr. Lee, the floor is yours. Do you see my screen? Yes. Yeah, okay, good. Yeah, thank you for the introduction. Yeah, my name is Minu Jakely, usually called by Jake. Sorry, no, we are not. Probably the screen that you are seeing is not yours, I guess. Oh, okay. Let me see. Are you sharing the presentation? Yeah, let me see. Yeah, let me see if I can share you again. Sure. Do you see the screen now? Yes, yes. Thank you. Okay. Yeah, sorry for the hassle. All right. So, yeah. I'm Jake from the University of today. Today, I'm going to talk about the evidence-driven, a little bit different aspect of the interpreting machine learning, and then major kind of the problem that I'm tackling is reinforced learning. So, I'm going to talk about that in the following slides. So, what is evidence-driven learning? Evidence-driven learning is the way that we are paying more attention to specific samples, not the part of the samples or attributions or the Or attributions, or the method that were local features, or something. It's just kind of looking at a sample. Why do you care about the sample? This point of relatability and also performance as well. So let's say we have something happens and maybe in here is kind of the trivial case, it's a broken jar or something, but maybe it's just kind of severe like that the car crash involved a self-driving car or something. Of the self-driving car or something. And then what we may do is replay all your past history and looking at and check all the trajectory to find the reason why this happened. Or another way is looking at some sparse samples and to find the reason why. Or explain, it doesn't necessarily fail to. It doesn't necessarily failed or negative cases. If it works fine, we can extract a rule by looking at the samples better than the old experiences that we are going through. So that's today's topic that I want to talk to you. And then let's step back and generally talk about why do we actually need interpretability? And then, Christy on Tuesday, went over this pretty well. And I may not need to repeat. And I may not need to repeat that one, but strategic pillars of the trustworthy AI actually cannot be implemented without the explainability or interpretability. And then it actually the key to the reliable or the robust solution setup. And also in case of the reinforcement learning, it can be related to the security or safety. To the security or safety of the systems as well. So, but today, what I want to kind of talk more is the opening of black box cases, not intrinsically or designed for the interpretive model, but still there are some cases that we actually need to deal with, pre-implemented or deployed black box systems. So, sometimes, so neural network, for example, or deep. neural network for example for deep learning we actually kind of categorize and the accessibility of the existing models we can categorize them as the self-interpretable systems when you can actually fully control or fully update the model you can read the weight of this the network and we can actually modify the weight or if you have only permission to read only of the about the existing weight of the neural networks and then Netflix. Then the interpretative capability can be implemented a little differently. And then when you don't have any full access of the model without looking at the inside of neural network, the only thing that you can kind of generate interpretation or produce the interpretation is just simply re-approximate the model by looking at the behavior. So you're kind of generating the So, you're kind of generating this behavior similarly to the existing black box model, but with some interpretive models. So, we call that as a real approximation model. And then this is the story about the real approximation approaches for the reimbursement money. So, let's take a look at the generality about interpretations. We have seen this week actually an amazing interpretation. An amazing interpretable machine learning models, and then it's out there for a while. So, there's some saliency approaches and the tension-based approaches for the natural language processing. Feature visualization has been here for a while. And the grand tour, just like a linearly projecting the data to some different axis and looking at the generated different interpretations. And also, feature, individual features, again focus it and looking at the Again, focus it and looking at their interpretations and editing them or dimensional deductions. We haven't seen many of these, and those are all fascinating source for interpretations. So whenever those sources are given, now it's a human, they are scientists or there are biologists, anybody who actually working on to understand what the solutions or what data actually give us. And then that's a human job actually for the interplay. And that's a human job, actually, for the interpretations. And then, when we have a source of information, and then maybe we can think about is there any other sources of informations or interpretations? And that's what we are focusing. And then we are looking at the evidence. And we are looking at the evidences as particular, maybe physical visual information. Fill is the visual information, and then we want to come from the all the like surveillance videos. We are not looking at all the data samples, we are looking at the particularly most representative samples to kind of reconstruct your memory. So, we all memorize something, but not every single moment. Just key moments that we memorize. And those are the key for the interpretation and also for learning as well. And also for learning as well. That's the basic idea of the evidence-driven reinforced learning. So before we jump into the actual models, let's talk about, let me introduce the reinforced learning in general. So reinforcement learning is the way you train intelligent agent by giving some reward. So like a training adult. So whenever you observe the Whenever you observe some positive behavior and you are providing a reward, and by doing that, you can kind of some positive action that related to that behavior can be reinforced. That's why called reinforcement money. And then how it trained. So we started with just no knowledge about the environment. You do not know what problem you are solving it, but you are taking random actions. Actions and then the action goes into the environment, and then that environment kind of generates like a changes about its action actually causes some changes of environment. You can observe the changed environment as your next state representation. And also, occasionally, you can get some reward. So, when I say the occasion reward, which means there's no guarantee that you get a reward immediately after you take it. Immediately after you take an action. The reward can be delayed. For example, your agent is playing a chess, then you may only get a reward whenever you win the game, not necessarily every move of your chessboard. Okay, so it has been applied in many different areas. Like you can easily think about taking an action. Think about taking an action, it can be the control problems in the robotics or industrial robotics and so on. Or it can be decision-making problems. So you can trade the stocks or in the finance market, and those are also another decision-making problems. Or sequential decision-making problem, that's just one decision. And it can be related to the recommendation system problems. So you want to Spotify recently. Spotify recently imported reinforced money and deployed that in developing the algorithms to help their customers' long-term satisfaction with the reinforced money. The healthcare, and recently DeepMind made this amazing work with the reinforced money to control the nuclear fusion and has been. Has been kind of been a lot of excitement there as well. So reinforced learning has very interesting and many successful applications, but still a lot of challenges exist because of the gap between the simulated environment, the actual environment, because it may work well in your virtual environment, but may not in your actual environment. But may not in your actual system. And then, how we can reduce this gap, how to make this one work, is one of the reinforcement agent algorithms that we may need to deal with. So we can summarize the reinforcement challenges in two extremes. And one branch is the robustness and scalability of your algorithms. Your model that works. Your model that works well in the one problem might not work well in the other. Or if the complexity problem grows, you might not, your model might not scale well either. And also in another branch, and taking a looking at this, so you in the perspective design the reward, you say you set some kind of reward to promote the certain behavior. To promote a certain behavior, but it may not capture the intention that you designed. It may end up with the different behavior. So let's say you want to kind of build a mock AI that just clean your desk. And then let's say your reward is your desk is clean, you get the reward positive. And maybe the robot ends a little. Maybe the robot ends up with swapping off on all the stuff out of your desk and breaking it. And it's clean afterward, then still be satisfying to maximize the reward, but that's not what you want, right? So those are the kind of the challenges that we have to deal with. And then it might not be able to accomplish, or sometimes it's pretty hard to, why it's working, why it's not working. Working why it's not working, and finding good explanation of the deep reinforcement is pretty hard. So that's why we pay attention to the evidence. Another aspect of the reinforcement learning, the data for the reinforcement learning is sequential, so it's not independently identically distributed. So IID assumption doesn't meet anymore. Assumption doesn't meet anymore because the current observation, if you take some action, that changes the next observations. So it's highly related to previous observations next. Then somehow kind of the capturing the most representative samples out of it, then you may, when it is deviated from the one observation to the other, then you can have some good evidence. Have some good evidence set and to form it. That's how we're going to design this evidence collection using deep reinforcement monitor. So as I said, we do not have any full access to the model. So here, we actually looking at deep reinforcement money, how it generates the behaviors. And then by looking at that, we attach a sparse vibe and reinforce money. Partial agent reinforcement models to estimate the similarity between the task observations and actions, and then using some additional filters in the middle, we kind of collect then the snapshots of our memory. And then eventually those set of the snapshots that are stored in the storage will become the evidence set for future interpretation. For future interpretations. So this is the typical learning curve in the reinforced learning. So x-axis in the bottom graph here, it actually shows the number of iterations, how many times. So whenever you see an observation, you take an action, and then if that gives them, you kind of see the reward and the total reward, and then you can move on to the next action, the next section, extra. Next section, next section, next one. So, and in the mean, in the middle of this interaction process, you learn about how to behave, how to play game in this case, and you increase your model to earn more total sum of rewards to play the game. So, in the beginning, you're kind of the explorer of the stages here, and then from the Here and then, from the exploration, you can see that the many different snapshots or evidence that you have collected on the top figure. And afterward, you may use sometimes the land analysis or some also to explore the new actions to see that if you can improve the policy or not. So, that process kind of continuously builds up. Of the continuously build up and adding more evidences, it's kind of the number of the images actually grows here as a collective evidences as your learning progress. But once it kind of converges to some policies, and it might not increase the evidence anymore because your observation is kind of limited. Right? So, then what do we do with those collected evidence? The evidence we can learn and take a look at and how in kind of generate our own interpretation from it. So let's take a look at a simple mage example. So the black area is actually wall in this mage settings. And then green area is the pole position. And then this, the pink one is actual agent. So agent starts from the top left. So, agent starts from the top left corner and kind of develops the way that you kind of define the path to the goal. And the green line actually shows the ultimate policy through this to the goal section here. And then whenever this kind of the corner line is something that I really think is the important moment that we may need to pay attention. And actually, you can see that this all this moment on top, this image on the right here actually shows the The right here actually shows that this top three relevant evidence that in our story, and that actually shows that these four different corner scene is actually stored in our stories for us to implement these general product interpretations. And then the number on the right, X slash, actually show the similarity between. Slash actually shows the similarity between the current position here and then the evidence that we have. And then the number on the left is actually shows the weight, how this is critical, showing that how this moment is critical to obtain the optimal policies. So given that presentation here, so we can see that this most important decision moment is actually captured in our SNAP. In our snack, the evidence set. And then we can see that what action needs to be had to take. So, on the arrow here is the direction to go. And then as a policy, and then green actually shows the positive reactions, and the red shows that this negative actions. So, and given that observation, we can see that this how what kind of actions most recognized. Of actions most recommended in certain cases, the corner cases in the left here. And the right figure in here actually shows the evaluation of the long-term policy. So we call that as a Q value. So for the given state, and then what is the best value function or given state? So you can see that the Given state. So you can see that in each location, you can see the low values from around the wall area, because you cannot move in there, you don't have any experience there. And then around the goal, you have a higher values. And then away from goal, it has a lower values. And then our experience in here, starting from the top left to reaching the goal here, you can see the growing importance when you get closer to the goal. Get closer to the goal, and also if it is heading toward the goal. But if you see that the corner samples in here, and it is way away from the optimal policy, and then your importance of the sample can be lower. But maybe still it's important the evidence to analyze the failed cases if your policy is close to these samples. So let's take a look at the more kind of the fun games in here. It's Ms. Pac-Man in a tar game. And after the kind of 300 iteration of the training, this is like a top three snapshot that we have observing for interpretations. And here, all three examples in here, basically in a different location of the Pac-Man. And an interesting thing here is this all actions and identity. It's all actions and identical, and then soccer means no actions, and the red means it's negative. Nobody tells us this, doing nothing in a Pac-Man game is not good. Because you might cope and you are not earning any point, you are spending up, kind of wasting your time, maybe, and the ghost will come and unfortunately this is not good. But that's all at the level of this small number of experience. Small number of experience. And then if you train for during the 1.2 million iterations, after this is some additional kind of observations. And then what it tells actually us. And then here, in the first figure here, actually, Pac-Man is moving left. And there's no food on the left, but there's food on the real right. So moving away from your food is not good. Away from your food is not good. That's what it tells. And then the other two figures, and actually, your pygmy is moving toward the food. And so, cracking for food is a good power that you have alone in a good way. And then after the 2.4 million, we can see that the very interesting observations here. That's pretty kind of the intuitive, though. So, in here, the Pac-Man is in here, and then right next to your, there's a ghost. So, and then heading toward the ghost is not good. It's what this tells. So, don't go near the ghost. And you're starting to learn moving away from the ghost. But if you take a look at the two and three, it is like you'll kind of the The you have a food and you have a ghost at the same time, but and here it actually all favors to going toward the food regardless of the existence of ghost. And that's why it hasn't finished its learning. We can see that it may need more samples that need to make a correct decision when you are food and ghost near at the same time. And then eventually. And then eventually it will develop the good policies. And so we can guide the reinforcing agent to better train or in a more data efficient way. So that is the interpretability of the interpretation given the evidence-driven learning. And then let's think about that. So there's always interpretability and accuracy or performance trade-off in how we can. Problem is trade-off and how we can overcome it and using the evidences. So, one way that we are looking at is the improving experience replay in reinforcement learning. Reinforcement learning, a deep reinforcement learning error, we use experience replay to avoid the sampling bias. So, if you are looking at the most recent samples, it is based upon the current policy and the data, the observation is pretty close. The observation is pretty close to each other. So that's why we have a very large data kind of buffer, put all the past experience in it, and then randomly sample the data from it so you can train the more unbiased powers from it. But we do not have indefinite size of the memory. And so you may have the at some point, you may have a limited size and that can cause of this. And that can cause this still bias of the sampling. So the way that you can avoid and kind of minimize this, kind of the memory limitations is why don't you just store the relevant, the evidences here instead of all your experiences? And that actually helped with improving the performance. We tried adding to the different Atari games in here. Here's 11 Atari games. Here is 11 attire games. And in many cases, they actually improve the performance of the learning, and sometimes it's marginal, but it's still better than the existing approaches or the vanilla approaches. And then if we're switching the gear of this, the reinforced learning agent in the longer horizon, so let's say it is learning throughout its lifetime. Throughout its lifetime, it is often called lifelong learning or continued learning problems. And then here, the problem is you do not have any guarantee of your environment, it stays the same. You may tackling the different problems as it evolves. Maybe the environment changes or the located the different locations and to solve different problems and so on and so on. Since the one and so on. So the major problem of the continual learning is catastrophic forgetting. So let's say you have the prior representation from the previous sample in black circle here, and your representation can be modeled using the F of T and then after the After the next iteration of the T plus one, your model can be kind of shifted toward the new samples in the blue circle here. And then it captures a model is now forget the past data. And then this can be mitigated by using the evidence. We construct the evidence in the past experience, not everyone, but the disrespect. One, but this red circle may kind of sufficient enough to cover the old knowledges and maintain for the way while even you're learning the new examples. So this is the kind of the modeling that is what we are doing is here in the middle, based on the snapshot using the monitor and the experience replay, we actually regularize the later representation. So, using this regularization process, we can make this continuum learning benchmark in the catcher and the flatly voltage experiment here. We were able to make this one kind of very robust to the changes of the test. So, here, the experimental result in the briefly saying that here we are looking at in a vertical y here actually shows the changes of task. shows the changes of task and then each figure actually shows the performance on the the task each task so task one and task two has a different parameters and different environment settings but you are actually learning and you know you know the after the 25 000 iterations you are actually dealing with a different task after the like 50 000 iterations you are dealing you are learning you're you're moving Dealing, you're learning, you're moving to the different tasks, but evaluated on the original task. If you're just looking at this, the plot with the catch-own top row, interesting thing is, even though you are learning on the second task, your first task is continuously learned from the different task. So it's not kind of the adapting, adapting, not only for the adapting the new task, you are making. You are making the previous solution better based upon your snapshot and the representation regularization. So, the other results actually shows the similar things in here. And this one is actually evaluation, it's not the test task, evaluation of the individual task performance and without snapshot, without the regularization, with the regularizations. And actually, with And actually, with having both regularization and then snapshot replay, you can see that the green is the most stable performance comparing to the others. The others kind of oscillate a lot during this switch of the task. And this is a kind of simple test representing the plot of comparing this with the snapshot plus the regularization and without regularization here. And without regularization, here actually shows that you may have better structured high-dimensional representation. So you can actually better relate the task and better leverage your own task-specific informations throughout the learning. So the closing, what I'm kind of presenting here, this evidence-driven or evidence-based learning. Learning is a new way to look at this problem solution and/or providing the interpretations and also provide the efficiency of the learning and then solution to the continued learning by mitigating or forgetting problems. And this you have samples for the interpretations. And now you can use it combined with the attribution method to looking at the saliency of this particular information. Particular information, the samples. And also, you are not kind of relevant to tie onto the specific algorithms. So you can use any machine learning algorithms. And also, you can use evidence. You can collect an evidence as any stages of the machine learning. But here is the one key fundamental question that you can ask is, is the most representative sample session? Presentative samples and this actually can be an evidence. And then the answer can be tricky because it can be different case by case of the problem by problem. Even for the same problem, depending on what case that you are looking at, it can be different. So we might need to rely on the human interpretation in that case. So how we can develop the good communication method with the human and how we can kind of use a different or design of the new connotation. Use a different or design of the new kernel tricks to strong evidence selections or non-Bayesian approximation to make those processes faster and sparse or local evidences collection for their interpretation thereafter. It's an interesting direction that you can go in. So these are my students. Actually, Zhang recently defended his dissertations. Those are the, and we try to close it. And we try to close it for collecting the questions from the audience. Thank you so much. Thank you. Okay, we're a bit late on the schedule. Maybe we have time just for one quick question from the audience, if any. Okay, I have a question and it's about the possibility of transfer. Of transfer what has been learned from one model to another in a completely different environment. So I was wondering whether it makes sense, for instance, to have like a shared collection of those snapshots, but from completely different models. And this would make maybe easier to transfer common information between different in different scenarios. That's very good question here. So, here the samples can be quite different. So, if it is two different, so we may need to kind of assume that what are the difference that we have different, is it the same environment for different tasks? Do we have same observations or not? Then, how does the, if the observation is different, how do we can relate those difference in a higher level representations to be connect? Presentations to be connected. So let's say I do not have any camera readings, but I only have this only snapshots or evidences from the cameras. And then the other problem that I'm solving is the natural language processing. Might not work, but maybe, yes, we can transform these visual interpretations into the natural languages so we can actually deal with such changes. But these reducing the transformation is another. For the transformation is another thing that we may need to consider. But there's a potential that we can do for conceptual or meta-level learnings. It can be applied on there. And then another aspect is not just the observation, it can be the actions. If it is two different tasks, do they share the same action space? Maybe, yes, maybe not. If they don't, and then there are And then there should be some other thing that we may need to think about: how we can map the difference between the actions here and there. So another higher level about the strategy or decision making or action choice to map the action spaces can be additional thing that make. But to answer your question, there's a potential, but it may very hard way to... But it may be a very hard way to go there. Okay, okay, thank you so much. Let's thank you. The next speaker, who is Anfi Ngun. He's a PhD candidate in the computational system biology group at the BM Research Institute, and he's going to talk about construction.