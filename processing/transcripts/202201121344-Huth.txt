Is going to speak next on mapping time scales of cortical language processing. Take it away, Alex. Great, thank you. Hey, everyone. I'm excited to be here. And I'm very glad for Antonio's talk just now because he introduced a lot of ideas that I think are going to be useful for understanding what I'm talking about. So yeah, I'm going to talk about mapping time scales of language processing and cortex. I want to briefly start by talking about... By talking about the people who really did this research, who made this all possible. We did this in collaboration with a team at the Brain-Inspired Computing Lab at Intel, with Vivo, Nicole Bekage, and Javier Turek. And the people in my lab who were working on these projects were my master's student, Shivan Gimato, PhD student, Shelly Jane, and RA Amanda LaBelle. So, this is all their work that I'm going to talk about here. Talk about here. So, to give a little bit of context of what I'm saying and the kinds of models that I'm going to show, there's been a real explosion in the last couple of years of people using deep neural network language models. So these are neural network models that are trained to predict the next word in a text given the previous words. This is what we call a language model. So a lot of people have showed that these deep neural network language models are These deep neural network language models are really incredibly effective at predicting human brain responses to natural language. And the way that this works is that the internal representations of the language model are extracted after they sort of read each word in a piece of text. And then those representations are mapped to brain activity using usually a linear regression model. I think the first instance of this is a paper from Layla Oheby in. From Layla Webby in 2014. Layla gave a talk yesterday. She does fantastic work in this area. My student Shayley and I had a paper showing this in 2018. Layla has shown this in a couple other papers, and there's been this sort of exponential growth in the number of papers that are doing this type of analysis recently. But there's been a big problem here too, which is that these models are really challenging to interpret. Like we don't know how the Like, we don't know how the language models work. We don't know how these deep neural network language models work at all. We don't really know what information they're using. We don't really know how to interpret their representations. So the kind of question that we're trying to attack in this piece of work that I'm going to talk about today is how do we use these models to do science, to like understand something new about the brain? So I hope I can sort of point you in this direction and show that we have at least one. And show that we have at least one handle on doing something with these models. So, with that kind of context out of the way, let's talk a little bit about time scales and memory. And so, this relates very closely to what Antonio was just talking about. The basic premise here is that in order to understand language, in order to process language, we need to remember information at many different time scales. So, I'm going to read you a little snippet of text here and then talk. Read you a little snippet of text here and then talk a little bit about the different kinds of information that you need to store in order to understand this. So the text goes like this. We drove to the pizza shop to pick up our order. Then I dropped Wendy and the food off at the park. She would greet the guests there and make sure they were fed while I fetched the other supplies. Planning this party drove me nearly crazy, but it was worth it. Easy enough to understand, but it turns out that we need to do a lot of different things to actually kind of get what this text means. Kind of get what this text means. So, at a long time scale, there's information about the topic, about like what's going on here, right? So, you see phrases like the park, the guests planning the party. You infer from sort of combining all this information that this is about a party in a park, right? Like this is describing the preparation for a party in a park. And this requires integrating information over this whole brief narrative here. At a sort of medium time scale, things like pizza and the food, these are referring to the same actual things in the world, Wendy and she. So this is co-reference to sort of resolve this, which I think we mostly do pretty flawlessly and without any trouble. We need to have some memory of things that happened over the last sentence, the last few dozen words, to pick up that she refers to this person that was already mentioned. That she refers to this person that was already mentioned, or the food refers to this bit of food that was already mentioned. And then at the sort of shortest time scale, we can have something like this, where this particular word drove appears in two different contexts here. We drove to the pizza shop or planning this party drove me nearly crazy, where this word means different things in these two contexts, right? You don't need to hear the whole narrative to understand that drove means like literally we were in a car. Drove means like literally, we were in a car in the first instance, and is in this metaphorical sense in the second instance. But my point here is that we need information at many different scales, right? So we need some sort of short-range information to understand language, to parse a sentence, to figure out the syntax of a sentence. We need some medium-range information, and we need some long-range information as well. So let's talk about like Let's talk about how we can use different styles of memory in computational models and how that can kind of give us a handle on understanding what these models tell us about the brain. So I'm going to try to address three questions in this talk. The first is, how much of each time scale do we need to understand language? And this is more sort of a theoretical and a computational question. A second question is, how do we build a computational model that explicitly captures these types? That explicitly captures these time scales. This is going to be a sort of neural networks computational question. And then the third question is: do different brain areas handle different time scales of information? We kind of know that they do, but can we use what we sort of learn from the first two questions to try to answer that question or compare to answers that other people have found? So let's start with the first one here. How much of each time scale do we need to understand language? Language. So to start sort of figuring out what we even would need to know to understand this question, let's suppose that what we need to do to understand language is model the statistical dependencies between words. So this is sort of solving this language modeling problem. One thing that's been noted several times, I think maybe most famously in this paper by Lynn Tegmark from 2016, is that From 2016, is that the mutual information between words in natural language decays as a power law with those words separation? All right, so I'm illustrating that on the left here. I cannot see my mouse. That is annoying. I don't know if I can turn that on. Okay, crap. Well, on the left here, you can see I'm plotting just sort of an example of what this might look like with mutual information. Of what this might look like with mutual information on the y-axis and some sort of normalized units and separation between two words on the x-axis. And I'm plotting this sort of power law-like relationship where the mutual information is proportional to a power law as like t to the minus d. So it's a power law with some power of d. Typically, looking at things in these linear scales is not what we do with power laws, though. We like to look at them in log-log plots, like we're showing on the right. So here, So, here we're plotting log mutual information versus log separation, and here the power law becomes this diagonal line. All right, so this type of relationship is kind of what we'd expect to see in natural language, is that you have this power law-like amount of information. You have a lot of information, a lot of mutual information between words that are very close together, less mutual information between words that are very far apart, but the decay is kind of slower than you might expect based on other processes. Expect based on other processes. So, what does this mean in terms of like time scales? This kind of directly tells us how much we need to remember at each time. But one problem in translating this into computational models is that a lot of the computational models we use, I'm not going to talk about transformers here. They're kind of different topic. I'm going to stick to recurrent neural network models. In these recurrent models, in Models in standard RNNs and in gated RNNs, which is what we're going to use, memory decays exponentially. This is just kind of a fundamental thing about how these models work. When you have recurrence with kind of the forget gate style of like gating of information like LSTMs or GRUs, or just sort of naively passing information from time step to time step like standard RNNs. This just results. This just results in like an exponential decay of information. That whatever information you have at one point will tend to, I mean, it'll either explode if your network is unstable, or it will decay exponentially to zero over time if your network is nice and stable and performs well. There's a really lovely paper from a few years ago that goes into sort of analyses of this exponential memory behavior in these networks. Memory behavior in these networks and does this fabulous thing of like rederiving gated recurrent neural networks, so like LSTMs and GRUs from almost first principles. It's by Telek and Olivier. I really strongly recommend this paper if you're interested in RNNs. I love it and it's a basis for a lot of what I'm talking about here. But basically, the point that I want to draw right here is just that the memory in these networks tends to decay exponentially. So on the right again, here I'm showing On the right, again, here I'm showing this power law-like relationship that we expect for language. And on the left, I'm showing the type of relationship that can kind of easily be captured by an RNN, by some kind of recurrent neural network. And these things are pretty fundamentally incompatible, right? So you can't choose any single time scale of an exponential decay and have it act like a power law. Exponential decay will law. Exponential decay will always go to zero long before or you know to arbitrarily close to zero long before the power law will. So these two things are kind of in conflict. So our question here is going to be how do we actually build something that respects these power law-like relationships out of exponential memory, exponentially decaying memory? So what we did in this paper that Siobhan That Shivangi wrote and published last year at iClear was we showed that in order to approximate this power law decay, we can do that using exponential memory, at least approximately, if we use a specific mixture of exponential time scales. So this mixture is given by an inverse gamma distribution. So what this means is that if we have time That if we have time scale in words in this case, because our network is like reading words one at a time in text, then the sort of amount of information we need at each time scale should follow this inverse gamma distribution that you see on the right here. And what that means is essentially that this would have like a lot of short time scale memory, a bit of medium time scale memory, and a little bit Scale memory and a little, like, very, very long time scale memory. And when you add all these things up, they very nicely approximate this power law curve. So this relationship was pretty interesting. And I think this was a neat insight that you can sort of turn exponential memory into this power law-like relationship, which really, I think, reflects natural language much better if you use this specific combination time scale. So, this is kind of answering this question. Combination time scale. So this is kind of answering this question that we had of like how much memory do you need at each different time scale. But now, how do we actually turn this into a computational model? How do we take this insight and turn it into a computational model? So this is where the sort of other insights from this telec and Olivier paper really came to bear. So the model that we're basing this on is it's a long, short-term memory LSTM language model. This is a sort of basic tool that's been used in NLP for a while. It's not state-of-the-art. Used an NLP for a while. It's not state-of-the-art anymore, but we understand it sort of theoretically much better than we understand like transformers, which are state-of-the-art. So we're focusing on this for now. More or less, what we did is we took what was the state-of-the-art LSTM language model, which is from this Merity et al. paper. Essentially, what this does is it predicts probability distribution over the word at time t given the words at times one through t minus one. All right, so it's trained with. All right, so it's trained with like a cross entropy loss and back prop through time. So, in order to establish this kind of distribution of time scales, we used one of the other sort of tools from this Telec and Olivier paper, which they showed that the time scale of actually each unit in an LSTM, so each unit in this network, is controlled by this one parameter. Controlled by this one parameter, which is the bias of its forget gate. Now, I'm actually not going to go into the sort of details of how LSTMs work and why this is true. Again, you should look at this paper. It's a fantastic paper. But more or less, the bias that forget gate controls this average value by which the state gets multiplied at each time step. So, of course, if you make that average value very close to one, then the state gets multiplied by values that are very close to one at each time step and the memory decays. Close to one at each time step, and the memory decays very slowly. And if you make the bias negative so that the average value is close to zero of the forget gate, then you multiply your state by a value that's close to zero at every time step, and then you have very short time scale memory. So you can see how kind of this is controlling actually the rate at which memory decays in each unit of this network. So what we did using this kind of information is we actually assigned each unit in this network. Actually, we assigned each unit in this network a specific time scale ranging from short to long based on this inverse gamma distribution. So, I've sort of drawn that here as this like linear gradient. That's a lie. Something like 30 to 40% of the units are very short time scale. And there's another 30 to 40% that are medium, and then only a few that are like very long time scale. But that's the essence of what we're doing. But that's the essence of what we did to sort of build this model that captures time scales. We found that this improves language model performance. So if we train this model and test it on new data, we get a bit of a bump in predictive performance of new words. Essentially, we just see this as like, this is a nice regularization for the model because it's sort of pushing it towards something that it should already know. We also showed in this paper, I'm not showing that here, that the model. The models actually learn to approximate this reasonably well on their own. If you just train them on natural language, they do actually sort of find something that looks a lot like this distribution. And if you mess with the statistics of the corpus that the model is trained on so that you don't have these the same distribution of dependencies, then you really change the distribution that it learns. So I think this idea that the This idea that in order to do language modeling, to understand language using exponentially decaying memories, that you need this kind of inverse gamma distribution of time scales. This really seems to hold true. And I'm happy about this result. Okay, so now I've shown you that we have this computational model. We know kind of how much of each time scale we need. We've built this computational model that kind of captures this information. So different parts of the model explicitly. Explicitly represent information at different time scales, right? So for each unit in this network, we know what the time scale was that we assigned to it. So we know that like this is a short time scale unit or this is a long time scale unit, which that's actually the thing that we're going to use now to try to interpret what this model is doing when we fit it to brain data. All right. Yeah, so let's move on to the brain data part. Okay. Okay, so the way that we're going to be looking at this is using this natural language fMRI experiment. This is sort of the stock thing that we do in my lab, and there's now a bunch of labs around the world that are doing like similar things. The basic idea is we just have people lay in an MRI scanner and listen to stories, listen to natural language stories. In the data that I'm In the data that I'm that we sort of present in this paper, uh, we had six subjects uh listening to 27 stories each. From uh, these stories were taken from the Moth Radio Hour, which is a very like fun podcast radio show with people telling like good, interesting stories. I've been in a lot of MRI experiments. Uh, this is by far the least painful and most fun MRI experiment I've ever been in. Um, just laying and listening to stories is something we do all the time anyway. So, uh, so might as well. The time, anyway. So, uh, so might as well record our brain data while we're doing it. Uh, if anyone's interested in like the fMRI details, um, whatever, we can talk about that. Uh, later, uh, we have maybe the one important thing is that we use a TR of two seconds. So we get an image of the whole brain every two seconds, um, and we're using these 2.6 millimeter voxels. Okay, so we have this data with people just listening to stories. We have for each Listening to stories, we have for each voxel in the brain, we have a time course of how it's responded across these 27 stories, many, many hours of data. Now, how do we try to model this data using the computational model that we developed a moment ago? So, here's the basic idea of our sort of encoding model framework. So, on the bottom here, we have the human fMRI responses that we get from brain. So, for one voxel in the brain, we have a response time. One voxel in the brain, we have a response time course, this R of T, which we get from playing the moth stories to the human. We then take the exact same words, the same words the person heard, and we put those into this LSTM language model. So it's as if the model is reading the words, you know, reading the same story that the human is hearing. And then we extract a state vector from this language model after it reads each word, right? So after we input. So, after we input one word, the model updates its state, and then we extract that vector as a representation of sort of what we think the state might be like, or what might be some interesting features at that point in time. And then we fit this linear regression model. So we try to predict the voxel's response, this response time course, as this weighted sum of the values in these state vectors over time. These state vectors over time. So, here I've sort of changed what the vector I'm talking about is. So, for example, x0 of t, this is going to be the value of the, say, the first element in the state vector, the shortest time scale unit. So we have units sorted by time scale at each point in time. So for each moment in time. And xn of t is going to be the value of the longest time scale unit at each moment in time. And we're fitting these. You know, fitting these weights beta. There's also some temporal kind of messiness that happens here because, of course, the human FMI responses are very slow. There's the delay in the bold response. We're actually fitting these finite impulse response models here. I don't want to talk about the details of that because it's not really important to our story, but feel free to ask if you're interested. So, yeah, of course, we know the R, we know the X's. The only question here is: what are these? The only question here is what are these betas? So we're fitting these encoding model weights. Essentially, for each voxel, we get a vector of values of how much of each dimension, how much of each of these features from the language model state vector do we want to use in order to predict the responses in that piece of brain. And we just fit these using regularized linear regression. This is like ridge regression. Okay, so once we've fit these models, now Models. Now, of course, we can test them on hell.data. We can say, how well do these models actually predict responses to some new story that the model hadn't heard before? I'm not going to actually show you the results from that. I'll show you a histogram of what that looks like. It works quite well. This works much better than sort of older models that we had a few years ago, but not quite as well as the state-of-the-art transformer-based things. But what I want to show you right now is how we can use the betas, the weights that we fit in this model. The weights that we fit in this model to interpret something new about what each little piece of brain is doing. So what I'm going to posit here is that we can actually estimate the kind of time scale of the information that's represented in each voxel in cortex by just examining its weights. So here I'm showing two example voxels. For each voxel, this is showing This is showing the weights. This is actually the absolute value of the weight on each of the features in the feature vector that we extracted from this LSTM language model. And what I'm showing is the voxel on the top, you can see, sorry, so the y-axis here is this absolute weight magnitude, and the x-axis is the time scale that we assigned to that unit originally, right? So we still know what time scale we assigned. We still know what time scale we assign to each unit because that was how we designed this network in the first place. And this is in units of words. So we have time scales ranging from a little less than one word, which is like very short memory decay, out to 100,000 words, which is very long memory decay. So what you can see is in the voxel on the top, it seems to have a lot of weight on units that have time scales in the kind of, you know, less than 10 words range. The bulk of its weight. 10 words range. The bulk of its weight is there. Whereas the voxel on the bottom seems to have much more of its weight on units that are in the kind of 100 to 1,000 word time scale range. So how do we take this and sort of turn it into some singular measure of time scale? What we're going to do is define this value Tv, which is the time scale of a voxel, as essentially its center of mass. Center of mass in these plots that I'm showing here. So we take the squared weights multiplied by the time scale on each unit, divided by the sum of the squared weights. And this gives us a single value that is really just like the center of mass in this time scale space. Now, this gives us a measure for each voxel of sort of what is the time scale, at what kinds of memory is necessary. Is necessary for generating the representations that that voxel represents. So let's look at what that looks like across the brain. All right, so here's a map of one subject's brain. This is shown as a, I have some little 3D maps as well as a large flat map. So this is the entire cortex that we've like flattened out so that you can see all of cortex at once. What we're showing here is the voxels that are significantly predicted by this model are shown in some color, and the voxels. In some color, and the voxels that are not significantly predicted are we just show the sort of gray underlying curvature of the brain. And then we've colored the significant voxels according to their time scale, so their TV, as well as how well predicted they are. So if it's a brighter color, they're better predicted. And if it's a dimmer color, or a darker color, they are worse predicted. And so what we can see here is a pattern that in some ways. See, here is a pattern that, in some ways, really matches like what we'd expect. So, for example, the auditory cortex, which is marked as AC here, this is sort of the lowest level of language processing. We'd expect to see maybe the shortest time scale representations there, if there's some kind of hierarchy of time scale representations across the language processing system. And that is indeed where we see the shortest time scale voxels, the voxels with the lowest TV values. We see We see maybe similar, but not quite as low time scale in like posterior prefrontal cortex. So that's like Broca's area and the other low to mid-level areas, language processing areas in the part of prefrontal cortex that's closest to the back. And then if you move forward in prefrontal cortex, you see these longer and longer time scale representations. So more blue voxels showing up in the far anterior prefrontal cortex. We also see some of the See some of the longer time scale representations appearing in the inferior temporal cortex and around the sort of angular gyrus temporarietal junction that's sort of next to AC there. So in a lot of ways, this kind of matches some things that we would expect to see. But this is a new way of measuring this. And again, I want to reiterate that this is maybe Maybe a kind of proof of concept of this idea that we can build, we have a hard time understanding what these neural network models are telling us about brain function. The fact that they just predict the brain activity well is something, but it doesn't really tell us what the brain is doing. But here, we've taken this neural network, we've designed it specifically to have one interpretable parameter, which is the time scale of each unit, and then we're using that to Scale at each unit, and then we're using that to map this time scale of representation across cortex, and it's giving us sensible results. So, we're excited by this. Uh, I have one more thing to show you. I hope I'm not running too far over time. So, one thing that we've tried to do with this model since sort of getting this basic result that was published in NERIPS in, it's about to say last year in 2020, is we can compare to things that people have found previously. So, this question. Have found previously. So, this question of what time scale of information does each part of cortex represent in language is not a new question. This is something people have looked at quite a bit. In particular, in this study that I just adore, this is like one of my favorite language fMRI studies from the, whatever, from forever. This is from Uri Hassan's lab from 2011, from Yulia Lerner. The way that they examined time scales and representation was they took a language. They took a language stimulus. It was actually a moth story similar to ours. And they scrambled it at different scales. So they either broke it up into paragraphs or sentences or individual words, and then they randomly rearranged those things. And they reasoned that if you see reliable responses in some brain area at some level of scrambling, that means that you haven't destroyed what. You haven't destroyed whatever information that brain area cares about. So, typically, what they found is that for some brain areas that would respond really reliably to the full story, if you scrambled at the scale of sentences, then those brain areas would stop responding because presumably they're representing some information that is longer time scale. And they found this kind of nice gradient of time scales, especially along the temporal lobe. Now, what we tried to do here is Now, what we tried to do here is we just tried to actually directly replicate this in silico using our models. So instead of running a whole experiment where we had subjects listen to these many different scrambled versions of these stories, we just took these stories, we scrambled them at different scales, and then put them into our model and asked our model, like, how big, how reliable would you think the responses would be in each voxel? And what we get out is actually a fairly nice match to what Fairly nice match to what they found in this study. I could go into details of how I think what we found is a little bit different from the original learner result and maybe matches a little bit better some later results, but I'm not going to do that in the interest of time. But this is just to illustrate kind of the power of this paradigm of building these computational models using natural stimuli is then we can go back and do these. In addition to having an interpretable model, Interpretable model, we can go back and do these like in silico new experiments there where we don't have to actually collect new fMRI data, which is expensive and annoying. Finally, I just want to say one more thing about what we can do to kind of interpret what these models are telling us. And that is we can ask what types of different linguistic functions might be performed by different parts of the model. By different parts of the model, right? So, presumably, this model has to do a lot. In order to be a good language model, this neural network has to do a lot. It has to recognize the topic of a paragraph. It has to do some kind of syntax, syntactic parsing, figure out part of speech to get local dependencies. And presumably, there are different parts of the network that are accomplishing this, and they might in fact be segmented by time scale. So, what we've done here, what actually Shayla. What we've done here, what actually Shaley did here, is probe the model. So, test how well different units within the model could solve different NLP tasks and then look at that across different time scales. So, what we found here, I think, pretty well matches our intuition. So, for example, if you want to predict part of speech of each word in a piece of text, then pretty much all of the information. Then, pretty much all of the information about that is carried in these short time scale units. So, on the right here, we're showing how well we can predict each of these things using units that have time scales in these different bins. The bins are kind of weird widths, but I think that's set up so that they're an equal number of units in each bin. So you can see that for part of speech, which is this very kind of local phenomenon, you really only need the short time scale units. So, all of that information is being carried by these short time scale units. If you want to do something like named entity recognition, which is a slightly Entity recognition, which is a slightly maybe higher-level task of figuring out, like, is this a person we're talking about? Is it a place, et cetera? Then it seems like most of the information about that is actually carried in this like mid sort of time scale. Whereas if you want to do something like figure out the topic of a passage of text, then that information is really selectively carried in these very long time scale units, which again, I think kind of matches our intuition. But this is giving us another handle on. This is giving us another handle on sort of understanding the representations in this model, not just in terms of what the time scales are, which is what we assigned to them, what we know our priori, but what they might actually be used for in the model. All right. So just to conclude here, I want to recap what I talked about. The first thing I showed was that the distribution of time scales for natural language processing should roughly follow this inverse gamma distribution. This inverse gamma distribution. That's what we showed in the iClear paper in 2021. That's like a fun theoretical result. We built this RNN language model, really an LSTM language model with this inverse gamma distribution of time scales. We showed that it predicts fMRI data very well. We could use the model to directly kind of measure the time scale of representation in each foxel. We can use it to replicate these previous experiments in silico, and we can actually. Silico, and we can actually maybe try to interpret what kind of information is present in each of the different sets of units. That's work that's in prep. I want to end just by talking about some exciting future directions here. So one that is already been talked about yesterday by Layla in her talk is: can we build neural network language models that learn directly from the brain instead of doing this kind of two-step Of doing this kind of two-step thing where we train the neural network model to solve some task and then use it to predict the brain. That's something that I'm working on with Layla now, and we're really excited about collecting very big data sets that are necessary for that. Two is, you know, what can these language model-based encoding models tell us about the selectivity of each brain area? Like really what each brain area is doing in terms of language processing more deeply than maybe time scale. More deeply than maybe time scale or this kind of broad segmentation of function, as I talked about here. This is something that Shaley Jane is working on actively. And three is, can we invert these encoding models to try to decode language from fMRI? So can we figure out what words the person is hearing from their fMRI signals, which is fun and interesting in ways that are not sort of directly scientifically understanding the brain. This is something that we're actively working. This is something that we're actively working on. My grad student Jerry Tang has a paper that is close to finishing on that. So that's it. I just want to wrap up by again thanking the people who were involved in this research. Shayley Jane, Shivangi Mato, and Amanda LaBelle were really responsible for the stuff that I talked about today, as well as Vo, Nicole Bekich, and Javier Turek at Intel. And thank you all for listening. I'm happy to take your questions. I have one question. I have one question for you, Alex, which is that if you look, there's a lot of problems that have a power law of time scales, right? So you talk about language, the same is true in natural movies, for example. And so it seems like if you wanted to do kind of any task, having this inverse gamma weighting of exponential time scales is probably the right approach. And I wondered if you thought about like using that for LSTMs for like video process. For, like, video processing or other tasks. That's my question. Oh, that's cool. We haven't really thought about that because we mostly just focus on language. I haven't done any like LSTM video stuff at all. Yeah, I definitely wouldn't be surprised if that's true there as well. That would be very cool to see. Perfect. And then Alana and then Antonio. Awesome. Cool. Thanks, Alan. Awesome, cool. Thanks, Alex. Cool work. I was wondering if you have related the time scale things that you found and to like the parse tree. Like there's already some things that join words together naturally in text. And so like the parse tree comes to mind. And like, have you looked at whether the time scales are sort of paying attention to those links? Like, is that something that would be possible? Yeah, it definitely is something that should be possible. Yeah, it definitely is something that should be possible. We haven't done that explicitly. The closest we've come is is this kind of analysis, like the probing analysis here. Yeah, that would be neat. That kind of maybe hews a little closer to some of the parsing sort of modeling efforts that Layla and her group have been working on recently. But yeah, that. Recently. But yeah, that could be really interesting. One thing that I want to note, which I, you know, this gets into sort of statistical physics that I don't understand deeply, but one of the arguments from this Lenin Tech Mark paper was that the reason that you see this kind of power law relationship decay of mutual information in natural language. Mutual information in natural language is that natural language is actually kind of generated by a tree-like process. And that apparently power laws are kind of a signature of things that are generated by hierarchical processes like that. Interesting. Yeah, which I can't say like I okay, Antonio's waving his hands as if like maybe. But. But yeah, so I do think there's some connection there that might be deeper than we're kind of making it out to be. It might be that forcing what is fundamentally a more tree-like sort of process or fundamentally a more tree-like thing into a recurrent neural network model where you have just a fixed length state that's propagated from time point to time point, that this is just kind of a That this is just kind of a lossy and crummy thing to do, and that we should be looking at different types of models. But in the end, this kind of has to be, you know, it's implemented in the brain somehow. The brain is kind of a recurrent neural network. So, you know, I don't know. That was kind of a tangent. I apologize. But I have one follow-up question, which is, so I saw your paper, and the first thing I thought was, like, has anybody done this with regular LSTMs? Like, are there like detections? Regular LSTMs? Like, are there like detectable time scales? Can you tell that, like, if I change this word back here, that this particular hidden state changes by a lot? Do you know what I mean? Yeah. Are you aware of anything like that? So I haven't seen a lot of analysis of that, to be honest. We did a lot of variations of that kind of thing for Shivangi's paper, where we really looked at, if I, yeah, if I. Yeah, if I typically like ablate a word at some point in the past, what effect does that have on the state at this time point? And then you can average across many different ablation locations and look at what this sort of decay of information looks like. That's one way to measure time scale, and it seems to work okay. There is another way to look at this, which is not doing the kind of ablation experiments and is actually really easy. And is actually really easy, which is, again, you can just look at the forget gate biases in your network, and those map kind of directly to time scales. So as long as your network is not very deep, which for recurrent neural networks, like depth doesn't really offer that much, then that kind of tells you a lot about the time scales in the network. And maybe to add on to this point, one more thing is that I didn't really. Um, that I didn't really, I mentioned kind of briefly here. Uh, I did say that, like, uh, you know, if we if we train this network without any kind of time scale prior, that it does recover something that looks like an inverse gamma distribution in the time scales. Uh, I think that's really something that would come out more strongly if we'd trained on a bigger data set. I think if you train on a big NF data set, like you don't really need this kind of prior in there, um, it's going to learn the distribution of time scales that it needs to solve the problem. Solve the problem. But this was specifically a model that was trained on a pretty small data set on the Wall Street Journal, whatever. So it needed like adding the prior did help us here. Okay, cool. And then Antonio, you had a question. Okay, well, I enjoyed very much your talk. Very interesting. Let me tell you, I was very surprised with this power law claim. I really don't understand it. And it's not true. And it's not true that hierarchical things must necessarily be has a power law. Well, in particular, because language is constrained by syntax and phonology, and the phonological domains and syntactic domains, well, you have a problem of remembering. Your brain does not succeed very well. So I'm very surprised. I don't know this paper, but for sure I read it and And well, but my question is the following. I really enjoyed it. I was asking myself if, well, influenced by my own talk, so we tried to find the FRAM in the EEG data, the structure of the stimulus. Well, you have a wonderful stimulus because you have someone talking, you can make the synthetic analysis, and you can make the phonological analysis. So I know. So I know that for prosody, you can describe nice trees distinguishing, for instance, and Brazilian, European, Portuguese using the context-tree model. So you can do it. And you recover irregularities which have already been found by people, by phonologists. So I guess it would be very interesting if you could use your data. your data to try to see if you could recover the at least the phonological the phonological structure but i'm afraid that with fMRI this is not possible because we don't have a good time precision it would i would suggest using eg data so this may be something you we could try to do together because i have a g lab very good in sao paulo and we have been working a lot with with with um With identification of pathological patterns. It turns out that you can distinguish Brazilian Portuguese from European Portuguese just by the structure of the context tree behind. So I think it would be very interesting. We could go in this direction. I don't know anything about the syntax. I know recently was interesting sometimes, but I found it very interesting. I doubt about the polar load, but this is. About the follow-up, but this is Lini and Tech Mark. I should read it. But maybe they found something because even if you read Proust, I don't think the power law. And Proust is very long. You can have several pages in order to conclude the sentence. But even in Proust, I doubt a little bit. But I enjoyed it really very much. Thank you. I mean, certainly it's, I think, power laws, if you go out to long enough separations, then. If you go out to long enough separations, then it gets just impossible to estimate these things. But at least at scales that you can estimate reasonably on good pieces of text, this power law mutual information relationship does seem to hold up. And we replicated that on text ourselves. They actually did it at the character level, not at the word level. We did it at the word level, and it comes out to be more or less the same. So yeah, I mean, certainly we can do some things to look at like phoneme representation or these lower. Like phoneme representation or these lower-level representations using our fMRI data. We've done quite a bit of work on that, actually. It is a lot messier than looking at higher-level things like syntax. Semantics is beautiful. Semantics seems to be like the real driver of fMRI responses to language, maybe because it's slow. But certainly we can see some things about phonology as well. Yeah, this is a thing I'm... Yeah, this is a thing. Can I encourage you to sort of wrap up this thought, Alex? Just because we're running a bit late, but I'd love to hear what's on the tip of your tongue. Yeah, I was just going to say, this is one of the things that Layla and I are planning on investigating. So we're collecting this big data set. It's going to be joint fMRI and MEG of a bunch of data. So a bunch of natural language data. So I'm excited to see what new fast things will come out of that. Good. Awesome. Thanks again, Alex, for this fun talk. Alex, for this fun talk. Thank you. And next up, we have our last talk for the session from Jay Pina, who's a