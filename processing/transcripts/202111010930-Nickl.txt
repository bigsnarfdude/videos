Okay, well, thanks so much, Gabrielle. Thanks also for organizing this. It's a great pleasure to talk here. I'm sort of a shadow organizer because BANF doesn't let two people from the same institution co-organize. So I had to organize and now give a lecture, which is sort of a double workload. But of course, it's a great pleasure. I mean, you know, we have been trying to work at the interface of these areas. Interface of these areas, Gabriel and Francois and I, and several others that I want to mention here previously, briefly, which are mostly coming out of sort of the Cambridge crowd here. So there's Kweko Abraham, who just rejoined Cambridge after postdoc in Paris. There's Jan Boer, who is a current PhD student in Cambridge, who is sort of on the job market now. Matteo Giordano, who was one of my former PhD students who just moved to Oxford. Former PhD students who just moved to Oxford, then, of course, François and Gabrielle and Sven Wang, who is at MIT now. These are all collaborators from whom I learned a lot. And I think the purpose of this talk is just give a bit of an overview of some of the things we've been thinking about and how they relate to the different areas. And I'll try to be more really of an overview sort of BERT perspective here instead of getting too technical, maybe also to. Instead of getting too technical, maybe also to start, you know, set the floor up for some discussions as we go along. But of course, sort of the whole thing is driven by a desire to understand inverse problems somehow in the context of data science and of real statistical measurements sort of set up in a probabilistic framework. So, what we're thinking of having is, and by the way, if someone has any questions, please feel free to interrupt me anytime. Me anytime. Yeah, I prefer that. So you have data like that we in statisticians' language would just call this a regression model. So you have response variables yi and covariates xi, and they arise according to a regression model with some statistical error or noise that we call epsilon i. For simplicity, I'll assume the noise to be Gaussian. Data can often be matrix or general vector space valued, but for simplicity, you could just think of real valued data. Even though in applications, often there's some vector space. Applications often, you know, there's some vector space V say behind. And so basically, you have a regression model in the statistician language, but the regression functions arise from a particular parameterization that people in inverse problems would call a forward map that sort of maps some unknown coefficients data that could be the coefficients of some PDE or of some other transform or so into some other space of functions, which then is where your regression functions live, and that you sort of measure discrete. And that you sort of measure discretely along some discretization of Xi's that maybe live on some manifold, something like that. And sort of the main challenge here will arise from the fact that the map that sends the unknown parameter theta into sort of this forward solution of your PDE or whatever it is, that this map is non-linear. And somehow the inverse problem is to determine theta from this forward data g of theta, but in the statistics. Of theta, but in the statistical setting, you don't have the idealized sort of complete forward data g of theta, but rather you discretize it along a certain grid of points xi, and then you take this regression type response observations. So that's sort of a real world statistical description of a large class of such inverse problems, but I haven't really said anything in particular now of how these G-stata maps arise. In fact, for statisticians, these are pretty standard models where you just have a parameterization of your regression model. A parameterization of your regression models, but somehow the story is that the parameterization of your regression models is not some high-level conditions that you put on them, but there are some very concrete structure coming from a partial differential equation or the like. So you want to exploit that structure in your theory. And so I have several model examples in mind. In fact, the references I will give you will highlight and pinpoint that some concrete inverse problems for which we developed some theory. They're sort of a nice Some theory, there's sort of a nice sort of, I don't want to call it model example because mathematically it's actually very complex, but it somehow is also not too complex. So sort of one of the main examples that François and Gabrielle and I worked on are these non-abelian X-ray transforms, which will maybe feature in some other talks. I won't talk too much about them in mathematical detail. Some other problems we worked on is just a classical Carderon problem, which people will know better from electric impedance tomography. Um, from electric impedance tomography applications, and there are some sort of slightly, from an inverse problems point of view, easier model examples, like sometimes in the applied math literature, or at least Andrew Stewart calls this Darcy's problems, where you want to find a diffusion coefficient of an elliptic PD, but where you have interior data in your domain and not just boundary data. And there could be similar such problems coming from photoacoustics with Schrodinger equations. So we do have a set of model examples that we worked through. Model examples that we've worked through by now, but I rather still want to talk you through some sort of general ideas that maybe, particularly for the statisticians, maybe easier to grasp because you don't get lost in sort of concrete PD material. Even though really, if you don't fill it with life, I think the general theory is also not very meaningful. So now, what does the statistician do in this case? Well, sort of the workhorse of statisticians would be like trying to do something like a least. Like trying to do something like a least squares procedure where you look at the least squares fit of your regression model that goes back to Gauss, at least if not before, where you look at sort of you know just the fit of your response variable yi from sort of the candidate, my cursor, well, anyway, from the sort of the candidate regression model G of theta here. And you just sum the squares of these residuals. And typically, you would try to optimize here. And in fact, of course, non-linear. Here. And in fact, of course, non-linear inverse problems have been studied from this optimization point of view, sort of in this classical Austrian school, for instance, or in the 90s. So you try to optimize here, but there are some challenges because when the geop theta map is non-linear, the criterion function you have here is non-convex. And in contemporary statistical science, we're particularly interested in high-dimensional parameterizations or even infinite-dimensional models for theta. So somehow having a non-convex criterion poses some challenges for the optimization. For the optimization approach, at least, and also otherwise, in fact. And one very successful approach that has been very influential in the last 10 years that is often associated with the work of Andrew Stewart, although there are really lots of people also preceding him already in the Finnish school and elsewhere and afterwards that did some very innovative computational and inferential work is to sort of, you know, maybe model your theta parameter by some Gaussian process prior so that it's a very Gaussian process prior, so that is a very flexible infinite-dimensional model for the thetas. And then sort of look at the posterior distribution that arises from this hypothetical assumption that your prior is a Gaussian process prior and the given regression data, which just by a straightforward application of Bayes' formula is sort of a re-weighted version of the log likelihood, the exponentiated log likelihood, which is just e to these least squares fit in this Gaussian model here. Fit in this Gaussian model here. And then you sort of have a penalization or regularization term, which in the classical regularization literature is just your penalty of the Tikhonov regularizer or something. But really for the Bayesian, it is somehow the norm that describes the geometry of your Gaussian process prior. So really, you would be targeting to maybe understand the posterior measure, a random probability distribution on your parameter space that comes from this re-weighted version. Now, of course, if you were just to try to find Of course, if you were just to try to find the most likely point of this posterior, this would be the maximum posteriority estimate, you would be back at optimization. So that is then just basically the classical optimization approach where you've chosen a particular penalty. But of course, the interesting bit here in the non-linear situation is that you can, first of all, it is hard to optimize and you can, once you have this posterior measure, maybe aim at computing other things than the optimizer. Computing other things than the optimizer. For instance, the posterior mean, which actually from a Bayesian decision theory point of view is a much more natural point estimator than the maximizer. And of course, here you see also that this problem of local optima that you might have in mind when you think of this being non-convex is not something that you would expect an attempt to compute the posterior mean to be very sensitive to. So you could ask yourself, well, if this is my program, can I do some clever algorithm to compute this? Some clever algorithm to compute this posterior mean and perhaps even the posterior quantiles. And so, this is where, of course, the large toolbox that was developed over the last 25 years of Markov-Jain Monte Carlo algorithms comes into the game. And that's also where Andrew Stewart's sort of contribution really lies was not just to write down the Bayesian formalism, but specifically to write down a class of feasible algorithms that can deal with such non-convex problems also in high dimensions. And that, you know, maybe you can't easily prove something. You can't easily prove something for that, but it seems to work very well in practice, and applied people like it, also for reasons that have to do with uncertainty quantification that I will mention in a second. But just to give you a sort of a quick idea of what you would be doing when you run an MCMC algorithm, particularly I'm just choosing here one that is gradient-based, so one can see a little bit the comparison to maybe earlier work of the sort of iterative regularization literature. That I mentioned before, you would just do a very simple scheme. You take your log posterior, which is the log likelihood function, so that's just the least squares fit, and you subtract the penalty term here. And then you can just compute the gradient of this whole thing. That requires one evaluation of your forward map. So that is typically in these inverse problems a step you can do, maybe like solving an elliptic PDE or the X-ray. Elliptic PDE or in the X-ray case, you would have to solve just a matrix ODE. So these are things you can do numerically. They're still not cheap, but they don't require you ever to compute an inverse map or even to speak of one. And then you just iterate this. You start at some theta naught, and then you go into a certain step size delta times this gradient direction that comes from your sort of posterior surface. And you add some Gaussian innovations here, scaled by a certain step size. And so that gives you. Certain step size, and so that gives you sort of an iterative method that will run around your posterior surface. And if you think what you do, instead of just greedily going for one optimizer, you actually along your ergodic average of this Markov chain, you just collect many of them. So you can then, in particular, you can, for instance, compute the average of all of them. Sorry if my cursor doesn't really work so well here, but so that would be just the empirical mean estimate of the true posterior mean, because one can show that. Posterior mean because one can show that this gradient descent algorithm that I had on the previous slide actually its invariant measure of this Markov chain is precisely equal to the posterior measure. So it's sort of set up in a way that you ultimately, at least when your Markov chain mixes, target the right limit distribution. So and then you can do two things. You can compute sort of a summary statistic, which here I wrote map because these pictures come from a linear problem where they're actually equal to the posterior mean, but just think of it as the mean, sort of that's the It as the mean, sort of that's the blue curve in the middle, that is sort of your point estimate, the actual algorithm you use to reconstruct. But since you collect all these many iterates as you go along the chain, and since this chain never really starts to collapse and just gets stuck at an optimizer, but actually keeps wandering around because of these Gaussian innovations, it sort of gives you not just an idea of the right estimate, but also somehow the spread of the posterior mass around the center of the distribution. So, what people like. Around the center of the distribution. So, what people like to show pictures like that because you get these red curve clouds around your estimator that seem to suggest that you might be wanting to hope that the true function actually is somehow inside of this confidence corridor or credible corridor around clustering around the posterior mean. So, this is something that in applications is very attractive because it gives you sort of an estimate of the uncertainty in your reconstruction, which is sort of, you know, you're doing something that the optimizer cannot. Of you know, you're doing something that the optimizer cannot give you, at least not off the shelf. Um, so these algorithms have been around now for quite a while, they've been quite successful. There's even a SIAM journal on uncertainty quantification that, you know, whose remit to a certain degree is to deal with such algorithms. So, of course, you can ask yourself, can we prove that these methods make sense? And that is sort of one of the things that we've been trying to work out a little bit in the past few years. Years, what one can actually say. Now, there are three key questions one can ask here, and I will try to give you some sort of ideas of what sort of theory about the non-linear inverse problem you might want to input in order to get such guarantees. But before we look at what that theory is, let me first talk you through the kinds of guarantees I have in mind. So, we would have the first question is sort of consistency. Question is sort of consistency. So, this posterior distribution, which is not just the optimum, like the posterior mean, but that's like a whole posterior measure that sits on your parameter space. You could ask yourself, does it somehow concentrate most of its mass near the actual parameter that generated the data? So it's very important to understand here that we're not taking a Bayesian subjective approach to analyze the performance of the algorithm. So, the Bayesian approach is just a way to model the regularization step. Regularization step. It is nothing subjective. One doesn't have to fear it if you're sort of a data scientist. The prior ideally plays no role, not a role different from any other kind of regularizers or kernel choices that you might do in other methods. But what it does have on top of the standard interpretation is that you don't just get an algorithmic output, but you get these whole posterior distributions. You could ask yourself, does it actually somehow collapse towards a Dirac measure near the true point? Then the next question. Then the next question, once you have that, is: well, you're never going to have the posterior distribution accessible in closed form. So you will not be able to say, well, that is exactly my posterior distribution because of the non-linearity. The posterior is not Gaussian. So it's something that you can only approximately sample from by MCMC. So now if your parameter space is high-dimensional and your model is non-convex, you can ask yourself, what does it actually mean to compute that thing? I mean, so I have my iterates of my Markov chain, but does it mean Of my Markov chain, but does it mix quickly enough that I can say that the samples I'm drawing from are close to my invariant measure? Yeah, there's an ergodic theorem that tells me I converge there, but perhaps the time it takes to approximately recover what I want to recover there scales exponentially in dimension and so on. So maybe I'm just generating random numbers and I'm computing not the right thing, even though it is statistically meaningful. I might have proved that, but that doesn't mean I can compute it. And then Compute it. And then somehow the holy grail from a mathematical point of view in this story is: well, okay, let's suppose my posterior has been proved to be consistent, and let's suppose I have given you computational guarantees. Do the error bars like these cloud curves that we use in applications when we want to quantify the uncertainty of our recovery, do they actually mean anything? Are they valid confidence sets? Are they error bars that if I use them in, if I compute them in one? them in if I compute them in one problem and then I feed them into my next problem uh is the stability sort of of the error bar sort of does it propagate into the new problem well that is only possible if I can give a frequentist statistical guarantee for that confidence interval no so so maybe the main contribution of statistical science to science is the notion of confidence intervals and and like being able to to give these error bars at certain significance levels which other algorithms you know machine learning doesn't give you that they just give you a black box Learning doesn't give you that, they just give you a black box output. But if you want to know whether you can use what you do with the prediction interval, then you need to get such guarantees. So, this is what is called uncertainty quantifications. And so you can ask yourself, if I choose to use this Bayesian method for uncertainty quantification, is it something that I can trust? And well, we've worked out answers to all these three questions, which maybe they're not the final answers. There are some first answers, I would say, that the teacher. First, answers, I would say, that teach you a little bit about that. And in all these three answers, what we found is that really you need to know some very specific things about the forward map. If you want to prove anything for a concrete PD, there's some questions you can ask. Does this forward map have these properties that you can then try and check for the PD in question? And then, well, we checked it for a few model examples. So I'll talk you through some of that now for all these three types of guarantees. For all these three types of guarantees, the first was again statistical consistency. So I said we want to say that the posterior distribution, which is this conditional measure and it's random because it still depends on the data, charges like, you know, basically does, let's put it, the complement does not charge any thetas away from the ground truth theta, not by more than like some rate of convergence or contraction epsilon n that goes to zero. So that is a typical notion that people use in Bayesian statistics. Notion that people use in Bayesian statistics to speak about the statistical performance of posterior measures. Once you have something like that, you can typically also get a convergence rate for the posterior mean, which is sort of the thing you actually want to know. They're coupled, but you have to do the first and then deduce the second from the first. And so here are some papers that appeared in the last couple of years about such results for concrete non-linear inverse problems. Probably the first rigorous result that works for the most sought-after Gaussian process priors. Sought after Gaussian process priors, which people like to use also for computational reasons, is in this paper where we studied a particular very representative non-linear inverse problem coming that was previously studied by Miko Sardo and Guto Urman and Gabriel and others. And so there's some contraction rate or convergence rate that is sort of nicely algebraic in inverse sample size. So the problem is not too hard. There are some other inverse problems that are too unstable. We might hear a little Too unstable, and we might hear a little bit about this in the next talk. Where actually, in the Cardalon problem, when you run this program and you want to look at smooth conductivities instead of maybe very particular piecewise constant ones, then the rate is very slow here. It's inverse logarithmic. That's a paper with Kuiko Abaham. And then in some other inverse problems that are more of more simple elliptic type or parabolic type, you can get such rates as well. There's also some sort of set of general conditions that can guide you. That can guide you when you have a particular inverse problem, which you can probably find in these two papers. I'll give references at the end. But really, the main thing from a PDE perspective that you need here is something that is well known in the inverse problems literature is sort of a quantitative stability estimate. So what you want to be saying is that you can somehow control the distance between two points in my parameter space, theta and theta naught, in some norm that I do not specify here. Norm that I do not specify here now, but that should be somehow as strong as you want, and in particular, it should be living on the theta parameter, the one you want to infer, and not on the forward level, that you can somehow control that in a sort of maybe Lipschitz or held away by the implied distances of these parameters in my forward data. And this doesn't have to hold completely globally, but it has to hold. And here is an interaction with the regularization that you use in your prior. Use in your prior, that it has to hold on bounded sets for your regularization norm. So if you use a typical Martin-Gaussian process prior, then for this H norm here, you could take some Sobolev norm. So you could think of having to prove that you can control the distance on your parameters by the solutions of these PDs, at least uniformly on fixed balls for a Sobolev norm on your parameter space. And maybe you could allow for some Hilde exponent here. So you need a quantification of So, you need a quantification of the amount of injectivity in your forward map. If you have a gauge, you would have to sort of quotient that out or do something similar. I'm not talking about gauges here, I'm just looking at injective things. And that's something that we could prove for like, you know, that we basically effectively use in these four papers and that you could try to prove in other settings as well. So that is a global, but not completely global. There's some kind of clever regularization argument that shows that you don't need to control this in the whole unbounded parameter space, but just unbounded safe. Parameter space, but just on bounded sets. For computability, there will be Sven Wang talking on Friday, I think the last talk, in much more detail about a long paper that we have, which really sets somehow up the foundations for when you can even prove that these problems are polynomial time. Because of course, you try to find either an optimizer or a posterior mean of a high-dimensional problem. So you could ask yourself, does that could, in principle, since you're non-convex, be an NP-hard problem? non-convex p and n p hard problem and we gave some conditions that make it polynomial time in principle um which which in essence um have to do with with work on on log concave measures and sampling from log concave measures and there's sort of some deeper facts from analysis that go back to work by felix otto and wasserstein gradient flows that tell you that certain diffusion processes in high dimensions can mix very quickly towards low concave measures now our measure Log concave measures. Now, our measures are not log-concave, but there's a nice key theorem that we prove that says that under a certain local convexity assumption or gradient stability assumption, which I'll discuss in a second, posterior measures are somehow miraculously always approximately log concave. And so in these cases, even though it's only approximate, you can make that quantitative and actually deduce from it that you can have polynomial time algorithms for these non-convex sampling problems. And the key condition concerns. And the key condition concerns the linearization of your forward map. So you kind of have to compute a derivative near the true point theta naught. That gives you a linear map acting in directions of your parameter space, which in order to do MCMC, you will discretize at some high-dimensional level. And so basically, what you think is you need a stability estimate, a quantitative injectivity result for the linearization of your forward map. So for instance, if you have something of that. If you have something of that kind for the normal operator, then you would get something of that kind. But in principle, the condition you want is that the linearization of your forward map has a lower bound in terms of the directions it acts on. And the constant could depend on the dimension. So there could be some ill-postness, some local ill-postness that is expressed in the terms of this exponent kappa here. And that's something, again, we checked for a class of PDs, like for non-abelian X-rays and also for Schrodinger equations. You can check that. Example equations, you can check that, and in this case, you can actually prove that this gradient-based MCMC method, this Langevin algorithm, when you tweak it a little bit, has a strictly polynomial mixing time in all the relevant parameters. So the relevant parameters are the dimension, they are sort of the spikedness of my posterior, which is expressed by the informativeness of like the number of samples n, and also the noise level epsilon at which I want to control the error. So I can say. So, I can send you to these papers and to Friday's talks for more details. This does come with some non-asymptotic mixing time bounds for these ergodic averages, but I will skip that slide. Just saying that you get nice exponential bounds for computing ergodic averages from this Markov chain. So from a concentration of measure point of view, these are not just asymptotic mixing results. There's something genuinely holding for fixed sample sizes. But that's just for the specialists. For the specialists, and um, are we doing time-wise? I have a few more minutes or 10 minutes or so. I can't hear you now, sorry. Yes, you still have another 10. Okay, okay, so I think I want to spend maybe last 10 minutes talking about the mathematically most subtle part of the story. Um, so you know, we said for global consistency, what you need is global injectivity quantified by some stability estimate in suitable norms that should. Estimate in suitable norms that should not be logarithmic unless you're happy with logarithmic rates. So, some Hilde sort of stability or Lipschitz stability. Then, for mixing times for computability, one of the key components is you need to assume the first assumption on global stability. I should have said that. So, you just local gradient stability is not enough. You need global stability plus local stability. But if you have both, then the posterior somehow already. Have both, then the posterior somehow already sits in the right region of approximate low concavity, and then this local gradient stability is a way to really quantitatively exploit bounds on mixing time. So you can show that these things are computable due even though they're non-convex. But from a statistical point of view, really the key question is, can we say something about uncertainty quantification? Can we say that if you compute error bars from your posterior distribution, that actually, at least if you have a lot of data, they are actually exact confidence regions that. Exact confidence regions that give you an estimate for the error in the reconstruction, or is this just some Monte Carlo randomness that you show to the user, but really doesn't mean anything? And all these Bayesian UQ people are just basically creating a lot of random numbers without any meaning. So you could ask that. So, and I think that's like when I first saw Andrew Stewart's talks and the people in his community as a statistician who had worked on confidence sets, I thought, well, okay. Who had worked on confidence sets? I thought, well, okay, these people are just creating random numbers and it has no meaning. But of course, when people use it in practice and like it, and it is sort of popular, maybe there's something that works. And so you can ask yourself, what is the mathematical theory that mathematical statisticians have developed to give justification for such Bayesian methods? And there is a famous theorem that actually goes back to Laplace, and that is called the Benstein-Formises theorem after Sergei Benstein and Richard Formises, who were the key contributors in the early. Who were the key contributors in the early 20th century, even though the rigorous theorems are really due to Le Carmen and then van der Waat in his book, which tells you that in low-dimensional models, by low-dimensional, I mean a fixed dimension, and then sample size goes to infinity as dimension stays fixed. So that's very much classical asymptotics from the 20th century, if you want. Then these posterior distributions here will actually stabilize at a particular normal distribution that. Particular normal distribution that has a centering, which is the either posterior mean, or it could also be set a bar, could also be the maximum likelihood estimator, something nice, something reasonable. And it also has a very canonical covariance structure, which is the inverse of the so-called Fischer information, which, you know, if you've ever taken a statistics course, you will have heard about the Fisher information, which is sort of a way to summarize the complexity of your statistical model. The bigger your Fischer information, the more the easier it is to make valid inference. And in particular, And in particular, the inverse of difficult information is a way to measure an a priori lower bound for the best spread or covariance that your confidence sets and estimators can attain, if it exists. And it is a key hypothesis of this Laplace-Bench-Fermitas term that the inverse Fischer information has an inverse, because otherwise this normal distribution that pops up on the right-hand side as approximating the posterior distribution isn't really, that doesn't really make sense. And that is something you can prove on the mildest. And that is something you can prove on the mild assumptions if the feature information is invertible in a fixed low-dimensional model. Now, you could ask yourself: can I run that program to an infinite-dimensional model, perhaps with PDEs and inverse problems and so on? Well, Benson-Fomites terms in themselves, if you want to prove them in high dimensions, that's already even in a classical statistical model is a tricky story. There's some work that Ismail Castillo and I did almost 10 years ago that provides a framework to do that. That. But if you want to run this past a general locally asymptotically normal model rather than just a simple regression model, there is a key thing that pops up, which is the so-called information operator, which is somehow the infinite dimensional analog of saying that the Fischer information has to be invertible, only that in infinite dimensions, we're not talking about matrices any longer, but we've forced with the problem of solving new PDEs that maybe in that form haven't been around yet, or at least not explicitly. So when you start to run So, when you start to run this program that Castillo and I developed to these inverse past these inverse programs, you have to start to ask some questions that are expressed in terms of this information operator, which in a Gaussian regression model, which is a simple example, but maybe the most important one for the theory, is really just you take the linearization of your forward map, compute its adjoint. One can discuss what the adjoint should be for, but they're canonical choices, and then take it together with itself. So, in inverse problems, this would be called the norm. So, in inverse problems, this would be called the normal operator, and we call it the information operator. Call it whatever, it's just what it is. But what you need to do in order to get these men's information theorems in infinite dimensions, you need to somehow study when you can solve equations with that operator. So if you want, in particular, you need surjectivity properties. So, maybe morally, let's suppose I don't want to find the whole theta naught, but just test against a particular test function psi, and then later I will run through a large class of test functions. Run through a large class of test functions. Well, the key ingredient here will be that the psi has to be somehow in the range of this information operator. So, in particular, I need to find a solution psi tilde in my regularization space for which I can solve this equation. So, this is maybe, you know, if you've seen source conditions and things like that in the sort of traditional inverse problems literature, that is a bit related to that, only that the interaction here with the penalty term is somehow more explicit than that with. Term is somehow more explicit than that we're dealing with information operator in the first place. Now, you can ask yourself, when can I solve this equation? And this is in these non-linear problems, not always an easy thing. So when I first ran into these issues, I managed to find a simple elliptic PD, the Schrodinger equation with interior measurements where I could do it, which is in this paper here. It gives a concept proof that this problem can work and this program can work, but from a PD perspective, this is. But from a PD perspective, this is still fairly simple, even though for me it was not initially, but you know, one has to learn slowly. Um, for these x-ray type problems, this is the content of two papers that I wrote with François and Gabriel, where they sort of figured out, well, sort of if there's a like a folklore, well-known fact that a lot of people in this area have worked on is that this I star I is a psi dio of order minus one that is sort of elliptic in the interior of a simple surface or something like that of your domain. A simple surface or something like that of your domain. But for what I need, since these regularization spaces typically have to go all the way to the boundary and I is not local, you need to really include the study of the boundary effects. So you can't just use an ellipticity result in the interior of your domain. And that's where actually a lot of trouble comes from. And in these two papers, we sort of shed some light on the issue. But it's sort of very specific to this particular problem. And if you look at the general class of inverse problems, you could ask. The general class of inverse problems, you could ask yourself when can I solve such equations that I have up here, and which are just like a PDE analog of asking when is the Fisher information invertible, that leads to questions that often haven't been answered yet. But if you have such an inverse, if you can prove it, then in a paper that is just about to appear in the Annals of Statistics, we proved a general infinite-dimensional Benstein-Fomis's theorem for Bayes methods in such inverse problem settings where you can then deduce. Problem settings where you can then deduce from that that at least inference on linear functionals for arbitrary test functions ψ will be valid and the error bars will be correct by virtue of this kind of infinite dimensional Bench and Formisis term. And you can also show pictures that this is not just asymptotic. So here we show MCMC plots computed along our MCMC chain. The green dot is sort of the true mean. Sort of the true mean of my posterior draws. The red dot is the true ground truth that I want to cover by my confidence interval. And the black dots give you the one sigma. And if you think of three sigma or something like that, giving you a confidence interval, they do indeed, these plots do cover the truth as you want. So you can not just provide a theory that uncertainty quantification based on these Bayesian posteriors is valid, but you can also numerically verify it. And of course, so I think maybe one of the main questions that pops out of these results is: if I wanted to, for the PD people in particular, but also for the statisticians, it's very important, when does this information operator actually have the required surjectivity properties that I can prove these theorems and that I can make sense of that limit here, which is from an information theoretic point of view, the best limit you can attain. This is the Grammar-Rau lower bound in a semi-parametric sense. So for statisticians, Semi-parametric sense. So, for statisticians, somehow whether this is a well-posed root n estimable problem for which a Benchland-Vermeters theorem can hold true ultimately reduces to an PDE question solving, I mean, it's not PDE, it's like solving an equation. Yeah, and the I star I comes from your forward map, so there's some PDE or inverse problem structure incorporated in here. So, really, that's the canonical question. You could ask yourself, when can I solve these equations? And we have only a very piecemeal understanding of that so far. In all the examples on this slide, the I star, I. the examples on this slide the i star i happens to be an elliptic operator and in particular then is closed which is sort of you know halfway through of of of showing that that you can invert it but there you know there's no reason why this inver information operator should be elliptic in other examples um here's one thing that for those who haven't who have seen my earlier talks they will not have seen that yet that's something i did earlier this spring with gabriel which isn't super deep but it gives a concrete counterexample that in some of these Concrete counter example: that in some of these PDE examples, the fission information actually simply is not invertible, and there are no root and consistent estimators, even though everything is smooth. And that's actually a classical problem people study in applied math. You want to find this diffusion coefficient of an elliptic PD. So you want to find a theta in the solution of this elliptic equation. Andrew Stewart sometimes calls it Darcy's problem, but in this regularization literature, this has been around for a while. It is globally injective. You also have local curvatures. Injective, you also have local curvature, so it's a problem where you get posterior consistency. You can also give computability bounds, and you can also, in principle, compute the posterior mode in some of these examples. So it's not a hard problem. So if I'm saying that something doesn't work, I haven't picked like something that is completely impossible. No, actually, that's sort of a classical fruit fly example of the applied math literature. We want to find the conductivity here from maybe noisy observations of the solutions of the PD. And if you want to look at the PDF, And if you want to look at the base consistency, this is in this paper with Matteo Giordano in inverse problems. And what we proved, just let's look at the simple case. We discussed this in breadth in the paper, but let's say the source function is PD is two. I just see the solution of the standard Dirichlet problem, and I want to infer the inner product of theta with a test function that is smooth, and that just along one ray connecting the origin of the disk. Connecting the origin of the disk and the boundary vanishes. So, for instance, if it is a smooth function that is supported in one quadrant and zero elsewhere, and you find a whole lot of these functions, of course, and that is otherwise non-negative, then you satisfy that. So, this is not some weird unsmooth function. This is a smooth function, nice support, and like, you know, somehow a very representative example of a function for which you might think that Bench and Formis' theorem should hold true. In particular, in all the previous theorems, In particular, in all the previous theorems, all these functions would have been covered. But in this case, we can prove that due to the strange geometry and, in particular, the lack of ellipticity of the information operator for this PD, psi, such a nice, you know, a very large representative class of smooth psi, is not in the range of the information operator. And it's not even in the range of the adjoint score operator, which is the necessary condition due to a paper by Van Davaat in the early 90s for root and consistent estimability of this. root n consistent estimability of this parameter. So we have here a non-linear inverse problem where everything is smooth. The conductivity is smooth, boundary function is smooth, solution of the ellipticity of the PD is smooth. So this is not a problem of lack of regularity, but you still cannot root n consistently estimate this parameter simply because of PDE obstructions that pop up when you analyze this adjoint operator, which has sort of a funny transport term built in, which can be solved in general. In particular, for this PDE. In general, in particular, for this PDE, there is no bench line for Mises theorem. So, if you report credible sets based on posteriors for theta, which actually a lot of the applied math papers on this PDE, you will find in uncertainty qualification journals. Certainly, you will not be able to prove that the credible sets are confident sets by virtue of a Benjamin-Famises theorem. Maybe you have some other clever way to do it that I don't know about, but here's an impossibility result. So, there's no way we can, in general, expect that this program always succeeds and really. Program always succeeds. And really, to conclude, it sort of leads to this question: okay, can we get global stability estimates for the forward map? Well, we believe if we don't have a gauge, maybe one, you know, if there's some quantification of injectivity and you have some integrating factors that you can play around with, maybe you can do it. Do we believe in local stability after linearization? Well, maybe we also believe that something like that will be possible. And I think there is a constructive research program going forward to answer these questions. Going forward to answer these questions. When it comes to uncertainty quantification and pension from Mises' theorems, the theory is much less clear. Can we classify non-linear inverse problems according to a criterion that the normal operator, the information operator, say, at least has closed range or is elliptic or so? Well, I don't know, but there are certainly counterexamples. And it may well be that things are more towards the negative type of negative results that I've just mentioned, that the fissure information just doesn't exist. Information just doesn't exist, the inverse of the feature information just doesn't exist, but it's completely uncharted territory at the moment. And hopefully, maybe in a few years, we'll know more. So, if you want to read up a bit about this, sorry, for there are quite a few papers now. I will, just when this term over, my plan is to write up some lecture notes. I'm also visiting ETH Zurich next year to maybe write these notes. So, hopefully, maybe next summer there will be some more accessible text. But if you want to read on this stuff at the moment, On this stuff at the moment, maybe the most accessible paper would be this one to start out with, or any of the others. And thanks for your attention. And sorry that I covered so much, but that's it.