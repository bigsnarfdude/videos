Okay, I have to use this. So first let me remind you what a plank is. So a plane is just the space bounded between two parallel hyperplanes. And so the classical problem says basically that if you have an n-dimensional convex body and it's covered by a collection of planes, then the sum of the width should be at least Some of the width should be at least the minimal width of the convex body that is covered. So Tarski proved it for the unit disc and the three-dimensional ball and Bang solved the conjecture for arbitrary convex bodies. Bang also asked whether the width of the plank should be measured with respect to the convex body that is being covered. And Ball solved this question. All solved this question for the most important case, which is for symmetric convex body. The general question is still open. And so formally a plane is just a region defined in this way. So phi also. Phi is a linear function um on the dual space with norm one, m is a real number, and w is just a positive number. And W is just a positive number. W is called the half-width of the Planck. And Bolt's Planck theorem basically says the following. So it's just the contrapositive of the Planck theorem. Sorry, the statement, the natural statement. It basically says that if you don't have, so if there's some of the width, the half-width of the Planck is not, well, is less than one, your planks won't cover. So there will be a point. So there will be a point, there will be a vector, a unit vector, which is not in the plan. So obviously the Planck problem is the Planck theorem is sharp. If you take parallel hyperplanes which do not overlap, then you get the extremal case. However, in this talk, I'm going to be concerned. However, in this talk I'm gonna be considering planes that are symmetric about the order. So I forget about the numbers, the real numbers m. So I just look at this sort of planks. And well, the statement just for these kind of planks looks like this. This is still sharp. Well, it is still sharp in the sense that if you consider x to be the x to be the to be L1 and phi i's to be the standard basis of L infinity, then you still get a sharp statement. So however if you change the space X here, if you look at very particular spaces, you might be able to get to improve upon the condition upon this condition. So, for example, if you look at If you look at Hilbert spaces, maybe you can say something more. And that's basically what Ball did for complex Hilbert spaces. And he proved that if they had width square at up to one, then you still can find the point which is basically satisfies the theorem. And so this is a complex planned theorem. So you can still beat this sequence. You can always find a vector C, which is an H, a unit vector satisfying this condition. So in the case when, so basically if you take planes with the same width, all of them have the same width, then the statement looks like this, right? So basically you will need wider plants in order to completely cover the... In order to completely cover the complex n-dimensional sphere. So, for the classic plan problem, this is what you get, and complex tells you that you still need wider planes in order to completely get the optimal state math. So, the question is, so what happened in the real case? So, in the real case, this is obviously not, so the getting a square root of n is obviously not possible. Getting a square root of n is obviously not possible, but uh so why? Because if you take two n vectors which are uniformly spread on the unit circle, so you have three vectors, six vectors like that, then whichever vector you take is going to have inner product less than this. Less than this vector, right? So in this case, this would be, let's say, rate of six. So let's say this is rate of V1, V2, V3, and then you have so this would be the inner product is always, for this vector space, a particularly is going to be equal to sine of pi over six for in for general. Phi over six for in for general vector, which is going to be five, right? But so this very simple statement is connected to a very old conjecture by Feystault, which basically says, well, which was all basically about two years ago by Jan and Polianski. Jan and Poliansky. So, and but the conjecture is stated in terms of zones. And zones are just the intersection of a Planck with a sphere. So this is the way to define them. Instead of looking at width, you look at spherical width, you look at all the points which are spherical distance w of some great distance. Oh, some greater circle, which is okay. So let's call how some looks. And so the conjecture says basically that if you have a collection of zones of equal width, with equal spherical width that cover the unit sphere, then the width should be at least 5. The spherical width should be at least by. This particle would be at this point already. So that's the conjecture. Which is obviously a natural conjecture. Okay. So as I said, Polensky and Jan and Polensky solved this in 2017. I didn't know about this. I didn't even know about the conjecture. I just thought I was just solving an optimal plan theorem. And apparently, well not. So I independently basically solved this in 2008. basically solved this in 2018 and the probes are basically well are completely different so their approach is a more classical approach they use Bang's lemma and they use a very nice inductive argument to get the the optimal status it's a very nice proposal so anyway so for example if you look at the extremal case which is basically Extremal case, which is basically the case when you have something like this in a two-dimensional subspace, cutting the sphere, and you look at zones with width 1 over n, which is the width of the classical Planck problem, you get something like this. It's obviously very suboptimal, so there is a lot of space that needs to be covered. So, as soon as you get the right bound, you get a very nice picture covering the whole sphere. Anyway, that's too good some pictures. So, the strategy I'm going to follow to prove the theorem is basically the strategy that Bolt used for proving the complex plane theorem. But there is a main difference in the proofs. So basically, Bolt uses at some point he studies the behavior of complex polynomials and he uses maximum modulus principle. So, obviously, that cannot be done in the real case. The real case. So you need to do something different, you need to look at different things, different objects. And it turns out that the right objects in this case are just trigonometric polynomials. And you need to use extreme properties of trigonometric polynomials. So I'm going to be working, well, with this rescale version of the theorem, instead of looking for a vector of norm one, I'm just going to be looking for a vector of a s of norm square root of n. Uh any questions so far? Any uh any questions so far? So I'm gonna give some intuition, motivation definition of inverse second vector. So the problem basically consists in finding the maximizer of this function subject to this quadratic constraint. Right? So you want to find So you want to find a vector for which these inner blocks are big, subject that the vector has normal square root of family. But this little thing here is difficult to handle, right? It's not nice. So instead of looking at that, we just look at the product. So we look at the product and this one we can differentiate, we can apply nice things. So we hope at the end that the maximizers of this At the end, the maximizers of this problem help us, and basically, the factors of this product are large enough so that we get the desired result. Okay, so this is, so Andrews used this method to prove something called a strong polarization problem in his PhD thesis. He also basically followed the same approach as Walt for the complex one problem. One problem. This is one of the propositions which basically describes the structure of the optimizers of this problem. So if V is a maximizer of this problem, then it should satisfy this condition, right? So B is just a linear combination of the B case and the B. The VKs and the coefficients are just the reciprocals of the inner products. So if I look at the gram matrix defined by these unit vectors BJ's and I define this vector W so I define W k to be just a reciprocal of the inner product against B k. Against BK, then I can see that when I multiply h by w, I just get 1 over w. Right? So I just get, so when I multiply h by w, I'll just get w inverse. And that's just because this expression here is going to become v, right? Because that's basically facing here. So you have w k w okay now. Okay, anyway. So they satisfy this equation. So I'm gonna call an inverse, so for a matrix M, I'm gonna say that w is an inverse eigenvector, if m times w is equal to double j inverse. So now the theorem can be stated in terms of inverse Endian vectors of a gram matrix. So what I want to matrix. So what I want to do is to find an inverse second vector of h for which basically all its components are less than equal to this little number here. So that's just because w is that, I want these to be bigger than something, and Chad give me this little number here. So after some manipulations of this statement, which I'm not going to do here, but very simple. So you can basically see that this statement is equivalent to this lemma tree. So I'm just going to tell you what n is. So basically the matrix M is defined like this for an inverse eigenvector W and for H here. So basically we justify the existence of some inverse eigenvector by the same method, so with this by some optimization like this. And then we define this matrix and basically we can restate our problem just Our problem just uh basically we can just if we solve this lemma, we solve the problem. So that's just basic very easy algebraic manipulations. It's not nothing really complicated happening there. So you have a symmetric positive matrix that maps one to one. One is just a vector of ones. So yes. Well, that's probably not good. Okay. And yeah, so if you have a matrix satisfying these two conditions, so the second condition basically says that if you have a vector that belongs to this ellipsoid, then it is kind of inside the high parallel, right? Doesn't work, it's basically something like let's say. something like set in the series, you have a purple one. This is the dimensional and then you have some ellipsoid here. So this is kind of the feature. The ellipsoid is inside. So this ellipsoid is C transpose M C on this proton and this is the well So this is just a dimension entry. Okay, so if you have these conditions, then you have to prove that all the diagonal entries are bounded by this number here. And you can see that this is the right number just by looking at the definition of the hotel matrix and So, what do I mean? I think in order to avoid working with the inverse, we just make this substitution. Basically, c equals to m times p and this makes everything this basically makes everything ping in terms of the matrix n. So, okay, so I'm just gonna give you a very quick sketch of the proof, but in a Edge of the proof, but instead of proving the optimal thing, I'm just going to prove this one, which is actually the one that corresponds to the classical time problem, which is a bit easier to just explain in a few minutes. Okay, so the proof is basically you look at this ellipsoid, which is defined, basically the one defined by this equation in the second condition of the lemma. And you look basically at two-dimensional equations And you look basically at two-dimensional x-rays of this ellipsoid. Uh so given a vector B on the ellipsoid, which is orthogonal to one, you denote H V to be the subspace spanned by P and one. So you look at the, now you define E sub-index B to be the two-dimensional ellipse that you get by intersecting E with H V. H V. And this can be easily parameterized like this. So that was probably straightforward. That's why basically I took B to B or Corbinal to 1 and B in the ellipsoids in order to get a very simple parameter. And now the second condition of the lemma tells me that, well, yeah, if I multiply. Yeah, if I multiply this vector by m, m1 is not one, this is not mb. So therefore, tells me that the second condition of the lemma is one. Tells me that all these products are going to be less or equal to one, right? For all v and for all theta. But this is just uh well a trigonometric polynomial, right? Well, a trigonometric polynomial, right? This just happens to be a trigonometric polynomial, which is basically telling me that it's bounded by one for each, for HP. So I have a family of trigonometric polynomials that are bounded by one. Same thing. And so now, but I also know some information about this polynomial at zero. So if I evaluate it at zero, then this becomes one, this becomes zero, so this is just a period of. This becomes zero, so this is just a parameter once, which is one. And then I can compute the first derivative of the polynomial. I can evaluate it at zero. And this gives me plus. And this thing here is just one transpose times m times v. m is symmetric, so this is just again, and one is mapped to one, so this is just one again. B is orthogonal to one, so this is just zero. So, this is just zero. So, we get that the first derivative is zero, and now we compute the second derivative. So, it's very easy. And then you evaluate it at zero, and you get this. Oh, sorry. So, the second derivative is equal to that. So, now we're in a position to apply a very well-known inequality for trigonometric polynomials, which is called Bernstein. Which is called Bernstein's inequality. So it tells me that the L infinity norm of the derivative is controlled by n times the L infinity norm of the polynomial. So if I apply Bernstein inequality twice, I get obviously n squared here. And then I know all my polynomials are bounded by one, so I have basically that. In particular, the second derivative at zero of all the polynomials is less or equal to. Is less or equal to n squared. So I have this for all b, which is on the ellipsoid and it's orthogonal to 1. And this gives me this inequality for all b in here, right? So now, basically, the last step is just to pick the right vector b. And I just pick the vector b which is on the ellipse and it corresponds to the x. To the eigenvector or turnout to one with the largest eigenvalue lambda. And I can compute this, which gives me n times lambda. This will be bounded by n times n minus 1 by this inequality here. And well, this is just okay, I'm typing. So this is just basically the L2 operator norm of the matrix, which bounds all the Of the matrix which bounds all the diagonal elements, right? And this tells me basically this gives you the inequality. Now, this is for obviously for the classical Planck theorem, but if you want to do, I'm not going to do this because it's a bit more involved, you cannot apply just Burns and inequality, you need to play a bit with the polynomial. But the point is that for if you want to get the optimal bounds, then you have to look at very particular To look at very particular subspaces for each element of the diagonal. So, to get the bound for the first element of the diagonal, you have to look at this space, sorry, this two-dimensional subspace, and this will give you the right optimal bound. And uh I think how much time do I have? Well, I think I can lose the I think that we must talk. So question? So have you tried to apply this nice analytic method to tackle other problems of similar type? Yeah, I mean, there is a very nice problem, which is called the polarization problem, which basically says. Which basically says that so you have a sequence of unit vectors in SME1. Then, so this is a conjecture. Then there exists a Munich vector V here as well, such that Such that the product of the inner product should be at least n to the minus n over 2. So, this is not been solved yet. So, sorry? Okay. Yeah, so and probably this method could potentially give us. Potentially give a solution for this. Ambrose was trying to solve something a bit stronger than this, which basically at the end, this, well, instead of having here the product of the inner products, you have the sum of the reciprocals of the squares of the inner product. And you want these to be less or equal to n squared. Is it something that would imply this equation? Is it something that would imply this equiangular conjecture? Sorry? This implies that you can put on the sphere. Would this follow? Is it stronger? This one? The first one. Yeah, but I'm not sure. But so the point is that Ambrus solved this for the two-dimensional case when the vectors are in R2. But the problem also is still open. And the thing is that for this statement, For this statement, this lemma becomes very nice because instead of having here mkk, you have the trace of the matrix. So working with the trace of the matrix is always nicer than working with the product of the diagonal entries of the matrix. What do you think? Because, well, the trace of the matrix is also the sum of the eigenvalues. So so, but still, I I also try to to to solve this and I'm still trying to do it. Probably eventually I'll be able to do that. Any other questions? No questions, but think positive. 