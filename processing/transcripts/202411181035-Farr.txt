These before just a diagrammatic way to show conditional probabilities and to turn some joint density into a separable set of smaller dimensional densities that have conditionality encoded in them. So for example, this PGM over here, A arrow to B, shows that the joint distribution over A and B can be written as this probability over A times the conditional probability of B. The conditional probability of B conditioned on the value of A. And so this is also a DAG, a directed acyclical graph, and it needs to be a DAG. Such graphs need to be a DAG in order to represent an actual factorization of a joint probability. So a few variables that I'm going to be showing here. I'm going to use this big lambda to talk about population hyperparameters. If we're talking about some parametric model for a population, like a power law to describe. Like a power law to describe a mass distribution, that power law index would be inside of this lambda. n I will be using to talk about the number of observed events. Theta sub i I'll use to refer to source parameters for event i. This d character I'll use to talk about the detection of that event. And then finally, the small d I'll use to talk about the data. And you can think of this kind of like the strain data for a particular event. Good. So this is the general DAG. Is the general DAG that describes our probabilistic catalog? So there's some ultimate set of parameters that describe the astrophysical distribution. Once you know those, you can draw some number of events. Think of these as true source parameters for each of the events in the catalog. And then once you have, let's say, focus on this one here, this event's source parameters, you generate the signal from that for those source parameters, you put it into noise to make. Source parameters, you put it into noise to make data, and then you also need to decide you need to have detected this event in the first place. And so, we say this is the general DAG because this is describing the dependency of source parameters on the overall astrophysical population and allowing the possibility of detection to depend both on source parameters and on data, which is not an actual representation of the detection process with LIGO Virgo. We'll get into that in a second. We'll get into that in a second. So, to simplify repeated observations over the catalog, we can use this plate. So, here, this plate is just indicating independence in the catalog. In other words, you can think of it like the noise and say this data is independent of the noise and other strain data for different events. And the values that we draw for this data are independent of other events source parameters that we draw from our astrophysical population. Astrophysical population. Okay. And so we also have the Poisson part of this. So we can use this calligraphy K to talk about the expected number of astrophysical events or the related parameter K here, which is the expected number of detected events. So we're multiplying the astrophysical detection rate by the probability of detection from our given astrophysical population. And we'll use this shorthand. So when I write down P of D here, Down P of D here, conditioned on hyperparameters. What I'm really saying is that we are marginalizing over the physical distribution. And then for each event that we are talking about, we're marginalizing over the data realizations we had in order to assess the detectability of a given event. And so ultimately, this is basically the fraction of your astrophysical population that you expect to detect. All right. And so putting that all together, so we can distill this. That's all together so we can distill this DAG into our separated joint probability. We have the Poisson part here of counting up the number of events in our catalog. Hey, Ben, we've lost you. The probability we have no observed data given the surface. We would observe data given the source parameters that we expected, that event conditioned on the data that we observed and maybe the source parameters i. All right, I should say please don't hesitate to interrupt and ask questions at any point. We cannot see your slides. Are you screen sharing? You can't see my slides. I am screen sharing. No, this is maybe try try again, try to stop the sharing and share again. It was fine. Could you five slides before? Yes, before, yes. Yeah, we just lost you for like half a minute. Okay, now you're good. Good? Okay. Yes, you're good. All right, so did you see the plate? Yes. Good, all right. Perfect. Okay. Hopefully that doesn't happen again. Hopefully that doesn't happen again. I also saw this one. I think you're going to have to go. All right. Good. Here? Yes. All right, perfect. So here is our joint density separated by events in the catalog. We have our, this is our population model. So probability over source parameters given the hyperparameters that describe the population. This is our, think of this like our strain likelihood. This is probability of observing data, the strain data, given, I guess, that the The strain data, given, I guess, at the source parameters for a given event. And then this is the probability of detecting that event. So how likely it would have been above whatever threshold we implement in our searches. Good. And so I've been hinting at the separation of this detection and what detection is conditional on. And so there's two different approaches that folks have taken with astrophysical inference, with the Gravitational Wave Transient Catalog. The gravitational wave transient catalog. And so Reed and Maya in their paper refer to these two options as physical and unphysical, rather suggestively. So the physical DAG describes what actually represents our data collection and search process. So you define, so if you think about your data, your catalog, your probabilistic catalog in a forward, in a generative way, you pick your hyperparameters. Once you've got your hyperparameters, that gives you however many, you can draw however many events you want to to get. You can draw however many events you want to to get true source parameters for the events in your catalog. Once you have each event's true source parameters, you can generate data to represent the strain data you would expect to observe. And so that involves realizing noise and putting in your signal model in the data. And then once you have established your data, then you can establish whether detection occurred or not. So in other words, if you were running a templated search that you have an SNR for that signal that is above threshold or a false. For that signal that is above threshold or a false alarm rate that is past your threshold to consider the detection. The alternative is to have detection depend on true source parameters, which we don't actually have access to with a real gravitational wave transient catalog. And so I'll say that the methods that use this DAG, they don't explicitly use this model to describe the data. Instead, what they do is they attempt to fit the observed population and then Observed population and then effectively divide out by the estimate of the sensitivity of the selection effects involved with our searches. And Reed and Maya do a really great job of walking through this derivation here and showing that by doing that process, you're implicitly assuming this DAG, which doesn't truly represent how our catalog is generated. And that detection is, instead of being dependent on data, is dependent on truthful parameters, which we don't have access to. Don't have access to all right. So, focusing now on the physical DAG, we can take a look at our hyper-posterior here. So, we've got a prior on whatever hyperparameters are going into our population model. We've got this sensitivity term here. This is quantifying the fraction of our astrophysical population that we would expect to detect. And then we have this multiplication over all of the events in the catalog that give the single event information going into the shape of the distribution. Going into the shape of the distribution. Okay, so now getting to where normalizing flows are going to be helpful. So, looking at this sensitivity term here, in other words, what we are doing when we integrate over the data realizations, we're figuring out what fraction of data realizations would effectively give us a signal above threshold. And the way that we quantify this in practice is we use very large injection sets. We choose some fiducial population models. Fiducial population model, usually very simple. Think power law, mass distribution, uniform and co-moving volume, et cetera. You draw some very large number of candidate sources from that population. You simulate the signals for those events. You inject them into real data, and then you run them through your search pipeline. Once you have those injection sets, then you can estimate this term for new population models by just re-weighing those samples. By just re-weighing those samples, effectively summing over all of the found injections and taking weights that are your new population guess divided by whatever the fiducial population was that was injected. And so this is one case where we have this kind of important sampling integral that I'm going to motivate normalizing flows to replace to be able to fix some of the, I think, limitations that we are starting to encounter in our likelihoods. So, going back to the hyper posterior, we Posterior, we also often marginalize over the details of each of the events in the catalog. So instead of having this full joint distribution here, which is a probability density over hyperparameters as well as the details of each event in the catalog, we often will marginalize over individual event properties shown here and just have a posterior density over the hyperparameters. If we don't care about the single events detail, We don't care about the single events details, then again, this is another important sample integral. So, here is that term for each of the events in the catalog that we have. And if we just do some manipulation of variables here, we can see that if we so we have this fiducial prior or fiducial population model effectively that we have used when we collect single event parameter estimation samples, we do the same thing that we did effectively with the sensitivity data products. We take the PE samples that we have, we compute weights that are our new population. We compute weights that are our new population guess divided by whatever the fiducial prior was that was used to generate the PE samples, and we have another important sampling integral that goes into our likelihood. So why are these a problem? So the fidelity of the model that we can fit to the data is often limited by the accuracy of these important sepling integrals. These important sampling integrals. If you are trying to guess a population model that looks dramatically different than the fiducial prior or the fiducial population that was used for injections, you can very quickly run out of effective samples in these important sampling integrals. And in the case of some of the data-driven models that we've been fitting to the data, the limit of these likelihoods and their accuracy of these important sampling integrals is often the limiting factor in how sharp of a feature we can fit. In how sharp of a feature we can fit to the data. And so, why you might be interested in sharp features depends on the model you're trying to fit. Maybe you want to guess that there is a really very strong low spin population for black holes, or there is a really strongly aligned population of spin tilts. These are just a couple of examples of what you might want to fit to the data, but we can't if our important sampling integrals limit the fidelity of our likelihood. So, there's been a bunch of different approaches in trying to solve these problems. So, for example, you could fit normal distributions to your individual PE samples. Once you fit a normal distribution, then you're effectively interpolating between these densities. You can use KDEs, et cetera. But I'm going to motivate why I think normalizing flows are an ideal solution for this. All right. So, as I've hinted at, I think the data-driven Hinted at, I think the data-driven models are especially a good use case for improving the fidelity of this likelihood with normalizing flows. This is an example from the GWTC3 populations paper, just showing a bunch of different data-driven models that we use to fit the primary mass distribution of the most massive black holes in the binaries in GWTC3. So, my group in particular focuses on spline-based models. So, just to motivate how our models work, so we use Models work. So we use spline interpolation, and the parameters that go into our model, the hyperparameters, are effectively, you can think of it like the control points of these splines. So the y coordinates, maybe the x coordinates of these different knots are the free parameters that are in our model. And we turn the knob on all of those during an MCMC to fit to the population. So to give you an example of what that might look like in a regression context, here's some data that we want to fit some smooth distribution to. That we want to fit some smooth distribution to. These control points are the hyperparameters in our model, and we do an MCMC over the locations of these different control points to fit this underlying distribution. So you can think of this kind of like the mass distribution, for example, in GWTC3. All right, so an example from actually fitting the catalog here are a bunch of realizations of our spline model fit to the primary mass distribution. Distribution. We generate lots of samples from our MCMC, and then when we actually show figures in the paper, we just show the credible intervals. And so, this is the kinds of bands that you typically see in the populations papers that we put out. All right, so GW Inferno is the library that we have developed. So, if you're interested in looking at the SPLINE implementation that we use, you can take a look at the code there on GitHub. But now, to get into why normalizing flows are a Are a useful replacement for the important sampling integrals. So, first, we've already talked about normalizing flows today, but very briefly, they're a one-to-one mapping in an attempt to take samples from some arbitrary distribution and normalize them to make them look multivariate normal. So, I'm going to be using a package by Kays Wong, FlowMC, to do this, but there's several other normalizing flow libraries out there that are very. Flow libraries out there that are very powerful. So, first, I'm going to use normalizing flows in two contexts. The first is in normalizing event likelihoods. So, taking the single event PE samples and training a normalizing flow to learn the transform for each of the single event likelihood samples, quote unquote, to make them look normal. And so, by that, I mean I use a cost function where for each of the PE samples, I use a weight that is one over the fiducial prior. And that basic Fiducial prior. And that basically, you can think of it kind of like imposing a uniform prior distribution on all the parameters. It just limits the bookkeeping and makes something that is proportional to the likelihood, a normalizing flow whose density is proportional to the likelihood. And so this is kind of effectively what I'm proposing to think of replacing the GWTC3, GWTC PE samples with is this kind of a normalizing flow. So why this is useful. Why this is useful is that, in especially libraries that are able to use these transformations, even so not only does it give you a really high fidelity fit to the likelihood, it also gives you the ability to sample in the latent space where everything is multivariate normal and well-behaved. So if anybody has used NumPyro, this kind of code should look familiar. But the idea is you take, we have our catalog of information. All of these are normalizing flows for each of the events in the catalog. Are normalizing flows for each of the events in the catalog. You just draw from your multivariate normal distribution, do the transformation into the physical space, and you now have a bunch of fair draws of what your events could have looked like. So to give you a sense of how good the normalizing flows are in this context, for GW151012, I'm showing three different sets of values here. And they're showing basically all agreement between the PE samples that were collected. Samples that were collected, draws directly from the normalizing flow, and then also sampling the normalizing flow density to draw samples from that likelihood function. And all of them agree very well. All right. And then this is what that can look like in an astrophysical model. So again, this is some numpyro example here, where you then take your draws from the catalog that were from multivariate normal distributions that you do the forward transform for, and then just plug into your guess. For and then just plug into your guess at the astrophysical population. So these are the distributions that I'm showing here. So all in all, once the normalizing flows are trained, the weights when stored to file are at most a few megabytes for GWTC3. So it's a really, I think, effective compression of the transient catalog in addition to being, I think, a more useful data product. All right. And then the other thing we can use the flows for is in figuring out the sensitivity term and avoiding important. out the sensitivity term and avoiding important sampling integrals there. So going back to this term here, this integral over the data for the amount of data that is over threshold, this is often referred to as p det. It's generally a well-behaved function of source parameters. So other effort in doing this has made use of the fact that we do typically use very simple distributions for drawing injections that we use to put into the search pipelines. If you have a If you have a simple distribution with an analytical CDF, then a lot of your work is done already. You can use the probit function, the inverse CDF of the normal distribution, to normalize your samples. That's for the injection distribution, which is slightly different than the detected population. So this is an example with the O3 injections. In black, are showing draws from the astrophysical distribution that was assumed in generating the injections. And then in green, it shows. And then in green shows which ones are actually detected. And this is in that transform space. And so you can see that more massive binaries are more likely to be detected, closer binaries are more likely to be detected, the general things you would expect from this. Yes, question. We have a couple of minutes left. Okay. Got it. So this is the result of training that flow on the injections, and it does a good job of capturing all the important features in the physical parameters. All the important features in the physical parameter space. And so, this is again what that looks like. And so, this gives you the ability to seed your analysis with just draws of values for the CDF. And then you only ever evaluate your detection sensitivity in parts that are relevant for your astrophysical distribution, which is a nice change from what our important sampling integrals have had to do in the past. So, this is an example of fitting our spline model with this type of likelihood. Spline model with this type of likelihood. So, this is the primary mass distribution. And you can see the wiggles here are allowed to get pretty sharp, which is the ultimate goal. And then not only do we get to sample the hyperparameters, we're also sampling the full joint distribution. And so you get all of the useful correlations to be able to investigate which events are driving particular features that you're inferring from the astrophysical distribution. And all of this is very extendable, very easy to run on GPU. Extendable, very easy to run on GPUs. This analysis took like 10 minutes to run on a GPU. So it makes a really efficient, a really efficient MCMC analysis when all put together. So I hope this has given you a good sense of why normalizing flows I think are going to be a useful way for us to encapsulate our probabilistic catalogs going forward and what we can look forward to as far as being able to fit sharp features and interesting data-driven. Features and interesting data-driven models to the population going forward. So, with that, if we've had time, I can take any last questions. Thanks.