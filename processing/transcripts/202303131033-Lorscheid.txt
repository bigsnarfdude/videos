Okay, so welcome back. Our second speaker for this morning is Fondi Van Lorscheid from Dieta University of Berlin and you'll talk about the categories of metroids and makeup models. Thank you. Yeah, thank you. Good morning. I'm quite excited to be here and to meet a lot of new people that I partially met online or that I know from papers. From papers and another guides chance to meet, so it's really a very nice setting here. I'm looking forward to discussing with a lot of you during this week. I mean, I want to talk about a few things where I put some question marks or which might lead to further discussions this week, but we will see how this goes. So, a little bit about my history. So, Federico Setsky is now one of the veterans. He's now one of the veterans in matrix theory. I still feel like a newcomer. Well, yeah, I started with that a few years back when I talked with Matt about his theory of matrix with coefficients. I noticed that F1 geometry has an interpretation of that as a point in the Russian menu. This is somehow how I started to work with Matt. And this is a little bit um the progress of this line of thought that we have This line of thought that we have. And what I want to do here as a first goal is: we have this theory of Matt and Nason for matrix with coefficients. We want to define notion of morphisms between some strong maps with some tweaks. And then this I want to do with care and really pass ideas to you. And then later I will speed up a little bit and it's more a story how to put this in geometrics. How to put this in a geometric setting using one geometry and maybe link this to some other properties in metric theory. And I want to exemplify this with a top polynomial and some results from that. So that's a little bit that. So the first thing that I want to do, let's see, is to review Baker Bowler theory, which I don't know if everybody is familiar, and for that, we need to introduce as a first. To introduce as a first thing an algebraic object that's the backhaul of that. And the object that I want to use are bands. So I have to excuse ourselves for introducing so many different algebraic objects, but we do a constant effort to kind of simplify things and get a robust notion. So bands, it's a generalization of rings, that's how you should think of it, and it's somehow the most. And it's somehow the most simplistic structure that came up with that do-the-job, that cover all most of the things that we concern. So it's a special kind of order blueprint, if you know this before, but it's a little bit simple. So let's do that. And it's a sorry, I didn't say this. So everything is based on ideas together with Matt Baker, Manuel Jarhan and Tong Jin. The latter two are belong to the younger generation. Belong to the younger generation. I'm just saying that they're drawing into this community, and they have a lot of good peers and it's good to work with them. And Bent is a project that we're writing up. Matt's together with Star. With no transition to becoming a. Yeah, so give it two or three years so you know this will help. Okay, any questions on what I so yeah, first thing are monoids, so pointed monoids, which are monitors with an extra fixed element zero, which satisfies that zero times A is zero. My monoids are all commutative and multiplicatively written in this talk. Written in this talk. And in a monic, we can define the unit group, like in a ring. These are all multiplicatively invertible elements that multiply to one with some other element. And a point monoid counts with the ambient semaring, which is basically the monoid summaring. Write this ambient summaring as A plus, but with a little tweak that the zero of A is identified with the zero in the summer. Identified with zero in the summary, so we divide out this relation. Yes? Do you really want one times a to equal one? One times a big one. Oh, no, no, it should be a. Oh, yes, thank you, thank you. That's very embarrassing. Yeah. Okay, so there's an ambient semi-ring, and the monod embeds as multiplicative. It embeds as a multiplicative submonit just by mapping 0 to 0 and the other elements to 1 times the elements. The elements can be written all as finite formal sums of elements and monitors of the motor 0 by junction of sets. And last thing that we need to know from this slide is that the notion of an ideal of this ambient summary. Of this ambient semi-ring. This is just a subset that satisfies the usual axioms for a ring that contains zero, is closed under addition, well, it's a semi-group to be precise, and is closed under multiplication by FLOSB. Okay, this allows us now to define a band very compactly. Namely, a band is a pointed monoid with some extra structure, namely with an ideal NB in the ambience. B in the ambient summer ring, which we call the null set, such that every element A in B has a unique additive inverse minus A, which is characterized by the property that A plus minus A is an NB. There's no other element satisfying with this. Okay, so eventually bands will satisfy two fusion axioms, which are important for certain purposes, but I wanted to keep things simple here, so I just remove some. I'm sorry for. remove them. I'm sorry for that much and wrong. But so that is quite quite simple object. In the node set, you should think about it as such form sum of elements to be equal to zero. If we would have a ring what we would do is just divide by this ideal and the sum would be really equal to zero in the quotient. But semi-rings are not so well behaved. So we can quotient by an ideal but Quotient by an ideal, but we might identify more elements than in the ideal with zero. So in the tropical setting, typically we get a zero ring if we divide out anything, which is not what we want. So we have to work with this type of structure to incorporate what we want. Okay, there's a type of notion of morphism, which is basically structure-preserving map. It's multiplicative, it maps 0 to 0, 1 to 1, and for every finite To one, and for every finite formal sum in the node set, the sum of the images is an element of the node set of the image band. This defines us a category of algebraic objects. As I said, it should be thought of as a generalization of rings. I explained in a moment how to think of that. Just one thing to take away. Nedur is something like a field in the category of events, in the sense that sense that the units are precisely all non-zero elements, like fields characterized within the category of rings, right? Okay, now we're ready to go to look at some examples. The first one, let me explain how a ring is a band. We take the multiplicative module of the ring and we define the null set as all finite sums of non zero elements, which add up to zero in fact inside the ring. In fact, inside the ring. And this defines a fully phase form wetting of the category of rings into the category of bands. But let me not spell all details and keep things simplistic. So rings are bands in a natural way. And if R happens to be a field, then the associated band is an idyl. Some other examples of idyls which do not come from fields. Which do not come from fields, but are very important for us, are the following. So the arguably most important one is a Krossna hyperfield, which comes actually from a hyperfield, but if you don't know that, just forget about it. As a band, it's a multiplicative monoch just consisting of 0 and 1. It's the obvious multiplication. And the node set consists of all. And the node set consists of all multiples of one except for one itself. So I mean zero is in there, and one plus one equals zero, one plus one plus one equals zero, and so forth. So it's obviously not coming from a field. It's something like if you want the tensor product over all finite prime fields, because you get all these relations. But let me not make this precise. And this is a basis for usual matrix, as we will see. Matrix, as we will see in a bit. Then there's something like the sine hyperfield, where we have an additional element minus one. The multiplication is as you imagine. And the null set captures relations how sums of real elements are zero. So if we have a non-empty sum of real elements and it's equal to zero, then the sum must contain at least one positive real number and one negative real number. Negative with a number. And that's this condition here. Either it's totally zero or both coefficients are not equal to zero. Then for the tropical geometric in the audience, I will ask the tropical hyperfield because this one they use here as a Berkowitz convention where I use the non-negative real numbers together with multiplication as multiplication instead of addition. I think should explain the question of how. Explain the classical hyperfield more slowly. Why is that what I should explain? So, why is re-explain the classical hyperfield? I'm somehow not seeing it. Why is that the right description of the classical hyperfield? I'm just going to be very basic, so just say it in yeah, just say it in five sentences instead of one sentence. Okay, I mean, as it is, it's a definition. But suppose we know what the Crescent hyperfield is. So tell us what The crest of the hyperfield is. Tell us what it is. Okay, so if you have a hyperfield, then you say a finite sum of elements is in the null set if zero is in the hypersum of the elements. And this does a job for the Kersna hyperfield if you use the hyperfield language. Does this answer you? zero is an element of the sum, right? That's the yeah. And if you take a sum, like one, any sum, ones other than. So ones other than just one one with the whole hyperfield, right? It might contain zero. You also need you can imagine also the correlation. Can I explain slightly good one? Okay. I mean just take this definition for now and just take, I mean it's Just take, I mean, it's a name that has some reason, but we don't need this to understand the reason for this talk. Okay, so tropical hyperfield are the non-negative fields with multiplication as a monox structure. And when it's a tropical sema field, addition is the maximum, if we use this Verkowitz convention. And in the hyperfield, we don't have addition, but it's an old set. And here we use this tropical vanishing condition that if we have. Vanishing condition that if we have finite sum of elements, it's zero if the maximum occurs twice among those coefficients in there. And this encodes the condition to be tropically zero. And I mean, VROI introduced this hyperfield to rephrase tropical geometry. And I think nowadays we understand more or less how this works. And there's a matrix here we attach to it, which are valued matrix or tropical linear. Matrix or tropical linear space. So I come to that in a moment. Then there's kind of a slightly different object which comes from partial fields, which are common in representation theory of matrix. So partial fields are also bands, but side remark, we just need this particular one. It has the same multiplicative structure as a sine hyperfield, 0 and plus minus 1. But the node set is a It's just a minimal node set that's forced by the axiom. So 1 plus minus 1 is 0, and then all multiples of it, because it has to be ideal. So the coefficient here needs to be equal to z. Okay, so these are the main examples. So we only need to cross out the regular partial field for this talk, but I thought you liked these two. I'm sorry. Yeah, right there. Greater than or equal in there? Yeah, zero is always inside of the idea. But so, like, just the element one is in the middle side. No, this is one plus minus one. It's the same n, yes. So it's um like the same. I'm also sort of confused about this last example because one plus one equals one here, right? No, no, no. Here, one plus mine has to be. No, we don't have a sum, so it's better don't think of it that way. I mean, you can, but then probably you're getting confused as Diane, and it's an experiment of sorts that I don't want. It's an experiment of sort that I don't want to do right now because I want to study other things. At some point, Herbert tells us what this last example marks. This is like 0 plus minus 1 within the integers. So this addition comes from whenever we can define the sum inside the integers, which is only the case if the coefficient is equal otherwise, it's going away from zero. Like S is a quotient of R F one, is it? Is a quotient of R, F1 is a subset of R. Yeah, that's also a good way to see it. Yeah, it's a long story, and I tried to shortcut here to get to the next thing. So if that's okay. If there's more doubt in the audience, maybe I spend some more time here. There's some way of seeing it as through this is just completely random. There's some kind of like tropicalization operation on fields, like where, like. Like where, like, you've got, you like, look at a group rank and you look at the elements that could sum to zero, and then that image would be the null set. Yeah, you can, I mean, this what guys you can define as quotients. So, this is great, R, sorry, yeah, this is just any field other than F2 model of the unit group. So, it says if you So it says if you add elements to zero and the sum is non-empty, then you need at least two elements in the sum. That's all. This here is a quotient of the reals, model of the positive reals. This here is a quotient of the subjective absolute value to R greater equal plus model of the kernel. Great. So model model sorry, things that go to one. And this here is a sub. And this here is a sub object of the integers or of the wheels. Oh, that's great. So, like, one of these guys, like Krasnar, is the terminal object of it. Yes, this is the terminal object of adults. Sorry, thanks for mentioning this. And this is the initial object of adults or bands. This was important to say, yeah. Okay, so this one isn't terminal and bands? No, no, no. It characterizes such. It characterizes certain types of prime ideals, but differently than what we saw before. Is it for a nice terminal object? Yes, so zero bands. Okay, into zero ring. Yeah, so the category of bands is as nice as you want, like rings. It's complete, co-complete. Has an internal home, I don't know what. So it's a good category to work with. This is what's important for what we do and why we define things. What we do and why we define things. What we do, hyperfields and partial fields are very auto-categories and algebraically very much more complicated than this. Which brings me to the next slide. So idylls are somehow much better than subcategories. Okay, we love fields, so I'm not claiming that, but they're behaving better than partial fields, better than hyperfields, and they're more general. So this is a good basis for what we're. Is a good basis for what we want to do for beta bolus here in particular. If we want to go to geometry, usual algebraic geometry needs rings, coordinate rings of algebraic varieties, and bands are playing this part for us. Okay, so next thing that I want to do is to review Baker-Border theory in a nutshell. So these are just matrix with coefficients. These are just matrix with coefficients. The definition of a matrix over an idyll is as follows. So everything here is framed over an idyl, and then later we'll push things towards geometry, but for now we are staying in the world of something like vector spaces, matrix over field-like objects. We fix the ground sets E with elements 1 to n and a rank Elements 1 to n and rank between 0 and n. Then the Grassmann-Plucker function is a non-zero and alternating function of particles of elements of E to B. Non-trigger means at least one value, just non-zero and alternating. If we switch elements, we get a sign, and if two elements are equal, it's zero. So I didn't write this out. Again, in an effort to simplify things. The thing that Simplify things. The thing that's really more relevant are the Kloker relations. So these are the usual Plucker relations for Grassmania. So the same normal jumber of exchanging elements and getting a sign in here for all kind of choice set we do. But instead of writing equal to zero here, we say it's inside the node set. So this formal sum of elements, these are elements in B. This should sum to This should sum to zero in the sense that's in this zero ideal. Okay. And a B matrix is just a B star class of a Grossman-Plucker function. So we multiply all values simultaneously with the same unit. That's the action of B star unit and we quotient output this action. So that's the definition of a B matrix. If B is a field, If B is a field, you know the interpretation. So this is precisely a point of the Grassmanning of R subspaces of K to the N, if K is a field. So in other words, a metric over a field K is just an R dimensional subspace of K to the N. So that's the geometric way you should think of a metric, and we come back to that later. And we come back to that later, to the theme of vectors. Yes, don't you usually have to quotient out by like a larger torus to get the interpretation as a subspace? One can also do that, and then you get something which we call rescaling classes, also sometimes called projective equivalence classes. And of a sine hyperfield, it goes under the name of reorientation classes. But these are classes of matrix, these are torus orbits. Of matrix, these are torus orbits geometrically. So you have this torus action on the Rasmanian, and then if you quotient out this action, you get a torus orbit together. So one s the unique inverse in B which is minus one. So interpretation sex. Okay, so that's the definition. Any questions, doubts on that? Okay, let's look at some examples. So first, we want to recover a matrix in the theory, and that's as I announced before. It's the same thing as a k matrix, where we define the Grassmann-Plucker function simply by sending an article of elements to one, if this article performs phases, and to zero if not. Into 0 f0. Since the units of k, the unit group has only one element 1, there's a unique Gross Multiplica function representing this matrix, so there's no ambiguity of multiplying with units here. So this is a way to realize matrix within Baker-Baula theory. Oriented matrix corresponds to S-matrix or a sine hyperfield, and the Rasmus click. Field and the Rasma-Klucker function is literally the same what's called the Schiroto in zero-for-red matrix. Valued matrix correspond to T matrix and this interpretation just comes, I mean Rasmal-Plocher function is just the same as a Rasmal-Plucher function in Raswensel's um terminology, where we have to realize the fuzzy ring as an idle, which is possible. I didn't include this on the slide before. didn't include this on the slide before. For one, it's just probably clarifying to say that the Grassmann Luker functions just literally, sorry, the Grassmann-Lucker relations sort of literally translate to symmetric version of basis exchange axiom. Yeah, I think. See this if you have two terms not equal to zero, there must be a second module. Must be a second multiple of terms that's not equal to zero, and we exchange two elements. Okay, here's Ekerbola's theory in a nutshell without being too precise, but you should kind of appreciate the features of it. It's a hard theory, so I don't want to go through that here. But there are different features that come. So first of all, there's a cryptomorphic A cryptomorphic. Each B metric has a cycle set, which is a subset of B to the N. And a metric is characterized by a cycle set. And even more, Meta-Nasen give a cryptomorphic description of a matrix in terms of conditions on the cycle set, similar to cryptomorphisms of usual matrix. So this generalizes just to all kinds of matrix. There's a good notion of There's a good notion of a dual of a B matrix, M star. And hand in hand with duality, there's a notion of orthogonality. So V to the M just has a scalar product, as you can imagine. The usual standard scalar product being equal to zero means being in the null set. And turns out that cycles are orthogonal to cosycles. So cosycles are just the cycles of the dual matrix. And this orthogonal. And this orthogonality allows us to define vectors as the orthogonal complement of cos cycles. Easy definition. The cryptomorphic description in terms of vectors has been worked out by Laura Anderson in a separate paper. It's kind of subtle, and we come back to this issue of subtleness later in the talk. And finally, also very important, there's a good notion of minors. So visions and contractions are defined. Okay, so these are kind of the central statements of Baker-Waller theory, and we put these things into action soon enough. Here's one last notion that's also important for us. Nedil is called perfect. us Nidil is called perfect if for every B matrix M vectors are orthogonal to covectors. The definition and in general this is not the case. That's the drawback here. That's one issue with vectors, how they don't behave well. I mean all fields, vector spaces, it's just a natural language, but for other objects they're not so stable somehow. But there are a lot of Somehow. But there are a lot of examples. So all fields are perfect. All partial fields are perfect, if you know what. In particular, F1 plus minus. These three hyperfields that we considered, K, S, and T are perfect. But there are many non-perfect adults. And for example, there are many hyperfields that are not idults or kind of others. There are also tropical hyperfields. Kind of pale there are also these tropical hyperfields, so something like coordinate rings of tropical varieties are generally not perfect. I mean, we even, the issue here is that we don't have a good characterization of perfect adults. That's kind of one question mark that I explain later. In more details, it's somehow a very real subset, this perfect adult insight of adults. Hold on to this definition, it comes back into play soon enough. So, this was kind of reviewing now Baker-Bowler theory. And I want to explain you what are morphisms of B matrix. So, for matrix, everybody knows there's a notion of strong reps. And this turns matrix into a nice category that's quite well behaved. Would like to do this for matrix coefficients. Matrix coefficients. I spent a good part of last year thinking about that. And there are some problems that I explain in the moment. And I tried different things until Matt pushed me in the right direction. And then things start to develop. And I think today I'm quite convinced that that's a correct notion of morphisms that we should work with when we want to categorize things. Okay, the first thing is an input from F1. thing is an input from F1 theory on the most naive level of F1 vector spaces. So F1 vector space is just a point set. So we use add a base point zero to our ground sets here. It's a little tweak, but this will pay off in a moment. And in a one linear map between two point sets, this is just a map that preserves the base point with a Preserves the base point with the additional property that two non-zero elements, that two distinct non-zero elements cannot be mapped to the same non-zero element. So the fibers of non-zero elements from E2 have at most one element. Okay, this pays off because we can define an edge joint by just mapping F sharp from E2 to E1, different opposite direction, by mapping an element D here. By mapping an element d here to its unique inverse if it exists and to zero if it doesn't exist. Yeah, zero is mapped to zero. You can think, I call this adjunct because you can reinterpret this in terms of linear algebra. So if you take the vector space generated by E1, E2, 0 becomes like a 0, then such a map would be a matrix with at most one non-zero entry per column and row. Column and row. Then the adjoint here, so the linear map associated with the adjoint, just the adjoint matrix. So it's kind of the adjoint of F1 vector spaces. That's kind of the F1, the naive F1 approach that you take. But we can do this also with matrix. So when you do, then we say the pointed B matrix is a matrix for which Is a matrix for which zero is a loop. So our base set now has zero, but it's done. Important thing, zero is a loop, which means that the Graphic filter function evaluates to zero whenever zero appears among the elements. Since alternating, we can shift zero to FPI or other entry when it's still zero. So if you have a usual matrix, the loop is a loop or channel function. Okay. Okay, so now we are ready to define morphisms for perfect idyllics. So for perfect idyllic everything is perfect as the name says. So we assume on this slide that B is perfect throughout. And we take two pointed B matrix. So the EIs are, as in previous slides, pointed sets. This is zero. And then And then a morphism is simply a strong F1 nonlinear map between the ground sets. Strong. It's a generalization of strong, as we know it, for matrix. And the way that's best to phrase it here is pulling back functions from the vector display. Oh, sorry, this should be a B, sorry, class type. So mesovectors, the set V to the E2. Functions V to V. We can pull back these functions via the map F to V to the E1. And the condition that's strong means that maps co-vectors of M2 to covectors of M1. And then if you run through the standard cryptomorphisms in the case of Skrassner-Huiperfield, you'll see that this characterizes strong map of the matrix. A strong map of a matrix. That's one way to say. I mean, another way to say this is over the Persian hyperfield is that co-cycles are mapped to unions of co-cycles. More familiar. And this defines us a category of point matrix. So these conditions are composable, obviously. And yes. Is simplification of a matrix no longer a strict? This is not an example. This is because then we don't have an adjoint. I mean, if we want this adjoint, which is a dualization of the categories, so it's an anti-equivalence of the categories, then we have to erase this parallel, the simplification. What is this when our ideal is an actual field? If it's an actual field, well, this map F defines us a map of vector spaces, and the condition is that it preserves the subspace. So I want to try to understand that. Yeah, this map here can send certain several coordinates to zero, and others are mapped to other coordinates with conditions that you don't. Coordinates with conditions that you don't map two coordinates to the same one. Generic morphisms, in your sense, are projections, these are coordinate projections. I mean, think of matrix with at most one non-zero answer per column row. It gets a linear map. Like, for example, does it follow from these axioms that the set of things which set to zero is the plan? Yeah, okay, I don't see it, so I can see I'm not sure. I mean, the only condition is that co-vectors to call vectors, if this here it happens to be very large, like the old. This here it happens to be very large, like the whole space. You take then you can take any one linear map. There's no condition on to be strong. But yeah, I don't think so, but let's figure this out later. Not sure if there's more parcelment, but I mean, you should think of, I give you an example on the next slide. Example in the next slide, and go over that. But for usual matrixes, they are just restrictions and contractions and their compositions by the factorization theorem for strong maps. Identifying parallel elements. I I'm also positive what's the catalog. Let's think about it later. Let's think about it later. So I use this theorem as a justification for that. This is a good notion for morphisms and a good way to categorize matrix with coefficients, namely their equivalent characterizations. So a map, F1 linear map between the ground sets defines a morphism. Well, first of all, if and only if the adjoint defines a morphism between the dual matrix. Defines morphism between the dual matrix, so we get really an anti-equivalence of categories, a dualization theory for categories. Then we really have cryptomorphism in terms of cycles. So the pullback of cold cycles is orthogonal to cycles that characterizes property to be strong. In condition two, do we need to sort of take the co loops of M two star and M one star and make them loops to make it? Make them loops to be capital. Since I tried to avoid that. So if we dualize, the loop stays a loop. It doesn't, I mean, the dual of a pointed matrix, it's just the dual of we remove the loop, take the dual, and put the loop back into it, actually. So somehow yeah this sense it's not symmetric. Somehow that um yeah, we need this particular element. We need this particular element. Okay, and then we also have a quick morphism for Rasana-Pluka functions, which is just the usual incidence relation for maps between vector space subspace preserve. So think of the best example is a flag of vector space. So the map F is really the identity. So F of the K is just K. And then what we see here. And then what we see here is just the incidence relation of flex of vector space. But we can do this more generally for f's. Then you get lengthens in the domain of quiver, Rasmanians, these things. So it reflects known relations from algebraic geometry. Any circumstance you can characterize in terms of reference for the functions. And this works if B is perfect. If B is not perfect, things break down. And they're not so nice anymore. And they're not so nice anymore. These two things are still equivalent, but if we use that, then maps are not composable. And vectors behave very strangely. So condition one is very bad. But before I go to that, let me give you the examples. If you have a point matrix for it and a point subset, then the restriction defines a strong map from the restriction into the metric, which I call the restriction fulfillment. which are called for us to fulfill. We can also consider the contraction and get a projection map. And here this is a contraction of pointed metrics. So the loop is preserved even if the loop is tested. Pointed subset means that it contains the matrix? Yeah, it's um we we just define these operations for pointed matrix which it's always kind of sparing the loop and putting it like this. Of sparing the loop and putting it back in its place afterwards. And then the conceptually right motion is to consider point subset which contains error valid axis points. Matrix quotients are automorphisms over the Prussian hyperfield, but then you get more generally the notion of a matrix coefficient over coefficients, non-triple coefficients. And here's a non-example. And here's a non-example that Chris mentioned, identifying parallel elements. It's not a strong. But since we don't have an adjoint for that, I think it's better to take them out and don't consider them as morphisms. Depends on what you do. This gives kind of categorically the nicer picture, the more powerful picture. Okay, so if we want to move to non-perfect details. Perfectly built, and the problem is: I mean, things that I mentioned before, but the most simple case of things that we want to consider is morphisms, restriction, and contractions, they fail to map vectors to preserved co-vectors or the joint preserved vectors, whatever. But they fail to be more physics in this motion. And as I said, I was puzzled for a long time. As I said, I was puzzled for a long time until at some point we noticed that this subcategory of perfect indulgence is very mysterious. It has very nice properties. Very easy to prove that it's closed undertaking limits. It's complete subcategory. So, what we do is, if we have given any adults, we define the perfection of B as a limit over all perfect adults. Overall, perfect adults that receive a map from bell. Um it's um reflection of categories. If I use sophisticated language, more basic, it says that perfection is really a perfect ideal, so it's really the property that you want. It's It's okay, you get the category of perfectly built. There are maps that comp the morphisms are the maps that compute with the maps from B. So, morphisms of B algebras, if you want. And then it takes a limit within the categories within the category of adults, and it coincides with the category as signs. Limit within the category of perfect adults. You can compute limits in subcategory. Um so this guy is perfect and the perfection is as mysterious as subcategory of perfect adults kind of an equivalently hard problem but we can put our hands on it. We know some perfect adults into which it maps so we can just enlarge the node set a bit and we know Knows that a bit, and we know something perfect, but it's not the smallest one. So the conclusion is that the canonic morphism of B into the perfection, into the limit, it's a bijection. It's an isomorphism of the underlying monoliths. The only thing that happens is that we enlarge the node set. So it's something like we generate a perfect ideal. One way to phrase it is that the node set of perfections just intersects. Set of perfections, just the intersection of all perfect node sets that contain the node set of B. Problem? Ah, a mouse. Computer just crashed. Okay, so uh let me restart the computer and Computer and just tell it. So, the big problem is that we don't have an explicit description of this perfection. And we would like something like an algebraic description. If you have a null set take elements, they take certain linear combinations or cancel elements or something like that. We don't know that. So, that's one of the big tasks here to do: to find explicit description. And I think if one takes more energy. And I think if one takes Laura Anderson's paper and works through these rather sophisticated constructions that are in there, that we can derive at an explicit description. But there are some subsequent question marks in theory where we can solve that. Okay, and then the conclusion is that if we're given non-perfect adduce, we just define a We just define a morphism as morphism over the perfection. Or more precisely, we say a pre-perfect metric over B, it's a metric over B perfect. I mean, they have the same ground set, so it doesn't change so much, but we have a larger null set in the perfection, so we get more matrix. So, matrix in the sense of Matt and Nason are pre-perfect matrix. We get a little more, but this doesn't have to worry us, I think. To work because I think. I mean, they're still matrix in the sense of perfection. And then we have a well-working multiple of the machine. Uh uh okay. Do you remember the slides? I think I think you should comment on that. So, this is the last thing that I said. So this is the last thing that I said. We define the category of pre-perfect matrix just as a category of perfection and then we recover Baker-Bola's theory as I mean there are objects in this category, this embeds, but we get a well-working notion of homophysics. Okay, so now I think I have 10 minutes, right? So I'm doing much further than I intended to. Doing much further than I intended to do. Let me give you some impressions on how to generalize this two-matrix model. So this is kind of the end of the story in Baker-Wolo theory with the big question mark, how to find an explicit characterization of the projection. I would be happy to hear your thoughts on that. And I would like to put here I would like to put here. I want to put this into geometry, so I use methods from F1 theory to do that. And here I will not define this nicely, not even have the time, but I cheat my way out. So from bands, we want to get to band schemes, like from rings, to usual schemes. Well, the easiest way to say this is an F and Ben scheme, it's just a representable function the category of bands. Functor, the category of bands. I've taken the onset V on a slash. This could be, should reach the spectrum of V. But I don't want to introduce notation at this point. And then a band scheme is simply a functor from bands to set that's covered by F band schemes. I don't tell you what's an open cover. This is kind of a little longer story, but if you have seen one version of F1 schemes, it's basically always the same for what I'm saying. Basically, always the same for what's happening systematically. We do this yoga for bands. I'll give you some examples in a moment. Morphism of bands themes is just a transformation of functors. So it's just a bunch of maps between the value sets, such that it's functorial. And then two examples. The first one we can define project and space as functions that may. Space as a functor that maps the band to this tuple of homogeneous coordinates. So we take the affine space one dimension up, remove the zero, and divide out the action of the unit group. We can write the elements as homogeneous, in terms of homogeneous coordinates, which are unique up to joint multiplication by the unit. Okay, and then we can also define projective varieties as sub-varieties, in particular the various main is characterized inside. Is characterized inside an ambient project of space by the clocker relations. There are also the alternating relations, well, we have space. But just think of it as the usual resonal geometry. Should consider everything with your intuition from algebraic geometry. Things behave very much like that. Some discrepancies, but in terms of a more factorial view. A more factorial view, it will be hexed. Okay, and then in our first work, we introduced the notion of a matrix bundle. So for matrix, we go to matrix over band, then we geometrize that. And the way we can do that is in terms of a Rasmal-Klucker function, which sends R tuples of E to the global section of some ample line bundle. ample line bundle. This corresponds to a morphism to projective space or more precisely into glass menu because we impose some plural relations on this gross from Pluto functions. And I mean you have to go through a couple of facts but in the end you see that first of all for an ideal mean matrix correspond one to one to matrix bundles like To matrix bundles like vector space of a field correspond to vector bundles or spec k. This transfers here. And second of all, what I just said is that matrix bundles on X correspond to morphisms into the RASMANI. It's just the usual yoga of having section ample line bundle, morphism to project space, which happens to be empty into the RASMA. So, in other words, the Grasmanian is a model space of. The Grassmanium is a model space of matrix bundles. And matrix are just points in the Grassmanium. It's another cryptomorphism of matrix, if you want. As usual in modelized theory, there's a universal family on the modelized space, which is a matrix bundle associated with identity or progressive maintenance. And then a matrix bundle on some scheme is just the pullback of the universal bundle to that scheme. To that scheme. But think of it like vector bundles. So the universal bundle is really like the topological bundle for the press menu. But in this funny language. And now I can, I think that's maybe a good point to stop. I just explain you how to define morphisms for matrix bundles. For matrix bundles, which is also a little delicate, and I just give you a five-step procedure how to get to that. First of all, we can explicitly construct a cycle bundle, the universal cycle bundle, as a sub-bundle of the trivial sheaf of the graph menu. So you know the topological bundle is a sub-bundle of O to the n. We have also a vector bundle where we get to, but vector bundles don't behave well with respect to probabilities. Cycle bundles are very stable. So first we have to be friends with cycle bundles and then we have to lure out kind of the sensitive beams of vector bundles from this definition of cycle. And that's what we're doing here kind of in this procedure. So the second thing, as I said, cycle buttons are stable on a pullback for any matrix. Any matrix which is some pullback of the universal bundle, some scheme, we define the cycle bundle as a pullback of the universal cycle bundle. And the good thing is that this recovers cycles of Vaker Bolos matrix, B matrix, just I mean the usual cycles are will be sections of the cycle bound. Okay, then we are ready to Then we are ready to define the vector bundles as an automotive complement. So, I mean, this trivial sheaves comes with the notion of orthogonality, shiftified sense, and then we find the automotive complement, which is a subsheaf of O and of the vector. So the X problem as for dilds, code vectors are in general not orthogonal to vectors. What we do. To lectures. What we do is now we construct the perfection for band schemes. So we can construct for every band scheme perfection, which is kind of tricky process. And I would like to touch it with my hands by finding an explicit description for the perfection often you do. But you can use a sledgehammer and just bang on X and perfection springs out, but it's a very abstract. But it's a very abstract categorical construction. Okay, and they come as a desired property. Vector bundles are autonomous complex to covector bundles. And this allows us to define a morphism between pointed matrix bundles as near one linear map between the ground sets such that the pullback pulls back cold back. Puts by co-vectors or the co-vector one or two co-vector one. That's a subvaluation. Okay, so this is how you define morphisms for matrix bounds. It's kind of most of it is kind of very conceptual. The only thing where I get value egg is perfection because of it. Really don't understand it in terms of algebraic relations or Okay, let me just I think it's a good way to stop here.