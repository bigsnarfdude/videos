And thanks for having me as a speaker. And well, I will catch up with ensemble control. The things that I'm focusing on is design and feedback method. So that's something that Ganto is not talking about. So the warm-up is already done. And for the design, I'm also catching up the idea of act computationally while keeping think uh keep thinking mathematically. Mathematically, no trends are not. That's a perfect summary of the speech. That's really fetching speech. And then I will later on talk about feedback methods. So the setting is the same, so I'm having linear systems. Here I'm also focusing on discrete time systems. So that's different from the previous talk. And I will mainly focus on discrete time systems. Focus on discrete time systems. From the theoretical point of view, when you want to come up with criteria, everything stays the same. But when you want to talk about how to compute inputs, then the setting is different, and I will explain this in a second. The parameter space is again a compact set with empty interior and a complex plane, and its complement is connected. So that's fine in that talk. Yeah, the solution is denoted with this phi. Yeah, and the solution is to note that it is fine. So the basic problem is suppose we have an ensemble reachable system and I'm talking about reachability because it's discrete time as well. So their reachability and controllability is different. So when we talk about continuous time systems, reachability and controllability is the same, but in discrete time is different. So assuming that this is ensemble reachable is just equivalent to saying that when we have given a To saying that when we have given a target function and an epsilon, we can find a solution. So the problem is solvable, but how does the solution look like? That's the topic for the next slides. How really to come up with an input? Because Andrew was mentioning that there are approximation results, but these are existence results, right? And I mean, some of them are constructive, most of them are not, and I will explain. Not and I will explain how to proceed in that case. So the first thing is consider the solution formula, right? And in discrete time, when we have a single input system, the input are just scalar values. So we can write this as a polynomial here, right? And then we're immediately in the setting that was discussed previously. Can we find a suitable polynomial? And that's equivalent because the input values are just the coefficients of the polynomial. Input values are just the coefficients of the polynomial. We need to find the coefficients and the degree. And the degrees, the time it takes to ski the origin towards a given function. So that immediately yields a problem of complex approximation, for ideal approximation. But in continuous time, we see as an integral operator. And so that's an integral equation then, and you need to somehow solve approximately a. Solve approximately a linear integral equation. And then you're in the business of inverse problems. But I think the techniques are not immediately applicable in that case. So you have some regularization techniques and stuff, but I'm not totally convinced that these are the real tools to use. Okay. So the starting point are these sufficient conditions because at least we were... Because at least we want to know that the problem is solvable, right? So the first two are the necessary conditions that Gantra was mentioning already. So we have pointwise reachability, we have the spectral disjointness, and this, what I heard this morning, the magic condition. But anyway, it is a magic condition. And the first sufficient condition that Huber and I myself were coming up with is that the eigenvalues are simple. So the necessary conditions in place and The necessary conditions in place, and either as one or as two. It can happen that both are satisfied, but at least one of them should be satisfied. For each case, I will present some methods how to extract these things. And so let's start with the first one. And that's probably, I can catch up here because Punta was already mentioning, that's the idea of the proof, right? You take a state-space terms or a coordinate term. Space terms for a coordinate change, and then you take the part of the characteristic polynomial that the coefficients are constant, and then you define this fancy polynomial. And Gunter was doing that on the blackboard, and here I can just summarize what he did. The fabulous or remarkable fact is that when you apply this polynomial here to that matrix, you can decompose the n-dimensional problem. Pose the n-dimensional problem to n one-dimensional problems. So you have some kind of degree of freedom here. We have polynomials that we have to choose, and each polynomial is then responsible for approximating one coordinate of the target function. So we can completely separate the problem into n one-dimensional problems. So what happens if you have multiple eigenvalues there, this or fails actually? Or fails, actually. Right, good point. So we're going back. So this sufficient condition is necessary, but not on the entire parameter space, but on an open and dense subset. So the only thing that can happen is that we have isolated multiple eigenvalues. Is the geometric and algebraic multiplicity the issue then? That's an issue here, right? Issue then? That's an issue here, right? So, when you have off-diagonal entries in the Jordan block, then you're lost. My understanding we put in the talk this morning is that the geometric multiplicity had to be one. Yeah. Yeah, that's right. So that's an issue. When you have multiple eye values on an interval, then you're lost. The only thing that can happen is that you did it on isolated points, and this condition here is able to This condition here is able to cable that case as well. Because maybe you could have this payable satisfied. I forgot that. Yeah, I can write down an example. That's a very well-known example. It's just the harmonic oscillator. Here you see that the characteristic polynomial of A in the barrier is just c squared minus plus plus theta squared. And you can take the parameter interval, just the unit interval. And you see that when you're not in, when the parameter is not zero, Not in when the parameter is not zero, the eigenvalues here are located, let's say, on the imaginary axis. But when it comes to zero, the entire imaginary axis from minus one to one, that's the spectrum of the eight. And then zero, you have a double eigenvalue. And all the others are. But the geometric, the multiplicity is one. That's important. Yeah, that's important. Yeah, that's important because you have to get this. That's also important when you want to, with change of coordinates, bringing it into the Jordan canonical form is not a good idea because of the multiplicities. Okay. So we have to solve these one-dimensional problems. And on the next slide, I just recap how the Slide, I just recap how the formula looks like. So, that was also counter mentoring today. We are mapping the parameter space to this image of this function here. And the topological properties are that the image is again a compact set because K0 is necessarily one-to-one. And when you extrict it to the image, then you have a homeomorphism. There's also empty interior. There's also an empty interior, and so here we have a polynomial, and here we have a continuous function, and the approximation takes place on the image of this set, of this function. In that case, the approximation will take place on the imaginary axis. Okay. So the question is: how to find this polynomial? Also today, this morning, the Weiestas result was mentioned, and here is, that's from, yeah. This is Megillian's result, and that's the definite answer. That's if and only if, and here's only the sufficiency statement, because that is, these conditions are necessary, that was well known for around the 19th century. Around the 19th century. That was well known. Guys in complex approximation were trying to prove this result for at least 30 or 40 years. And finally, Megillian came up with a proof. And they were just thinking that this result is too good to be true because they were failing to show it. Yeah, right. So the topological assumptions on P is that are on this ombud. Assumptions on P set on this omega here. That's the model of this set here where the approximation takes place. It has to be compact, complement connected, and you have continuous function on the entire compact set. And if you have interior points, then the function has to be analytic. And analytic here means polymorphic. Not real analytic, but. And then you can find a polymorphic. This nice book here is a good reference, also for historical comments and so on. But the proof is highly non-constructive. And so it gives us criteria, but when we want actually to compute something, this is not of use for us. And the easiest thing that you can start with is: okay, let's say the parameter space is not on the integer. Not on the internet imaginary axis, but you can extend this to this setting as well. Let's think about that. The spectrum is part of the real axis. This is A0. P is this set here. And you add also to add a bit more of regularity to the target functions, and then put And Bernstein polynomials are appropriate to approximate it, and you have an alternate, an error bar. Once you have an accuracy given in advance, this gives us an estimate on the degree it takes. So, how large, what's the degree of the polynomial is given by this error bound, and the coefficients are given here by the polynomial. You can compute the coefficients in terms of the target function. So once you have the degree and the target function, you can immediately compute the polynomial. And the polynomial gives you the control inputs, because it's just the coefficients of the polynomial. So these are things that are required. We need an error bound for the approximation and an idea how the polynomial looks like. And then we are fine. And interestingly, I mean, the Weierstadt. And interestingly, I mean the Weierstas result was from 1885, and more than 100 years later, that's the only real explicit estimate that I'm aware of. If anybody has other precise error bounds, then I'd be happy to know them. That's the only reference that I'm aware of. Okay, so with this result, we can... So, with this result, we can do the following procedure. We have a real interval, and then we do a change of coordinates, and that's the new target function. And once we have, if this satisfies a Lipschitz condition, then we can take as the polynomials, the Bernstein polynomials. And that's the function here. And here, the other subscript is the degree. And the degree is given by this error bound, which I've just Which I've just discussed. And then we just plug this into the polynomial that we were constructing. And then we just need to come up with a monomial representation. And then we have the result that this polynomial here gives us the finite time it takes to steer to this target function and the input values. Yes, we're done. That solves really the problem. Because with this construction, we can guarantee. Because with this construction we can guarantee that our target state is in an epsilon neighborhood of the target function. Okay. But obviously not all spectra are nice, that they are just located on the real axis. What happens? Sorry, that's something I will discuss later. The other condition is the sufficiency condition that we have real simple eye values. Real eye simple eye values all over the time. And then we can just diagonalize the system. And now things are a bit different than previously. Because when we just apply a polynomial to this diagonal matrix, it turns out that we are not able to decompose it into n one-dimensional problems, but we can decompose it in one polynomial that has to satisfy n. Has to satisfy n scalar problems, but simultaneously. So the idea is if we can find n polynomials that satisfy each of these one-dimensional problems, the task is then to come up with one polynomial that satisfies all of them. And the idea is, of course, I think sketched this morning as well. sketch this morning as well. So we will say p of z is equal to p one of c times q one of oh sorry that's k here k of c from 1 to n. That's on this slide. So here that's the construction. And eigenvalues. So let's say they are completed somewhere. They are on the real axis. Somewhere that you're on the real axis, and you have a0, a1 of p, but you can also have an eigenvalue curve which is in the complex plane, doesn't it? That's fine. Then we can approximate on each of these domains here, right? And we would get these p's. But yeah, we need to glue them together in a certain way. And here comes this image. These indicator functions that were present this morning as well. So we can define indicator functions and enlarge this area a bit because we have spectral disjointness, so we can enlarge this set a bit such that we have this one here and this one here. And then we can define a function h, let's say for one. h1 of c is one. Is 1, which C is an element of A1 of B and 0, C is an element of AK of B. So let's say these are just two of them. Right? And what we see is, since these domains are disjoint, that's a holomorphic function. And holomorphic functions can be Functions can be approximated by polynomials. The tool is the second complex approximation result that started this whole machinery. So Runge's result was published in the same year that the Weierstrass results. And it's a bit different. Here you have a compact set in the plane, a complement connected, and the function should be holomorphic on a larger set. Holomorphic on a larger set because so this omega here is just a compact set and so we want to approximate it, it has to be holomorphic on a larger set. And these orange areas are this larger set. This can be easily extended to a holomorphic function on the larger set. And that's the function that we are approximating. The important part is at this That this result can also be proven explicitly. You can put in the target function and the epsilon and construct the degree and construct the coefficients of this polynomial. It's more complicated than the Weierschlass or the Bernstein polynomial, of course, but you can do this. Yeah, I've done this, and it's not available in classical textbooks, and I know now why this is not available. Know now why this is not available, but it takes a lot of computations. When you prepare a course and complex analysis, that would take too long to prove this result. But when you want to use this for constructing inputs, you can use it. Okay. So again, when all the these images here lie on the real axis, Here lie on the real axis. We can use the Bernstein polynomials for these guys here, and polynomials we get by applying Rune's result, and then we're fine. But still, we have only considered the case where we have here real integral. What if this is not the case? So then we're in this case. So then we are in this case where we have eigenvalue curves or arcs. And when you look at the literature, you can find this result. It's also a special case of Legend's result, and this was shown 25 years earlier, where he was starting to extend the results, try to get to the most general setting. And if your parameter or your domain is a Jordan arc, Your domain is a Jordan arc, and you have a continuous function. I mean, Jordan arcs have did not have, or do not have interior points, so no analyticity condition here. And then for each epsilon, you can find a polynomial. The good news is you can use this result, or you can use the proof of this result to construct the polynomial. The only thing you have to do is The only thing you have to do is you have to use conformal mappings. I will not explain this in with formulas. I will show a picture and try to convince you how the construction procedure works. So you have your parameter space and you have, let's say the Jordan arc, A of P. A of P. So that's the mapping A. And from the parameter space, you have your target function that is a mapping into the complex plane. And the idea now is you have a Jordan arc, you can continue this, or you can close this to a Jordan curve, and without loss of generality, you say the origin is contained in this Jordan curve. Fine. And the thing you have now. You have now at hand is you have the Riemannian mapping theorem. Namely, there is a whole conformal mapping that's called as phi. That maps this guy here to the closure of the unit disk. Well, the origin is mapped to the origin, and the boundary is mapped to the boundary. And this can be done such that it is homeomorphism on the boundary, so you can go back here. You can go back here, and you can also go back here. These are homomorphisms. And then you can define a mapping from here to there. Namely, you just take the mapping F and do the composition with A inverse and phi inverse. So that's then a mapping wish from the unit boundary of the unit disk. Of the unit disk to C. And why is that of use? Well, the thing is that this is just the second result of Weyerstrasp, where you approximate two pi-periodic functions. Because two pi-periodic functions are just functions on the unit circle. And that's a function on the unit circle, and you can approximate this. Circle, and you can approximate this with a polynomial. And the result that Bernstein uses, because we need to have an error bound, and this error bound was known way earlier than the other one on the compact integral. And also you have an explicit representation of the polynomial named the Fayes polynomial. So that means this function here, this can be approximated. Be approximated by a polynomial that's called capital F to the N. So the thing is that we have to do is how to come up with conformal mappings. That's a different topic. I mean, people in complex analysis are writing whole textbooks on how to construct conformal mappings, and there are tools and group of Tools. Boy Treffethen wrote a book using Schwartz-Christoffel mappings, and there is a other I forgot the SIPA algorithm for numerical computations quite good. And so that's a different story. I'm just using on the big mathematically and want to do the computations. And these are the These are the tools that I would guide. And one side remark: I'm a bit cheating here because when you look at this polynomial, k runs from minus n plus 1 to plus n minus 1. And you have c to the k, so that's a rational function. It's not a polynomial. So when you want to have this equation, you need to come up with a polynomial description of n. Polynomial description of n of phi c. You need to have a polynomial for that one. That means you need to find polynomials for 1 over phi, and you need to have a polynomial that approximates the conformal mapping. But these constructions can still be reduced with conformal mappings to the Conformal mappings to the to Rumis results. Everything reduces, or construction procedure reduces everything to the to ruminous result, but the only thing you have to do is you have to come up with conformal mappings. But there are techniques for this. Okay. So on switching topics, because that was addressed by, I think, Matthew asked a question about. I think Matthew asked the question about what about stabilization in this area. And so, from a conceptual point of view, when you want to talk about feedback and the inputs are parameter independent, so the feedback is in that case in this infinite-dimensional setting an operator mapping from an infinite-dimensional state space to the finite-dimensional some finite-dimensional space. And, well, a good natural. And well, a good natural choice would be somehow an averaging operator, in that case, an integral operator. It should be a bounded linear operator because we're talking about linear systems. So the multiplication operator was a bounded linear operator, so we want to keep in that class. So F should be at least a bounded linear operator. Okay, so let's analyze what are the properties when we use this. Properties when we use this. And here is again what is written down. So the overall system now here in discrete time looks like that. We have this A term, the multiplication here, and then we have this feedback term. And I'm also using here a mixture of open loop and closed loop and closed loop. So in terms of these multiplication operators, we have a linear system of that form. And that's the thing that we want to use. And stabilization would then be just. And stabilization would then be just take u equal to zero. Okay, so the first thing is that for this concept of ensemble reachability, this is invariant under this class of feedback, which is natural, right? You do not change the reachable set when we add something here. Okay, and that's true for all bounded linear feedback operators here. Operators here. That's as in the finite-dimensional linear case. Okay, but when we switch to the stabilization problem, namely is can we find such an F such that this system is asymptotically stable in the sense that for each starting point we approximate, or we in the long run, we reach the origin, and then we see And then you see, well, ensemble feedback cannot be useful for stabilization. And why is that the case? So, f is an operator that maps the infinite-dimensional space to a finite-dimensional space. When this is bound linear, and if the range is finite-dimensional, it's a compact operator. And since this multipliation operator is also bounded linear, this is a compact operator. This is a compact operator. And with a compact operator, you cannot change the essential spectrum. Either it is stable or it's not stable, but with this feedback, you're not able to change the spectrum. So this class is too restrictive that can be used for stabilization. That's sad news. But in some sense, there is a motivation for the class of feedback that I was. For the class of feedback that I was considering earlier. So we're considering relaxed feedback, ensemble feedbacks, in this sense that I'm allowing that the feedback operator is also a multiplication operator. Like the state operator and the input operator, they are all multiplication operators. So let's take just also multiplication operators. And so the problem becomes once we have A and B, Have A and B. What are nice sufficient conditions that we can find a matrix function f such that this system is ensemble regional? And the good thing is that we have these pointwise checkable conditions because when we want to see if this is, I mean we could use the characterization given by the Jani, where we have all these, but then we have to compute all the powers of A minus B. Compute all the powers of A minus BF, which is no fun, I think. And so the idea is to use these pointwise checkable sufficient conditions to come up with conditions that gives us the thing that we have to add to pointwise reachability. I mean, pointwise we need to have a this is pointwise reachable. That's the least thing we have to assume on on on A and B. On A and B. And the easiest case is that we have a single input system. And if this is point-wise reachable, then we can find a feedback matrix, so this is just a row matrix here, such that A minus B F is uniformly ensemble reachable. And the idea is very simple. We can just go to the finite dimensional case, define ourselves some kind of Some kind of eigenvalues, eigenvalue curves. Let's say they are all on the real axis. And then we can define this characteristic polynomial just as the product here and plug this into Ackermann's formula. This F here is also again explicit, can be explicitly computed. By the way, now I see. By the way, now I see why. So f, that's not the target function, but that's that's a feedback. So sorry. Should be a k maybe. Okay. But that's one thing why I think this is interesting. And another thing is when we talk about the location of the spectrum, when everything is contained on the real axis, things Things work out very nicely. But the idea is to come up with feedback and to enforce our system with the feedback such that all spectra are located on the real axis, so everything is fine. That's another motivation. While a mixture of open loop and closed loop inputs is of use. How many times have it? I think it's already passed. Okay, okay, then. Okay, then. I'll skip the multi-input case and this construction and say wrap up with a summary. So I'm have considered constructive methods and feedback methods. For these kinds of systems. And the next steps will be really to figure out how these kind of these conformal mappings, I mean, there is some kind of freedom, right? It's not specified how we close the Jordan arc to a Jordan curve. There we have freedom. How can we use this freedom for computations? And also, can we use this degree of freedom to add? Degree of freedom to actually come up with nice conformal mappings. So that's the essential part. But here we have freedom, so that's kind of can be done simultaneously. And this is also important when we want to approximate these two objects. This heavily depends on how the structure of this. So there are good techniques, and these Warz-Christoffel mappings work good if this is a polygon or something, but we are able to use this. Or something, but we are able to use this. Maybe we can also approximate the door mark by a polygon. So there is a lot of work to do. And in the literature or complex analysis, these topics are not covered. At least I haven't found them. I've talked to several complex analysis people and they were, you know, I don't know. Here's a book on conformal mappings, but I did my finding. Yeah. Yeah. Okay. And also another idea is: I mean, that's following the original proof of Ward's result, but maybe there is a direct one, because, I mean, that's also a simply connected domain. If you do not close the loop, I close the Jordan arc to a loop, Jordan curve. Maybe can we just apply a conformal mapping to that in that case as well. But I have some ideas. We did not get it that far. We did not get at that far to to tell you about the process, but that's certainly one thing where we do not need to do this kind of construction turnaround. Here are some references, and thanks for your attention. Any question while then if you were to use closed loop uh feedback only, no open loop mixture, um Open with the mixture. The limitation you mentioned about moving the spectrum with the contact operator, would that still prevent any reasonable notion of approximates to this? Maybe the notion that you talk about is already sort of approximate in in some sense, but you stop saying approximate when you move to the more abstract setting. Um I'm so that was I'm not sure. So that was I'm not sure. I'm not sure. I'm not. Currently, at the moment, I'm not aware of this approximate stabilization. Could I ask a question on the ether? Yeah, please. It's about something high. I'm just curious because I haven't looked at this field for 40 years, but in the early 1980s to mid-1980s, there was a huge amount of activity on Of activity on parametrized classes of systems and how to build feedbacks that would depend nicely on the parameters. So, here the parameter was continuous, which is like having a finance about parameters. And there were some beautiful, you know, topological theory, we had obstructions in terms of homology and triviality of tangent bundles and things like of that sort. Is any of that relevant to the current way of looking at problems? Way of looking at problems? To be honest, I'm not aware of these works. It was a hugely active area and control theory. It came from so-called systems for rings, but then, and Triffon knows about this. But in particular, a sub-area of that was parametric families of systems of linear systems. I'll tell you some. I know some words which feedback things for finitely many systems, but they're simultaneously that this is this old work that doesn't depend on the grammar. So you don't see the streamer that's special and you can find out when you find the key that also depends on the.  Well, thank you for the basic function. Yeah, I think that it could be I think that's the only thing just yeah, if you go outside, there's nothing too simple for us. So yeah, I wasn't there. I also did one. Sorry guys, that's I don't think I'm gonna shall break until this is done. They're looking into it. It's a security alarm, but it's not the planet test. And it's something that you don't like the smell. The logic is right in the middle.  Talk about formally outside. Sometimes I think those things we can step out of my job.