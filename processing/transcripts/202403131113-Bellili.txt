Okay, so thanks. I would like to thank the chairs for their kind invitation and a warm welcome for everyone for attending my talk. That's a long title. This work is done in collaboration with two PhD students and Emin and we. So I think you have had a lot of motivational slides for why social access is important and so on. So Social success is important and so on. So, in the interest of time, I won't have to upcap, so I just try to go with the punchline. So, here I'm considering a toy scenario of unsourced random access where I have two users which would like to transmit their packets over two slots. So, this slotting idea is crucial because to manage computational complexity. So, as So each user will divide its two packets into two smaller chunks. This one is smaller than this because there will be concatenated coding added and that concatenated coding is useful so as to stitch the sequences together once they are decoded separately in every slot. So that's the coupled compressive sensing idea. So then So then each binary sequence here is encoded separately. And then during slot one, user one and user two just transmit their first binary sequence. And here you can write that model over this slot as the code book here, a matrix of code book. And then you have the channel here, which is in the case of multiple antenna. In the case of multiple antenna, there is Brooks parsity. So here we see two rows corresponding to two users. Then you can use AMP, MMB, compressed sensing to infer or detect the transmitted sequences over that slot one. And the same thing happens over slot two. Okay, so the problem here is that because you have separate processing over each slot, the message The messages here, or those chunks, they may come out of order because this matrix multiplication is invariant if you permute the rows and the columns. So there got to be some kind of stitching here, and coupled compressive sensing, they introduced this tree-based stitching, which is basically using the information, the redundancy that we have introduced here, so that we can stitch. Have introduced here so that we can stitch the sequences. Okay? So, with that being said, this talk is about another perspective where we remove the need for concatenated coding and therefore we also run it. Okay? And you see how we're gonna do that. So, basically, because we don't have concatenated coding, so each user would split again, so it's slotted transmissions, but into equal size binary sequences. And then you have those. And then you have those received signals. So, in principle, you could now again play the same game and then reconstruct using compressive sensing in every slot, but then how would you stitch? So, the idea was then to somehow deconstruct this matrix, as I will show in the next slides, write it as a product of two matrices, and then as so you will have a matrix delta here, right? Delta here, right, which is telling you which sequences here have been picked up by users in every slot, and then there will be the channel matrix here. Again, in principle, so this is a bilinear program here. You have two unknown matrices, okay? This is basically telling you the formation sequences that were transmitted, so the positions of the non-zero coefficients, and you have the channel as well. So, if you can, this is a bilinear problem which you can solve separately in L. Problem which you can solve separately in every slot, but then how is it gonna stitch? So, the stitching here will rely on the spatial signatures. So, you see here we have the same channel, okay? So, because these two, if you want, recovery problems are reconstructing the same channel, so it makes sense to make them communities. And by doing that, you have that joint processing which will pop up those messages in the right order, okay? So, in the same order for Okay, so in the same order for user one here, user one here. So basically you just stitch in the order in which you reconstruct the messages. Okay, so that's what happens. So that's how we move and we don't need that three-base stitching. So in the few coming slides, I will just go over how we can deconstruct the channel and then how we can deconstruct and so on. Okay? So here is again slot one, user K. So the transmitted code word is basically you can write it as the code book matrix times a vector here, which times a vector here which has a position one which corresponds to the code word that this vector chooses. So that's the transmitted code word which you can write as C times delta K if you call this delta K. And then the received signal corresponding to that user K is basically that transmitted code word times the channel. Which you can write as C times delta times H. And you have the same thing for all the users. You have the same thing for all the users, and here you are summing up all seed sequences. Okay? So once you do that, that's what we have here. You can basically stack all those deltas here corresponding to all the users in a single matrix here. And you have different positions for the ones corresponding to the different code words that are selected by users. And you also stack the channel vectors here. Vectors here in one matrix, and then you have this decomposition. So, this, as I mentioned before, these are the two unknown matrices that we would like to reconstruct. So, all of this is over slot L. So, I'm just considering still one slot and then we will expand to multiple slots. Okay? We will have to do if the channel is also, we have some structure and prior, we'll have to declare. Structure and prior, we'll have to deconstruct that channel. So basically, you write H transpose as F remission, F is spatial DFT matrix, so that's equal to identity. I didn't do anything here, okay? But then, as soon as you do that, this is the angular domain channel, which is sparse if you have multiple sparse or multi-part propagation, okay? And then you just plug that matrix, so you plug this, substitute this for. So you plug this, substitute this for H transpose here, and you have another factorization of your matrices. So now the goal is still a bilinear problem, again over each slot, where the two unknown matrices are these, and these are known matrices. So this is the code book and this is the DFT matrix. So we will use this, what we call general purpose pipe bump algorithm. So this algorithm is based. So, this algorithm is basically constructing, as I will show in the next slide, I will spend two slides trying to, or three slides trying to explain how this works because we would like to customize it. We have to go inside and then try to modify the messages and so on, so that we can make that joint processing happen. Can you explain a little bit why you would like to go to the spatial DFT domain? Yeah, so if you have those channels, if you have Channels, if you have multi-path sparse propagation, if you multiply that channel by the DFT, spatial DFT, that matrix is sparse. So it deconstructs a channel. So this is a sparse matrix and this is the known DFT matrix. Okay, so as I've said, this algorithm reconstructs two unknown matrices, U and B. So here we have four matrices, but two of them are known. Here we have four matrices, but two of them are known, so you can just call this U and this B, but then you'll have to exploit that structure, so that's why we have to modify that frequency. Okay? So here is the general purpose by BAMP algorithm, which was proposed in our group back in 2022. So there is a more detailed version on Archive. So it's a bias optimum algorithm, so it relies on message passing and so on. So here you are interested in finding the posterior of those two matrices, okay? These are two unknown matrices. Matrices, okay? These are two unknown matrices. So you write the by S formula, and then here, once you condition on U and V, those become separable, okay? And then you have, we assume separable, so it's very important to understand here that the here we assume separable price on the rows of U and the columns of V H or the rows of U. Okay, because later on in unsourced access we have another basic structure and then we have to customize And then we have to customize this fact. So if you have this, then that's my factorization. Just I will call this factor node Fij for the convenience, and that's the factor graph, how it looks like. Okay? So you can show that these messages are Gaussian, like the same mechanics of A and P and so on. So using those quadratic approximations and central limit theorem. So we show that those. Linear theorem, so we show that those outgoing messages are a broadcast message, so it's Gaussian. And the case of conjugate priors on the rows of and columns of V, you can basically denoise. If this is Gaussian and this is Gaussian, or it is something that you can denoise from a Gaussian, you solve there, and you construct the two meters. Now, the problem happens is that in many practical situations, these are not conjugate priors. So, you have, for example, binary discrete or whatever. Binary discrete or whatever, sparse. So then we use the same idea of what. So we just split those vectors here. So you can think of this as one arm here just unrolled. So I'm just showing one of these here. Okay? And this is collapsing all the factor nodes here and here at the same time. So we use the auxiliary variables. So here we split every UI into two auxiliary variables. Into two auxiliary variables, and same thing for VJ. And then we will use the fact that this is Gaussian, and we make sure that we have a Gaussian message here so that you are back into this situation and you can denoise at least from that side. And here it's again a Gaussian message, which and then we denoise component-wise. Okay, so it's the same idea as band, but extended to the bilingual case here. So if you, at iteration t, assume that you have computed a Gaussian extensive message here, you can. Gaussian extrinsic message here, you can denoise, and then you have a posterior, and then you have to compute the extrinsic information again. So, how you do that is just the difference between the incoming and the outgoing. So, that's how you compute. And it's not just second-order statistics here, so we have the mean and the variance, so you also compute the extensive mean this way. So, this is known how you can find those extensive messages. And now that you have a Gaussian extensive message here. Have a Gaussian extensive message here, you can basically denoise element-wise here if you have sparse or discrete wires. Okay, okay, back to our, yeah, so this is one more slide on the general purpose by VAMP, and I wanted to show you. So, this is another block diagram representation where this, what we call bi-MMSC, is this entire thing. And those are the outside priors, and this is where you compute the extensive information. So, just wanted to give you Just wanted to give you two, I mean, the performance behavior of that algorithm. So, here, when you have Bernoulli-Gaussian prior on V and Gaussian on U, so by VAMP, so it's called big VAMP because we have a generalized version of it, so bilinear generalized VAMP. So, it works much like the other algorithms, but its true advantage is when you have a discrete prior on one of the two matrices. Okay, so here I'm comparing to bygamma. Here, I'm comparing to byGAMP. So, basically, by GAMP, we have tried hard to make it work, but I'm curious if anyone has tried by GAMP with discrete priors and what's their feeling, and does it work or not? So, we have a good discussion. So, that's the big VAMP with its state evolution. So, we can, due to this modular structure, you can understand that we can develop a generalized version of it. So, where you also isolate this nonlinear output in a separate block, and then you. Your output in a separate block, and then you propagate those extensive messages in between. Okay? And this will be crucial when we deal with quantized massive answers and the access once again. Okay, so back to our problem here. So that was our received signal over one slot. So if you stack all the received data here over all slots, you can see that you can write everything in a matrix matrix multiplication. In a matrix matrix multiplication. Okay? So here again, you can call this Y, call this C, so you have diagonal sort of books here. Same codebook on the diagonal, and this is the delta which stacks all the delta matrices over all slots. But the channel is the same. Okay? You have one channel, basically, over all slots. Again, because we are using, we'll be using this 5 band, so we can call this U and call this V. So that's again my model. So that's again my model. So now we have, because as I said, we have to allow by like communication between those slots. So we have to write down the entire factor graph for all slots. So again, this is the bias rule. Remember here that these two matrices are known. So again, so here we know C and we know F, so you can write the bias rule this way with those deltas which are enforcing the fact that you. Which are enforcing the fact that u is equal to c times delta and v is equal to f times h. Okay? Now we use the independence across slots, so the data is independent across slots, so you factor this. And then we use the separable priors because the data is independent among the user C. And also we assume that the channels are independent and you have this factorization. So once you have that factorization, So, once you have that factorization, you can draw the entire factor graph. So, here I'm collapsing everything into matrix, matrix variable nodes. So, I'm just what we have done before, now it's just one matrix, okay? So, here, this is where I mentioned that we need to be careful because the structure that we have on unsourced access is that you remember we had that deltas, so we have the columns and there is. So we have the colons and there is one non-zero. So it's a non-separable prior. So it's not row-wise, but it's colon-wise. So here we split again. So we have delta plus and delta minus two auxiliary. So for the channel, it doesn't matter because the channel is IID. So you consider it row-wise or colour-wise, it doesn't really matter. But for but for the data, we have to define or split that delta into two matrices where we define delta, uh the inner one, in terms of its rows. In terms of its rows, and the outer one in terms of its colours, but these are equal. Okay? So, this thing is one instance of what we have seen for by bar. This is by LMMSE. This is one by LMMSE. So, here we have the L slots. Okay? So, here we have the L slots. That's the by LMMSE that we have seen before, that red box. Red box which is providing us with Gaussian messages. Okay, so each by LMMSE here, like internally, it's sending a Gaussian, sorry, a Gaussian message. So here you have a precision matrix and you have the mean. Okay? So you have this from all the slots and you have also those for the use. And now we need to. We need to somehow combine all those independent messages. So that's how we can plan the information from the different slots because we have the same channel. So these are Gaussian messages. You can show that you have a Gaussian information here about this same V. So remember V is equal to F times H. F times H. So it's the product of a known matrix times your channel. So here it's the sum of those precisions, and then you have the sum. This is not exactly the mean. So the mean is basically the inverse of this times P. So this is the potential matrix if you want. So once you have this, now this acts as a noisy A noisy observation over your matrix V. Okay? So then just remember, as I said, this is the delta factor node which is enforcing the fact that V is equal to F times H. So you can just replace this here. Okay? And now that you have assumed again that you have some computed iteration. Some computed at iteration t minus one or iteration t, extrinsic messages here, and you have those priors that you're trying to enforce. For instance, we have said that here we're considering multi-pass-pass channels. So basically, this is a Gaussian extrinsic message which you can use to denoise those channel components element-wise. And then you have posterior. Okay? So once you have a posterior Gaussian message, you have that those. Gaussian acid, you have those second-order statistics, you can compute the extension confirmation in the same way that I explained before. Okay, so you take this minus this, and then you can fix that for the precision, but then for the mean, you have the weight and sum. Okay? Now you have this Gaussian observation, right? And you have those priors, Gaussian priors on H. So you have everything you need to. So, you have everything you need to perform pure LMMSE to reconstruct or to find the posterior mean and variance of this matrix. We need those posterior signals here. So, you need to propagate that posterior signal here so that you can basically denoise and technical back. And also, you need them as internal messages by LMMSE. Okay, so as I said, you can perform LMMSE estimation in closed form to find the posterior mean and variable. To find the posterior mean and variance of H. But once you find the posterior mean and variance or covariance, because we have vectors here, of h, you can find the posterior mean and covariance on the rows of vh of the rows of vh of the calendar v. Okay, and then you propagate them here. And then the same drama happens here on every slot from the U side. Okay, so this is a message passing algorithm where you pass messages back and forth. Or you pass messages back and forth until the algorithm converts. Okay? Okay, here are some simulation results. So consider 50 antennas, total number of bits is 100, which you will split into multiple sub-chunks. And then this is the total number of channel users. And this is the proposed algorithm. And this is the covariance-based algorithm. I think Alex did that. This is one of our algorithms we proposed back in 2020. Algorithms we proposed back in 2020, which is also concatenated coding free. And the idea was to reconstruct the channels but then cluster the channels, and you get the assignment matrix. And once you have the assignment matrix, you cluster the sequences. So those are two. This is the tensor-based modulation. There are two versions of this algorithm. And so that's for the infinite resolution case. And we also, because we have a generalized version for the algorithm, we have also Also tested the performance for quantized, and you see that we get good results, even down to two bits, and then we can accommodate that much more users. So, even with the two bits, it's here, so we require minus 10 dB to accommodate 400 users. And these are showing the state evolution just in terms of channel estimation. You can see that here we get a good match, here we still have like this little mismatch. Still have like this little mismatch, you're still investigating. So we can basically use state evolution to predict and perform some theoretical predictions. Okay, so with this, I conclude my talk and with two messages. So binary recovery is essential for concatenated code-free massive and source-dynamic access, and joint processing is inevitable to operate at fixed dimensions. And if you have any questions, please. And if you have any questions, please.