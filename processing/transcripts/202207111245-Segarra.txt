Yeah, so I'm an assistant professor at Rice University, Rice ECE, and over the summer I'm also doing a visiting researcher position at MSR. And today I'll be talking about principles in visual neural nets. And a lot of credit goes to Mitch Radenberry, who was also part of the previous talk. And let me see. Okay. Thanks. And Nick Lace, who is an undergraduate that worked with us. Cool. So let me just indeed get right to it. So no need to motivate graphs and neural networks. Motivate graphs and neural networks to this audience. But let's try to go a little bit beyond graphs and signals on the nodes of the graph or features of the nodes of the graph in two different ways. The first one is in terms of the structure, trying to go beyond just pairwise relations into higher order networks. For example, you know, messaging gaps and say WhatsApp, right? So there's, of course, just a pairwise relation where you're sending a message to someone else, but you also have groups, which are relations between multiple agents. But you can also think of But you can also think of papers that are written by more than two authors, etc. So these kind of higher-order relational structures tend to exist quite a bit. And then going beyond features or signals in the nodes to signals on other parts of these high-order networks, for example, in the edges, triangles, or high-order relations themselves. In particular, you think of data in the edges, then that naturally makes us think about flows. Naturally, it makes us think about flows, flows of signal, mass, energy information between agents. And this could be, you know, the flow of traffic in a city, the flow of birds that are migrating or people that are migrating between states or countries. Or this could be the flow of material in supply chain going from extraction to production to transportation to consumption. And if we think about flows, one thing to notice though is that these flows are unspecific. Is that these flows are skew symmetric? And what do I mean by this? Is that let's say that here you have a flow in this very small graph where a blue edges correspond to a flow of plus one, so one in the direction of the edge. And here we have the same flow, right, but represented as minus one in the opposite direction. And here we have once more the same flow, but with blue and red edges. The thing to notice here is that all of these are representations of the same underlying. Representations of the same underlying piece of information, right? But if you represent it as vectors, one would have once, the other one minus ones, and the other one's plus and minus ones. So, if you want to learn or process or do anything with this data, we better be aware that there exists these equivalence classes of ways to represent our data. So, if the architectures that we use are cognizant of these equivalent classes, then we'll be able to learn faster and generalize better, hopefully. So, that will be the key of what we'll be discussing today: how to be true to the invariant or the symmetries in our data in order to learn better from it. Very good. So, over the next 15 minutes, I'll try to cover three things. So, first, just how to incorporate these higher-order relations, and we'll do these in PHL complexes, how to operate in those higher-order relations. In those higher order relations, and we'll do this through hash theory, discrete hash theory. And then how to use those operations and stack them and put some non-innetts in the middle to build graph neural networks in hopefully a principled way. And you'll see what I mean by that. So higher order relations, this is maybe a lot of people here are familiar with this already, but I don't want to assume too much. So we'll represent this as superficial complexes. This is simplicial complexes. Let me do just a quick definition by example over here. So, a simplicial complex is a set X of a finite subset of another set V that is closed under restrictions. We can think of V as being the nodes themselves. So, singletons of this set will be the nodes, one, two, three, four, etc. Pairs will be the edges, you know, one, four, three, four, etc. Triplets will be the triangle, for example, five, six, seven. And close-end restriction, it only means that if the It only means that if the triangle 567 is present, then it must be that the edges 5667 and 57 are present, and the nodes 56 and 7 are also present in our sets. That's what the simple distribution means. And once we have that structure, then it's very easy to define features or signals on top of the different levels of this construction, on top of the different simplicities. So just assigning values to the nodes, the traditional way to think about graph signals that will be attached. Signals that will be attaching information to your zero simplices, you know, flows to your one simplices, to triangles, and so on and so forth. So one can certainly think about this construction. How can we then operate on data on k-simplices? It's another interesting question, which is the second order in the menu here. And we'll do this in the following way. So for this, I need to introduce a little bit. For this, I need to introduce a little bit of notation over here. So, let's call CK being a vector space with oriented k-simplicies of our simplicity complex as a basis over some field, for example, the real numbers. So it's attaching a real number to every element in the case of CK for the K synthesis. So, CCO, for example, would be our traditional graph signal, so attaching a number to nodes. To nodes, you know, C1 will be attaching a number to oriented edges, etc. And then we need a way to move between these spaces. And we'll do that through a boundary operator, del k. So del k will take us from ck to ck minus one. For example, del one will take us from information in the edges to information in the nodes. And this boundary operator will be, will work as you thought, as you think it should. You thought, as you think it should work. Okay, so again, let me just do a description definition by example over here. So let's say that we have the element three times one, two, right? So that guy lives in here because the oriented edge one, two attached to the real number three. And if we apply del one to that, this will result in having three attached to the head of that edge minus three attached to the tail. And this thing over here. And this thing over here indeed is an element of CC. This, you know, again, you can see just from the definition, it's a linear operator. And if we look at the matrix representation of this linear operator, it's a well-known incidence matrix of the graph or the graph skeleton of this initial complex. And it also induces an adjoint operator, which is a coordinate operator, Deltranspose. Deltranpose will take us up in this. Take us up in this chain of spaces. So, with those definitions under our belt, we define this now quite famous operator, which will be the kthash applaution, which works in CK to CK. So, it takes us from nodes to nodes, from edges to edges, from triangles to triangles, and so on. And will be entirely defined by the boundary operators that we have defined in the previous slide. Previous slide in this way. So let me try to gain a little bit of intuition of what this means by specializing this for different case. When k is equal to zero, we have del zero is nothing. And here we have del one, del one transpose, but d is just the incidence matrix, so it's db transpose, which is the combinatory Laplacian that we know and love. So it's just a regular Laplacian over here. When we think of edges, though, so now we have both of these terms are non-trivial. And they relate essentially to two different ways in which two edges are neighbors of each other. So you think of a simplicial, a very simple simplicial complex like this, where this triangle over there is fixed, is filled. So, two edges, for example, you know, the two blue edges over here, you can think of these two edges as being related because both of them are incident to this one node. On the other hand, the two pink edges over here, apart from being incident to a common node, they also are part of the same triangle. So that's another way of being linked, essentially being linked through the hydro dimension. Through the higher dimensions. So we are in a one simplex. We can be related through the pseudo-simplices through nodes or through the two simplicies through triangles. And these two types of relations are indeed perfectly captured by these two different portions of the Husha Plaxian. One that takes us to the dimension back, below and up, sorry, below and up, and one that takes us to the dimension above and back. Good. So, there's another appealing feature of the Hansa passion over here is that it has a very natural decomposition. So, its spectral decomposition is very naturally related to the composition of flows into three interpretable and orthogonal spaces. And that can help whatever trainable architecture we build on top of it, it should help in learning what types of flows are more relevant for. Flows are more relevant for the setting at hand. So, let me just leave it there. If there are more questions, I can come back. So, then a simplicial complex network, we can think of it as something that takes us through signals in Jth dimension to signals in Lth dimension. This should not be the same. That depend on the boundary operators, it should depend on the graph, if this were to be just a graph or in the whole situation complex. And we have some learnable parameters, W. learnable parameters W and if we think of something going from C0 to C0 that would be a graph neural network but this of course a marginal construction and the idea here is instead of trying to essentially propose one architecture and then see whether it works well in practice or not let's just think of three different properties that we would wish this architecture to satisfy and then look for something that actually satisfies this if at all. That actually satisfies this, if at all. The first one is permutation equivalence, and this brings us to our third item in today's agenda. The first one is permutation equivalence, that probably most people are quite familiar from graph neural networks. And the idea here, and I won't get into the specific notation, but the idea here is if you give me some supercial complex network and you apply that to your input CJ and you get some. That to your input CJ, and you get some output, which is this red box on the left. And then, if what you do afterwards is permute the input and permute all the indexing in your simplicial complex accordingly, the output of that should be just a permuted version of what you got at the beginning. So, this is very simple to understand, very intuitive. The output should not depend on the arbitrary indexing of your nodes. Now, when you go, you know, nodes. Now, when you go, you know, nodes are not orientable, so there's no orientation vivariance into the classical way of thinking of graph neural networks. But we saw that flows and higher-order information is orientable in gene event. So in this case, you also want to be agnostic to a specific orientation chosen because that's arbitrary. So that we can encode as this orientation equivalence property, where, again, if you start with some superficial complex network and you apply it to some input CJ, that's Or when you apply it to some input CJ, that's the red box on the left. And now you change the orientation of your input, and then you change the orientation of your whole sequence complex accordingly. Then the output of that should be just a change in orientation of what you got originally. Again, we would like to be learning somewhat in the quotient space of these orientations that we're choosing and not in just these vector representations with one and plus minus one. It's kind of the idea. And the last thing that we would want these architectures to have is the idea of simplicial awareness, so that if there is information to be learned in high order structures, we want that information to be learnable. It doesn't mean that you always have to depend on this information. Maybe the information is useless and you might as well learn that to ignore it. But at least you want to have the ability. You want to have the ability to be able to learn from all the levels of your simplicial complex. That's the idea of being simplicial awareness, and let me not get bogged down in the notation. But being simplicial awareness, you have the ability to learn from all of the levels of your simplicial context. Okay, and then we propose a specific architecture that satisfies these three properties, and we apply it for the Properties and we apply for trajectory prediction. So, let me just skip the thing. So, what is this architecture? So, this architecture is probably the simplest thing that would come to mind. Or almost, maybe not the simplest, but almost the simplest thing that would come to mind. So, it's a layered architecture. It's a layered architecture. In every layer, so let's take that here. We're going from flow to flow, from C1 to C1. In every layer, what we have is the current input to the layer, those C1Ls, and then we're going to go. Else and then what we're going to do is we're going to learn you know ways to combine those and then we're also going to be going to the triangles and down and combining that. So changing information with other edges for with which I share a triangle. And in this case, also exchanging information with other edges with which I share a node. And these two exchanges have different ways that we learn, because these two exchanges in general could be of different nature. General could be of different nature. So, we do that. We do that, we apply element-wise non-linearity and we stack a bunch of these layers. And then at the end, if you're doing interjective prediction, maybe you want to map back down into the node space, but that you can also do with just one more, one additional layer. Another key thing that we want to say is: you know, whether this architecture that we propose here, whether That we propose here, whether it satisfies the three properties that we posited or not, and then we have these results, which says the following. This architecture satisfies the three properties, permutation, covariance, orientation, covariance, and spatial awareness. And we call that being admissible where it satisfies the three properties. If the nonlinearity satisfies the following, right? So if we assume this nonlinearity, The following: right, so if we assume this non-linearity or this activation function to be continuous and an element-wise non-linearity, then if scoring is admissible, then it must be that this non-linearity is odd and indeed non-linear. It cannot be a linear archival. So, the key thing here is that these properties that we were proposing for this architecture somewhat guided the design of this choice. Of this choice of the activation function. So, for example, something like RELU would not work well in general because it is not odd. And something like a TAMH would tend to generalize better. So that's why we choose for our construction TAN-H. And then we showed a few experiments. So I know that, so we have eight minutes. So let me just very briefly. So, let me just very briefly mention something here. So, here, the idea in trajectory detection: the idea was: so, we have three types of trajectories: one going above this obstacle, one between the obstacle, and one below this obstacle, right? So these things in white are just obstacles that the worker cannot go through. And everything in blue is just a triangulation of the walkable space. Walkable space. And then the idea is that we train on these types of trajectories. And then we're given a partial trajectory, for example, all the way up to here. And then we want to determine where this walker is going to move next. And we compare this with some baselines, in particular with something like an RNN, for example, that's not aware of the simplest structure or even of the graph structure of this underlying domain. And we see which one predicts the. And we see which one predicts better. And the key thing here is that, you know, in a case where you're able to memorize these paths, then something like an RNN works actually the best. But in terms of generalizability, either learning when the path might go in the opposite direction, or observing just two types of path and trying to generalize over the third types of path, which are these two columns over here. Our proposed methodology, which is here by scorn with 10h as an activation function, ends up beating these other baselines or these other ways of approaching this problem. And this, you know, goes to show that at least empirically, these properties that we posited as being decidable for generalization end up being indeed end up leading indeed to generalizable. Indeed, to a generalizable architecture. That's kind of the main message of this whole presentation. So I don't want to go into other experiments, but we did run some experiments with real data and even on some other discretizations of a space that are not based on simple cell complexes, but other more general cell complexes. And the things that we've described so far also generalize to those cases. Generalize to those cases. There's a lot of work in this area. Somebody, people attending this workshop over here, I think this is a fascinating topic. And if you're interested, this is not exhaustive, but somewhat rich list of the work. This is the first paper, is the one that I talked about today. And there are many others here in this list. Happy to share it. And let me skip the conclusion just to leave five minutes for questions. for questions. Thanks all for your attention.