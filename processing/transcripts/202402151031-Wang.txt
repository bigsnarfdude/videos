So Leno mentioned uh he is semi-local uh because he's from Edmonton so I think I'm uh quarter local. And uh today I'm going to talk about uh uh something that we have been uh working on recently called Spartan Learning. Uh so I also would like to thank you for putting me on this very right spot because uh we have a lot of Because a lot of this is inspired by the discussions that happened over the past few days. So, the motivation for this talk will be the group discussion on interoperability. On Monday afternoon, we had a heated discussion inside this room led by Hada. And where we talked about the difference between interpretable models and experimentable models and how to build these models. Models and today, just as to say things right, I'm only going to talk about models that's interpretable. Okay, so I'm going to build a model that has a simple structure from the beginning, and I'm going to use a tool of causality. So, this word has come up here and there sporadically throughout this workshop. And I think it's sort of like everyone in the room, and everyone talks. Room, everyone talks about it, knows about it, but we haven't talked about it so formally. And one of the motivations for having this talk about causality is also from this discussion on interoperability, especially when we talk about one approach we talk about interoperability is to first think about some handcraft features. So if like the first talk by Chris, when we think The first talk by Chris, when we think about the birth, we have experts think these are the predictors. And I would argue that if you want to use expert knowledge to include some features in your model, usually your experts will think about causal features. So we ask them to put their genes into your prediction model. They will think, okay, this gene is important for this outcome. They think about this causal. So to align. So to align with that, we'll try to build a causal model so that this handcraft feature will make sense eventually. And I think everyone has misfeedings about causality, but everyone agrees on two things, I hope. First, it's very challenging to build causal models without experimental data and because of the challenges by unmet confounding. And the second, but this is very important, it would be great if we can get causality. Get causality. Because it's not only important for interoperability, but also important for all the other topics that we have been talking about throughout this workshop. Fairness, we talk about, we can use causality to define fairness. Robustness is the topic of today's session. And the generalizability of a distribution shift. Robustness. So it's something that's very difficult to do, but it would be great if we can have it. Have it. So, the main message that I want to give in this talk is that there is one possible solution that can hopefully solve both of these problems of interpretation and causality. And this is a feature that statisticians are all very familiar with for the past few decades. And this is the assumption of sparsity. Statisticians have been talking about sparse causal models. Been talking about sparse causal models, the sparse association models like Lascelle or something else for the past two or three decades. We are all very comfortable with these kind of assumptions. A lot of us are making these assumptions at realist. So of course sparsity will give you, will make a model more interpretable. It will have fewer features in the model. But what I will try to convince you is that today sparsity also offers a potential solution to the problem caused. A potential solution to the problem caused out. Okay, so you sort of kill two swords with one soul. Okay, some disclaimers before I start my talk. All of this is not big data. There's no deep learning in this talk. And I'm only going to talk about things under the term of linear models, simple linear models. But the main idea extends far beyond the simple models I showed you. So this is more like a theoretical. So this is more like a theoretical framework to think about the causality. I'm using sparsity to achieve both causality and interoperability. And so this is a step towards the theme of the workshop, the trustworthy AI. Okay, so I'm quickly going through this motivating example. I think we have been talking about examples of this kind a lot throughout this workshop. So let's say we want to predict credit card default. Okay, so we want to predict if it's Card default. Okay, so we want to predict if someone will default on their credit card payments. And so we can use some data. So the data we get is have 23 explained a variables on 25,000 users. And this is credit card payment data from October 2005 provided by Major Bank in Taiwan. So the goal here is to build a prediction model for the risk of default. So it's a classical machine learning type of Type of functions. And if we are in the old days, if we talk about, only talk about machine learning or AI, and not worry about trustworthy, then we would just use some deep learning models. And actually in 2009, this year analyzed this data set and they compared different prediction models. They discovered that neural network will give you the most accurate prediction. And this is not surprising to any of us here because we now all know that. Any of us here because we now all know that deep learning is great at getting prediction of models. But this is why we're here, right? We worry about the trustworthiness of our machine learning model. And one aspect of this tool quote Yoshio Benjo here is deep learning is good at finding patterns in release of data, but they cannot explain how things are connected. So this relates to the first theme on Monday, which is interpreted. So, one way that if we one very easy way, if you just want to get interpretability and direct reaction from sensation will be we can just use some Lasso model. This is a very simple model, you impose spacetime, and then I'm going to try this Lasso model on this data set. Okay, so again the outcome is the probability of default from this data set, and the predictors will have 19 covariates. The predictors will have 19 covariance because we cannot use the sensitive attributes such as age, sex, education, and marriage to predict their credit card payments. Because these models can potentially be used to make decisions to call someone's bank account. So we don't want to use this covariance to make this kind of important decisions. And then these are the top seven predictors selected by Lasso. Okay, so just the plain regular Lasso. The plain regular soul goes through the path, these are the top seven predictors. Okay, so if it goes through this, it will find that there are some, this one particular feature that caught our eyes, which is credit limit. And I put a minus sign here, which means the higher their credit limit is, the less likely they are going to default. So if you think about this from a causal perspective, this doesn't really make sense because if your credit limit is higher, Because if your credit limit is higher, which means you will be able to borrow more money from the bank, which means you have a higher incentive to run away. So if you think about the causal effect here, if there's any causal effect should be positive. But we get a negative coefficient here. And anyone can tell me why we get a We get a negative association between credit limit and default. When you're richer, you get a higher credit limit than yours limit. Yeah, exactly. So there are other, what we call confounders, right? So people with a higher credit limit may be more considered more credit worthy, and these people will less likely to run away. So this kind of association, we call this spirits association. We call this spirit association because it's not causal association. But if you are not happy with the word spirits, you should think of this as non-causal association. But then someone may come to say, so what? I'm just interested in building a prediction model. As long as this can help me predict the rate of default, why should I care about if this is causal or not? So this relates back to our discussion on the second day of this workshop, which is fairness. Of this workshop, which is fairness. Because credit limit itself may be unfair, may not be a fair attribute. Historically, there may be some subgroups that receive lower credit limits because of discrimination. So there's one example, this is actually quite recent, news from BBC saying that this is a claim into Apple credit card, and they offer different credit limits for many websites. So, and the Apple's co-founder admitted that algorithms used to set limits might be inherently biased against people. So, if you use a credit limit to predict default, because of this unfair association between gender and the credit limit, so this model, this association model, might reinforce the historic bias against some particular subgroups. So, this is out of a concern fairness. So, how to deal with this, how to remove this spirit correlation? We have this whole field of causal inference. So, this is a field that has developed a lot over the past few decades, and it promises you a lot, prompts you to remove these spirit correlations. You get an observational data set under various kinds of assumptions, and they say they can help you remove this. They say they can help you remove these various associations. One prominent example is the instrumental variable example, mainly developed by econometricians, and I think in the past three decades has won three Nobel Prizes in economics. And there are also other approaches such as negative controls by epidemiologists and statisticians, and the fundamental adjustment methods developed by computer scientists. So this is really a topic that has received a lot of attention across many different fields. Potential across many different fields. But it has also received a lot of, okay, so let me just mention this. So many of these methods rely on some assumptions, underlying assumptions. So if you used to think about this from a causal graphical model perspective, this will correspond to some missing errors in the graph. So if you don't make any assumptions, the underlying model graph is dense. So everything is connected to each other. So you are able to. So, you are able to get causal inference if you assume that some of the arrows are not there. So, all of this, most of these approaches make this type of assumptions. And if you are for a potential outcomes framework, then this will translate into condition dependence assumptions, among potential outcomes, or minus data values. So, these are two different viewpoints of a single set of assumptions. And let's see. And let's see. Yes, and as I said, it has received a lot of attention across many different fields. And not all the attentions are positive, right? Also, it received a lot of criticisms across many different fields because all of these approaches rely on some very strong assumptions that are not tested for some data. And this is why some people who are very strong, they intuit causal inference will also be. Into causal inference, but also people who are very strongly against causal inference because they rely on strong and untestable assumptions. And so this is just a very quick summary of the three frameworks that I mentioned so far. First approach is deep learning. We know that this is correct, probably the best for making it for prediction, but they have some problems that we worry about in this workshop. The second approach is LASO, which statisticians have been. Which statisticians have been making a lot of contributions in the past few decades. One key advantage of this disease will be a very explainable or interpretable, but may not be fair or has some other problems due to spiritual associations. The third field, counter-inference, promises you to remove these spiritual associations, but it relies on some very strong and attestable assumptions. So, you know what I'm going to say? So, you know what I'm going to say? I'm hopefully providing this framework of sparse causal learning that hopefully addresses the problems of each of these fields. I mean, this is a very broad claim, but I'm not saying that this is one approach that will solve all the problems we see here. That's almost impossible. But it provides a solution to address an alternative approach. An alternative approach to avoid the shortcomings of each of these frameworks. So, in particular, it will enhance the trustworthiness of the model, it will make it more interviewable because this is a sparse model. It will make it more fair, more stable, and more generalizable because we are trying to get a causal model. But compared to conventional causal inference frameworks, we are also going to make it more robust. And somewhat And somewhat testable. So, I mean, this is a very general framework, so I will try to be a little bit more specific to consider a particular example here. So, let's say, so this is the, in causal inference, it's called multi-cause causal inference, but you can think of this also from a prediction point of view. You have one outcome y, such as the rate of default, that you try to predict. And you have a bunch of explanatory variables, right, like the coveries you have on your custom data set. Have on your customers in the data set. I will call them X. So X is multi-dimensional. You try to predict scalar Y. And what makes things more difficult is if you think about this underlying generating model, what are the factors that decide people's behavior when there are a lot of underlying factors going on here? So one category of this is called confounders that affect both X and Y. There are also things that only affect X, which we call them X or X. Which we call them x of x. These only f, we call them x of y. But what's really causing a problem for causing influence is confounders that relate to both x and y. So I'm going to make this simplify assumption. This can be relaxed later, but just to make things simpler. And we're going to assume that all the treatments, all the explanatory variables are independent condition on the latent variables. Okay, just to give you some idea of why this is possible. Of why this is possible. And before I go into my solution, I was to first to show you that this is not a trivial problem, right? So even under this simplified assumption, this causal effect is still not identifiable because of this confounding by these latent variables. So here we don't have any experiments and we don't observe any of the confounders. So it th it's a very difficult problem, so it's not surprising that if you don't make any assumptions, this is not identifiable basis. This is not identifiable. This is a very difficult problem. Yes. Are you assuming lithium? No, not yet. Not yet. Everything here is still fully non-farm. Okay, so there are these results from, so one potential solution to this problem from the causal inference community is called negative controls. Okay, so what it says that if you know that you can find this Q-dimensional, okay, then this negative control. Then this negative control approach will ask you to find Q treatments that has no effect on the outcome Y. So if someone who really knows how people behave, how people deal with their credit card payments, like psychologists or maybe whatever, if someone who, some domain experts really come up with this very strong prior knowledge, then some of these explanatory variables is not going to affect your outcome. And you need to know which explanatory variables are not going to affect your outcome. Are not going to affect outcome. If someone really comes to you with this knowledge. Or if you think about the gene expression example, so some the origins come with the knowledge that some of the genes is not going to affect your trait. So if they come up with this precise knowledge, then there is a framework in causal inference called native controls that shows you that you can use that knowledge, the prior knowledge, to figure out the causal effects for the remaining features. So this is something that has already been. So, this is something that has already been done in causal inference. It's called negative controls. So, I will refer to this kind of results as exact causal inference because it relies on some assumptions, like these causal effects, beta 1, 2, beta Q, are exactly 0. And you need to know exactly where the treatments are. And this is, as I said, strong and untestable assumptions. So, and what his fast calls are learning to do is just relax these assumptions. It just relaxes assumption a little bit. But I think it's in an important way, in a sense that we shouldn't let the causal effects beta identifiable, as long as the effects of the treatments or the expanded variables on outcome is sparse. So you don't need to know which treatments have zero effects. You don't need to know which genes has no effects on the outcome. All you need to assume is the effects of the genes or effects of the treatments, effects of your expansion. Of the treatment effects of your expanding red walls on all kinds of spots. But this is the sparsity assumption. And then, what we show is that you can use this fast assumption, which people have been making in the past few decades, at least in statistics, to solve this problem of causal inference, to deal with the problem by measuring confounding. So, to make this a little bit more formal, our theory says that in this particular setup, under some regulatory conditions, Setup under some regulatory conditions, which I'm not going to go into details. This causal effects of the set of variables x and y is identifiable if you can find at least q plus 1 zeros in all the p treatments. Yes? What kind of assumptions you have? Are these assumptions verifiable or something? Just like a confounder kind of thing you have assumed which cannot be verifiable. I'm going to. Okay. So that so first this assumption is necessary in. So, first, this assumption is necessary and sufficient. So, I'm particular about this sparsity level. So, you need at least Q plus 1 zeros in your big vector of G matrix. So, if you only have Q, it's not identifiable. If you have Q plus 1, it's identifiable. So, necessaryness of G. And second to go back to your question, this is testable. So, the assumption on the sparsity level is testable from data. Data. You need to know what Q is. Yes, we need to know what Q is. And then we need to assume this multi-cause influence setup where you have this connection dependence. And I will show you, it will become clearer later what exactly means how we test this assumption of the sparse level. Enough of yours to confound your achievement. So if you just come Confound your treatment. So if you just come pick a treatment that's not related, like a random error, then this is not going to help. It's for obvious reasons. No, all the X are dependent on the condition of the confounders. So there's no effect between the X. Yeah. Do you sorry? Do you allow confounders to all come? Yeah, that's why it's called confounders. Why it's called confound us. But yeah. So you can affect X, all the X, and then the Y. And it's also give you interpretable models, because we have a sparse model here. So I don't think I have enough time to go through why this is possible. So I wanted to explain this is not magic, but I don't have time, so let me just Uh at all time, so let me just uh uh maybe uh uh maybe uh actually okay let me just uh explain why this is uh actually possible and I will stop there. So let me try to explain why this is uh actually possible, right? Why the assumption of sparsity is enough. Um so I'm going to show you the idea under this very simple model, okay? So let's say. Simple model. So let's say you only have one unmeasured component. You know A prior E is only one dimension, but you don't measure it. You have three treatments and you have one outcome one. So in this case, if you don't make any additional assumptions, these effects beta one to three are not identifiable because of the confounded by U. And you can see that you can allow the effect from U to Y. But what we're going to show is that this beta is actually identifiable as long as this vector is sparse. As this vector is sparse. And here, the sparsity level we need is one. So there are three betas. So we're going to assume the L-deal norm of this is less than or equal to one, which means you need at least two zeros, a minus three, but you don't know which two. Now we're trying to use data to help us figure out which of the betas are zero. So how are we going to do that? We don't know how to do that. We don't know how to do that ourselves, so we're going to call some help from outside. We're going to invite three experts. So each of these experts will assume that one of these beta is zero. So remember the results from native controls, so our U here is one dimensional. So if any expert, if you tell me any of this beta is zero, I can use that information to figure out what's the beta for the other two. So any of these experts, based on their Any of these experts, based on their assumption, they will be able to figure out what are the betas, what are the other two betas. They may not be correct, but they will get each of the experts will get an estimate. So first expert, looks like Einstein get an estimate. It's a bit beta expert one. Second one get a another beta tilt. Third one get a beta child. Okay. Now my assumption says that at least two of the beta are zero. That at least two of the beta are zero, which means at least two of the experts are correct. So maybe let's say just the first two are correct, the third one is wrong. So what that means is, since the first two experts has a correct assumption, their causal estimate will be correct. So the first two experts will get an estimate that's close to the true causal effect. Third one may not. Because third one is not making a correct assumption. Okay? But of course, we don't have. But of course, we don't have that information, right? We don't know which estimate is correct. All the data we have at this point are the three estimates. Each one estimate is from different experts. Now the question is, how do you figure out which experts are correct? You know, this is just the key idea of here. Vote. Vote. Majority vote. Majority vote, exactly. So the majority vote says that the majority of this estimate is always correct. Of this S bus is always correct. And when does this majority word work? It works if the majority is actually correct. In this case, what does it mean? For them to be correct means their beta is zero. So yeah, sparsity. Now you might have a tight situation, supposed to have four experts and not in the instead of three, so what's gonna happen? Right, so if you have two against two and this kind of thing, right? Have two against two, and this kind of thing, right? So, this is what we call the underlying assumption. If the experts are wrong, we assume that they will, so they would, so all the right experts, they will get the same answer because they will be the same causal, true causal effect. But we're assuming that if they make mistakes, their mistakes do not coincide. So, they will not be, so it's like you hand out the exams to your students, right? You don't know that the Students, right? You don't know the solution to your exams. So, what do you do? You collect your student answers and compare them to figure out which one is the correct answer. So, you need to make the underlying assumption that you are doing a reasonable good job of lecturing so that all your students, right, if they get it right, they will get it right together. But if they get it r uh wrong, well, you can either one of these will work. Either one of these will work. First assumption is the majority of your students are correct. So if you are very confident about your lecture, you assume the majority of them are correct. And this will translate to a stronger assumption on a sparse level. Second, if you are going to assume that if your students make mistakes, okay, and then their mistakes are sort of independently, they don't copy from each other, then their mistakes will be different from each other. Then in that case, it's also okay, right? You just need a handful of good students because they will converge to. Because they will converge to the same solutions, and everyone else will deserve a design solution. In that case, you can also identify it. Yes, but they will arrive at the same solution. Because if both of these assumptions are correct, so s by 1 assumes beta 1 to be 0. Because this assumption is correct, he will correctly estimate that beta 2 is also 0. And the same for s per two, vice versa. And that's the magic behind this framework. So I think I just have enough time to show you the reference. Thank you very much. Yeah, thank you, Dr. Littiboro, for your last talk. Do we have any questions? Yeah. So you thought the example actually very interesting because I think actually it's related to the so-called Florandi condition, IV refraction. The Plurandic condition in IV referral, right? In IV, if that holds, I can also assume the direct effects of confounders are sparse. Exactly, follow your framework. Right, so in this kind of expert voting approach, I should also mention it's not new. As you mentioned, it's the IV regression. And also, it stays back to people in this diagnostic medicine. So, one of my PhD advisors works in this traditional Chinese medicine. They will invite some traditional Chinese doctors, and each of the doctors may. Doctors. And each of the doctors may give a different prescription. And the assumption is also the majority of these doctors will give you the right prescription. So this kind of idea is not new. But I think the relationship between this kind of technique with the sparsity assumption is interesting, because sparsity assumptions, what gives you interpretable models, what we have been making in the past few decades already, but it can also give you, have you identified its causal effects. So, I mean, you're making assumptions about whatever the unconfounding thing is, but that sort of suggests that I'd like P to be larger than Q and preferably noticeably larger than Q. Does that suggest a strategy of just saying, let me collect as many X's as I possibly can? I want to make this as high-dimensional as possible. Yeah, the variable assumption. X conditionally. So, okay, so great idea. Okay, so there are two underlying assumptions which has already been hinted in throughout this talk. One is that this X needs to be, they cannot affect Charlie. So this is one big assumption. And also, this is another assumption.