So, just to remind you where we left off yesterday, we were looking at the random two-sat problem, and the goal was to determine the number of solutions. Z of phi was the number of satisfying assignments of a random to such formula. And because that random variable turns out to scale. Random variable turns out to scale exponentially. What we were trying to get at is actually the log of this random variable divided by n. And the goal was to show that this random variable 1 over n log z, 1 over n log number of satisfying assignments, converges to an expression predicted on the basis of physics method called methods called the beta-free entropy. And whatever the beta-free entropy was. Whatever the bit of reentropy was is displayed here. So it was expressed as an expectation over a bunch of random variables, where these mu i's came out as samples from a probability distribution that was defined by a stochastic fixed point equation. What we managed to do so far in the first three steps of the proof was to prove prove was to prove that the limit of 1 over n of expectation log z of phi actually equals this quantity. And in order to avoid the very few unlikely cases where log z is zero, I have to take a max of one and the log of z here in order to avoid the expectation being minus infinity for trivial reasons. So we managed to get So, we managed to get to prove that actually the expectation of our random variable is given by Bd. What we are still missing is convergence in probability. So, the aim in this missing last bit of the proof is going to be to show that log of Z divided by N actually converges to B D, to this magical beta free energy in probability. So, how do we do that? Well, the proof strategy is that we will actually establish an upper bound on 1 over n log z. Namely, what we are going to prove is that with probability tending to 1, 1 over n log z is upper bounded by the beta free entropy. And this upper bound, together with the fact that the expectation is The fact that the expectation is approximately equal to the beta-free entropy then implies convergence in probability. Because if with a non-vanishing probability your random variable would be a bit smaller than Vd, then thanks to this upper bound, the expectation couldn't catch up anymore. So you would be left with a gap that would contradict the formula that we obtained for the expectation. So all we need to do is we just need All we need to do is we just need to prove this upper bound here. And to this end, we are going to use a technique called the interpolation method. But in order to apply the interpolation method, I'm going to introduce one, we need one little extra twist, namely, what we need to do is we need to introduce a soft constraints version of the two-set problem. So the original two-set problem that we started from asks for That we started from asks for actual satisfying assignments. There's no compromises, you have to satisfy every single clause in your two-sub problem. And now we are going to relax that and we are going to say, well, you know what? I'm willing to make some compromises. It's okay if you leave some of the clauses violated, but I'm going to charge you an e to the minus beta penalty factor for every violated clause. And so this is kind of And so, this is kind of like how the POTS model compares to the graph coloring problem. And ultimately, we are going to take this term beta to infinity. So ultimately, in the limit as beta goes to infinity, we are going to essentially recover the hard constraints version. Now, one thing that is obvious from the definition of the partition function and of this soft version here is that the hard The hard partition function with the hard constraints is always upper bounded by the soft version. And the main reason why we go out of our way to introduce the soft version is because we can apply off-the-shelf concentration inequalities to the soft version. Specifically, we can use the Izuma-Hofding inequality to prove that log Z beta of phi Of phi, so the log of the soft partition function, is actually equal to its expectation plus a little of an n error term, the phi probability. And that is because in the soft version, if I add or delete one single clause, it can only at most change log z beta by plus or minus beta. It can only shift the log of the partition function by so. The partition function by so much. Whereas in the hard-constrained version, if I add one single clause, it could sink the ship. It could cause the entire formula to be unsatisfiable and wipe out an enormous number of satisfying assignments. So that is why we use the soft constraint version, so that we can apply this concentration result here. So, in other words, in the case of the soft constraint version, convergence in probability to the expectation. probability to the expectation is actually trivial. So all we need to do now is we just need to upper bound the expectation of log z beta in this soft constraint version. In other words, all we need to do is to prove that if we take the double limit, this beta, this penalty parameter going to infinity and m going to infinity of one on m expectation log z beta is upper bounded by Is upper bounded by our beta free energy. And so this is where the interpolation method comes in. Actually, what we are going to prove is that this expectation, the expectation of 1 on n log z beta, is upper bounded by the corresponding soft version of the beta free energy, which is defined here, but it's very. Which is defined here. But it's very easy to see that as you take beta to infinity, as you make beta large, this quantity will actually converge to the original hard Bd. So this inequality here does the trick for us because Bd beta actually converges for beta to infinity to Bd. So this is what we are left to prove, and we are going to use the interpolation method. The interpolation method to establish this result. And so here's how the interpolation method works. Here's the big idea. We introduce a time parameter t. At time t equals zero, we have our original combinatorial problem right here. We have the original two-sub-problem. That is, we have an actual two-sub problem. is we have an actual two-sat formula with a bunch of Boolean variables and a bunch of clauses. Every clause contains two variables. Every variable might appear in several clauses. And so that's the picture at t equals zero. And now as we increase t, we are going to move towards a simpler picture, towards this picture over here that we get at time t equals one. Equals 1. Now, the big difference between this picture on the right and the one on the left is that on the right, we have a graphical model, a graph structure that has several connected components. And in fact, every component contains only a single variable and a bunch of constraints. So, what we are going to do as we move from t equals 0 to 2 equals t equals t equals 0 to t equals part t equals 1 is we are going to dissolve the interactions between the different variables in our combinatorial problem and the semantics of the constraints in this problem here is that they are effectively fake clauses so these are claw these are not real clauses that contain another variable but these are kind of actors that pretend to the variable over here To the variable over here, that actually there's another variable, another ghost variable sitting down here that pretends to also take part in this clause. And this ghost variable here pretends to also have a probability of being set to true or false that is going to be drawn for each of these fake clauses independently from our distribution, from the solution to this. From the solution to this density evolution equation. So, this is not a real clause. It is a clause that only really contains this one variable and that pretends that there's another variable sitting here that isn't actually there. It just tells this variable, you know what, there's somebody else I'm talking to on the other side. There's this other guy. And I have to tell you, this guy is a really difficult case. He's sending me this particular message. And he really, really wants to be, for example, he really wants to be true with 99% probability. wants to be true with 99% probability, so I need u to be false because otherwise I'm not satisfied. So this is kind of a virtual clause that only is visible to this variable. Now from the viewpoint of the variables, the basic intuition behind the interpolation method is that the picture is not going to change much. Namely conceptually this variable, if our no long-range correlations intuition is correct, this variable shouldn't actually care. Correct, this variable shouldn't actually care if there's real clauses with real variables down here or these virtual clauses, these phantom clauses. And in particular, the number of clauses in which each variable participates, the number of phantom clauses is going to be distributed just like before. So there's a Poisson D number of phantoms for every variable. every variable. And as a result, this means that because as a result, this means that the number of constraints, the number of phantom clauses here in this right picture is going to be twice as big as the number of real clauses over here. So in order to put them on the same footing, what we need to do is we need to introduce some balance. So we just need to weigh this left picture down a little bit by appending a bunch of constant factors to the Bunch of constant factors to the partition function so that the number of constraints doesn't change throughout the process. Now, as we move t from 0 to 1 in this picture, what we will do is we will reduce the number of actual random two clauses. So you can think of this as actually removing real clauses from the problem as we increase t. And we will increase the number of such factors. The number of such phantom clauses commensurately. And of course, we will also keep control of these ballast factors and we will remove them as we proceed in order to keep the total number of constraints, the total number of boxes in this picture constant for any t between 0 and 1. Why do we do that? The main reason is that, like I said, The main reason is that, like I said, we have this totally disconnected picture here on the right, where every connected component only contains a single variable. So, computing the partition function over here is a cinch because the partition function turns into a product over connected components, and each connected component only has a bounded size and only contains a single variable. So, it's really easy to compute the partition function over here. Partition function over here. And therefore, what we would of course like to do is we would of course like to relate the partition function in the original picture with the partition function in this grossly simplified model. And so just to write down the partition function, what does it look like? Well, we have to sum on all possible truth assignments of our variables. The number of variables doesn't change as we move. Then here we have the term corresponding product. The term corresponding product corresponding to the real clauses. Here we have this product corresponding to the phantom clauses that, like I said, pretend that they are talking to somebody else on the other side that is sending them a message drawn from this fixed point distribution that we constructed. And we have these balanced constraints as well, which pretend that there's actually two of these go. That there's actually two of these ghost variables talking directly to each other. So that's the partition function of the model for intermediate values of t. And the magic in the interpolation method is that it turns out that it's easy to show, easy to see, that the derivative of log z beta in terms of t is non-negative. Negative. And therefore, what we can say is that the partition function here on the left is upper bounded by whatever we get here on the right, which is easy to compute. And it turns out that what we get on the right is precisely the beta-free energy. So the only job left to do is basically compute this derivative here to check that in fact the partition function doesn't go down. The partition function doesn't go down as we replace real constraints by phantoms. And this computation is not awfully difficult because, as you can see in this, as you can see here, the parameter t only enters into these variables mt, mt prime, mt double prime, which are Poisson variables. So all we need to do is we need to take a derivative of this expression here. Expression here with respect to the Poisson densities of these variables mt, mt prime, mt double prime. And combinatorially, this turns into a calculation that is very similar as the ones that we did in the Eisenman-Sim star scheme. Namely, basically, what we need to control is the impact on the partition function of a few local changes. So we just need to control what happens. So we just need to control what happens to our partition function, for example, if we add one more random two clause or if we add one more phantom. And so this calculation takes maybe two or three pages, but it's not awfully difficult. So once you know what to do, it's fairly straightforward. So the courage that it takes to apply this method is basically just to be bold enough to suspect. Be bold enough to suspect that something like this might actually work. And it does. And so when you take this derivative and you calculate it meticulously, you discover that this expectation here, this derivative of the expectation here, actually turns into a sum of squares. So of course, it turns out to be non-negative. And all the complications of the model, all the correlations and what have you, are actually hidden in the model. In these squares. So, I mean, the mere algebraic fact that you end up with the sum of squares implies that you get a non-negative value. And therefore, we obtain the upper bound on the soft version of the partition function, and therefore we obtain the convergence in probability of the hard constraint partition function that we wanted to prove. Function that we wanted to prove. So that completes the proof of the random two-side problem. There's a bunch of slides with references and like I said last time, I think there's also a file uploaded where all the relevant references are actually written out. Anyway, let me stop talking about Tusadwite here and And continue with the group testing problem. Or should we actually have the break now and maybe continue with the group testing later? Yeah, sure, we could. Let's see if are there any questions for Amin? So you're happy to just prove positivity of the derivative and you don't need to compare it to the derivative of B beta? No, no, no, not at all. I mean we are happy to just get to just get the the fact that what we obtain here is an upper bound of on on what we On what we started from. That's all we need. And so, I mean, this interpolation method was first developed by Guerra for the SK model. And I mean, you just basically have to be bold enough to believe in this trick. And then the calculations turn out to be reasonably easy. Other questions? Yeah, I think if there are no other questions, we can move forward with the rest of the talk. The rest of the talk, all right. So, I'll just sort of move on to group testing then. Yeah, so Philip is also in the chat and happy to answer any questions, especially about group testing, I suppose. Right. Okay, so right, so what I would like to explain you now is: so, one thing that happened. So, one thing that happened in the last 10 years or so is that these physics ideas that for some time were used primarily to analyze problems in random discrete structures, random graphs, random matrices, you name it. So, these methods in the last 10 years or so have been applied increasingly to Bayesian inference problems. problems and one specimen of such and one particularly beautiful specimen of such an inference problem is group testing so when we started looking into group testing about maybe a year and a half ago or so the following scenario seemed very far-fetched imagine that there's a new virus spreading across the planet and Planet. And there's a very large population, for example, the population of Europe or North America or so. And there's a scarce supply of test kits. There's a scarcity of test resources in order to check people for infection. And in the early stages of this infection process, when the infection is just getting started, you may assume that the number of infections are infected. You may assume that the number of infected people actually scales as a polynomial n to the theta of the entire population size. So, in the very early stages of such a pandemic process, right at the start when you really want, when you might still have a chance maybe to stop the pandemic from spreading, the number of infected people can reasonably be modeled as n to the theta for some theta between 0 and 1. The theta for some theta between 0 and 1. And now, what you would like to do is you would like to conduct as small a number of tests as possible so that you can exactly identify the set of infected people. And what we are assuming in this particular version of group testing called non-adaptive group testing is that all of these tests are conducted in parallel. So you do one round of testing only. Of testing only, and so you cannot base your subsequent tests on the test results that you learned from previous rounds of testing. And we do, however, assume that the tests are precise enough so that we can pool several patients and put them into a test group and test all of them simultaneously. And then if And then if, I mean, and then we assume that the, for simplicity mostly, we presume that the test result is exact in the sense that if any one individual in the group that we tested is infected, then the test result will be positive. Whereas if there's not a single infected individual in the group, the test will come back negative. Okay, so does this problem description make sense? Does this problem description make sense? I mean, the setup is that understandable? Yeah. Okay, and now the question is, how many tests do we need in order to identify the set of infected people? And to be precise, there's two separate questions that you might want to ask. One question is: can we actually identify the set of infected people on the basis of the test results? People on the basis of the test results information theoretically. So, assuming that there's absolutely no limit on our computational powers, we have unlimited computational resources, can we then pinpoint the set of infected people? And the second question, somewhat more ambitious maybe, is whether you can, in fact, solve the problem algorithmically. So, is there a polynomial time algorithm that, given the test results, is going to Given the test results, it's going to tell you who the infected individuals are. And one great feature, of course, in this problem is, like in a few other inference problems as well, one great feature in this model is that in this problem is that we have the test design at our discretion. So it is up to us how we want to use our test kits, how we want to form these test groups, and how we want These test groups, and how we want to allocate patients to groups. In particular, we are assuming, for the sake of simplicity, I'm going to come back to this in the end, that there's no limitation on the pool sizes, on the sizes of test groups that we can form, and also no limitation on the number of groups that a single individual can join. And so, this problem actually fits into a general framework of Bayesian inference problems. Problems. So, in these Bayesian inference problems, like for example the stochastic block model that I mentioned already in the first lecture, there's a hidden ground truth that we aim to learn. We know what the distribution of this ground truth is. For example, in this problem, we know that we assume that we approximately know the number of infected individuals. And based on this ground truth and its known distribution, And it's no distribution, we are presented with some data. In this case, the data is the test results. And our task is now to recover the ground truth from the test results, from the data that we can observe. And that means that we effectively aim to draw a sample from the posterior. We try to effectively randomly sample from the posterior of the ground truth, given the Of the ground truth, given the test results. And so that would be the optimal inference algorithm, the optimal inference scheme in this kind of problem, an observation that is sometimes called the Nishimori identity in physics, but it's really just a Bayes rule. Okay, so in particular in the group testing problem, what we would like to do is we would like to, for inference algorithm, ideally we would like to draw a sample from the posterior of the set of Of the set of infected individuals given the observed data, given the test results. Okay, so coming back to this setup of group testing, let's maybe begin with a simple information theoretic lower bound. Remember, k is the number of infected individuals and n is the total population size. And we are assuming that k scales as n to the theta for some. As n to the theta for some number theta between 0 and 1. Then one thing that you can observe is the following: there's obviously n choose k possible sets of infected individuals. Any k set is one possible set of infected individuals. And if you conduct m tests, the total number of possible test results that you can possibly get is 2 to the m. Get is 2 to the m. So if you want any chance at actually recovering the set of infected individuals correctly, you had better make sure that 2 to the m, the number of possible outputs, is at least n to the, n choose k, is at least as big as the number of possible sets of infected individuals. If you use Stirling's formula, this simplifies to this expression. simplifies to this expression over here. And throughout we are going to see this term k log n as the correct order of magnitude of the number of tests that we need. And we are going to be interested in finding the right leading term here, the right leading function of theta here in front. And what I'm going to do in due course is complete this picture over here. At the moment, Here. At the moment, based on this simple counting bound, we can say that down here in this red triangle where this axis here is theta and this axis here is this function, there's no life at all. So we can be sure that in this red triangle, it is information theoretically impossible to solve the group testing problem, no matter how clever you are and no matter what test design you use. In fact, in this In fact, in this area, it is impossible to solve group testing even adaptively, even if you conduct several rounds of tests and can base your design on the previous test results. So now, what about positive results? What can we get? What can we say positively? Well, one instantaneous idea that you might have is, well, we could just come up with a randomized test design. With a randomized test design, we could basically just use a random hypergraph as our test design. And presumably, it's a good idea to make this hypergraph biregular. So we would want that each patient takes part in an equal number of tests, which I call delta. And we would probably also want that each test contains an equal number of patients, which I call gamma. And if you think about a little bit, If you think about a little bit, so guided by this previous slide here, one thing you would like to make sure, obviously, is you would like to maximize the entropy of the test results. In other words, you would like about half of the tests to be positive and a half of the tests to be negative, because otherwise you're most definitely going to undershoot this bound over here. And so it turns out, upon reflection, Turns out upon reflection that the correct degrees, the correct number of tests per individual to maximize entropy is this. Remember, m is the total number of tests, k is the number of infected individuals, and the correct number, the correct degree for the tests is this. Remember, m is the total population size. So that's maybe a promising first step. That's maybe something you might want to explore a little bit before. Potentially, you might want to try. Little bit before, potentially you might want to try something more complicated. And in fact, this test design has been investigated, and we know very precisely, information theoretically, at least, what's going on with this test design. Namely, we can write a specific function. Remember, this k log n is always going to be there. We can write a specific function of theta, namely max one minus theta on log two, theta on log squared two. 2 theta or log squared 2 in front of this scaling order. And it turns out that this function here marks the threshold where the group testing problem can be solved information theoretically on this random hypergraph test design. So in particular, Johnson, Aldrich, and Scarlett proved that your group testing problem is information theoretically impossible to solve if you have. Solve if you have if you conduct fewer tests than this. So, if you conduct fewer tests than this, your random hypergraph does not do the trick. It's not going to be able, this particular test design is not going to be sufficient to solve the group testing problem. On the other hand, one can prove that above this threshold, if you conduct more tests than this, so in other words, in this yellow area up here, actually you can solve the. Actually, you can solve the problem if you are willing to spend exponential time because the problem reduces to a hypergraph vertex cover problem. Okay, so in the yellow area, at least we can solve the problem in exponential time. In the red area, we can't solve the problem. And in the white area, we don't at this point know a test design that could be used to solve the problem. Now you could ask: what about algorithms? Can we actually do this efficiently? Can we avoid an exponential running time for the inference algorithm? And in fact, Aldrich Beldassini and Johnson proposed an algorithm called DD, Definitive Defectives. And so this is a very simple, greedy algorithm that works as follows. Obviously, one thing you can do without making any mistakes. Do without making any mistakes, is you can look at the negative tests that you get, and you can rest assured that whoever takes part in a negative test is healthy, because otherwise the test would have been positive. So we can directly exclude all of these individuals and conclude that they are healthy because they appear in negative tests. Now we are only left with positive tests, and we might be now in a position where In a position where, thanks to the first step, there's a test like this that only now contains one single individual. So, this test is positive, and we already know based on the first step that all the other individuals in this test, except this guy here, are healthy. So, it's safe to conclude that this guy here must be infected. And so, that's what the DD algorithm concludes. And so, how about the other individuals, the individuals where we are not so lucky? The individuals where we are not so lucky that they now pop up in just a unary test, well, the DD algorithm will just declare them uninfected. It will just say, you know what, this person is probably fine. And so it's a very simple-minded procedure. It's going to make mistakes unless it is lucky enough that this simple greedy step is at times. 3D step is entirely accurate. And in fact, it might produce an output that is inconsistent with the test results. Like, for example, in this case here, there's no explanation why this test here should be positive. Okay, so that's one greedy algorithm you could maybe try. It's maybe worth analyzing something like this. And in fact, we can analyze this greedy algorithm and other like greedy algorithms. For example, Like greedy algorithms. For example, there's a slightly enhanced version of this DD algorithm called SCOMP. And the result is that these algorithms work in this blue triangle. So here's the specific function. If you are in this blue area, so if you conduct more tests than this, your greedy algorithms will work. If you conduct fewer tests, then unfortunately, they are not going to work. Unfortunately, they are not going to work. So, even though in exponential time on this random hypergraph test design, we can actually identify the set of infected individuals, even in this yellow area here, these greedy algorithms will fail. And so that's actually a kind of picture that you see a lot in such inference problems. Namely, for example, if you take this value of theta here, you see that there's a hard face down here. Phase down here. So we know that the problem cannot be solved down here. So this is an impossible phase, it's impossible to solve the problem. Then comes a phase where we don't know an efficient algorithm, but we know that the problem can be solved in exponential time. And then comes a regime where the problem can be solved easily. So this impossible, hard, easy kind of picture crops up a lot in the context. Pops up a lot in the context of inference problems. And of course, the question, the natural question is: can we actually make this blue area, this yellow area blue? Can we find an algorithm that solves the problem in this yellow area as well? And naturally, another question is: how about this white area? Is there any life going on there? Can we do something clever here, perhaps? And so. And so, I mean, to cut to the chase, in this random graph design, in this random hypergraph design, we simply at this point do not know of an algorithm, no matter how sophisticated, that can actually solve the problem efficiently in this yellow area. But fortunately, in group testing, it is up to us to define the test design. And so it turns out that there's another Turns out that there's another, a better test design that actually admits an efficient polynomial time inference algorithm that also works in this area that was previously yellow. So the new algorithm called SPIF comes with a tailor-made test design that allows for efficient inference of the set of infected individuals, even in this blue area that used to be difficult. Blue area that used to be difficult before. And it precisely, so now the area where we can do efficient inference precisely matches the this black dotted line where for the random hypergraph design, the problem can be solved information theoretically. Okay, so how does this test design work? It's actually inspired by some work on low density. Work on low-density parity check codes. Low-density parity check codes are very powerful error-correcting codes and they can be decoded by belief propagation. And a particular design that is very amenable to belief propagation was proposed in this paper here. And the idea is to superimpose a geometric structure, a spatial structure. Structure, a spatial structure on top of your random graph. Here's what we do for group testing. We partition the set of individuals and the set of tests that we conduct. We partition these sets into compartments of equal sizes. So here's my first compartment, it contains some number of individuals. Here's the second compartment. It contains the same number of individuals, and so on and so forth. And so these compartments. Forth. And so these compartments of individuals are arranged topologically along a ring structure. So, in the end, here you can think of this as wrapping around. So, here it wraps back to the left, and ultimately, the last compartment is adjacent to the first compartment of individuals. And similarly, for the tests, so we have an equal number of compartments of An equal number of compartments of tests. Each of these compartments has equal size, never mind these blue tests here for the moment. And the tests are also arranged in this ring topology with one compartment of tests corresponding to each compartment of individuals. Now, the idea is, so what about the number of compartments? Well, the number of compartments grows with m. Compartments grow with M, but it grows fairly slowly. And how do we now design the tests? How do we now decide which individuals take part in which test groups? Well, the idea is to say that an individual in this compartment, for example, takes part in the tests in its own compartment and in the following s minus one compartments, where s is some parameter that we choose. That we choose to go to infinity even more slowly than the total number of compartments. So we have this ring topology with the individuals being arranged in a ring, the tests being arranged in a ring, and each individual takes part in a bunch of random tests in its own compartment, in the following compartment, and so on, up to range, up to distance s minus one compartments to the right. And so So, the way we allocate individuals within the compartments is just random. So, every test receives an equal number of individuals from each compartment where it gets tests from, and each individual joins an equal number of tests in each of the S compartments where it participates. So, it's a blend of a geometric construction and of a random construction. Like I said, Construction. Like I said, a similar idea was previously used in low-density parity check codes and also in compressed sensing, which is another example of an inference problem that you might learn about next week. So that's the idea of spatial coupling. And so why, and so one extra feature of this construction that I should mention is This construction that I should mention is that we single out a few starting compartments, a few initial compartments, namely the first S compartments of tests and individuals. And we make the ring a bit thicker at this point, if you like. So we add some extra tests in these seat compartments, in these initial compartments, in order to get a head start on the initial. Start on the initial S compartments. And now the big strategy for the inference algorithm is going to be something like this. Because of the extra tests that we added in the seed compartments, a greedy strategy, like for example the DD algorithm that I described, is going to suffice in order to correctly identify the set of infected individuals in these compartments here, in the C compartments. In the seed compartments. So, thanks to the extra tests and by using the greedy algorithm, we are going to be able to 100% accurately identify who is infected in the seed compartments. And then what we are going to do is we are going to move along the ring and diagnose one compartment after the other. And the improvement over the plane random hypergraph is that. The plain random hypergraph is that we get extra information. Namely, for each test down here, we already know the precise infection status of most of the individuals that take part in the test because we already diagnosed all of the previous compartments correctly. So assuming that we diagnosed the previous compartments correctly, we can then start and get more information, extract more information out of these tests than we could have a priori. Than we could have a priori, and therefore enhance our inference powers for the next compartment, and so on, until we reach the end of the ring. So that's the strategy behind spatial coupling. And this strategy is actually useful not just for group testing, but like I said, for other inference problems as well. It's a very powerful trick that you can try to use for many inference problems. Many inference problems that interest you, perhaps. So, like I said, here's the algorithm. We run the DD greedy algorithm on the seat compartments. As we move along, of course, we can always be certain that any individuals that appear in negative tests are, in fact, uninfected. That's not a problem, that's perfectly legal. And now, how are we going to diagnose the individuals in the next compartment? Well, what we are going to do: Next compartment. Well, what we are going to do is we are going to introduce a score w for each individual x, and we are going to compute the value of the score for every individual x in the new compartment that we are diagnosing. The score x will depend on the tests where individual x takes place and on the status of the other individuals in the test that we diagnose. Individuals in the test that we diagnosed already. And we will simply then, in the next compartment, say or decide that the K over L individuals with the highest scores are probably the ones that are infected. Unfortunately, we are going to make some mistakes in this process. We are not going to be 100% accurate as we move along the ring, but luckily, But luckily, thanks to the expansion properties of this random graph, there's a combinatorial cleanup step that we can use in order to finish the job and correct after the event, after we are finished with the ring, to correct any mistakes that we might have made. So, how do we define this score? That's, of course, the difficult bit in the description of the algorithm. The idea The idea is to look at the number of as yet unexplained positive tests that are to the right of individual X, either in the same compartment as X or otherwise to the right of X. So why does that make sense? Well, if I look at, so what do I mean by an unexplained positive test in the first place? Well, I mean a test like this. Mean a test like this. This test here reports a positive result. So something, one individual in this test here must be infected. But the test is as yet unexplained in the sense that all of the individuals from the compartments that I diagnosed before, all of the individuals in compartments to the left of X, only are only diagnosed as healthy. Are only diagnosed as healthy. So, this test here doesn't contain any individual in the currently known compartments that are identified as infected. In other words, I don't know why this test is positive yet. It must be one of the individuals that I haven't diagnosed already. So, such an unexplained positive test is, of course, a cue, a hint. Q, a hint that X might be infected. And so, because obviously, if X were infected, that would explain why this test is positive. So, then I would know. And therefore, this test being positive, the more tests like this that I see, the stronger my belief that X might be infected. Otherwise, for example, if there's absolutely no single such unexplained test, Single such unexplained test, then I would probably go and bet some money that X is actually healthy. Okay, so this number of unexplained tests is a clue, a hint that X might be infected. And in fact, we can analyze based on our randomized construction, based on this spatially coupled geometric superimposed random hypergraph. We can actually, with a bit of thought, analyze the distribution. Thought analyze the distribution of the number of such unexplained tests. Namely, if I look at an individual and I look at a test that is J minus one compartments to the right of X, and in this test X participates, but it's currently unexplained, then I can say the number of such tests is approximately a binomial variable with parameters. Oops, with parameters delta over s. Delta is the number of tests where x participates, s is the size of the scaling window, 2 to the j over s minus 1. So if x is infected, I get this binomial distribution. By contrast, if x is uninfected, I get this binomial distribution where the minus 1 drops down from the exponent. And it turns out that the mean of this distribution, this binomial distribution, This distribution, this binomial distribution, this is the red curve here, is always bigger. So, a big number of unexplained tests is an indication that X is probably infected. And therefore, a first idea to define the score, a first idea to try and figure out who is infected might actually just be to simply count the total number of unexplained tests. The total number of unexplained tests in which an individual participates. So, for this x, I would simply count from its own compartment up to s minus one compartments to the right, how many tests are you in that are positive and that are as yet unexplained? So, that seems like a good idea because this number should be bigger, typically, if x is infected than if x is not infected. Not infected. So maybe I can use this variable to discriminate between the two cases. And it turns out that that already gives an improvement. So you can already significantly improve with this idea over the simple greedy algorithms. So you can actually make a large part of the previously yellow area blue using just this simple score. Unfortunately, Unfortunately, however, there remains a yellow area where the simple minus score doesn't do the trick. This idea, the simple idea of just taking a sum, is not good enough. In this yellow area, you will make so many mistakes that this method hits the wall quickly. So it's no good in this yellow area. So, in particular, this trick doesn't suffice to actually hit the information-theoretic bound of the random hypergraph. Random hypergraph. So let's see if we can do better. Let's see why that is. So here's the second attempt. The second attempt is to take one more insight into consideration, namely, if a test is rather close to X, is rather far to the left, and is as yet unexplained, then that test is actually a stronger hint that X is infected than an unexplained test, very many compounds. Test, very many compartments farther away to the right. And the reason for that is that in an early unexplained test, in an unexplained test far to the left, I already diagnosed most of the individuals that take part in this test. So there's very few white individuals in this test that have white spots on my map that I haven't diagnosed yet. And so this kind of this kind of And so, this kind of test is probably a stronger hint than a test that comes much later and that still has very, very many white spots on its map. And there's very many individuals in that test that I haven't diagnosed yet. So, the idea would be to introduce weights and to say, well, I'm going to give a larger weight to the early unexplained tests than to the late unexplained tests. Then to the late unexplained tests. And we are going to see on the next slide how you can actually derive the optimal weights using belief propagation. And belief propagation tells you that the optimal weights are these simple expressions down here. And turns out that with these optimal weights, these belief propagation inspired weights, you can actually completely fill this yellow area and make it blue. Area and make it blue. So, this will allow you to actually precisely hit the information theoretic threshold. These weights are optimal and they completely close the gap. So how do we use belief propagation to derive these optimal weights? So here's the idea. Here's my individual X that I'm trying to diagnose. Here's my test A. And now I want to figure out Figure out what the probability is based on this information, what the clue is that A sends to X, what the message is that A sends to X as to its infection status. And so one thing that I can do is I can completely ignore any healthy individuals in the sense that these are individuals that I can positively identify as healthy, either because they appear negative tests or because I already diagnosed them as healthy in the previous. Diagnosed them as healthy in the previous iterations. And so I can completely focus on these blue tests, on these blue, on these, sorry, on these white individuals down here. These are individuals that I don't yet know. Now I can presume, obviously, that these individuals are independent. They are kind of independent. The only dependence is that the total number of infected is going to someone to k. So each of them is infected with. Infected with probability k on n independently. And if I look at a test that is j compartments, j minus one compartments to the right of x, then I can actually very precisely estimate the number of such undiagnosed white spots on the map, and that's going to be J on S times gamma, and that's approximately N on K times log two by the choice of gamma. The choice of gamma. And so I can then calculate the log likelihood ratio. We already saw that log likelihood ratios sometimes simplify computations, the log likelihood ratio of X being healthy or X being infected. And of course, if X is infected, then I have a perfect explanation why this test is positive. The reason is that it contains the infected individual X. The infected individual x. And otherwise, I'm going to have to hit an infected individual down here. And the probability of this is just given by that expression down here. If you simplify that, you get precisely minus wj, minus wj, because I swapped numerator and denominator over here. So this is how you derive the optimal weights. To be honest with you, this is not what we did initially. Initially, we meticulously derived Meticulously derived the large deviations weight function of this weighted sum over here. And we did a Lagrangian optimization to find the optimal weights. But later on, we discovered that actually we could have used the simple belief propagation trick all in all. Okay, so yeah, so this means that at this point we have an efficient algorithm that matches the threshold. Matches the threshold, the information theoretic threshold of the purely random hypergraph. Previously, this triangle here was unknown to us. We didn't know what's happening here. And this gap is closed by this theorem here, which shows you that, in fact, in this red triangle, there's no life. So, in this red triangle, it is not possible to solve the group testing problem. There's no way of There's no way of actually, I mean, no matter how much computation time you're willing to spend on the problem, it is information theoretically impossible down here to identify the set of infected individuals. Right, so how does the proof of this result work? I'm not going to go into any details because it's rather more combinatorics than probability theory. Than probability theory. But one important trick is to use dilution. So the idea is something like this. Suppose we have an algorithm, an efficient algorithm that works for a low infection density, for a low value of theta, for example, over here. And now we want to solve the problem for Solve the problem for a high infection density, somewhere over here, for example, then what we can do is we can simply dilute our population. We can simply take our population and add a huge number of negative samples, effectively a huge number of healthy dummy individuals. And then we can use the test scheme down here. The test scheme down here. And therefore, if I can show that for a large value of theta, theta close to one, it is not possible to solve the problem, then that implies that it's also impossible to solve the problem for a lower value of theta. So in other words, this means that in order to prove the information theoretic lower bound, in order to make this triangle red, I only have to make the tip of the triangle over here red. Over here, red. A second observation is regularization. So we can actually prove that optimal test designs are approximately regular. Intuitively, one reason for this is imagine that you take a test that contains a whole lot of individuals, many more than in the regular, many more than the average. Then you can check that actually with. Then you can check that actually, with high probability, this test is going to be positive. So, unfortunately, this test is not going to give you a lot of information. You can essentially replace it by a constant. And therefore, big tests are not particularly useful. For similar reasons, very small tests are not particularly useful because they are going to be negative virtually all of the time. So, only tests So, only tests that are approximately of the average size are going to be any good to you. And from the regularity on the test side, you can then deduce that also the variable side must be essentially regular. Another idea that actually goes back to the paper by Misa and Toninelli and was also used by Aldrich in 2018: is that we can Is that we can that there's some positive correlation going on in this problem, namely that we can use some tricks based on the FKG inequality to effectively assume that our graph is not just regular, but essentially also doesn't contain a whole lot of short cycles. So because of this positive correlation phenomenon, it's not a good idea to have test designs that contain a lot of short cycles. A lot of short cycles, essentially. And so at this point, you can probably already see that we are approaching the random test design, at least morally. We are talking about, we are concluding that a good test design is essentially random. And then there's a final trick based on the probabilistic method that tells you that if your number of tests is too small, there's going to be a small term. Tests is too small, there's going to be some disguised individuals in such a regular test design without many short cycles. And this means, so disguised individual means that this is an individual that only appears in tests that contain another individual that is actually infected. So, this means that you cannot tell from the test results whether this particular individual is healthy or infected. Is healthy or infected because it's masked by, because its test results are masked by another actually infected individual. And so it is simply not possible for you to correctly diagnose these infected individuals. And what we can show using the probabilistic method is that, in fact, there's quite a few such disguised individuals, enough that there's both healthy and infectious. Both healthy and infected disguised individuals. So, right, so the summary is that based on this idea of spatial coupling, that, like I said, is actually very useful. It's actually a great idea that I'm sure will extend to a lot of other Bayesian inference problems. And inspired by belief propagation to some extent, at least in hindsight, we can come up with an optimal efficient inference algorithm. An optimal efficient inference algorithm. And we also have a matching information theoretic lower bound. So, what we learn is that, in fact, this random graph test design is information-theoretically optimal. And finally, what we can see is that there is an adaptivity gap. Namely, like I told you in the beginning, in this red triangle down here, not even adaptive group testing schemes are going to work. Testing schemes are going to work. That means that even if you do several stages of tests and you base your test results in the subsequent stages on the test results, your test design in subsequent stages on the test results that you obtained earlier, you're not going to be able to get into this red triangle down here. However, Max and Philip observed that actually in this red triangle over here, Actually, in this red triangle over here, you can do adaptive group testing. There's a two-stage adaptive group testing scheme where you first do one round of tests and then one second round of tests based on the results of the first round that will correctly identify the set of infected individuals over here. So, this means that there's that from this point. That there's that from this point, log2 on one plus log2 on, there's a gap between what you can do adaptively, namely, you can also solve the red triangle, and what you can do non-adaptively. So finally, let me maybe make a confession, namely, of course, the test design, this group testing scheme that I described, is utterly impractical. This is not something that we are going to be. That we are going to be using in order to fight COVID-19. And that is because the tests that we, the groups, the test groups that we have to use are way too big in order to be practical in any realistic scenario. And also, the degrees, the number of tests in which the individuals take part are relatively Part are relatively large. Another problem is that, of course, in a real-world lab, the number of tests that you conduct is quite small. I mean, let's say relatively small for the purposes of a mathematical asymptotic analysis. However, what we implemented and what you can actually try is you can try to run leaf propagation directly on a random. On this random hypergraph test design for relatively small sizes, for relatively small values of n, like for example, n equals 1000 here. And it turns out to work relatively well in the sense that you can actually identify. So, this axis here is the individuals, and this axis here is the probability that a specific individual is healthy. So, you can actually. Is healthy, so you can actually identify successfully quite a lot of healthy individuals and quite a lot of certainly infected individuals. And for the ones in between, you can actually calculate using leaf propagation or at least approximate the posterior probability that they are infected. So, I guess what I'm saying is that there's, of course, this theoretical result that in and of itself is not directly practical. Directly practical, but nonetheless, some of the ideas can be used and implemented, and they are potentially, yeah, I mean, they are not outrageously unrealistic. It's not completely clear to me whether this is useful or not useful in practice, but it, I mean, to me as a layperson, it seems like something that might actually have at least a little bit of. Have at least a little bit of potential in the practical world. I don't know about COVID-19 or any specific disease, but I guess what I'm saying is that these theoretical methods can have practical ramifications that are perhaps even remotely useful. Okay, here's the references, and I guess that's it. Thanks for your attention, and I'm happy to take any questions. Yes, so well, thank you. So we'll give participants, I'll allow you to unmute yourselves. And let's just give a big round of applause for to Amin for his very nice lectures. So with this, we're going to stop the recording and the live stream on YouTube. And Um