So I would be, let's say, pleasantly surprised if you didn't have that problem here. I was wondering, I fear that when you simulate, you're actually the result is biased. So in our case, we did find biased studies when we were doing it in sample, but when we did it out of sample, because here, okay, let me show. So, when we did this, where we split it into training and test, and trained the classifier separately, on this data itself, we did not have the bias problem. I'll have to look at the paper to see if there could be. Since your classifier is going to have noises parameters, whenever you sample with the non-parametric booster test statistics with noises parameter, you have to make a bias correction. For classical booster. Correction for a classifiable person. Here is. So, in this case, we don't train the classifier again. Does that make the difference? So, the classifier is not part of the resampling part. But the test statistics is simulated using the most of the time. The test statistic is. But it's for the same classifier. So, you are not re-estimating the parameters? No. And that's the least to avoid? Okay, I will think about it. It's a good question. I am happy to talk about it. Question. I am happy to talk with you this afternoon. I wonder. I mean, if you don't run into the problem, is the problem. If you do, it's something that we can do. No, so when we did in-sample, with the bootstrap particularly, we did run into the problem, which is why we only do the permutation one for the in-sample one. So we did have a problem in this case when we did the bootstrap instead of the permutation, which is why we did the permutation. But I did not know about this paper, so I will go back into it. I would think there, and maybe that's why we had a problem here. But we don't have a problem in the other case. Okay. But I don't know why. That's a very good comment. So let me make the same point as well. Guy, so this is also not a model independent. I thought the model is basically the first half of the data. If I understand you correctly, basically you're answering the question: does the second half of the data? Question: Does the second half of the data look more like the simulation? More like the first half of the data. And then you're kind of saying, okay, the only alternative that is relevant is the alternative already realizes the data, and that's the one that optimizes more because you can't get power for all possible alternatives. Correct. So, in this case, especially because we use a classifier, like high-demand, like random forests, neural networks, like none of these have studies on which direct. Have like studies on which directions they are powerful in. So, in this case, we have this thing that if the classifier is consistent in that problem, like for whatever the two samples are, like if it's consistent in actually finding it, finding the true classifier in some sense, then our test is also consistent. But if the classifier doesn't have that consistency in actually finding the true differences, then yeah, the test won't look very good. And then I I guess the important question is whether or not the discrepancy to the simulate background simulation is because the data, which way is it anomalous? Is it anomalous in kind of eucalyptus way or is it anomalous? I totally agree. That is definitely like this that is what yeah that's exactly the point here which is we don't know. Can you go back to this slogan? You can go back to this flow chart. So you have like this PLOP value reject, I guess. You could do like a type of CLS type thing. You could estimate the power of the test. Using the empirical distribution of the test data, you would know what the... So maybe I missed it. Do you know what the power is? So this table basically gives you the power of the different methods. So these are basically the power estimates. This is like in percentage, but like it's but this is this is the power table. So what is that number signal strength say of 0.1? That means a tenth of the data is in the signals. This one? Well anything? Well not 0.1. So 0.1 is 10% of the data is signal. So 10% of the experimental data is signal. And in this one we basically go up to like Basically, go up to like this: 0.5% of the data is signal. Yeah. Did you go for Elon Mill Gimme's book with Jane Murray on the progression test? It's a 2016 ADA and it's published. So he had a very nice result for fermentation tests. So it's a probabilistic classification. Yeah. So with fermentation tests, you control the With implementation tests, you control the type 1 error. Then it has a theoretical result where you relate the power to how well you estimate your regression of the list of host feature with bounds. So I don't know if you want to comment on that. What is relevant or not? I think that would precisely say what the power of this would be. Yeah. But it's connected to how well we estimate a. How well we estimate a mean square of here. You estimate the uh class posterior of that, so it's not, yeah. So, in this case, also we estimate the class, like the probability. There's a theoretical law for you. You say how much what the power is, and it depends on how well you do your progression. So, in this case, we didn't look into that, but that would be very interesting to look into. Like, theoretically, we haven't looked into this problem at all. At all. But I think it's directly related to Linux work. The power depends on the altar and how resource it has. How does it depend on the altar? Well, it's basically show that you have high power on a wide range of alternatives. And it depends on how well you estimate. So basically we rewrote, I mean, the same test, but is there all binary classification? Binary classification as a regression, right? So how well, yes, the aim is, I mean, the difference with the probable y1 given x minus probable y1, right? And it was easier because it found a density ratio, which is the difference, the posterior colour. But you have to estimate your class posterior. I see. And you can see how well you estimate what the mean square or whatever method you use. It doesn't matter what method it is. Depending on how it performs and that it actually tests by ID, they data controls, but then the power, he showed that you have high power against a wide range of terminals. And this depends on how long you estimate your class material. So it's a 2016 idea. Yeah, I know. Yeah, but it can be extended. Yeah, yeah, yeah, but it can be extended to dependent data. Then let's first see that are there any questions online? Questions on Live? Yeah, okay. So then let's go Bob and then edit. I think it's worth mentioning it was actually 20 years ago, Priest at 2003, Jerry Freeman of the famous Freeman Tip Sharani, he gave a very short talk on using a classifier. Using a classifier for the two-sample test, and if a classifier couldn't tell the difference, then pass the two-sample test. Very short note, but it's fun to go back and what he had to say 20 years ago. So in fact, it's cited in your paper, in the archive post. I think that's the first year. Most people don't know that, but I think that paper is the first time we have ever seen anybody just using classifiers for identifying. I think he did it kind of on the fly. I don't remember because they had well. Gunter Zeck was coming with this energy test to that meeting, and I asked the curate to take a look at it. And I know he's giving a talk about it all the way to the top. I think he didn't actually do it in practice. He proposed that this money may be the actual. It's very very short and over the few speculations about depending on what classifier you use. Depending on what classifier it is, we'll get to the next so some of the previous modeling transitions, right, because I've been quite a few over the years, right? One of the issues is thinking about issues in the data, right? Inequality, for example, you know, like powers in the detector, et cetera. So I think when you think about systematics, it's not only about nuisance practices, right? And especially when you're talking here about, you know, chopping your data into a path, if one did that kind of naively, your second half might have, you know, Your second half might have radiation damage or pixel higher, which you pick up as a sick. So, how are you going to perform such? So, ideally, we should be doing the in-sample test where we don't split the data because, like I said, this is expected to have higher power and this will not have the issue that we are talking about. Again, like I said, this is just like the weighted thing between computational Between computational power you need to do this versus doing an easier test which might have problems. I guess the point I'm trying to make is this one is much slower than the other methods, but ideally if you have the computational power, you should be doing this one, where you don't split the data. The other thing I also want to mention is that this was one of the main reasons for doing the active subspace method to see when we found To see when we found something, what did we actually find? And so you can think of this as sort of a diagnostic step where you then go into the classifier to see, okay, did you actually find something that makes sense? Something that like we expect to find? Or it needs more investigation. It's not an end product in itself. But it can be a little bit more complicated than that, right? Because there can be helpful effects going on with the technology. Would be multiple effects going on with the factors throughout the data set, right? It would just be, you know, there's one foul missing from the cellular mission the whole way through. So your diagnostic test, right, could pick things up on average. But there may be some other effects that the whole context could. Let me just comment on it. So what's kind of interesting with this thing, so like in your situation, it would actually not declare an anomaly because it's declaring an anomaly if the second half looks like the first half and not like the single patient. So if the second half Like the simulation, so if the second half has some kind of missing detector, it would not look like the first half and would not be an anomaly, which is also kind of weird. And so I think it's kind of goes to show that we're actually interested in like a three-sample test. We're interested in seeing, so does the data look like the background? And if no, does it look like some interesting new distribution? And so we're answering the question: does the data look like the background? Maybe with no, but we don't know whether or not there's something out there. We don't know whether or not there's something out there that actually looks interesting. I mean, first half and second half doesn't have to be divided by time. Well, yeah, but in any case, you're only entering the. I'm not saying it's a good idea, but it doesn't mean time-dependent things. Oh, it's such a good idea. So I was actually thinking in a scenario where all of the data are perfect. Okay. And I was trying, I was. And I was trying I was wondering what what happens because you you have labels for the background and you have a certain number of background events yeah and I think you've heard us say you know we cannot go beyond a few billion whereas there are many more data events so do you know what happens in this method if timing with the Monte Carlo sample does not reach the Sample does not reach the tails that are present in the data? Let me think about that. So, if there's background data, you're saying that doesn't go into the data? Yes, so you've, I mean, I mean, many times we end up having events in the tail, but since we look at the distribution, we can see, oh, it's exponential. So, even if you don't have any simulation, you should probably build it. But in this method, would those events appear as Is that would those events appear as a signal? I have to think about it. But can I make a comment about this? Yes, because something I actually didn't mention, but it's a feature and an issue also for us. Like our technique do require that this reference sample has higher simplicity than the data themselves. Because otherwise, I mean you you wanna populate uh the the space that is The space that is created by the data. And if you have less statistics, then so what they are doing is a balanced classification. What we do is an unbalanced regression that can be seen as a classification, but we also have this requirement that we should have more events in the class zeros, more events describing the standard model than the data, so that we're sure we have we don't have statistical factuations in our knowledge. In our knowledge of the standard model, because we don't know how to treat that. If it happens that you have some outlayer falling out, let's say, outside this part of the reference, then in our technique, the regularization would sort of kill the model by taking infinite values, because the prediction would go infinite there. But in general, you would see outlayers in the data set. see outlayers in the in the statistics, but but you would ended up not using our techniques for that kind of problems because you couldn't see outlayers by eyes and you just put a cut, then you don't really need this kind of tools like all over Q. In a sense if you have some data set and you see there are some outlayers with respect to your reference uh distributions, you could try to perform a test like a simple cut on the tail of that specific distribution and see uh how significant that is. Even before applying this format, we are working on it. I understand that. So I understand that we can go and say we do not look where there's no support for it. Then I don't know if there are many physicists who said there are many things which didn't fail, so if you do have a tail of the term, Because of your methods, then require this sort of unique more statistics of a standard model, let's call it, than the data. And none of them has sort of a diagnostics as to whether you're seeing something beyond what you can make a statement. Like, is there a way of knowing because everything is multi-dimensional, and this is why I'm asking. If there's a way of saying this particular event is being targeted as interesting, but it's beyond That it's beyond where I trained, or is this a way of saying that? I would say that that's a different problem. So, you're talking about an outlier detection problem. So, there's a key distinction between this type of phenomenal detection and then outlier detection. So, what you describe is an outlier detection problem where you're trying to detect events at the tails of distributions. All of these are actually looking more at the bulk and then less at the tails. The animal is kind of defined by a collection, like an X. Is kind of defined by a collection, like an access of events. Something is called collective animated access. So those are slightly different parts of the proposal, not the outside package. Yeah, I'm definitely not talking, this would not apply to Bohemia, for instance. What I'm saying is it would never be related to that. But it's like the high Q squared, high energy transfer things where you might inspect a couple of events, and then I'm asking myself, is it a new signal or is it just that the model does not have enough? Or is it just that remotely not everyone else these things? I think that's the same set of techniques that you would apply in place like that. So, this method is basically designed so that it can find this cluster of points that stands out above what the background distribution says the distribution should be. So, it's like a higher density of points in some area compared to other points. But it's not like in the In the in not the domain. Like it has to be a collection within sort of the space of the right. So what I'm saying is that the fact that the green distribution, which is uniform here, in reality it's not uniform. You might have the meshes to be exponentially fallen. Correct. And then you might end up having a couple of points there. And okay, apparently they will be set. trying to to put these bands or like our peaks really close like far away in the tails what you will see is is that you still recover when in uh this data not having any support from the reference you have for our test at least the like ratio excluded to infinite you cannot get to infinite because we regularize the model to display infinite so you have some power uh to detect it but it would be suboptimal with respect to what you do have if you do another kind of test You have to do another kind of task which is specifically for out there. That was what I was saying before. So it's not that we don't see anything, but we sort of regularize the model so that we are not overfitting those kind of things when they are that wage. So it's clearly some optimal with respect to other things. But you still will see something. Maybe not a thousand sigmas, you will see, I don't know, three percent. Sigmas, and you can see I don't know three, four sigmas or two sigmas depending on how many of these outlayers you are and how far away they are from the tail. But yeah. Thank you. Yeah, you have this nice picture there. So I don't like high dimensions. I haven't understand them. So that situation doesn't seem to me to really need it. So if you just look at the x, y. So, if you just look at the x-marginals and I think about a histogram, I don't think I need a test to know that. So, the question is: you have an example where in 10 dimensions I can do all I want with the marginals, I'll never find no difference, but your test picks out that somewhere up there in some very complicated thing, there is something. I wish I had a graphic for this, but I think there's a certain temperature. I think there's a simple example. Think of two copulas that have the same marginal. Think of something that has the same marginalness, but along this axis, it's complete. And so I don't know if I have, hang on, let me see if I have to. I've been wondering for a long time whether high-energy physics data is like that. As truly, I know it's both of them. So, okay, this is not realistic, but like, this is what I'm talking about. So, this sort of an example. And if you actually look at the marginals of the x and the y, they're the same. But it's obviously there's a pattern. Now, think of this in high-dimensional space. So, this is actually the example I had to explain the active subspace method because, like, for the active subspace method, also, it's. Like for the active surface method, also, it's the same thing. Like, if your classifier is like this, if you do like just look at one of these dimensions, you can't see what the classifier is doing either. It's a good quick answer. Yeah, it's a great question. I'm glad I was able to show this. Guys, we never, sorry, well that was nothing sages. But, well, on this, I think we have real examples. Yeah. Many of our papers it's already been shown worries us that element can exploit high dimensional cor correlations and find something to but what if my point was gonna be getting back probably what Andre said but also what you have said about five sigma I mean five sigma gets abused a lot but I think it would be really abused if you want to start talking about it in this context because in practice what's going to happen is this is going to be an exploratory technique it's a global Technique. It's a global p-value anyway, so fast thing does apply. You've got to worry about systematics. But you're going to end up doing hypothesizing after the results are done. You're going to go look and find out where this thing is. That has all kinds of issues, but that's what's going to happen. People are going to start hypothesizing what causes that pump or what causes that excess. And so once real physicists start using it, there's going to be a lot more than comparing some comparing uh some scale probability that three times ten more so i i see it as a great uh tool yeah and a great tool for initialization and then you gotta deal later with the fact that you're working yeah it's uh similar to that seems like it's finds the biggest phenomenon or biggest difference between the data and the and the background simulation and uh biggest difference may not be the most interesting Biggest difference may not be the most interesting difference. Correct. And what do you do once you find that big difference? Usually, what people do when they're looking for something specific is that they cut out the data that they don't understand and they try to focus on data that they do understand. So that's the triage procedure here. Once you find something and you're bored with it, because you don't think that that's physics, you think it's just mismodeling in Monte Carlo. Do you fix it? Do you cut it out? What do you do? What do you do? So, in this case, I feel like I'm out of my knowledge depth, but what I can tell you is that, like, this was one of the again, one of the motivations for the active subspace method. And so, technically, you could go look into this. So, you look into these plots to see what variables is picking. So, I can then identify where it is. Exactly. You identify where there is, and then you can do what you said, which is cut out that region, and then you can do it on the rest of the space. Do it in the rest of the space, but I wouldn't know how to do that because I don't know how it actually connects to the wave input. Right, but it sounds like in order to find a small signal, you have to explain every event in your data that is anything bigger than the small signal. Because it could have looked more signal-like than the tiny thing that you're actually interested in. Yeah, it's good. This sounds like an iterative procedure. This sounds like an iterative procedure that you use to correct your simulation. And it's a slip very slow because if you cut, you can make smaller and smaller boxes around things that if you've got it. And we already know that that can push p-dots that start to p-backing. Can we look at the opposite scenario? It's a lovely method. Suppose we run it and there is nothing there. The experiments and the background look identical. Background looks identical. Now, that's not very interesting and it's not publishable. What we want to do is use that to set limits on these various other signals. Which bits of C C phase phase does it rule out? What limits does it set on the mass of the Z prime or what you? So we have to inject alternative hypotheses. How far back do you have to go to do that? Do you have to retrain everything or is it just a simple plugin at model? We would have to retrain it. We would have to retrain it. For every signal, you would have to retrain the classifier again. I'm thinking. Yeah, yeah. So it's actually, you can just basically reinterpret it. Once you have a fixed classifier, you just run, you don't need to retrain, you just run whatever signal model you want through it, you set the limit. For a lot of models, it will not have power, so you'll not be able to set the limit. But for any fixed To set the limit. But for any fixed pipeline, you could just run it through. And if you just happen to find a model for which this is sensitive, you're probably able to display it. I think you do need to retrain. I think you do need to retrain because I mean it's if you just happen that it picks up a face plate or like a regional data, like your Susie model also populates. Yeah. You're not lucky and you're able to exclude it. But I think the problem is that if the base case was such that it didn't find anything, that means it's actually just a random classifier. It has no information. So it would be a problem. I think what you're saying is true if we already found something and then there was more signal in the same direction, then it's actually trained something sensible. But if they both are actually the same distribution, then it just could be like a random like it's randomly guessing. It's like looking at a point at like classical coin. Yeah, yeah, it's like classical coin basically. Yeah, yeah, it's like classical coin basically. People have discussed this whether or not it always randomly initializes. Exactly. So this would be like a random classifier. Yeah, if it actually found nothing, that means the classifier is useless. Like it's just a coincidence. But that's the end case when you've discovered everything, right? Because all the signals are now background because of the terror condition. And that's you want to be in this case because that means you've done it. That's true. Or maybe until we get to that point then. Maybe until we get to that point, then what if your reference sample was, let's say, a single sample? So let's say you have some detective systematics or whatever. Could you like admit some of these alternative scenarios into your reference sample? I think that's what we might need to do to do the background system, like incorporate the background system addicts. We haven't thought about it yet. But I think that might actually be the way to go ahead. Where you might have like we have to think about how to do it. We really haven't planned for a thousand times. We really have a plan to hear. Okay, any other comments? If not, then that's perfect timing. That's good for me. Thanks a lot.