And the point is to study the version of the Wigner-Weisskopp atom in the edibaric limit. So let me try to show how it works. Okay, so the setup is that of the standard Vikno-Wiskop atom that was designed in the 30s, but we make it time-dependent. This has been revisited many times, say in the mid-70s by Davies from the math community, but also The math community, but also very recently because it's a nice playground to study open systems. And what we are going to deal with is a time-dependent D plus one level atom, which is interacting with a photon field. And this photon field will be a Bosa field with a photonic dispersion relation, but no helicity or things like that. And we will use a coupling, which will be time-dependent, to couple these two items. To couple these two items in such a way that the one excitation sector will be preserved so that the system is not too complicated to analyze. So what kind of motivations do we have to go back to these kind of questions again? I guess the main motivation we had was to study the effect of an environment when you are able to monitor a quantum system to control it, say in the adiabatic In the adiabatic limit, for instance, in case it's not really isolated from the environment, you want to quantify the effect of this environment. This is one of the main motivations that we have. And we see, when we do the analysis of this model, we see the emergence of an effective evolution so that you can ask questions about Markovianity. It seems to be a natural question, especially this week. And another motivation which I will not talk about is to be able to say something about the way the emitted photons build up. The emitted photons build up in the process, but I won't have time to discuss that. Alright, so being yes, he works, sorry. I'm supposed to say something about rigorous results, so I have to walk you through the notations and stuff. So the idealizer we have lives on C D plus 1. It's a time-dependent Hamiltonian, which drives it. It's given by its vector. It's given by its spectral decomposition. So you have eigenvalues. You have eigenvalues that are denoted by alpha j of t. I put up these eigenvalues here. The excited states, normalized vectors corresponding to these eigenvalues are denoted by phi j of t. And there is an extra state, which is the ground state, which is special. The eigenvalue is zero, but what makes it special is the fact that it's time independent. So it means that the space The space of excitation of your atom is perpendicular to this run state and it's time-independent. Okay, that's going to play a role later on. And now a few hypotheses, if I can do that, right? Since I'm going to deal with the adiabatic limits, we like to have some spectral hypotheses, and in particular, if I plot my eigenvalues as a function of time, I want to make sure that they don't. Time, I want to make sure that they don't cross one another, they get separated. So I have a minimal gap between the eigenvalues over the time span that I'm looking at, which is strictly positive. It's this quantity delta zero. And there is another regularity assumption. Everything is going to be as smooth as you want. I don't want to deal with things which are not smooth as a function of time. And concerning the Bose field, so I have my Fox space on the one particle. Fox space on the one particle Hilbert space, which is L2 of R3. The variable would be the momentum k in R3. And I have created an amelioration operator which satisfies the standard connotation relation. And I write the Hamiltonian of the field in a standard way. This is the number operator at momentum k, and I multiply by the energy omega of k, which, as I said, was taken to be the modulus of k. All right, now the coupling. Alright, now the coupling. So the coupling allows me to define the full space of my, Hilbert space of my system, the tensor product of the two. And it's again a time-dependent operator, which is a sum of contributions. And they are labeled by the excited state, sorry. So they have this following form. So on the part of the atom, you have an operator which takes something in the ground state and puts it in the excited state now. state and puts it in the excited state number j with an amplitude which is given by this number vj of t which is a complex number whereas on the side of the of the field while you annihilate a photon if you want with an annihilation operator which is a smeared out version of these annihilators that I have by means of a function which is L2 and that's the so-called form factor. And then you add the Hermitian conjugate of this expression to Conjugate of this expression to have something which is smooth and self-adjoint, so that we are in a standard setup to study the Schrodinger equation. So my Hamiltonian, as I said, is the sum of these three contributions. The only thing that I added here is a constant, copying constant, that's going to be real, and soon it's going to be small. Since we want to say something about rigorous results, we have to be in a certain asymptotic regime in order to prove anything. For time-dependent Hamilton. For time-dependent Hamiltonians, there are basically no other way. So, but before doing that, let me just try to say something about the spectrum of this operator. So, if I plot the spectrum horizontally, and if the coupling is equal to zero, then the spectrum is given by the addition of the spectra of the two pieces that I have. So, that's the continuous spectrum of my photon. And then I add the eigenvalues corresponding to the atoms. But now, if the eigenvalues, I'm sorry, if the copper I'm sorry, if the coupling becomes non-zero and even small, well, this picture is not true anymore because the eigenvalues tend to disappear, turn to resonances, and then you have continuous spectrum. You may have a ground state depending on the properties of the form factor and everything, but this is typically the spectrum that you get for any fixed time t for a Hamiltonian. Okay? Just ask me if things are not too new. Things are not too clear. Okay, and now the time-dependent Schrodinger equation that I want to study allows me to introduce a second small parameter in my model, which is this parameter epsilon, that's supposed to be small and non-zero, which describes the adiabatic limit. So I'm assuming here that my Hamiltonian is time-dependent, but it varies very slowly in time. And mathematically speaking, one way to deal with that is to introduce. To deal with that is to introduce this small parameter, which is the inverse time scale of your Hamiltonian. That is the time scale it takes for your Hamiltonian to vary in an order one fashion, if you want. And what you do is you change the time. You introduce the microscopic time, which is the physical time multiplied by this inverse point k. That's my variable t, which is here. And therefore, the small parameter jumped in front of the time derivative over there. Time derivative order. Okay? And this is how it's done in Messiah. If you're not familiar with that, you could look at it in that book. And the point is that in the adiabatic limit, you can actually control the dynamics. Basically, there is a slogan which says that if you start in an eigenstate of your system at time t of zero, then you will follow basically the eigenstate by continuing. Eigenstate by continuity in time. But it requires certain spectral hypotheses, and typically the Hamiltonian we have will not give you enough information on the spectrum so that you can apply this. So we cannot bluntly apply this adiabatic approximation in order to deal with this question. As I said, we want to do quantum control and see how the environment messes up the picture that we have. Okay? So it depends on the properties of the form factor. It might be indeed the ground state might stay there. That's what you would say. But not the excitations. Okay, so by construction, and this is something you can check with what we had, the one excitation sector, which is given by linear combinations of excited states in the atom part and the vacuum in the field part. In the field part, plus the ground state in the atomic part, and one photon characterized by a function f in L2. I'm going to call this f the photon, which is not really okay, but nevertheless, this set of vectors is preserved under the dynamics. So we can rephrase the full dynamics into that sector, and that's what I'm going to do in the rest of this slide. So there is nothing but rewriting things, it's there. So I'm So I'm going to look for solutions which are given by, which are characterized now by these factors zj, which now become time dependent. They will give you the probability if you square the modulus to be in the excited state at time zero, say. And you have another piece which is time dependent, which is that function that describes the photon that has been emitted in the process. Alright, so I have to. Alright, so I have to introduce certain objects that describe my evolution equation. So I introduce the matrix A, which is just this restriction of my full atomic matrix to the excited state, because I'm dealing with this time independent basis with indices running from one to D, so not the ground state here. So this is the piece that describes the excitations, if you want. That describes the excitations, if you want. The excitations, so the Zj's, I put them collectively in a vector, and the interaction is now governed by a vector which I call W of T here, that's my interaction or coupling vector, which is constructed by means of these amplitudes which tell me about how the coupling between the photon and the excitation takes place, the amplitude of this thing. And there is And there is a sum which is there, which has to do with the fact that I'm expressing things in this basis which is time-independent, whereas in the case before where I introduced my coupling, I used the basis which was time-dependent. But nevertheless, this is information about the coupling, nothing else. Fine. And now I can rewrite the Schrodinger equation, and here it is. It looks simpler than it was before. If you say lambda is If you say lambda is equal to zero, you have just the evolution of the excited state. So that's the pure adiabatic problem for your excitation in your atom. And on the photonic part, you just have the evolution, the free evolution of your photon, which is there. And here is the effect of the coupling that involves the form factor G. And this is the photonic part at time t, and vice versa, you have the effect of the Of the excitation on the evolution of the photon. Okay, so that's the quantity we want to study in the limit where lambda is small and epsilon is small as well. So we have two small parameters and we want to make sure not to couple them right away and see if we can distinguish different regimes. That's the question we have. Alright, so in order to state the result, I have to introduce another important player, which is the correlation function. That's this function. That's this function of t, which describes the scalar product, if you want, of the form factor and the evolution of the form factor under the free photonic field, if you want. That's one way of expressing it. In terms of an integral, this is how it looks like. And what we want to request is that this function, as a function of time, together with its derivative, decays fast enough. I don't want to tell you exactly how fast, but like what say. But, like, one say as t goes to infinity, if it goes like one over t to the fourth, it's fine, together with the derivative. So, we have more details about that, but it doesn't really matter for now. And this is important because it says that your system is such that the field really behaves like a reservoir somehow. The excitations go away, they don't come back and mess up with the dynamics of the small system. So, it's important to have this condition, as you all know. One thing to As you all know. One thing to note is that by construction, this function turns out to have a Fourier transform which is positive. That's because the function itself has specific properties being written this way. Okay, so I'm noting that because it's going to appear later on in the expression. And this is the result that we have on the atoms dynamics. So on the excitations. I'm interested in what's happening in the atomic parts. Part. So, under the hypothesis that I loosely mentioned before, the gap hypothesis, the regularity, and the decay hypothesis, we can prove the following results. So, let's forget about the first line for the moment. The result says the following. The excitation vectors, so the state of your atomic part of your system, say, is approximated by an expression which I'm going to walk you through in a minute. Going to walk you through in a minute, up to an error, and this error will be small when epsilon is small, when lambda is small, and provided this term here is small as well. So here you see I cannot just do anything. I have to make sure that things are not too, I mean the coupling and the epsilon are not in certain regimes in order to make sure that this is really going to zero. That has to do with the fact that I'm, of course, doing some kind of perturbation. I'm of course doing some kind of perturbation theory and I stop at some point. So that's why I have a lambda to the fourth. That tells you where we stop somehow. And what is the approximation given by this theorem? It's the following. First of all, this result is true in case we start with an initial state that has all the excitation in the atom. Nothing in the field, but everything in the atom. So that's the condition which is there. And it says Is there. And it says that: okay, this vector is a sum of contribution which are spanned by vectors, which are the excitation vectors, phi j at time t. That has to do with the adiabatic approximation. We start at time 0 and we look at time t. So we see that we have up to complex coefficient, we have linear vectors, I mean linear combination of vectors, which are the excitations at time t. Time t. Now we have the initial condition for our excitation along the vector, the corresponding vector, but taken at time zero. So this one is what you put in your system. And then you have a first phase which is here. This phase here is expressed here. It's the Berry phase. So that's really reminiscent of the edge-value approximation. And then you have a factor which is more serious here. It's a modulus, it's a factor which has a modulus smaller than one. Has a module smaller than one. So you start seeing the effect of the field. It's going to make this excitation amplitude go down. And the amplitude you get is an integral of something, and the something is computed explicitly in terms of the Fourier transform of this correlation function I mentioned before that you take at the eigenvalue you're considering at time t. And you have the coupling strength which appears here, and then you have this factor. Here, and then you have this factor. This is just to tell you that we can compute all these things in principle from the Hamiltonian and the correlation function. And then there is another piece, which is a phase, which you maybe don't care about if you look at the amplitudes of these quantities, but just because it matters here, you have a phase which appears in the adiabatic approximation, that's the so called dynamical phase, and then you have uh another piece of the phase which is uh uh which the the field is responsible for. Which the field is responsible for because it has this factor lambda 2 in front. And again, this is something you can compute in terms of these quantities we have before. Okay? Sorry, what is z of zero? What if you say where z to z of zero? Oh, that's the initial condition in my atom. My atom is such that initially I have a linear combination of excitations along these vectors phi j of zero at the vectors and z j of z. And the vectors. The components. So if you start, say, with excitation number one and nothing, you will just have one element here and you will have a one there. All right. So as you can imagine, once you have a formula like that, you can worry about different regimes. So this is the contents of the next slide. And I'm going to just go quickly through that and say that there are three. And say that there are three main regimes. The first one is the easiest one. That's the case where the coupling is very small. Actually, zero could be fine as well. By small, I mean lambda squared is much smaller asymptotically than epsilon. So the first sub-regime is when lambda is actually even smaller than epsilon itself. In that case, the approximation I gave you, I swear, gives you the following. It's just the pure adiabatic approximation. If you want to control your system. If you want to control your system using adibiotics, that's exactly what you have. And then the effect of the field does not draw the picture at this level, because the coupling is not strong enough. Next sub-regime, well, lambda squared is still small than epsilon, but lambda squared is larger asymptotically than epsilon squared. Now you start seeing things that come from the effect of the coupling. So this is the leading order that you got by your antibiotic. That you got by your antibiotic control. And you see that the first correction to this thing, which is still vanishing in this limit here, is of this form. And then you have a remainder which is smaller than this quantity here. So that's adiabatic or quasi-adiabatic dynamics. And if you want to start seeing something which does not vanish, you take lambda squared to be equal to epsilon, of the same order, and then this purely AD. And then this purely adiabatic approximation becomes wrong by a factor of order one, which is just this decay in the amplitude that now does not go to zero, I mean does not go to one as a correction, but has something which is of order one. Same thing for the phase here, it's still of order one. And then finally, you have the strong coupling regime in which everything goes down in the field exponentially fast, actually. Fast actually in the factor lambda 2 divided by epsilon, which is supposed to be large. And then everything goes to the field very quickly, so the excitation disappears and you're done. So what you can see with this analysis is that we have a full picture somehow of the effect of the environment on the coupling, on the control, the adiabatic control of our model, if you wish. Is that okay? Is that okay? So uh how much time do I have? Roughly? Five is twenty minutes? You've got five minutes. Okay. So let me try now to okay, to give you a word about the approach and that will lead us to this Markovianity thing. So we do what everybody does. You solve the equation for the photonic part and you put it up in the excitation part. You massage the equation. You use the fact that you have two small You use the fact that you have two small parameters, and voila, you get an approximate equation for z, approximate in the sense that this is a linear equation which we all like, plus a remain a term that you can bound and which is small when lambda is small and epsilon is small. And the piece which appears there is not a self-adjoint operator anymore because you have the effect of the field and it turns out to be a perturbation of the generator for the excitement. Generator for the excitation part, which is self-adjoint. It is about lambda squared, which is what you expect. And it's nicely represented by a rank one operator here, which is made up of this vector w of t, which contains the amplitude of the couplings, as I said before. It's really determined by the coupling and the eigenvectors of my Hamiltonian. And you multiply it by this vector gamma epsilon of t, which looks like this Fourier trend. I mean, This Fourier transform, I mean, yeah, which looks like the Fourier transform, if you want, of this gamma function, but against the different eigenvalues of the operator, because it's the of your self-adjoint operator here, it's the exponential of this a of t that you multiply by x, and you integrate over x, multiplying out by the gamma correlation function. We don't care about the form somehow, but it's related to the Fourier transform that I mentioned before was positive, and this is important in the sense that. And this is important in the sense that it tells me a little bit what I can expect if I adopt this as the approximation for my dynamics. Because if I look in perturbation theory for small lambda and I look at the spectrum of this guy, making use, so for lambda is equal to zero, I just have the pure eigenvalues of my A matrix. And then if lambda becomes positive, what appears here is the effect of this perturbation here. Is the effect of this perturbation here, which turns these eigenvalues to resonances if you want? It turns out that all the imagery parts go down, and that has to do with the fact that the gamma function has a Fourier transform which is quasitative. So it looks like we have something that describes the decay of this excitation vector, as I mentioned before. So, so far, so good, and it's kind of elegant, I would say. I like this operator a lot. This operator a lot. Fine, and then so this is the expression of these eigenvalues. These eigenvalues give rise to the quantities that I mentioned before. And in order to get the result, then we eventually have to come up with an approximate solution to that approximation as well. And this entails the control of the the effective propagator that solves this equation with this generator here. You don't want to see the details. You don't want to see the details. Sure. Okay? Right. So, okay, so that's the approach. And now, the last question about Markovianity. We can look at the reduced density matrix on the atomic part in this one excitation sector. And by definition, what we get is the atomic density matrix, which has a block diagonal part if you look at the excitation and the ground state. At the excitation and the ground state decomposition of your Hilbert space. It's going to be given by this rank one or pure state part in the excitation. And down there in the ground state, you put whatever you need to have a trace one operator. And in particular, initially, we started with that expression there. And I'm referring to this, there is a review, a very recent review, about this way of constructing things and different models. And this is one of them which appears. So now what we have. So now, what we have on top of that here is the fact that Z is actually almost equal to the propagator that I mentioned before, satisfying an equation of this type. And I could say, okay, let's forget about the error term. And I assume that Z of T is given by precisely this propagator. And that's my effective evolution. And let's see what it says about the dynamical map that I can put in order to describe this. So I wanted to analyze a map which the Analyze a map which describes when it acts on such an initial condition exactly the result that I have here, assuming that this is an exact relation. Well, this is what you need to do. I mean, if you take that seriously and you act with this map on this initial state, you get to that one. So that's the right way of developing. At least that's a way to define the dynamical map which reproduces what we want. This is what we want. Fine. Now we look at the evolution equation for this dynamical map, which we can deduce, of course, because we know the evolution of the piece which depends on time, which is there. That's this non-self-adjoint evolution we had before. Okay? And then we can express everything in terms of the generator of this effective evolution for our excitations. And what we do is And what we do is the standard thing: you take the generator, you separate the read part and the imagery part. I give it different names. So the read part is minus Q, the imagery part is minus IH. Q and H are self-adjoint. And if you take into account the spectral decomposition of this read part Q, the eigenvalues are QJ, the eigenvectors are F, J, F, T, you can come up with vectors, I mean with operators, rank one operators, with this. Rank one operators with these vectors, eigenvectors, and the ground state vector we had before. And it turns out that the generator of the dynamics of this dynamical map has the nice form of a Lindbladian, where you see the standard commutator with H, and then you have a sum in the dissipator of rates, which are supposed to be positive if you are a Lindbladian, and then you have the standard form of the dissipator part, which are explicitly computed in terms of everything. Explicitly computed in terms of everything. Okay, and then you go to this review that I mentioned, and you will see that the CPTP, I mean, the dynamical map is CPTP, if and only if these rates are positive, meaning that this operator has to be a positive operator. And that's equivalent, if you prefer to look at the cross-map representation, to saying that u star u is between 0 and 1. Okay, that's the same thing. And now you say, okay, Thing. And now you say, okay, what happens to our case? We had this nice evolution. I mean, it's a matter of taste, I agree, but I like it. So we have this nice evolution, which is the thing that we started off. And you say, okay, good. What if I look at the operator Q corresponding to it? It's exactly this piece which is there. And this Q, in case you have more than one excitation, is never a positive operator because it's a rank two operator. Because it's a rank 2 operator, and unless this vector here is an eigenvector of this guy, this will have no sign. You will have zero eigenvalue, then you have a positive and a negative eigenvalue. So you cannot say that the dynamical map, if it's generated by this unitary propag, I mean this propagated, which is precisely not unitary, but generated by this function here, you cannot use that to define this CPTP map. So what about the approximation? So, what about the approximation we have? Don't worry, as I said, we had to massage a little bit more the evolution that was generated by this in order to get our approximation. And indeed, if you just look at what we did, you will see that the true generator of the approximation that I wrote as a sum of different quantities turns out to be a bit messier than the one we had before. The one we had before. It's related to it, but it's not exactly it. And with this guy, one actually can show that the read part, the corresponding part, the Q of T, is positive. And therefore, the approximation we have, not only is it a good approximation when epsilon and lambda go to zero, but it's also given by a dynamical map which turns out to be completely positive and trace-preserving. So that's the end, and I thank you. That's the end, and I thank you for your attention. Thank you. Questions? I have one. Oh, go ahead. May I ask? Thank you very much for the very interesting talk, I feel well the first question is what about Mercurial? Is it so the CPTP? Yes, but So the CPGP, yes, but is it for example CPG visible correspondent dynamics? Well, this is the definition, if I'm not mistaken, you will tell me, right? This is, I mean, this lambda Ts, the way it's defined, turns out to be divisible for sure. That's because it's driven by an operator, and then you know everything is fine. It's a property. Everything is fine. It's a propagator. It has the propagation property. And now the only thing which remains is to know if for any time Ts you do have a CPTP map. And this is ensured by the fact that this is really a Linlad operator. So this is the affimotyp here. And what about the next question is related to my talk on Friday. My talk on Friday, what about the rates gamma from T for large times? Do they converge to some constant values, for example? So can we hope for a semi-group description on the symptoms, semi-group description of Larslins? Or did you consider such a question? So what we did is we considered the case where the generator would, where actually the Hamiltonian would be time independent. So it boils down. Independent. So it boils down to this situation. And indeed, we are able to control the dynamics. So there is no adiabatics there, there's no point. So it's large time asymptotics and it's only small coupling that you could. So we are able to control things for large times and small couplings. And therefore, what we get is that if you massage the equation enough, you do have something which is again CP. Which is again a CPTP map, which is now given as a semi-group. It's really a function, this lambda, it's a function of t minus s in that case. So we do reproduce the equivalent situation when we have a time-independent generator equivalent in the realm of semigroups there. But only for large times. So, in general, the dynamics is not Lacorian. The exact dynamics, so the exact dynamics. No, no, the exact dynamics is not. Exactly, just like here, the exact dynamics is not. But within a certain approximation, if you agree to give up on the corrections and you keep the approximation, then you may or may not have something which is Markovian. And it turns out that, depending on what you do, you do have something which is Markovian or you don't. We have both cases. For the time-independent case, we see that it's not Markovian. We see that it's not Markovian if we adopt a certain approximation, or I mean a certain way to approximate things. And if we do further approximations, we do have another approximation, which is still good for large times and lambda small, but which now turns out to be Markovian. So it's really dependent on how you define your approximation, where you start defining things. So the Markovianity, like the Dennis theory, follows from the second uh order perturbation in the upper. Or the perturbation in the other. Right, yes. Okay, and maybe a very general question. Can we call for a generalization of very powerful resonance theory for not small yanda, for alternative yanda? I'm not the right guy to ask. He's there in the audience. You should ask Marco. That's what he does. He doesn't leave hope right now, I'm afraid, yes. No, no, everything is protocolative here because we really want. Everything is projogative here because we insist on proving things. And as I said, if you have a system which is time-dependent, even the simplest 2x2 generator which is time-dependent, you have no clue how to have an approximation of the solution. So you have to say you're in an asymptotic regime or another. And if it's allowed, the very last question, your form factor is in the Is independent on the excitation on G. Is it a technical restriction or it's an important restriction? So, in your Hamiltonian, what if J depends on the sub-index J? Sorry, I'm not sure I got it. Where is it? The form factor is something which is given a priori. It's really in the model. Yes, I understand. But it's could you please show the Miltonian interaction with Miltonia? Come on. Yes, the other one, the previous one? The previous one, yes. Yes. Am I going in the right direction? No, of course not. Exactly. So yeah, exactly. Second line. So I want that you have D excitations. So phi1, phi2, and so on. But G is one for all. You generalize that G is all that we have its own G for every participation. G1 G one, G two and so on. J ideas. Yes, yes, you're absolutely right. We we in principle we we could make G depend on J. G depends on J, you're right. Of course, things would, I mean, the picture would be a bit different. I'm not totally sure we would be able to find something which is similar and as simple as what we got. Because despite what you think, it's simple. But I'm afraid I cannot guess just like that what the answer would be. I mean, some of the properties will change, but if Change, but in principle, the analysis-I mean, the way we approach it is something that you can adapt to that. The end result should look different, and maybe the details of the estimates might be more complicated because you have more items to deal with. But in principle, the model, indeed, you're right, should be generalized to that level of complexity. You're absolutely right. Do we have any other questions? Thank you very much. You were welcome. Any other questions? It's getting late in the day. One final quick one for me. So, if you end up with a Macovian limit, you just end up with a simple exponential decay of excited state population. Is that what you see? It depends in which model. If you really insist on having something which is time-dependent, as a function of time, you know, it depends what you put in. So, it's not very simple time-dependent. Dependent is I just get my exponential decay. So the exponential decay, you will not, and it's not going to be in the time factor, but rather in the time scale, the adiabatic time scale. So you get this exponential, as I said, maybe I should show because. If it wants to. Yes. You see the excitations, the way they go to zero, is governed by this factor which is here. Governed by this factor which is here, and the exponential decay is really in the factor lambda squared over epsilon. And this 1 over epsilon is the time scale, the adiba time scale. So it's more in that point of view that you should think of. Or you go to the time independent setup, in which case it's going to be the time which is there. And you would see this exponential decay in the corresponding regime. Okay. I think we're at the end of a long day. Let's thank all speakers of the day. Thank uh all speaker today. And next is dinner, right?