Francesco Di Lauro, who was a PhD student on this project. And then we had a postdoc called Jean-Charles Crash, who was poached by Amazon and left in the middle of the project to earn twice as much as I do, probably. And also another postdoc, Tania Zeraner, who recently also left to work on sort of more statistical and medical problems. But overall, we were very lucky, a very good team. So I'm going to try and I'm going to try and sort of make a joke. I'm not going to be as successful as Rick, but my presentation will not have renewal equations, nor COVID, nor big data. So I feel like an imposter. But I can say that there is going to be some Bayesian inference, some networks, and an all-direction of machine learning. So I hope that that's kind of enough to tease you to maybe pay attention to what I'm saying. Attention to what I'm saying. So, overview of my talks. I'm going to start with some motivation or introduction to the problem. And then I'm going to talk about probably the most sort of daring approximation that we try to pull off in this project. And that's to approximate an SIS epidemic on certain families of networks as a simple burst-ender process. And then I'm going to show you how that led on. And I'm going to show you how that led on to a biogen inference framework, which basically allows us to look at epidemic data at the population level, but allows us to actually say something about the underlying class of networks on which these epidemics have happened. And then also how this method then can be extended to try and predict epidemics, so epidemics which were only sort of partially observed, and then finally some conclusions. And then finally, some conclusions. Do feel free to interrupt me at any point in time. So at some point, I sort of got very interested in this problem of, you know, we know that the structure of the network is very important for how the epidemic spreads on the network. So then I thought that if this is the case, then it must be also true that if you are observing an epidemic on a network, so for example, the number of Number of infected cases every day. So it could be new cases or it could be the prevalence every day. Then that information surely should hold some important information about the network itself. And then we started looking around the literature and then we noticed that there are sorts of different ways of going about this problem. So some people were really ambitious and they were trying to go to the micro level and literally try to infer each and every link. Each and every link of the network. And in this situation, they have used so-called cascade models. So basically, you start effectively like an epidemic in an ode and then you let it run for a while and then you follow it until maybe the cascade dies out. But the disadvantage of this is that you need a huge amount of data that basically covers almost each and every node and each and every link to have a chance of being able to recover the links in the network. And recently, The network. And recent work also kind of hinted to the fact that this problem, in many cases, in some sense, it's ill-posed or there are serious identifiability issues. Now, there are some other ways to think about network inference, and maybe we can say something about the structure of the network. So it's kind of mesoscopic, not locally, not globally, but maybe things like communities and clustering, or we can be. Or we can be sort of a little bit even more loose, if you like, and then we can think about, you know, how about if we can just infer maybe the average degree in the network, or we can say something about the network type or family. And that's what I'm going to focus on. So basically, I'm going to show you how we can look at epidemic data at the population level, and then we use that to be able to say something about the underlying network, about the family of the underlying network. Work about the family of the underlying network. So, I mean, of course, this is a very complicated problem, and often full recovery of the network is not needed. And as I said here, as I hinted to it, our aim here is to infer the family of the network. And we are going to focus on three classes or families. So, the regular network, Berdus Reigny, and the Borobashiolbert. And basically, our aim is to say, okay. And basically, our aim is to say: okay, so if I have some output from an epidemic that happens on such a graph, so you can see here an example. So, for example, I have dailies or some kind of equidistant prevalence information. Can I indeed identify whether the underlying graph was regular, Erdős Rainy, or Borobashi Albert? And we only use discrete observation, and this is again at population level. And of course, we have seen a lot of We have seen a lot of very good talks talking about, you know, how we can, when we have, for example, stochastic models, then the likelihood is very clear. And then when we have deterministic, then the situation is a bit problematic. And here also, if we think about the epidemic model as a continuous time Markov chain, then immediately we have a dimensionality problem. So we have a Markov chain on a two-to-the-n-dimensional system because every node can be susceptible and infected. Can be susceptible and infected, then obviously that's gonna become a major challenge straight away. So, we need to deal with that somehow. And then, the way to deal with that, what we suggested, is to introduce this birth and death approximation. So, basically, the two to the n is going to be brought down to n plus one because we are going to argue that it's possible only to focus on the number of infected individuals and we do not really care about where they are on the network. And all of this is done by using sort of SIS epidemics to keep things at. As epidemics to keep things a bit simple. So, initially, this was a fairly theoretical question and problem, if you like. So, I wanted to emphasize here that we only look at what was the result of the epidemic unfolding on the network. So, we are simply looking at data at the population level. So, why birth and net approximations? Well, an SIS epidemic on a network, it's like a birth and net process, right? So, if we are looking at the number of infected nodes. Are you looking at the number of infected nodes at prime t, then that can either go up or it can go down, and hence the idea of you know looking at this as a birth and death process. And of course, in terms of the network, we have a two to the n dimensional model, and the two simple ingredients are the transmission rate tau across an SI-link. So, a node, a susceptible node connected to an infected node can be infected at rate tau, and then we have recovery rate gamma, which is in. And then we have recovery rate gamma, which is independent on the network. And as I said, this results in a Markov chain on 2 to the n dimension 2 to the n. And here at the top, you just see a sort of a realization of an epidemic, if you like. So here we can see an infection, and then we can see a recovery. So here we had an infection, here we had a recovery, and then is that right? So here we had an infection. No, sorry, infection and infection, and then we have a... Infection and infection, and then we have a recovery here. Okay, so I'm going to now move on and tell you that if we want to do the birth and death process approximation, we have a major challenge on our hands. And namely, the big question is what are the rates that we are going to use in this birth and death process? And in order to ease you into this discussion, let us think of the complete network. So let's assume that we have a network where everybody is connected to everybody else and we want Everybody else, and we want to write down the equation, then the Chapman programmer of equations follow straight away. And we can write down the probability of observing k infectious nodes at time t. And then obviously, this is an up jump from the rate of jumping up. So generating a new infection when we have k minus one infected individuals. This is the recovery, and this is just leaving the current state. And for a fully connected graph, this AK. Graph, this AK, the AK rates can be easily calculated because the infection pressure is simply the product between the number of infected, the number of susceptible, and then the per contact transmission rate. And that's why the fully connected network, modeling the fully connected network is so easy. So AK is known, and we have an exact master equation. And of course, for the fully connected network, we only need equations over 0, 1, 2, and up. Over 0, 1, 2, and up to full network size. So we don't need to to the end because the location, the position of the affected nodes on the graph does not matter because nodes are topologically equivalent. Okay, so the big question then, what is AK in the general case? So if now we have a network, let's say an Erdős Reigny or a regular or a Borabascholbert, what can we say or how can we capture this rate somehow? So we were thinking to sort of initially that this can be done only by some tricks or by a little bit of cheating. And then we thought to ourselves, you know, why don't we try and learn these rates? So of course we cannot write down analytical expression for the rates, but why don't we study them a little bit computationally and see how do they look and what can we say about them? And there are certain observations. There are certain observations that are very important to make. So, obviously, the number of SI links is a random variable itself. And here I'm showing samples. So, K1 and K37 means that we have one infectious node and then we have 37 infectious nodes and then 73 infection nodes. And this distribution is basically the number of SI links that you can experience by running the same simulation over and over. The same simulation over and again on the same network or on different realizations of the network. So, this SI, so the SI count when you have K-infectious nodes in the system, so this itself is a random variable and it has a sort of very interesting structure, and this is sort of easy to understand. And more than that, these variables are also kind of correlated, if you like, because if you are in a situation where right now you have lots of asylum. You have lots of assigning, then in the next stage, of course, this count is likely to stay close, so it's going to stay high, maybe going to grow even higher or just decrease by very little. So try to kind of keep that in mind, that the S actually is quite an interesting correlated random variable. So, how can we learn the rates? So, we choose a network family and we choose some network parameters, tau and gamma, and then we run lots of GRSP simulation and we try to count. And we try to count what is this rate, the infection rate on the graph. And it turns out that the best way to count this is what we call a time-weighted counting. So it's not enough to simply count and, for example, take the average that you see in the distribution. So of course, the first tendency would be: if I want to say how much is my rate when I have 73 infectious individuals, let's take the mean of this distribution. Let's take the mean of this distribution, but that tells us not to be the right thing to do, but rather we have to do a time-weighted average. We have to take into account how much time a Markov chain spends in a certain state, and therefore, this time-weighted average is going to be the correct thing to use. Now, of course, we can rely on the theory of Markov chains and we could also find these rates differently using sort of maximum likelihood principles, and we could simply count the. Principles, and we could simply count the number of up jumps and down jumps in the presence of k-infectious nodes. But again, this approximation, which is a little bit empirical, seems to be much more stable. And here I'm just give you an example of how the simulation would look. So here you have some times just using Gillespie simulation, counts, number of infected, number of SI links, time spent in those states. And then you can, if you want, you can count the up jumps, you can count the down jumps, and you can even compare this. Even compare this empirical estimation of the rates to that coming from the theory of Markov chains. Now, so what is the upshot of this and what do we observe? And the first important thing that we observe is that, of course, these AK rates are going to look like a parabola, which is not surprising. So, when you have very few infectious individuals, the number of SY-links by definition is low. And the same is true. The converse is true when we have lots of infectious guys. Lots of infectious guys, and then the optimum is reached somewhere in the middle. But what is interesting to note about these curves is that different network families produce very different curves. So, what you can see here, that for example, the Boroba-Scholbert, in the case of highly heterogeneous graphs, then this AK is shifted to the left. And then you can see that for the regular and for the Erdős Scheny, Erdős Reigny seems to be shifted a little bit, but regular seems to be shifted a little bit to the right. seems to be shifted a little bit to the right and then the regular seems to be the adjustment seems to be to be the most central one and then when we saw this we were really happy because finally we are seeing that actually the structure of the network manifests itself somehow in this uh in these measurements that we are taking from the network but now you can ask yourself okay so um so what now so you are generating lots of networks you are simulating lots of things You are simulating lots of things on the network. You are looking at this curves, but how can this be taken forward? And then the way how we took it forward, well, actually, that's going to be in the next slide. So let me just go back here a little bit. So once we have this numerically computed rates, of course, we can ask ourselves, is this actually a good approximation? So if I now take this numerically, So, if I now take these numerically computed rates and I'm going to, let's say, plug them in, put this little head to indicate that this is an approximation, then if I put these numerically approximated infection rates in the master equation as I would like it, then how would the average of many simulation compare to the average that we can get from this approximate model? And as you can see on these graphs, in many, many cases, the agreement is extremely good between this approximation and the true simulations. Approximation and the true simulations. And here there is a little bit, there is a little more sort of detailed analysis. So we look at different starting conditions and for different graphs. And you can see that overall, so this is the difference between the Gillespie simulation and this approximation. You can see the differences typically are very small and this system is behaving really well. So this is quite promising. And here the network was nodes of a Nodes of a thousand. So obviously, 10 out of 1,000 here, that's pretty good. That's basically 1%. And again, I'm emphasizing that these parabolas are different for different network families. Okay, so as I hinted already, so just looking at the parabolas alone is not very useful, but then what we decided to do is to propose a parametric model. And the parametric model had some very nice motivation. Nice motivation of how to come up with it. So the K and the N minus K, we wanted to keep that because that comes from the fully connected network. But then we have seen that some of these parabolas are more, some parabolas are more peaked than others. So therefore, we have introduced this power P. So this can control the flatness of the curve or the peakiness of the curve. Of course, we need some scaling parameter because if we do, you know, higher infection rate. Um, you know, higher infection rates or lower gammas and so on, then these parabolas are going to sort of shift up and right. And we also saw that these parabolas can shift to the left or to the right depending on the network type. So therefore, we introduced a further parameter, alpha, which exactly controls the skewness. So we have this Q, we have the flatness or peakiness, and finally we scale the parabola. And then what we have done is we have taken all this simulation and all these measurements that we have This simulation and all these measurements that we have taken from all these many networks and different parameter combinations, and we fitted this CP call it CP alpha model to the data. And then we have plotted the resulting parameters, so P, C P and alpha, in this three-dimensional plot. And we were extremely happy when we saw the plot. And I hope that you will agree, especially people who kind of do clustering and machine learning, that this looks very much like an open. Machine learning: that this looks very much like an optimal or ideal clustering plot because these networks seem to be in different parts of the parameter space. So, here I have the regular, here I have the Ardio-Shrani, and here I have the Boroba-Scholbert. And basically, this gives us hope that if now I have an epidemic happening and I try to infer, I set up an inference framework, and I'm trying to infer what C alpha PR, I can look at this table or I can look. I can look at this table, or I can look at this plot and use it as a lookup table and try to identify what the underlying network was. And indeed, this is what we have done. Now, of course, here between Adios Chenia and regular, as I'm going to show later, the boundary is not that clear because, of course, if you make the network denser, then these two clouds will eventually merge. But nevertheless, these sort of plots, when we had them in this way, they looked very promising. So, as I mentioned, we are going to use this to set up an inference framework. And actually, what we are going to do is we are going to do a Bayesian framework and we are going to use this numerically computed point clouds as priors. So, we are going to do some sort of further smoothing on this, but we are going to use them as priors. And then, the idea is that using these priors, we are going to set up an inference framework where we look at population-level data and we try to infer the best C. Infer the best Cp alpha points that makes the model closest to the data. So, how do we go about this? So, we have some data in terms of equidistant data on epidemic realization. And here, one important point is that we take data across the whole epidemic. So, up to sort of quasi-statistic, if you like. So, we sample the whole epidemic. And then, later on, when we talk about epidemic prediction, And then later on, when we talk about epidemic prediction, I'm going to talk also about the case where we don't take data across the whole epidemic. But for example, maybe we only take data up to this point, and we are going to talk about what are the implications of those. But for this network classification problem right now, we are going to take data across the whole epidemic curve. So let me briefly tell you how we go about this. So the likelihood is very natural because we have a Markov chain. So we have a Markov chain. So we have a Markov chain, so we can obviously numerically write down the likelihood, and the likelihood is going to be just the product of the jumps. So going from, you know, ki minus one, so this was a count at time i minus one, ti minus one, and this is the count at time ti. And simply we are just writing now what's the probability that the Markov chain will jump from ki minus one to ki in time ti minus ti minus one. So there's the likelihood, and then Likelihood, and then the priors I already hinted. So, we use some Gaussian kernel density estimator to make those clouds much, much smoother. And then, basically, we use those as priors, and then we use the Bayesian framework to come up with the posterior. And the posterior basically is going to tell us what is the probability of observing the various network families given the data that is given to us. And on kind of paper, this seems very easy, but computationally, there were many challenges. Computationally, there were many challenges which I found out from my postdoc. Most of the work on this was done by Jean-Chas, but actually, computing this likelihood apparently is not simple at all. But conceptually, the idea is very simple. So, let's look at some results. So, basically, what we do now is we run epidemics on networks of different types. So, here we have Erdős Reigny, here we have regular, and here we have Borobas-Jalbert. And basically, what And basically, what these so basically each of these vertical bars represents the posterior when fitted to one single simulation. And this is the posterior. So these are basically the sum of three probabilities. So here you can see that the 60%, with 60% probability, this data came from an Erdős Reigny, and with 40% come from a regular, Borobash Yolbert doesn't even show up. Now, when we test here. Now, when we test here for the regular, then you can see that most of these columns are orange, which basically means that in the posterior, the regular network dominates, energy strain occasionally creeps in. And finally, in the Borabashial bottom, most of these columns are green, which basically means that the posterior points towards the correct network. So, on average, we had no misclassification. And when we look at the confusion matrix, then this also on the Matrix, then this also on the main diagonal, we have very high, very high values. But as you would expect, there is a little bit of confusion between regular. So, for example, the true label is Erdős Scheny, and occasionally this can be interpreted as regular. And sometimes the regular can be interpreted as Erdős Sheny, but again, this is not surprising because obviously the networks are denser and denser, then obviously Erdogani and regular can look quite similar. Okay, so basically, the kind of takeaway message from here is that there is hope in some sense that we can just look at population level data and with some appropriate methodology, we should be able to say something about the graph. And of course, here we are talking about regular Erdős Renia and Boroba-Schalbert. And maybe retrospectively, actually, what we are detecting here is degree heterogeneity, right? So basically, the more the position. Basically, the more the posterior points towards, let's say, Borabas, Schalbert, or Erdős Scheney, that means that the underlying network has heterogeneity, which then, of course, can be exploited for control and for various other things. So when we finished this piece of work, we started wondering what would happen now if we could try to use this framework to observe an epidemic, let's say only partially, and then try to predict sort of. To try to predict, sort of, you know, I think it's called forecasting, right? Try to forecast a little bit as to what's going to happen in the future. And the framework is very much the same. So we have some data as you have seen it before. The only difference now being that we are not going to explore the whole epidemic, but maybe data is going to be limited, let's say, to this point in time or even to fewer points. And actually, we are going to look at what is the impact of how much of the Impact of how much of the epidemic we are observing. But the kind of framework is going to be more or less the same. So, again, we are going to have some priors that we have learned already for the three different networks. And again, the idea is to then compute the posterior and to find the correct network class. But of course, here there is an extra complication because. Extra complication because once we have found the network family and we have found those key parameters, the C, alpha, and the P, then if we want to produce predictions, then we need to go back to the master equation. We need to go back to this PK T equation, and then we need to worry about these rates. So we had here PK minus one over T, and I'm going to stop writing now. But the idea is that But the idea is that if we have a network class and some associated parameters, then what we need to do is we have to take this CAαP, we have to fit it in our parametric model to work out what our rates are. And then these rates then have to be fed into the master equation in order to be able to then produce some kind of prediction, let's say, going forward from this point, right? So the process is a little bit more complicated because. Little bit more complicated because we need to use the sort of push-forward operator, if you like, after we have the parameters, then we need to solve the forward model in order to produce the prediction. So, mathematically, just a little bit to summarize, maybe for clarity. So, this is going to work like this. So, again, these are our key parameters. And then this is our network class. And posteriors are going to be. And posteriors are going to be uninformative. Sorry, the priors are going to be uninformative. So we can say that all three networks initially are equally likely because we have no idea about what the underlying network is. And then we can play around with the posterior in various different ways. So we can have a network class-specific posterior for you. So basically, if we say that we know what the network class is, then we can find the posterior. Then we can find the posterior for the most likely CαP values, or we can literally integrate these parameters out, and then we can find an overall posterior for the network class. And by playing with combinations of these, then we can produce our credible intervals. So, again, as I said before, once I have drawn some samples from the posterior that I agreed on, then I need to build my rates. And let's call this AK hat. My rates, and let's call this ak hat. And then I need to compute pk. So I need to produce my probabilities of observing k infectious individuals at time t, and then I need to use this to produce the credible intervals going forward. Okay, so let me show you some results. So this is very much a pictorial representation of what I showed you on this slide. So what you see here. So what you see here, so imagine that you have a posterior and you are choosing C alpha P's from the posterior. Then you produce by the parametric model your rates and then you solve PK. And basically the gray lines here, these are PK profiles, right? But now, because we are in a Bayesian setting, the question arises: if we want to now produce a caddable interval, we can either Produce a credible interval, we can either go with something like the conditional mean. And in this situation, the conditional mean is the red line, which is basically just averaging over all the posterior distribution, or we can choose the most likely parameter value, and that's the map. And the same can happen if I assume that I know that there's an underlying Erdogany or there is a Borabas-Jalbert, then I can again look at the conditional mean and the Again, look at the conditional mean and the map. And then, if I have no knowledge of the networks before, before I started all the exercise, then I can weight all these approximations, which are network specific, to produce an overall approximation, which is no longer network-specific, but takes a view across all the three networks. So, basically, here the map, so the map that goes over all three is just basically a weighted average of all the dashed curves. Average of all the dashed curves here. And then the black and the black is the mean. The black is the mean of the continuous red, continuous blue, and continuous green. Okay, so there was quite a bit of sort of getting our head around it until we understand how to use this and how to generate sort of credible intervals out of this. So let's see what happens if. Let's see what happens if we base our prediction on conditional mean. So, what we are doing here, as you can see, that in the first case, we only run the epidemic up until we observe 160 infected individuals. So, basically, the data stops here, and here we observe 320 individuals, and then the estimation, so the data stops here. And then, what you see in the grade, and this is the sort of cell. And this is the sort of 70 and 90 percent. So the darker gray is the 90 percent credible intervals. And what you see in red is realization of the epidemic. So imagine that we stop taking data at 160, but then what we do is we continue to run the epidemic multiple times to generate a profile of how the true epidemic would look like. And then we are plotting that against our confidence intervals. Confidence intervals and the difference between these that these are different individual realizations of the epidemic. And as you can see, that overall the results are not bad. So typically the red sort of forward realizations of the epidemic sit within the gray cloud. Not always. And for some networks, they do better than others. Another important effect that we are observing, that the higher we go with the data, so the more data we take, So, the more data we take, then the credible interval narrows down because, of course, some of the uncertainty in the parameter and the network class decreases, and then you can observe these narrower bands. And the predictions, in many cases, sits nicely within the credible intervals. So, let's see what happens if we use the map. So, of course, with the map, as you would expect, we are going to get much narrower credible regions, and of course. Regions and of course, the map only accounts for stochasticity and uncertainty in the epidemic process itself because we have chosen one single value, which implies one network and one set of parameters. And as you can see here, again, overall good results, but of course, for some realizations, we have also mismatched. But statistically speaking, overall, our predictions are pretty good. Um are pretty good. So, conclusion: so we introduced the bulk net process approximation for SIS on networks. And here, I think there is scope for some theoretical results as well. So, I did not have time to talk about, but there is another aspect to this work: namely, that we have shown that there is a PD limit to these. To this burst and death process formulation. And here on this animation, you can see with the dashed red line is the solution of the PDE, and then the blue line is the histograms, temporal histograms taken from GSD simulation. And you can see that the agreement is very good. So maybe for those who are interested, I can talk about this late, you know, set up a separate meeting. But we also had this paper on the PD limit, and here we believe that there is some scope for some. We believe that there is some scope for some theoretical results, which we were not able to do in our group. So, we also proposed some rates. So, we proposed a way to measure the rates that led to network classification, which I think is quite nice. And I think that to me seems quite amazing that looking at population level data, we are able to say something about the underlying network. I mean, of course, in most cases, the data is. Work. I mean, of course, in most cases, the data is much, much more complex. And of course, this approach has its limitations, but I think it was a good experience to go through all this calculation and all this project and actually show that in many cases it is actually possible and that data has information indeed about the network as well. Now, in terms of extensions, I mean, of course, we can consider SIR, maybe real-world networks or networks with more complex structures. Or networks with more complex structures. So, I was wondering, for example, how would this infection rate look like if we have a network, let's say, with two very distinct communities? Like, would we have a parabola with two humps or something like that, like a bimodal distribution? I don't know. I haven't tried it. But probably I'm going to stop here. Thank you for your attention and put up this last slide with the three papers and some code on GitHub if you're interested. Thank you very much, and I welcome any questions. Questions. Thank you. Questions, comments, Joe. All right, so thank you, Isfan. I sort of had a question, and then you're I sort of had a question, and then your very last slide, you answered it a bit. But so, do you have thoughts on, or yeah, the slide before this, do you have thoughts on what happens in your bottom bullet, particularly for SIR and say, maybe something like the Small Worlds Network? How well do you think that you'd be able to do this sort of stuff? The honest answer is that I have not. Answer is that I have no idea because I haven't tried it. But I think it's possible. Of course, the complication is that the Markov chain would be on two dimensions and our lovely kind of parabola is going to be some kind of complicated manifold. And I'm not 100% sure how to handle that. So, yeah, so the answer is I think probably it would be good, but I don't. Probably it would be good, but I don't have any numerical evidence to back it up. Other questions? But that would also relate probably to your paper in some sense. No, Joel, that you had some nice paper on sort of spatial small world networks where you had some PD limits to them. Plausibly, yes. I need to think about. Um, I need to think about that myself quite a bit. Um, yeah, uh, I mean, I think it'd be so thinking about SIR would be quite interesting to me. I think, you know, I think that there might be potential to see kind of early on, you know, does this give us the ability to make some sort of projection on an SIR model? I guess, could you also comment on SIS if you had, say, a small worlds network? say a small worlds network or some other so the the only the the the only the only network that we tried it on is actually on lettuce and it works really well so i i i sneaked in two slides at the end because i said that maybe somebody's going to ask the question so so when we submitted the paper then the referees were a bit they said you know why don't you try some more networks and then we tried the lattice and we were quite surprised that that worked very well so maybe That worked very well. So, maybe this graph doesn't tell you much, but basically, what we had to prove is that these coefficients are density dependent. So, basically, in order to have a PD limit, you have these rates, these AK rates, and these have to be sort of density dependent, which basically means that AK, when scaled with N is only going to depend on the fraction of infected nodes. And then we prove numerically that there is that. That there is that these curves are indeed density dependent and they are universal. So when they are scaled properly, they all sort of lie on top of each other. So therefore, we believe that there must be a way of producing some sort of proof. But yeah, I can't do it myself. I don't think so. Okay. Any other questions? Any other comments or questions? Then I have a quick question that I tend to ask in these conferences. What's do you think, Istvan, the robustness of some of these machine learning approaches when you're distinguishing between different structures of the network? So if you add a little bit of noise to your data, how much it changes the How much it changes the ability to separate these different types of networks? I probably am not the kind of right person to answer this question because machine learning is something that sort of I'm trying to get my head around as well. We haven't, I mean, for example, in those classes that I showed you, we have used a reasonable number of, so basically, So basically, the way how we produce these clouds is that we produce Borabash shortpark networks with different degrees. For radio strain, of course, we produce networks with different p-values. For regular, again, you know, different degrees. And then on top of it, we vary tau and gamma. And then sort of, this is the kind of class that you ended up with. And at this point in time, the only thing that I can say is that there is definitely can be confusion between the Arduino and the regular, and that you can see it all. And that you can see it also already from simulations. So you can simulate an epidemic on an erdioshene and on a regular, and if you are allowed to play with the parameters, you might have different infection rates or recovery rates on the graphs, then you can actually make the evolution, the temporal number of infectives, so the prevalence, you can make it look very, very similar. I even had bets with my postdoc saying that, you know, just because Saying that you know, just because visually two things look very similar, it doesn't mean that the algorithm or the likelihood cannot distinguish between them. And I think that we haven't settled the question fully. I think that you can misclassify. But I don't have a quantitative to say that it's 10% or 15%. I think it's really parameter and situation dependent. Okay. Thank you. Okay, thank you. So, any other questions? I don't see at this time. So, let's thank once again, Istvan. So, thank you. And I believe this is the end of today's activities as far as the presentations go. So, we'll see you guys online audience tomorrow morning. Audience tomorrow morning at 9 a.m. local time. So thank you. And I hand over the microphone to the real organizer. So should we stop the recording? Okay, thanks. And I don't know if you're still logged in, Greg. It seems like. Greg, it seems like. So it seems like there were three potential options for people to do this afternoon. CC.