I'm very glad I'm invited for this presentation. I can hear lots of typing. Oh, thank you. Yeah, I'm a bit sad that I can't be there. I mean, I kind of missed the workshop or conference kind of. Conference kind of spirit now. And it would be nice to be there, but that's what it is. So, yeah, I'm going to do a presentation on default base vectors for testing precisions. And as the workshop is called foundations on objective-based statistics. And I do wonder sometimes, and I think this is something that I've learned from Ed and from Jim and from some of the people who are present there. Some of the people were present there as well. It's just what do we mean by objective base? And I try to elaborate that in the setting of testing. So, mostly base factors. And I first start with the problem of the, so it's, I try to be a little bit more concrete. Then I say something more general about objective base vectors. And I tell something about a criterion called across sample consistency. Called across sample consistency. So these are the first three sections, and those are kind of more philosophical. And then there's some results. I'll do something a little bit more mathematics, a little bit more mathematical, but I try not to do too much. And when I'm going to discuss across sample consistency, I would like to have your input on it as well. So I'm going to pose a question and hopefully you can think along. So the case sample position. So, the K sample precision testing problem. So, what are we talking about? We're talking about K samples, meaning K groups, basically independent groups. And each group is normally distributed with their own means and with their own precision. And the no hypothesis here is whether all pairs of precisions are the same or not. So, all precisions are the same or not, and K is fixed. And the alternative is there is a pair that. Is there is a pair that where the positions are not the same? And where do these, where does this come up? So, so in the null hypothesis is a kind of assumption that you need for the standard ANOVA. And it's also kind of thing that you can think of in a regression setup. So, in regression, you have this assumption of homogeneity of errors. And in more simple terms, you can think of measurement instruments and whether they have the same measurement. And whether they have the same measurement error. Okay, so just eyeballing the problem on its own. So you have in this setup, you have K is five groups, you have 30 data points here, 40 data points here, 10 data points, 20 data points, and 200 data points. And from the outset, you can't look at it and stare at it and say, yeah, these variances are all the same. I actually generate data from under the node that they are the same. So it's a couple of problems here that can make it difficult for instance. That can make it difficult, for instance, different sample sizes. And you know, well, the question is: can you get evidence most of the time when you're doing an ANOVA, you want to get evidence for the norm? Okay, so this is a classical problem. So there are some tests available in the frequency sense. There's the Spartan test, 1930 something, and 1960s, you've got the Levin's test, and more robust, you've got the Brown Forsyth test. The Brown falsified test. And all these tests have the same kind of flavor that they are chi-squared test or F-test. And what you do, you compute a p-value. And if the p-value is smaller than 0.05 or some other number, you just make a decision and say, reject the null hypothesis. And if you hope actually that you do not reject the null hypothesis, because then you can do when you're on Nova or you can just make some, you cannot destroy. This makes an you cannot destroy your assumption of homogeneity of errors. So, in the Bayes setup, you do something a little bit different. You look at base vectors and you compute a base vector. So, I denote D here with data, which is my kind of simple notation for that. And then you can, based on the base factor, you can calculate posterior model probabilities. So, the probability, the model probability, given the data that you know are possible. Data that your null hypothesis is true or the alternative hypothesis is true. And you can do uncertainty configuration, you can do model averaging. So it's all these things you can do. But you can also make a decision, but you don't have to. So that's kind of the setup. And now let's talk about objective base vectors in general. So what do we mean by that? So first, again, the setup in more general setting. So the fields. In a more general setting, so the field experts have some kind of prior knowledge or prior belief about the null hypothesis or the null model and alternative model. And we just deal with two models here. So just to keep the feel simple, you can apply Bayes theorem, and then you can see that you can update these posterior, these prior model probabilities to posterior model probabilities using the base factor. Okay, and we are in a nested setting, so. A nested setting, so that's what I'm going to talk about and define the base factor in that sense. So the base factor is a ratio of marginal likelihoods, and I most of the time write the null model at the bottom. So that's the point null restriction here. And at the top, you have more parameters. That's kind of the game that you play. And now, as a statistician and as an objectivesian, your goal is to find a pair of prior. And most of the time, we talk. Of prior, and most of the time we talk, we say prior, but it's actually always a pair of priors when you deal with base factors. And I always think of these priors as not as beliefs on the parameter, but really as a tool for statisticians to tweak and to play around with. And these base factors, they have interpretation of evidence, and they are very strict in the sense that if it's six, then it's six times one likely under the alternative. Or you can do it, flip the way around. Or you can do it, flip it all the way around, you get evidence for the null as well. Okay. And again, the game is just to find good pairs of priors, and from these pairs of priors, you construct the base vector. And you are restricted into a certain extent that you cannot use improper priors on the test-relevant parameters. That means that you cannot, in general, not use Jeffrey's transformation invariant priors. So it tells you that the parameterized. So, it tells you that the parameterization does matter. So, that's another conclusion you can take out of that. And you cannot use simple conjugate priors because they fail some certain designerata. And I'll go and tell a little bit more about these designerators. And I'm quite sparse with my references. So I'll just mention here most of these ideas are kind of old and they come from Jeffrey's, actually, almost 100 years ago. Years ago, and it culminated in this area. This time, and there's something called this book. It's like this: some massive thing, this theory of probability. And good review are here by Christian and Judith and Nicolas. And I dumped it down even further for psychologists to read more on this. And you can get some overview of it if you really want the dumped-down version. And Jeffrey's will. And Jeffries was very, very intuitive in some sense. And it's problematic because if you understand what he means, then it's very clear, very, very interesting, but it's very hard to figure out what it means. So throughout some time, you've got work by Susie, Jim, Annabel, Gonzalo. They really formalized these ideas what good base factors are, what these dramas are. And it was further extended by Enis Clark. Further extended by Lise Clyde. And I forgot her name, but she's nice too. So, what are good base factors? So, good base factors, we have certain type of behaviors, certain type of invariances, and those invariance are, in this case, so for this, if you think about the case sample problem, it should not matter whether I call the first group one, the second group two. One, a second group, two, third group, four, etc., or any other, I can label them whatever I like. It should not change my base vector, it would be very weird because my hypothesis is about whether they have the same precisions or not. It should also not matter to how they are measured. So if we do a study in the Netherlands, we use Celsius as temperature, but in the US, we might use Fahrenheit. And that's kind of annoying that if you have a base factor of seven here, You have a base vector of seven here in the Netherlands, but somebody else does a study in the US, all of a sudden they have points there, 0.007. It doesn't make sense, so you have to take that into account as well. Predictive matching is something that's important, but I will not say too much about it because it doesn't really appear and doesn't give much more conditions. So, just to repeat what I just said, because sometimes I forget to. Repeat what I just said because sometimes I forget stuff. So, the point of a statistician is just to find good pairs of priors, and using these conditions, you kind of restrict the space of all pairs of priors such that you end up with certain priors that work well to construct a base fix for that. I think so, these are invariant kinds of restrictions. And now you can look at consistency restrictions. And these consistent restrictions, this one is. This one is a famous one. It's kind of frequentist, and I just think it's very, very natural to have. Is that if you have case samples, this should be a capital K, if you have K samples and all your data, all your samples, all the sample size go to infinity and your data is actually generated under the null, then the base factor should go to zero in the sense that your posterior model probability for the null goes to one. All goes to one. And similarly, if the data comes from the alternative, you should gain evidence for the alternative and regardless of which prior you use, as long as they are super zero and one. So these are natural conditions that you can have. And there's something called a quad sample consistency. And that is particularly interesting in the case that you have multiple groups. And then you don't let all samples. Then you don't let all samples go to infinity, but just one, for instance. And I just want to discuss that in a bit. There's also something called information consistency that we'll see in a bit as well. And there's another thing that I think recently is very interesting for me is just think about what happens for base vectors on the sequential use. So you get data, you continuous update, because these base vectors are measures of evidence. Are measures of evidence. And I probably don't have enough time for that. So, but it's an interest that I have, and we can always talk about that later. So, send me a note back if you want to discuss it further. Okay, so we talk about a core sample consistency. So, this is kind of the setup in what we were discussing before. So, this concrete example with the precisions. So, the null hypothesis is all pairs of precisions are the same, or there exists a pair of Or there exists a pair of positions that is not the same. So suppose you have a base vector and you fix the first, the data sets of the first k minus one samples. So n1, two, and k minus one is all fixed. So you have the sufficient statistics also fixed, but you let the data of the last sample go to infinity. So now the question is, how should this thing behave? So this base factor that you have, how should Baseback that you have, how should it behave? And I would like some participation here. So I've got four options. But this base vector should go to zero or for the minus one divided by 12. It should be constant or infinities. I can't see your video. Is anybody going for A? Or can somebody? Or can somebody shout? Ed, what do you think? I think the answer is C. Okay. Anyone else for C as somebody does somebody would somebody go for A? P somebody go for D? Somebody go for D? Okay. So that is correct in the sense that, well, it's correct. You can debate about these things, whether it should or should not be the case that it should be C. But the thing is that I do think that it should be a constant because this base vector, this case sample base vector is about your info, it's about. Your information between your samples. If your sample, the last sample goes to infinity, then you know a lot about the precision, but you do not know, you do not have infinite information about that precision compared to the other positions. So that's kind of the setup. And there's this other problem that's kind of related, and it is the K-1 sample problem that in that. That in this case, you know a variance. Okay, so you know a variance that you want to test whether all the k-1 samples have exactly the precisions or exactly one divided by that variance, or there exists one sample that has a different kind of precision. And the question is: should these two problems be rated yes or no? Okay, so and how can you relate this to a little bit loose? A little bit elusive kind of version of these two problems is that you have two measurement instruments, you have one measurement instrument you saw many times, and you have an estimate of precision. And then what you can do is you have it, it breaks down and you want to have a new measurement instrument and you want to measure with the same type of precision, but you only have 10 observations. Can you say something about that new measurement instrument? And because you have a good estimate. And because you have good estimates, that should be okay to do, so you can do point testing. On the other hand, if you have your you start with only 10 observation and you make an estimate of this measurement instrument, but the new one has 10,000, how would you do that? And is that still equivalent or not? And it will never be exactly equivalent, but okay, it should be close to each other. And it shouldn't matter too much if you do it. Matter too much if you do consider this as a two-sample. Okay, so this idea of limit consistency is kind of philosophical. And I worked in my PhD thesis. And so what Ed just said, it's this criterion. So it's called limit consistency. So if you have this problem here, then you would like that this base vector converges to a constant. And Victor Pena, who did his PhD with Jim, he built on top of this and he came with this idea. So, okay, it's a constant that it should go to, but that constant should depend on your data that you fixed. And that constant should be a base factor that is of the k minus one sample problem with known variance. Varies. Okay, so this is what a cross-semble consistency means. Any questions? Okay, just shout if you have a question. So here are some results. So in this problem, for the problem of case testing case samples of the case sample precision problem, there exists a base factor that does obey all these. Obey all these different data. It can be labeling invariant, measurement invariant, predictively matched, it's information consistent, model selection consistent, and cross-sample consistent. Okay, and there's a little bit more to say about this in the sense that your the limit itself, the limiting base factor, is also analytic. And what's also nice is that you have an asymptotic normality. So that's also interesting in the sense that you can start with a two-sembling. In the sense that you can start with a two-sample problem, and if you measure one sample many times, that you convert to a one-sample base effect. Okay, so I just said, so this kind of the result that there exists, and I'm going to show you to a certain extent how you construct such a base vector. And first of all, you use this concept of measurement invariance, and this is kind of an illustration. But illustration of how objective thinking or objective base or what you would like to call it helps in constructing these base rates. So, in this construction is that we would like to have these invariances and how to construct these based on these invariances. First of all, what I would like to remind you is that you cannot just use any priors and the Jeffreys and translation invariant priors don't work, so you have to think about. Price doesn't work, so you have to think about the parameterization. Think about the parameterization. Okay. So, first, the measurement invariance is that, so this is just the baseline data set. You've got all these variances here, observed variances, and it should not matter. This base vector for this case or this case should not matter. It's exactly the same data. The only thing I did is just shuffle these up and down because it's about the precisions, so it should not matter whether they are shifted or not. Whether they are shifted or not. So, in other words, what we want to get is a base factor that is invariant on the location shifts of these individual samples. Okay, so one way to think about, so what this suggests is to use right half prior on the locations here. And that's the first step we think about. And there's another thing that you have to worry about is the scaling. Is the scaling so you can have this problem, this data set again, and you just multiply things by 100 or so, or 27. Then you have exactly the same data, but it's just on a different scale. So regardless whether you do things in time, minutes, or milliseconds, it should not matter. The variability is the same. So for that, that implies that there's another nuisance parameter in this problem. So this So, this one way to get this nuisance parameter is just look at the average precision. And then we parameterize the other parts, the other parameters, as the proportion of total precision. So, this theta here. And in this context, in this parameterization, then your parameter theta under the null should be one divided by k, so the number of groups of samples you have. And alternative is they are. And alternative is they are not the same as one five that K. So, this comes boils down to this pair of priors that you can think about: is that so you have identified new parameters, just the locations, and this average here, this average position, and you just embed this into the alternative, which is high-dimensional here, this vector theta. Okay, so if you want to have So, if you want to have the scale invariance, what you should do is use the right hyperprior. If you just do the computations and integrate everything out with respect to these right hyperpriors, you get the following reduced likelihood. That's how I call what I call it. And one thing that you should notice is that this base factor depends on the data only through the ratio of the Ratio of the sums of squares, and the sums of squares are location invariant. And as you can see, they also scale invariant in the sense that it doesn't matter if you multiply them or not. So that's exactly what you wanted, what you wanted. That's what you get out of it as well. Okay, and then the natural prior to use is a D Schlayer prior because this looks resembles a bit like a multinomial. And if you want labeling in And if you want labeling invariance, you can use all the use parameters and w to be the same. And if you do this computation, you get the following out. And this looks a little bit annoying. So this is a Luicella function. So it's a hypergeometric function on speed. That's what I call it most of the time. And so these are vectors of k minus one dimensions. And these are the multivariate. Uh, the multivariate beta function, okay. So, I'll tell a little bit about this Luicella function. So, this Luicella function is actually by definition almost this multivariate integral, and that's that's annoying to solve. So, when you have the base vector, you also kind of want people to compute it easily. And one way to this Luichella has a series representation, which doesn't help you either because it's an annoying series. Annoying series that goes, you have to sum k minus one indices. But the nice thing is that it also has a one-dimensional parameterization or integral representation. So this is very nice. So it's easy to compute. So in principle, you have to deal with a k minus one-dimensional integral, but you transform it into one-dimensional integral. And that's quite helpful in this case. So these things I use, so these invariances I used. So to construct this base vector, you get white half price. It's kind of natural thing to do. For the D shape, I use the same hyperparameters. This I didn't show, but you have to, it's something that you can do. And you can, the result is that you need proper prior on these parameters theta. And the result is. Theta. And the result is an analytic base vector, and you can compute it easily. And now, what you need to do is just show that it's across sample consistent and model selection consistent and information consistent. Okay. And the last one actually helps you to define. So I said something about you should use DHLA prior, but this tells you which DHLA prior you should use. Instead of DHLA prior u, it tells you that you should use the slate prior u, all of them being a half. You all of them being a half. Okay. Any questions? So let's just go through a cross-sample consistency. So, what's the result? So result is that I do not do the asymptotic normality here, but I'm just saying that. So what's the result? It says that if your, I always do the last one, by the way. So I let the last sample, the sample size of the last sample go to infinity, but you can do it with any of them as well. Can do it with any of them as well because they are labeling a variable, so it doesn't matter. So that's why it's always this SK with the NK going to infinity. This should be NK as well. Okay, so all you need is some concentration. In that sense, if you got that, then you have that this is of order one. And this base vector converges. So this base vector here can be constructed. Is can be constructed from the following setup. So here, use the same right-half prior on the locations, but you use a different prior on the positions. And these positions are multivariate, so-called beta prime distribution scaled with the true parameter that you know in the k-1 sample. So I don't. So, I don't want to go too deep into it. I just tell some elements of the proof. And it's kind of quite simple. You just have concentration that allows you to do a Taylor approximation. You swap, limit, and integral. You do some recognition. And you have to deal with some technicalities, which is very annoying, but that's what it is. So, because of this, you can use ChebiGev, you can do a Taylor approximation, and what's so nice about And what's so nice about this is that you kind of can focus on so this is stochastic, here is stochastic, but all these terms are non-stochastic because you can all just simply evaluate the sample at the right point. So you can kind of make a deterministic problem out of it. So the idea is swapping limit integral. And if you do that, you can recognize something that looks like exponential here. And if you rewrite this part, And if you rewrite this part, you see something that looks like exponential here as well. If you do that, you get this part. And if you know that under the null, the null hypothesis, you can get this out as well. That you almost recognize that this is the ratio marginal likelihood for this problem, the k-1 problem. Okay, so you're only missing this kind of factor here that you can get back out of this part by. Out of this part by a change of variable. And when you do that, you can, this is a recognition game, then you get an analytic base vector that's also, so this base vector is also analytic, and this trig is called a generalized confluent hyperg function on the second chronic. Okay, then you have to deal with technicalities. I'm not going too much into it, but you have to make sure that these work very well. Sure, that these work very well, uh, that these are bounded, and yeah, what you can do is some type of delta method to get the automotive normality out of it, so it's kind of standard. So, so that covers the across sample consistency, and so it's nice that you can actually, if you don't know a one-sample base vector for this problem, you just start with a two-sample and just trade it, and then you get a one-sample base vector out of it. You can approximate. Base vector out of it, you can approximate one sample base vector out of it. So, let's look at model selection consistency. So, for this thing, what I like I was thinking about, can I show so the difference between the one I did before was before I fixed all the samples and only let the cave sample data sample size of cave sample go to infinity. Now, I let all the sample sizes go to infinity, and the trick is to be is to rewrite what's happening in the sense that I just index the last one again by n and I also allow all these sample size going to infinity at different rates. So you had, for instance, n1 is 40 and 2 is 30 and 5 was 200 and I just keep that ratio and I'll just let everything go to infinity at the same time. So that's what C does and I also define a gamma gamma j. Define a gamma, gamma and j is like the relative variance with respect to the last one. Okay, so the last one is one, and for the last c is also one. And we call that this base vector depended on the ratio of sums of squares. And if you just play around with it, multiply, divide by the right variances, you get a following out. And this here, this x is X is distributed according to F distribution because everything is normal, so you're kind of everything is nice. Then you also have this concentration here. And the trick to prove it, and then you can get the statement. The statement is the following, that you get a base vector that behaves like this. And the odd thing that I found was playing around with it is that you get something that's exponential here. And I'll explain that in a bit. I'll explain that in a bit as well. But just annoy the hell out of me. And under the null, it behaves like a square root, modified by square root, and on the alternative, like a square root. But the dominant part is basically here. And this is just a constant independent of n here. But it does depend on the ratios, how you let things go to infinity, n's go to infinity, and how these ratios. And how these ratios of variances are set. Okay, under the null, you can get something that's also nice in a log term. You can see it perhaps better. And under the alternative, it's exponential. You can have the dependence on k, so that's nice as well. And it also depends on the prime instance, and so that's also there. Okay, scheduler proof, it's kind of the same idea. It's kind of the same idea. You have some concentration and then you do a Taylor approximation. And what I like about this is you do lots of gamma function asymptotics. And based on the gamma function asymptotics, you can get actually everything out of it. And the weird stuff is, well, it's not surprising, of course, that you can get it from the beta function, but you can also get it from this hypergeometric functions, these Lloyd-Cheller functions. You have to work a little bit hard for that. Work a little bit hard for that, you do some hypergeometric sequence manipulations, and you have to deal with technicality. This is really, really annoying, but okay. So, this is tailor approximation because we are case samples. You problems you deal with this. I use multi-index thing to get this data approximation where these are vectors. So, this is also a vector here, solid by that. And you have here a And you have here a product rule, and then you have to think about this product rule for like beats, so like beats product rule multivariately. So that's also annoying, but this thing is also annoying. So that's basically what's written here. And you have to think about, so by definition, what I mean here is that you have sub vectors here that you have to go over to get this correct. And this binomial of vectors is just a multiplication of binomial. Is just the multiplication of binomials. And to simplify math, just to give you the spirit of what's happening, is that let's focus on the zero of order term. So it's just kind of the same idea. You do not have to study this in a stochastic sense, just evaluate it at the right point. So again, because this X here is F distributed, you can just enter. So it starts to concentrate at one. Okay. Concentrate at one. Okay. So first, this term here, it still depends on n. You can see that it depends on n here and here. And this hypergeometric function, this is shorthand notation for it, also depends on n. So you have to deal with what's happening here. So if you do the beta function asymptotic, so this is standard, then you get this weird, oh, well, you can work it out. Then this G here captured all the exponential behavior. Exponential behavior. And this could be problematic, especially under the null, because you want your baseback to be consistent. So it should not grow. That's the point. Okay, so plug in, whatever. You get to this part now. And this is mind-boggling at first, but the thing is that you can handle this very easily. And then once you realize the structure. Once you realize the structure of these hypergeometric functions, all these Lohrichella functions. Okay, the trick here is to move this around a little bit because this is a lower term, this is an upper term, but you want the end dependence on certain upper terms and certain lower terms. So, this is the upper term I was talking about. Depends on N, this depends on N, but you want to remove it here. That's one. Second of all, you have these upper terms all depend on N. These upper terms all depend on n, this one depends on n. You want some kind of cancellation here. And if you do that, you can do that using something called the FUF transform. And there's a multivariate version, multicellular version of this as well. So multivariate version of this as well. Then what happens is that here you have this N, and here you've got this N. So only at these upper terms, these B's and this D, this cancel out and you get some. This cancels out, and you get something that is of order one. Well, not all of order one, but this part is of order one. So that's this part. And you've got something here as well. I missed the product here, it seems. Okay, so if you combine everything, you can have all these terms. This comes from the exponential term that I had from the beta aspiratorics. This is from the hypergeometric function, the Loicella function. Function, the Lowicella function, and you can combine these terms. And if you notice, that if under the null, this gamma is all one, so this in product is just C. So these two cancel. So you don't have exponential growth in N. This part here, you can shove it in here. There's no N dependence. And this part in this part also cancel out perfectly when gamma is one. So it's kind of a miracle why this works as it works, but I thought it was fun. And all this points that. It was fun, and all this boils down to gamma functional. So now comes the technicality part, and which is horrible. But I just mentioned it. We have to deal with this here. And again, when gamma is one, everything cancels out perfectly. And it actually also happens with higher order terms in the theta approximation. But to show that, you need to do some combinatorics, you have to count very well. Combinatorics, you have to count very well because otherwise things go bad. But if you do that correctly, then you can show that these terms here, so this without these exponential terms that you can actually factor out, then under the null, it's just order one, but under the alternative is something that is of order and to the power of whatever gradient you have. Whatever gradient you have. So that's the tough part. And the trick then is just rewriting this sum of all these vectors into vectors of certain kind of sums and then work your way from there. There's one last thing you have to show that gamma has a minimum at one. So it always grows if gamma is not equal to the null hypothesis. The gamma is not equal to one. Yeah, so this is again the result. Yeah, so this is again a result. I'm not sure how much it buys you because these kind of results are unknown. Dr. Ocel and Johnson did this already. They did it in a much more general setup, but I thought it was fun to also have control over how you go to infinity with all your ends and the effect of the prior and effect of the number of samples. So I thought it was quite fun to look at. And lastly, so. And lastly, so we showed that for with this Niels Lebra, you can get nice good behavior that we noticed before. And then one last thing is something called information consistency that if you it's a it's a fixed sample consistency so you fix the number of samples that you have regardless of the outcome of no, it does matter what the outcome is. So in this case, again, I just let the last one go to infinity. So you so the To infinity. So you have and say nj, so certain kind of group. You have two samples of it. So you can calculate sj squared and you can calculate skj squared. But the last one is a much, much higher order than you want this base factor to explode. So this is also a philosophical thing that's quite Jeffrey's in the sense that you falsify the null hypothesis. And to do that, you want this base factor to blow up. And you can get this when taking your And you can get this when taking your parameter, it being smaller equal than a half, but yeah, you just take it a half because that quite at the cost of it. And by labeling a variant, you take all of them a half. So this is it in the sense that I talked about the case sample problem. I showed some ideas of how to think about the problem in this object. Think about the problem in this objective Bayesian way and how to use this invariance to construct these base factors and the consistencies to really get pinned down the base factor that works with all these properties that you want it. And all of them just falls from gamma functional asymptotics. And it's quite simple in general, the ideas. It's just annoying to write it all down. To write it all down, I still need to update the archive version of this. Sorry. And there are some open questions regarding robustness, but I think that it extends relatively easily. There's also, again, the mentioning of safety in the sense that you can get type 1 error control across time and you can sequentially just track the base fact and do decision based on that, do decisions on it. And there's an open question on whether across samples. An open question on whether a cross-sample consistency is something that holds more generally and not only in the case that I studied. So if anyone has an idea, please tell me and then we can collaborate on it. And here are just some references. Okay, thank you. Thanks for your attention. Questions? Comments? Alexander, I have a question. Have you thought? Very nice talk and nice greetings from sunny Philadelphia. Have you thought about high-dimensional? I mean, the one other thing to now take to infinity is K. How did What did how did these things break down? I'm not sure about so, so I can say something about information consistency in that sense, that information consistency requires just two samples to be two and two, or more than large and equal than two. All the other samples can be zero. So in that sense, k could be large and you do not observe any. And you do not observe anything, and then you still have information consistency. Regarding K going to big in the base vector here, yeah, there's the dependence on K here that might give you some insights. But I haven't really thought too much about it, but it seems that if K is large, then you can acquire evidence for the null much quicker. Okay. Okay. Yeah. Thanks. Perhaps I can ask a question. I'm really troubled by this exponential here, but I'm not sure if anyone else is. I figure it's kind of not standard, but. Does that exponential mean that there's inherent randomness in the base? Inherent randomness in the Bayes factor that just doesn't go away? Yeah, it's kind of sloppy on my behalf because I'm rewriting this Taylor series in such a way that I can just go over all these. So these are the well, the order is just the summation of all these L's. So these are the multiplicities with the partial derivatives. And I just sum over all. And I just sum over all of them being p is zero to infinity. And when I do that, this is just that's where the exponential comes from. But it's kind of very sloppy because if you do simulations, under the alternative, the variance kind of blows up with n. And for under the alternative, under the null, it doesn't On the null, it doesn't matter that much. You get a little bit more stabilization there, but the variance remains kind of similar. But on the alternative, it blows up a little bit, but not as much as this here. But this goes to infinity much faster. It seems to be having a good chat anyway without using. Yeah, yeah, you don't need us. I think we can go for coffee. Any more questions? Any more questions or remarks? Can I just ask? When you mention this exponential, does this show up also in the asymptotics? Does it bring any additional problems? Just wondering. It is of a little bit, this contributes much more. So, in that sense, it's controllable. And yeah, that's the point. But it's also, I'm not sure how I respond to this properly. It seems to work, but it doesn't capture everything. That's the problem that I have with it. That's one. But this is really the dominating behavior. So, how much does it contribute? I'm not sure. Okay, thanks. Thanks. Okay, so if you can hear me, do you have any further questions?