Thanks for the introduction. So first I would like to thank the organizers for the nice invitation. So I'm still new to the area. I'm here more to seek answers rather than provide ones. So today I'm going to talk about this multi-scale inverse problem based on transport equation. This is based on joint work with Xing Li, one of her former students a couple years back, and more recently with also with Liu Zhao from Peking University and Tan Bei from UK. University and Ten Bay from UPSD. So here's online. First, I will give a very brief introduction on kinetic theory because I thought that it's not very familiar to this audience. And then I will focus on two aspects of a simple kinetic equation, which is actually very familiar to the audience, the radiative transport equation. And then I'll talk about stability and also the numerical methods, and then some confusion goes to the method. So, I guess people have seen this diagram a lot. It's actually adapted from an old paper of Anquist and the E in 2003. So, there are these four scales in the past because they can only see my screen. So, I just want to make one of those things.  Oh, right there, right there. That's how we're going to be there. No, no, it's not. Can you airdrop your escape as well? You may draw your list of this one, maybe and then. I think I would like LCLS to my supply on the screen. I don't know where it is. Okay, so I'm gonna just go to the channel. I would say the hand on this computer and then I'll go on here on the right. This one we can show screen. So it's not just a shape. It's colourful. I think it's better. It's done. I think it's back. Yeah, because we moved it to this other laptop, but now I need to put them in soft. It's not necessary that you zoom on the laptop. No, no, no, it was soft. You can see it, right? Okay, so we don't need to share screens. No, which one is your confidential? This one is mine. It's almost fine. Yeah, no, you can see. Alright, so sorry for the interruption. And so this is where this kinetic theory sits. So it connects the molecular dynamics with the continuum mechanics. And I think the molecular dynamics and continuum mechanics were studied before this kinetic theory. So the kinetic theory, only there's a lot of progress in the past decade or two, I think, because people realize that continuum mechanics is not enough to describe most more physical or Most more physical or biological, even social phenomena. But the molecular dynamics is still too expensive. And because, oh, I think that is something is missing at the bottom and minus. Oh, command and minus, okay. Yeah, and I think it's froze. It it does not it doesn't show that in my computer. Oh no, there is the minus oops. Is that what you want? Can you do it? I will do it with this. Yeah, yeah. Okay. Okay, so I think I can just only do this. Alright, so um so then because of this uh kinetic theory, so it sits in this position is natural, is multi-scale by nature. And here by multi-scale problem, I mean the problems that are connected by this asymptotic relation. For example, from quantum to molecular, you have this semi-classical limit, and then from molecular dynamics to And then from molecular dynamics to kinetic theory, you have the main field or Boltzmann-Grad limit. And then from kinetic theory to continue mechanics, you have the hydrodynamic limit. So let me give you a very general form of kinetic theory. So basically, the idea of this kinetic theory is that it tries to look at this many particle system from a statistical point of view. That is say, you assume all the particles are indistinguishable, and then this F is to And then this f is to denote the possibility of finding one particle at a given location with a certain velocity at time t. So this is the general form. So we have the transport term with velocity field v, and we have the acceleration term with this external potential phi, which can be prescribed or obtained self-consistently through either Poisson equation or Maxwell equation. Or Maxwell equation. And then the right-hand side, this is the collision operator. So it describes the interaction among particles or the interaction of particles with the background. So what's interesting there is that you have this small parameter, you have this parameter epsilon, which is a dimensionless parameter. So this alpha, beta, and gamma, they are three constants that determine the macroscopic regime. So they are not necessarily integers. Necessarily integers. So let me give you two specific examples. So probably the most well-known and famous example is the Boltzmann equation. So here I'm writing down the most plain version, so there's no external field. And you don't need to read through this collision operator. It looks a little complicated. But there are just two things I want to point them out. So this Boltzmann collision operator, just based on the assumption that the interaction among three or more particles. Among three or more particles, they are very rare, so that's why you only see this quadratic function among f. And also, there is the gain, so the positive part is the gain part, and the negative part is the loss part. So, basically, that just means that, so if you remember the meaning of this f, that's the probability of finding a particle with velocity v and a certain location x at time t. So, the last part just means that you have particle started at this velocity, but you collide with another particle. But it collides with another particle, so it moves out to a different velocity. Whereas for the gain part, is that you have two particles with a different velocity, but after collision, it becomes the velocity of V. So in this case, so this Boltzmann equation was originally derived to model these rarefied gas particles. So when the gas is very dense, then we can see that they are very easy to collide. So that's why that's described by this one over epsilon. So epsilon is the 1 over epsilon. So epsilon is called a Newsen number. So in this dense region, this epsilon is very small, and you can show that this Boltzmann equation leads to the compressible Euler equation, to the leading order. And of course, you can do high-order extension to get Navier-Stokes or Bonetto Superbotette. The second example is a relatively new model. It's called Lemmy-Fouca-Planck equation that appears in plasma physics or economics. Economics. So I know that many of you are familiar with focal block operators. So if you see here, that's just replacing of Laplacian by this fractional Laplacian. So in this case, the correct macroscopic regime is that you rescale the time and the space in the right way. So I need to say that here you can also just view it as you rescale the time and the space by epsilon. So that means you zoom out. But for this one, you have to rescale the space. This one, you have to rescale the space and time in a different way so that the corresponding limit is this fractional distribution limit. But today, we're just focusing on the simplest kinetic equation, which is the steady-state magnetic transport equation. So it models the dynamics of photon particles in materials with different optical properties. So here is steady state, so there's no time dependence. Steady state, so there's no time dependence here. And here, as with this Boltzmann collision operator, it has the gain part and the loss part. So the gain part just means that the particles with a different velocity v prime will become the velocity v after scattering. So this kappa is the scattering kernel. And the last part is that you start with velocity v, but after some mechanisms, it becomes something else. So this sigma actually contains two mechanisms. So it can be due to scattering. Be due to scattering or due to absorption. And this one we need to prescribe the boundary condition. And here, so this gamma plus or minus just denotes the incoming and outgoing part of your boundary. Let's simplify the model even further by assuming that this scattering kernel is homogeneous, so that we can move it out the integral. integral. So let's regroup the collision operator in the library. And that means I decompose this sigma to sigma s and sigma a. So the sigma a that's the absorption coefficient and the sigma s that's the scattering coefficient. And then so for this problem the interesting scaling is called the diffusive scaling. So basically that is in the highly scattered regime. So that means this scattered Regime. So that means the scattering effect is amplified by this one epsilon, whereas the absorption effect is mediated by this order epsilon. And for this one, the corresponding macroscopic limit is the diffusion limit. So actually, the formal derivation is quite straightforward. So let me walk you through that. So we just do an asymptotic expansion and you plug it into And you plug it into the transport equation. To the leading order, you'll see that this F0 is this L of F0 equals 0. And if you look at this L, this is F average minus F. So that means this F0 should be a velocity independent. So this rho 0 is the macroscopic quantity. It only depends on time and the position. And at the next order, you just combine, so you just collect the order. So you just collect order epsilon term and here you need to invert this L operator. So it's invertible as long as this one is in the range of L and which is the case here. So we can explicitly write out the relationship between this F1 and F0, and F0 is rho-not. And then to the next order, F2 comes into the picture. But the good thing is that if you take the average, this term is going to vanish because this L of L. To vanish because this L operator, if you look at it, if I take the average, then this one is zero. So in the end, we have this elliptic equation for the density row. And of course, we need to prescribe the boundary condition for this row zero. And this actually non-trivially depends on the boundary condition of f. You actually need to solve a boundary layer problem. But if this phi is independent of V, then it's the same thing. Dependent of V, then it's the same thing. Okay, so now let's look at the inverse problem. So for the transport equation, so I think you have seen this in the conference, basically is you have, you define this orbital operator from the incoming boundary condition to the measurement, and you are trying to infer this coefficient sigma s and sigma a. And the same thing for the corresponding diffusion. The corresponding diffusion limit, which is the Caduran problem. So you're given this Dirichlet to Neumann map, and then again you are trying to infer these optical properties. So now let's put these two problems side by side. So on one side, you have this transport-based problem, which is higher dimension because it depends on both X and V. But it's more accurate. So on the right, you have this diffusion limit, which is simpler, but less accurate. Now the question is, so we know that at the forward level, so it's well known that this diffusion model is a good approximation to the transport model in the sense that the difference between your F and the rho, if you measure in the correct sense, that is point-wise of the order epsilon. So now the question is, can we ask the So now the question is, can we ask the same thing at the inverse level? In other words, what is the relationship between the recovery from the diffusion model compared to the recovery from the transport model? In fact, in the literature, these two problems have been well studied individually. And it's known that for the transport-based problem, it's well-posed, whereas for the cataron problems, ill-posed. So here we are trying to. Il-post. So here we are trying to understand the relationship between the two. So how do we go from the sword postness to the ill-posedness? And the second question is, say, in the end, we need to solve it numerically, right? So the thing is that if the to be recovered coefficient has a magnitude difference, and so that means we have this multi-scale feature in the recovery, can we come up with the numerical methods that Paris? Numerical methods that have uniform performance. So let me explain each of them. So, first, for the stability, let's look at this linearized setting. So, you have seen this linearized setting also. So, basically, that means, so here for the interest of time, I'll just focus on the recovery of the scattering coefficient by assuming that the absorption coefficient is given. So, the linearized study, meaning that I assume that I have. I assume that I have a given background, which I denoted as this sigma s zero, and the to be recovered a scattering coefficient is only a small perturbation of this background. So that is denoted as this sigma s tilde. And then we can write down the equation for this perturbation. So it's pretty similar to the original equation. And then also we can write down the corresponding adjoint equation to the original equation. adjoint equation to the original equation. And then we multiply this difference by the perturbation by the adjoint variable and then the adjoint problem by this perturbation. So we end up with this linear operator on sigma x. So actually you have seen this one in Chinese talk and also yesterday's talk of Jan's and Fernando's. So in the end you have seen that we have this linear operator on sigma s. So basically this guy So basically this guy is a computable quantity. So this gamma epsilon, you can see that this g is the solution to the adjoint problem and this f0, that's the solution to the original problem. So in sum, that means we have this linear integral operator on this perturbation sigma s tilde, and we are trying to invert this linear system. So by Linear system, so by to get this sigma s to. And the question we want to answer is: what is the stability of this inversion? In fact, in one dimension, we can ask an even basic question. Because we know that this gamma, so this gamma is the solution to, so gamma is the product of the solution to the original and adjoint problem. So we can actually vary the incoming boundary condition for the slow. Boundary condition for the forward problem, and then we can vary the location of the measurement. So, say the question is: if we vary them extensively, can we uniquely determine sigma as ta? And we know that for this form, it boils down to if we substantially vary this delta, y, and phi, is this gamma going to span the whole L2 space? And the answer is no to in one dimension. So, in one dimension, we have shown. So, in one dimension, we have shown that if there is no absorption, then this gamma epsilon is actually a constant, so independent of epsilon. So, there is no way that it can uniquely recover this as total. And if sigma A is non-zero, then we show that this variation of gamma epsilon is becoming smaller and smaller when you are approaching this diffusive regime. So that means the recovery becomes harder and harder, and if epsilon is zero, then we. zero, then there's no way to recover it. But in high dimension, we actually show that this is, we show the instability result. So basically, we know that for stability is that if you perturb the data, so this B is your data, so if you perturb the data a little bit, how far your recovery becomes. So we can put that into mathematical terms that we define this distinguishability coefficient, basically saying that if we consider all the set of Consider all the set of sigma such that if you put this sigma to the equation, the right-hand side will be delta perturbation to the true data. Then what is the maximum distance of the possible sigma compared to the ground truth? And we show that this distance is actually bounded below by delta over epsilon. So here, so delta is easy to understand. Basically, it tells that if your perturbation is small, of course, you are going to get a better. More, of course, you are going to get a better recovery. But what is bad here is that you have this one over epsilon dependence, meaning that if your epsilon is very small, your recovery will be order one over epsilon away from the ground truth. So that means even if your data is contaminated by a small noise, then your recovery will be very big away from the ground truth. So, this is the first result to connect these two models in the linearized setting. So, here let me also mention two very nice later results on the nonlinear setting. So, one is by Ru Yu Lai, Ching Li, and Gotha Wu Men. So, they have shown that, so here I've just tailored the result to this specific case that you recover the scattering coefficient. So, this is the stability result. So, this is the stability result. So, they show that the difference between the recovery is bounded by the difference of the albedo operator. What's important is that you have this pre-factor, which blows up when epsilon goes to zero. So this also implies that when you are close to the diffusive regime, then the recovery becomes harder and harder. And another result is by Hung Kai Jiao and Yi Min Zhong. So they actually convey the same message, but in a very different flavor. But in a very different flavor. So they prove this instability result. So basically, what they say is that if the difference between the recovery, so if there is a lower bound, which is theta, then how different would be the orbital operator? And when epsilon is relatively small, that means if you are in the kinetic regime, then it's not so different, so it's zeta to some, so it's the polynomial of theta. But when you are in the diffusive regime, But when you are in the diffusive regime, then it's exponential dependent on theta. Which means that if your theta is relatively big, that means your recovery is bad. But the ability operator is still very close. So this also tells you that it's unstable when in the diffusive region. So now the thing is that we have to understand the problem, but in the end we still need to compute it. So I guess I do not need to say too much about this. To say too much about these two frameworks. So, here in the rest of the talk, I will focus on this optimization framework. Let's look at this linearized setting first. So, this is what you have seen before. You have this linear operator, this sigma s tilde, and this is how we write it into this optimization formulation. So, in fact, we can translate the previous, this instability or stability to. Instability or stability degradation results into this numerical setting. So that basically says this gamma epsilon, or actually the linear operator, is asymptotically low rank. In the following sense, that means there exists a function which is order one, but when you apply this linear operator on this function, you get order epsilon. And here I divide, so I add this denominator just to rule out the possibility that this gamma itself might be order epsilon or order one. itself might be order epsilon or order one of epsilon. So I don't want to take into account that. Okay, so basically you can see that in terms of numerics, the intrinsic difficulty is that this linear operator is asymptotically low rank. And of course the way to do that is to do a regularization. And so here let me state a very rather standard result, this Tickernorf regularization. So basically if your data Basically, if your data is contaminated by this noise delta and you have this regularization weighted by alpha, so if your sigma, so sigma tilde is the solution to this optimization, and sigma tilde true, that's the ground truth. So the error has two components. So one component, you can see that that's from the noise. So that tells you how the noise is propagating to the recovery. And of course, To the recovery. And of course, if you have better, if you have a larger regularization, then this one is smaller. So this actually corresponds to the stability of the problem. The other one, so this is Z, so we assume this range condition, so that the ground truth can be written. So this Z actually comes from the ground truth. So this one says that if alpha, so alpha is this regularization constant. So if alpha is zero, so this error vanishes. So that actually tells me that. This error vanishes. So that actually tells you the accuracy when you change your problem by adding this regularization. So, here the main message I want to convey here is that you know there is a balance to strike between this accuracy and stability. So, it seems that there's not much we can do when this A is this asymptotically low rank. But at the next stage, there is something, there is actually something we can do. So, here we just assume that we have this S V D of A and S V D of A and then a very straightforward calculation tells you if you do this gradient descent, this is going to be the convergence rate of your gradient descent. And this Sn, that's the smallest eigenvalue of your A. And if you remember, this A is asymptotically low rank, that means this Sn is going to be order epsilon, whereas this S1, that's order 1. So here from this coefficient, you can see that if alpha is large. You can see that if alpha is large, of course, you're going to get a faster convergence rate. But that's not the case because the previous error analysis tells you that you cannot choose alpha arbitrarily large. You actually, the optimal choice would be you need to match these two terms. So that actually indicates that this alpha has to be this S n squared. So it's very small. So in other words, this coefficient is very close. Efficient is very close to 1. So if you do gradient descent, then you will get extremely slow convergence. And of course, we know that for this, the way to go, of course, the natural way to go is to use the second-order information, which is the hash. So here I do not need to go into details of, because everybody knows this adjoint state method. So basically, you have the derivative that involves computing 2P. Computing two PDEs. One is the forward and one is the adjoint. And then if you want to compute the hash, then you have to create a gigantic Lagrangian and then you do the variation. And this is what we have. So basically, this tells you that you have the Hessian acting in this direction, sigma delta, hat of sigma, and that is the form of this Hessian. This hash, and basically, that involves you computing the additional 2 PD solders. So, you see that if you want to compute the gradient, then you need to compute 2 P D's. And if you want to compute hashen, you need to compute 4 P D's. After computing this hashen, we see that, so yeah, so basically this one is from the regularization, so that's the simple part. And this one is from the problem itself. So this hash actually. So, this hash actually gives another angle of looking at this stability deterioration. Basically, we can show that this hash is low rank in one dimension. So that's why in one dimension it's not recoverable. And in higher dimension, we show that it's asymptotically low rank. So it's in a similar definition as before. Okay, so that's. Okay, so then we know that for this asymptotic low-rank problem, what we can do is that we can do this Newton-type method. So basically it's the Newton-CG method. So I don't want you to read these details. So basically what I want to tell is that if you are using this Newton method, then it's regardless of the condition number of A. So no matter what condition number of A is, you are going to, in fact, for the linear problem, you can easily show that you just converge in one step. In one step. So, using this Newton's type method, you can just get rid of this issue of EO conditioning. Let me quickly show you two numerical examples. So, that's the numerical setup. So, we have this ground truth, and this is the boundary position. And we have two controls of error. One is compared to the ground truth, and the other is compared to the previous iteration. So, I will omit these two by directly show you the table. So, basically, this table tells you that: so, one is with voice and one is without voice. And you can see that for the gradient descent, it actually takes a few thousand steps to converge, whereas if you're using this Newton CG, it just converges in less than 10 steps. But of course, there's some price to pay because we know that if you're using this Newton CG, then every step is more expensive. CG, then every step is more expensive. You need to do an iteration, you need to do a conjugated gradient. But still, if you compare, the number of BDE solvers is significantly small than the gradient descent. So that means computing this hash is worth the effort. And for this example, so actually this is the true multi-scale example, although it's not very easy to see. So basically, so here the topic recovered. So here to be recovered, the sigma s, so you have a higher magnitude here and the lower magnitude there. It's not very easy to show. And here we start with the constant initial gas and you can see that over 10 iterations is already very good compared to the volume truth. So in conclusion, so I hope that explained clear that the stability issue Clear that the stability issue between these two models through this asymptotic relation. And in terms of numerics, for now, I think the message I said is that for the destability stuff, because it's intrinsic in the problem, so it looks like even you add regularization, there's not much you can do there. But of course, this is probably not true. Like Yunan has asked the same question yesterday. And also, Bian has mentioned that you can use a different mismatch functional. So I don't know if that Mismatch functional. So I don't know if that if you are using a different mismatch functional, would that increase the stability of the problem for at the future level? And for the for the numerics that show that with this hashion is actually really helps. So for the future work, actually there are a lot of things we can do. So for the first one, it's actually for fun. So recently I learned that there's another So recently I learned that there's another formulation of this inverse problem instead of using this algebra instead method. So you can formulate it as a set of break problem. So I'm just very interested to work out the details and see what are the difference between these two approach and whether there is some gain if we formulated the problem in a different way. But of course here we are taking this optimization framework and there is the similar parallel question you can ask in the Bayesian Asked in the Bayesian integer framework. And the next two questions actually is, yeah, I don't know. So the next two questions is more like theoretical. So I want to get some insight from the theoretical aspect. So one is that whether you can increase the stability of the problem by any subfrequencies. Actually, there is some theoretical work on that. But of course, that would change the problem a little bit. And here I keep saying that this low rank is our enemy, because that's Low-rank is our enemy because that gives us a problem. But we have seen a lot of talks in the conference that people really want low-rankedness. So, the thing is that can local rankness become a good trend here? And of course, you can, but there are many other kinetic equations that have similar feature, so we can apply it to other kinetic equations. But with that, I'm going to end my talk. Any questions in the comments? 