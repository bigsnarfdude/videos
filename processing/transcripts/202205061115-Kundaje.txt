Answer, Professor Anschuntendai. And he's a professor of assistant professor of genetics and computer science at Stanford. His primary research areas in large-scale computational regulatory genomics. In particular, her methods include the integrative analysis of heterogeneous type throughput functional genomic and genetic data with the goal of identifying this and deciphering regulatory elements and data. Ciphering regulatory elements and their long-range interactions. His final goal is to learn predictive regulatory network models across individual cell types and species to improve detection interpretation of natural and disease-associated genetic variation. And yeah, today is gonna present us his work and the floor is yours. Thank you so much. I hope you can hear me. Yes. Yes, excellent. Thanks. Um, so let me start by just thanking all of you for the kind invitation. And I wish I could have been there in person. I really love Banff. Before the pandemic, I used to attend these meetings quite often. So hopefully, we can do this again next time in person. But thanks again for the invitation. So my lab is in the Department of Genetics and Computer Science at Stanford. We are at the interface of leveraging machine learning for genomics. Leveraging machine learning for genomic discovery. And today I'll talk to you about how we use a variety of relatively simple interpretation methods, but coupled with deep learning models trained on genomic data to discover novel insights into genome function. So let me just start by motivating the problem. So as you know, the human genome is a long DNA molecule with 3 billion letters of basis. Billion letters of basis. The basic alphabet is ACGD. And what's interesting is if you look at the human population as a whole, each of our genomes have differences and we exhibit significant variation of our genome sequences. And this variation is often associated with various traits. You know, for example, various phenotypic traits, changes in hair color. Rates, changes in hair color, height, benign, nothing to do with disease. But some of these variants, you know, genetic variants, mismatches, indels in the sequence, can also increase your risk towards specific diseases like cardiovascular disease, type 2 diabetes. And some of these variants are quite common in the genome. Others are very rare. Sometimes there are denominator mutations. And all of these have interesting potential functional consequences or not. And so, you know, cataloging. And so, you know, cataloging genetic variation across human populations is a very important first step to decipher the genetic basis of traits and diseases. So this has been quite popular. There have been population scale sequencing efforts across just regular, you know, healthy individuals without necessarily a particular interest in disease. This is to just catalog a natural variation of the genome. Of the genome. So, here I'm showing you an example of a single nucleotide variant. Each individual has two copies of the genome. So, this person has an A and a C at this position. This person has an A and an A. Some other person might have a C and a C. And there are different frequencies of these combinations of variants. Now, you can, of course, apply this to case control studies, where you not only sequence a collection of healthy controls, but Of healthy controls, but also a collection of individuals with a disease of interest, common disease of interest. And then you can perform these statistical association tests where you take every variant and you just, you know, estimate the frequency of the different versions of the variant. And you perform an association test, chi-square test, or some other statistical association test that tells you how associated this variant is with the cases versus controls. And so this is a very Controls. And so this is a very popular approach, and it's been allowing for these are called genome-wide association studies, and they've been very popular to study genetic loci and their associations with different traits and diseases. So here's an example of what a GWAS looks like, a genome-wide association study looks like for an Alzheimer's disease. On the x-axis is the genome. Each segment here is a different chromosome, and each point here is a genetic variant. Each point here is a genetic variant in the genome. The y-axis is a statistical significance of association, negative log 10 of the p-value of association of each variant with Alzheimer's disease. And you can see, you know, most variants don't have any association, but there are some, these huge towers of loci. These variants clearly have very strong association with disease and may have functional roles in predisposing individuals to Alzheimer's. Exposing individuals to Alzheimer's. So, this way you can map genomic loci and their potential role in different diseases and traits. But this doesn't take you very far. I mean, it's very important, tells you these loci, but what is their function? How is this genetic variant potentially affecting molecular function and hence cellular function and then disease? These are open questions. And so that's where we get to decoding genome function, which means like if I'm given a genetic variant, some location in the genome where Variant, some location in the genome where there's a single nucleotide mutation or some other kind of mutation, or I'm looking at some DNA word in the genome. What does this letter do and what does this word actually do functionally, right, or does not do? And one important thing to notice is when you're trying to interpret genome function in terms of like what's the functional role of a piece of the genome, it gets more complicated because humans are multicellular organisms and there are thousands of Cellular organisms, and there are thousands of diverse cell types and tissues in the body, and they all share the same genome largely, at least through most of the course of a single human's lifetime. The genome is relatively static. It's accumulating mutations, but in general, it's pretty static. So the same sequence is giving rise to or is encoding all of this amazing diversity of function. And the reason it's able to do that is because the genome is basically a global instruction set and different instructions. And different instructions, different sets of genes, different sets of control elements, as I'll show you in the next few slides. Different components of the genomes are used to construct transcriptional programs, to construct cellular programs that define every cell, cell type, tissue, response to stimulus, and so forth. So, when you're interpreting the sequence of the genome, the context really matters. The same piece of sequence can behave very differently in a liver cell. Very differently in a liver cell versus, um, you know, versus a cardiac cell versus a neuron. So that adds a level of complexity. Basically, you need context-specific interpretation of the genome. And by context, I mean, you know, biological context, cellular context, the cell type or cell state, or the condition, age, all of these contextual variables affect the actual instantiation or function of a piece of the genome. And so to understand. And so, to understand genome function, we need to also understand what the anatomy of the genome is. So, if you take the DNA and you unwrap it, you obviously have genes, and there are 26,000, approximately 26,000 protein-coding genes in the human genome. The coding portions of these genes directly code for proteins, but that accounts for just about 2% of the genome, right? So there's a remaining 98%, which is not really coding for proteins itself. And different genes are activated and repressed. Different genes are activated and repressed, so different repertoires of genes are activated and repressed in different cell types, tissues, conditions. And so it's really this precise program of genes that get activated or repressed that defines cell state. And if you're activating repressing genes, you need a control system, right, to turn things on and off. And that happens to be encoded in the genome as well. These are control elements. These are the segments of DNA that act as like on-off switches. I'll just call them as control elements. Control elements. And their DNA also encodes information. And I'll get to exactly what that means. But while we know where all the genes are in the genome, mapping control elements and their context-specific activity has been still elusive. And we don't really have a global map of these control elements in different cell types and tissues. And so, what's happened really quite nice is there have been revolutions in genome molecular profiling technology, which allow you to like obtain genome-wide. You to like obtain genome-wide maps of various kinds of biochemical events. So, for example, if you're interested in the uh, this is a let's say this is a protein that binds the DNA and it has a regulatory role, it turns genes on and off. If you want to know where this protein binds in this in one particular cell type, you can perform one experiment, and that experiment gives you a readout at every base in the genome. And the signal, the height of the signal, the strength of the signal corresponds to the like. The strength of the signal corresponds to the likelihood of the event of this protein having bound there. And what these experiments are doing is they're using sequencing as a readout. But the sequencing reads, the heights of these signals is really just the coverage, the number of sequencing reads falling at each position. And there are clever protocols to use sequencing to essentially capture shadows of different biochemical events. So this protein is generating these shadows, and the experiment can use. And the experiment can use sequencing readouts to capture the shadows. And in some sense, the strength of the shadow tells you the likelihood of the event. And you can do this for all kinds of different biochemical modifications. Here I'm showing you some protein binding sites. These are called epigenomic or chromatin modifications. These are biochemical flags like acetylation, methylation added to various proteins on DNA. And this is real data. You can see how they localize to very specific types of elements. Here you're seeing an active. Types of elements. Here you're seeing an active gene body with a specific mark. These are promoters of genes. These are things that current genes on. These are called enhancers. These further enhance activity of genes. You can see the enhancers have specific marks. You know, they have these orange and yellow marks. And then here's a repressed region of the genome that doesn't have any of these marks, but has the gray mark. But the basic point to take from this slide is that you can now perform experiments that give you these beautiful molecular paintings of the genome. Molecular paintings of the genome, and these are context-specific people. So, there is a different painting for every marker across the entire genome in every cell type. And so, what NIH has done is they've now funded many large projects. These are two of which I've been involved in for the past 10 years. What they're allowing is to obtain like reference maps of molecular profiles of the genome. So, just as we had the reference genome, we now have a reference regular. We now have a reference regulum of sorts, a reference molecular map of the genome. But this is, as I mentioned before, a context-specific map. So it's in some sense a tensor, it's a three-dimensional tensor where you have the genome as one axis, you have hundreds of different molecular readouts as the other axis, hundreds of different proteins, RNA levels, various kinds of epigenomic modifications across the entire genome in every cell type, tissue, individual. Type tissue individual single cell that you can get your hands on, right? So it's a gigantic tensor. And so this data is really amenable to using powerful machine learning models to understand how DNA sequence is somehow encoding all this information. In the end, like DNA sequence has all this sort of instruction set encoded, and it's contextually being used to produce these beautiful molecular maps or molecular profiles in different cell types. Profiles in different cell types and tissues. And so, our goal is to really understand the sequence basis, how DNA sequence encodes all this context-specific information. And I'm going to talk to you today about how we use machine learning models and interpret them to do two things, to decode the control system of transcription, of gene expression in the genome. That is to decode the DNA sequence, how it encodes all this information. And then also to. Information and then also to use the same models to predict how mutation genetic variants affect function through molecular profiles. Okay. And one of the reasons why this is particularly important is all of those genome-wide association studies I talked about, which allow you to map genetic loci associated disease, actually about 95% of all currently available common disease variants, they are not actually found in protein-coding regions. Actually, found in protein-coding regions of genes, so they don't directly disrupt the protein sequence or the gene sequence that codes for proteins. They happen to be enriched in these control elements. So, they're breaking the control system of the genome, thereby disrupting gene expression and cellular function. And over time, basically, you get cellular dysfunction and disease. So, let's start with the first part, which is decoding syntax of regulatory DNA. And everything I'm going to show you today is extremely collaborative. Is extremely collaborative work. We are a purely computational lab, but we collaborate with a ton of fantastic biologists. So, these are the students who have done a lot of this work that I'm going to talk to you about today. And this is primarily a collaboration with Julia Zeitlinger, who's a fantastic biologist at the Star Wars Institute. So, let's go back to our cartoon map of different types of elements in the genome. And I mentioned these control elements, right? On-off switches that turn genes on and off. Switches that turn genes on and off. So, what is it in the sequence that makes these control elements special? Well, what happens is they act as landing pads for proteins that act as regulatory proteins. So what these proteins do is they combinatorily and cooperatively bind DNA, and they bind very specifically because they recognize specific sequences, DNA words or motifs encoded in these DNA sequences. And it's really, there are beautiful syntax. Really, there are beautiful syntactic constructs of these DNA words. Evolution has basically played with the syntax. And every regulatory element is essentially has a syntax that recruits or has the ability to recruit specific combinations of proteins. And these specific combinations of proteins will be expressed in specific cell types and they will bind specific elements and turn genes on or off that are relevant to that cell's function. Okay, so we really want to dissect this. Okay, so we really want to dissect this underlying syntax. We want to understand the code of these control elements, how evolution has given rise to this kind of syntax that drives function. And so we rely on these molecular profiling experiments. And as I mentioned before, they capture shadows of these proteins. Basically, you can use sequencing experiments to capture these kinds of the shadows kind of look like this. You get signals on either side of the protein. Either side of the protein event. So let's say this protein is bound this location, you get these kind of beautiful single base resolution profiles of sequencing read signals. And sometimes they have different, you know, they have asymmetries on the two strands of DNA. Sometimes they are unstranded, but they have these kind of, here's an example of a shadow of a protein in another kind of experiment called DNA accessibility. You know, where the protein binds, you see kind of a valley. You see kind of a valley of signal, and right next to the protein, you see these spikes. And the precise shapes of the spikes, the precise spans of them, the precise structure of these profiles really encodes information about the protein DNA contacts. And so we really want to model this data at very high resolution as a function of sequence. So there are hundreds of these kinds of experiments performed by various consortia, as I mentioned before. And our goal is to try to use this information and these Try to use this information, these beautiful dynamic molecular maps of protein binding events to discover the underlying syntax. Okay, so how do we make this into a machine learning problem? Well, we can take any such experiment. Let's say we have binding data for some protein in some cell type. We get, as I mentioned, a genome-wide readout at every base, and there's a real-valued signal at every base, which is basically a discrete count of sequencing reads. You can take the genome. You can take the genome and think of just basically binning it into little chunks of, let's say, a thousand base pairs. And you basically end up having millions of these chunks, right? Each of them is thousand base pairs of sequence. And the sequencing experiment gives you a quantitative readout corresponding to each of those sequences. So basically, it becomes a classic classification or regression task where the input is sequences, millions of sequences, and you're trying to learn a model that can map these sequences. Model that can map these sequences to binary or continuous labels, and the labels are coming from the experiment, right? That's the sort of the likelihood of binding or the number of sequencing reads corresponding to biochemical activity. And so, of course, there have been many, many models built in this space over many years. We have built a new class of models. These are like, think of them like sequence to sequence models or what we call sequence to profile models. Our goal is to walk across the genome. Our goal is to walk across a genome, 3 billion basis. And this is a convolutional neural network. I'm not going to spend too much time on the architecture. There's nothing special here. These are dilated convolutions with residual connections. But the basic idea is it's a sequence-to-profile model. And the goal is to walk across a genome. The model has a receptive feed of about 3000 base pairs. And it walks across the genome and tries. And it walks across the genome and tries to map every base in the sequence to these profiles, these count profiles of reads at single base resolution. And we use basically a multinomial loss function that models the distribution of read counts across a sequence. And we fit the model using classic, you know, nothing special, PyTorch, Keras stuff. Now, the key trick is once you fit this model, what can you do? Is once you fit this model, what can you do with it? And we've built many tools to try to understand how the model is able to make these fits and how it's able to make these predictions. And these are pretty standard too. We've taken classic approaches and built on top of them. So one approach we have is if the model is able to make very accurate predictions, maybe we want to understand the attribution assigned to every base in the sequence as to how it's giving rise to this accurate prediction. So we've built feature attribution methods like deep learning. Feature attribution methods like deep lift, fast ice, and uso. Deep lift is an accurate and efficient back propagation method, kind of like saliency maps. It uses difference from, it uses a finite difference approach instead of gradients. So the idea is you're trying to get an estimate of the importance of every base in the sequence relative to a predicted output. And we have some nice ways of computing this for profile outputs, summarizing that. But basically, it's simple back propagation. Basically, it's simple backpropagation. You backpropagate a message that is equivalent to gradients, but instead of infinitesimal effects, it is looking at finite difference effects relative to some reference activation of every neuron and every layer. But in the end, what you end up getting is an additive decomposition of the prediction of the model into base resolution contribution scores. And so that allows us to highlight important basis sub-sequences in any portion of the model. In any portion of the genome with respect to some specific readout in a specific cell type. What we can then do, so this is very useful if you want to interpret individual sequences. We might want to summarize what are the global patterns being learned by the model across the entire genome for a specific experiment. So we've built a distillation tool called TF Modisco. It's a motif discovery tool. What it does is it uses this deep lift algorithm or any feature attribution method. You get base resolution important scores for. Resolution importance scores for every base in the genome, and of course, most of the genome isn't doing anything, so the importance scores end up being very low. So, we just throw away all of those pieces of the sequence. You end up with these little sequits. These are sub-sequences of high importance. And then we have a clustering algorithm that uses basically graph-based clustering like Leyden or Loewen clustering with novel distance metrics designed for these kinds of. Distance metrics designed for these kinds of like continuous sequences with continuous scores assigned to basis. But the basic idea is you compute similarities, you align these sub-sequences, and then you cluster them and you collapse them into non-redundant motif representations. It's a very powerful way to take the millions of parameters of the neural network and then collapse them into non-redundant patterns, global patterns that the model has learned. It's a much better option. It's a much better alternative to interpretation of filters and visualization. There are many advantages to this, and we've showcased this in several recent papers. Okay. So I'll just show you what we actually get on real data. Our models are actually extremely accurate. So these base resolution models that we published last year, what we do is we train the models on a subset of chromosomes and we predict on another subset. And I'm showing you some specific loci here. Some specific loci here. These are four proteins that bind some regulatory control element. And these are the observed profiles, and these are the predicted profiles. You can see in many cases, you cannot make out the difference. At single base per resolution, they are extremely, extremely accurate. And they can also actually denoise data. Sometimes if data is pretty sparse in some regions, you get pretty good denoising. We've quantified this genome-wide, of course. These are cherry, you know, these are not cherry-picked examples, but these are two specific control elements. But these are two specific control elements in the genome. Biologists typically like to see the actual data, so we like to show these vignettes. But then we've quantified actual performance across entire chromosomes, held out chromosomes at different resolutions. And this is the Jensen-Channon divergence between the observed and predicted profiles at different resolutions. And the blue curves are showing you concordance between replicate experiments. If you do the experiment twice and you computed similarities, Computed similarities, what would be the Jensen-Shannon divergence between those profiles? And the red curves are showing you the same metric, but for the observed versus predicted profiles from the models. And you can see in many cases, they are extremely close to the replicates, right? So we are really kind of maxing out prediction performance, which is great. So now we've got extremely accurate models, but we want to actually understand what they're doing. So now we can take these models, we can make predictions for any element in the genome. For any element in the genome, here I'm showing you some shadows of these four proteins. And we can interpret, for example, we can take deep lift and get attribution scores with respect to each of these four models. So what you see is the same piece of the genome is being differently interpreted by different proteins. You can see this DNA word right here, which is used by all four proteins, but then you have this word, which is specific to this protein, this word specific to this protein. This beautiful context-specific usage of different parts of the sequence by different proteins. Paths of the sequence by different proteins, and the neural network can highlight at single-base resolution what these are. And of course, we can map these to known DNA words, and we can get really nice high-resolution annotations of the cominatorics of different DNA words driving different proteins in different cell types. We can also run this modisco algorithm across the entire genome for all of these proteins. And what we find is the recognition codes for these proteins are actually very complex. So, we're only modeling four proteins. So, we're only modeling four proteins in one cell type. And you might think, or at least traditionally, the approach has been to think that each protein has a unique recognition sequence. So, you might need like maybe four to eight sequence motifs to describe the recognition codes for these four proteins. What we find is you need actually close to 50 quite distinct motifs to explain binding of just four transcription factors, four proteins in one cell type, right? Now, there are 6,000. Right now, there are 6,000 proteins that potentially have regulatory function that can recognize DNA sequences, and they bind in a combinatorial fashion. You can imagine the explosion, the actual complexity of the recognition, context-specific recognition sequences that combinations of proteins will produce, giving rise to the immense diversity of cell types and tissues that we see in the human body. And just to show you, you know, each of these are different DNA words, two different representations. Words, two different representations, and these are the contribution of these DNA words to these four different proteins. You can see very interesting quantitative combinatorics. Like this word is very important for binding of these three proteins. You know, this word is specifically important only for nanob. This one is, there are two different binding modes for nanob. And so there's lots of interesting novel biology here. I don't have thrown to spend too much time on that because that's not really the focus of this meeting. Of this meeting. But just to show you that, you know, the neural network can demo discover a lot of these beautiful recognition codes and the underlying interpretation of each sequence in a context-specific way can reveal really exciting, interesting information about the underlying encoding in the sequence. Now, we are actually interested in syntax, not just like annotation. So far, I've shown you. You know, annotation. So, so far, I've shown you annotation, right? What we really want to understand is the underlying syntax, and by that, I mean, like, you know, for example, higher order properties of these sequences. So, if I have multiple words, right, are there interesting like spacing constraints? You know, are there specific combinations of words that like to be at certain distances away from each other with certain orientations in certain combinations? Those are the kinds of questions you want to answer. And typically, if you want to understand: To answer, and typically, if you want to understand syntax, like really functional syntax, biologically, you typically do perturbation experiments. So, for example, you can construct reporter sequences, synthetic sequences, and you can embed one DNA word here and one DNA word here. And you can systematically move one of these words towards the other. So, you can construct libraries of sequences where the only difference is the spacing between these DNA words. You can put these libraries into cells and you can actually measure activity. Into cells, and you can actually measure activity. That's one way to, you know, to test syntax biologically. You can also do mutations in the genome in genomic sequences, not using synthetic sequences in the native context, like CRISPR experiments, for example. You can take actual genomic sequences and you can mutate words, right? I can mutate this one, I can mutate this one, I can mutate combinations, and I can again do readouts for different combinatorial perturbations of the sequence. Now we can simulate. Now we can simulate the same thing in silico, right? Like, why not do it? We can basically do the same kinds of experiments using the model as an oracle. You just perform these experiments and the model might be able to tell you really exciting things about these marginalized syntax experiments or genomic perturbation experiments, right? That's what we do. So I'll show you two such examples. Here, we're trying to understand the syntax spacing syntax between these two DNA words. Okay, this DNA word recruits a nano. Okay, this DNA award recruits the nanog protein, this DNA award recruits the octfo protein, and so we're constructing synthetic DNA sequences and we embed nanog at this position, the nanog motif at this position, and now we systematically move the OXOX motif towards nanog. And the neural network in real time is predicting the binding profiles of nanog and ox4, the two proteins, right? You're seeing different response functions. I mean, you're seeing different responses as a function of spacing alone. As a function of spacing alone. So, here on this plot, what we are plotting is this response function. As a function of distance on the x-axis, what is the response in the strength of binding of the nano protein, which is the golden curve, and the octfo protein, which is the red curve? And you see something beautiful pop up. You see this, first of all, very strong asymmetry. Like OC4 really doesn't care about the syntax. It doesn't care where Nanog's position is relative to the Oxorx motif. The OXOX motif. Whereas an anox protein really cares. And as the spacing decreases, you can see an exponential increase in binding. And then, in fact, as you get closer, you start seeing this beautiful periodic behavior, right? There's a periodic spacing that is really liked. The frequency of this period is exactly 10.5 base pairs, which happens to be the helical tone of DNA. And so, what the neural network has learned is that these two DNA words, if they are on the same side of the helical term, If they are on the same side of the helix, that is, if they are multiples of 10.5 away from each other, you see a cooperative effect. That is, the two proteins cooperate and it improves the binding of this nanog protein. If you are on the opposite side of the helix, that is if you are in between the 10 and a half base periodicities, then you anti-cooperate. So you actually see a dramatic reduction in cooperativity, right? So we're seeing a spacing-dependent cooperativity. We're seeing a soft syntax, it's not a hard syntax. Syntax, it's not a hard syntax, there isn't one preferred spacing, there's an exponential increase as a function of distance, and there is a periodic syntax. Right, all of this is beautifully captured by the neural network, and it learns a very fundamental property of DNA, the helical spacing. It also learns another fundamental property of DNA, which is 150 base pairs is one helical turn of DNA. Sorry, one turn of DNA around this nucleosome protein complex. These are very fundamental units of Fundamental units of chromatin and DNA that the neural network has learned by just mapping sequences to profiles. And we can extract it by just using it as an oracle and playing games with what kind of inputs we throw into the data, right? So we're seeing asymmetric cooperativity, spacing dependence, soft syntax. We can do the same thing in the genome to replicate this. Like, you know, this could be some out of distribution artifact of the neural network. We're just throwing in some synthetic sequences. It could imagine anything. Why don't we do this? Anything. Why don't we do this actually for genomic sequences containing the same two motifs? So here's that same enhancer I showed you before: the control element. It contains this motif, which is the binding site for OC4, contains this motif, which is the binding site for Nanog. And you can see again, Nanog really requires these two motifs. OC4 only cares about this motif. So we can simulate what would happen if we deleted this motif or we scrambled it. And you can see the model predicts complete destruction of OC4 binding. Complete destruction of october binding and complete destruction of nanot binding. So you can see an indirect effect of this motif destroying binding of its protein and destroying binding of another protein, nano, right? We can do the reverse perturbation. Now we destroy the nano motif, we scramble it. Now you can see there's no effect on oct4, but there is a pretty decent effect, not a complete destruction of nanog. So you can actually destroy the nanog motif and the oct4 motif can still recruit nanog. Motif can still recruit nanotech, right? Very interesting. Now we can average this across the entire genome, and different enhancers have different spacings. And so we can again plot as a function of spacing what the response would be. This is through in-genome perturbations. And we again see the beautiful asymmetry. You see no effect for OP4. And for NANOV, you see this exponential rise again with the 10 and a half base pair. It's, of course, a little bit weaker because our marginalization isn't as. Because our marginalization isn't as effective. There's lots of other stuff going on in those sequences. And we're just looking at the marginal effect of this perturbation. The marginalization isn't as good as the marginalization of the synthetic sequences, right? So it gives us two orthogonal approaches to study syntax. And we've done this for now, thousands of proteins. There's all kinds of beautiful syntax we can learn. And the most exciting part is now we can design actual genome editing experiments. So what Julia ended up doing in her lab is we used the model. Is we used the model as an oracle and we scored millions of different syntactic perturbations. And these are very expensive experiments and we can't do these in high-throughput. So we really need to get these right. So we ended up doing a lot of different experiments and then we ended up looking for ones where we could design experiments that would actually work. In a sense, you know, you can't, with CRISPR experiments, you can't disrupt any nucleotide you want as yet. There are specific constraints on the kinds of sequences. You know, constraints on the kinds of sequences you can actually perturb in the genome, and so we were able to do use a model to help experimental design. And here's one example: this is a control element that has a SOX2 motif and an anode motif. Here's the SOX2 motif. That's one protein. This is the bile-type sequence. Okay, and this is the prediction of the model. This is the observed data. Again, these are on test sequences that the model has not been trained on. Now, the model says. Now, the model says if you mutate this TT to an AG, you should see a very strong effect locally and a reasonable effect a little bit far away. So you're seeing a spacing effect, right? We perform the actual genome editing experiment, and that's what we see. We see like complete destruction right proximally at the site, but we see attenuation at this shadow as well. This confirms like a medium-range SOX2, SOX2 cooperative. SOX2-SOX2 cooperative effect. Now, what we can do is for the same mutation, now we can measure binding of the other protein, nanog. And again, our predictions match up very nicely. So you can see if you break the nanog, SOX2 motif, it actually affects nanog over a wide range of distance. Now we can invert it and actually mutate the nanog motif and see what happens. And what we see here is on the nanode binding, you see again local effect, very short range cooperative. Very short-range cooperativity. And then, if you mutate the nano-motif and look at effects on SOX2, the model predicts no effect, and the experiment shows no effect either. So, we get very nice single-base pair resolution validation of this underlying syntax. So, in the last three minutes, I'll just show you how we can address the second question, which is deciphering genetic variants. Again, we can use a model as an oracle. If our goal is to interpret a genetic variant, we can A genetic variant, we can take a sequence containing one version of the variant. This is what the model predicts for binding of a specific protein. We can now switch the allele, we can switch from C to G, push this prediction through the model again. The model now predicts this beautiful amplification of the profile at single base pair resolution. So, here the model predicts a very strong effect. This happens to be a very well experimentally validated variant. People have performed actual experiments to show this. Performed actual experiments to show this is in fact the case. But we don't have to stop there. We can again use interpretation to understand why the model is making this prediction, what is mechanistically happening in the sequence. And here, what the model tells us is when you have the C version of the variant, you have this weak DNA word that drives the blue profile. And when you switch the C to a G, it produces this really strong DNA word. And this happens to be the binding site, the binding motif of the SPI-1 transcription factor. SPI-1 transcription collector, SPI-1 protein, and these two motifs collaborate and cooperate to give rise to this gigantic increase in binding. So we can get really nice hypotheses about how variants work. You can apply this to Alzheimer's disease, all of these different genomic loci, to figure out within these towers of variants, which ones are functional. So, one issue with genome-wide association studies is while they can give you associations of the variant with the disease, when variants are in Disease, when variants are in very close proximity to each other, they get co-inherited. And so, when you look at association statistics, they look almost identical. That is, all variants that are in a block, in a close by block, get co-inherited. Their frequency patterns in cases and controls are almost identical just statistically due to the fact that they are co-inherited. Maybe just one of them is actually causal. The other just happened to tag along, right? So, you can't figure out which variant is actually causal. That's the reason you see these towers of variants. The reason you see these towers of variants, it's not just one variant, you see like blocks of variants showing high associations. So, we can try to use a model to figure out exactly what's going on. And of course, Alzheimer's is a disease of the brain. It's a neurodegenerative disorder, and there are hundreds of cell types in the brain. So how do we actually dissect these cell types, obtain molecular maps, and then use these models to predict effects of these mutations and variants in these loci? So for that, we leverage single. So, for that, we leverage single-cell data. And what that allows us to do is obtain these kinds of molecular profiles again at single-cell resolution across the entire genome for all cells in the brain. We can then perform cell-cell similarity maps. So you can imagine performing like global similarity scores or distances between cells. We can cluster cells in this activity space, and we can project this into a UMAP, two-dimensional UMAP. And you can start. And you can start when you cluster these cells, you actually can denote or discover beautiful cell populations capturing different cell types. So you have microglia, you have oligodendrocytes. So you can denote or take the single cell data and discover novel cell types, right? Or cell types at very high resolution. We then build pseudo-bulk profiles for each of these cell types. That is, we collapse the cells belonging to each cell type cluster. We get these, again, beautiful aggregate profiles. Aggregate profiles. We fit neural networks mapping sequence to profiles for each of these cell types. And then we propagate all the variants that were identified in the genome-wide association study through these models. There's one model for every cell type. We take all the thousands of variants, propagate them, and the model will predict whether that variant has a strong effect or not. And so, here's an example of how we can do it for one such locus. This is a locus that's associated with Alzheimer's. These are the, there are 165 variants in this locus that. 165 variants in this locus that are all very strongly correlated statistically. It's unclear which one is the causal one. We can overlap these variants with the annotations from the data itself and narrow them down to 24. We can pick these 24 variants and score them through the models for each of the cell types. And we get one variant. We can perform experiments and show that they actually, you know, this variant happens to be in a control element that is specifically active in oligodendrocytes. It loops. Olikodendrocytes, it loops in three dimensions to regulate the expression of the PICAM gene. And when we use a model's interpretation tools to understand what the variant is doing, it disrupts the binding site of a protein called FOS. You can see the A to the G switch creates this DNA word. And so we can get really beautiful interpretations like genetic variant blah blah blah disrupts the sequence motif of this FOS protein in a control element in of the PICAM gene in all. Of the PICAM gene in oligodendrocytes in the brain, right? So we can actually develop very detailed hypotheses about the function of these variants. I'll skip this in the interest of time, but we've done this also for de novo mutations in congenital heart disease patients using, again, single-cell data. And we've, in fact, performed CRISPR experiments recently validating these predictions, and we get very nice results. So I'll just summarize there. Hopefully, I showed you that large-scale growth molecule. Showed you that large-scale molecular profiling data sets can help you help us decipher genome function by leveraging them to train neural networks that can accurately map DNA sequences to molecular profiles at single base resolution with unquestioned accuracy. We can use simple interpretation tools to decipher functional DNA letters, words, syntax by using the models as oracles. The models can be used again to decipher disease-associated mutations and variants. They can be used to actually They can be used to actually design genome editing experiments, and they're starting to provide very interesting clues for downstream potential therapeutic interventions. And with that, I'll just thank my lab. These are fantastic lab members who've done all of this work over many, many years. And as I mentioned before, none of this is possible without awesome collaborators who generate beautiful data, validate all our findings. And I really appreciate the collaborative spirit that they have. Collaborative spirit that they have, and of course, the consortia you're a part of and the funding sources. Thanks. Thank you very much for your talk. So do we have questions in the room? Okay, thank you for the amazing talk. So I have a bit of a technical question. So you train your six So, you train your system on sequences from an accessible region, regulatory accessible region. But then you use this model to scan the entire genome. So, do you think that this can introduce some kind of sequence bias of some sort? Yeah, great question. So, you're absolutely right. I mean, how you train the models really matter. So, we don't only train on the accessible regions, we actually train genome-wide. So, the model gets to see already. So, the model gets to see already chromosome-wide. So, the model definitely gets to see signal regions and non-signal regions. And that's very important because otherwise, if you only train on the accessible sites, the model doesn't know how to behave in the background of the genome, which is very diverse, right? And so, there are very important tricks for how to train the model in terms, really, in terms of data selection and how you also account for the class imbalance. Like, you've got millions of background regions. You know, millions of background regions and a few hundreds of thousands of foreground. So, all of that is very important. But once you do that, the models get quite good at predicting effects inside and outside regulated regions. But more importantly, maybe the question you're asking is we're training on only one type of data, right? And that's correct. So, we cannot, for example, if we train on accessible regions, we cannot predict effects on splicing, we cannot predict effects on Predict effects on polyagnolation or other kinds of molecular events. But the nice thing is the same approach, we've now been applying it to all other kinds of data, right? We're applying it to RNA binding proteins, we're applying it to similar polyadination sequences, polyadination data, applying it to splicing data. The same ideas work quite well. So as long as you're training your models on a wide range of modeling. Models on a wide range of molecular readouts with all the caveats of designing your models for those specific types of readouts, changing your receptive fields, changing how you train the data, conceptually the same idea works quite well, actually. So we've got had good success with polyadenylation. We've had good success with protein binding accessibility. Our focus has largely been on transcriptional regulation, but similar approaches can work quite well in our. Can work quite well in other settings as well. Another question is about like you showed very nicely that there is an effect in the linear distances of cooperating transcription factors. I was wondering if you think of applying this type of approach also considering the 3D structure of chromatin, if the data is available. Yes, yes, absolutely. So right now we are focusing on local effects. local effects and