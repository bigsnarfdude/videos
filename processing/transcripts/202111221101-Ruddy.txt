You know, it's nice to see a bunch of familiar faces. I wish I could be there. Nice to also see some familiar names, even if I don't have faces to those yet. I think I remember in grad school, Irina gave me Peter Haydn's red book to learn about moving frames. It's my first exposure to it. So really fun conference idea and really glad to be speaking here. So today I'll, as Francis mentioned, I'll be talking about rigid motion variance of curves through iterated integrals. And the method we used to construct these invariants was the moving freight method. So I'm currently in the data science department at University of San Francisco. So that's why you'll be seeing a Google Slides rather than Beamer presentation to complete my transition into data science. So what I'll talk about today, I'll start by describing sort of the main object of the talk. So the thing we're invariantizing. So these thing we're invariantizing. So these iterated integrals of curves. Then a little bit about the method we use to produce invariance and how we sort of construct an explicit invariantization of this object with respect to a particular group action. And then I'll end with some examples. So please feel free to stop me at any time. Last time I gave this talk, it was like 25 minutes and I was talking at the speed of light. So I'd much rather to go slow and be. Rather to go slow and be interrupted for questions this time around. So, the object we'll be concerned about is iterated integrals of curves or also I'll say paths alternatively. So when I say a path or a curve, I mean this object right here, right? So we have a map from the close interval zero to one to R2. So something with a start point and an end point, right? And I'll also be considering curves, not just in R2, but in principle. Not just in R2, but in principle, curves, you know, in any n-dimensional real space. And some of the things we talk about today can be extended to sort of less smooth curves, but I'm going to stick with using smooth curves in R2 as kind of like the guiding intuition for what we're doing. But just be aware these can be sort of extended to, you know, rough paths, as they call it in the iterated integral community. So, in principle, why are we looking at these paths? So, these paths, in reality, might represent some continuous sequential data. Maybe you have some time series going on with n features. Maybe you're looking at a drawing in R2. So you have someone drawing some sort of path and you want to examine this drawing. Or you have the path of some object navigating space in three-dimensional world. So, what we want from these types of objects are geometrically relevant features, right? Because this is an infinite-dimensional object, right? I have some path, it's one-dimensional, but in reality, what I want is sort of like a finite vector of features maybe from this infinite dimensional object. And I want those features to be sort of maybe geometrically meaningful, right? So, for instance, So, for instance, I could evaluate a bunch of Euclidean invariants on the curve. I could, like, you know, make a mesh of what the curvature is at each point, compute it, get a list of these such values, and this would be a finite vector of geometric features I've obtained from the curve. And the reason I might want to do this is because it might be helpful if I want to do some sort of learning or modeling. Generally, these things take as input finite dimensions. Things take as input finite-dimensional objects. And so, my path or curve represented in this way isn't really a good input data for these types of algorithms, right? If I want to sort of like predict something from the curve, like maybe classify curves into one of five classes, then I'm not going to use the sort of x of t, y of t as like input into some algorithm for classification, right? I'm going to use some vector of features to do this. Vector of features to do this. So I could be doing shape analysis or this sort of fun example of human activity recognition. So what is that? So in human activity recognition, you might have sort of sensors on different parts of someone's body, think like a Fitbit. I think I have Fitbit on right now, right? So it's tracking my movement in space, perhaps. And then the person is doing some action, right? So you might have sensors attached to different parts of their body. Sensors attached to different parts of their body, and maybe they're running, right? And as they run, or jump, or swim, or do whatever, they're creating paths in R3, each one of these sensors, right? So I have a bunch of sort of curves or time series data, however you want to think about it, in R3 representing this person's movement, right? I might want to classify them based on this data. So we'll be looking at sort of one option for turning curves into finite vectors. Curves into finite vectors of features. And this is using iterated integrals. So we use iterated integrals to sort of project from this infinite dimensional object into that finite vector of values that hopefully contain some useful information for modeling or prediction or something like that. So there's a nice text here with some references on how what's also called the signature method. So this is called the iterated integral signature of the curve. integral signature of the curve, no relation to the differential signature, except in nomenclature. But this is a really nice work on using the sort of geometric features one obtains from iterated integrals on paths to do some machine learning and see some success. I will note here that this isn't like the cutting edge of machine learning in time series. I think cutting edge stuff is really kind of very uninterpretable at the moment. Very uninterpretable at the moment. So, I think the sort of motivation and goal behind using these methods is to come up with alternatives that have a little bit more grounding in interpretable kind of features and things like that. Yeah, any questions at this point, by the way? Okay, cool. Okay, cool. So, what are these iterated integrals of a path that I keep mentioning? So, suppose we have, again, our nice path in R2 represented by this. So, x of t, y of t here. So for a path in the plane, these would be some iterated integrals, right? So I have the integral from 0 to 1 with respect to x of t, the integral from 0 to 1 with respect to y of t, or I can integrate with respect to x and then y, y then x, or with respect. Y than x, or with respect to x twice. And I can sort of do these over and over again. So all the different combinations of these iterated integrals. So we iteratedly take the integral along this path. And all these different combinations make up the iterated integral signature, right? And we usually sort of collect these values in these ways. So these are like the first level values, right? So I take one integral with respect to either path, the second level values here. Second level values here, I take sort of two iterated integrals with respect to different combinations, and then three, and then so on, right? And so we sort of represent it in this way via numbers here. So one and two here represent x and y. Oh, awesome. You only integrate exact differentials or you can integrate other things. Sorry, say that again? Do you integrate all the exact Do you integrate all the exact forms or you can integrate other things? So here in theory, I have some sort of functions representing x of t, y of t, right? And I do sort of, I can integrate this just directly as shown. Okay, so yes, so you can integrate these things, right? But Things, right? But you don't actually need to. So, this is sort of, I'll show in another slide, is that this is enough to sort of characterize the whole curve. So, yes, one can sort of consider those integral values and like, you know, think about that space of features, but this is simply just a choice one is making. So, the choice is to consider these integrals over, you know, some other integrals. Is that kind of what you were going for there? You were going for there, okay? So, like, if you do a US dot example, then actually y of r dt appears there. Yes. So, so, okay, it's just like one step down. Oh, I see, I see, yes, yes, yeah. I mean, I guess I was interpreting it as also like, which is, which is a different question that I will answer, but there are other types of integrals that you can do along the curve, right? Do along the curve, right? And these are like better or worse. This is simply like a choice which gives you some nice statements that you can do. You talk about orthogonal groups, right? And one of the integrals you may think about like elastic, right? Integral of kappa square dt. That's one you live about, right? Sorry? One thing what? Integral of the square of the geometry is a thing you allow. Is this thing you allow in the late integration? So, no, we actually don't take integrals of specific. So, this is an interesting question. So, we don't consider sort of integrals of differential invariance along the curve. There are some relationships between the invariants we will construct in this integral space and the differential invariance, so that if you were to differentiate some of these invariants many times, some different Many times, some differential invariants might kind of pop out if you do it in this, you know, differentiate in an invariant way. But that's sort of exploratory. I don't want to make any definitive statements about the relationships there. But here we're just considering these type integrals and the sort of space of values that you get for a particular curve and looking at constructing invariance in that space. I think Evelyn has her hand up in Zoom. Yeah, I don't understand what is the role of R to me, but it looks like R is one or is it? So we integrate first with respect to T here, and R is just a dummy variable. So we first integrate with respect to T. So you could think about this as the integral from 0 to R of X prime of T dt, right? And then you get a function. And then you get a function with respect to R, and then you integrate with respect to R. So the R here is kind of a dummy variable because we want one value. So Michael, essentially, you do what Boris said, Maris said, if you do X, so second integral is just X T Y, right? From 0. Yeah. So, you know, it's the, you know, X, yeah, X of R minus X of Z. Yeah, x of r minus x of zero, kind of, yeah. But couldn't make it sufficiently magnificent. Yeah, so why not go just like okay? All right, great questions, by the way. Anything else? Okay. Okay, cool. So basically, from these integrals, right, we turn a curve from a curve into this infinite sequence of values. Into this infinite sequence of values, right? You know, in various ways, we just get this infinite sequence and sort of arranged in these nice levels, right? Level one, level two, level three, and so on and so forth. All the different combinations one can construct. But why, right? So why look at these integrals as opposed to some other integrals? Well, there's a nice characterization of the path given by this infinite sequence, right? So in 54. So in 54, Chen showed that this infinite sequence was a characterization of smooth paths up to translation and tree-like extensions. So if I translate the path, I don't change the iterated integral signature. So that's what we call this infinite sequence, the IIS or iterated integral signature. So translation does not change the signature here, and neither does these what are called tree-like extensions. So if I go out, I trace my path. I trace my path and then I trace it back the same way and I continue along. These two things are basically equivalent. So they characterize paths up to these two types of transformations. And in recent times, Hamblian lines have extended this to not just smooth paths, but more irregular paths as well. So this is kind of like a nice representation in some point or a curve. So we translate it from, you know, a representation of perhaps, you know, Representation of perhaps, you know, using, you know, t squared, t cubed into a representation of infinite sequence of values, right? And I should also know it's parametrization invariant. If I sort of choose a different parameterization of the curve, it also doesn't change the iterated integral signature. And there is kind of a notion of convergence, which I don't want to get too into the weeds there, but it kind of means that paths which share the first, you know, Which share the first, you know, x-many terms of the iterated integral signature are close in some way. So, if I were to take two paths with the same IIS up to a certain point, there is some notion of those paths being close together. So, this kind of representation of a curve is nice in not just the fact that it's unique, but it has some kind of nice properties as well. So, that's why we kind of look at this representation. At this representation, um, and so by truncating a given at a given level, we get a finite representation of our curve, right? Um, so instead of the infinite sequence, we can pick some level and we get a finite vector kind of representing the curve in some way. Obviously, we no longer have uniqueness of the curve once we truncate there, but we do have some geometric features contained within. So, for instance, just looking at first and second level stuff, we can see. First and second level stuff, we can see some geometric things pop out. So, here, if I take the second level truncation of a two-dimensional curve, I get a vector with six values here, right? So the one, two, one, two, two, one, one, one, and two, two. Okay. And even at this low level, we have some nice geometric features, right? So it's easy to see that one and two simply represent kind of the starting and end points of the path. The starting and end points of the path, or the chord going from the start point to the end point here, right? So, if I were to evaluate the integrals, we just get the simple functions, you know, x of one minus x of zero and y of one minus y of zero. So, if this were to be at the origin, one and two would simply describe the x and y value of the endpoint of the curve. And in the second level, we can sort of tease out a nice geometric invariant. So, this combination of entries gives us what's called. Combination of entries gives us what's called the levee area of a path. So, this is the area if I were to draw a chord from the start point to the end point, and I were to sort of label things above the chord as positive and below the chord negative. So these, the sum of these areas together would be what's known as the Levy area. So, in the second level, we have some sort of geometric things hidden, right? So, in this way, we can, following the theme, we go from this infinite curve to a finite. From this infinite curve to a finite vector containing some nice information about the path. Okay, any other questions here? All right, so note that I mentioned the orthogonal group and I said some nice functions of part of the path. These functions were Euclidean invariance. were Euclidean invariance, so invariance with respect to rigid motions, right? Because the path, the iterated integral signature is translation invariant, we can sort of, you know, I'll sort of kind of move between Euclidean and orthogonal with a bit of, you know, equivalency, right? So when I think, when I say orthogonal invariance in the IIS, I'm thinking about sort of the orthogonal invariant Sort of the orthogonal action because the whole all the values are translation invariant. And the reason again we look at these invariants is because some of the previous applications I mentioned, rigid motion invariants, you know, might be nice, right? So if I'm looking at, you know, human activity recognition, maybe I don't care specifically about the orientation of the person. I just care about the raw action they're doing, right? I don't care if they're doing things at an incline. Doing things at an incline, I want some sort of, you know, rigid motion invariant features from a given action in R3, right? Or if I'm doing shape analysis on some shape, I also might want rigid motion invariants. So this is one of the main motivating questions here. We have a nice representation of our paths, right? You know, someone told me they're very useful for something else, and they told me that rigid motion invariance might also be useful. So my first inclination is, well, in this. My first inclination is: well, in this representation space, let's find some invariance of paths. So, in particular, we can ask questions like, you know, what does the space of integral invariance look like for a particular action? So, for polynomial invariance, Yoshidiel and Jeremy Reisenstein showed that all these invariants, all general linear action invariants, polynomial invariants, can be constructed using some nice classic. Constructed using some nice classical invariant theory methods. This is a very nice, fun paper to read, in my opinion. And so, for instance, here are the lower-level polynomial invariants for paths in R2, right? So, we see our old friend the levee area here. And I think this, I forget how to geometrically interpret 1,1 plus 2, 2, but it means something. And then here are the sort of other polynomial invariants. And you'll notice that they all look very linear. They're they all look very linear, um, but that's because they're all they're all still polynomial invariants, it's just that these uh entries in the IAS satisfy certain relationships, okay? So these entries are actually not totally independent, they satisfy what are called shuffle relationships. Um, so for instance, if I multiply the values of a curve for one and one, two, and I multiply these together, this will always equal the value one, one, two plus. equal the value 112 plus 112 plus 121 on a curve. And from this, in this way, we can always go from polynomial invariance to linear invariance. But the main point here is that there is some redundancy between all of these invariants here, right? So I can construct all linear invariants in some nice, concrete way, but there's a ton of redundancy, interdependencies between them. So the goal of this project was So, the goal of this project was to kind of do the following, right? I want to describe a minimal functionally independent set of invariants, perhaps for each truncation level of the IIS. So for instance, you know, a priori, I might say, I'm going to use the 10th level truncation to represent my curve, right? How do I invariantize this curve? How do I construct invariance of my curve in this space? You might guess the method we're going to use by the name of the conference, but we'll get there in a second. Conference, but we'll get there in a second. Ambitiously, we kind of wanted to look at the general linear group and all of its subgroups, but we sort of settled on orthogonal invariance as like a starting point for constructing invariance. So we specifically look at the orthogonal actions or rotations and reflections on the ambient space of a curve and the action that induces on the IIS here. And again, we just look at rotations and reflections because the curve is already inherently. The curve is already inherently translationally invariant. Okay, so our goal here: we want to describe some invariance and characterize the equivalence class of the curves IIS under this action. So I'll pause again. Any other questions? All right. So to do this, we use the FellsOver moving fray method. You know, I'll outline some of the key terms very quickly here. But you know, it's going to follow the same basic premise as the first example you might have seen of the moving frame method in R2, right? So here I have orbits of the rotation action in the plane, and I can create a cross-section for this action. So a linear space here that intersects each orbit. Space here that intersects each orbit exactly once. And from this cross-section, I can construct my moving frame map. So a map that simply takes a point in and spits out the group element, taking each point to the cross-section here. So for the rotation action, we have a nice sort of rotation here that rotates each point to its intersection with the positive y-axis. So using the two ingredients of a cross-section and moving frame, we can construct invariance for our action. Construct invariance for our action. So, again, this means that two points are equal if and only they have the same cross-section representative. And thus, the functions we obtain by taking in a point and outputting the value are sort of our invariants. And if we sort of construct all of them, it's a minimal functionally sort of independent set of invariants that sort of generates all of them in some smooth way. So, in this sense, our cross-section is kind of like. Sense our cross-section is kind of like our choice of canonical form in some sense for our points. But the thing that's really nice about this and about the cross-section moving frame map is that to evaluate the invariance for a particular point, right, you actually don't need the invariantized formulas, just the moving frame map. So this is a nice feature that we'll sort of take advantage of, right? So let's say I don't know what the invariants are for this action. I simply want to know what is the value of a point on the cross-section. Well, I can just use the move. The cross-section. Well, I can just use the moving frame map for that. I don't have to know the formulas for my invariance. And this is nice because oftentimes for very complicated actions, if I were to actually compute the invariance with respect to a cross-section, it'll be horrible, horrible functions that I never want to look at with my own two eyes, right? But I may want to know what are the values of these functions for a particular curve. So that's kind of one of the themes here that I want to emphasize. Okay, so what's the action we're considering? We're considering the action of rotations, reflections on the ambient space of a curve. And we want features of this curve such that they don't change from C to C prime, right? So I want the features I obtain to be the same for this curve as they are for this curve. So I'll consider actions of orthogonal matrices of D dimension for curves that lie in R to D. So this action. So, this action on the ambient space of a curve induces kind of what's called a joint action on the IIS of a curve, right? So, by a joint action, I mean that the output of the action on each level only depends on the entries at that level. So, for instance, if I want to know, you know, if I rotate a curve and I want to know what the resulting curves, first level features are, well, they're only going to depend on the Going to depend on the features from the original curves first level. And same thing for the second level. If I want to know what are the second level features for a curve after it's been rotated, I simply have to look at the action on the second level features of a curve. So in some sense, this rotation action is acting on each level kind of independently in some sense. This is why we call it kind of a joint action. And so our goal will be to find across. So our goal will be to find a cross-section for the orbits of this action. However, this space is kind of difficult because these elements are not independent. You remember that there are shuffle relations between all of these elements. So, you know, 1, 1, and 2, 2 depend on the values for 1 and 2. And then the fifth level values can be obtained by doing things with the lower level values. So it's kind of a very Level values. So it's kind of a very complicated space. So, what we actually do is we actually look at kind of a further level down. So, we pass, so we sort of start with a curve, we go to its IAS, and then we pass its IAS through what's called the log transform. So, this is just a transformation taking our space from a place with a lot of interdependencies to a space where we have values of the curve which are wholly independent of one another. So these elements are indexed what are called Linden words, which is some combinatorial restriction on the combinations of ones and twos that you can use. So what's nice here is that we pass to something with no interdependencies, and then this is a bijection. So it's easy to pass, take a curve, pass to its IAS, and go to its log IAS and its IAS and back. So there's a nice map sort of going from here to there, and you can sort of seamlessly. And can sort of seamlessly travel between one and the other. So, for instance, if we truncate at that second level, we go from that six-dimensional feature vector to a three-dimensional one. So, there are some interdependencies here between these values, but I can sort of compress them into a three-dimensional vector of features, which are totally independent from one another. So, basically, it's kind of like the image of all curves of the map I. Of the map IIS of C is a three-dimensional output. So, in the space of, if I were to sort of look at all curves, map them through the IIS into R6, what I'd get is a three-dimensional space, and the log transform sort of like transforms, sort of compresses that into a linear space. And so, for instance, I don't want to go too far into the map, but for instance, Uh, the map, but for instance, I can get C12 by one two minus two one. Sorry, is there a question? Yeah, I had a question. Is it algorithmic step to slope transforming? Is it algorithmic or is something you have to do by observation? I think it's a lot of, it should be related to integration by parts, right? Yeah, so it is algorithmic. So you can, you can just, so it's difficult, but you can describe it. So for instance, here is the how you would get from How you would get from the I how you would obtain each entry. Okay, let me start over. So it's easy to come up with a formula for how you obtain every entry of the log IIS from the IAS. So for instance, C1 and C2 are simply one and two, and C1, two is given by sort of the bracket between one and two. And these brackets are kind of the key to coming up with how you go from the IAS to the log IAS. So if you were in R3, Log IAS. So if you were in R3, C12 would be bracket one, two, C13 would be bracket one three, and so on. Is it easy to predict like in a complicated situation? What is the dimension? Do you have a formula of a dimension? Like what is the dimension of log space? So if you translate it order two, you get like from six to three dimensional. Order three, do you have a formula for dimension? So yes, it does exist, but it's basically. But it's basically a combinatorial formula based on how many Linden words there are of each length. So there is some formula. I don't know it off the top of my head, but there is some formula if I say, you know, I'm looking at a path in D-dimensional space and I want to look at the nth truncation order. Then I can get the dimension of the log IAS for that value. I don't remember the formula, but it's based on the number of Linden words of each length. In words of each length for that many number of letters, letters being one, two, three, one, two, three, four. Any other questions? Okay, so again, we passed from a curve down to this log IIS space, and now for the action. And now for the action, right? So we're looking at the action of a rotation on the ambient space curve. So I'll denote the output as C tilde here. And we can actually look at the transformation, the sort of induced transformation on the curve in the log IS space. So for instance, if I were to take, rotate a curve and look at the curve C tilde, then the resulting values at the first level, Values at the first level simply change by a nice linear transformation. So the output curve, I can describe the values of the log IIS in terms of the values of the original curve. On the second level, it gets a little bit more complicated. So basically, the idea is you arrange all the second level values in this nice skew symmetric matrix. And then when I transform a curve by this orthogonal action, the resulting values on the second level transform. On the second level, transform in this way. So I multiply, I do this sort of, well, the word escapes me, conjugation action, I think, you know, A times the matrix, A transpose, to get the resulting transformation. So we can sort of describe the action on the first two levels in this very nice way. And you'll see that it's kind of a joint action, right? So the transformation of the first level coordinates only depends on what's going on on the first level. depends on what's going on in the first level. The transformation of the second level coordinates only depends on the second level coordinates of the original curve. So basically, finding a cross section for the action on the log signature space, because the second, first, and second level are sort of big enough to contain the orbits of the orthogonal action, it's actually equivalent to simply find a cross-section for the action of the orthogonal group on the direct product. On the direct product of Rd and skew symmetric matrices for an action that looks like this, right? So basically, I can sort of translate my question to finding a cross-section on the whole log IIS to finding a cross-section simply for this action. That's because A acts jointly. So once I find a cross-section here, it sort of doesn't depend on what's going on at the higher levels. So I can simply look at level one and level two to construct sort of an invariant. To construct sort of an invariantization of the entire log IIS and thus the entire IIS of the curve. So we sort of distilled all of these components to simply apply the moving frame method for this particular action. Yeah, and one thing I'll note here is that we can do this specifically on one and two because the dimension is high enough so that it can contain the orthogonal action. When we looked at some Action. When we looked at some other actions like the projective group action, we actually had to sort of it sort of wasn't big enough to contain those orbits. We had to look at level one, two, and three. And part of the hard part, part of the reason we stuck with just the orthogonal group at first was because the level three action is a mess. So that's for future work there. So to construct a cross-section here, we do Section here. We do a little bit of fancy stuff. I kind of won't, I'll kind of gloss over the fun algebraic and variant theory details here. But the idea is that we construct sort of a relative section in iterative ways, right? So we kind of, you know, zero out a few coordinates, zero out a few more, and iteratively do this. But the sort of unique thing is we iteratively do this over the complex numbers, just so that we can use nice algebraic tools to sort of show that we. Tools to sort of show that we are in fact getting a unique cross-section and sort of describe explicitly the space of coordinates for which don't work for our cross-section. So we do this over C, we translate it into a cross-section over R that works for most curves. And the idea here is that we're sort of able to do this cross-section explicitly. And we get our genericity arguments, meaning that we show that it works. Meaning that we show that it works for most curves in this space via arguments in C. And if you want to taste, this is the explicit cross-section. So we set most of the first level coordinates to zero except for the last one. And then we set most of our second level coordinates to zero, except for that off-diagonal bit of our skew symmetric matrix. And I'll show a pictorial example of this K in a second. And we do so iteratively, right? So we And we do so iteratively, right? So we first sort of bring a point to the cross section in the first level, and then we sort of zero out just a few of the second level's coordinates, and then a few more, and then a few more. And this way we explicitly construct a cross-section for a general action. So it's hard to describe the moving frame map in general, but I can describe an iterative procedure that will work in general for this action. And this is the And this is the cross-section here. So basically, we have this VM space, which is like represented by a vector and a stew symmetric matrix. And every point in the space, besides a very small subset of measure zero, we can bring to an element sort of a canonical form that looks like this. So all the elements here are zero, and then everything here is zero except for that off-diagonal. Okay, so why do we do this? What do we get out of this explicit cross-section? Well, we have an Cross-section. Well, we have a nice canonical form or characterization of the log signature of a path under rotations. So, what this means is that two smooth paths are equivalent up to translations, rotations, reflections, and those tree-like extensions if and only if their log signatures have the same value on the cross-section we've constructed. So, two paths are equivalent up to all these transformations if and only if they have the same value there. And even better, because the action is joint over the different orders, we actually have a characterization of the truncated IIS under each rotation. So, if I pick a particular truncation order, I know that the features in this sort of lower dimensional space are characterized by the values of up to the k order log signature map. And all of this is very explicit. We provide a method of invariantizing particular, any particular curve with respect to the cross. Particular curve with respect to the cross section. So, if you give me a curve, I can give you the map taking it to its cross section on level one and two, meaning I can evaluate all invariants that I want without needing to actually compute their formulas. And I can sort of invariantize these features in this way. Evelyn, I think you have your hand up in Zoom. Yes, thanks. So, I have a first question about the first theorem. So, I mean, the moving frame is a very local construct. Very local construction. So, how do you include reflection there? I mean, where did you kind of get a result for reflection in your construction? Good question. So, when we do this oversea, there is some, we are constructing not cross-section, but relative section. So, you know, we're intersecting our orbit in precisely, I'm not sure, I think, you know, some amount of points, right? So, we have some linear space over C. Space over C. But when we move to R, we consider actually a subset of that linear space. So, like, kind of like, you know, we sort of like take the positive portion of some giant linear space and we're able to show. So we're able to show that this is sort of a relative section for most points via our, you know, sort of moving around oversea using the genericity claims there. Claims there, and we're able to show it's a relative section for most points. And then when we pass to R, we're able to say that, okay, we look at just the positive portion of this giant linear space, and this actually intersects each of those orbits uniquely. So the sort of modding out by rotations basic or reflections kind of comes from some of these greater than zero constraints that we have in our cross-section. Okay. I'll look at the second theorem before I ask a question about it. Sure. So the second theorem is basically the first one, but just that since the action is joint, is that I can kind of consider each truncation level separately. Because the moving frame map only depends on the first two orders, I really only have to concern myself with that space. And I get sort of the invariantization of the rest of the higher levels kind of for free. Of the higher levels, kind of for free, basically. Do I understand correctly that view invariants are set of six numbers? Or it is so, example, you evaluate everything, you get six of how many numbers? Because you compute integral until the end, right? It's not you compute until. It's not a computational key. So the dimension of the invariance depends on the dimension of that ambient space and the orthogonal group. So for instance, here, if I go to this log IIS, if I look at the first two levels, I have a three-dimensional space and my action is one-dimensional. So basically, I get two invariants at this space. So I zero out this value. So I zero out this value and then I make this value positive, and then I have kind of two invariants here. The signature is a curve, or is it what is a signature? What object is a signature? I see. So the signature is just an infinite sequence of values, right? So I go from a curve, and then each of these are, for a particular curve, this is just a sequence of numbers, right? So this is kind of like a representation of a curve in the space. And if I were to sort of In this space. And if I were to sort of truncate at this level, this six-dimensional vector is kind of a finite dimensional vector just sitting in R6, where many curves might map to the same vector. And I'm looking at the action specifically on this space. So for instance, I can compute the log IIS of a curve and then transform the curve and then see what the resulting section. Resulting second-level coordinates are for the resulting curve simply by applying the action to these values for the other curve. No, I understand the difference between this and that signature, but you have something at the end which characterizes the core after rotation, right? What is this? So you have, you say, if two signatures are the same, then two curves are related by rotation. What is this signature? How it looks like? Is it a curve? How it looks like is it a core? Is it a finite set of numbers? Is it an infinite sequence of numbers? What is it? Okay, I see. I think I understand the question now. Yes, so basically it's the so in the first theorem, it is an infinite sequence of numbers again. So it's basically just like the log signature characterizes a curve up to translations and tree-like extensions. We simply take this infinite sequence of numbers, consider the action on it, and then sort of come up with a canonical form. Sort of come up with a canonical form for the log signature under this action. So, what we're left with is the log signature again, but it's been transformed so that some of these entries are zero, which sort of is equivalent to bringing it to some cross-section in the space. Which would be like quantum numerics. And what does the second serum say? So, this just says that I can sort of do this at any truncation level. So, here, this is kind of considering the whole infinite log signature. Whole infinite log signature. But the idea is that I can sort of do this invariantization at any order I want. So I can consider, you know, the space of all, you know, order k log signatures and come up with the invariance just in this space by looking directly at this sort of order k space here. Yeah. And then I would get like a fine sequence of invariants, but when you say C. But would they separate? Would I have two non-equilibrium works for which this finite sequence is the same? Yes. So this truncation turns the log signature from kind of a bijection to, you know, it's not injective anymore, right? You can have two curves which are not equivalent that have the same sort of features when you truncate them. But you have to truncate curves. Yeah, and so it's like a tele series approximation. No, no, no, no, no, television. That's whole story, yes. Do they know how to characterize curves with the same truncated log signature? So that is that's actually like a really nice question. So that's something that I think there was a paper by Bernd and some others a couple years ago. And some others a couple years ago, which investigated this question both from a symbolical and numerical perspective. So, in general, I don't think it's known. Like, I don't think there's some like, you know, all curves up to order K have this relationship. But there are some nice papers investigating like what do curves look like at that have the same features at given truncation orders, but more of an exploratory analysis rather than some sort of generalizable statement. Sort of generalizable statement, if that makes sense. Hey, Michael. Yes. Kind of a related question. I mean, I don't understand this completely, but how do you do like reconstruction for prescribed invariants? Yeah, my question exactly. So that is an open question. The inverse problem is more difficult. So the sort of motivation here is that you're kind of in a setting where I have the curve. Setting where I have the curves and I want to apply some sort of classification algorithm to them, right? And so then I would use these as simply, I'm extracting features from the curve to make some sort of modeling or some sort of prediction on them, right? But the question of, you know, I take some features, right? And I want to see what kind of curves map to those features, that's an open question. And then it makes it even more complicated thinking, okay, not just the features. Thinking, okay, not just the features, but these particular invariant, you know, agreeing on these invariantized features, you know, I guess the simplest way to say it would be, you know, it's whatever curves map to the same features brought to a cross-section, I guess. That's not a satisfactory answer, but it's kind of a these flavor of questions are open and very interesting. My mic back off for integral signatures, right? Without group pattern, there is a reconstruction procedure for these chance signatures, right? Exactly. So if I were to, you know, truncate order three and I look, you know, what curves map to the same third level truncation order? Now, there is a description of, you know, if curves match to some truncation order, there are some like, you know, There are some, like, you know, analytic constraints on how far apart these curves can be. I don't know too much about the details there, but there is some notion of them being close or similar in some way. But as far as which curves, you know, some other sort of description of them, I'm not sure. Okay, so I'll just end with some pictures and then I can take some more questions. pictures and then I can take some more questions. So here is what you know what it kind of looks like in R2. So in R2 it's kind of you know it's kind of silly. This cross section basically amounts to taking the endpoint of the curve and moving it to the y-axis. So I just take C2 and set it equal to zero. And this is equivalent to moving my curve so that it looks like this. And then I might reflect it to make sure that the Levy area is always positive. So I take a curve. Positive, right? So I take a curve, move it to the y-axis, maybe do a reflection if I want the levy area to be positive. And then that's my cross-section in R2, right? And so we'll see that to sort of, you know, these two curves in blue and orange here brought to the cross section match in this way. In R3, there's two steps. So R3 is a little bit more indicative of how it works in general. So again, the first step in our cross-section is always bringing. First step in our cross-section is always bringing the endpoint to some axis. So we chose the last axis just because it made computation a bit easier, right? So we bring the curve to some axis, the last axis, the end point there. And then here we have to do some zeroing out of coordinates on the second level, right? So first thing we start with a curve in blue, we bring it to the axis, so then we get the curve in orange, and then we rotate it around this axis to get the curve in green. Okay. So what is this? Okay, so what is this second rotation? Well, that's sort of making this C13 coordinate zero, which these level two coordinates sort of correspond to the levy area of projections of the curve. So if I take a curve in R3, the level two log signatures can be interpreted as if I project my curve onto the XY plane. I take the levy area. That's C12. I project my curve onto the Y. My curve onto the yz plane. I take the levy area, that's c23. So basically, what we're doing when we're rotating around the z-axis is we're turning, we're forcing the levy area along one of these projections to be zero, right? So that's a geometric interpretation of the cross-section in the lower dimensions. Yeah, so I mean, what's next? Some of these questions are already asked. First, are these helpful in any way for applications? Are these helpful in any way for applications? Right, I talk a big game about you know, these are features that we might want to use for other things. Are they actually going to perform well? Are they actually useful features? These are questions that I think would be interesting to explore. So you take a task, see how well the IAS features perform, maybe invariantized versus non-invariantized, see if that sort of lends some geometric intuition for any model, right? Then, of course, other group actions are of interest and the inverse problem. Um, and the inverse problem, which I don't have listed here, but is also sort of a big one as well. Okay, um, so thank you all for listening for the awesome questions. Um, if there's time, I'll take more questions, chat about whatever. So, thank you all. Any questions for Michael? I do have a question. I'm Evelyn. I'm online. Sounds good. Go ahead, Evan. Yes. So you have a section which is linear once you have the log signature and the log thing. And you have a process to figure out a group element that takes you to this section. Yes. My question is: how do you certify that there is only one group element that takes you to the section? You to the section, and how is actually this section of degree one? I mean, is there actually only one intersection of the orbit with this section you chosen? Yes. I mean, those are two different questions, actually. Yes. So, how do we certify it? Basically, we're able to show that kind of the action is free in some sense. So, it's kind of just a To um, like a nice argument, so for instance, I think it's easiest to look at this second level here, right? Um, so you move, you know, the linear section and the complex space is simply, you know, making this coordinate zero, right? And in the real space, we add the restriction that the levy area is positive. So that's C1, two, you know, greater than or equal to zero. And that's how we sort of get rid of the reflection there. sort of get rid of the reflection there. Basically, the way we certify it is that, you know, if you take a curve in this space, so if you take a curve or sorry, not a curve, but a point VM in this space, which satisfies these constraints, right, and I apply a rotation action to such a point, right? The only way that it can still satisfy these relationships is that A is the identity action. So the only action that So, the only action that moves a point that satisfies these constraints to another point on the cross section is the identity. So, this certifies that it can't go elsewhere. Okay, thanks. To continue that, so, but I'm guessing there are still sort of singular curves. So, if you're levy area zero, you can't specify it to be one sign or the other. So, you still have a reflectional ambiguity. So, that's a good point. Majority. So that's a good point. Yeah, Peter's noted that there's kind of like a big hole here and that the singular curves for this cross section do include, you know, closed curves, right? So I don't consider curves which have the same start and end point, right? You can do a similar construction, like a similar moving frame construction specifically for those curves, but for this cross section, those are considered singular. Are considered singular points, right? Or curves with zero levy area are kind of like singular points for this moving frame map. So those do have to be kind of considered separately. We did some computations for level one and two, and it's really not. So we did it specifically for one and two, because we think that, or sorry, two and three, because that's where closed curves might be the most important, right? So like some sort of shape analysis. So we specifically did a moving frame. So, we specifically did a moving frame for those two cases. So, closed curves in R2 and R3, but we don't have a general result for closed curves in R to the D. I have a question, follow up on that. And actually, there's a question to the audience. Anywhere related to those problematic cases, has anybody explored a method where you would apply several moving frames to the, because you have infinitely any coordinates, right, to deal with. Many coordinates, right, to deal with that you can use. So you take a little piece of it, you invariantize it, and obviously you might run into a problem there. And then you take another piece, you invariantize it as well, and another piece, you invariantize it several times, getting a bunch of different invariants. And that kind of, does that ring a bell to anyone? Or does that have a related problem? No, but it's preserved. There's a water right there. So basically, it's just independent things. Basically, it's just independent things, if I understand. So, so there was a number of CIGT, right? So, it doesn't mix them. Right. So, so you do just it's independent. It's great. I'll probably it needs to cook up a little bit before it becomes more. Do you think you would choose a cross-section of coyote and then low the like? No, I would pick a bunch of different cross-sections. Different cross-sections. Oh, it's the same order of different orders. And then try to link them. I have a specific example. So, okay, I just wanted to quiz quickly if anybody, that's what we're talking about, right? Is it in general like the question is kind of in an abstract sense, you know, relating different sets of invariants you get from different cross-section choices? Yeah. Essentially, that's what I have in a case where I just actually scissors. I have in a case where there's actually syzygies up to the higher order moments or numbers are related to the first ones. I'm using those syzygies to relate the cross-sections. But if that doesn't ring any bell, that's