I'll begin by thanking the organizers for organizing this meeting. As someone who's organized a fair amount of meetings in my career, I understand how much work it is and how well this one's being run. So when I got the invitation from Louie to give this talk, I thought they want me to talk about Monte Carlo. That's something I've been thinking about for 25 years now. I can probably do that. That and then a few weeks after that, I decided to start thinking about preparing the talk. And I realized he wanted me to talk about systematics. And that is something that I knew very little about at that point in time. And at that point, I had a decision to make whether I was going to continue or not, and I decided to, obviously. And so I started reading papers. I did Google searches. Papers. I do Google searches and diving into the literature trying to figure out what was meant by systematics in Monte Carlo. There's not a lot under that title. But after some investigation, I came up with a non-exhaustive list of things that kind of play a role from what I, from my viewpoint, in that literature review. Literature review. One is just model error and how you incorporated it into Monte Carlo or whether you should or not. Nuisance parameters came up a lot, so you get high-dimensional problems where high-dimensional distributions where you really care only about a subset, right? And how does that affect your implementation of Monte Carlo? Maybe your Monte Carlo methods are not producing what you think they are, right? And I don't necessarily mean coding errors. Let's just Necessarily mean coding errors, let's just assume that we all code perfectly the first time through with no bugs in everything that we talked about from here on out. Another issue that came up repeatedly was maybe what I put down here is maybe the Monte Carlo hasn't been run long enough, but I don't even really mean that. However much effort we put in, how good are our results? All right, and then another one that came up a lot, especially Came up a lot, especially in a lot of the articles from the early 90s I found, where everything was great about the Monte Carlo, but they would do things in there that would make use of the output sub-optimally. They would almost induce bias in the results by the methods they use to deal with the output. Well, I am not going to talk about all of this. I'm going to talk about this one. Okay? All right, that's going to be. All right, that's going to be the first part of the talk. Basically, how can we assess whether we've run it long enough, or at least how good our estimation and other features of it are after that amount, after the run. One thing that didn't come up explicitly, but I'm interested in a lot of my research recently, is there's often two sample sizes to think about, right? There's the Monte Carlo sample size, and there's observed data sample size. And those should Signs. And those should both play a role in how we assess our Monte Carlo experiments. And so I'm going to talk about how to, no, I'm going to talk about two little pieces of how you might think about doing it. So statisticians love notation. Fortunately, there's not a lot on this slide. Okay? But capital F here is going to be a distribution we have. It's of interest, right? It's what we're. Of interest, right? It's what we care about. We think about it being pretty complicated. Maybe it has integrals in it that we can't compute, derivatives, and so on. It is what it is. But I'm going to use Monte Carlo to try to learn something about F. That's what I'm going to do. And that's something I'm going to try to learn: theta. Theta. Theta is a vector here. It often consists of things like a mean, variances, medians, quantiles. It could be marginal density functions. Very often, in some of the applications I've worked on, we have a really complicated system. We have some random inputs to it. We want to understand what happens with the output. We want to understand what happens with the output when we put it through the complicated system. This all fits under this framework. So the way I think about Monte Carlo is we simulate some realization of a stochastic process. And I'm going to simulate a finite sample, x1 up to xm. It can be good old-fashioned Monte Carlo, meaning classical. Meaning classical Monte Carlo IAD sampling. It could be Markov chain Monte Carlo. It could be simulating mixing processes. It can be much more general than anything. It can be quite general. Simulation of any stochastic process at this point. What we'd like to do, so I should say this first part of the talk is kind of agnostic to the particular method of simulation you're using, all right, conceptual. Alright, conceptual. I'm going to focus on the conceptual level, not on the detailed kind of conditions. We think we want this Monte Carlo sample size to be sufficiently large. Well, the reason I want it to be sufficiently large is because I'm going to construct a statistic, right, that's a function of that simulated data, and I hope it's pretty close to these things I care about. So, classical example is just a A classical example is just a univariate mean, right? I just want a univariate mean, and I would just estimate it with the sample mean. And if m is large, the strong vault, then one of its incarnations would say we're pretty close to 5. All right? So that would be the justification for using Monte Carlo in this context. Well, no matter how large that Monte Carlo sample size is, there's some error. I don't get it exactly. And so it's a some And so it's some unknown, it's multivariate in this context, error between the thing on my statistic, my estimate, right, and the truth. But just because I don't know it doesn't mean we can't assess it through its approximate sampling distribution. Now, it is very possible to simulate stochastic processes for which a central element theorem does not hold. It's very simple to come up with examples, actually. But this is just a But let's just suppose it does. We're sufficiently large, and then we have some such limit theorem where the mean is centered at the origin and there's some covariance matrix sigma. Now, in general, sigma is wiggly complicated. Because, for example, in simulating a Markov chain, it has to account for the temporal dependence in the simulation. It also has to account for the dependence between the components, and it's all wrapped up in that one. And it's all wrapped up in that one matrix. And it's often high-dimensional. And some of the neuroimaging applications I'm working on, I'm thinking about posteriors that have five or six million parameters. I'm not estimating a full covariance matrix for all five or six million. We have nuisance parameters on that setting too. But we need to estimate it in order to assess this simultaneous Monte Carlo error. Right, so I'll just say if you click on this link, you'll see lots of methods for doing that. All right, so I spent five or six years of my career just coming up with methods for doing that in a reasonable way that have good frequency properties. I'm not going to mention that any further. Let's just assume we can do it now. Simultaneous? Yeah, because I want a confidence region for this whole. A confidence region for this whole parameter. Not just one component. So there's lots of ways to create confidence regions based upon you've estimated signal. There's the classical way where you do. There's a lot going on to the slide. I'm going to explain every piece of it. And this is just in the two-component setting. Okay, but the classical way would be to draw the usual elliptical regions. They're minimum volume, they have the right coverage probabilities, they have nice asymptotic properties. The one thing they do not have are nice marginal interpretations. And so that's what I want. I want something that has nice marginal interpretations because maybe in a Bayesian setting I'm interested in one marginal density. Interested in one marginal density, one parameter, I want to make estimates or difference about it. Or maybe there's just lots of nuisance parameters and I only care about a smaller subset. So these things are really a pain. So what's a solution? Well, one possible, so I'd like a rectangular region because it'd have nice marginal interpretation, right? But one thing I could do is just One thing I could do is just ignore the dependence between components and estimate individual, you know, for each component, just estimate its error individually and then construct that box. And that's indicated by this inner box, right? Well, the problem is it's too small. It's going to have terrible coverage probabilities if you do that. Well, the alternative, a standard alternative, I shouldn't say the, is to do something conservative like a bond. Is to do something conservative like a Bonfroni correction or any of the other corrections you can do for multiplicity. And that's indicated by the outer box. And so that's going to be too big. It's going to cover too much. It's going to be too, you know, it's just going to be too conservative. But now I've lower bounded a box and I've upper bounded a box. There's got to be something in between that works, right? Well, this is the final piece. This is the final piece. All you have to do, if you want to keep kind of the ratio of the heights and the widths constant, is to search along this line segment using a simple bisection method in order to find something that has the right probability. All right? And you can do that wicked fast even in high dimensions. So to give you kind of an illustration of how this might work, this is a simulation from a one-dimensional distribution, but I want to make inference. But I want to make inference about three things. All right. Well, first of all, I want to do the mean, and then I want to do the 10th percentile and the 90th percentile in order to get some sort of kind of central region of interest. All right? So we heard that Monte Carlo simulation might have a billion observations earlier. I've got a thousand. Okay. Just for illustration purposes. This is the Illustration purposes. This is just a KDE estimate of the resulting density function. The black line here is the estimate of the mean. This is the 10th percentile, and this is the 90th percentile. Now, the important part here are the shaded regions, right? Because these are those confidence regions, this simultaneous 95% confidence region for all three components. So this describes how precisely we're estimating each. How precisely we're estimating each of these, and notice they're not all equal. We're estimating the 10th percentile much better than we are even the mean or the 90th percentile. So this kind of graphically allows you to assess how precisely you're estimating the features you're interested in. So if you think that, wow, I really care about the tiny percentile, I want it to be more precisely estimated, you've got to run it longer. You've got to run it longer. Make the width of the thesis interval smaller. On the other hand, if you can't run it longer, well, at least you understand the uncertainty in your estimate of that point. I should have said this is all implemented in an R package called SIMTools. This is an example taken from that package. So So all of that assumed that we're producing a good sample, right? We're producing a good representative sample from the distribution of interest. Well, sometimes that's hard, all right? And so I want to take a step back and say this is also a key feature of doing good Monte Carlo experiments. So this is not really an issue in good old-fashioned Monte Carlo, right? Because there you're making Carlo, right? Because there you're making IID draws from the distribution of interest explicitly, right? But you know, it depends on sample size mostly at that point. And MCMC is very important. It's much harder to do. And I hope you note that it's not just for Bayesians. Because I work on MCMC a lot, I've been accused of being a diagonal Bayesian, but actually that's not true. In fact, I'm going to In fact, I'm going to show you a frequent application hopefully by the end of my talk. So, the workhorse algorithm in MCMC is Metropolis Hastings. I'm not going to go through it too much because I think it's a pretty standard algorithm. Now we're dealing with a target density, not just a distribution. The key feature here is I have to pick a proposal distribution that depends on potentially a vector of parameters H. You, given that the simulation is at a current state, you draw a proposal and you accept that proposal if it, you know, with probability, with this probability. Just a ratio here. So normalizing constants don't matter and things like that. But you can get a, you can decide to reject it, in which case you'd have a repeated value in your simulation. This choice of H is utterly crucial to good finite sample performance of the algorithm. There's been a lot of work on kind of upper bounding the convergence of these things. We took a different route. We want to say, can we identify when the choice of H is going to cause the algorithm to fail? Because then if we know when it's going to fail, before we start the simulation. Going to fail before we start the simulation, we can avoid it. So, I'm going to define a sub h here. So, this is just the Hastings ratio of the minimum of 1 to make sure it's a probability. So, this is just the acceptance probability given your current state of x at the proposed value. That's all this formula says. So, the theorem in our paper says no matter what the point, x is, No matter what the point x is, but distance, I'm being a little bit agnostic about the distance. We do it for both total variation and Wascherstein distances that we just heard in my presentation that's related to them in optimal transport literature. But we can bound the distance at the t of step below by this quantity, 1 minus a sub h. This is the number of steps you've simulated at that point in time. Well if this looks like it's if a sub h is pretty close to zero But this thing's pretty close to one no matter how many iterations you've simulated and that's a problem because then you're not converging the distances in points So the answer to when does Metropolis Hastings fail is choose say the proposal scaling or other features of it so that we avoid this being approximated. This being approximately zero. I'll give you a concrete, a more concrete example. It's very common in Metropolis Hastings to have Gaussian proposals. So I'm going to consider a generic Gaussian proposal here. mu is just a mean function based on the current value of the chain. If mu of x is a constant, you recover the independent sampler. If it's just x, you recover the random walk algorithm. You can see You can set it up as x plus the gradient of the log and the target to get mala. Because I'm allowing a general covariance matrix, you have all the Riemanni and manifold versions of those things as well. So this covers a decent sized class of very common standard basic things to term the Chopa safety zero things. Well, you can bound this acceptance probability above by this quantity. So let's think about this quantity. Well, this is the target. This quantity. Well, this is the target distribution, so this is totally arbitrary at this point. The conclusion does not depend upon features of the target distribution. This is from the proposal, so it's a constant. 2 pi is obviously a constant. The only thing I get to pick is h to the d over 2. So this means if the dimension is large, and I choose h bigger than or equal to 1, we're in trouble. Right? Right? So you have to choose H small in order to avoid poor behavior. If you really, one of our theorems says is if you let D go off to infinity, so for really large settings, right, then H should scale like 1 over D to a power. It gets complicated to also incorporate the observed data size in here, but we do it in our paper. But we do it in our paper. I'm not going to tell you the details because it would take a few pages to describe fully. But the point here is that if you let D scale with n, so you're letting the parameter size grow with the number of observations, then h also has to be scaled according to that. The fundamental idea here is that you have to have the variance of your proposal distribution be smaller in some sense than the variance of the targeted distribution. In some sense, than the variance of the target distribution. So you encounter that this is sometimes in Bayesian applications where when n gets very large, you have a posterior contraction around the mother. We have a theorem about that as well. So you need that scaling to also contract. Yeah, go for it. H is a very good thing. Uh age is a dimensionful quantity, so it could be small and light years and big and metameters. What is small and what is big? Yeah, I think we're doing everything on the scale of the problem. Okay. So how do we define the scale of the problem? It's whatever the units of your problem are. X. It's whatever it is. Okay. Okay. But f of X. But f of x could also be y and L. What scale do you just find the scale? This is a scale with a proposal distribution only. No, but it has to be related to some built-in scale into f of x, right? Yeah, and I just said our theorem says, right, that the variance of the proposal has to be smaller than the variance of the target. Okay, cool. Okay, any questions about that stuff? All right, well now for something completely different. All right. I want to give a particular application that was developed by a colleague of mine at Sarah's. Colleague of mine at Sarah's Charlie Geyer a long time ago. I think it's used in statistics, I think it's underappreciated in statistics, but it is just a method for using Monte Carlo methods to approximate a likelihood. So the basic setting that Charlie considered in his early work was now I'm going to let my target density function depend upon some parameters, gamma, right? Parameters, gamma, right? I'm going to assume that all I know is h sub gamma. I cannot compute the normalizing function. And so you have the standard likelihood calculation that things like the gradient or profiles or things like that are tough to calculate because you don't know the normalizing function. So that's the setting that he considered. And the basic solution was to use important sampling. And it connects with both good old-fashioned Monte Carlo. Next with both good old-fashioned Monte Carlo, and sometimes you have to use Metropolis Hastings to fit these things as well, inside of important center. So I'm going to suppose I can pick a density, G sub alpha. Alpha is fixed. It's something I pick, right? And it has this form. I know N in this case potentially, and I know B sub alpha. But we can simulate from it. Well, why is this interesting? Why is this interesting? Well, if I look at the ratio of the unnormalizing target density to the unnormalized imported sampling density, right, a simple calculation says that that's equal to the ratio of the normalizing confidence. Well, the reason why that's good is because it's an expectation. I can say those things are actually an expected value, which means I can appeal to a strong law if I do simulation to estimate them. So if I draw random. So if I draw random samples from or Markov chain samples, I can just use a sample mean to estimate that ratio of normalizing constants. Now, alpha is fixed, right? It doesn't play any role in the inference about gamma. These are all fixed things. So if I want to take derivatives, they don't matter. You can approximate the log likelihood by just taking the log of the ratio of the unnormalized densities, you know. Densities, you know, minus the log of the sum. If the Monte Carlo sample size is large enough, it converges. I mean, it will well approximate the true likelihood. All right. Charlie proved that it does converge. He also proved that the Monte Carlo maximum likelihood estimates, so we just maximize it, it converges to the true MLE. He also proved that the profile Monte Carlo likelihood. Prove that the profile Monte Carlo likelihoods converge to the true profile likelihoods. He's got asymptotic normality and a lot of other results, too. So I'm not going to go into all of those theoretical details. I'll say that Charlie and I co-supervised a student that actually implemented this in an architecture on a large class of statistical models that previously could not be fit by any other method. Good thing I'm almost done. Okay. So this is another way we might want to incorporate both the Monte Carlo sample size and the observed data sample size. So in a follow-up paper, one of Charlie's students proved that if we take the Monte Carlo maximum likelihood estimator, which is a function of both the Monte Carlo sample size and the observed data sample size, that it has this asymptotic. That it has this asymptotic distribution. Now, gamma, they do not assume that the model is correctly specified. And so, gamma star minimizes the cobalt divergence between the two. Notice that in the covariance here, it has both the observed data sample size and the Monte Carlo data sample size. And so this is one way to incorporate both directly in your likelihood analysis. Directly in your likelihood analysis. Now, I mean, if M is a billion, that's probably not going to dominate if N is 100, right? But for smaller sample sizes, it could be very important. I'll just say there is no free lunch, right? There's no free lunch because the construction of the important sampling distribution can be very complicated and require very complicated MCMC algorithms to suddenly come up. To simulate from, because likelihoods can be quite complicated to match. Last slide. I'll say that there are settings where simulation is costly and you can't simulate essentially an infinite number of events. You just can't do it. And so it's not exactly clear what to do in those settings. I have some thoughts about it, but we can talk about that later. I didn't even scratch the surface of the work on. The surface of the work on high-dimensional meaning when the observed data size goes is large and the dimension of the problem is large. I just gave you a taste of some of the results. And I think there's more work to be done on incorporating both this Monte Carlo error and the alternational error. Thank you very much. Watching that. No. Well, there are people watching. There's no questions. Maybe you can restart the talk. So I just want to